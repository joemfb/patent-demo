<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626758-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626758</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13289222</doc-number>
<date>20111104</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<us-term-of-grant>
<us-term-extension>136</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>17</main-group>
<subgroup>30</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>707731</main-classification>
<further-classification>707609</further-classification>
<further-classification>707723</further-classification>
</classification-national>
<invention-title id="d2e53">Filtering system for providing personalized information in the absence of negative data</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5185813</doc-number>
<kind>A</kind>
<name>Tsujimoto</name>
<date>19930200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5519608</doc-number>
<kind>A</kind>
<name>Kupiec</name>
<date>19960500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5649068</doc-number>
<kind>A</kind>
<name>Boser et al.</name>
<date>19970700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5675819</doc-number>
<kind>A</kind>
<name>Schuetze</name>
<date>19971000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5870740</doc-number>
<kind>A</kind>
<name>Rose et al.</name>
<date>19990200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>5905863</doc-number>
<kind>A</kind>
<name>Knowles et al.</name>
<date>19990500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>5920854</doc-number>
<kind>A</kind>
<name>Kirsch et al.</name>
<date>19990700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>5920859</doc-number>
<kind>A</kind>
<name>Li</name>
<date>19990700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>5926808</doc-number>
<kind>A</kind>
<name>Evans et al.</name>
<date>19990700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>5940821</doc-number>
<kind>A</kind>
<name>Wical</name>
<date>19990800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>5974412</doc-number>
<kind>A</kind>
<name>Hazlehurst et al.</name>
<date>19991000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6009410</doc-number>
<kind>A</kind>
<name>LeMole et al.</name>
<date>19991200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>6014666</doc-number>
<kind>A</kind>
<name>Helland et al.</name>
<date>20000100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>6189002</doc-number>
<kind>B1</kind>
<name>Roitblat</name>
<date>20010200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>6285999</doc-number>
<kind>B1</kind>
<name>Page</name>
<date>20010900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>6341277</doc-number>
<kind>B1</kind>
<name>Coden et al.</name>
<date>20020100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>6347317</doc-number>
<kind>B1</kind>
<name>Singhal</name>
<date>20020200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>6397211</doc-number>
<kind>B1</kind>
<name>Cooper</name>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>6430559</doc-number>
<kind>B1</kind>
<name>Zhai</name>
<date>20020800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>6560590</doc-number>
<kind>B1</kind>
<name>Shwe et al.</name>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>6567797</doc-number>
<kind>B1</kind>
<name>Schuetze et al.</name>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>6728701</doc-number>
<kind>B1</kind>
<name>Stoica</name>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>7031961</doc-number>
<kind>B2</kind>
<name>Pitkow et al.</name>
<date>20060400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>2001/0047353</doc-number>
<kind>A1</kind>
<name>Talib et al.</name>
<date>20011100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>2003/0004995</doc-number>
<kind>A1</kind>
<name>Novaes</name>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>2003/0069877</doc-number>
<kind>A1</kind>
<name>Grefenstette et al.</name>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00027">
<othercit>Dempster, A.P. et al., &#x201c;Maximum Likelihood from Incomplete Data via the EM Algorithm,&#x201d; Journal of the Royal Statistical Society, Series B 39, 1977, pp. 1-38.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00028">
<othercit>Joachims, T. et al., &#x201c;WebWatcher: A Tour Guide for the World Wide Web,&#x201d; Proceedings of the International Joint Conference on Artificial Intelligence, 1997.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00029">
<othercit>Notice of Allowance issued in U.S. Appl. No. 09/721,008, mailed Feb. 24, 2009, 7 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00030">
<othercit>Notice of Allowance issued in U.S. Appl. No. 12/470,530, mailed Oct. 8, 2010, 8 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00031">
<othercit>Office Action issued in U.S. Appl. No. 09/721,008, mailed May 20, 2004, 55 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00032">
<othercit>Office Action issued in U.S. Appl. No. 09/721,008, mailed Sep. 21, 2005, 20 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00033">
<othercit>Rocchio, Jr., J.J. et al., Relevance Feedback in Information Retrieval, G. Salton (ed.), &#x201c;The SMART Retrieval System: Experiments . . . ,&#x201d; Prentice-Hall, 1971, pp. 313-323.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00034">
<othercit>Vapnik, V.N., &#x201c;Statistical Learning Theory,&#x201d; John Wiley &#x26; Sons, Inc., New York, 1998, pp. 1-721.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00035">
<othercit>Yu, Clement et al., &#x201c;Finding the Most Similar Documents . . . ,&#x201d; Proceedings of the IEEE Forum Research and Technology Advances in Digital Libraries, May 19-21, 1999, pp. 150-162.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00036">
<othercit>Office Action issued in U.S. Appl. No. 12/987,046, mailed Mar. 24, 2011, 8 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00037">
<othercit>Notice of Allowance issued in U.S. Appl. No. 12/987,046, mailed Jul. 7, 2011, 6 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>18</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>707609</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707723</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707731</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>4</number-of-drawing-sheets>
<number-of-figures>6</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>12987046</doc-number>
<date>20110107</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>8060507</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13289222</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>12470530</doc-number>
<date>20090522</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>7890505</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>12987046</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>09721008</doc-number>
<date>20001122</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>7567958</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>12470530</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60194429</doc-number>
<date>20000404</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120066220</doc-number>
<kind>A1</kind>
<date>20120315</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Alspector</last-name>
<first-name>Joshua</first-name>
<address>
<city>Colorado Springs</city>
<state>CO</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Kolcz</last-name>
<first-name>Aleksander</first-name>
<address>
<city>Colorado Springs</city>
<state>CO</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Alspector</last-name>
<first-name>Joshua</first-name>
<address>
<city>Colorado Springs</city>
<state>CO</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Kolcz</last-name>
<first-name>Aleksander</first-name>
<address>
<city>Colorado Springs</city>
<state>CO</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Finnegan, Henderson, Farabow, Garrett &#x26; Dunner, L.L.P.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>AOL Inc.</orgname>
<role>02</role>
<address>
<city>Dulles</city>
<state>VA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Alam</last-name>
<first-name>Shahid</first-name>
<department>2162</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Systems and methods are provided for ranking documents. In accordance with certain implementations, a user selects one or more documents for viewing from a set of documents. Documents selected by the user for viewing within a predetermined time period are maintained in a profile associated with the user. At least one positive word vector is generated using words contained in at least a segment of at least one of the documents stored in the profile. At least one negative word vector is generated using words contained in at least a segment of at least one of the documents that was not selected by the user for viewing. Document word vectors are generated for documents to be ranked. A vector space relationship analysis of the positive word vector, the negative word vector, and the document word vectors is performed. The documents are ranked based on the performed vector space relationship analysis.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="130.13mm" wi="186.44mm" file="US08626758-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="154.43mm" wi="189.06mm" file="US08626758-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="225.55mm" wi="168.57mm" file="US08626758-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="245.45mm" wi="186.52mm" file="US08626758-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="210.90mm" wi="173.06mm" file="US08626758-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application is a continuation of U.S. application Ser. No. 12/987,046, filed Jan. 7, 2011 (now allowed), which is a continuation of U.S. application Ser. No. 12/470,530, filed May 22, 2009 (now U.S. Pat. No. 7,890,505), which is a continuation of U.S. application Ser. No. 09/721,008, filed Nov. 22, 2000 (now U.S. Pat. No. 7,567,958), which claims priority from U.S. Provisional Application No. 60/194,429, filed Apr. 4, 2000. The entire contents of the above applications are expressly incorporated herein by reference.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading>
<heading id="h-0003" level="1">Technical Field</heading>
<p id="p-0003" num="0002">The present invention relates generally to information filtering, and more particularly, to a system for creating a model of user preferences for content based on a user's history of selecting text articles for viewing in an online environment.</p>
<heading id="h-0004" level="1">Statement of the Problem</heading>
<p id="p-0004" num="0003">The huge amount of information available at any one time in the evolving world wide information infrastructure, and particularly the volume of information accessible via the Internet, can easily overwhelm someone wishing to locate specific items of this information. Although it is advantageous to have such a large quantity of information available, only a small amount of it is usually relevant at any time to a given person. In order to provide a manageable volume of relevant information, an intelligent filtering system that &#x2018;understands&#x2019; a user's need for specific types of information is invaluable. A user model which captures the user's preferences for information is thus required. Many methods are known in the art for creating such a user model with varying degrees of intrusiveness and effectiveness.</p>
<p id="p-0005" num="0004">One presently known way of reducing this tremendous volume of information to a relevant and manageable size is to use an &#x2018;information filtering agent&#x2019; which can select information according to the interest and/or need of a user. However, at present, few information filtering agents exist for the evolving world wide multimedia information infrastructure, and particularly for the Internet.</p>
<p id="p-0006" num="0005">Historically, user modeling has been applied to information filtering in the literature and in practice. This modeling has become important commercially with the advent of the Internet. The Internet makes possible access to information, product sales, services and communication for anyone with access thereto. However, the Internet presents an overwhelming amount of information and a large number of items to purchase. It is thus difficult for a human to sort through this tremendous volume of Internet content without some help from a filtering or recommendation service. Therefore, &#x2018;personalization&#x2019; of Internet content and advertising is needed to reduce the myriad of choices down to a manageable number for a given individual.</p>
<p id="p-0007" num="0006">All previously known personalization technologies rely on building a model of a user's preferences. Therefore, personalization requires modeling the user's mind with as many of the attendant subtleties as possible. Ideally, a perfect computer model of a user's brain would determine the user's preferences exactly and track them as the user's tastes, context, or location change. Such a model would allow a personal newspaper, for example, to contain all of the articles the user is interested in, and none of the articles in which the user is not interested. The perfect model would also allow advertisers to generate banner ads with 100% &#x2018;click-through&#x2019; rates (i.e., a viewer would peruse every ad displayed) and would allow e-commerce vendors to present only products that each given user would buy.</p>
<p id="p-0008" num="0007">Fill-in profiles represent the simplest form of user modeling for personalization technology. When using a fill-in profile, the user fills in a form, which may ask for demographic information such as income, education, children, zip code, sex and age. The form may further ask for interest information such as sports, hobbies, entertainment, fashion, technology or news about a particular region or institution. Internet web sites that have registration procedures typically request information of this sort. Vendors may target advertising based on these profiles in exchange for users having access to the content site. Such profiles are the basis for almost all of the targeted advertising currently used on the Internet. This type of simple user model misses much of the richness of a user's interests because these interests do not necessarily fall into neat categories. Privacy-concerned users may also purposefully enter inaccurate information when forced to deal with this model. Furthermore, most people have trouble articulating the full range of their preferences even when not restricted by a form.</p>
<p id="p-0009" num="0008">Another filtering method is called &#x2018;clique-based recommendation&#x2019;, which is also known as &#x2018;collaborative filtering&#x2019;. This method presumes that if a person's stated preferences are similar to those of a group or clique of others in some aspects, the person's preferences will also be similar to the clique's preferences in other aspects. For example, if a particular viewer likes a certain set of movies and a clique of other viewers enjoy the same set, then it is likely that other movies enjoyed by that clique will also be enjoyed by the viewer. Because the Internet makes it easy to collect preference information for a large group, collaborative filtering has become the basis for many presently known recommendation services. Note that collaborative filtering is a richer form of recommendation than a fill-in profile because, for example, it is difficult to characterize a book simply by noting that it is in the category of sports. A problem with clique-based systems, however, is the need for explicit feedback by the user, such as a buying or rating decision.</p>
<p id="p-0010" num="0009">Feature-based recommendation is a more sophisticated form of preference profiling because it considers multiple aspects of a product and how they may interact. For example, a person may like movies that have the features of action-adventure, rated R (but not G), and which receive a good review by a particular critic. Such features or attributes of a product can be used to create a sophisticated preference model for an individual user. A multiple-feature classifier such as a neural network can capture the complexity of user preferences if the feature set is rich enough.</p>
<p id="p-0011" num="0010">Text-based recommendation is a rich form of feature-based recommendation. Years of research in information retrieval has yielded methods of characterizing text which are quite effective. These methods are generally referred to as word vector-space methods. The concept behind text-based &#x2018;recommenders&#x2019; is that documents containing the same frequencies of words can be grouped together or clustered. Documents whose word frequencies are similar are considered closely clustered m the word vector space. Thus, if a user selects certain documents, then it is likely that the user would want to read other documents that have similar word frequencies. Because most of the information on the Internet (including news, product descriptions, and advertising) is in the form of text, text-based recommendation methods can be used to more accurately determine users' preferences for all sorts of Internet information. It is desirable that such methods be completely unobtrusive to a user, by not requiring the user to fill in a form or rate products.</p>
<p id="p-0012" num="0011">Several techniques are known in the art for prioritizing word-based content by asking users to rate articles on a numerical scale. These techniques assemble training data that contains both positive (highly rated articles) and negative (low rated articles) data. However, the need to rate articles is a burden to users. If a user is asked to look at all the articles in an archive or news site and read all the ones of interest, it is also possible to assemble a set of positive data (all the articles the user read or clicked on) and negative data (all those not read). Although the user is not asked for a numerical rank, a binary value can be assigned to each article (either read or not read). However, this, too, is a burden. The more usual scenario for an online newspaper has a reader perusing some of the articles but not having time to read all of them. One cannot assume, a priori, that unread articles are of no interest to the user, so the negative data are thus uncertain. Thus, what is needed is a truly unobtrusive system which operates on only positive data.</p>
<heading id="h-0005" level="1">Solution to the Problem</heading>
<p id="p-0013" num="0012">The present invention overcomes the aforementioned problems of the prior art and achieves an advance in the field by providing a system which allows a viewer to read an online newspaper or other content site in an information environment such as the Internet in a manner already familiar to the viewer.</p>
<p id="p-0014" num="0013">The method of the present invention observes the user's actions during the normal course of browsing through a content site, and creates a model of the user's preferences for various types of articles. This model is created as an Internet user &#x2018;clicks&#x2019; on articles which the user desires to read, without requiring any other feedback from the user. The user model is then employed to reorganize the content site so that the articles preferred by the user are brought to the fore for easy access. This model can also be used to present the user with advertising material based on the user's demonstrated interests.</p>
<p id="p-0015" num="0014">The system of the present invention performs the above functions by using word vector-space representation of the documents combined with adaptive learning techniques. A word vector for a document is created by counting all the occurrences of each word in a document and creating a vector whose components comprise the word frequencies. A document is represented by a point in a high-dimensional space whose axes represent the words in a given dictionary. Thus, similar documents are close together in this vector-space. Generally, &#x2018;stop words&#x2019; (&#x201c;and&#x201d;, &#x201c;the&#x201d;, &#x201c;on&#x201d;, etc.) are eliminated and stems of words are used so that, for example, &#x201c;see&#x201d; and &#x201c;seeing&#x201d; are considered to be the same word. The word vector of an article forms the input to an adaptive ranking engine. The output of the ranking engine is a value which represents the strength of a particular user's preference for reading that article. In this manner, the contents of an online newspaper or an archive of any type can be rank ordered by the numerical value of the output of the ranking system.</p>
<p id="p-0016" num="0015">Known techniques for prioritizing word-based content by asking users to numerically rate articles assume, a priori, some arbitrary low rating for unread articles, so the negative data are thus uncertain. Therefore, a truly unobtrusive system should operate on only positive data. The system of the present invention creates a user model by ranking articles for an individual user without requiring negative data or feedback from a user (reader). This user model is then used to personalize content in an information environment such as an online newspaper.</p>
<p id="p-0017" num="0016">Various techniques, such as fill-in profiles, collaborative filtering and text-based methods, may be used to create a user model for content preferences. Text-based methods are used as input to a learning system in the present invention. There are a variety of learning systems known in the art for use with text-based features. These learning systems include Bayesian techniques, Rocchio's method, neural networks, and support vector machines. Most of these methods are computationally expensive, especially when it is desired to optimize estimates of negative data, making the methods unsuitable for an online system serving large numbers of users. Because of the high-dimensional vector space in which word-based ranking engines operate, a simplification of some of these methods, especially support vector machines, is possible. Although the use of Bayesian techniques and Rocchio's method may not be computationally efficient for online systems, these methods may nevertheless be employed by the present system where off-line processing is feasible.</p>
<p id="p-0018" num="0017">In the prior art, a user typically forms a query by typing in keywords or a natural language query. The document set is then searched for similarity to the query. Typically, Rocchio's method for information retrieval is employed to determine similarity to the query, which involves specifying a distance measure (often a normalized inner product) between the query vector and the document vector. Documents with the smallest distance are ranked highest. The method of the present invention can use a Rocchio's algorithm to rank articles as well as other techniques, which are described below.</p>
<p id="p-0019" num="0018">The present system uses word vectors to represent documents. In the system of the present invention, queries are not explicitly typed by the user. Instead, a query is formed implicitly by the vector of all documents previously read by the user. This query is a potentially long vector of word frequencies, which represent the personal interests of the user. In contrast, the document set is composed of short vectors, typically a headline or title and, perhaps, a lead paragraph or a summary. Therefore, the present system measures distance between the long vector representing the implicit query of the interests of the user and the short vectors representing the contents of the document set. These short vectors, in one embodiment of the invention, may have binary components representing the presence or absence of each word, thereby simplifying the computation of document relevance.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 1</figref> is a diagram of an exemplary embodiment of the present system for personalizing content and advertising;</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 2</figref> depicts a subset of axes representing three dimensions in a word vector space;</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 3</figref> is a flowchart illustrating, at a high level, steps which may be performed in practicing one embodiment of the method of the present invention;</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 4</figref> is a diagram depicting a document classifier in two dimensional space corresponding to two words in a dictionary; and</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIGS. 5A and 5B</figref> illustrate the use of a Support Vector Machine in tagging articles.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0007" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 1</figref> is a diagram of an exemplary embodiment of the present system <b>100</b> for personalizing content and advertising. As shown in <figref idref="DRAWINGS">FIG. 1</figref>, documents, such as articles in an online newspaper, are stored in a document database or other document repository <b>104</b> at a content site <b>102</b>. Content site <b>102</b> also includes content server <b>103</b>, which is coupled to document database <b>104</b>. The embodiment described herein uses the Internet <b>115</b> (or any other suitable information transmission medium) to transmit the documents from content server <b>103</b> to a computer <b>110</b>, where the documents are viewed by a user via a web browser, or the like. In an exemplary embodiment, HTTP protocol is used for fetching and displaying the documents, but any suitable document display protocol may alternatively be employed.</p>
<p id="p-0026" num="0025">The system described so far allows unpersonalized delivery of information to users. In order to personalize the information for a particular user, a database is created that identifies each unique user through a login procedure in the presently described embodiment. Either the content site <b>102</b> or a third party (not shown), therefore, maintains a separate login database <b>106</b> of user information.</p>
<p id="p-0027" num="0026">In the presently described embodiment, information associated with a given user is divided into two databases (or any other type of document repository). One database <b>106</b> contains information facilitating user login and password registration, and a second database <b>107</b> is used to store user profile data. Profile database <b>107</b> contains combined &#x2018;word vectors&#x2019; (described in detail below) that represent the articles read by a user. These combined word vectors are accompanied by algorithm-specific representation of the profile data (&#x2018;user query vectors&#x2019;, also described below). To allow for users' interests to change, this profile data is made adaptive by periodically deleting documents that are &#x2018;old&#x2019; based on any desired criteria. One method of providing an adaptive profile is to store a fixed maximum number of documents and delete the oldest documents when the maximum number is reached. Data in profile database <b>107</b> is used by a ranking engine <b>105</b> to rank articles, contained in document database <b>104</b>, for each user. The ranking of articles, explained in detail below, is accomplished by software and/or hardware on content server <b>103</b>, or on another computer, using the method of the present system. Ranking engine <b>105</b>, along with login and profile databases <b>106</b>/<b>107</b>, are typically part of content site <b>102</b>, but may, alternatively, be located at a third party site, remote from content site <b>102</b>.</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 2</figref> depicts a subset of axes representing three dimensions in a word vector space <b>200</b>. The three axes shown represent the words &#x201c;filter&#x201d;, &#x201c;text&#x201d; and &#x201c;Internet&#x201d;. A typical dictionary for use with documents has thousands of words, but it is infeasible to depict such a high-dimensional vector space on a two-dimensional drawing. It is to be understood, however, that a typical word vector space may have thousands of dimensions. Each point in the diagram of <figref idref="DRAWINGS">FIG. 2</figref> represents a document with the coordinates of the point representing the frequencies of the corresponding word within the document. For short document excerpts such as headlines of news articles, the values of the components are likely to be zero or one. In general, the entire word vector representing the document will likely contain many zeroes because most of the significant words in document dictionary <b>111</b> will not be in any one document. In this particular word vector space <b>200</b>, as well as in the &#x2018;real&#x2019; high-dimensional vector space, documents containing similar concepts are found close together. On the other hand, dissimilar documents are far apart as given by a distance measure in this space. A typical metric, well-known in the art, representing the distance between two documents in this vector space is formed by the normalized dot product (also known as the inner product) of the two vectors representing the documents.</p>
<p id="p-0029" num="0028">As shown in <figref idref="DRAWINGS">FIG. 2</figref>, the group of points indicated by reference number <b>202</b> represents a cluster of documents each containing the words &#x201c;text&#x201d; and &#x201c;filter&#x201d;, and the group of points indicated by reference number <b>203</b> represents a cluster of documents containing the words &#x201c;text&#x201d; and &#x201c;Internet&#x201d;. For example, the document indicated by reference number <b>201</b>, at coordinates (2, 5, 3, . . . ) contains 2 instances of &#x201c;filter&#x201d;, 5 instances of &#x201c;text&#x201d;, and 3 instances of &#x201c;Internet&#x201d;, as indicated by the relative distances of each component word along its respective axis.</p>
<p id="p-0030" num="0029">As discussed in detail below with respect to <figref idref="DRAWINGS">FIG. 3</figref>, the present method creates a word vector for each document in database <b>104</b>. A word vector comprises a number of word count descriptors (up to the total number of words in the document dictionary), each of which indicates a count of all of the occurrences of each word contained in document dictionary <b>111</b> that are also contained in a given document, or a preselected segment thereof, such as a headline, lead or summary. The count usually has a maximum value of 1 for a short headline; in an alternative embodiment, the count may be set to 1 for any number of occurrences (greater than one) of the same word within a document. In another alternative embodiment, the present method may use different dictionaries according to the context of a given word. Document dictionary <b>111</b> typically contains many thousands of words relevant to certain selected topics. An exemplary word vector is formatted as shown below in Table 1, although any format suitable for input to a particular ranking engine <b>105</b> may be employed.</p>
<p id="p-0031" num="0030">
<tables id="TABLE-US-00001" num="00001">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="offset" colwidth="21pt" align="left"/>
<colspec colname="1" colwidth="77pt" align="left"/>
<colspec colname="2" colwidth="119pt" align="left"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="2" rowsep="1">TABLE 1</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry>Dictionary words</entry>
<entry>W1, W2, W3, W4, W5, . . . Wn</entry>
</row>
<row>
<entry/>
<entry>Words in document</entry>
<entry>W1, W3, W4</entry>
</row>
<row>
<entry/>
<entry>Word vector</entry>
<entry>1, 0, 1, 1, 0, . . . 0</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0032" num="0031">As can be seen from Table 1, dictionary <b>111</b> contains words W<b>1</b>-Wn, and the document headline (or lead) contains words W<b>1</b>, W<b>2</b>, and W<b>4</b>. The word vector thus formed is:</p>
<p id="p-0033" num="0032">1, 0, 1, 1, 0 . . . 0</p>
<p id="p-0034" num="0033">where a &#x201c;1&#x201d; is placed in the ith position (the ith word count descriptor) in the word vector for each Wi appearing in a given document headline that has a counterpart Wi in the dictionary, and zeroes are placed in all other positions. Words in the document that are not in the dictionary are ignored.</p>
<p id="p-0035" num="0034">Full word frequency information, as shown in <figref idref="DRAWINGS">FIG. 2</figref>, describes the general case for a word vector space, in that the word count descriptors in each word vector may have a count greater than one. However, for headlines and leads, word frequencies are rarely greater than one. Therefore, restricting a word count descriptor to a binary frequency (i.e., a count of either 1 or 0 for each word) is often a very good approximation. A binary word count descriptor also simplifies implementation of the present method, and provides better results when used in conjunction with some algorithms such as support vector machines. It is to be noted that the present invention may employ word vectors comprising either binary or full-frequency word count descriptors.</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 3</figref> is a flowchart illustrating, at a high level, steps which may be performed in practicing one embodiment of the method of the present invention. Operation of the present system <b>100</b> is best understood by viewing <figref idref="DRAWINGS">FIGS. 1 and 2</figref> in conjunction with one another. As shown in <figref idref="DRAWINGS">FIG. 3</figref>, at step <b>305</b>, in an off-line, or pre-processing operation, a word vector is created for each document in database <b>104</b>, or for a preselected subset of documents therein (i.e., a single web page), typically from words appearing in headlines or leads. Word vectors may be stored in a user's profile in profile database <b>107</b> in a First-In First-Out (FIFO) queue. Note that either the full vectors or only their pointers can be stored in a FIFO. Alternatively, the documents in database <b>104</b> may be pre-processed to generate word vectors which are stored in a separate part of database <b>104</b>, with pointers to the respective full documents. At step <b>310</b>, in response to a request by a user, a set of document headlines or otherwise synopsized documents (a &#x2018;document set&#x2019;) is presented to a user by content server <b>103</b>, to be displayed on computer <b>110</b>. At step <b>312</b>, if no documents are selected from the presented document set (e.g., if the user exits the current document set, or chooses another document set), processing continues as described below at step <b>325</b>; otherwise, processing continues as directly follows.</p>
<p id="p-0037" num="0036">If all documents in database <b>104</b> are not pre-processed to create word vectors (i.e., if step <b>305</b> is not performed), then at step <b>315</b>, the word vector for the user-selected document is created on the fly as the user reads the document. Next, at step <b>320</b>, a user query vector is created which represents all of the documents that have been selected by the user over the period of time covered by the documents presently stored in the user's profile. As indicated above, the profile data is made adaptive by periodically deleting older documents. This user query vector is part of the user profile for this particular user, which is stored in profile database <b>107</b>. A user query vector is a type of word vector created in essentially the same manner as the word vector described above. An exemplary user query vector is formatted as shown below in Table 2, although any format suitable for input to a particular ranking engine <b>105</b> may be employed.</p>
<p id="p-0038" num="0037">
<tables id="TABLE-US-00002" num="00002">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="offset" colwidth="14pt" align="left"/>
<colspec colname="1" colwidth="98pt" align="left"/>
<colspec colname="2" colwidth="105pt" align="left"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="2" rowsep="1">TABLE 2</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry>Words in selected documents</entry>
<entry>W1, W3, W4, W1, W4, W1</entry>
</row>
<row>
<entry/>
<entry>Dictionary words</entry>
<entry>W1, W2, W3, W4, W5, . . . Wn</entry>
</row>
<row>
<entry/>
<entry>User query vector</entry>
<entry>3, 0, 1, 2, 0, . . . 0</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0039" num="0038">A user query vector is created which represents each document selected by a user by indicating a count of all of the occurrences of each word contained in document dictionary <b>111</b> that are also contained in the documents which are presently stored in the user's profile. As shown in Table 2, with respect to words W<b>1</b>-W<b>5</b>, which appear in document dictionary <b>111</b>, the selected documents in the user profile contain a total of 3 instances of word W<b>1</b>, 0 instances of word W<b>2</b>, 1 instance of word W<b>3</b>, 2 instances of word W<b>4</b>, and so forth. The user query vector thus formed is:</p>
<p id="p-0040" num="0039">3, 0, 1, 2, 0, . . . 0.</p>
<p id="p-0041" num="0040">Accordingly, at step <b>320</b>, the existing user query vector for the present user is updated by the word vector representing the words present in the currently selected document, both of which are then stored in the user's profile in database <b>107</b>. At this point, processing may proceed directly to step <b>325</b>, or continue at step <b>312</b> (described above), where it is determined whether the user selects another document or exits the current set. As indicated above with respect to step <b>312</b>, if another document in the current set is selected, then step <b>320</b> is repeated; if the user exits the current set (or document display program), processing continues with step <b>325</b>, below.</p>
<p id="p-0042" num="0041">At step <b>325</b>, all (or, more likely, a random sample; there are usually too many articles available for computational efficiency) &#x2018;unread&#x2019; articles, or alternatively, all articles that are presented to the user but not selected for reading, are labeled as &#x2018;negative&#x2019;. If it is feasible to process the unread (or unselected) articles off-line, then steps <b>326</b> through <b>328</b> are performed to take advantage of the Expectation Maximization (EM) algorithm (Dempster, A. P., Laird, N. M. and Rubin, D. B.: 1977, &#x201c;Maximum likelihood from incomplete data via the EM algorithm&#x201d;, <i>Journal of the Royal Statistical Society, Series B </i>39, 1-38). The EM algorithm provides a mechanism for augmenting a ranking system for personalization of news with unlabeled documents (i.e., documents that are labeled neither positive or negative with respect to a viewer's interests) when no negative data are present. If off-line processing is not feasible or not desired, then processing continues at step <b>330</b>, described further below.</p>
<p id="h-0008" num="0000">Document Labeling Using the Expectation Maximization Algorithm</p>
<p id="p-0043" num="0042">In the alternative embodiment of steps <b>326</b>-<b>328</b>, the on-line material provided by content server <b>103</b> is assumed to be composed of sections (sports, business news, etc.), where each section consists of a list of article headlines or summaries, each pointing the user to a full version of the article. Since a user usually has no trouble identifying the general section of interest, the present alternative embodiment rank-sorts the article leads in each section such that the material likely to be more relevant to the user appears near the top of the list and is thus easier to access. In such a context, personalization of these news articles is reduced to user-specific ranking of articles in each newspaper section.</p>
<p id="p-0044" num="0043">Given labeled documents/articles and unlabeled documents/articles, the Expectation Maximization algorithm provides a means of estimating the positive/negative labels which are missing from the unselected (or unread) documents. The EM algorithm is a general iterative procedure designed to obtain maximum-likelihood parameter estimates in incomplete-data environments. Using labeled/unlabeled data corresponding to selected/unselected documents, at step <b>326</b>, the EM algorithm is executed in a sequence of two-step iterations until convergence is reached. In the Expectation step, the current parameter values are used to estimate the missing data labels while, in the Maximization step, the values of model parameters are re-computed using the fixed labels as well as the label-estimates through application of the maximum likelihood principle. The result of executing the EM algorithm is a set of relevance probability estimates, for the documents in the unlabeled set. As a further enhancement to the document labeling performed in step <b>326</b>, the EM algorithm may be implemented in conjunction with a multinomial naive Bayes classifier.</p>
<p id="p-0045" num="0044">At step <b>327</b>, all documents/articles negatively labeled in step <b>326</b> are stored with the corresponding negative labels in the user's profile in database <b>107</b>. At step <b>328</b>, all of the positive labels assigned by the EM algorithm in step <b>326</b> are discarded.</p>
<p id="p-0046" num="0045">At step <b>330</b>, adaptive ranking engine <b>105</b> is incrementally &#x2018;trained&#x2019; using a Support Vector Machine (SVM) or other word vector space ranking technique by means of the user query vector created in step <b>320</b>. In the case of an SVM, a determination is made if the new word vector is a &#x2018;support&#x2019; vector by means of an optimization technique known in the art. If not, no further action is needed; if the new word vector is a support' vector, a new decision boundary is calculated and a reprioritization of articles must be calculated. In the case of a Na&#xef;ve Bayes ranking algorithm, new probabilities are calculated and used to reprioritize the list of articles. In the case of Rocchio's algorithm, new combined vectors are composed for reprioritization of articles. Note that this recalculation may be done either on the fly or, periodically, offline.</p>
<p id="p-0047" num="0046">Document ranking techniques which may be used advantageously with the present system are described below with respect to <figref idref="DRAWINGS">FIG. 4</figref>. Document ranking can be a continuous process as the reader reads new articles each day and ranking engine <b>105</b> is incrementally trained. Note that Support Vector Machines are advantageous here because if new articles are not support vectors, retraining is unnecessary. Support vector machines work well for high dimensional spaces and do not require time-consuming preprocessing. Furthermore, the word vectors can be used directly once the support vectors are found; i.e., no re-representation of the data (as, for example, with a hidden layer of neurons in a multi-layer perceptron neural network) is needed. The stored word-vector representation functions advantageously in determining the decision boundary. The use of Support Vector Machines as learning engines to build a categorizer for the purpose of tagging (categorizing) articles or prioritizing advertisements ('ads') is described in greater detail below with respect to <figref idref="DRAWINGS">FIGS. 5A and 5B</figref>.</p>
<p id="h-0009" num="0000">Document Ranking</p>
<p id="p-0048" num="0047">At step <b>335</b>, the word vector for each document in the database (or, more practically, in the group of articles desired to be ranked) is provided as input to ranking engine <b>105</b>. In an exemplary embodiment of the present system <b>100</b>, the building of profile database <b>107</b> is based only on the information relevant to the user's choice, such as headlines and leads in documents in document database <b>104</b>. This can be generalized to images and sounds for use in more advanced information systems. The user query vectors represent the preferences of each user and are composed of only positive data in the sense that it is believed that the user prefers to read similar articles. It is, however, advantageous to also have feedback from each user indicating what types of articles the user would prefer not to read (the negative data), but negative data are not available in the presently described embodiment of the invention.</p>
<p id="p-0049" num="0048">The present system utilizes the assumption that the articles not read can be used to form similar vectors of word frequencies representing the negative data for each user. The positive vector and negative vector together can form a &#x2018;query&#x2019; for selecting preferred articles using the method of Rocchio (Rocchio, J.: 1971, Relevance feedback in information retrieval, in G. Salton (ed.), <i>The SMART Retrieval System: Experiments in Automatic Document Processing</i>, Prentice-Hall, pp. 313-323). Rocchio's method is commonly used for search engines on the Internet where, as previously mentioned, the query consists of typed in keywords and is short. The documents to be searched on the Internet are web pages and may be quite long. In the case of the present invention, the situation is reversed. The query is long and the documents, such as headlines, may be quite short. The &#x2018;learning&#x2019; in Rocchio's method comprises simply assembling the positive and negative vectors based on user actions. The query is formulated as the positive vector minus a fraction of the negative vector. The ranking of documents uses the distance metric in the word vector space whereby documents close to the query vector are ranked more highly than distant ones.</p>
<p id="p-0050" num="0049">A second approach to document ranking used in an alternative embodiment of the present system may be termed Na&#xef;ve Bayes. Bayesian techniques refer to the use of conditional probabilities for, in this case, calculation of relevance in document ranking. If count is made of all documents read by a user, along with a corresponding determination of how many of them contain word W<b>1</b>, a determination can be made of the probability of the relevance of the document, given that it contains word W<b>1</b>. Similarly, a determination can be made of the probability of non-relevance, given W<b>1</b>, if negative information is available. Na&#xef;ve Bayes refers to a naive assumption that each word in a document is independent of all others. Thus, it is possible to determine the relevance of a document by simply multiplying the relevance/non-relevance probabilities of the words contained therein using the rules of statistical inference. The higher the probability, the higher is the rank given by the simple algorithm.</p>
<p id="p-0051" num="0050">A further alternative embodiment of the present system uses a learning technique known as a Support Vector Machine (SVM), which can be thought of as a type of neural network. An SVM works best in a high dimensional space of the type formed by words in a dictionary. In high-dimensional spaces, classification boundaries, such as the boundary between relevant and non-relevant documents, are often hyperplanes. A hyperplane (simply a straight line in two dimensional space) is described by a linear equation leading to a particularly simple distance calculation, which is well-suited for fast online implementation.</p>
<p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. 4</figref> depicts a document classifier <b>400</b> in two dimensional space corresponding to only two dictionary words, represented by axes W<b>1</b> and W<b>2</b>, respectively. The generalization to the case of thousands of dimensions operating in the case of the invention is understood by those skilled in the art but is impossible to represent in a two-dimensional diagram. The data points in the diagram of <figref idref="DRAWINGS">FIG. 4</figref> correspond to word vectors of relevant and non-relevant documents (indicated by &#x201c;+&#x201d; and &#x201c;&#x2212;&#x201d; characters, respectively) for a unique user. It is apparent from <figref idref="DRAWINGS">FIG. 4</figref> that, for the most part, a straight line (dotted line <b>401</b>), called a &#x2018;classification boundary&#x2019;, separates the two (relevant and non-relevant) classes of documents with only a few errors. Note that if we had a third axis available, it is possible that a plane could correctly separate all exemplars with no errors. This observation motivates the theorem that a separating surface in high-dimensional spaces is likely to be a hyperplane. The degree of relevance of a new document to the user may be measured by the distance to the classification boundary, as indicated by arrow <b>404</b> between a support vector and classification boundary <b>401</b> (e.g. rank of article=1/distance). Those documents whose word vectors are located within this predetermined distance from the classification boundary form the &#x2018;support vectors&#x2019; of the SVM, which are shown in <figref idref="DRAWINGS">FIG. 4</figref> as document groups <b>402</b> and <b>403</b>. There are a variety of methods known in the art for determining these support vectors. The distance from the classification boundary within which a word vector must fall to be considered a support vector may be determined by an optimization procedure, usually quadratic programming.</p>
<p id="p-0053" num="0052">A new document may or may not be a support vector. In accordance with the system of the present invention, the set of support vectors assembled as a user reads documents determines the learned preferences of the user. The present system uses SVMs for document ranking, as well as classification, by means of a distance measure. Any one of several known heuristic methods based on computational geometry may be used to determine if a new article's word vector may be a support vector. If it is unlikely to be such, then no retraining of the support vector machine is needed. The heuristically calculated distance to the classification boundary of an SVM may be used to determine when to retrain the SVM. Using SVMs in the environment of the present system results in high accuracy combined with computational efficiency.</p>
<p id="p-0054" num="0053">Support Vector Machines (SVMs) (Vapnik, V. N.: 1998, <i>Statistical Learning Theory</i>, John Wiley, New York) represent a class of machine-learning algorithms that explicitly estimate the location of the inter-class boundary. SVMs have been shown to be very effective in many classification tasks, including text categorization. In the classification setting, an SVM transforms the input domain into a possibly infinite dimensional space, in which the classification boundary can be modeled as a hyperplane. To identify the best hyperplane, an SVM finds a set of training points that directly determine the classification-error margin and ignores the rest of the training set. The chosen points are known as support vectors. In particular, given a set of linearly-separable points,
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>{<i>x,y:y=&#xb1;</i>1<i>x&#x3b5;</i><img id="CUSTOM-CHARACTER-00001" he="3.13mm" wi="2.79mm" file="US08626758-20140107-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/><sup>D</sup>}<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
an SVM is defined by a hyperplane for which the inequality
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>y</i>(<i>w&#xb7;x+b</i>)&#x2267;1<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
is satisfied by all elements of the training set and the norm of the hyperplane normal vector, z, is minimal. SVM implementations which may be used advantageously in the present system include a linear-kernel SVM and, for example, the SVM_LIGHT package written by Thorsten Joachims (Joachims, T., Freitag, D. and Mitchell, T.: 1997, Webwatcher: A tour guide for the world wide web, <i>Proceedings of the International Joint Conference on Artificial Intelligence</i>).
</p>
<p id="p-0055" num="0054">After applying one of the document ranking methods described above to the word vector for each document in the group of articles to be ranked, ranking engine <b>105</b> outputs, at step <b>340</b>, a document relevance value which represents the strength of a particular user's preference for reading a given article. The relevance values are temporarily stored in a file (e.g., file <b>112</b> in <figref idref="DRAWINGS">FIG. 1</figref>), in order to be sorted.</p>
<p id="p-0056" num="0055">At step <b>345</b>, the document relevance value output by ranking engine <b>105</b> is then input to content server <b>103</b> which uses the article's rank value to determine whether to present the article to the user, or to determine how to rank the article for the purpose of later presentation to the user.</p>
<p id="p-0057" num="0056">Finally, at step <b>350</b>, the relevant documents (or articles) that comprise the output of step <b>345</b> are sent to content server <b>103</b> in rank order, from which point the documents are sent to computer <b>110</b> to viewed by the user.</p>
<p id="h-0010" num="0000">Advertising Personalization</p>
<p id="p-0058" num="0057">The present system <b>100</b> may also be used to personalize advertising or marketing messages for each individual user. Presently, advertising is often served by agencies that may not be part of the content site <b>102</b>. Therefore, these agencies typically use a separate server computer <b>108</b> to send advertising material stored in advertising database <b>109</b> to ranking engine <b>105</b>. Advertising may also be personalized by information stored in profile database <b>107</b>. This personalized information is often demographic information such as age, sex, zip code, family income, interests and such other information as may be requested by a web site (e.g., site <b>102</b>) as part of an initial registration procedure. The present invention uses the word-based information stored in a given user profile in database <b>107</b> to personalize advertising either with or without regard to any other user profile information that may be collected. One step in such personalization is associating each document in database <b>104</b> with an interest category. Advertisers can use interest categories for targeting advertisements to users most likely to be interested in a particular product or service. As described below, the present system <b>100</b> automatically labels articles with categories useful to advertisers and also labels users with a hierarchical description of their interests, which can be passed on to advertisers in a context-specific manner.</p>
<p id="p-0059" num="0058">In operation, profile database <b>107</b> contains information regarding each user's interests gleaned automatically from the articles that they have read. This information is sent by ranking engine <b>105</b> to ad server <b>108</b> whenever a user is logged into a content site served by the ad network. The ad server can then prioritize advertising or marketing messages in a manner similar to the way the content server prioritizes articles. In an exemplary embodiment, these priorities are assigned by ranking engine <b>105</b> and simply passed on to content server <b>103</b> and ad server <b>108</b>. Alternatively, ranking engine <b>105</b> sends ad server <b>108</b> a list of user interests, in order of relevance, to be used internally in the ad serving process.</p>
<p id="h-0011" num="0000">Categorization of Articles Using Support Vector Machines</p>
<p id="p-0060" num="0059">Support Vector Machines can also be used as learning engines to build a categorizer for the purpose of categorizing (classifying) articles or prioritizing advertisements ('ads'). <figref idref="DRAWINGS">FIGS. 5A and 5B</figref> illustrate the use of a Support Vector Machine in tagging articles. As shown in <figref idref="DRAWINGS">FIG. 5A</figref>, during learning (training) mode, labeled documents <b>501</b>(<b>1</b>) through <b>501</b>(<i>n</i>) are input to SVM categorizer <b>505</b>. In the example shown, documents <b>501</b>(<b>1</b>) and <b>501</b>(<i>n</i>) have word vectors (1, 0, 0, . . . ) [reference no. <b>503</b>(<b>1</b>)] and (0, 0, 1, . . . ) [ref. no. <b>503</b>(<i>n</i>)], respectively, with corresponding predefined labels &#x201c;baseball&#x201d; <b>502</b>(<b>1</b>) and &#x201c;golf&#x201d; <b>502</b>(<i>n</i>). Support vectors <b>505</b> and corresponding classification boundaries are generated by SVM categorizer <b>505</b> during this learning process. It can be seen that the correspondence between the labels and word vectors is determined by the relative position of the label (i.e., the word) in document dictionary <b>111</b>.</p>
<p id="p-0061" num="0060">As shown in <figref idref="DRAWINGS">FIG. 5B</figref>, after an initial learning period, SVM categorizer <b>505</b> receives input document <b>501</b>(<i>n+</i>1) having word vector (0, 1, 0, . . . ) [ref. no. <b>503</b>(<i>n+</i>1)]. At this point, SVM categorizer <b>505</b> is &#x2018;trained&#x2019;, therefore, it automatically outputs the label &#x201c;football&#x201d; <b>502</b>(<i>n+</i>1), which has the corresponding word vector (0, 1, 0, . . . ) [ref. no. <b>503</b>(<i>n+</i>1)]. SVM categorizer <b>505</b> is able to provide this label, since the category &#x201c;football&#x201d; has been predetermined and used in training the categorizer.</p>
<p id="p-0062" num="0061">A support vector machine can be trained to distinguish between categories of articles. For example, by training on articles about &#x2018;football&#x2019; and articles about &#x2018;not football&#x2019;, a binary decision boundary can be formed. This process can be repeated multiple times for each category desired. One known method categorizes articles using the score of each category as assigned by these multiple binary categorizers.</p>
<p id="p-0063" num="0062">In one aspect of the present system, articles or advertisements are initially assigned to an appropriate user-interest category. In order to prioritize articles for ads, a person reads articles and tags them with interest categories suitable for advertisers. For example, a reader of an article about French Polynesia may be categorized as having an interest in &#x201c;travel to tropical islands&#x201d; After enough articles are labeled, a support vector machine learns to automatically label future articles related to &#x201c;travel to tropical islands&#x201d; with appropriate tags, so that these articles may be presented to a viewer having an interest that has been &#x2018;tagged&#x2019; as including these categories. Someone with knowledge of users' interests and their relation to advertising categories of interest to vendors tags the articles appropriately during training. The trained categorizer then can label new articles with interest categories appropriate for a particular advertising campaign. Each user would then have an advertising profile consisting of a set of interest categories, as evidenced by their reading habits, with a relevance score attached to each. This technique is an advance over the current state of the art where interest categories are assigned to keywords used in searching a page, no relevance is done, and only unsophisticated Boolean operations can be performed on the categories. The present system allows for sophisticated numerical shades of gray in describing interests of a user. These are part of the user's profile.</p>
<p id="p-0064" num="0063">Note that the above technique will also work for unlabeled news articles that need to be categorized in various new categories. Support vector machines will also work with word-based documents of any sort that require automatic categorization. For example, certain sales documents may be appropriate for wealthy customers while others may not be. Advertising may then be ranked for a particular user and/or documents may be categorized in particular categories as a function of the distance from the classification boundary in a support vector machine. After users' interests are prioritized with a numerical score of relevance, the categorized ads can be matched-up with the viewer's prioritized interests.</p>
<p id="p-0065" num="0064">In an alternative embodiment, the present system personalizes advertising in essentially reverse fashion (relative to the above-described method) by first selecting a context (e.g. time, location, or page tag such as &#x201c;Wednesday afternoon&#x201d;), and then classifying each document in the restricted advertising user profile in a hierarchical fashion, according to a set of categories shared with an ad server. The degree of relevance of the interest categories for each user is then communicated to the ad server so that advertisements are presented to the viewer in the context selected.</p>
<p id="p-0066" num="0065">Given a pool of advertisements eligible for display, the ad server prioritizes those which have been associated in the ad database with the interest categories returned by the present method. This method uses hierarchical interest categories (e.g., sports&#x3e;basketball&#x3e;college&#x3e;Duke) and gives a prioritized set of interests (e.g., Science 0.9, local 0.4, college basketball 0.5, etc.) as opposed to an unprioritized list without any structure. The current art uses Boolean operation on keywords or interests (e.g., golf AND women) without any score assigned to the relevance of these interests for a user.</p>
<p id="p-0067" num="0066">It may, in some cases, be advantageous to use more than just the headlines of news articles to perform the ranking because of the small number of words involved. In such cases, it is possible to include a summary of the article for use in generating the word vector. The full article is likely to be too long and may slow down the computation of the ranking engine. A summary allows a richer and more specific match to user interests in a word vector space. A summary may consist of the first paragraph of a news story or a more sophisticated natural language processing method may be employed to summarize the articles. Summaries generally lead to better precision in ranking articles according to user preferences than leads but may not be quite as precise as whole articles. However, the use of summaries is likely to provide better computational performance than the use of entire articles due to the fewer number of words involved.</p>
<p id="p-0068" num="0067">While exemplary embodiments of the present invention have been shown in the drawings and described above, it will be apparent to one skilled in the art that various embodiments of the present invention are possible. For example, the specific configuration of the overall system, the specific steps performed in ranking documents, as well as the particular methods employed for document ranking, the specific format of the word vectors and user query vectors, as described above, should not be construed as limited to the specific embodiments described herein. Modification may be made to these and other specific elements of the invention without departing from its spirit and scope as expressed in the following claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A computer-implemented method for ranking documents, the method comprising:
<claim-text>maintaining, in a profile associated with the user, documents selected by a user for viewing within a predetermined time period;</claim-text>
<claim-text>generating at least one positive word vector using words contained in at least a segment of at least one of the documents stored in the profile;</claim-text>
<claim-text>generating at least one negative word vector using words contained in at least a segment of at least one of the documents that was not selected by the user for viewing;</claim-text>
<claim-text>generating document word vectors for a group of documents to be ranked;</claim-text>
<claim-text>performing, using at least one processor, a vector space relationship analysis of the positive word vector, the negative word vector, and the document word vectors;</claim-text>
<claim-text>ranking, using the at least one processor, the group of documents based on the performed vector space relationship analysis;</claim-text>
<claim-text>classifying the documents selected by the user for viewing in predetermined categories;</claim-text>
<claim-text>ranking, based on the ranked group of documents, the predetermined categories; and</claim-text>
<claim-text>storing the ranked categories in the profile.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>storing the documents selected by the user for viewing in the profile; and</claim-text>
<claim-text>deleting, from the profile, the documents selected by the user for viewing prior to the predetermined time period.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein deleting the documents comprises:
<claim-text>determining whether a predetermined number of documents is stored in the profile; and</claim-text>
<claim-text>deleting from the profile, when the predetermined number of documents is stored in the profile, the documents selected by the user for viewing prior to the predetermined time period.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>storing the positive word vector in the profile.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>storing the document word vectors in the profile in a first-in first-out queue.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>sending the ranked categories to an ad server; and</claim-text>
<claim-text>receiving advertisements associated with the ranked categories from the ad server.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. An article of manufacture comprising a computer-readable storage device that stores computer-readable instructions which, when executed by at least one processor, cause the at least one processor to perform the following:
<claim-text>maintain, in a profile associated with the user, documents selected by the user for viewing within a predetermined time period;</claim-text>
<claim-text>generate at least one positive word vector using words contained in at least a segment of at least one of the documents stored in the profile;</claim-text>
<claim-text>generate at least one negative word vector using words contained in at least a segment of at least one of the documents that was not selected by the user for viewing;</claim-text>
<claim-text>generate document word vectors for a group of documents to be ranked;</claim-text>
<claim-text>perform a vector space relationship analysis of the positive word vector, the negative word vector, and the document word vectors;</claim-text>
<claim-text>rank the group of documents based on the performed vector space relationship analysis;</claim-text>
<claim-text>classify the documents selected by the user for viewing in predetermined categories;</claim-text>
<claim-text>rank, based on the ranked croup of documents, the predetermined categories; and</claim-text>
<claim-text>store the ranked categories in the profile.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The manufacture of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the instructions further include instructions that cause the at least one processor to:
<claim-text>store the documents selected by the user for viewing in the profile; and</claim-text>
<claim-text>delete, from the profile, the documents selected by the user for viewing prior to the predetermined time period.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The manufacture of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein, to delete the documents, the instructions include instructions that cause the at least one processor to:
<claim-text>determine whether a predetermined number of documents is stored in the profile; and</claim-text>
<claim-text>delete from the profile, when the predetermined number of documents is stored in the profile, the documents selected by the user for viewing prior to the predetermined time period.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The manufacture of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the instructions further comprise instructions that cause the at least one processor to:
<claim-text>store the positive word vector in the profile.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The manufacture of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the instructions further comprise instructions that cause the at least one processor to:
<claim-text>store the document word vectors in the profile in a first-in first-out queue.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The manufacture of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the instructions further comprise instructions that cause the at least one processor to:
<claim-text>send the ranked categories to an ad server; and</claim-text>
<claim-text>receive advertisements associated with the ranked categories from the ad server.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. A system for use in ranking documents, the system comprising:
<claim-text>one or more processing devices; and</claim-text>
<claim-text>one or more storage devices that store instructions which, when executed, cause the one or more processing devices to implement:
<claim-text>a content server configured to enable a user to select documents for viewing from a set of documents;</claim-text>
<claim-text>a profile associated with the user configured to maintain information specifying documents selected by the user for viewing within a predetermined time period;</claim-text>
<claim-text>at least one positive word vector generated using words contained in at least a segment of at least one of the documents stored in the profile;</claim-text>
<claim-text>at least one negative word vector generated using words contained in at least a segment of at least one of the documents that was not selected by the user for viewing;</claim-text>
<claim-text>document word vectors generated for a group of documents to be ranked;</claim-text>
<claim-text>a ranking engine configured to:
<claim-text>perform a vector space relationship analysis of the positive word vector, the negative word vector, and the document word vectors; and</claim-text>
<claim-text>rank the group of documents based on the performed vector space relationship analysis; and</claim-text>
</claim-text>
<claim-text>a classifier configured to classify the documents selected by the user for viewing in predetermined categories;
<claim-text>wherein the ranking engine is further configured to rank, based on the ranked group of documents, the predetermined categories; and</claim-text>
<claim-text>wherein the profile is further configured to store the ranked categories.</claim-text>
</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the instructions further cause the one or more processing devices to:
<claim-text>store information specifying the documents selected by the user for viewing in the profile; and</claim-text>
<claim-text>delete, from the profile, information specifying the documents selected by the user for viewing prior to the predetermined time period.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein instructions further cause the one or more processing devices to:
<claim-text>determine whether a predetermined number of documents has been selected by the user; and</claim-text>
<claim-text>delete, from the profile, when the predetermined number of documents has been selected by the user, information specifying the documents selected by the user for viewing prior to the predetermined time period.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the profile stores the positive word vector.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the profile stores the document word vectors in a first-in first-out queue.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, comprising an ad server configured to:
<claim-text>receive the ranked categories from the ranking engine; and</claim-text>
<claim-text>present advertisements associated with the ranked categories to the user.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
