<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626782-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626782</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12986051</doc-number>
<date>20110106</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>JP</country>
<doc-number>2010-007445</doc-number>
<date>20100115</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>51</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>7</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>17</main-group>
<subgroup>30</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>13</main-group>
<subgroup>14</subgroup>
<symbol-position>L</symbol-position>
<classification-value>N</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>707758</main-classification>
</classification-national>
<invention-title id="d2e71">Pattern identification apparatus and control method thereof</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5760346</doc-number>
<kind>A</kind>
<name>Kobayashi et al.</name>
<date>19980600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5761087</doc-number>
<kind>A</kind>
<name>Yoshimura et al.</name>
<date>19980600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5862049</doc-number>
<kind>A</kind>
<name>Sato et al.</name>
<date>19990100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6225986</doc-number>
<kind>B1</kind>
<name>Sato et al.</name>
<date>20010500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6239792</doc-number>
<kind>B1</kind>
<name>Yanagisawa et al.</name>
<date>20010500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6288711</doc-number>
<kind>B1</kind>
<name>Tanaka et al.</name>
<date>20010900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6463176</doc-number>
<kind>B1</kind>
<name>Matsugu et al.</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6611258</doc-number>
<kind>B1</kind>
<name>Tanaka et al.</name>
<date>20030800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>7054850</doc-number>
<kind>B2</kind>
<name>Matsugu</name>
<date>20060500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>7881524</doc-number>
<kind>B2</kind>
<name>Matsugu et al.</name>
<date>20110200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>8014572</doc-number>
<kind>B2</kind>
<name>Xiao et al.</name>
<date>20110900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382118</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>8028284</doc-number>
<kind>B2</kind>
<name>Ohkawa</name>
<date>20110900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>718100</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>8325999</doc-number>
<kind>B2</kind>
<name>Kapoor et al.</name>
<date>20121200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382118</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2009/0087036</doc-number>
<kind>A1</kind>
<name>Imaoka</name>
<date>20090400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382118</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2009/0089235</doc-number>
<kind>A1</kind>
<name>Torii et al.</name>
<date>20090400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2009/0324060</doc-number>
<kind>A1</kind>
<name>Sato et al.</name>
<date>20091200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>JP</country>
<doc-number>3078166</doc-number>
<kind>B</kind>
<date>20000800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>JP</country>
<doc-number>2002-008032</doc-number>
<kind>A</kind>
<date>20020100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00019">
<othercit>U.S. Appl. No. 12/970,712, filed Dec. 16, 2010. Applicant: Takashi Suzuki, et al.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00020">
<othercit>B. Moghaddam, et al.,&#x201c;Beyond Eigenfaces: Probabilistic Matching for Face Recognition&#x201d;, M. I. T. Media Laboratory Perceptual Computing Section Technical Report No. 443, Apr. 1998.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00021">
<othercit>B. Moghaddam, et al., &#x201c;Probabilistic Visual Learning for Object Representation&#x201d;, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 19, No. 7, Jul. 1997.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>6</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382145</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382190</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382103</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382115-128</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382218</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382154</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382170</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382224-227</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707758</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345589</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345619</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>358538</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>711130</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>718100</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>9</number-of-drawing-sheets>
<number-of-figures>11</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20110179052</doc-number>
<kind>A1</kind>
<date>20110721</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Sato</last-name>
<first-name>Hiroshi</first-name>
<address>
<city>Kawasaki</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Sato</last-name>
<first-name>Hiroshi</first-name>
<address>
<city>Kawasaki</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Fitzpatrick, Cella, Harper &#x26; Scinto</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Canon Kabushiki Kaisha</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Saeed</last-name>
<first-name>Usmaan</first-name>
<department>2169</department>
</primary-examiner>
<assistant-examiner>
<last-name>Vo</last-name>
<first-name>Cecile</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A pattern identification apparatus identifies a pattern that exists in input data using registration data including data of a reference pattern or a feature amount thereof and identification parameters defining processing details for comparing the input data with the registration data, the apparatus holding the registration data and the identification parameters in association with a label. The apparatus acquires registration data, identification parameters, and a label generated by an external apparatus, and registers the acquired registration data and identification parameters in association with the acquired label. If the registration data and identification parameters associated with the same label as acquired label are already registered, in the additional registration, either the stated acquired identification parameters or the identification parameters that is already registered in association with the same label are selected, and the stated acquired registration data and the selected identification parameters are additionally registered in association with the acquired label.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="175.18mm" wi="143.26mm" file="US08626782-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="232.33mm" wi="128.78mm" orientation="landscape" file="US08626782-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="217.34mm" wi="149.18mm" file="US08626782-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="143.09mm" wi="178.48mm" file="US08626782-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="168.23mm" wi="117.18mm" file="US08626782-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="224.11mm" wi="166.20mm" file="US08626782-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="136.91mm" wi="173.06mm" file="US08626782-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="223.44mm" wi="132.84mm" file="US08626782-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="252.05mm" wi="169.59mm" orientation="landscape" file="US08626782-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="173.74mm" wi="120.57mm" file="US08626782-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0002" num="0001">1. Field of the Invention</p>
<p id="p-0003" num="0002">The present invention relates to a pattern identification apparatus capable of additionally registering a pattern to be identified, and to a control method thereof.</p>
<p id="p-0004" num="0003">2. Description of the Related Art</p>
<p id="p-0005" num="0004">A facial identification technique for identifying an individual's face can be given as an example of an identification technique using pattern recognition, or a technique that typically identifies an object that is a subject within image data as the same object that is a subject in a different piece of image data. As used hereinafter in this specification, &#x201c;pattern identification&#x201d; will refer to judging differences in individual patterns (for example, differences in people that are separate individuals). On the other hand, &#x201c;pattern detection&#x201d; will refer to judging items that fall into the same category without distinguishing between individuals (for example, detecting faces without distinguishing between individuals).</p>
<p id="p-0006" num="0005">A method such as that described in Baback Moghaddam's <i>Beyond Eigenfaces: Probabilistic Matching for Face Recognition </i>(M.I.T. Media Laboratory Perceptual Computing Section Technical Report No. 443) (&#x201c;Document 1&#x201d; hereinafter) and Probabilistic Visual Learning for Object Representation (IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 19, No. 7, July 1997) (&#x201c;Document 2&#x201d; hereinafter), can be given as an example of a facial identification technique. This features an algorithm that enables the real-time registration and additional learning of faces by replacing individual identification problems caused by faces with a two-class identification problem having feature classes called &#x201c;subtracted faces&#x201d;.</p>
<p id="p-0007" num="0006">For example, with the generally-known facial identification that employs a support vector machine (SVM), in order to identify the faces of n people, n SVM discriminators that identify the faces of registered people and the faces of people that are not registered are necessary. SVM learning is necessary when registering the faces of people. In SVM learning, it is necessary to store large amounts of data including the faces of people to be registered, people who are already registered, and other people, which results in extremely long calculation times; thus methods that carry out calculations in advance have generally been used.</p>
<p id="p-0008" num="0007">However, according to the method in Documents 1 and 2, additional learning can be rendered essentially unnecessary by replacing the individual identification problem with the two-class identification problem discussed hereinafter. The two classes are a varying feature class related to lighting variations, expression/orientation, and so on among images of the same person, or an &#x201c;intra-personal class&#x201d;, and a varying feature class related to images of different people, or an &#x201c;extra-personal class&#x201d;. Assuming that the distributions of the stated two classes are constant regardless of the specific individual, a discriminator is configured by having reduced the individual facial identification problems to identification problems of the stated two classes. A large number of images are prepared in advance, and a discriminator that identifies the varying feature class for the same person and the varying feature class for different people carries out learning. For a new registrant, it is acceptable to store only an image of the face (or the result of extracting the necessary features from the image of the face). During identification, differential features are taken from two images, and the stated discriminator judges whether the people in the images are the same person or different people. Through this, a large amount of learning, which was necessary when registering the faces of individuals using an SVM and the like, is no longer necessary, and objects can therefore be registered in real time.</p>
<p id="p-0009" num="0008">However, even if the algorithm renders additional learning unnecessary, additional learning is nevertheless a useful method for improving identification capabilities. For example, even if the configuration is such that identification is carried out using the two classes of identification problems as described above, the identification capabilities can be improved through additional learning that adjusts the identification parameters so as to enable the proper identification of a newly-registered image. There is actually a higher demand now than there was before to introduce additional learning that improves the identification capabilities by adjusting the identification parameters so as to be specialized for data registered by a user. For example, there are cases, in the recognition of individuals using the faces of people, where prioritizing an improvement of the robustness with respect to the aforementioned variations leads to degradation in the identification capabilities for similar people. It is often the case in consumer devices such as digital cameras that particularly plural similar people, such as parents and children, siblings, and so on are registered. Additional learning is thought to be a useful method for improving the identification capabilities for similar people while also maintaining the robustness with respect to variations. Carrying out additional learning generates parameters specialized for the identification of people registered by a user, and particularly people that are similar, which can be assumed to improve the identification capabilities.</p>
<p id="p-0010" num="0009">However, if a user who owns multiple devices carries out additional learning in the individual devices in order to improve the identification capabilities, the specifics of the additional learning will differ from device to device, and thus the identification capabilities will also differ from device to device as a result. Furthermore, if such a user wishes to have the same identification capabilities in his or her multiple devices, in order to achieve the same identification results for an identification target that has been registered in the multiple devices, it is necessary to carry out additional learning for that identification target in each device; this places a heavy burden on the user and requires a significant amount of time for learning.</p>
<heading id="h-0002" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0011" num="0010">Having been achieved in light of the aforementioned problems, an aspect of the present invention provides a pattern identification apparatus and a control method thereof that enable the sharing of identification parameters acquired by additional learning with other devices and achieve a reduction in the effort and time required from the user for the additional learning.</p>
<p id="p-0012" num="0011">According to one aspect of the present invention, there is provided a pattern identification apparatus that identifies a pattern that exists in input data using registration data that includes data of a reference pattern or a feature amount thereof and identification parameters that define processing details for comparing the input data with the registration data, the apparatus comprising: a holding unit configured to hold the registration data and the identification parameters in association with a label; an acquisition unit configured to acquire registration data, identification parameters, and a label generated by an external apparatus; and an adding unit configured to add the registration data and identification parameters acquired by the acquisition unit to the holding unit in association with the label acquired by the acquisition unit, wherein in the case where registration data and identification parameters associated with the same label as acquired by the acquisition unit are already held in the holding unit, the adding unit selects either the identification parameters acquired by the acquisition unit or the identification parameters held in the holding unit in association with the same label, and adds the registration data acquired by the acquisition unit and the selected identification parameters to the holding unit in association with the label acquired by the acquisition unit.</p>
<p id="p-0013" num="0012">Also according to another aspect of the present invention, there is provided a pattern identification apparatus that identifies a pattern that exists in input data using registration data that includes data of a reference pattern or a feature amount thereof and identification parameters that define processing details for comparing the input data with the registration data, the apparatus comprising: a holding unit configured to hold the registration data and the identification parameters in association with a label; an acquisition unit configured to acquire registration data, identification parameters, and a label generated by an external apparatus; and an adding unit configured to add the registration data and identification parameters acquired by the acquisition unit to the holding unit in association with the label acquired by the acquisition unit, wherein the adding unit: holds compatibility information indicating an identification parameter that can be shared with a different device type; and extracts an identification parameter that can be shared from among the identification parameters acquired by the acquisition unit by referring to the device type of the external apparatus and the compatibility information, adds the extracted identification parameter to the holding unit in association with the label, and supplements missing identification parameters by using a default value or additional learning.</p>
<p id="p-0014" num="0013">Also, according to another aspect of the present invention, there is provided a control method for a pattern identification apparatus that identifies a pattern that exists in input data using registration data that includes data of a reference pattern or a feature amount thereof and identification parameters that define processing details for comparing the input data with the registration data, the method comprising: holding the registration data and the identification parameters in association with a label in a holding unit; acquiring registration data, identification parameters, and a label generated by an external apparatus; and adding the registration data and identification parameters acquired in the step of acquiring to the holding unit in association with the label acquired in the step of acquiring, wherein in the case where registration data and identification parameters associated with the same label as acquired in the step of acquiring are already held in the holding unit, the step of adding selects either the identification parameters acquired in the step of acquiring or the identification parameters held in the holding unit in association with the same label, and adds the registration data acquired in the step of acquiring and the selected identification parameters to the holding unit in association with the label acquired in the step of acquiring.</p>
<p id="p-0015" num="0014">Furthermore, according to another aspect of the present invention, there is provided a control method for a pattern identification apparatus that identifies a pattern that exists in input data using registration data that includes data of a reference pattern or a feature amount thereof and identification parameters that define processing details for comparing the input data with the registration data, the method comprising: holding the registration data and the identification parameters in association with a label in a holding unit; acquiring registration data, identification parameters, and a label generated by an external apparatus; and adding the registration data and identification parameters acquired in the step of acquiring to the holding unit in association with the label acquired in the step of acquiring, wherein in the step of adding, an identification parameter that can be shared is extracted from among the identification parameters acquired in the step of acquiring by referring to compatibility information indicating an identification parameter that can be shared with another device type and the device type of the external apparatus, the extracted identification parameter is added to the holding unit in association with the label, and missing identification parameters are supplemented by using a default value or additional learning.</p>
<p id="p-0016" num="0015">Further features of the present invention will become apparent from the following description of exemplary embodiments with reference to the attached drawings.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram illustrating an exemplary configuration of a digital camera in which a pattern identification apparatus has been applied.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 2</figref> is a process flowchart of a pattern identification apparatus according to an embodiment.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 3</figref> is a block diagram illustrating an exemplary configuration of an input data identification unit.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 4</figref> is a flowchart illustrating an exemplary process performed by an input data identification unit.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 5A</figref> is a block diagram illustrating an exemplary configuration of a data registration unit according to a first embodiment.</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 5B</figref> is a block diagram illustrating an exemplary configuration of a data registration unit according to a second embodiment.</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 6</figref> is a block diagram illustrating an exemplary configuration of a data sharing processing unit.</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 7</figref> is a flowchart illustrating an example of a registration data/identification parameter sharing process.</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 8A</figref> is a block diagram illustrating an exemplary configuration of a compatibility confirmation unit.</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 8B</figref> is a diagram illustrating an example of a compatibility information table held by a compatibility information holding unit.</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 9</figref> is a flowchart illustrating an example of an identification parameter sharing process according to the second embodiment.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DESCRIPTION OF THE EMBODIMENTS</heading>
<p id="p-0028" num="0027">Hereinafter, a first embodiment of the present invention will be described in detail with reference to the appended drawings.</p>
<heading id="h-0005" level="1">First Embodiment</heading>
<heading id="h-0006" level="1">Configuration of Pattern Identification Apparatus</heading>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram illustrating an exemplary configuration of a digital camera <b>100</b> in which a pattern identification apparatus according to the first embodiment has been applied. Although the present embodiment describes an example in which the pattern identification apparatus has been applied in a digital camera, it should be noted that the present invention is not limited thereto, and it should be clear that the pattern identification apparatus can also be applied in an information processing apparatus such as a personal computer or the like.</p>
<p id="p-0030" num="0029">In the digital camera <b>100</b>, an imaging optical system <b>1</b> is configured of an optical lens that includes a zoom mechanism. This imaging optical system may also include a driving mechanism for the pan/tilt axial directions. Typically, a CCD or CMOS image sensor is used as an image sensor in an image sensing unit <b>2</b>. The image sensing unit <b>2</b> outputs, as image data, a predetermined image signal (for example, a signal obtained through subsampling and block readout) in accordance with a readout control signal from a sensor driving circuit (not shown). An image sensing control unit <b>3</b> controls the timing at which image sensing actually occurs based on an instruction from a user (an angle-of-view adjustment instruction, the depression of a shutter button, and so on) and information from a data registration unit <b>5</b> or an input data identification unit <b>6</b>. An image recording unit <b>4</b> is configured of a semiconductor memory or the like, and holds image data that has been transmitted from the image sensing unit <b>2</b>. In addition, the image recording unit <b>4</b> transmits image data at a predetermined timing in accordance with requests from the data registration unit <b>5</b> and the input data identification unit <b>6</b>.</p>
<p id="p-0031" num="0030">The data registration unit <b>5</b> extracts information of an object to be identified, typically from image data, and records/holds that information. Specific details of the configuration of and actual processes performed by the data registration unit <b>5</b> will be described later. The input data identification unit <b>6</b> identifies a target object, and more typically, an object within image data, based on input data and data acquired from the data registration unit <b>5</b>. The specific configuration of and details of processes performed by the input data identification unit <b>6</b> will also be described later. Note that the data registration unit <b>5</b> and the input data identification unit <b>6</b> may each be dedicated circuits (ASICs), processors (reconfigurable processors, DSPs, or the like), and so on. These units may also exist as a single dedicated circuit or as programs executed in a generic circuit (a CPU of a PC).</p>
<p id="p-0032" num="0031">An external input/output unit <b>7</b> is used in the exchange of data with an external apparatus. Typically, this exchange can be thought of as exchange of data with an external apparatus using an external storage apparatus such as a Compact Flash&#xae; device. Alternatively, input/output can be carried out via a network device, or a direct connection with the external apparatus (for example, a wired connection such as USB, a wireless connection such as Bluetooth or infrared communication, and so on) may be used. The external input/output unit <b>7</b> inputs/outputs results output by the data registration unit <b>5</b> and the input data identification unit <b>6</b> to and from the external apparatus as electronic data. Data input/output with the data registration unit <b>5</b> will be described in detail later. Meanwhile, a destination for the output of identification results may also include the use of a monitor such as a CRT, a TFT liquid crystal display, or the like in the external input/output unit <b>7</b>. The image data acquired from the image sensing unit <b>2</b> and the image recording unit <b>4</b> may be displayed, or the results output by the data registration unit <b>5</b> and the input data identification unit <b>6</b> may be displayed superimposed upon the image data.</p>
<p id="p-0033" num="0032">Meanwhile, a user can carry out various types of operations including setting a mode for carrying out data registration operations, operations regarding registration data, and so on via an operation unit <b>8</b>. A connection bus <b>9</b> is a bus for performing control and making data connections between the aforementioned constituent elements.</p>
<p id="p-0034" num="0033">&#x3c;Overall Flow&#x3e;</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 2</figref> illustrates an example of the overall processing performed by the pattern identification apparatus according to the present embodiment. Hereinafter, a process carried out when the pattern identification apparatus identifies an input pattern will be described with reference to <figref idref="DRAWINGS">FIG. 2</figref>. As will be described in the following, when identifying a pattern that exists in input data, the pattern identification apparatus employs identification parameters that define registration data expressing a reference pattern and the processing details for comparing the input data with the registration data. The registration data includes any of, for example, the data of the reference pattern as a whole, data of a local region, a feature amount, and the like. Although the following describes the case where the pattern to be identified (an identification target pattern) is an object within an image, and more specifically, the face of a person, it goes without saying that the present invention is not limited thereto. In other words, various other types of data such as text image data, audio data, and so on can also be applied in the present invention.</p>
<p id="p-0036" num="0035">First, the input data identification unit <b>6</b> acquires image data from the image recording unit <b>4</b> (S<b>00</b>). Then, the input data identification unit <b>6</b> detects a target object, or to be more specific, carries out a detection process for the face of a person, on the acquired image data (S<b>01</b>). A known technique may be used as the method for detecting the face of the person from the image. For example, a technique such as that proposed in Japanese Patent No. 3078166, Japanese Patent Laid-Open No. 2002-8032, or the like may be employed. Even in the case where an object aside from the face of a person is to be detected, the detection can be carried out using one of the aforementioned techniques by employing a desired object as training data.</p>
<p id="p-0037" num="0036">If a target object does not exist within the input image (No in S<b>02</b>), the process ends. However, in the case where a target pattern does exist within the input image (Yes in S<b>02</b>), the input data identification unit <b>6</b> determines whether the processing mode is a registration mode or an identification mode (S<b>03</b>). The registration mode or the identification mode can be set in advance by the user using the operation unit <b>8</b>. In the case where it has been determined that the mode is the registration mode (Yes in S<b>03</b>), the data registration unit <b>5</b> carries out a data registration process (S<b>04</b>). The specific content of the data registration process will be described in detail later. In the case where the mode is the identification mode (No in S<b>03</b>), the input data identification unit <b>6</b> carries out an identification process on the input data (S<b>05</b>). The input data identification process will also be described later. Finally, the result of the data registration process or the input data identification process is output to the external input/output unit <b>7</b> (S<b>06</b>). The foregoing is the overall process performed by the pattern identification apparatus according to the present embodiment.</p>
<p id="p-0038" num="0037">&#x3c;Input Data Identification Unit <b>6</b>&#x3e;</p>
<p id="p-0039" num="0038">The input data identification process (S<b>05</b>) performed by the input data identification unit <b>6</b> will be described hereinafter. <figref idref="DRAWINGS">FIG. 3</figref> is a block diagram illustrating, in detail, an exemplary configuration of the input data identification unit <b>6</b>. In the input data identification unit <b>6</b>, an identification data generation unit <b>21</b> extracts information necessary to identify a target object from the image data acquired from the image recording unit <b>4</b>, using identification parameters held in an identification parameter holding unit <b>24</b>. A registration data acquisition unit <b>22</b> acquires registration data necessary for the identification of input data from the data registration unit <b>5</b>. An input data identification calculation unit <b>23</b> performs an identification process on the input data using the identification data acquired by the identification data generation unit <b>21</b> and the registration data acquired by the registration data acquisition unit <b>22</b>. The identification parameter holding unit <b>24</b> holds various types of parameters used during the identification process. In other words, the identification parameter holding unit <b>24</b> holds identification parameters that define the details of processing for comparing the input data (image data) with the registration data for the purpose of identification. Note that the result of the processing performed by the data registration unit <b>5</b> (in other words, the registration data) may be stored in the identification parameter holding unit <b>24</b>. The identification parameters are stored as unique parameters for the registration data for each instance of registration data, or in other words, along with label information of the registration data. In this manner, the registration data and identification parameters are held in association with labels.</p>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. 4</figref> is a flowchart illustrating an example of the identification process carried out by the input data identification unit <b>6</b>. First, the registration data acquisition unit <b>22</b> acquires the registration data from the data registration unit <b>5</b> (S<b>10</b>). The registration data is typically a normalized image that has been enlarged or reduced to a predetermined size through slope correction or the like after first cutting out the target object from the input image. Alternatively, a similar process as an identification data generation process (described later) may be carried out in advance, and a feature amount that was extracted may be employed as the registration data. This process will be described again during the descriptions of the data registration unit <b>5</b>. The identification data generation unit <b>21</b> acquires, from the image recording unit <b>4</b>, the input data that is to be processed (in this example, the input image) (S<b>11</b>). Next, the identification data generation unit <b>21</b> acquires, from the identification parameter holding unit <b>24</b>, the parameters to be used in the identification (called the &#x201c;identification parameters&#x201d; hereinafter) (S<b>12</b>). Here, the &#x201c;identification parameters&#x201d; typically includes a method for extracting a feature amount (described later), the number of dimensions of dimensional compression, and so on.</p>
<p id="p-0041" num="0040">Next, the identification data generation unit <b>21</b> carries out an identification data generation process using the input data to be processed and the identification parameters (S<b>13</b>). Note that in the case where the registration data is an image, the identification data generation process is carried out not only on the input image but also on the registration data. In the identification data generation process, the identification data generation unit <b>21</b> cuts out the target object (typically the face of a person) from the input data, and extracts a feature amount in accordance with the identification parameters after first normalizing the target object to a predetermined size and orientation. Here, for example, the feature amount may be calculated using the luminosity of the image as-is, or the luminosity may be converted to a local binary pattern, increment signs or the like, and the result thereof used as a feature amount that is more robust with respect to variations. Alternatively, the result of reducing the dimensions of a feature vector by carrying out principal component analysis (PCA), independent component analysis (ICA), or the like on the feature amount may be employed as a new feature amount. Further still, the feature amount may first be set to a partial region and then extracted. Hereinafter, this partial region will be referred to as a &#x201c;local region&#x201d;. For example, in the case where the target object is the face of a person, locations that are likely to appear in the features of an individual, such as the eyes, the vicinity of the mouth, and so on in the face may be set as a local region, and may then be extracted as a feature amount after performing a conversion such as that mentioned above. How many local regions to use, where to set the local regions, and so on may all be held in the identification parameter holding unit <b>24</b> in advance as identification parameters.</p>
<p id="p-0042" num="0041">Next, the input data identification calculation unit <b>23</b> carries out a matching process on the registration data acquired in S<b>10</b> and the identification data (the feature amount obtained in S<b>13</b>) obtained from the input data acquired S<b>11</b> (S<b>14</b>). The matching process typically involves a correlation process on the feature vectors extracted from the registration data and the input data, respectively. In the case where there are multiple pieces of registration data, the matching process employs all of those pieces of registration data in a round-robin format. The correlation process may be a simple dot product, or a correlation function may be found.</p>
<p id="p-0043" num="0042">For example, in the case where a local region is set and feature extraction is carried out as described earlier, the feature amount matching is carried out on the local regions that correspond to the input data and the registration data, respectively. The same number of matching results as there are local regions is obtained, but the average of those multiple values may be used as the final output. Furthermore, instead of a simple average, a weighted average found in advance through learning or the like may be found. In this manner, it is known that carrying out matching at a local level is robust with respect to variations such as illumination variations, part of the face being hidden, and so on.</p>
<p id="p-0044" num="0043">Meanwhile, as the output of the matching process, a situation in which a binary (0 or 1) expressing whether or not there is a match with the registration data is output and a situation in which a normalized output value (a real number value from 0 to 1) is output as a likelihood can be considered. Furthermore, in the case where there are multiple pieces of registration data (multiple registrants), a likelihood may be output for each piece of registration data (registrant). Alternatively, only the result for the registration data that matches the closest may be output. In addition, rather than a likelihood for each individual piece of registration data, a likelihood for a class to which the registration data belongs may be output as well. In other words, in the case of a person, the likelihood of an ID (name) of that person may be output instead of a result for individual registered face images.</p>
<p id="p-0045" num="0044">The foregoing has described an example of a processing flow performed by the input data identification unit <b>6</b>.</p>
<p id="p-0046" num="0045">&#x3c;Data Registration Unit <b>5</b>&#x3e;</p>
<p id="p-0047" num="0046">Next, the data registration process performed by the data registration unit <b>5</b> will be described. <figref idref="DRAWINGS">FIG. 5A</figref> is a block diagram illustrating, in detail, an exemplary configuration of the data registration unit <b>5</b> according to the first embodiment. In the data registration unit <b>5</b>, a registration data generation unit <b>31</b> generates registration data necessary for identifying an individual target object from the image data acquired from the image recording unit <b>4</b>. For example, in the case where a two-class problem, such as the intra-class and extra-class discussed in Document 1, is judged, an image of the face of a person is typically used as the registration data. Meanwhile, the image data of the face of the person detected through a facial detection process is stored in a registration data holding unit <b>32</b> after the size, orientation (planar rotation direction), and so on have been normalized. At this time, the label information set for the registration image by the user through the operation unit <b>8</b> or the like is also recorded in the registration data holding unit <b>32</b>. Note that it is also possible to reduce the amount of data by holding, as the registration data, only the data necessary during identification in the registration data holding unit <b>32</b>, rather than holding the image data itself. For example, in the case where an identification calculation is carried out using vector correlation of the local region of that object, it is possible for the data registration unit <b>5</b> to cut out only that local region in advance and hold the cut-out local region as the registration data.</p>
<p id="p-0048" num="0047">As described thus far, the registration data generation unit <b>31</b> extracts the necessary information as appropriate from the image, and stores data obtained by carrying out a predetermined conversion as described earlier in the registration data holding unit <b>32</b> as a feature vector for carrying out the identification of the object.</p>
<p id="p-0049" num="0048">A learning unit <b>34</b> receives the registration data from the registration data generation unit <b>31</b> and carries out additional learning so as to improve the identification capabilities of the registration data. For example, when employing a method that carries out identification having set a local region as mentioned earlier in the descriptions of the input data identification unit <b>6</b>, the number of local regions, the dimensional compression method and number of dimensions of the feature vector in the local region, and so on are learned for each instance of registration data. A known technique can be employed as the method of learning performed by the learning unit <b>34</b>. For example, in the case where a method that carries out matching for each local region as described earlier is employed, a boosting technique as exemplified by AdaBoost can be applied having taken a combination of the local region and the parameters thereof as a single weak discriminator. At the most basic level, the registration data may be used as the learning data. In other words, the additional learning is carried out using the registration data that has already been registered and data that will be registered thereafter. Alternatively, data that is only for learning may be held in advance for the purposes of the additional learning. In this manner, the additionally-learned parameters are stored in the identification parameter holding unit <b>24</b>.</p>
<p id="p-0050" num="0049">A registration data selection unit <b>33</b> reads out necessary registration data from the registration data holding unit <b>32</b> in accordance with a request from the input data identification unit <b>6</b> described earlier, and transmits the registration data to the input data identification unit <b>6</b>. A data sharing processing unit <b>35</b> carries out a process for sharing the registration data and the registration data identification parameters with other devices via the external input/output unit <b>7</b> or the like. Hereinafter, the content of processing performed by the data sharing processing unit <b>35</b> will be described in detail.</p>
<p id="p-0051" num="0050">&#x3c;Data Sharing Processing Unit <b>35</b>&#x3e;</p>
<p id="p-0052" num="0051">Processing performed by the data sharing processing unit <b>35</b> will be described. <figref idref="DRAWINGS">FIG. 6</figref> is a block diagram illustrating an exemplary configuration of the data sharing processing unit <b>35</b>. In the data sharing processing unit <b>35</b>, a registration data sharing unit <b>41</b> carries out a process for sharing the registration data. For example, the registration data sharing unit carries out a process for sharing the image data for pattern identification with other devices. The registration data sharing unit <b>41</b> compares a pair that includes the label information and the image data (registration data) with registration data already acquired via the external input/output unit <b>7</b> that is currently held. As a result of this comparison, registration data that has label information that is not yet held is copied to and stored in the registration data holding unit <b>32</b>. On the other hand, in the case where the same label is already registered, it is determined whether or not to store that data based on predetermined criteria. The number of pieces of registration data for the same label, whether or not all the pieces of registration data will fit in the storage space, and so on can be given as examples of the predetermined criteria. Note that it may be judged whether or not the registration data is the same. This is because situations in which different labels are assigned to the same registration data are also possible. In such a case, the user may be prompted to specify which label is the matching label through the operation unit <b>8</b>.</p>
<p id="p-0053" num="0052">An identification parameter sharing unit <b>42</b> carries out a process for sharing the identification parameters. Meanwhile, an identification parameter resetting unit <b>43</b> resets the identification parameters so that the pre- and post-sharing identification parameters resulting from the process performed by the identification parameter sharing unit <b>42</b> are consistent. The specific content of the processes performed by the identification parameter sharing unit <b>42</b> and the identification parameter resetting unit <b>43</b> will be described in detail hereinafter.</p>
<p id="p-0054" num="0053">Note that, as described above, it is possible for multiple pieces of registration data to be registered in association with a single label in the present embodiment. As opposed to this, with respect to the identification parameters, a single type of identification parameter is registered for a single label, as will be described later. In this manner, an improvement in the identification capabilities and conservation of memory space is realized by performing identification on images having the same label using the same identification parameters. However, in the case where, for example, a single label has been assigned to a specific person, there is the possibility that carrying out identification with a single identification parameter will conversely reduce the identification capabilities if both an image having a front view and an image having a side view exist under the same label. Accordingly, attributes such as the shooting direction (front view, side view), the shooting environment (sunlight, fluorescent light), and so on may be added to the labels, and the same identification parameters may then be used in the case where the label and attributes match. If such is the case, the phrase &#x201c;the labels match&#x201d; in the following descriptions of the sharing process can be taken to mean &#x201c;the labels and attributes thereof match&#x201d;. In addition, the aforementioned problem can be avoided by employing a label in which the person's name and the shooting direction is written.</p>
<p id="p-0055" num="0054">&#x3c;Identification Parameter Sharing Process/Resetting Process&#x3e;</p>
<p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. 7</figref> is a flowchart illustrating an example of processes performed by the registration data sharing unit <b>41</b>, the identification parameter sharing unit <b>42</b>, and the identification parameter resetting unit <b>43</b>. First, the registration data sharing unit <b>41</b> acquires, from the external input/output unit <b>7</b>, external data that includes the registration data to be shared and a label (S<b>21</b>). Next, the identification parameter sharing unit <b>42</b> confirms whether identification parameters used for the identification of the registration data exist in the external data (S<b>22</b>). Here, the identification parameters are unique parameters for the registration data that is the target, such as that has been additionally learned by the learning unit <b>34</b>. The determination as to whether or not the identification parameters exist may be carried out by preparing data, in advance, for determining whether the identification parameters exist in the external data, with the identification parameter sharing unit <b>42</b> reading out that data and carrying out determination. In the case where the identification parameters do not exist (No in S<b>22</b>), the registration data sharing unit <b>41</b> advances to a registration data sharing process (S<b>27</b>).</p>
<p id="p-0057" num="0056">However, in the case where the identification parameters exist (Yes in S<b>22</b>), the identification parameter sharing unit <b>42</b> acquires the identification parameters from the external data (S<b>23</b>). Then, the identification parameter resetting unit <b>43</b> determines whether or not it is necessary to reset the acquired identification parameters (S<b>24</b>). Here, the determination as to whether or not the resetting is necessary may be carried out, for example, as follows. First, it is confirmed whether identification parameters having the same label as the identification parameters contained in the external data exist in the identification parameter holding unit <b>24</b>. In the case where such identification parameters do exist, whether or not the sets of identification parameters are the same for that same label. In the case where both sets of identification parameters are exactly the same parameters, no action needs to be carried out (that is, resetting is unnecessary), but in the case where the sets of identification parameters are different, resetting is necessary. In the case where resetting is unnecessary (No in S<b>24</b>), the procedure advances to an identification parameter sharing process (S<b>26</b>). However, in the case where resetting is necessary (Yes in S<b>24</b>), the identification parameter resetting unit <b>43</b> carries out an identification parameter resetting process (S<b>25</b>). This identification parameter resetting process is realized by aligning both sets of identification parameters to one of the sets of identification parameters, for example, as described below. That is, of the acquired identification parameters and the identification parameters that are already held, the identification parameters are aligned to the identification parameters that have a higher identification rate, using the registration data that belongs the same label as evaluation data. Alternatively, and more simply, the time at which the identification parameters were learned (that is, the time at which the identification parameters were generated) may be recorded in advance, and the identification parameters may then be aligned to the newer of the identification parameters. Further still, the user may be allowed to select which identification parameters to align the identification parameters to via the operation unit <b>8</b> or the like.</p>
<p id="p-0058" num="0057">Next, the identification parameter sharing unit <b>42</b> shares the identification parameters (S<b>26</b>). In the case where resetting was carried out in S<b>25</b>, the parameters may be updated to those parameters, whereas in the case where labels that correspond to the acquired identification parameters are not held in the registration data holding unit <b>32</b>, the parameters may simply be copied and stored in the identification parameter holding unit <b>24</b>. In S<b>27</b>, the registration data sharing unit <b>41</b> registers the registration data contained in the external data as described above. Note that the process of S<b>27</b> may be executed immediately after S<b>21</b>.</p>
<p id="p-0059" num="0058">As described thus far, according to the pattern identification apparatus of the first embodiment, by sharing both the registration data and the identification parameters, it is possible to share the identification capabilities among devices as well. For this reason, it is possible to reduce a problem in which the identification capabilities vary significantly depending on the device that is used. It is also possible to realize a reduction in the amount of time for additional learning.</p>
<heading id="h-0007" level="1">Second Embodiment</heading>
<p id="p-0060" num="0059">Next, a second embodiment will be described. In the second embodiment, a configuration that realizes a function for implementing sharing among devices that perform different identification processes in the context of the registration data/identification parameters sharing function according to the first embodiment will be described. Note that descriptions of configurations that are the same as those in the first embodiment will be omitted in order to avoid prolixity. Furthermore, for the sake of simplifying the descriptions, the pattern that is to be identified will be described as the face of a person in an image, but the applicability of the present invention to other patterns aside from the faces of people is as described in the first embodiment.</p>
<p id="p-0061" num="0060">&#x3c;Data Registration Unit&#x3e;</p>
<p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. 5B</figref> is a block diagram illustrating an exemplary configuration of the data registration unit <b>5</b> according to the second embodiment. As compared to the first embodiment (<figref idref="DRAWINGS">FIG. 5A</figref>), a compatibility confirmation unit <b>36</b> has been added and part of the content of the processing performed by the data sharing processing unit <b>35</b> is different.</p>
<p id="p-0063" num="0062">The compatibility confirmation unit <b>36</b> enables the sharing of parameters to the greatest extent possible among devices that share registration data and identification parameters, even in the case where the identification method used by the input data identification unit <b>6</b> is different among the devices. Hereinafter, the specific processing content of the data sharing processing unit <b>35</b> and the compatibility confirmation unit <b>36</b> will be described in detail.</p>
<p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. 8A</figref> is a block diagram illustrating an exemplary configuration of the compatibility confirmation unit <b>36</b>. In the compatibility confirmation unit <b>36</b>, a compatibility determination unit <b>141</b> determines whether or not the identification parameters contained in the external data acquired via the external input/output unit <b>7</b> are parameters that can be used in that data identification apparatus (that is, whether the parameters are compatible). A compatibility information holding unit <b>142</b> stores information regarding identification parameters that are compatible among multiple identification methods (that is, among multiple types of devices). <figref idref="DRAWINGS">FIG. 8B</figref> illustrates an example of compatibility information stored in the compatibility information holding unit <b>142</b>. As shown in <figref idref="DRAWINGS">FIG. 8B</figref>, it is favorable for the compatibility information to be in table format. This compatibility information table <b>143</b> indicates identification parameters that are compatible, or in other words, can be used among different types of devices. The content of the compatibility information table <b>143</b> may be held in the device in advance, but may also be updated from the exterior via, for example, the external input/output unit <b>7</b>.</p>
<p id="p-0065" num="0064">The content of the compatibility information table <b>143</b> will now be described. Each row in the table expresses a type of device serving as the source of the identification parameters, whereas each column expresses a type of device serving as the destination. The cells in the table on the diagonal from the upper-left to the lower-right have registered data between the same type of device, and thus all of the parameters are compatible (complete compatibility). The cells that are not on that diagonal have registered data among devices of different types, and thus include partial incompatibilities. Furthermore, compatible parameters are written in the compatibility information table <b>143</b>. Accordingly, in, for example, the case where the identification parameters of the device whose type is &#x201c;camera A&#x201d; are to be registered in the device whose type is &#x201c;camera B&#x201d;, the dimensional compression method and the local region are compatible, and thus it can be understood that the base vectors and number, position, and so on of the local regions can be shared. As opposed to this, in the case where the identification parameters of the camera B are to be shared with the camera A, only the local region identification parameters can be shared. This is because, for example, the camera B is a device type that is superior to the camera A.</p>
<p id="p-0066" num="0065">Note that the information of the parameter compatibility is not intended to be limited to that shown in <figref idref="DRAWINGS">FIG. 8B</figref>. For example, in the case where the pattern identification method is a method that unifies correlation values on a local region-by-local region basis, the information may include the number of local regions, the positions of the local regions, the dimensional compression methods for each local region, the number of feature vector dimensions for the local regions, and so on. Furthermore, learning algorithms, correlation computation methods, calculation resources, and so on may also be included as compatibility information. For example, including a learning algorithm in the compatibility information makes it possible to share the initial values of the learning among devices that have the same learning method. Meanwhile, by including calculation resources in the compatibility information, it is possible to carry out control so that additional learning that takes time is executed at an appropriate timing (a timing at which the necessary resources have been secured) and with an appropriate device (a device that can secure the necessary resources). Meanwhile, information of device types that provide external data (registration data, identification parameters, labels) may be held, for example, in the external data. Alternatively, in the case where the external input/output unit <b>7</b> acquires external data from an external storage device, the information of the device type may be stored in the external storage device under a predetermined data name. Furthermore, in the case where the external input/output unit <b>7</b> acquires the external data through communication with another device, the information of the device type may be acquired when, for example, the communication is established. Further still, it is also possible to store only the information of the source device in the compatibility information table.</p>
<p id="p-0067" num="0066">In S<b>26</b> of <figref idref="DRAWINGS">FIG. 7</figref>, the identification parameter sharing unit <b>42</b> registers only the identification parameters that are compatible, and supplements the missing identification parameters with default values. Alternatively, the missing identification parameters may be supplemented through additional learning. The identification parameter sharing process of S<b>26</b> will be described hereinafter with reference to <figref idref="DRAWINGS">FIG. 9</figref>.</p>
<p id="p-0068" num="0067">First, the identification parameter resetting unit <b>43</b> acquires the compatibility information of the identification parameters from the compatibility information holding unit <b>142</b> (S<b>31</b>). Next, the identification parameter resetting unit <b>43</b> acquires identification parameters that can be shared from the external data based on the acquired compatibility information (S<b>32</b>). Then, the identification parameter resetting unit <b>43</b> measures the identification capabilities based on the identification parameters acquired in S<b>32</b> (S<b>33</b>). At this time, default values are set for the identification parameters that could not be acquired from the external data due to the incompatibility relationship (that is, the missing parameters). Meanwhile, for the identification capabilities, the identification rate may be measured using the registration data as evaluation data. Next, the identification parameter resetting unit <b>43</b> determines whether the measured identification capabilities exceed a predetermined value (S<b>34</b>). In the case where the measured identification capabilities exceed the predetermined value (Yes in S<b>34</b>), the process ends. However, in the case where the measured identification capabilities do not exceed the predetermined value (No in S<b>34</b>), the identification parameter resetting unit <b>43</b> causes the learning unit <b>34</b> to execute additional learning (S<b>35</b>). The additional learning may be continued until the predetermined value is exceeded while, for example, changing the initial value, but because there are also cases where the additional learning will not come to an end, a limitation may be set so that the number of instances of learning does not exceed a predetermined number. Note that in the case where there is absolutely no compatibility in the identification parameters acquired from the exterior, in essence, only the registration data is added, the identification parameters take on the default values, and the additional learning is carried out.</p>
<p id="p-0069" num="0068">As described thus far, according to the second embodiment, it is possible to share, to the greatest extent possible and even among devices whose identification methods differ, parameters for which additional learning has been carried out in order to identify the registration data.</p>
<p id="p-0070" num="0069">Although whether or not to execute the additional learning was determined in accordance with an evaluation of the identification rate in the above example, it should be noted that the present invention is not limited thereto. Additional learning of the identification parameters may be carried out each time new registration data is held. Alternatively, necessity information indicating whether or not the execution of additional learning is necessary may be included in the compatibility information of the identification parameters, and the determination as to whether or not to carry out the additional learning may be performed in accordance therewith. For example, in <figref idref="DRAWINGS">FIG. 8B</figref>, when identification parameters that are registered to a device whose device type is &#x201c;camera A&#x201d; are registered in a device whose device type is &#x201c;server&#x201d;, the learning unit <b>34</b> is caused to execute the additional learning if a flag indicating additional learning is set. At this time, if there is compatibility in the learning algorithms, the parameters to be shared can be set to the initial values of the additional learning. Typically, the location of local features, the number of local features, and so on can be employed as such initial values of additional learning.</p>
<p id="p-0071" num="0070">As described thus far, according to the second embodiment, it is possible to suppress the indiscriminate execution of time-consuming additional learning by carrying out the additional learning only in the case where predetermined conditions are met after the registration data identification parameters have been shared. Therefore, according to the second embodiment, registration data and parameters for identifying registration data through an identification method that is unique to a device can be shared among multiple devices, thus making it possible to unify the identification capabilities and realize a reduction in the additional learning time.</p>
<p id="p-0072" num="0071">The present invention can be applied in image sensing apparatuses such as digital cameras, camcorders, network cameras, and so on. The present invention can also be applied in PCs, mobile devices, and other such integrated applications.</p>
<p id="p-0073" num="0072">Embodiments of the present invention have been described in detail above, but the present invention can take the form of a system, apparatus, method, program, storage medium, and so on. Specifically, the present invention may be applied to a system configured of multiple devices or to an apparatus configured of a single device.</p>
<p id="p-0074" num="0073">According to the aforementioned embodiments, it is possible to share additional learning results with other devices and thus achieve a reduction in the effort and time required for the additional learning.</p>
<p id="p-0075" num="0074">Aspects of the present invention can also be realized by a computer of a system or apparatus (or devices such as a CPU or MPU) that reads out and executes a program recorded on a memory device to perform the functions of the above-described embodiments, and by a method, the steps of which are performed by a computer of a system or apparatus by, for example, reading out and executing a program recorded on a memory device to perform the functions of the above-described embodiments. For this purpose, the program is provided to the computer for example via a network or from a recording medium of various types serving as the memory device (e.g., computer-readable storage medium).</p>
<p id="p-0076" num="0075">While the present invention has been described with reference to exemplary embodiments, it is to be understood that the invention is not limited to the disclosed exemplary embodiments. The scope of the following claims is to be accorded the broadest interpretation so as to encompass all such modifications and equivalent structures and functions.</p>
<p id="p-0077" num="0076">This application claims the benefit of Japanese Patent Application No. 2010-007445, filed Jan. 15, 2010, which is hereby incorporated by reference herein in its entirety.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A pattern identification apparatus that identifies a pattern that exists in input data using registration data that includes data of a reference pattern or a feature amount thereof and identification parameters that define processing details for comparing the input data with the registration data, the apparatus comprising:
<claim-text>a holding unit configured to hold the registration data and the identification parameters in association with a label;</claim-text>
<claim-text>an acquisition unit configured to acquire external data that includes registration data from an external apparatus;</claim-text>
<claim-text>a first determination unit configured to determine whether registration data which has the same label as acquired by the acquisition unit are already held in the holding unit;</claim-text>
<claim-text>a second determination unit configured to determine whether the external data further includes identification parameters which are different from the identification parameters held in the holding unit in association with the same label as acquired by the acquisition unit if it is determined that the registration data which has the same label are already held in the holding unit;</claim-text>
<claim-text>an adding unit configured to add the registration data and identification parameters acquired by the acquisition unit to the holding unit in association with the label acquired by the acquisition unit if it is determined that the external data further includes different identification parameters;</claim-text>
<claim-text>selection unit configured to select, if it is determined that different identification parameters are already held in the holding unit in association with the same label, one of the identification parameters acquired by the acquisition unit and the identification parameters held in the holding unit in association with the same label; and</claim-text>
<claim-text>an update unit configured to update the held identification parameters with the acquired identification parameters and to add the acquired registration data together with the acquired identification parameters, if the acquired identification parameters are selected by the selection unit, and to add the acquired registration data together with the held identification parameters, if the held identification parameters are selected by the selection unit,</claim-text>
<claim-text>wherein at least one processor functions as at least one of the units.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,
<claim-text>wherein in the case where registration data and identification parameters associated with the same label as acquired by the acquisition unit are already held in the holding unit, the adding unit evaluates an identification rate for the identification parameters acquired by the acquisition unit and the identification parameters held in the holding unit in association with the same label, respectively, and selects the identification parameters having the higher identification rate.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein in the case where registration data and identification parameters associated with the same label as acquired by the acquisition unit are already held in the holding unit, the adding unit selects the identification parameters having the newer generation time from the identification parameters acquired by the acquisition unit and the identification parameters held in the holding unit in association with the same label.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,
<claim-text>wherein the input data is image data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. A control method for a pattern identification apparatus that identifies a pattern that exists in input data using registration data that includes data of a reference pattern or a feature amount thereof and identification parameters that define processing details for comparing the input data with the registration data, the method comprising:
<claim-text>holding the registration data and the identification parameters in association with a label in a holding unit;</claim-text>
<claim-text>acquiring external data that includes registration data, and a label from an external apparatus;</claim-text>
<claim-text>determining whether registration data which has the same label as acquired by the acquiring step are already held in the holding unit;</claim-text>
<claim-text>determining whether the external data further includes identification parameters which are different from the identification parameters held in the holding unit in association with the same label as acquired by the acquisition step if it is determined that the registration data which has the same label are already held in the holding unit;</claim-text>
<claim-text>adding the registration data and identification parameters acquired in the step of acquiring to the holding unit in association with the label acquired in the step of acquiring, if it is determined that the external data further includes different identification parameters;</claim-text>
<claim-text>selecting, if it is determined that different identification parameters are already held in the holding unit in association with the same label, one of the identification parameters acquired in the step of acquiring and the identification parameters held in the holding unit in association with the same label; and</claim-text>
<claim-text>updating the held identification parameters with the acquired identification parameters and adding the acquired registration data together with the acquired identification parameters, if the acquired identification parameters are selected by the selection step, and adding the acquired registration data together with the held identification parameters, if the held identification parameters are selected by the selection step.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. A non-transitory computer-readable storage medium on which is stored a program for causing a computer to execute a control method for a pattern identification apparatus that identifies a pattern that exists in input data using registration data that includes data of a reference pattern or a feature amount thereof and identification parameters that define processing details for comparing the input data with the registration data, the method comprising:
<claim-text>holding the registration data and the identification parameters in association with a label in a holding unit;</claim-text>
<claim-text>acquiring external data that includes registration data and a label from an external apparatus;</claim-text>
<claim-text>determining whether registration data which has the same label as acquired by the acquiring step are already held in the holding unit;</claim-text>
<claim-text>determining whether the external data further includes identification parameters which are different from the identification parameters held in the holding unit in association with the same label as acquired by the acquisition step if it is determined that the registration data which has the same label are already held in the holding unit;</claim-text>
<claim-text>adding the registration data and identification parameters acquired in the step of acquiring to the holding unit in association with the label acquired in the step of acquiring, if it is determined that the external data further includes different identification parameters;</claim-text>
<claim-text>selecting, if it is determined that different identification parameters are already held in the holding unit in association with the same label, one of the identification parameters acquired in the step of acquiring and the identification parameters held in the holding unit in association with the same label; and</claim-text>
<claim-text>updating the held identification parameters with the acquired identification parameters and adding the acquired registration data together with the acquired identification parameters, if the acquired identification parameters are selected by the selection step, and adding the acquired registration data together with the held identification parameters, if the held identification parameters are selected by the selection step.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
