<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625916-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625916</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13541151</doc-number>
<date>20120703</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>KR</country>
<doc-number>10-2007-0028886</doc-number>
<date>20070323</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>11</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>46</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382238</main-classification>
<further-classification>382232</further-classification>
</classification-national>
<invention-title id="d2e71">Method and apparatus for image encoding and image decoding</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>4155097</doc-number>
<kind>A</kind>
<name>Lux</name>
<date>19790500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524024</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>7469069</doc-number>
<kind>B2</kind>
<name>Kim et al.</name>
<date>20081200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382236</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>8014026</doc-number>
<kind>B2</kind>
<name>Cho et al.</name>
<date>20110900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>8086053</doc-number>
<kind>B2</kind>
<name>Kim et al.</name>
<date>20111200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382239</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>8126053</doc-number>
<kind>B2</kind>
<name>Song</name>
<date>20120200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2005/0281473</doc-number>
<kind>A1</kind>
<name>Kim et al.</name>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382236</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2006/0146930</doc-number>
<kind>A1</kind>
<name>Kim et al.</name>
<date>20060700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524003</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2006/0164544</doc-number>
<kind>A1</kind>
<name>Lecompte et al.</name>
<date>20060700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2006/0233251</doc-number>
<kind>A1</kind>
<name>Kim et al.</name>
<date>20061000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2006/0251330</doc-number>
<kind>A1</kind>
<name>Toth et al.</name>
<date>20061100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382236</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2006/0268990</doc-number>
<kind>A1</kind>
<name>Lin et al.</name>
<date>20061100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2007/0019730</doc-number>
<kind>A1</kind>
<name>Lee et al.</name>
<date>20070100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2007/0065026</doc-number>
<kind>A1</kind>
<name>Lee et al.</name>
<date>20070300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2008/0219576</doc-number>
<kind>A1</kind>
<name>Jung et al.</name>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382238</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2008/0240238</doc-number>
<kind>A1</kind>
<name>Yoshino et al.</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524012</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2009/0080515</doc-number>
<kind>A1</kind>
<name>Nagaraj et al.</name>
<date>20090300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3752402</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>CN</country>
<doc-number>1933601</doc-number>
<kind>A</kind>
<date>20070300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>EP</country>
<doc-number>1 569 461</doc-number>
<kind>A2</kind>
<date>20050800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>EP</country>
<doc-number>1773071</doc-number>
<kind>A2</kind>
<date>20070400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>JP</country>
<doc-number>2005-198269</doc-number>
<kind>A</kind>
<date>20050700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00021">
<othercit>International Search Report dated Apr. 8, 2008.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00022">
<othercit>Chinese Office Action issued in Application No. 2008-80009532.4, Feb. 24, 2011.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00023">
<othercit>Yung-lyul Lee, et al.; &#x201c;Lossless INRA Coding for Improved 4:4:4 Coding in H.264/MPEG-4 AVC&#x201d;; ITU Study Group 16&#x2014;Videio Coding Experts Group&#x2014;ISO/IEC MPEC and ITU-T VCEG (ISO/IEC JTC1/SC29/WG11 and ITU-T SG16 Q6) JVT-P016, Jul. 19, 2005; XP030006058.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00024">
<othercit>European Search Report issued in Application No. 08704690.0, Mar. 21, 2011.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00025">
<othercit>Communication dated Mar. 28, 2012 issued by the Intellectual Property Office of P.R. China in counterpart Chinese Patent Application No. 200880009532.4.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00026">
<othercit>Communication, dated Apr. 25, 2013, issued by the European Patent Office in counterpart European Patent Application No. 08704690.0.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00027">
<othercit>Communication, dated May 28, 2013, issued by the European Patent Office in counterpart European Patent Application No. 13161879.5.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00028">
<othercit>Communication, dated Jun. 7, 2013, issued by the European Patent Office in counterpart European Patent Application No. 13161887.8.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>6</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>None</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>13</number-of-drawing-sheets>
<number-of-figures>16</number-of-figures>
</figures>
<us-related-documents>
<division>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>11965104</doc-number>
<date>20071227</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>8244048</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13541151</doc-number>
</document-id>
</child-doc>
</relation>
</division>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120269449</doc-number>
<kind>A1</kind>
<date>20121025</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Sohn</last-name>
<first-name>Yu-mi</first-name>
<address>
<city>Seongnam-si</city>
<country>KR</country>
</address>
</addressbook>
<residence>
<country>KR</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Han</last-name>
<first-name>Woo-jin</first-name>
<address>
<city>Suwon-si</city>
<country>KR</country>
</address>
</addressbook>
<residence>
<country>KR</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Sohn</last-name>
<first-name>Yu-mi</first-name>
<address>
<city>Seongnam-si</city>
<country>KR</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Han</last-name>
<first-name>Woo-jin</first-name>
<address>
<city>Suwon-si</city>
<country>KR</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Sughrue Mion, PLLC</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Samsung Electronics Co., Ltd.</orgname>
<role>03</role>
<address>
<city>Suwon-si</city>
<country>KR</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Motsinger</last-name>
<first-name>Sean</first-name>
<department>2669</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Provided are a method and apparatus for image encoding which improves encoding efficiency in accordance with image characteristics by performing prediction in lines and performing a one-dimensional transformation in lines on an input image, and a method and apparatus for image decoding. Encoding efficiency of an image may be improved by generating a prediction sub residual block using neighboring residues and performing a one-dimensional discrete cosine transformation (DCT) on a difference residual block which is a difference between an original sub residual block and the prediction sub residual block.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="106.93mm" wi="209.89mm" file="US08625916-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="225.13mm" wi="129.79mm" orientation="landscape" file="US08625916-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="179.58mm" wi="75.95mm" file="US08625916-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="79.76mm" wi="84.24mm" file="US08625916-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="219.79mm" wi="112.10mm" file="US08625916-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="218.78mm" wi="101.52mm" file="US08625916-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="209.13mm" wi="118.36mm" orientation="landscape" file="US08625916-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="154.18mm" wi="106.09mm" file="US08625916-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="113.71mm" wi="71.80mm" file="US08625916-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="114.64mm" wi="105.49mm" file="US08625916-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="220.73mm" wi="115.15mm" orientation="landscape" file="US08625916-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="138.68mm" wi="96.44mm" file="US08625916-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="190.75mm" wi="98.21mm" orientation="landscape" file="US08625916-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="122.85mm" wi="89.75mm" file="US08625916-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED PATENT APPLICATION</heading>
<p id="p-0002" num="0001">This application is a divisional application of U.S. application Ser. No. 11/965,104, filed Dec. 27, 2007, which claims priority from Korean Patent Application No. 10-2007-0028886, filed on Mar. 23, 2007, in the Korean Intellectual Property Office, the disclosures of which are incorporated herein in their entirety by reference.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">1. Field of the Invention</p>
<p id="p-0004" num="0003">Apparatuses and methods consistent with the present invention relate to image encoding and image decoding, and more particularly, to image encoding which improves prediction efficiency and compression efficiency in accordance with image characteristics by performing prediction in lines and performing a one-dimensional transformation in lines on an input image.</p>
<p id="p-0005" num="0004">2. Description of the Related Art</p>
<p id="p-0006" num="0005">In general, according to video compression standards such as Moving Picture Experts Group (MPEG)-1, MPEG-2, MPEG-4 Visual, H.261, H.263 and H.264, image data is compressed by dividing an image frame into a plurality of image blocks, performing prediction on the image blocks and thereby obtaining prediction blocks, and transforming and quantizing differences between the original image blocks and the prediction blocks.</p>
<p id="p-0007" num="0006">The prediction performed may be intra prediction or inter prediction. Intra prediction is performed on a current image block by using data of restored neighboring blocks, which is included in the current image block. Inter prediction is performed by generating a prediction block that corresponds to a current image block from one or more video frames previously encoded using a block-based motion compensation method. According to related art methods, generally, data of neighboring blocks used for intra prediction comprises pixels of neighboring previous blocks, which are adjacent to the top and left of the current image block. In this case, top and left pixels of the current image block, which are adjacent to pixels of previous blocks, have small differences between prediction values and original pixel values due to their close distances from the pixels of the previous blocks. However, pixels of the current image block, which are disposed far from the pixels of the previous blocks, may have large differences between prediction values and original pixel values.</p>
<p id="p-0008" num="0007">Meanwhile, according to H.264 standards, two-dimensional discrete cosine transformation (DCT) is performed on residual data obtained by using inter prediction or intra prediction in 4&#xd7;4 blocks. According to related art Joint Photographic Experts Group (JPEG), MPEG-1, MPEG-2, and MPEG-4 standards, two-dimensional DCT is performed on the residual data in 8&#xd7;8 blocks. In the two-dimensional DCT, although horizontal or vertical correlations exist in the residual data, the correlations between data in a residual block may not be efficiently used.</p>
<p id="p-0009" num="0008">Thus, a method of image encoding which improves compression efficiency by improving prediction efficiency is desired in order to cope with a restriction of a transmission bandwidth and provide an image having higher quality to a user.</p>
<heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0010" num="0009">Exemplary embodiments of the present invention overcome the above disadvantages and other disadvantages not described above. Also, the present invention is not required to overcome the disadvantages described above, and an exemplary embodiment of the present invention may not overcome any of the problems described above.</p>
<p id="p-0011" num="0010">Aspects of the present invention provide a method and apparatus for image encoding which improves prediction efficiency and compression efficiency when an image is encoded, and a method and apparatus for image decoding.</p>
<p id="p-0012" num="0011">According to an aspect of the present invention, there is provided a method of image encoding, including generating a plurality of sub residual blocks by dividing a residual block having a predetermined size; generating prediction sub residual blocks of the sub residual blocks by using residues of previously processed neighboring sub residual blocks; generating difference sub residual blocks by calculating differences between the prediction sub residual blocks and the sub residual blocks; and transforming the difference sub residual blocks.</p>
<p id="p-0013" num="0012">According to another aspect of the present invention, there is provided an apparatus for image encoding, including a division unit which generates a plurality of sub residual blocks by dividing a residual block having a predetermined size; a residue prediction unit which generates prediction sub residual blocks of the sub residual blocks by using residues of previously processed neighboring sub residual blocks; a subtraction unit which generates difference sub residual blocks by calculating differences between the prediction sub residual blocks and the sub residual blocks; and a transformation unit which transforms the difference sub residual blocks.</p>
<p id="p-0014" num="0013">According to another aspect of the present invention, there is provided a method of image decoding, including determining a division mode of a current residual block to be decoded by using information on a division mode of the residual block which is included in a received bitstream; generating prediction sub residual blocks of a plurality of sub residual blocks of the residual block by using residues of previously decoded neighboring sub residual blocks in accordance with the determined division mode; restoring difference residues that are differences between the prediction sub residual blocks and the sub residual blocks and are included in the bitstream; and restoring the sub residual blocks by adding the prediction sub residual blocks and the difference residues.</p>
<p id="p-0015" num="0014">According to another aspect of the present invention, there is provided an apparatus for image decoding, including a residue prediction unit which generates prediction sub residual blocks of a plurality of sub residual blocks of a current residual block to be decoded by using residues of previously decoded neighboring sub residual blocks in accordance with a division mode of the residual block included in a received bitstream; a difference residue restoration unit which restores difference residues that are differences between the prediction sub residual blocks and the sub residual blocks and are included in the bitstream; and an addition unit which restores the sub residual blocks by adding the prediction sub residual blocks and the difference residues.</p>
<p id="p-0016" num="0015">According to another aspect of the present invention, there is provided a method of image encoding, including dividing an input image into a plurality of image blocks and generating prediction values of pixels of each image block in horizontal or vertical lines; generating residues that are differences between original values and the prediction values of the pixels, in lines; and performing a one-dimensional discrete cosine transformation (DCT) on the residues in lines.</p>
<p id="p-0017" num="0016">According to another aspect of the present invention, there is provided an apparatus for image encoding, including a prediction unit which divides an input image into a plurality of image blocks and generates prediction values of pixels of each image block in horizontal or vertical pixel lines; a subtraction unit which generates residues that are differences between original values and the prediction values of the pixels, in lines; and a transformation unit which performs one-dimensional discrete cosine transformation (DCT) on the residues in lines.</p>
<p id="p-0018" num="0017">According to another aspect of the present invention, there is provided a method of image decoding, including restoring residues that are differences between prediction values and original values of horizontal or vertical pixel lines and are included in a received bitstream; predicting pixel values of each pixel line to be decoded by using pixel values of a previous pixel line decoded in a predetermined order; and decoding pixels of the pixel lines by adding the predicted pixel values of the pixel lines and the restored residues.</p>
<p id="p-0019" num="0018">According to another aspect of the present invention, there is provided an apparatus for image decoding, including a prediction unit which predicts pixel values of horizontal or vertical pixel lines to be decoded by using previous pixel lines in vertical or horizontal lines in a predetermined order; a restoration unit which restores residues that are differences between prediction values of the pixel lines and original pixel values of the pixel lines and are included in a received bitstream; and an addition unit which decodes pixels of the pixel lines by adding the predicted pixel values of the pixel lines and the restored residues.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0020" num="0019">The above and other features of the present invention will become more apparent by describing in detail exemplary embodiments thereof with reference to the attached drawings in which:</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram illustrating an apparatus for image encoding, according to an embodiment of the present invention;</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIGS. 2A</figref>, <b>2</b>B and <b>2</b>C are diagrams illustrating examples of when a residual block is divided into a plurality of sub residual blocks, according to an exemplary embodiment of the present invention;</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIGS. 3A and 3B</figref> are diagrams for illustrating a method of generating prediction sub residual blocks, according to an exemplary embodiment of the present invention;</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 4</figref> is a diagram for illustrating a method of generating prediction sub residual blocks, according to another exemplary embodiment of the present invention;</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 5</figref> is a flowchart illustrating a method of image encoding, according to an exemplary embodiment of the present invention;</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 6</figref> is a block diagram illustrating an apparatus for image encoding, according to another exemplary embodiment of the present invention;</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 7</figref> is a diagram for illustrating a method of predicting pixel values in lines by a prediction unit illustrated in <figref idref="DRAWINGS">FIG. 6</figref>;</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 8</figref> is a diagram for illustrating a method of predicting pixel values, according to another exemplary embodiment of the present invention;</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 9</figref> is a flowchart illustrating a method of image encoding, according to another exemplary embodiment of the present invention;</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 10</figref> is a block diagram illustrating an apparatus for image decoding, according to an exemplary embodiment of the present invention;</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 11</figref> is a flowchart illustrating a method of image decoding, according to an exemplary embodiment of the present invention;</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 12</figref> is a block diagram illustrating an apparatus for image decoding, according to another exemplary embodiment of the present invention; and</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 13</figref> is a flowchart illustrating a method of image decoding, according to another exemplary embodiment of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION OF EXEMPLARY EMBODIMENTS OF THE INVENTION</heading>
<p id="p-0034" num="0033">Hereinafter, the present invention will be described in detail by explaining exemplary embodiments of the invention with reference to the attached drawings.</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram illustrating an apparatus <b>100</b> for image encoding, according to an exemplary embodiment of the present invention.</p>
<p id="p-0036" num="0035">The apparatus <b>100</b> divides a residual block, that is, a difference between an original image block and a prediction image block into a plurality of sub residual blocks, generates prediction sub residual blocks of the sub residual blocks by using neighboring residues, and transforms difference sub residual blocks that are differences between the original sub residual blocks and the prediction sub residual blocks.</p>
<p id="p-0037" num="0036">Referring to <figref idref="DRAWINGS">FIG. 1</figref>, the apparatus <b>100</b> includes a prediction unit <b>110</b>, a first subtraction unit <b>115</b>, a division unit <b>120</b>, a second subtraction unit <b>125</b>, a residue prediction unit <b>130</b>, a transformation unit <b>135</b>, a quantization unit <b>140</b>, an entropy encoding unit <b>145</b>, an inverse quantization unit <b>150</b>, an inverse transformation unit <b>155</b> and an addition unit <b>160</b>.</p>
<p id="p-0038" num="0037">The prediction unit <b>110</b> divides an input image into a plurality of sub blocks having a predetermined size and generates prediction blocks by performing inter or intra prediction on each of the sub blocks. The inter prediction is performed by using a reference picture that was previously encoded and then restored. The prediction unit <b>110</b> performs the inter prediction by performing motion prediction which generates motion vectors indicating regions similar to regions of a current block in a predetermined search range of the reference picture and by performing motion compensation which obtains data on corresponding regions of the reference picture which are indicated by the motion vectors, thereby generating a prediction block of the current block. Also, the prediction unit <b>110</b> performs the intra prediction which generates a prediction block by using data of neighboring blocks of the current block. The inter prediction and the intra prediction according to related art image compression standards such as H.264 may be used and a variety of modified prediction methods may also be used.</p>
<p id="p-0039" num="0038">When the prediction block of the current block is generated by performing the inter prediction or the intra prediction, the first subtraction unit <b>115</b> calculates prediction errors by subtracting pixel values of the prediction block from original pixel values of the current block. Hereinafter, a prediction error between an original pixel value and a prediction pixel value is defined as a residue and a block composed of a plurality of residues is defined as a residual block.</p>
<p id="p-0040" num="0039">The division unit <b>120</b> divides the residual block into a plurality of sub residual blocks. In more detail, assuming that the size of the residual block is N&#xd7;N (where N is a positive number equal to or greater than 2), the residual block is divided into the sub residual blocks having the size of any one of N&#xd7;1, 1&#xd7;N, and a&#xd7;a (where a is a natural number smaller than N).</p>
<p id="p-0041" num="0040"><figref idref="DRAWINGS">FIGS. 2A through 2B</figref> are diagrams illustrating examples of when a residual block is divided into a plurality of sub residual blocks, according to an exemplary embodiment of the present invention. <figref idref="DRAWINGS">FIG. 2A</figref> is a diagram illustrating an example of when a 4&#xd7;4 residual block <b>210</b> is divided into a plurality of 1&#xd7;4 sub residual blocks <b>211</b>, <b>212</b>, <b>213</b> and <b>214</b>. <figref idref="DRAWINGS">FIG. 2B</figref> is a diagram illustrating an example of when a 4&#xd7;4 residual block <b>220</b> is divided into a plurality of 4&#xd7;1 sub residual blocks <b>221</b>, <b>222</b>, <b>223</b> and <b>224</b>. <figref idref="DRAWINGS">FIG. 2C</figref> is a diagram illustrating an example of when a 4&#xd7;4 residual block <b>230</b> is divided into a plurality of 2&#xd7;2 sub residual blocks <b>231</b>, <b>232</b>, <b>233</b> and <b>234</b>. Although only a 4&#xd7;4 residual block is described as an example, the present invention is not limited thereto. The present invention may also be similarly applied to a variety of residual blocks such as an 8&#xd7;8 residual block and a 16&#xd7;16 residual block.</p>
<p id="p-0042" num="0041">Referring back to <figref idref="DRAWINGS">FIG. 1</figref>, the residue prediction unit <b>130</b> generates prediction sub residual blocks of the sub residual blocks by predicting residues of the sub residual blocks of the residual block divided as illustrated in <figref idref="DRAWINGS">FIG. 2A</figref>, <b>2</b>B or <b>2</b>C by using residues of previously processed neighboring sub residual blocks.</p>
<p id="p-0043" num="0042"><figref idref="DRAWINGS">FIGS. 3A and 3B</figref> are diagrams for illustrating a method of generating prediction sub residual blocks, according to an exemplary embodiment of the present invention. In <figref idref="DRAWINGS">FIGS. 3A and 3B</figref>, Rxy represents a residue at a location (x,y) (x,y=1, 2, 3, 4). A method of generating prediction sub residual blocks by dividing a 4&#xd7;4 residual block into a plurality of 1&#xd7;4 sub residual blocks <b>311</b>, <b>312</b>, <b>313</b> and <b>314</b> or <b>321</b>, <b>322</b>, <b>323</b> and <b>324</b> will now be described with reference to <figref idref="DRAWINGS">FIGS. 3A and 3B</figref>.</p>
<p id="p-0044" num="0043">Referring to <figref idref="DRAWINGS">FIG. 3A</figref>, the sub residual blocks <b>311</b>, <b>312</b>, <b>313</b> and <b>314</b> included in the 4&#xd7;4 residual block are separately predicted by using residues of neighboring sub residual blocks previously processed in a predetermined order. The prediction may be performed in an orthogonal direction to a division direction of the sub residual blocks <b>311</b>, <b>312</b>, <b>313</b> and <b>314</b>. For example, assuming that the sub residual blocks <b>311</b>, <b>312</b>, <b>313</b> and <b>314</b> divided in a horizontal direction are sequentially predicted in a downward direction, residues R<b>11</b>, R<b>12</b>, R<b>13</b> and R<b>14</b> of a first sub residual block <b>311</b> may be predicted by extending residues a, b, c and d of a previous residual block encoded prior to the current residual block in a vertical direction. That is, assuming that residues R<b>11</b>, R<b>12</b>, R<b>13</b> and R<b>14</b> of the first sub residual block <b>311</b> have prediction residues PR<b>11</b>, PR<b>12</b>, PR<b>13</b> and PR<b>14</b>, respectively, PR<b>11</b>=a, PR<b>12</b>=b, PR<b>13</b>=c and PR<b>14</b>=d.</p>
<p id="p-0045" num="0044">Also, assuming that residues R<b>21</b>, R<b>22</b>, R<b>23</b> and R<b>24</b> of a second sub residual block <b>312</b> have prediction residues PR<b>21</b>, PR<b>22</b>, PR<b>23</b> and PR<b>24</b>, respectively, prediction residues PR<b>21</b>, PR<b>22</b>, PR<b>23</b> and PR<b>24</b> of the second sub residual block <b>312</b> may be predicted by extending previously processed residues R<b>11</b>, R<b>12</b>, R<b>13</b> and R<b>14</b> of the first sub residual block <b>311</b> in a vertical direction. Likewise, prediction residues PR<b>31</b>, PR<b>32</b>, PR<b>33</b> and PR<b>34</b> of residues R<b>31</b>, R<b>32</b>, R<b>33</b> and R<b>34</b> of a third sub residual block <b>313</b> and prediction residues PR<b>41</b>, PR<b>42</b>, PR<b>43</b> and PR<b>44</b> of residues R<b>41</b>, R<b>42</b>, R<b>43</b> and R<b>44</b> of a fourth sub residual block <b>314</b> may be predicted by extending original or restored residues R<b>21</b>, R<b>22</b>, R<b>23</b> and R<b>24</b> of the second sub residual block <b>312</b> and original or restored residues R<b>31</b>, R<b>32</b>, R<b>33</b> and R<b>34</b> of the third sub residual block <b>313</b>, respectively. In this case, when each sub residual block is predicted by using residues of a previous sub residual block which are differences between an original image and a prediction image or by using residues of a neighboring sub residual block restored by performing a one-dimensional discrete cosine transformation (DCT), quantization, inverse quantization, and one-dimensional inverse discrete cosine transformation (IDCT) on a difference sub residual block and by adding the difference sub residual block to a prediction sub residual block.</p>
<p id="p-0046" num="0045">In <figref idref="DRAWINGS">FIG. 3A</figref>, the sub residual blocks divided in a horizontal direction are sequentially predicted in a downward direction. However, a prediction order of the sub residual blocks may be changed as illustrated in <figref idref="DRAWINGS">FIG. 3B</figref>.</p>
<p id="p-0047" num="0046">Referring to <figref idref="DRAWINGS">FIG. 3B</figref>, a fourth sub residual block <b>324</b> is predicted first, then a second sub residual block <b>322</b> is predicted, then a first sub residual block <b>321</b> is predicted, and then a third sub residual block <b>323</b> is predicted. In more detail, residues R<b>41</b>, R<b>42</b>, R<b>43</b> and R<b>44</b> of the fourth sub residual block <b>324</b> are predicted by extending residues a, b, c and d of a previous residual block, then residues R<b>21</b>, R<b>22</b>, R<b>23</b> and R<b>24</b> of the second sub residual block <b>322</b> are predicted by calculating average values of corresponding residues, each respectively from among the residues a, b, c and d of the previous residual block and from among residues R<b>41</b>, R<b>42</b>, R<b>43</b> and R<b>44</b> of the fourth sub residual block <b>324</b>. Also, residues R<b>11</b>, R<b>12</b>, R<b>13</b> and R<b>14</b> of the first sub residual block <b>321</b> are predicted by calculating average values of corresponding residues, each respectively from among the residues a, b, c and d of the previous residual block and from among residues R<b>21</b>, R<b>22</b>, R<b>23</b> and R<b>24</b> of the second sub residual block <b>322</b>, and residues R<b>31</b>, R<b>32</b>, R<b>33</b> and R<b>34</b> of the third sub residual block <b>323</b> are predicted by calculating average values of corresponding residues, each respectively from among residues R<b>21</b>, R<b>22</b>, R<b>23</b> and R<b>24</b> of the second sub residual block <b>322</b> and from among residues R<b>41</b>, R<b>42</b>, R<b>43</b> and R<b>44</b> of the fourth sub residual block <b>324</b>. For example, assuming that PRxy is a prediction residue of a residue Rxy, PR<b>41</b>=a, PR<b>21</b>=(a+R<b>41</b>)/2, PR<b>11</b>=(a+R<b>21</b>)/2, and PR<b>31</b>=(R<b>21</b>+R<b>41</b>)/2.</p>
<p id="p-0048" num="0047">The above described method of generating prediction sub residual blocks of sub residual blocks divided in a horizontal direction may also be similarly applied to sub residual blocks divided in a vertical direction as illustrated in <figref idref="DRAWINGS">FIG. 2B</figref>.</p>
<p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. 4</figref> is a diagram for illustrating a method of generating prediction sub residual blocks, according to another exemplary embodiment of the present invention.</p>
<p id="p-0050" num="0049">When a residual block is divided into a plurality of sub residual blocks having the one width as the residual block illustrated in <figref idref="DRAWINGS">FIG. 3A</figref> or <figref idref="DRAWINGS">FIG. 3B</figref>, residues of a current sub residual block is predicted by performing prediction in lines using residues of a previous sub residual block disposed in a orthogonal direction of a division direction of the sub residual blocks. However, referring <figref idref="DRAWINGS">FIG. 4</figref>, when a 4&#xd7;4 residual block is divided into a plurality of 2&#xd7;2 sub residual blocks, prediction residues may be generated by extending neighboring pixels of a previous sub residual block at least in one of a horizontal direction and a vertical direction. For example, assuming that a residue Rxy at a location (x,y) of a 2&#xd7;2 sub residual block <b>410</b> has a prediction residue PRxy, if upper neighboring previous residues a and b are extended in a vertical direction, PR<b>11</b>=a, PR<b>13</b>=a, PR<b>12</b>=b, and PR<b>14</b>=b or if left neighboring previous residues c and d are extended in a horizontal direction, PR<b>11</b>=c, PR<b>12</b>=c, PR<b>13</b>=d, and PR<b>14</b>=d. Alternatively, a prediction residue of a current sub residual block to be predicted may be calculated as an average residue of previous residues at the same horizontal and vertical line from the upper and left neighboring previous residues. For example, PR<b>11</b>=(a+c)/2, PR<b>12</b>=(b+c)/2, PR<b>13</b>=(a+d)/2, and PR<b>14</b>=(b+d)/2.</p>
<p id="p-0051" num="0050">Referring back to <figref idref="DRAWINGS">FIG. 1</figref>, the second subtraction unit <b>125</b> generates difference sub residual blocks by calculating differences between the prediction sub residual blocks generated by the residue prediction unit <b>130</b> and the original sub residual blocks.</p>
<p id="p-0052" num="0051">The transformation unit <b>135</b> performs DCT on the difference sub residual blocks. In particular, the transformation unit <b>135</b> performs one-dimensional DCT on the N&#xd7;1 or 1&#xd7;N sub residual blocks. For example, the transformation unit <b>135</b> performs one-dimensional horizontal DCT on the difference sub residual blocks which are divided in a horizontal direction as illustrated in <figref idref="DRAWINGS">FIG. 2A</figref> and then are predicted and performs one-dimensional vertical DCT on the difference sub residual blocks which are divided in a vertical direction as illustrated in <figref idref="DRAWINGS">FIG. 2B</figref> and then are predicted.</p>
<p id="p-0053" num="0052">The quantization unit <b>140</b> performs quantization and the entropy encoding unit <b>145</b> performs variable length encoding on difference residues of the transformed difference sub residual blocks so that a bitstream is generated.</p>
<p id="p-0054" num="0053">The difference residues quantized by the quantization unit <b>140</b> are inverse quantized by the inverse quantization unit <b>150</b> and inverse transformed by the inverse transformation unit <b>155</b> so that the difference sub residual blocks are restored.</p>
<p id="p-0055" num="0054">The addition unit <b>160</b> restores the sub residual blocks by adding difference residues of the restored difference sub residual blocks and prediction residues of the prediction sub residual blocks which are generated by the reside prediction unit <b>130</b>. The restored sub residual blocks are used when prediction sub residual blocks of next sub residual blocks are generated.</p>
<p id="p-0056" num="0055">Also, the apparatus <b>100</b> may further include a division mode determination unit (not shown) which compares costs of bitstreams generated by using a plurality of sub residual blocks having different sizes, and selects sub residual blocks having the smallest cost so as to use the sub residual block to divide a current residual block.</p>
<p id="p-0057" num="0056">The division mode determination unit determines a division mode of a residual block by dividing the residual block into a plurality of sub residual blocks having different sizes, generating prediction sub residual blocks of the sub residual blocks by using residues of a previous sub residual block, and comparing costs of bitstreams generated by transforming, quantizing and entropy encoding difference sub residual blocks. For example, the division mode determination unit divides an N&#xd7;N residual block into a plurality of 1&#xd7;N sub residual blocks in division mode <b>1</b>, into a plurality of N&#xd7;1 sub residual blocks in division mode <b>2</b>, or into a plurality of a&#xd7;a sub residual blocks in division mode <b>3</b>, compares rate distortion (RD) costs of bitstreams generated by transforming, quantizing, and entropy encoding the difference sub residual blocks generated in accordance with each division mode, and determines the division mode having the smallest RD cost as a final division mode.</p>
<p id="p-0058" num="0057">The division mode determination unit may also determine whether to perform transformation of the residual block or not by comparing costs of bitstreams generated by encoding difference residual blocks of a plurality of sub residual blocks having different sizes and costs of bitstreams generated by bypassing transformation of a residual block and quantizing and entropy encoding the residual block.</p>
<p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. 5</figref> is a flowchart illustrating a method of image encoding, according to an exemplary embodiment of the present invention.</p>
<p id="p-0060" num="0059">Referring to <figref idref="DRAWINGS">FIG. 5</figref>, in operation <b>510</b>, a residual block generated by subtracting pixel values of a prediction block from original pixel values of a current block is divided into a plurality of sub residual blocks.</p>
<p id="p-0061" num="0060">In operation <b>520</b>, prediction sub residual blocks are generated by predicting residues of the current sub residual blocks using residues of previously processed sub residual blocks. As described above, the prediction sub residual blocks are predicted by extending the residues of the previous sub residual blocks at least in one of a horizontal direction and a vertical direction in accordance with a division type of the sub residual blocks.</p>
<p id="p-0062" num="0061">In operation <b>530</b>, difference sub residual blocks are generated by calculating differences between the prediction sub residual blocks and the original sub residual blocks.</p>
<p id="p-0063" num="0062">In operation <b>540</b>, DCT is performed on the difference sub residual blocks in accordance with the division type. As described above, one-dimensional DCT is performed on N&#xd7;1 or 1&#xd7;N difference sub residual blocks. The transformed difference sub residual blocks are quantized and entropy encoded and thus a bitstream is output. Also, the sub residual blocks are restored by inverse quantizing and inverse transforming the quantized difference sub residual blocks and adding the processed difference sub residual blocks to the prediction sub residual blocks. The restored sub residual blocks are used when residues of next sub residual blocks are predicted.</p>
<p id="p-0064" num="0063">In a method and apparatus for image encoding according to the above exemplary embodiments described with reference to <figref idref="DRAWINGS">FIGS. 1 through 6</figref>, if horizontal or vertical correlations exist in a residual block, the magnitude of data generated by performing DCT in order to be encoded is reduced and thus compression efficiency is improved. For example, assuming that a 4&#xd7;4 residual block includes residues having vertical correlations as shown in a matrix as shown</p>
<p id="p-0065" num="0064">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mrow>
  <mrow>
    <mo>(</mo>
    <mtable>
      <mtr>
        <mtd>
          <mn>0</mn>
        </mtd>
        <mtd>
          <mn>10</mn>
        </mtd>
        <mtd>
          <mn>0</mn>
        </mtd>
        <mtd>
          <mn>0</mn>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mn>0</mn>
        </mtd>
        <mtd>
          <mn>10</mn>
        </mtd>
        <mtd>
          <mn>0</mn>
        </mtd>
        <mtd>
          <mn>0</mn>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mn>0</mn>
        </mtd>
        <mtd>
          <mn>10</mn>
        </mtd>
        <mtd>
          <mn>0</mn>
        </mtd>
        <mtd>
          <mn>0</mn>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mn>0</mn>
        </mtd>
        <mtd>
          <mn>10</mn>
        </mtd>
        <mtd>
          <mn>0</mn>
        </mtd>
        <mtd>
          <mn>0</mn>
        </mtd>
      </mtr>
    </mtable>
    <mo>)</mo>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
</maths>
<br/>
if two-dimensional DCT is applied, the matrix is transformed to
</p>
<p id="p-0066" num="0065">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mrow>
  <mrow>
    <mo>(</mo>
    <mtable>
      <mtr>
        <mtd>
          <mn>10</mn>
        </mtd>
        <mtd>
          <mn>5.4120</mn>
        </mtd>
        <mtd>
          <mrow>
            <mo>-</mo>
            <mn>10</mn>
          </mrow>
        </mtd>
        <mtd>
          <mrow>
            <mo>-</mo>
            <mn>13.0656</mn>
          </mrow>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mn>0</mn>
        </mtd>
        <mtd>
          <mn>0</mn>
        </mtd>
        <mtd>
          <mn>0</mn>
        </mtd>
        <mtd>
          <mn>0</mn>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mn>0</mn>
        </mtd>
        <mtd>
          <mn>0</mn>
        </mtd>
        <mtd>
          <mn>0</mn>
        </mtd>
        <mtd>
          <mn>0</mn>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mn>0</mn>
        </mtd>
        <mtd>
          <mn>0</mn>
        </mtd>
        <mtd>
          <mn>0</mn>
        </mtd>
        <mtd>
          <mn>0</mn>
        </mtd>
      </mtr>
    </mtable>
    <mo>)</mo>
  </mrow>
  <mo>.</mo>
</mrow>
</math>
</maths>
<br/>
However, if one-dimensional vertical DCT is applied, the matrix is transformed to
</p>
<p id="p-0067" num="0066">
<maths id="MATH-US-00003" num="00003">
<math overflow="scroll">
<mrow>
  <mrow>
    <mo>(</mo>
    <mtable>
      <mtr>
        <mtd>
          <mn>0</mn>
        </mtd>
        <mtd>
          <mn>20</mn>
        </mtd>
        <mtd>
          <mn>0</mn>
        </mtd>
        <mtd>
          <mn>0</mn>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mn>0</mn>
        </mtd>
        <mtd>
          <mn>0</mn>
        </mtd>
        <mtd>
          <mn>0</mn>
        </mtd>
        <mtd>
          <mn>0</mn>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mn>0</mn>
        </mtd>
        <mtd>
          <mn>0</mn>
        </mtd>
        <mtd>
          <mn>0</mn>
        </mtd>
        <mtd>
          <mn>0</mn>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mn>0</mn>
        </mtd>
        <mtd>
          <mn>0</mn>
        </mtd>
        <mtd>
          <mn>0</mn>
        </mtd>
        <mtd>
          <mn>0</mn>
        </mtd>
      </mtr>
    </mtable>
    <mo>)</mo>
  </mrow>
  <mo>.</mo>
</mrow>
</math>
</maths>
<br/>
As a result, if one-dimensional DCT is performed on a residual block divided in a horizontal or vertical direction according to an exemplary embodiment of the present invention, the magnitude of transformation coefficients generated is reduced in accordance with image characteristics so that compression efficiency is improved.
</p>
<p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. 6</figref> is a block diagram illustrating an apparatus <b>600</b> for image encoding, according to another exemplary embodiment of the present invention.</p>
<p id="p-0069" num="0068">The apparatus <b>100</b> illustrated in <figref idref="DRAWINGS">FIG. 1</figref> according to the previous exemplary embodiment of the present invention divides a residual block into a plurality of sub residual blocks, generates prediction sub residual blocks of the sub residual blocks, and transforms difference sub residual blocks that are differences between the original sub residual blocks and the prediction sub residual blocks. However, the apparatus <b>600</b> according to the current exemplary embodiment generates prediction values of an input image block, not a residual block, in lines and one-dimensional DCT is performed on the prediction values.</p>
<p id="p-0070" num="0069">Referring to <figref idref="DRAWINGS">FIG. 6</figref>, the apparatus <b>600</b> includes a prediction unit <b>610</b>, a subtraction unit <b>615</b>, a transformation unit <b>620</b>, a quantization unit <b>625</b>, an entropy encoding unit <b>630</b>, an inverse quantization unit <b>635</b>, an inverse transformation unit <b>640</b> and an addition unit <b>645</b>.</p>
<p id="p-0071" num="0070">The prediction unit <b>610</b> divides an input image into a plurality of image blocks and predicts pixel values of each image block in horizontal or vertical pixel lines. The prediction unit <b>610</b> predicts the pixel values in horizontal or vertical pixel lines in the same manner as the residue prediction unit <b>130</b> of the apparatus <b>100</b> illustrated in <figref idref="DRAWINGS">FIG. 1</figref> which predicts residues of current 1&#xd7;N or N&#xd7;1 sub residual blocks divided from a residual block by using residues of a neighboring sub residual block.</p>
<p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. 7</figref> is a diagram for illustrating a method of predicting pixel values in lines by the prediction unit <b>610</b> illustrated in <figref idref="DRAWINGS">FIG. 6</figref>. In <figref idref="DRAWINGS">FIG. 7</figref>, Pab represents a pixel value at a location (a,b) (a,b=1, 2, 3, 4) of an input image block and x, y, z, w, u and v represent pixel values of a neighboring block. Although only a 4&#xd7;4 input image block that is divided into a plurality of horizontal pixel lines is illustrated in <figref idref="DRAWINGS">FIG. 7</figref> as an example, the present invention is not limited thereto. An exemplary embodiment of the present invention may also be applied to input image blocks having different sizes and an input image block divided into a plurality of vertical pixel lines.</p>
<p id="p-0073" num="0072">Referring to <figref idref="DRAWINGS">FIG. 7</figref>, assuming that the pixel values of the current block are sequentially predicted in horizontal pixel lines in a downward direction, pixel values P<b>11</b>, P<b>12</b>, P<b>13</b> and P<b>14</b> of a first horizontal line <b>711</b> may be predicted by extending pixel values x, y, z and w of the neighboring block in an orthogonal direction of a direction of the horizontal pixel lines. Assuming that a prediction pixel value of a pixel value Pab at a location (a,b) (a,b=1, 2, 3, 4) is PPab, PP<b>11</b>=x, PP<b>12</b>=y, PP<b>13</b>=z, and PP<b>14</b>=w. Also, pixel values P<b>21</b>, P<b>22</b>, P<b>23</b> and P<b>24</b> of a second horizontal line <b>712</b> may be predicted by extending pixel values P<b>11</b>, P<b>12</b>, P<b>13</b> and P<b>14</b> of the first horizontal line <b>711</b> in an orthogonal direction of the direction of the horizontal pixel lines. Likewise, pixel values P<b>31</b>, P<b>32</b>, P<b>33</b> and P<b>34</b> of a third horizontal line <b>713</b> and pixel values P<b>41</b>, P<b>42</b>, P<b>43</b> and P<b>44</b> of a fourth horizontal line <b>714</b> may be respectively predicted by extending pixel values P<b>21</b>, P<b>22</b>, P<b>23</b> and P<b>24</b> of the second horizontal line <b>712</b> and pixel values P<b>31</b>, P<b>32</b>, P<b>33</b> and P<b>34</b> of the third horizontal line <b>713</b> in an orthogonal direction of the direction of the horizontal pixel lines. Here, the original pixel values or pixel values restored by being transformed, quantized, inverse quantized and inverse transformed may be used as pixel values of a previous horizontal pixel line in order to predict pixel values of a current horizontal pixel line.</p>
<p id="p-0074" num="0073">A method of sequentially predicting pixel values of an image block in horizontal pixel lines in a downward direction is described in <figref idref="DRAWINGS">FIG. 7</figref>. However, a prediction order of the horizontal pixel lines may be changed as shown by the prediction order of the sub residual blocks <b>321</b>, <b>322</b>, <b>323</b> and <b>324</b> illustrated in <figref idref="DRAWINGS">FIG. 3B</figref>. Furthermore, the pixel values of the image block may be predicted in vertical pixel lines.</p>
<p id="p-0075" num="0074">Referring back to <figref idref="DRAWINGS">FIG. 6</figref>, since the prediction unit <b>610</b> generates prediction values of pixel values of a current block in lines by using pixel values of a neighboring block, a problem of related art block-based prediction, in which prediction efficiency of pixels disposed relatively far from the neighboring block than the other pixels is reduced, may be improved.</p>
<p id="p-0076" num="0075">Meanwhile, the prediction unit <b>610</b> may predict each pixel value of the input image block by using a half-pel interpolation filter.</p>
<p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. 8</figref> is a diagram for illustrating a method of predicting pixel values, according to another exemplary embodiment of the present invention. <figref idref="DRAWINGS">FIG. 8</figref> illustrates pixel value P<b>11</b> illustrated in <figref idref="DRAWINGS">FIG. 7</figref> and previous pixel values u, v and x, which are disposed in a vertical direction of pixel value P<b>11</b>.</p>
<p id="p-0078" num="0077">Referring to <figref idref="DRAWINGS">FIGS. 7 and 8</figref>, when pixel value P<b>11</b> is predicted, an interpolation value h is generated at a half-pel location by using the previous pixel values u, v and x and a prediction value of pixel value P<b>11</b> may be generated by using the interpolation value h and the closest neighboring pixel value x. First, the interpolation value h may be interpolated by a 3-tap filter as shown in Equation <b>1</b> using the previous pixel values u, v and x.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>h</i>=(<i>w</i>1<i>&#xb7;x+w</i>2<i>&#xb7;v+w</i>3<i>&#xb7;u+w</i>4)&#x3e;&#x3e;4&#x2003;&#x2003;1),<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0079" num="0078">where w<b>1</b>, w<b>2</b> and w<b>3</b> represent weights given in accordance with relative distances between the interpolation value h and the previous pixel values u, v and x, w<b>4</b> represents a predetermined offset value, and an operator &#x201c;&#x3e;&#x3e;&#x201d; represents a shift operation.</p>
<p id="p-0080" num="0079">For example, w<b>1</b>=20, w<b>2</b>=&#x2212;5, w<b>3</b>=1, and w<b>4</b>=8.</p>
<p id="p-0081" num="0080">When the interpolation value h at the half-pel location between pixel value P<b>11</b> to be predicted and the previous neighboring pixel value x is interpolated, pixel value P<b>11</b> may be predicted by using the interpolation value h and the previous neighboring pixel value x as shown in Equation <b>2</b>.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i>11<i>=x</i>+(<i>h&#x2212;x</i>)&#xd7;2=2<i>h&#x2212;x</i>&#x2003;&#x2003;2)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0082" num="0081">Likewise, each pixel value of the current block may be predicted by generating half-pel interpolation values at half-pel locations and by using the half-pel interpolation values and the previous pixel values which is the closest to the current pixel to be predicted.</p>
<p id="p-0083" num="0082">Referring back to <figref idref="DRAWINGS">FIG. 6</figref>, the subtraction unit <b>615</b> generates residues that are differences between the prediction values and the original pixel values in lines.</p>
<p id="p-0084" num="0083">The transformation unit <b>620</b> performs one-dimensional DCT on the residues in lines. If the prediction unit <b>610</b> has predicted the pixel values in horizontal lines, the transformation unit <b>620</b> generates transformation coefficients by performing a one-dimensional horizontal DCT. If the prediction unit <b>610</b> has predicted the pixel values in vertical lines, the transformation unit <b>620</b> generates transformation coefficients by performing a one-dimensional vertical DCT. Meanwhile, although the prediction unit <b>610</b> has generated the prediction values in lines, the transformation unit <b>620</b> may alternatively perform two-dimensional DCT in blocks after the prediction of all pixels included in the image block is completed.</p>
<p id="p-0085" num="0084">The quantization unit <b>625</b> quantizes the transformation coefficients in lines and the entropy encoding unit <b>630</b> performs variable-length encoding on the quantized transformation coefficients so that a bitstream is generated.</p>
<p id="p-0086" num="0085">The quantized transformation coefficients are inverse quantized by the inverse quantization unit <b>635</b> and inverse transformed by the inverse transformation unit <b>640</b> so that the residues are restored.</p>
<p id="p-0087" num="0086">The addition unit <b>645</b> restores the pixel values in lines by adding the restored residues and the prediction pixel values generated by the prediction unit <b>610</b>. As described above, when two-dimensional DCT has been performed, the pixel values may be restored in blocks. The restored pixel values in lines are used when next pixel values in lines are predicted.</p>
<p id="p-0088" num="0087"><figref idref="DRAWINGS">FIG. 9</figref> is a flowchart illustrating a method of image encoding, according to another exemplary embodiment of the present invention.</p>
<p id="p-0089" num="0088">Referring to <figref idref="DRAWINGS">FIG. 9</figref>, in operation <b>910</b>, an input image is divided into a plurality of image blocks and prediction values of pixels of each image block are generated in horizontal or vertical lines.</p>
<p id="p-0090" num="0089">In operation <b>920</b>, residues that are differences between the prediction values and original values of the pixels in lines, are generated.</p>
<p id="p-0091" num="0090">In operation <b>930</b>, transformation coefficients are generated by performing a one-dimensional DCT on the residues generated in lines. A bitstream is generated by quantizing and entropy encoding the transformation coefficients in lines. As described above, although the prediction is performed in lines, the transformation may be performed in blocks as in a related art method.</p>
<p id="p-0092" num="0091">In a method and apparatus for image encoding according to the above exemplary embodiments described with reference to <figref idref="DRAWINGS">FIGS. 7 through 9</figref>, prediction values are generated in lines so that a distance between a current pixel and a reference pixel used for prediction is reduced. Accordingly, accuracy of the prediction increases and thus a bit rate may be reduced.</p>
<p id="p-0093" num="0092"><figref idref="DRAWINGS">FIG. 10</figref> is a block diagram illustrating an apparatus <b>1000</b> for image decoding, according to an exemplary embodiment of the present invention. The apparatus <b>1000</b> for image decoding corresponds to the apparatus <b>100</b> for image encoding illustrated in <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0094" num="0093">Referring to <figref idref="DRAWINGS">FIG. 10</figref>, the apparatus <b>1000</b> includes an entropy decoding unit <b>1010</b>, an inverse quantization unit <b>1020</b>, an inverse transformation unit <b>1030</b>, a residue prediction unit <b>1040</b>, a first addition unit <b>1050</b>, a second addition unit <b>1060</b> and a prediction unit <b>1070</b>.</p>
<p id="p-0095" num="0094">The entropy decoding unit <b>1010</b> receives and entropy decodes a compressed bitstream so that information on a division mode of a current residual block which is included in the bitstream is extracted. The entropy decoding unit <b>1010</b> also entropy decodes difference residues included in the bitstream, the inverse quantization unit <b>1020</b> inverse quantizes the entropy decoded difference residues, and the inverse transformation unit <b>1030</b> restores the difference residues by inverse transforming the inverse quantized difference residues. In particular, the inverse transformation unit <b>1030</b> performs one-dimensional inverse DCT if the current residual block is encoded in N&#xd7;1 or 1&#xd7;N sub residual blocks.</p>
<p id="p-0096" num="0095">The residue prediction unit <b>1040</b> divides the current residual block into a plurality of sub residual blocks in accordance with the extracted information on the division mode of the current residual block to be decoded and generates prediction sub residual blocks of the current sub residual blocks by using residues of previously decoded neighboring sub residual blocks. The residue prediction unit <b>1040</b> generates the prediction sub residual blocks of the current sub residual blocks in the same manner as the residue prediction unit <b>130</b> illustrated in <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0097" num="0096">The first addition unit <b>1050</b> restores the sub residual blocks by adding difference sub residual blocks of the current sub residual blocks which are composed of the difference residues output by the inverse transformation unit <b>1030</b> and the prediction sub residual blocks.</p>
<p id="p-0098" num="0097">The prediction unit <b>1070</b> generates a prediction block by performing inter prediction or intra prediction in accordance with a prediction mode of the current block.</p>
<p id="p-0099" num="0098">The second addition unit <b>1060</b> restores the current block by adding the prediction block generated by the prediction unit <b>1070</b> and the sub residual blocks restored by the first addition unit <b>1050</b>.</p>
<p id="p-0100" num="0099"><figref idref="DRAWINGS">FIG. 11</figref> is a flowchart illustrating a method of image decoding, according to an exemplary embodiment of the present invention.</p>
<p id="p-0101" num="0100">Referring to <figref idref="DRAWINGS">FIG. 11</figref>, in operation <b>1110</b>, a division mode of a current residual block to be decoded is determined by using information on a division mode of the residual block which is included in a received bitstream.</p>
<p id="p-0102" num="0101">In operation <b>1120</b>, prediction sub residual blocks of a plurality of sub residual blocks of the residual block are generated by using residues of neighboring sub residual blocks previously decoded in accordance with the determined division mode.</p>
<p id="p-0103" num="0102">In operation <b>1130</b>, difference sub residual blocks that are differences between the prediction sub residual blocks and the sub residual blocks and that are included in the bitstream, are restored.</p>
<p id="p-0104" num="0103">In operation <b>1140</b>, the sub residual blocks are restored by adding the prediction sub residual blocks and the difference sub residual blocks. An image is restored by adding the restored sub residual blocks and a prediction block generated by performing inter or intra prediction.</p>
<p id="p-0105" num="0104"><figref idref="DRAWINGS">FIG. 12</figref> is a block diagram illustrating an apparatus <b>1200</b> for image decoding, according to another exemplary embodiment of the present invention.</p>
<p id="p-0106" num="0105">Referring to <figref idref="DRAWINGS">FIG. 12</figref>, the apparatus <b>1200</b> includes a restoration unit <b>1210</b>, a prediction unit <b>1220</b> and an addition unit <b>1230</b>. The restoration unit <b>1210</b> restores residues that are differences between prediction values and original values of pixel lines and are included in a received bitstream, and includes an entropy decoding unit <b>1211</b>, an inverse quantization unit <b>1212</b> and an inverse transformation unit <b>1213</b>.</p>
<p id="p-0107" num="0106">The prediction unit <b>1220</b> predicts pixel values of a horizontal or vertical current pixel line to be decoded in a predetermined order by using a corresponding previously decoded pixel line.</p>
<p id="p-0108" num="0107">The addition unit <b>1230</b> decodes the current pixel line by adding the prediction values of the current pixel line and the restored residues. By repeating the above-described procedure, all pixels included in an image block may be decoded.</p>
<p id="p-0109" num="0108"><figref idref="DRAWINGS">FIG. 13</figref> is a flowchart illustrating a method of image decoding, according to another exemplary embodiment of the present invention.</p>
<p id="p-0110" num="0109">Referring to <figref idref="DRAWINGS">FIG. 13</figref>, in operation <b>1310</b>, residues that are differences between prediction values and original values of horizontal or vertical pixel lines and that are included in a received bitstream, are restored.</p>
<p id="p-0111" num="0110">In operation <b>1320</b>, pixel values of each pixel line to be decoded are predicted by using pixel values of a previous pixel line decoded in a predetermined order.</p>
<p id="p-0112" num="0111">In operation <b>1330</b>, pixels of the current pixel lines are decoded by adding the predicted pixel values of the pixel lines and the restored residues.</p>
<p id="p-0113" num="0112">The invention can also be embodied as computer readable codes on a computer readable recording medium. The computer readable recording medium is any data storage device that can store data which can be thereafter read by a computer system. Examples of the computer readable recording medium include read-only memory (ROM), random-access memory (RAM), CD-ROMs, magnetic tapes, floppy disks, and optical data storage devices. The computer readable recording medium can also be distributed over network coupled computer systems so that the computer readable code is stored and executed in a distributed fashion.</p>
<p id="p-0114" num="0113">As described above, according to the exemplary embodiments of the present invention, if horizontal or vertical correlations exist between pixels in an input image block, prediction efficiency and compression efficiency may be improved by performing prediction and one-dimensional transformation in lines in consideration of the correlations.</p>
<p id="p-0115" num="0114">While the present invention has been particularly shown and described with reference to exemplary embodiments thereof, it will be understood by those of ordinary skill in the art that various changes in form and details may be made therein without departing from the spirit and scope of the invention as defined by the appended claims. The exemplary embodiments should be considered in a descriptive sense only and not for purposes of limitation. Therefore, the scope of the invention is defined not by the detailed description of the invention but by the appended claims, and all differences within the scope will be construed as being included in the present invention.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08625916-20140107-M00001.NB">
<img id="EMI-M00001" he="15.16mm" wi="76.20mm" file="US08625916-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US08625916-20140107-M00002.NB">
<img id="EMI-M00002" he="15.16mm" wi="76.20mm" file="US08625916-20140107-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00003" nb-file="US08625916-20140107-M00003.NB">
<img id="EMI-M00003" he="15.16mm" wi="76.20mm" file="US08625916-20140107-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method of image encoding, the method comprising:
<claim-text>dividing an input image into a plurality of image blocks;</claim-text>
<claim-text>generating prediction values of pixels of each image block in horizontal or vertical lines;</claim-text>
<claim-text>generating residues that are differences between original values and the prediction values of the pixels, in lines; and</claim-text>
<claim-text>performing a one-dimensional transformation on the residues in lines,</claim-text>
<claim-text>wherein the generating prediction values of pixels of each image block in horizontal or vertical lines comprises:
<claim-text>predicting values of pixels in a first line of a current image block by extending values of a previously restored neighboring block; and</claim-text>
<claim-text>predicting values of pixels in other lines of the current image block by extending values of previously predicted lines of the current image block according to a predetermined process order.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the generating of the prediction values comprises predicting pixel values of each pixel line by using a previous pixel line transformed, quantized, inverse quantized, and inverse transformed in a predetermined order.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. An apparatus for image encoding, the apparatus comprising:
<claim-text>a processor and a memory comprising:</claim-text>
<claim-text>a prediction unit which divides an input image into a plurality of image blocks and generates prediction values of pixels of each image block in horizontal or vertical pixel lines;</claim-text>
<claim-text>a subtraction unit which generates residues that are differences between original values and the prediction values of the pixels, in lines; and</claim-text>
<claim-text>a transformation unit which performs a one-dimensional transformation on the residues in lines,</claim-text>
<claim-text>wherein the prediction unit predicts values of pixels in a first line of a current image block by extending values of a previously restored neighboring block; and values of pixels in other lines of the current image block by extending values of previously predicted lines of the current image block according to a predetermined process order.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The apparatus of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the prediction unit predicts pixel values of each pixel line by using a previous pixel line transformed, quantized, inverse quantized, and inverse transformed in a predetermined order.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. A method of image decoding, the method comprising:
<claim-text>restoring residues that are differences between prediction values and original values of horizontal or vertical pixel lines and are included in a received bitstream;</claim-text>
<claim-text>predicting pixel values of each pixel line to be decoded by using pixel values of a previous pixel line decoded in a predetermined order; and</claim-text>
<claim-text>decoding pixels of the pixel lines by adding the predicted pixel values of the pixel lines and the restored residues,</claim-text>
<claim-text>wherein the predicting pixel values of each pixel line to be decoded by using pixel values of a previous pixel line decoded in a predetermined order comprises:
<claim-text>predicting values of pixels in a first line of a current image block by extending values of a previously restored neighboring block; and</claim-text>
<claim-text>predicting values of pixels in other lines of the current image block by extending values of previously predicted lines of the current image block according to the predetermined process order.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. An apparatus for image decoding, the apparatus comprising:
<claim-text>a processor and a memory comprising: a prediction unit which predicts pixel values of horizontal or vertical pixel lines to be decoded by using previous pixel lines in vertical or horizontal lines in a predetermined order;</claim-text>
<claim-text>a restoration unit which restores residues that are differences between prediction values of the pixel lines and original pixel values of the pixel lines and are included in a received bitstream; and</claim-text>
<claim-text>an addition unit which decodes pixels of the pixel lines by adding the predicted pixel values of the pixel lines and the restored residues,</claim-text>
<claim-text>wherein the prediction unit predicts values of pixels in a first line of a current image block by extending values of a previously restored neighboring block; and values of pixels in other lines of the current image block by extending values of previously predicted lines of the current image block according to a predetermined process order. </claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
