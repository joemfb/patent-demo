<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626322-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626322</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12497534</doc-number>
<date>20090702</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<us-term-of-grant>
<us-term-extension>944</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>17</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>700 94</main-classification>
<further-classification>381 61</further-classification>
<further-classification>381 62</further-classification>
<further-classification>381 63</further-classification>
<further-classification>381 98</further-classification>
<further-classification>381 99</further-classification>
<further-classification>381100</further-classification>
<further-classification>381101</further-classification>
<further-classification>381102</further-classification>
<further-classification>381103</further-classification>
<further-classification>381104</further-classification>
<further-classification>381105</further-classification>
<further-classification>381106</further-classification>
<further-classification>381107</further-classification>
<further-classification>381108</further-classification>
<further-classification>381109</further-classification>
<further-classification>715716</further-classification>
<further-classification>715717</further-classification>
<further-classification>715719</further-classification>
<further-classification>715720</further-classification>
<further-classification>715721</further-classification>
<further-classification>715722</further-classification>
<further-classification>715723</further-classification>
<further-classification>715724</further-classification>
<further-classification>715725</further-classification>
<further-classification>715726</further-classification>
<further-classification>715727</further-classification>
<further-classification>715730</further-classification>
<further-classification>715731</further-classification>
</classification-national>
<invention-title id="d2e53">Multimedia display based on audio and visual complexity</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5521841</doc-number>
<kind>A</kind>
<name>Arman et al.</name>
<date>19960500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5752029</doc-number>
<kind>A</kind>
<name>Wissner</name>
<date>19980500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5758093</doc-number>
<kind>A</kind>
<name>Boezeman et al.</name>
<date>19980500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5760767</doc-number>
<kind>A</kind>
<name>Shore et al.</name>
<date>19980600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5841438</doc-number>
<kind>A</kind>
<name>Cave</name>
<date>19981100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>5852435</doc-number>
<kind>A</kind>
<name>Vigneaux et al.</name>
<date>19981200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6031529</doc-number>
<kind>A</kind>
<name>Migos et al.</name>
<date>20000200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6067126</doc-number>
<kind>A</kind>
<name>Alexander</name>
<date>20000500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348738</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6204840</doc-number>
<kind>B1</kind>
<name>Petelycky et al.</name>
<date>20010300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6320598</doc-number>
<kind>B2</kind>
<name>Davis et al.</name>
<date>20011100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345648</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6332147</doc-number>
<kind>B1</kind>
<name>Moran et al.</name>
<date>20011200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6369835</doc-number>
<kind>B1</kind>
<name>Lin</name>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>6463444</doc-number>
<kind>B1</kind>
<name>Jain et al.</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>6580438</doc-number>
<kind>B1</kind>
<name>Ichimura et al.</name>
<date>20030600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>RE38401</doc-number>
<kind>E</kind>
<name>Goldberg et al.</name>
<date>20040100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>6744974</doc-number>
<kind>B2</kind>
<name>Neuman</name>
<date>20040600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>386 96</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>RE38609</doc-number>
<kind>E</kind>
<name>Chen et al.</name>
<date>20041000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>6807361</doc-number>
<kind>B1</kind>
<name>Girgensohn et al.</name>
<date>20041000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>6928613</doc-number>
<kind>B1</kind>
<name>Ishii et al.</name>
<date>20050800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>7030872</doc-number>
<kind>B2</kind>
<name>Tazaki</name>
<date>20060400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>7082572</doc-number>
<kind>B2</kind>
<name>Pea et al.</name>
<date>20060700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>7085995</doc-number>
<kind>B2</kind>
<name>Fukuda et al.</name>
<date>20060800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>7143362</doc-number>
<kind>B2</kind>
<name>Dieberger et al.</name>
<date>20061100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>7149974</doc-number>
<kind>B2</kind>
<name>Girgensohn et al.</name>
<date>20061200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>7213051</doc-number>
<kind>B2</kind>
<name>Zhu</name>
<date>20070500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>7352952</doc-number>
<kind>B2</kind>
<name>Herberger et al.</name>
<date>20080400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>7469054</doc-number>
<kind>B2</kind>
<name>Aratani et al.</name>
<date>20081200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>7512886</doc-number>
<kind>B1</kind>
<name>Herberger et al.</name>
<date>20090300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715723</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>7528315</doc-number>
<kind>B2</kind>
<name>Goodwin</name>
<date>20090500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification> 84611</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>7669132</doc-number>
<kind>B2</kind>
<name>Widdowson</name>
<date>20100200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>US</country>
<doc-number>7890867</doc-number>
<kind>B1</kind>
<name>Margulis</name>
<date>20110200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>US</country>
<doc-number>7979801</doc-number>
<kind>B2</kind>
<name>Krishnaswamy et al.</name>
<date>20110700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00033">
<document-id>
<country>US</country>
<doc-number>2002/0143961</doc-number>
<kind>A1</kind>
<name>Siegel et al.</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709229</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00034">
<document-id>
<country>US</country>
<doc-number>2002/0154140</doc-number>
<kind>A1</kind>
<name>Tazaki</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00035">
<document-id>
<country>US</country>
<doc-number>2002/0154158</doc-number>
<kind>A1</kind>
<name>Fukuda et al.</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00036">
<document-id>
<country>US</country>
<doc-number>2003/0090506</doc-number>
<kind>A1</kind>
<name>Moore et al.</name>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00037">
<document-id>
<country>US</country>
<doc-number>2003/0091204</doc-number>
<kind>A1</kind>
<name>Gibson</name>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>381119</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00038">
<document-id>
<country>US</country>
<doc-number>2003/0184598</doc-number>
<kind>A1</kind>
<name>Graham</name>
<date>20031000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00039">
<document-id>
<country>US</country>
<doc-number>2003/0189588</doc-number>
<kind>A1</kind>
<name>Girgensohn et al.</name>
<date>20031000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00040">
<document-id>
<country>US</country>
<doc-number>2004/0027369</doc-number>
<kind>A1</kind>
<name>Kellock et al.</name>
<date>20040200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345716</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00041">
<document-id>
<country>US</country>
<doc-number>2004/0255307</doc-number>
<kind>A1</kind>
<name>Irudayaraj</name>
<date>20041200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>719328</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00042">
<document-id>
<country>US</country>
<doc-number>2005/0084232</doc-number>
<kind>A1</kind>
<name>Herberger et al.</name>
<date>20050400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00043">
<document-id>
<country>US</country>
<doc-number>2005/0275805</doc-number>
<kind>A1</kind>
<name>Lin et al.</name>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00044">
<document-id>
<country>US</country>
<doc-number>2006/0075348</doc-number>
<kind>A1</kind>
<name>Xu et al.</name>
<date>20060400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00045">
<document-id>
<country>US</country>
<doc-number>2006/0152678</doc-number>
<kind>A1</kind>
<name>Hung et al.</name>
<date>20060700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00046">
<document-id>
<country>US</country>
<doc-number>2007/0006073</doc-number>
<kind>A1</kind>
<name>Gerhard et al.</name>
<date>20070100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00047">
<document-id>
<country>US</country>
<doc-number>2007/0146388</doc-number>
<kind>A1</kind>
<name>Langmacher et al.</name>
<date>20070600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00048">
<document-id>
<country>US</country>
<doc-number>2007/0163427</doc-number>
<kind>A1</kind>
<name>Rigopulos et al.</name>
<date>20070700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification> 84609</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00049">
<document-id>
<country>US</country>
<doc-number>2007/0180979</doc-number>
<kind>A1</kind>
<name>Rosenberg</name>
<date>20070800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification> 84611</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00050">
<document-id>
<country>US</country>
<doc-number>2008/0077710</doc-number>
<kind>A1</kind>
<name>Kouvelas et al.</name>
<date>20080300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709250</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00051">
<document-id>
<country>US</country>
<doc-number>2008/0079690</doc-number>
<kind>A1</kind>
<name>Foxenland</name>
<date>20080400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345156</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00052">
<document-id>
<country>US</country>
<doc-number>2008/0098032</doc-number>
<kind>A1</kind>
<name>Wu et al.</name>
<date>20080400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00053">
<document-id>
<country>US</country>
<doc-number>2008/0162228</doc-number>
<kind>A1</kind>
<name>Mechbach et al.</name>
<date>20080700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00054">
<document-id>
<country>US</country>
<doc-number>2008/0183751</doc-number>
<kind>A1</kind>
<name>Cazier et al.</name>
<date>20080700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>7071041</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00055">
<document-id>
<country>US</country>
<doc-number>2008/0215979</doc-number>
<kind>A1</kind>
<name>Clifton et al.</name>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00056">
<document-id>
<country>US</country>
<doc-number>2009/0187826</doc-number>
<kind>A1</kind>
<name>Heimbold et al.</name>
<date>20090700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00057">
<document-id>
<country>US</country>
<doc-number>2010/0085379</doc-number>
<kind>A1</kind>
<name>Hishikawa et al.</name>
<date>20100400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345619</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00058">
<document-id>
<country>US</country>
<doc-number>2010/0169777</doc-number>
<kind>A1</kind>
<name>Weber et al.</name>
<date>20100700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00059">
<document-id>
<country>US</country>
<doc-number>2010/0169784</doc-number>
<kind>A1</kind>
<name>Weber et al.</name>
<date>20100700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00060">
<othercit>&#x201c;U.S. Appl. No. 12/497,462, Final Office Action mailed Jun. 12, 2012&#x201d;, 10 pgs.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00061">
<othercit>&#x201c;U.S. Appl. No. 12/497,462, Non Final Office Action mailed Feb. 15, 2013&#x201d;, 10 pgs.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00062">
<othercit>&#x201c;U.S. Appl. No. 12/497,462, Non Final Office Action mailed Dec. 28, 2011&#x201d;, 10 pgs.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00063">
<othercit>&#x201c;U.S. Appl. No. 12/497,462, Response filed Mar. 22, 2012 to Non Final Office Action mailed Dec. 28, 2011&#x201d;, 11 pgs.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00064">
<othercit>&#x201c;U.S. Appl. No. 12/497,462, Response filed Nov. 13, 2012 to Final Office Action mailed Jun. 12, 2012&#x201d;, 8 pgs.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00065">
<othercit>&#x201c;U.S. Appl. No. 12/497,541, Final Office Action mailed Jun. 13, 2012&#x201d;, 11 pgs.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00066">
<othercit>&#x201c;U.S. Appl. No. 12/497,541, Non Final Office Action mailed Jan. 23, 2013&#x201d;, 13 pgs.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00067">
<othercit>&#x201c;U.S. Appl. No. 12/497,541, Non Final Office Action mailed Dec. 28, 2011&#x201d;, 11 pgs.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00068">
<othercit>&#x201c;U.S. Appl. No. 12/497,541, Response filed Mar. 22, 2012 to Non Final Office Action mailed Dec. 28, 2011&#x201d;, 13 pgs.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00069">
<othercit>&#x201c;U.S. Appl. No. 12/497,541, Response filed Oct. 15, 2012 to Final Office Action mailed Jun. 13, 2012&#x201d;, 10 pgs.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00070">
<othercit>&#x201c;U.S. Appl. No. 12/499,794, Non Final Office Action mailed Oct. 25, 2012&#x201d;, 12 pgs.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00071">
<othercit>&#x201c;U.S. Appl. No. 12/499,794, Response filed Mar. 22, 2013&#x201d;, 9 pgs.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>18</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>700 94</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>12</number-of-drawing-sheets>
<number-of-figures>12</number-of-figures>
</figures>
<us-related-documents>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>61193852</doc-number>
<date>20081230</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20100168881</doc-number>
<kind>A1</kind>
<date>20100701</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Weber</last-name>
<first-name>Ralf</first-name>
<address>
<city>Cupertino</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Vergnaud</last-name>
<first-name>Guillaume</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Weber</last-name>
<first-name>Ralf</first-name>
<address>
<city>Cupertino</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Vergnaud</last-name>
<first-name>Guillaume</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Schwegman Lundberg &#x26; Woessner, P.A.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Apple Inc.</orgname>
<role>02</role>
<address>
<city>Cupertino</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>McCord</last-name>
<first-name>Paul</first-name>
<department>2656</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A computer-implemented method and system are provided for profiling or analyzing audio data based on an audio parameter. The audio data is divided into audio data into segments. Each segment is characterized by the audio parameter in a defined range. Effects are selected according to the segments meeting criteria for the audio parameter of the effects. A media presentation is authored or generated using the selected effects.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="114.72mm" wi="156.97mm" file="US08626322-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="211.92mm" wi="146.56mm" file="US08626322-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="259.76mm" wi="171.79mm" file="US08626322-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="256.37mm" wi="164.00mm" file="US08626322-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="258.83mm" wi="180.42mm" file="US08626322-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="259.25mm" wi="178.56mm" file="US08626322-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="258.83mm" wi="177.55mm" file="US08626322-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="262.21mm" wi="178.56mm" file="US08626322-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="194.99mm" wi="167.39mm" file="US08626322-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="259.33mm" wi="179.49mm" file="US08626322-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="108.29mm" wi="79.33mm" file="US08626322-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="130.73mm" wi="158.75mm" file="US08626322-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="132.08mm" wi="84.16mm" file="US08626322-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<p id="p-0002" num="0001">This application claims the benefit and priority of the U.S. Provisional Patent Application No. 61/193,852 filed on Dec. 30, 2008, which is hereby incorporated by reference.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">FIELD OF INVENTION</heading>
<p id="p-0003" num="0002">The present invention relates generally to the field of authoring media presentations and, in particular, to authoring media presentations using profiled audio data.</p>
<heading id="h-0002" level="1">BACKGROUND OF INVENTION</heading>
<p id="p-0004" num="0003">Current media presentation applications offer features for creating slides and manually customizing the ways in which a set of slides, i.e., a slideshow, is played. Such applications also offer features for attaching themes to slideshows, where such themes may affect the appearance and general behavior of the slideshows when played. In addition, such applications further offer features such as customizing slide colors, customizing transition behavior, customizing transition delay, and manually adding clip art/image/audio/video files to one or more slides in a slideshow. These applications also permit basic sequential transition, forward or backward, from one slide to another in a slideshow containing more than one slide. A user may customize the time that one slide should be viewed prior to the application invoking a transition to another slide, which may further have a custom viewing time associated with it, as well.</p>
<p id="p-0005" num="0004">However, current media presentation applications do not dynamically profile audio data, such as a slideshow soundtrack, based on various audio parameters, including beats per minute, rhythmic strength, harmonic complexity, and/or square root of the arithmetic mean of the square of density variations (RMS) strength. In addition, current media presentation applications do not utilize the profiled audio data to select appropriate effects, transitions, or filters and assemble them in useful ways to author a media presentation. Current media presentation applications also do not set effect durations, in/out points, and transitions in-sync with audio alone or the audio of a video.</p>
<p id="p-0006" num="0005">Moreover, current media presentations applications do not author media presentations by defining a layer, where the layer comprises one or more effects, associating media content with the layer, aggregating the layer with one or more other layers, and assembling the aggregated layers.</p>
<p id="p-0007" num="0006">Finally, current media presentation applications do not provide automatic, as well as user-defined, authoring, rendering, exporting, and sharing media presentations/slideshows in an easily integrated platform.</p>
<heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0008" num="0007">Accordingly, the present invention is directed to a system and method for authoring media presentations that substantially obviates one or more problems due to limitations and disadvantages of the related art.</p>
<p id="p-0009" num="0008">An object of the present invention is to provide systems and methods for profiling audio data based on various audio parameters.</p>
<p id="p-0010" num="0009">Another object of the present invention is to provide systems and methods for profiling audio data based on beats per minute, rhythmic strength, harmonic complexity, and/or square root of the arithmetic mean of the square of density variations (RMS) strength.</p>
<p id="p-0011" num="0010">Another object of the present invention is to provide systems and methods for automatically authoring a media presentation using the profiled audio data.</p>
<p id="p-0012" num="0011">Another object of the present invention is to provide systems and methods for selecting effects, transitions, or filters and assembling them in useful ways to author a media presentation.</p>
<p id="p-0013" num="0012">Yet another object of the present invention is provide systems and methods to adjust effect durations, in and out points, and transitions to be in-sync with audio alone or the audio of a video.</p>
<p id="p-0014" num="0013">Additional features and advantages of the invention will be set forth in the description which follows, and in part will be apparent from the description, or may be learned by practice of the invention. The objectives and other advantages of the invention will be realized and attained by the structure particularly pointed out in the written description and claims hereof as well as the appended drawings.</p>
<p id="p-0015" num="0014">To achieve these and other advantages and in accordance with the purpose of the present invention, as embodied and broadly described, a method comprises analyzing audio data based on an audio parameter, dividing the audio data into segments, wherein each segment is characterized by the audio parameter in a defined range, selecting effects according to the segments meeting criteria for the audio parameter of the effects, and generating a media presentation using the selected effects.</p>
<p id="p-0016" num="0015">In another aspect, a system comprises memory configured to store audio data and one or more processors configured to analyze the audio data based on an audio parameter, divide the audio data into segments, wherein each segment is characterized by the audio parameter in a defined range, select effects according to the segments meeting criteria for the audio parameter of the effects, and generate a media presentation using the selected effects.</p>
<p id="p-0017" num="0016">In another aspect, a computer-readable storage medium stores one or more programs configured for execution by a computer, the one or more programs comprising instructions to analyze audio data based on an audio parameter, divide the audio data into segments, wherein each segment is characterized by the audio parameter in a defined range, select effects according to the segments meeting criteria for the audio parameter of the effects, and generate a media presentation using the selected effects.</p>
<p id="p-0018" num="0017">In another aspect, a computer-implemented method comprises analyzing audio data based on an audio parameter, dividing the audio data into segments, wherein each segment is characterized by the audio parameter in a defined range, linking an audio requirements profile of each of one or more effects to an audio parameter, adjusting the one or more effects based on the audio parameter, generating a media presentation using the adjusted one or more effects.</p>
<p id="p-0019" num="0018">In another aspect, a system comprises memory configured to store audio data and one or more processors configured to analyze audio data based on an audio parameter, divide the audio data into segments, wherein each segment is characterized by the audio parameter in a defined range, link an audio requirements profile of each of one or more effects to an audio parameter, adjust the one or more effects based on the audio parameter, and generate a media presentation using the adjusted one or more effects.</p>
<p id="p-0020" num="0019">In yet another aspect, a computer-readable storage medium stores one or more programs configured for execution by a computer, the one or more programs comprising instructions to analyze audio data based on an audio parameter, divide the audio data into segments, wherein each segment is characterized by the audio parameter in a defined range, link an audio requirements profile of each of one or more effects to an audio parameter, adjust the one or more effects based on the audio parameter, and generate a slideshow/media presentation using the adjusted one or more effects.</p>
<p id="p-0021" num="0020">It is to be understood that both the foregoing general description and the following detailed description are exemplary and explanatory and are intended to provide further explanation of the invention as claimed.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0022" num="0021">The accompanying drawings, which are included to provide a further understanding of the invention and are incorporated in and constitute a part of the specification, illustrate embodiments of the invention and together with the description serve to explain the principles of the invention. In the drawings:</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 1</figref> illustrates an exemplary embodiment of an application in accordance with the present invention;</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 2</figref> illustrates features of an exemplary embodiment in accordance with the present invention;</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 2A</figref> illustrates features of an exemplary embodiment in accordance with the present invention;</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 3</figref> is a block diagram illustrating application features of an exemplary embodiment in accordance with the present invention;</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 3A</figref> is a block diagram illustrating framework features of an exemplary embodiment of in accordance with the present invention;</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 4</figref> illustrates an exemplary system implementing an application in accordance with the present invention;</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 5</figref> illustrates an exemplary implementation of an application in accordance with the present invention;</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 6</figref> illustrates an exemplary method in accordance with the present invention;</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 7</figref> illustrates an exemplary method in accordance with the present invention;</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 8</figref> illustrates an exemplary method in accordance with the present invention;</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 9</figref> illustrates an exemplary diagram in accordance with the present invention; and</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 10</figref> illustrates another exemplary method in accordance with the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0035" num="0034">Reference will now be made in detail to embodiments, examples of which are illustrated in the accompanying drawings. In the following detailed description, numerous non-limiting specific details are set forth in order to assist in understanding the subject matter presented herein. It will be apparent, however, to one of ordinary skill in the art that various alternatives may be used without departing from the scope of the present invention and the subject matter may be practiced without these specific details. For example, it will be apparent to one of ordinary skill in the art that the subject matter presented herein can be implemented on any type of standalone system or client-server compatible system containing any type of client, network, server, and database elements.</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 1</figref> illustrates an exemplary embodiment of an application in accordance with the present invention. The exemplary embodiment of an application <b>1000</b> comprises a document <b>1001</b>, a set of controls <b>1002</b> for controlling/manipulating the document <b>1001</b>, an edit layers and effect containers region <b>1003</b> (e.g., steps <b>6001</b>, <b>7001</b>), a background layer <b>1004</b> with effect containers and effects associated with the effect containers, a foreground layer <b>1005</b> with effect containers and effects associated with the effect containers, a text typewriter <b>1006</b> for associating text with one or more effect containers/effects/slides, a document length <b>1007</b> indicator, a select media content menu <b>1008</b> (e.g., steps <b>6002</b>, <b>7002</b>), a main effects library <b>1009</b>, a documents selection menu <b>1010</b> for selecting among available documents, one or more effects <b>1011</b> (which may be available according to a filter criteria) within the main effects library <b>1009</b>, a subset of the main effects library <b>1012</b>, a gap variable <b>1013</b> for separating an effect or effect container (with one or more effects and slides) from the next effect or effect container using a gap, a transition variable <b>1014</b> for separating an effect or effect container (with one or more effects and slides) from the next effect or effect container using a transition, and an effect style <b>1015</b> (also, name or property). The exemplary embodiment of an application <b>1000</b> illustrates a use of the application <b>1000</b> to create a document <b>1001</b> (e.g., steps <b>6004</b>, <b>7004</b>) using a background layer <b>1004</b>, a foreground layer <b>1005</b> (additional background/foreground layers may also be added to the document but may not be illustrated in <figref idref="DRAWINGS">FIG. 1</figref>) (e.g., steps <b>7008</b>, <b>7009</b>), customized gaps <b>1013</b> and/or transitions <b>1014</b> separating effect containers. Effect containers may comprise, for example, one or more effects from, for example, the main effects library <b>1009</b> or the subset of the main effects library <b>1012</b>; and, effects may further comprise one or more slides like, for example, images, movies, audio, text (e.g., a string with font information that may determine how text will look and feel), and other media content (e.g., steps <b>6002</b>, <b>7002</b>, <b>7006</b>, <b>7012</b>, <b>7013</b>). Effects may determine how the image and its surroundings will appear on a screen/display during play (e.g., an image may be displayed according to &#x201c;book theme,&#x201d; where the effect would be the feature/component for determining how the photos may be laid out or organized on-screen; an effect may store/pass information related to how a photo would bounce around on a screen; or, an effect may also store/pass information related to text, where the text may be added or associated with a slide, effect, layer, or document of a slideshow/media presentation). Further, effects may be filtered according to media content type using the select media content menu <b>1008</b>. For example, images, video, audio, text, and captions may be used to filter effects accordingly. Meaning, the relevant effects associated with each of the foregoing types of media content may be displayed accordingly upon invocation of the respective menu item from the select media content menu <b>1008</b>. Details regarding effects may be displayed in the effects style <b>1015</b> section, which may be positioned beneath each respective effect container, and corresponding to a presently active effect, in the effect containers region <b>1003</b>.</p>
<p id="p-0037" num="0036">In some embodiments, the exemplary embodiment of an application <b>1000</b>, and its features/components, may be implemented by one or more modules/engines (<figref idref="DRAWINGS">FIG. 3A</figref>, reference numerals <b>3020</b>-<b>23</b>) executed using an exemplary system <b>4000</b> (<figref idref="DRAWINGS">FIG. 4</figref>) with a central processing unit (CPU) <b>4001</b> (and, alternatively, multiple CPUs), memory <b>4002</b> for storing data (e.g., instructions from an operating system <b>4007</b> or one or more programs (e.g., <b>4008</b>, <b>4009</b>)) to be fetched by the CPU for execution, a display device <b>4003</b> for displaying the exemplary application <b>1000</b> using a graphics module to a display screen, a network interface card (NIC) <b>4004</b> for sending and receiving data over a wired or wireless communications network, local storage <b>4006</b> for storing media content and other data (e.g., an operating system <b>4007</b>, the exemplary embodiment of an application <b>1000</b>, other applications, etc.), and auxiliary device(s)/component(s) <b>4005</b> (e.g., TV (or, other display), portable storage, portable media player, etc.), which may all be connected via a bus for sending and receiving data according to a frequency (e.g., synchronous or asynchronous).</p>
<p id="p-0038" num="0037">In some embodiments, the features/components of the application <b>1000</b> may be described as follows. The document <b>1001</b> (also, <figref idref="DRAWINGS">FIG. 3</figref>, reference numeral <b>3001</b>) is the top level object of the media presentation/slideshow that may be created (e.g., steps <b>6004</b>, <b>7004</b>) using the exemplary application <b>1000</b>. The document is the object that may comprise: all of the custom/default layers <b>1004</b>, <b>1005</b> (also, <figref idref="DRAWINGS">FIG. 3</figref>, reference numeral <b>3002</b>) (e.g., steps <b>6003</b>, <b>7003</b>, <b>7010</b>), effect containers such as, for example, those within the effect containers region <b>1003</b> (also, <figref idref="DRAWINGS">FIG. 3</figref>, reference numeral <b>3003</b>); effects such as, for example, those within the effect containers (also, <figref idref="DRAWINGS">FIG. 3</figref>, reference numeral <b>3004</b>); gaps <b>1013</b> or transitions <b>1014</b> for separating or linking effects, respectively (also, <figref idref="DRAWINGS">FIG. 3</figref>, reference numeral <b>3012</b>); slides such as, for example, the images of <figref idref="DRAWINGS">FIG. 1</figref> or other media content as described above (also, <figref idref="DRAWINGS">FIG. 3</figref>, reference numeral <b>3005</b>, <b>3010</b>) (e.g., step <b>6002</b>, <b>7002</b>); frames <b>3006</b>; a document/layer/effect stack <b>3007</b>; a layer/effect/slide/filter stack <b>3011</b>; a playlist <b>3008</b>; an animation path <b>3014</b>; a song <b>3009</b>; a keyframe <b>3015</b> (which may, for example, be one dimensional (1D) <b>3016</b>, two dimensional (2D) <b>3017</b> or a vector (<b>3018</b>)); filters <b>3019</b>; a layer/effect container/effect/slide/filter stack <b>3013</b>; and, any other possible combination of the aforementioned. Moreover, a document may contain layers that may be stacked/placed one on top of another to provide the media presentation/slideshow with an added level of flexibility in what is available for actual display (e.g., steps <b>6003</b>, <b>7003</b>, <b>7010</b>). Accordingly, the application supports the presentation of less than all of the available layers. Stacking may involve a process, for example, of logically associating, or linking, layers. That is, a background layer <b>1004</b> may be considered the lowest level layer in a stack of layers, followed by a foreground layer <b>1005</b> and a plurality of other foreground layers, all of which would be logically associated according to their position from, for example, background layer <b>1004</b>, or from each other foreground layer. During display/play of a document such as, for example, document <b>1001</b>, the layers would be displayed/played according to their respective positions in the stack (logical associations). The next feature/component is the layers <b>1004</b> (background), <b>1005</b> (foreground) (also, <figref idref="DRAWINGS">FIG. 3</figref>, reference numeral <b>3002</b>) within a document <b>1001</b> (also, <figref idref="DRAWINGS">FIG. 3</figref>, reference numeral <b>3001</b>) (e.g., steps <b>6001</b>, <b>7001</b>). Each layer <b>1004</b>, <b>1005</b> of a stack of layers (e.g., aggregated layers; steps <b>6003</b>, <b>7003</b>) within a document can be positioned, sized, and rotated using the exemplary application <b>1000</b>. Further, each layer <b>1004</b>, <b>1005</b> may also have a custom audio file/track (or, alternatively, a set of audio files/tracks, or other media content) associated with it and other layers <b>1004</b>, <b>1005</b>, thus, providing a media presentation/slideshow with multiple audio files/tracks during presentation (e.g., steps <b>6002</b>, <b>7002</b>). Each layer <b>1004</b>, <b>1005</b> may also contain effect containers (like, for example, those illustrated in the effect containers region <b>1003</b>) (e.g., steps <b>6002</b>, <b>7002</b>), which may be linked together in a layer using transitions <b>1014</b> (also, <figref idref="DRAWINGS">FIG. 3</figref>, reference numeral <b>3012</b>) or separated from one another using gaps <b>1013</b> (or, alternatively, some other effect separation variable like, for example, random separation/transition, or a combination of gaps and transitions, etc.) (e.g., <b>7005</b>). Transitions <b>1014</b>, which through visual action/expression may create the appearance that two effect containers are linked together, may be able to provide a rather &#x201c;fluid&#x201d; (or, alternatively, a &#x201c;non-fluid&#x201d;) experience between effect containers when presenting a media presentation/slideshow. For example, transitions may be the visual action/expression of a page flipping, a slide dissolving, a slide being pushed along in any direction, a cube breaking apart (or, being assembled), a page rolling for the purpose of unveiling/hiding contents, a puzzle being assembled (or, disassembled), or any other type of visual action/expression applied to an effect container or slide and capable of being rendered on a display device. Slides in the exemplary application may be the actual image, movie, text, or other media content that may be within an effect, which may be within an effect container (e.g., steps <b>6002</b>, <b>7002</b>). Slides may have frames applied as an added layer (e.g., on top), where a frame may be a visual element/expression such as, for example, making an image appear as if it was taken using an instant photo camera (e.g., Polaroid&#xae;), is part of a filmstrip, has a solid/dashed/shadowed/other border surrounding it, or other type of frame-related visual element/expression. Further, each slide may have an animation path <b>3014</b> that may determine which part of a slide image, movie, text, or other media content, is actually displayed/played; similarly, an animation path <b>3014</b> associated with the slide may cause a panning/zooming effect to be executed on the image, movie, text, or other media content, where the panning/zooming may occur within the effect of the slide. As applied to a layer, a user may also customize an animation path <b>3014</b> via the exemplary application <b>1000</b> to, for example, smoothly transition a layer's rotation from around zero (0) degrees all the way to three hundred sixty (360) degrees, over a default or custom period of time (e.g., steps <b>6002</b>, <b>7002</b>). In some embodiments, transitions <b>1014</b> may have durations associated with them to determine how long the transitions are played. The transition duration may be subtracted directly from the total duration of the effect containers separated/divided by the transition. For example, when transitioning from an effect container with a three (3) second duration to another effect container with a three (3) second duration, that is, having a six (6) second total duration, using a transition with a one (1) second duration, the effect containers may only be played for a total of five (5) seconds (i.e., the total six (6) second duration of the effect containers minus the one (1) second transition display/play duration leaves five (5) seconds of display/play duration for the effect containers).</p>
<p id="p-0039" num="0038">In some embodiments, effect containers may be able to determine the order that images (or, alternatively, other media content) associated with a layer (e.g., steps <b>6002</b>, <b>7002</b>) are presented during a media presentation/slideshow. Such a determination may be based according to characteristics associated with the images (or, alternatively, other media content) (e.g., steps <b>6004</b>, <b>7004</b>). The characteristics may comprise a resolution, size, quality indicator, dots-per-inch, frames per second, window size, bit error rate (BER), compression type, or some other media content characteristic. The exemplary application <b>1000</b> may execute this process of assembling the layers (e.g., steps <b>6004</b>, <b>7004</b>) either manually or according to algorithms processing the characteristics and other layer-related data (described above). Further with respect to effect containers (e.g., a container or group of effects), multiple effects may be transitioned as one set into the next effect container. For example, effect containers are necessary in order for different text to be displayed on top of different effects. In some embodiments, from an implementation viewpoint, the effect containers permit the logical/physical grouping of different effects and link each of the effects to their respective different text, which is to be displayed on top of each respective effect. Each effect container may, for example, further contain a variable for storing a specific duration for determining how long each of the effects associated with an effect container (or, alternatively, &#x201c;within&#x201d; the effect container) are displayed/played.</p>
<p id="p-0040" num="0039">In some embodiments, a keyframe <b>3015</b> (which may, for example, be one dimensional (1D) <b>3016</b>, two dimensional (2D) <b>3017</b> or a vector (<b>3018</b>)), may be used by an animation path <b>3014</b> to guide or instruct the rate at which animation path <b>3014</b> should operate. Meaning, the higher the value of a keyframe <b>3015</b>, the increased rate the animation path <b>3014</b> may operate (e.g., a faster pan-zoom effect or a faster layer rotation), and the lower the value of a keyframe <b>3015</b>, the lower rate the animation path <b>3014</b> may operate at (e.g., a slower pan-zoom effect or a slower layer rotation). A 1D <b>3016</b> keyframe may be a keyframe that animates a property that has one value like, for example, a rotation angle. A 2D <b>3017</b> keyframe may be a keyframe that animates a property that has more than one value like, for example, a position (x-axis point, y-axis point) or a size (width/length, height). And, a vector <b>3018</b> keyframe may be a keyframe that animates a property that has more than two values like, for example, colors that manipulate the different values of their constituent color components (e.g., red, green, blue, alpha).</p>
<p id="p-0041" num="0040">In some embodiments, filters <b>3019</b> operate as visual elements that are applied to a layer, effect container, effect, or slide. A filter <b>3019</b> may be, for example, a shadow, blurred image, or some other compatible visual element capable of being applied to a layer, effect container, effect, or slide (e.g., steps <b>6002</b>, <b>7002</b>).</p>
<p id="p-0042" num="0041">In some embodiments, a playlist <b>3008</b> associated with a document <b>1001</b> may contain a list of songs (e.g., steps <b>6002</b>, <b>7002</b>). The playlist <b>3008</b> may organize songs such that they are played in a specific order, determined manually by a user of the exemplary application <b>1000</b>, or automatically through the exemplary application <b>1000</b>. An automatic playlist may be created according to song genre, file characteristics (e.g., type, size, date, etc.), or according to the feature for dynamically profiling a slideshow soundtrack based on various criteria like beats per minute (BPM), rhythmic strength (RS), harmonic complexity (HC), and/or root mean square density (RMS or RMS strength). The songs (e.g., a reference to a playlist) may be stored in digital format in local storage <b>4006</b> or on an auxiliary device/component <b>4005</b> that communicates with the system <b>4000</b> through a communications protocol or standard. The songs may be stored in a single file (or, other logical/physical data aggregator) or many files. In addition to songs, a playlist <b>3008</b> may contain other compatible media content like videos with audio content (which, for example, may be parsed from the video file into an individual song/audio file, or playlist). To associate a playlist, song/audio file, or any compatible media content with a document <b>1001</b>, the user may select it/them from the select media content <b>1008</b> menu and drag the respective playlist, song/audio file, or other compatible media content, via the exemplary application <b>1000</b>, into the effect containers region <b>1003</b> (see, for example, the reference to &#x201c;Drag Audio Here&#x201d; in the exemplary application <b>1000</b>) (e.g., steps <b>6002</b>, <b>7002</b>). Songs may be played in the background while a document is being displayed/played, or they may, alternatively, be associated with foreground layers or effects that may be organized on top of another, thus, enabling the songs to be switched in coordination with the various switching (e.g., via gaps or transitions) from one layer or effect to another (e.g., steps <b>6004</b>, <b>7004</b>). Further, songs may, according to a default setting, start and stop playing based on the start and stop times that may be given from a media player or media management application. The user of the exemplary application <b>1000</b> may, however, define a custom start or stop time via a song (or, playlist) menu option of the application <b>1000</b>.</p>
<p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. 2</figref> illustrates features of an exemplary embodiment in accordance with the present invention. As illustrated, the exemplary embodiment of an add effects container region <b>2000</b> (similar to that of <b>1003</b>; <figref idref="DRAWINGS">FIG. 1</figref>) contains three layers, the first is a background layer <b>2001</b>, and the second and third are foreground layers <b>2002</b> and <b>2003</b> (e.g., steps <b>6001</b>, <b>7001</b>, <b>6002</b>, <b>7002</b>, <b>7008</b>, <b>7009</b>). Effect containers are illustrated as 2004, 2005, and 2006 in the background layer <b>2001</b>. The foreground layers <b>2002</b> and <b>2003</b> also contain effect containers. Each effect container has effects with slides/media content within, such as illustrated by the slides (e.g., images) <b>2011</b> and <b>2013</b> in the second foreground layer <b>2003</b>. Moreover, gaps <b>2007</b>, <b>2008</b>, <b>2009</b> separate effect containers in foreground layers <b>2002</b> and <b>2003</b>. Also, transition <b>2010</b> separates (or, &#x201c;link&#x201d;) effect containers in the foreground layer <b>2003</b>. Further, an effects style <b>2012</b> is illustrated for the second effect container of the second foreground layer <b>2003</b> (e.g., step <b>7007</b>). The effects style may display one or more styles or properties such as, for example, a media presentation order, a thumbnail, a layout, a position, a size, a zPosition (e.g., the position in a three-dimensional (x, y, z) orientation), a base period, an effect presets, an effect settings overwrite, a matching layer duration, a recommended effect duration, a transition preset, a transition settings overwrite, a recommended transition duration, a filter preset, a filter preset criteria, a filter likelihood, a gap likelihood, a layer importance, a slide filter preset criteria, a slide frames criteria, an automatic filter likelihood, and a support per-slide customization (e.g., step <b>7014</b>). Other styles or properties may also be displayed at <b>2012</b> or in another section of the graphical user interface of the exemplary application <b>1000</b> or at the container region <b>2000</b> like, for example, a background color and an automatic filter mode (e.g., step <b>7015</b>). The styles or properties may apply to the effects, effect containers, layers, and/or document, and may further be customized for each of the foregoing or inherited from each other, whether set by default or is customized (e.g., step <b>7007</b>).</p>
<p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. 2A</figref> illustrates features of an exemplary embodiment in accordance with the present invention. The exemplary embodiment <b>2000</b>A illustrates an effect container <b>2020</b> with a phaseIn effect <b>2021</b>, a main effect <b>2022</b>, and a phaseOut effect <b>2023</b>. Also, the blurred image (or, movie or other media content) shown behind the &#x201c;main&#x201d; text illustrates an exemplary instance of a blurred effect during the main effect <b>2022</b> phase of the effect container <b>2020</b>. The phaseIn effect <b>2021</b> (e.g., like a fade-in) may be used, for example, to build a single/multi-slide layout, where as the phaseOut effect <b>2023</b> (e.g., like a fade-out) may be used, for example, to tear down/away or remove a single/multi-slide layout. Thus, the phaseIn <b>2021</b>, main <b>2022</b>, and phaseOut <b>2023</b> effects may be applied to a single slide or to multiple slides. Furthermore, there may be a time associated with each phaseIn effect <b>2021</b>, main effect <b>20222</b>, and phaseOut effect <b>2023</b>. The time spent on each slide of a multi-slide effect may be equally divided among the individual slides (e.g., total effect time divided by the total number of slide, and adjusted accordingly) or apportioned in a custom manner to each individual slide.</p>
<p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. 3</figref> is a block diagram illustrating framework features of an exemplary embodiment in accordance with the present invention. In some embodiments, the exemplary application <b>1000</b> executing on the exemplary system <b>4000</b> may cause the CPU <b>4001</b> to execute instructions for creating an electronic structure (e.g., <b>3000</b>) for storage in memory <b>4002</b>, local storage <b>4006</b>, or on an auxiliary device/component <b>4005</b>, such instructions may comprise: creating a document (e.g., <b>3001</b>); associating one or more layers (e.g., <b>3002</b>) to the document, wherein the layers (e.g., <b>3002</b>) are organized within the document (e.g., <b>3002</b>); associating one or more effect containers (e.g., <b>3003</b>) with the layers, wherein the effect containers (e.g., <b>3003</b>) are linked and are organized within the layers (e.g., <b>3003</b>); associating one or more effects (e.g., <b>3004</b>) with the effect containers (e.g., <b>3004</b>); and assembling the effects (e.g., <b>3004</b>), effect containers (e.g., <b>3003</b>), and layers (e.g., <b>3002</b>) logically within the document. The application features <b>3000</b>-<b>3019</b> are referred to and described in detail herein, and in view of the exemplary application <b>1000</b>, which may be executed, for example, on the exemplary system <b>4000</b>.</p>
<p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. 3A</figref> is a block diagram illustrating framework features of an exemplary embodiment in accordance with the present invention. The framework features <b>3000</b>A may comprise framework module units (or, modules) such as, for example, a core <b>3020</b>, a producer <b>3021</b>, a renderer <b>3022</b>, and an exporter <b>3023</b>. The features <b>3000</b>A may implement the structure/architecture of the exemplary application <b>1000</b>, and may be executed, for example, using a system like that illustrated in <figref idref="DRAWINGS">FIGS. 4-5</figref>.</p>
<p id="p-0047" num="0046">In some embodiments, the core <b>3020</b> module may be considered the low-level data structure module and it may, for example, perform routines for representing how a slideshow/media presentation document is constructed, and contain the necessary information for accurately representing a slideshow/media presentation document according to features, many of which are described herein (e.g., steps <b>6001</b>-<b>6003</b>, <b>7001</b>-<b>7003</b>). Some of those features may include, for example, features related to timing (e.g., gaps <b>1013</b>, transitions <b>1014</b>), positioning (e.g., background layer <b>1004</b>, foreground layer <b>1005</b>, effects of effect containers <b>2004</b>-<b>2006</b>, slides <b>2011</b>, filters <b>3019</b>, text <b>3010</b>), sizing (e.g., keyframe <b>3015</b>, animation path <b>3014</b>, as well as their interaction), and files (e.g., songs <b>3008</b>, playlists <b>3009</b>).</p>
<p id="p-0048" num="0047">In some embodiments, the producer <b>3021</b> may be considered the module for creating how a slideshow will look and feel (e.g., steps <b>6002</b>-<b>6003</b>, <b>7002</b>-<b>7003</b>), performing several analyses related to media content (e.g., images, audio, video of layers, effect containers, effects, and slides) (e.g., step <b>7016</b>), and automatically assembling slideshows/media presentations according to data that may result from the analyses (e.g., steps <b>6004</b>, <b>7004</b>, <b>7011</b>). The several analyses (e.g., step <b>7016</b>) may include analysis of characteristics related to layers, effect containers, effects, and slides. Such characteristics may include, for example, layer type (e.g., background <b>1004</b>, foreground <b>1005</b>), layer number (e.g., position in relation to the background-most layer <b>1004</b>), number of effect containers, length of gaps <b>1013</b> and transitions <b>1014</b>, type of transitions <b>1014</b>, type of effects, number of effects, number of slides, type of slides, document length <b>1004</b>, user preferences (e.g., for ordering layers, effect containers, effects, slides), audio analyses, video analyses, or other similar characteristics. After performing the several analyses using, for example, the producer <b>3021</b>, the resulting data from the several analyses may be processed by the producer <b>3021</b>, the core <b>3020</b>, the renderer <b>3022</b>, the exporter <b>3023</b>, or other module (e.g., step <b>7017</b>). The producer <b>3021</b> may, for example, interface with and utilize the application programming interfaces (API) of frameworks like, for example, browsers or QuickTime&#xae; to gather such information as thumbnail data and resolutions for images, as well as audio or video durations or other characteristics. The gathered information may then be processed by the producer <b>3021</b> in accordance with one or more general/specific algorithms (or, other analytical methods) and then used by the producer <b>3021</b> (or, other module with which the producer <b>3021</b> may call), for example, to automatically assemble a slideshow or media presentation document (e.g., <b>7011</b>). The producer <b>3021</b> may further, for example, assemble a document via core <b>3020</b> for play/display using the features of renderer <b>3022</b>, by accessing photos and coupling such photos with a style (e.g., <b>1015</b>). In addition, the producer <b>3021</b> may also, for example, perform audio analysis functions on songs <b>3009</b> or a set of songs (playlist <b>3008</b>) using such analysis like, for example, beat detection/mapping as described below. The producer <b>3021</b> may also keep track of available styles (e.g., <b>1015</b>), effects <b>3004</b>, transitions <b>3012</b>, and frames <b>3006</b>.</p>
<p id="p-0049" num="0048">In some embodiments, the renderer <b>3022</b> may be considered the play/display module. The renderer <b>3022</b> may receive slideshow/media presentation data from, for example, the core <b>3020</b> and producer <b>3021</b> and may render such data such that it may be sent to a graphics card or other display device (or interface) (e.g., <b>4003</b>). The renderer <b>3022</b> may interface with QuickTime&#xae; media player (e.g., the framework of QuickTime&#xae; media player) or another compatible application (or, framework) for audio/video decoding. In addition, the renderer <b>3022</b> may also interface with a composer-type application for actual rendering (e.g., of the slides), and the same or another similar application for applying filters <b>3006</b>.</p>
<p id="p-0050" num="0049">In some embodiments, the exporter <b>3023</b> may be considered the sharing module. The exporter <b>3023</b> may, for example, use renderer <b>3022</b> to export the slideshow/media presentation document to different formats (e.g., file formats) like those supported by QuickTime&#xae; or other similar applications. The exporter <b>3023</b> may, for example, obtain movie frame-type data from renderer <b>3022</b> and add it to a movie-type file. When the exporter <b>3023</b> is finished retrieving data for each movie, the slideshow/media presentation document would be available for access and sharing through the exemplary application <b>1000</b> or other applications that may access or handle the document in its final format.</p>
<p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. 4</figref> illustrates an exemplary system implementing an application in accordance with the present invention. The exemplary system <b>4000</b>, described above, may implement the exemplary application <b>1000</b>. Other modules and other routine programming-related matters may not be shown in <figref idref="DRAWINGS">FIG. 4</figref>, but would be understood and may be implemented by one of ordinary skill in the art without departing from the scope of the present invention.</p>
<p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. 5</figref> illustrates an exemplary implementation of an application in accordance with the present invention. The module units <b>5001</b>-<b>5004</b> and <b>5010</b>-<b>5013</b> of the exemplary implementation of an application <b>5000</b> are described more fully above for <figref idref="DRAWINGS">FIG. 3A</figref>. The module units <b>5001</b>-<b>5004</b> and <b>5010</b>-<b>5013</b> may be implemented, for example, by a standalone <b>4008</b>, <b>5008</b> or an embedded <b>4009</b>, <b>5009</b> application, respectively. Further, an exemplary system such as that illustrated in <figref idref="DRAWINGS">FIG. 4</figref> may execute the standalone <b>4008</b>, <b>5008</b> or embedded <b>4009</b>, <b>5009</b> applications. Other compatible or similar systems may also execute the applications <b>4008</b>, <b>5008</b> and <b>4009</b>, <b>5009</b> without departing from the scope of the present invention.</p>
<p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. 6</figref> illustrates an exemplary method in accordance with the present invention. The exemplary method <b>6000</b> comprises the following computer-implemented steps: defining a layer, wherein the layer comprises one or more effects <b>6001</b>; associating media content with the layer <b>6002</b>; aggregating the layer with one or more other layers <b>6003</b>; and assembling the aggregated layer <b>6004</b>. The exemplary method <b>6000</b> and, further, steps <b>6001</b>-<b>6004</b> may be implemented using an exemplary system such as that embodied in <figref idref="DRAWINGS">FIG. 4</figref>, which may execute the exemplary application <b>1000</b>, and as described herein.</p>
<p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. 7</figref> illustrates an exemplary method in accordance with the present invention. The exemplary method <b>7000</b> comprises the computer-implemented steps of the exemplary method <b>6000</b> with the addition of steps <b>7005</b>-<b>7015</b>. The exemplary method <b>7000</b> and, further, steps <b>7001</b>-<b>7015</b> may be implemented using an exemplary system such as that embodied in <figref idref="DRAWINGS">FIG. 4</figref>, which may execute the exemplary application <b>1000</b>, and as described herein.</p>
<p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. 8</figref> illustrates an exemplary method in accordance with the present invention. The method is designed to dynamically profile audio data, such as a song, playlist, or slideshow soundtrack, based on various audio parameters and to author the media presentation using the profiled audio data. As described more fully with respect to <figref idref="DRAWINGS">FIGS. 1 and 3A</figref>, a user may select a song/audio file or playlist via the select media content <b>1008</b> menu and drag the song/audio file or playlist into the effects containers region <b>1003</b> (see, for example, the reference to &#x201c;Drag Audio Here&#x201d;) of exemplary application <b>1000</b>.</p>
<p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. 9</figref> illustrates an exemplary diagram in accordance with the present invention. Each song/audio file, piece of music, or playlist has different properties (e.g., pitch, frequency, beats, etc.) over the duration of the song or songs. For example, as shown in graph <b>9000</b> of <figref idref="DRAWINGS">FIG. 9</figref>, the frequency of the song(s) changes over time.</p>
<p id="p-0057" num="0056">At step <b>8001</b>, the song/audio file or playlist (i.e., audio data) is analyzed based on various audio parameters. Producer <b>3021</b> may perform the analysis on the song/audio file or playlist in the effects container region <b>1003</b> of exemplary application <b>1000</b>. Alternatively, producer <b>3021</b> may analyze all of the songs/audio files stored in local storage <b>4006</b> or auxiliary device/component <b>4005</b>.</p>
<p id="p-0058" num="0057">In some embodiments, producer <b>3021</b> performs beat mapping to analyze the song/audio file or playlist. Beat mapping is a process for accurately finding the beats of music, including at the beginning of a song, after song breaks, or when the pace of the music changes. Beat mapping may also include beat pattern recognition (e.g., first beats of bars, drum breaks, etc.). It allows mapping of effects <b>1011</b> (or <b>3004</b>), transitions <b>1014</b> (or <b>3012</b>), or filters <b>3019</b> to the beat and the flow of the music. For example, when effects play for four beats (or one bar), beat mapping allows the effects, filters, or transitions to be synced to the real bars of the music.</p>
<p id="p-0059" num="0058">An example of a beat mapping algorithm is provided. It will be apparent to those skilled in the art that various modifications may be made to the beat mapping algorithm. The beat mapping algorithm has two phases: 1) beat detection and 2) analysis of the beats. During the beat detection phase, the audio files/songs are analyzed to determine features that indicate the pace of the music. The features of pace may include the beats, which is a strong change in sound energy (e.g., base drum or snare drums) or a change in notes of the music. The second phase is analysis of the beats to find the patterns of the music and to infer the full beat of the music. In some embodiments, analysis of the sound signal is performed on frequency bands. Analysis using frequency bands allows a first instrument, such as a bass drum, to be separated from voices or other instruments, such as snare drums.</p>
<p id="p-0060" num="0059">The beat detection phase may include Fourier analysis. For example, the analysis may include computing Fast Fourier Transforms (FFTs) of the sound signal on windows approximately 1/40 second wide at about eighty (80) windows per second. Each window may overlap. Before performing the FFTs, Hamming windows may be applied to minimize the edge effects on the FFTs. In this example, 80 FFTs per second result, which are then separated into bands (e.g., 8 or 16). For each band and each window, the energy, energy history, and the relative energy delta may be computed. Energy history may represent the sum of the energies a time interval before the window. The relative delta may represent the energy delta with the previous window with respect to the energy history. In this example, a large energy delta in a quiet part of a song is more important than an equally large energy delta in a loud part. Beats of music may be detected using the relative energy. The beats are a time when the relative energy delta is greater than a defined number. For example, this occurs when there is a loud boom in a piece of music. The defined number determines how sensitive the beat detection is. The defined number may be deduced from the music using energy variance or may be held constant.</p>
<p id="p-0061" num="0060">In some embodiments, the analysis of the beats phase may include finding patterns in the detected beats, removing extra beats, and/or handling missing beats. Various techniques may be used to analyze the beats including a phase-locked loop with gradient method, multi-agent systems, comb filters, or particle systems. It will be apparent to those skilled in the art that various modifications and variations can be made to the types of processes used for analyzing beats.</p>
<p id="p-0062" num="0061">An example of the analysis of the beats phase may include analysis using a type of auto-convolution and then using a type of comb filter algorithm. The type of auto-convolution may include generating intervals between the beats that are close to each other. The length of the intervals is then compared using a modified greatest common denominator (GCD) to find a close common denominator between them. For example, if two intervals are of length forty nine (49) and sixty three (63), the modified GCD will find that sixteen (16) is a close common denominator to them. A common denominator found often generally has a greater chance to be related to the local period of the beat. This type of auto-convolution analysis allows intervals to be related, and thus it may be statistically possible to find the number of beats per bar. The number of beats per bar is typically four (4), however it can be three (3) or (5). In addition, the auto-convolution analysis allows for identifying which beat period is associated with which beat. This may be used to build beat sets (i.e., beats that are related to each other using the given beat period) and then to find patterns or music phrases. Because the beat periods may be approximations due to the imprecision of mathematical processes, such as the sampling rate, the FFT window, beat detection, etc., the same interval may have a period that varies by two samples. Thus, a phase to consolidate the periods may be used. For example, if the periods are <b>123</b> and <b>124</b>, they may become the same period. The cardinal of the sets of beats that are related to each period may then be taken. The likely period of the mean beat may then be deduced. Because of the consolidation phase, which allows for an identification of related beat sets, an identification of several distinct phases of beats with different beat periods may be made.</p>
<p id="p-0063" num="0062">The comb filter analysis may include taking a comb function that is equal to one (1) where a beat was detected (A) and zero (0) where not (A) and convolving it with a function that is mainly zero and has triangular spikes around potential beats given a beat period and a number of beats (B(p,n)). The detected beats (A) may be analyzed as follows:</p>
<p id="p-0064" num="0063"><chemistry id="CHEM-US-00001" num="00001">
<img id="EMI-C00001" he="3.56mm" wi="30.40mm" file="US08626322-20140107-C00001.TIF" alt="embedded image" img-content="chem" img-format="tif"/>
</chemistry>
<br/>
the potential beats (B(p,n)):
<br/>
^ . . . ^ . . . ^ . . . ^ . . . ^ . . . ^ p=4, n=6.
</p>
<p id="p-0065" num="0064">The convolution gives a score of how the tried period p matches the detected beats in the signal. For example, the narrower the triangles, the more the match must be precise. This calculation is done for each beat. The period may be adapted to find the best match around the current period. The match may be done with two beats first and if the score is good (approximately equal to two (2)), then the beat is taken and the matching analysis may move to the next beat. If the beat is not good, then the match is attempted with four (4), eight (8), sixteen (16), and thirty two (32) beats. The amount by which the period may be adapted may depend on the number of potential beats that are being tested. For example, for two (2) or four (4) beats, the period may hardly change. For thirty two (32) beats, the period may change more. Thus, if there is a good match for the next potential thirty (32) beats, even if there is a large change in period, the analysis may be more correct. However, if the analysis is only done for two (2) or four (4) beats, then the analysis may focus on whatever extra beat exists. The comb filter algorithm described above may be performed forward and backward from a reference beat. The reference beat may be chosen using criteria meant to ensure that it is the main beat of the music. The main beat may be the biggest beat set and the comb filter algorithm may sufficiently extend from it both backwards and forwards.</p>
<p id="p-0066" num="0065">In some embodiments, beat mapping, including beat detection and analysis of the beats, is used to determine values for an audio parameter or audio parameters of a song. For example, for each song, producer <b>3021</b> analyzes the song using beat mapping to determine values for audio parameters <b>9001</b>-<b>9003</b> as shown in <figref idref="DRAWINGS">FIG. 9</figref>. The audio parameters <b>9001</b>-<b>9003</b> may include beats per minute (BPM), rhythmic strength (RS), harmonic complexity (HC), and/or square root of the arithmetic mean of the square of density variations (RMS strength). Other parameters may also be used. BPM may refer to the basic pace of a piece of music. Typically, the BPM for a song is not a constant value through the duration of the song. Rather the BPM adjusts dynamically throughout the song. The RS of a piece of music may characterize the rhythm of the piece of music. For example, a drum in a techno song may receive a high score, while a flute in a classical piece of music may receive a low score. In this example, the RS may be different between two pieces of music even though the pace of each of the pieces is the same. The HC of a song may represent how many instruments or musical parts are playing at the same time. HC is used to identify and differentiate different parts of a song. For example, HC may be used to identify and differentiate between the introduction, the waiting period (i.e., with few parts), and the main melodies (i.e., where the full range of instruments may be deployed) of a song. The RMS strength (i.e., square root of the arithmetic mean of the square of density variations) of a song is used to distinguish between the loud parts and the more peaceful of a song. The RMS strength may be based on the actual volume of the song and is used as a relative measure.</p>
<p id="p-0067" num="0066">At step <b>8002</b> shown in <figref idref="DRAWINGS">FIG. 8</figref>, the song/audio file or playlist is divided into segments. Step <b>8002</b> may be performed by producer <b>3021</b>. A segment may be defined as a period of time in the song/audio file or playlist where the audio parameter(s) <b>9001</b>-<b>9003</b> is in a range of values. The range of values may be user-defined or automatically defined by system <b>4000</b>. Producer <b>3021</b> may divide the song/file or playlist into chunks based on values of the parameters. If the audio parameters fall within a particular range of values, then producer <b>3021</b> may create a segment or segments. For example, as shown in <figref idref="DRAWINGS">FIG. 9</figref>, segments <b>9004</b>-<b>9006</b> each have a different range of values for audio parameter(s) <b>9001</b>-<b>9003</b>. In some embodiments, system <b>4000</b> uses the segments <b>9004</b>-<b>9006</b> of songs/audio files or playlists and associated audio parameter(s) <b>9001</b>-<b>9003</b> in the authoring or generation of slideshow/media presentations. More specifically, producer <b>3021</b> may apply the segments <b>9004</b>-<b>9006</b> of songs/audio files or playlists and associated audio parameter(s) <b>9001</b>-<b>9003</b> to effects <b>9007</b>-<b>9009</b> (also, <figref idref="DRAWINGS">FIG. 1</figref>, reference numeral <b>1011</b> and <figref idref="DRAWINGS">FIG. 3</figref>, reference numeral <b>3004</b>). Producer <b>3021</b> may also apply this information to transitions <b>1014</b> (also, <figref idref="DRAWINGS">FIG. 3</figref>, reference numeral <b>3012</b>), or filters <b>3019</b>.</p>
<p id="p-0068" num="0067">At step <b>8003</b>, effects <b>1011</b> (also, <figref idref="DRAWINGS">FIG. 3</figref>, reference numeral <b>3004</b> and <figref idref="DRAWINGS">FIG. 9</figref>, reference numerals <b>9007</b>-<b>9009</b>) are selected. The processes for selecting effects may apply to transitions <b>1014</b> (also, <figref idref="DRAWINGS">FIG. 3</figref>, reference numeral <b>3012</b>), or filters <b>3019</b>. Producer <b>3021</b> may select the effects. For example, producer <b>3021</b> may select effects according to whether the segments <b>9004</b>-<b>9006</b> meet an acceptance range(s) (or criteria) for audio parameter(s) of the effects <b>1011</b>. If the segment meets the acceptance range(s) (or criteria) for the audio parameter(s) of the effect, the effect is included in a list of possible effects for authoring a media presentation/slideshow. If the segment does not meet the acceptance range(s) (or criteria) for the audio parameter(s) of the effect, the effect is excluded in the list of possible effects for authoring a media presentation/slideshow.</p>
<p id="p-0069" num="0068">In some embodiments, the selection of effects may include additional steps. These steps may be performed by producer <b>3021</b>. For example, producer <b>3021</b> may associate each of the segments <b>9004</b>-<b>9006</b> with a segment profile. The segment profile may include the defined range of the audio parameter(s) for each segment. Producer <b>3021</b> may define an audio requirements profile for each of the effects. The audio requirements profile may include the acceptance range(s) or criteria for the audio parameter(s). A style of an effect may also have acceptance range(s). Producer <b>3021</b> may compare the segment profile for each of the segments to the audio requirements profile for each effect. Producer <b>3021</b> may determine whether the segment profile for each of the segments meets the criteria for the audio parameter in the audio requirements profile. If the segment meets the acceptance range(s) (or criteria) for the audio parameter(s) of the effect, the effect is included in the list of possible effects for authoring a media presentation/slideshow. If the segment does not meet the acceptance range(s) (or criteria) for the audio parameter(s) of the effect, the effect is excluded in the list of possible effects for authoring a media presentation/slideshow.</p>
<p id="p-0070" num="0069">In some embodiments, a user may select or filter (i.e., include or exclude) effects <b>1011</b> (also, <figref idref="DRAWINGS">FIG. 3</figref>, reference numeral <b>3004</b> and <figref idref="DRAWINGS">FIG. 9</figref>, reference numerals <b>9007</b>-<b>9009</b>) using application <b>1000</b>. A user may drag the audio file/song into the add effect containers region <b>1003</b> (see, for example, the reference to &#x201c;Drag Audio Here&#x201d; in the exemplary application <b>1000</b>). Producer <b>3021</b> may automatically filter or select the effects as described above. The selected or filtered effects may be displayed in the main effects library <b>1011</b> or in the edit layers and add effect containers region <b>1003</b> by producer <b>3021</b>.</p>
<p id="p-0071" num="0070">At step <b>8004</b>, a media presentation is authored or generated using the selected or filtered effects. The selected or filtered effects may be synchronized with the audio files/songs. Producer <b>3021</b> along with core <b>3020</b> and renderer <b>3022</b> may automatically assemble the selected or filtered effects with the audio files/songs in the effects containers region <b>1003</b>. Producer <b>3021</b> may automatically rearrange or assemble the selected or filtered effects to be synchronized with the audio files/songs.</p>
<p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. 10</figref> illustrates an exemplary method in accordance with the present invention. The method comprises the computer-implemented steps <b>10001</b> and <b>10002</b>. These steps were described above as steps <b>8001</b> and <b>8002</b>. The method of <figref idref="DRAWINGS">FIG. 10</figref> further includes additional steps <b>10003</b> through <b>10005</b>. At step <b>10003</b>, the audio requirements profile of the effects as described above is linked to the audio parameter(s) <b>9001</b>-<b>9003</b>, such as BPM, RS, HC, and RMS strength. Producer <b>3021</b> may perform step <b>10003</b>.</p>
<p id="p-0073" num="0072">At step <b>10004</b>, the effects may be adjusted based on the linked audio parameters <b>9001</b>-<b>9003</b>. The effect style <b>1015</b> may also be adjusted. For example, the transition duration <b>9010</b> or <b>9011</b>, effect duration, the in point of an effect (i.e., the time the effect begins), and out point of the effects (i.e., the time the effect ends) may be adjusted. For example, transition durations <b>9010</b> or <b>9011</b> may be adjusted to be shorter for a higher BPM. The transition durations <b>9010</b> or <b>9011</b> may be lengthened for a longer BPM.</p>
<p id="p-0074" num="0073">At step <b>10005</b>, a media presentation is authored or generated using the adjusted effects. Because the effects are adjusted based on the audio parameters <b>9001</b>-<b>9003</b>, they may be synchronized with the audio files/songs. For example, producer <b>3021</b> along with core <b>3020</b> and renderer <b>3022</b> may automatically assemble the adjusted effects <b>1011</b> (also, <figref idref="DRAWINGS">FIG. 3</figref>, reference numeral <b>3004</b> and <figref idref="DRAWINGS">FIG. 9</figref>, reference numerals <b>9007</b>-<b>9009</b>) with the audio file/song in the effects containers region <b>1003</b>. Producer <b>3021</b> may automatically rearrange or assemble the adjusted effects to be synchronized with the audio files/songs. In some embodiments, the method shown in <figref idref="DRAWINGS">FIG. 8</figref> can be used in conjunction with the method shown in <figref idref="DRAWINGS">FIG. 10</figref>.</p>
<p id="p-0075" num="0074">It will be apparent to those skilled in the art that various modifications and variations can be made to the present invention without departing from the spirit or scope of the invention. Thus, it is intended that the present invention cover the modifications and variations of this invention provided they come within the scope of the appended claims and their equivalents.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-chemistry idref="CHEM-US-00001" cdx-file="US08626322-20140107-C00001.CDX" mol-file="US08626322-20140107-C00001.MOL"/>
<us-claim-statement>What is claimed is: </us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A computer-implemented method for generating a multi-media presentation, comprising:
<claim-text>analyzing audio data based on an audio parameter;</claim-text>
<claim-text>dividing the audio data into a plurality of segments based at least in part on the audio parameter falling in a defined range for all audio data included in each segment of the plurality of segments, each segment having a segment profile including the defined range for the audio parameter;</claim-text>
<claim-text>accessing a plurality of effects to include in the multi-media presentation, each effect of the plurality of effects including an audio requirement profile;</claim-text>
<claim-text>comparing the audio parameter in the segment profile for each segment of the plurality of segments to the audio requirement profile for each effect of the plurality of effects;</claim-text>
<claim-text>generating, in response to comparing the audio parameter, a list of selectable effects based at least in part on the audio parameter meeting an acceptance criteria associated with the audio requirement profile; and</claim-text>
<claim-text>generating the multi-media presentation using the list of selectable effects.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the audio parameter is selected from the group comprising: beats per minute, rhythmic strength, harmonic complexity, and square root of the arithmetic mean of the square of density variations (RMS) strength.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the dividing step is based on a range of the audio parameter.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the acceptance criteria is an acceptance range.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. A computer-implemented method for generating a multi-media presentation, comprising:
<claim-text>analyzing audio data based on an audio parameter;</claim-text>
<claim-text>dividing the audio data into a plurality of segments based at least in part on the audio parameter falling in a defined range for audio data included in each segment of the plurality of segments, each segment having a segment profile including the defined range for the audio parameter;</claim-text>
<claim-text>linking an audio requirements profile of each of one or more effects to the audio parameter;</claim-text>
<claim-text>adjusting the one or more effects based on the audio parameter;</claim-text>
<claim-text>comparing the audio parameter in the segment profile for each segment of the plurality of segments to the audio requirement profile for each effect of the plurality of effects;</claim-text>
<claim-text>generating, in response to comparing the audio parameter, a list of selectable effects based at least in part on the audio parameter meeting an acceptance criteria associated with the audio requirement profile; and</claim-text>
<claim-text>generating a multi-media presentation using the list of selectable effects.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The computer-implemented method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the audio parameter is selected from the group comprising: beats per minute, rhythmic strength, harmonic complexity, and square root of the arithmetic mean of the square of density variations (RMS) strength.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The computer-implemented method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the dividing step is based on a range of the audio parameter.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The computer-implemented method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the step of adjusting further comprises adjusting one or more of the transition duration, effect duration, in points, and out points of the one or more effects.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. A computer-implemented system for generating a multi-media presentation, comprising:
<claim-text>memory configured to store audio data; and</claim-text>
<claim-text>one or more processors configured to analyze the audio data based on an audio parameter and divide the audio data into a plurality of segments based at least in part on the audio parameter falling in a defined range for all audio data included in each segment of the plurality of segments, each segment having a segment profile including the defined range for the audio parameter,</claim-text>
<claim-text>the one or more processors further configured to access a plurality of effects to include in the multi-media presentation, each effect of the plurality of effects including an audio requirement profile,</claim-text>
<claim-text>the one or more processors further configured to compare the audio parameter in the segment profile for each segment of the plurality of segments to the audio requirement profile for each effect of the plurality of effects; and</claim-text>
<claim-text>the one or more processors further configured to generate, in response to comparing the audio parameter, a list of selectable effects based at least in part on the audio parameter meeting an acceptance criteria associated with the audio requirement profile and generate a multi-media presentation using the list of selectable effects.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The computer-implemented system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the audio parameter is selected from the group comprising: beats per minute, rhythmic strength, harmonic complexity, and square root of the arithmetic mean of the square of density variations (RMS) strength.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The computer-implemented system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the dividing step is based on a range of the audio parameter.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The computer-implemented system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the acceptance criteria is an acceptance range.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. A computer-implemented system for generating a multi-media presentation, comprising:
<claim-text>memory configured to store audio data; and</claim-text>
<claim-text>one or more processors configured to analyze audio data based on an audio parameter and divide the audio data into a plurality of segments based at least in part on the audio parameter falling in a defined range for all audio data included in each segment of the plurality of segments, each segment having a segment profile including the defined range for the audio parameter,</claim-text>
<claim-text>the one or more processors further configured to link an audio requirements profile of each of one or more effects to the audio parameter, adjust the one or more effects based on the audio parameter, compare the audio parameter in the segment profile for each segment of the plurality of segments to the audio requirement profile for each effect of the plurality of effects, generate a list of selectable effects based at least in part on the audio parameter meeting an acceptance criteria associated with the audio requirement profile, and generate a multi-media presentation using the list of selectable effects.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The computer-implemented system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the audio parameter is selected from the group comprising: beats per minute, rhythmic strength, harmonic complexity, and square root of the arithmetic mean of the square of density variations (RMS) strength.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The computer-implemented system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the dividing step is based on a range of the audio parameter.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The computer-implemented system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the one or more processors is further configured to adjust one or more of the transition duration, effect duration, in points, and out points of the one or more effects.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. A non-transitory computer-readable storage medium storing one or more programs configured for execution by a computer for generating a multi-media presentation, the one or more programs comprising instructions to:
<claim-text>analyzing audio data based on an audio parameter;</claim-text>
<claim-text>dividing the audio data into a plurality of segments based at least in part on the audio parameter falling in a defined range for all audio data included in each segment of the plurality of segments, each segment having a segment profile including the defined range for the audio parameter;</claim-text>
<claim-text>accessing a plurality of effects to include in the multi-media presentation, each effect of the plurality of effects including an audio requirement profile;</claim-text>
<claim-text>comparing the audio parameter in the segment profile for each segment of the plurality of segments to the audio requirement profile for each effect of the plurality of effects;</claim-text>
<claim-text>generating, in response to comparing the audio parameter, a list of selectable effects based at least in part on the audio parameter meeting an acceptance criteria associated with the audio requirement profile; and</claim-text>
<claim-text>generating the multi-media presentation using the list of selectable effects.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. A non-transitory computer-readable storage medium storing one or more programs configured for execution by a computer, the one or more programs comprising instructions to:
<claim-text>analyzing audio data based on an audio parameter;</claim-text>
<claim-text>dividing the audio data into a plurality of segments based at least in part on the audio parameter falling in a defined range for audio data included in each segment of the plurality of segments, each segment having a segment profile including the defined range for the audio parameter;</claim-text>
<claim-text>linking an audio requirements profile of each of one or more effects to an audio parameter;</claim-text>
<claim-text>adjusting the one or more effects based on the audio parameter;</claim-text>
<claim-text>comparing the audio parameter in the segment profile for each segment of the plurality of segments to the audio requirement profile for each effect of the plurality of effects;</claim-text>
<claim-text>generating, in response to comparing the audio parameter, a list of selectable effects based at least in part on the audio parameter meeting an acceptance criteria associated with the audio requirement profile; and</claim-text>
<claim-text>generating a slideshow using the list of selectable effects. </claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
