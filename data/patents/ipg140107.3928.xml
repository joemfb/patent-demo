<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08624996-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08624996</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13726527</doc-number>
<date>20121224</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>9</main-group>
<subgroup>73</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>3482231</main-classification>
<further-classification>3482221</further-classification>
</classification-national>
<invention-title id="d2e43">Auto white balance algorithm using RGB product measure</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5867169</doc-number>
<kind>A</kind>
<name>Prater</name>
<date>19990200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6459449</doc-number>
<kind>B1</kind>
<name>Juen</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6947078</doc-number>
<kind>B1</kind>
<name>Kuwata et al.</name>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>7142238</doc-number>
<kind>B1</kind>
<name>Sawada et al.</name>
<date>20061100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>7184080</doc-number>
<kind>B2</kind>
<name>Kehtarnavaz et al.</name>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>7245319</doc-number>
<kind>B1</kind>
<name>Enomoto</name>
<date>20070700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>7352394</doc-number>
<kind>B1</kind>
<name>DeLuca et al.</name>
<date>20080400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>7436998</doc-number>
<kind>B2</kind>
<name>Steinberg et al.</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>7469071</doc-number>
<kind>B2</kind>
<name>Drimbarean et al.</name>
<date>20081200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>7474341</doc-number>
<kind>B2</kind>
<name>DeLuca et al.</name>
<date>20090100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>7536036</doc-number>
<kind>B2</kind>
<name>Steinberg et al.</name>
<date>20090500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>7587085</doc-number>
<kind>B2</kind>
<name>Steinberg et al.</name>
<date>20090900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>7620218</doc-number>
<kind>B2</kind>
<name>Steinberg et al.</name>
<date>20091100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>7630006</doc-number>
<kind>B2</kind>
<name>DeLuca et al.</name>
<date>20091200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>7653240</doc-number>
<kind>B1</kind>
<name>Otobe et al.</name>
<date>20100100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>7804531</doc-number>
<kind>B2</kind>
<name>DeLuca et al.</name>
<date>20100900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>7847839</doc-number>
<kind>B2</kind>
<name>DeLuca et al.</name>
<date>20101200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>7847840</doc-number>
<kind>B2</kind>
<name>DeLuca et al.</name>
<date>20101200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>7852384</doc-number>
<kind>B2</kind>
<name>DeLuca et al.</name>
<date>20101200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>7916897</doc-number>
<kind>B2</kind>
<name>Corcoran et al.</name>
<date>20110300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>7953250</doc-number>
<kind>B2</kind>
<name>Steinberg et al.</name>
<date>20110500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>7953251</doc-number>
<kind>B1</kind>
<name>Steinberg et al.</name>
<date>20110500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>7953287</doc-number>
<kind>B2</kind>
<name>Drimbarean et al.</name>
<date>20110500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>8036460</doc-number>
<kind>B2</kind>
<name>Nanu et al.</name>
<date>20111000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>8055029</doc-number>
<kind>B2</kind>
<name>Petrescu et al.</name>
<date>20111100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>8089555</doc-number>
<kind>B2</kind>
<name>Vakrat et al.</name>
<date>20120100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>8135184</doc-number>
<kind>B2</kind>
<name>Steinberg et al.</name>
<date>20120300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>8155397</doc-number>
<kind>B2</kind>
<name>Bigioi et al.</name>
<date>20120400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>8265388</doc-number>
<kind>B2</kind>
<name>Nanu et al.</name>
<date>20120900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>8339462</doc-number>
<kind>B2</kind>
<name>Stec et al.</name>
<date>20121200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>US</country>
<doc-number>8339471</doc-number>
<kind>B2</kind>
<name>Alcazar</name>
<date>20121200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>US</country>
<doc-number>2002/0051639</doc-number>
<kind>A1</kind>
<name>Enomoto</name>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00033">
<document-id>
<country>US</country>
<doc-number>2003/0038344</doc-number>
<kind>A1</kind>
<name>Palmer et al.</name>
<date>20030200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00034">
<document-id>
<country>US</country>
<doc-number>2003/0068080</doc-number>
<kind>A1</kind>
<name>Lu</name>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00035">
<document-id>
<country>US</country>
<doc-number>2003/0206179</doc-number>
<kind>A1</kind>
<name>Deering</name>
<date>20031100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00036">
<document-id>
<country>US</country>
<doc-number>2004/0113549</doc-number>
<kind>A1</kind>
<name>Roberts et al.</name>
<date>20040600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00037">
<document-id>
<country>US</country>
<doc-number>2005/0156330</doc-number>
<kind>A1</kind>
<name>Harris</name>
<date>20050700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00038">
<document-id>
<country>US</country>
<doc-number>2005/0247944</doc-number>
<kind>A1</kind>
<name>Haque et al.</name>
<date>20051100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00039">
<document-id>
<country>US</country>
<doc-number>2006/0252230</doc-number>
<kind>A1</kind>
<name>Yang</name>
<date>20061100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00040">
<document-id>
<country>US</country>
<doc-number>2007/0041064</doc-number>
<kind>A1</kind>
<name>Subbotin</name>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00041">
<document-id>
<country>US</country>
<doc-number>2007/0153341</doc-number>
<kind>A1</kind>
<name>Kang</name>
<date>20070700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00042">
<document-id>
<country>US</country>
<doc-number>2007/0211449</doc-number>
<kind>A1</kind>
<name>Holman et al.</name>
<date>20070900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00043">
<document-id>
<country>US</country>
<doc-number>2007/0273906</doc-number>
<kind>A1</kind>
<name>Tsuchiya</name>
<date>20071100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00044">
<document-id>
<country>US</country>
<doc-number>2008/0043444</doc-number>
<kind>A1</kind>
<name>Hasegawa et al.</name>
<date>20080200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00045">
<document-id>
<country>US</country>
<doc-number>2008/0054486</doc-number>
<kind>A1</kind>
<name>Murayama et al.</name>
<date>20080300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00046">
<document-id>
<country>US</country>
<doc-number>2008/0143851</doc-number>
<kind>A1</kind>
<name>Shi</name>
<date>20080600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00047">
<document-id>
<country>US</country>
<doc-number>2008/0296599</doc-number>
<kind>A1</kind>
<name>Mazzochette</name>
<date>20081200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00048">
<document-id>
<country>US</country>
<doc-number>2008/0309790</doc-number>
<kind>A1</kind>
<name>Nishiwaki et al.</name>
<date>20081200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00049">
<document-id>
<country>US</country>
<doc-number>2009/0014739</doc-number>
<kind>A1</kind>
<name>Chiang et al.</name>
<date>20090100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00050">
<document-id>
<country>US</country>
<doc-number>2009/0059025</doc-number>
<kind>A1</kind>
<name>Tsujino et al.</name>
<date>20090300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00051">
<document-id>
<country>US</country>
<doc-number>2009/0096910</doc-number>
<kind>A1</kind>
<name>Yasuda et al.</name>
<date>20090400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>34833301</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00052">
<document-id>
<country>US</country>
<doc-number>2009/0097745</doc-number>
<kind>A1</kind>
<name>Kim et al.</name>
<date>20090400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00053">
<document-id>
<country>US</country>
<doc-number>2009/0147099</doc-number>
<kind>A1</kind>
<name>Kim et al.</name>
<date>20090600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482231</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00054">
<document-id>
<country>US</country>
<doc-number>2009/0263022</doc-number>
<kind>A1</kind>
<name>Petrescu et al.</name>
<date>20091000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00055">
<document-id>
<country>US</country>
<doc-number>2009/0303342</doc-number>
<kind>A1</kind>
<name>Corcoran et al.</name>
<date>20091200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00056">
<document-id>
<country>US</country>
<doc-number>2010/0265527</doc-number>
<kind>A1</kind>
<name>Tsuchiya</name>
<date>20101000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00057">
<document-id>
<country>US</country>
<doc-number>2011/0026780</doc-number>
<kind>A1</kind>
<name>Corcoran et al.</name>
<date>20110200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00058">
<document-id>
<country>US</country>
<doc-number>2011/0058071</doc-number>
<kind>A1</kind>
<name>Bigioi et al.</name>
<date>20110300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00059">
<document-id>
<country>US</country>
<doc-number>2011/0069186</doc-number>
<kind>A1</kind>
<name>DeLuca et al.</name>
<date>20110300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00060">
<document-id>
<country>US</country>
<doc-number>2011/0074975</doc-number>
<kind>A1</kind>
<name>DeLuca et al.</name>
<date>20110300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00061">
<document-id>
<country>US</country>
<doc-number>2011/0102643</doc-number>
<kind>A1</kind>
<name>Nanu et al.</name>
<date>20110500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00062">
<document-id>
<country>US</country>
<doc-number>2011/0134271</doc-number>
<kind>A1</kind>
<name>Bigioi et al.</name>
<date>20110600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00063">
<document-id>
<country>US</country>
<doc-number>2011/0205381</doc-number>
<kind>A1</kind>
<name>Vranceanu et al.</name>
<date>20110800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00064">
<document-id>
<country>US</country>
<doc-number>2011/0221936</doc-number>
<kind>A1</kind>
<name>Bigioi et al.</name>
<date>20110900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00065">
<document-id>
<country>US</country>
<doc-number>2012/0008002</doc-number>
<kind>A1</kind>
<name>Bigioi et al.</name>
<date>20120100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00066">
<document-id>
<country>US</country>
<doc-number>2012/0014600</doc-number>
<kind>A1</kind>
<name>Nanu et al.</name>
<date>20120100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00067">
<document-id>
<country>US</country>
<doc-number>2012/0038788</doc-number>
<kind>A1</kind>
<name>DeLuca et al.</name>
<date>20120200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00068">
<document-id>
<country>US</country>
<doc-number>2012/0057041</doc-number>
<kind>A1</kind>
<name>Stec et al.</name>
<date>20120300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00069">
<document-id>
<country>US</country>
<doc-number>2012/0069198</doc-number>
<kind>A1</kind>
<name>Bigioi et al.</name>
<date>20120300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00070">
<document-id>
<country>US</country>
<doc-number>2012/0069222</doc-number>
<kind>A1</kind>
<name>Steinberg et al.</name>
<date>20120300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00071">
<document-id>
<country>US</country>
<doc-number>2012/0070087</doc-number>
<kind>A1</kind>
<name>Petrescu et al.</name>
<date>20120300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00072">
<document-id>
<country>US</country>
<doc-number>2012/0177289</doc-number>
<kind>A1</kind>
<name>Stec et al.</name>
<date>20120700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00073">
<document-id>
<country>US</country>
<doc-number>2012/0199857</doc-number>
<kind>A1</kind>
<name>Humpston et al.</name>
<date>20120800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00074">
<document-id>
<country>CN</country>
<doc-number>1916964</doc-number>
<kind>A</kind>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00075">
<document-id>
<country>JP</country>
<doc-number>2006-115039</doc-number>
<kind>A</kind>
<date>20060400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00076">
<document-id>
<country>JP</country>
<doc-number>2009-522869</doc-number>
<kind>A</kind>
<date>20090600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00077">
<document-id>
<country>WO</country>
<doc-number>2007/079039</doc-number>
<kind>A2</kind>
<date>20070700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00078">
<document-id>
<country>WO</country>
<doc-number>2009/095422</doc-number>
<kind>A2</kind>
<date>20090800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00079">
<document-id>
<country>WO</country>
<doc-number>2009/095422</doc-number>
<kind>A3</kind>
<date>20091200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00080">
<document-id>
<country>WO</country>
<doc-number>2011/044393</doc-number>
<kind>A1</kind>
<date>20110400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00081">
<document-id>
<country>WO</country>
<doc-number>2011/082366</doc-number>
<kind>A1</kind>
<date>20110700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00082">
<othercit>PCT Notification of Transmittal of the International Search Report and the Written Opinion of the International Searching Authority, or the Declaration, for PCT Application No. PCT/US2010/062605, report dated Feb. 24, 2011, 7 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00083">
<othercit>PCT Notification of Transmittal of International Preliminary Report on Patentability, Chapter 1 of the Patent Cooperation Treaty, International Preliminary Report on Patentability, for PCT Application No. PCT/US2010/062605, report dated Jul. 4, 2012, 7 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00084">
<othercit>&#x201c;Color balance&#x201d; from Wikipedia, the free encyclopedia dated Jan. 2011 downloaded from the Internet on Feb. 2, 2011 &#x3c; http://en.wikipedia.org/wiki/Color<sub>&#x2014;</sub>balance &#x3e; (6 pages).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00085">
<othercit>Patent Abstract for Chinese published application No. 1916964, published on Feb. 21, 2007. Window added interpolation method for Sinc function in image scaling device.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00086">
<othercit>John Mallon; Paul Whelan: &#x201c;Calibration and removal of lateral chromatic aberration in images.&#x201d; Pattern Recognition Letters, vol. 28, No. 1, Jan. 2007, pp. 125-135, XP002536872 New York, DOI: http://dx.doi.org/10.1016/J.PATREC.2006.06.013.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00087">
<othercit>Patent Abstracts of Japan, publication No. 2006-115039, published on Apr. 27, 2006, Image Processor, Image Processing Method and Computer Program.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>20</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>3482221</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3482231</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>4</number-of-drawing-sheets>
<number-of-figures>4</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>12651190</doc-number>
<date>20091231</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>8339471</doc-number>
<date>20121215</date>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13726527</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20130201200</doc-number>
<kind>A1</kind>
<date>20130808</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only" applicant-authority-category="assignee">
<addressbook>
<orgname>DigitalOptics Corporation Europe Limited</orgname>
<address>
<city>Ballibrit, Galway</city>
<country>IE</country>
</address>
</addressbook>
<residence>
<country>IE</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Mendez</last-name>
<first-name>Jose A.</first-name>
<address>
<city>San Jose</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Hickman Palermo Truong Becker Bingham Wong LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>DigitalOptics Corporation Europe Limited</orgname>
<role>03</role>
<address>
<city>Galway</city>
<country>IE</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Selby</last-name>
<first-name>Gevell</first-name>
<department>2664</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A method and device is provided for adjusting the white balance of a digital image by adjusting the values assigned to the red, green, and blue subpixels of a pixel in the image. The adjustment to the subpixels is determined by identifying pixels in the image that have an RGB product greater than a threshold value, wherein the threshold value is based at least in part on an average of the RGB products of each pixel in the image and a variance between the RGB products of the pixels and the average of the RGB products.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="176.61mm" wi="78.40mm" file="US08624996-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="180.93mm" wi="134.11mm" file="US08624996-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="214.63mm" wi="90.85mm" file="US08624996-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="209.89mm" wi="146.13mm" orientation="landscape" file="US08624996-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="208.87mm" wi="136.91mm" orientation="landscape" file="US08624996-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">PRIORITY</heading>
<p id="p-0002" num="0001">This application is Continuation of U.S. patent application no. 12/651,190, filed Dec. 31, 2009, now U.S. Pat. No. 8,339,471, which is hereby incorporated by reference.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">FIELD OF THE INVENTION</heading>
<p id="p-0003" num="0002">The present invention relates generally to digital image processing, and more particularly to a system and method for correcting the white balance in a digital image.</p>
<heading id="h-0003" level="1">BACKGROUND</heading>
<p id="p-0004" num="0003">Various types of light, present when a digital image is captured, can cause an object in the captured image to have a hue that is not present when the object is viewed directly. For example, various types of artificial light can cause objects in captured images to have an orange hue, while natural light under certain circumstances can cause objects to have a blue hue. This hue is most noticeable on objects within an image that a viewer knows are white and expects to see as white.</p>
<p id="p-0005" num="0004">To correct this type of image distortion, many sophisticated algorithms are applied to digital images to change pixel values for acquired image data into pixel values that will not show the distortion. This process of changing the values for acquired image data to make portions of an image expected to be white appear white is frequently referred to as adjusting the white balance of the image. White balance algorithms currently known in the art tend to be computationally expensive, and thus unsuitable for being implemented in hardware with real time logic and unsuitable for being implemented on smaller, mobile devices that have limited memory and limited processing power. Many white balance algorithms currently known in the art also require users to supply values for various settings in order for the algorithm to be executed, which undesirably increases a user's involvement in the picture taking process.</p>
<p id="p-0006" num="0005">The approaches described in this section are approaches that could be pursued, but not necessarily approaches that have been previously conceived or pursued. Therefore, unless otherwise indicated, it should not be assumed that any of the approaches described in this section qualify as prior art merely by virtue of their inclusion in this section.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0007" num="0006">The present invention is illustrated by way of example, and not by way of limitation, in the figures of the accompanying drawings and in which like reference numerals refer to similar elements and in which:</p>
<p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. 1</figref> is an illustration of a pixel grid of a display device;</p>
<p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. 2</figref> is a flowchart of a process for adjusting the white balance in an image;</p>
<p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. 3</figref> is a block diagram for an example mobile device in which embodiments of the present invention may be implemented.</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 4</figref> is a block diagram that illustrates a computer system upon which an embodiment of the invention may be implemented.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0012" num="0011">In the following description, for the purposes of explanation, numerous specific details are set forth in order to provide a thorough understanding of the present invention. It will be apparent, however, that the present invention may be practiced without these specific details. In other instances, well-known structures and devices are shown in block diagram form in order to avoid unnecessarily obscuring the present invention.</p>
<heading id="h-0006" level="1">General Overview</heading>
<p id="p-0013" num="0012">Techniques are described herein for performing white balance by adjusting the values assigned to the red, green, and blue subpixels of a pixel in order to cause color neutral areas of a picture to be rendered color neutral. Each pixel in a pixel array has an RGB product equal to the pixel's red value (R) multiplied by the pixel's green value (G) multiplied by the pixel's blue value (B). During the process of image capture, a first value corresponding to an average of the RGB products of each pixel in an image can be calculated and stored. A second value, corresponding to a variance between the RGB products of the pixels and the first value, can be calculated and stored. A subset of pixels with RGB products greater than the first value plus the second value can be identified. For the subset of pixels, the average values of R, G, and B for the pixels in the subset can be determined. Based on the determined averages of R, G, and B, gains can be determined that make the average values of R, G, and B for the subset of pixels equal. Based on the determined gains, the R, G, and B values for all pixels, or a subset of pixels, in the captured imaged can be adjusted.</p>
<heading id="h-0007" level="1">Pixels and Subpixels</heading>
<p id="p-0014" num="0013">Typically, a digital image file includes a set of data values for a 2-dimensional grid of pixels. The pixel grid comprises rows of pixels that run horizontally and columns of pixels that run vertically, and each pixel in the pixel grid is identifiable by the row and column that contain that pixel. For example, an 800&#xd7;600 image would have 800 columns of pixels and 600 rows of pixels for a total of 480,000 pixels. Column 155 and row 235, for example, would uniquely identify a single pixel of the 480,000 pixels in the grid.</p>
<p id="p-0015" num="0014">Each pixel in the grid can comprise a plurality of subpixels. <figref idref="DRAWINGS">FIG. 1</figref> shows an example of a 2-by-2 grid of pixels, with each pixel comprising three subpixels. For example, the pixel of row 1, column 1 comprises a red subpixel <b>101</b><i>r</i>, a green subpixel <b>101</b><i>g</i>, and a blue subpixel <b>101</b><i>b</i>. The value of each subpixel indicates the intensity at which an element that corresponds to the subpixel is illuminated within the image. The number of values that can be assigned to a subpixel depends on the bit depth of the subpixel. For example, in a 24-bit display, each of the three subpixels has a bit depth of 8, and can thus support 2<sup>8 </sup>(i.e. 256) different values, ranging from 0 to 255. As each of the three subpixels can support 256 different values, the pixel can produce 16,777,216 (256*256*256) different colors. Image sensors on image acquisition devices such as cameras capture the image data used to illuminate the pixel grid. Although the present disclosure gives examples related to 24-bit image data, 42-bit image sensors and image sensors with alternative bit depths are also known in the art and are compatible with the techniques described herein.</p>
<p id="p-0016" num="0015">A pixel appears as a neutral color, such as those on the gray scale, when the values for the red element, green element, and blue element are equal in value to one another. For example, the truest white a display is capable of producing occurs when all three subpixels are assigned values of 255; a true gray color is produced when all three subpixels are assigned values of 128; and black is produced when all three subpixels are assigned values of 0. Assigning high values (e.g. over 220) to each of the subpixels of a pixel will cause the pixel to produce a color that is near white. When the values of the subpixels are not equal, the color produced by the pixel will have a hue. For example, if the value of the red subpixel is 240, the value of the green subpixel is 240, and the value of the blue subpixel is 248, then the color produced by the pixel will have a blue hue.</p>
<heading id="h-0008" level="1">Identifying Near-White Pixels</heading>
<p id="p-0017" num="0016">In digital image acquisition, variations in light temperature can cause an image sensor to detect white and near-white colors as having such a hue. Techniques are described hereafter for identifying portions of images containing an undesirable hue and adjusting the values of the subpixels so that the display renders the near white portions of the image as color neutral and without the undesirable hue.</p>
<p id="p-0018" num="0017">Unlike prior art methods of adjusting an image's white balance that use luminance values or RGB sums to identify near-white pixels, the techniques described hereafter use a product of the R, G, and B values (RGB<sub>product</sub>) for individual pixels in an image to identify areas within the image that are expected to be color neutral.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 2</figref> shows a flowchart of a method implementing techniques described herein. During the process of image capture, an average RGB product (RGB<sub>avg</sub><sub><sub2>&#x2014;</sub2></sub><sub>prod</sub>) can be calculated based on a first frame (block <b>210</b>). Each pixel in the frame will have an RGB<sub>product</sub>, and the value of RGB<sub>avg</sub><sub><sub2>&#x2014;</sub2></sub><sub>prod </sub>is the average of the RGB<sub>product </sub>value for each pixel in the frame. A variance can be calculated using RGB<sub>product </sub>values for pixels of a second frame and the RGB<sub>avg</sub><sub><sub2>&#x2014;</sub2></sub><sub>prod </sub>value obtained from the first frame (block <b>220</b>). The variance value is the average variation between the RGB<sub>product </sub>values of each pixel and the RGB<sub>avg</sub><sub><sub2>&#x2014;</sub2></sub><sub>prod</sub>. An image with many contrasting colors will have a high variance while an image with one dominant color will have a low variance. For example, an image of a flower garden will typically have a higher variance than an image dominated by a single color wall.</p>
<p id="p-0020" num="0019">From a third frame, a set of &#x201c;near-white&#x201d; pixels are identified (block <b>230</b>) based on the variance determined at block <b>220</b>. In one embodiment, the near-white pixels are the pixels in the image that satisfy the condition RGB<sub>product</sub>&#x3e;threshold<sub>near-white</sub>, where the value of the threshold is (a*RGB<sub>avg</sub><sub><sub2>&#x2014;</sub2></sub><sub>prod</sub>+b*variance). Using variance as a factor in determining the threshold<sub>near-white </sub>value needed for a pixel to be considered a near-white pixel allows the threshold<sub>near-white </sub>value to change based on a statistical nature of the acquired image. In an image with a large number of different colors, i.e. an image with a high variance, the presence of a color with a particular hue is more likely to be a correct representation of the object being photographed than a distortion caused by the lighting conditions present when the image was acquired. Thus, in an image with a high variance, the threshold<sub>near-white </sub>value for including a pixel in the set of near-white pixels is higher.</p>
<heading id="h-0009" level="1">Excluding Saturated Pixels</heading>
<p id="p-0021" num="0020">In one embodiment, pixels with an R, G, or B value that is greater than a threshold<sub>saturated </sub>value, such as when R, G, or B is equal to or greater than 250, are considered saturated and are excluded from the set of near-white pixels. Saturated pixels are excluded from the near-white pixel set because a subpixel value above that threshold<sub>saturated </sub>value often contains skewed color information. For example, in an image with a very bright red light, the image sensor acquiring the image might not be able to accurately detect the intensity of the red component. As a result, a red subpixel of a pixel might have a value of close to 255, but if not for the limitations of the hardware acquiring the image, the value of the red subpixel would be higher. In such an instance, the image data acquired by the image sensor may not accurately reflect the ratio of the R, G, and B values present. Therefore, these pixels are excluded from the set of near-white pixels.</p>
<heading id="h-0010" level="1">Near-White Threshold Value Coefficients</heading>
<p id="p-0022" num="0021">When (a*RGB<sub>avg</sub><sub><sub2>&#x2014;</sub2></sub><sub>rod</sub>+b*variance) is used to determine the threshold<sub>near-white </sub>value, the a-coefficient and b-coefficient can be determined based on system design preferences. The a-coefficient adjusts how sensitive the white-balance operation is to images that are dominated by a single color. An image dominated by a single color will have a low variance, and most pixels will have an RGB<sub>product </sub>close to RGB<sub>avg</sub><sub><sub2>&#x2014;</sub2></sub><sub>prod</sub>. An a-value greater than 1 ensures that only pixels with an RGB<sub>product </sub>greater than RGB<sub>avg</sub><sub><sub2>&#x2014;</sub2></sub><sub>prod </sub>will be included in the set of near-white pixels. In images dominated by one color, difficulties exist in determining whether a hue is an accurate representation of the object in the image or a distortion. Thus, an a-value of greater than 1 can be used to cause the white-balance operation to not adjust the pixel values in such situations, under the assumption that it is preferable to not adjust an image rather than to apply an incorrect adjustment to an image.</p>
<p id="p-0023" num="0022">The b-coefficient determines the white-balance operation's sensitivity. For example, a large b-value decreases the number of pixels that will be included in the set of near-white pixels by increasing the threshold<sub>near-white </sub>value an RGB<sub>product </sub>must exceed to be included in the set of near-white pixels. In a system with a 24-bit image acquisition sensor, the a-coefficient might equal 1.2, and the b-coefficient might equal 2.5. When implementing the techniques described herein with image sensors of other bit-depths, the values of the a-coefficient and b-coefficient might be different.</p>
<heading id="h-0011" level="1">Gain Factors</heading>
<p id="p-0024" num="0023">An average of the R values (R<sub>avg</sub>), an average of the G values (G<sub>avg</sub>), and an average of the B values (B<sub>avg</sub>) for the pixels in the set of near-white pixels can be calculated. Based on R<sub>avg</sub>, G<sub>avg</sub>, and B<sub>avg</sub>, gain factors can be determined (block <b>240</b>), where G<sub>R </sub>represents a gain to be applied to red subpixels, G<sub>G </sub>represents a gain to be applied to green subpixels, and G<sub>B </sub>represents a gain to be applied to blue subpixels. According to one embodiment, the gain factors are determined to make R<sub>avg</sub>, G<sub>avg</sub>, and B<sub>avg </sub>equal. In such an embodiment, the gain factors can be determined for each of R<sub>avg</sub>, G<sub>avg</sub>, and B<sub>avg </sub>based on the following equations:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>G</i><sub>R</sub><i>*R</i><sub>avg</sub>=(<i>R</i><sub>avg</sub><i>+G</i><sub>avg</sub><i>+B</i><sub>avg</sub>)/3;<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>G</i><sub>G</sub><i>*G</i><sub>avg</sub>=(<i>R</i><sub>avg</sub><i>+G</i><sub>avg</sub><i>+B</i><sub>avg</sub>)/3;<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>G</i><sub>B</sub><i>*B</i><sub>avg</sub>=(<i>R</i><sub>avg</sub><i>+G</i><sub>avg</sub><i>+B</i><sub>avg</sub>)/3.<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0025" num="0024">Determining gain factors to make R<sub>avg</sub>, G<sub>avg</sub>, and B<sub>avg </sub>equal to (R<sub>avg</sub>+G<sub>avg</sub>+B<sub>avg</sub>)/3 is but one of many contemplated embodiments. Gain factors can also be determined based on lowering the two highest of the R<sub>avg</sub>, G<sub>avg</sub>, and B<sub>avg </sub>to be equal to the lowest of the three, based on raising the two lowest of R<sub>avg</sub>, G<sub>avg</sub>, and B<sub>avg </sub>to equal to the highest of the three values, or by numerous other techniques.</p>
<heading id="h-0012" level="1">Adjusting Based on Gain Factors</heading>
<p id="p-0026" num="0025">The determined value for G<sub>R </sub>can be used to adjust the R value of each pixel in the stored image; the determined value for G<sub>G </sub>can be used to adjust the G value of each pixel in the stored image; and the determined value for G<sub>B </sub>can be used to adjust the B value of each pixel in the stored image (block <b>250</b>). In some embodiments, G<sub>R</sub>, G<sub>G</sub>, and G<sub>B </sub>might be used to adjust fewer than all the pixels in the image, such as only the set of near-white pixels identified in block <b>230</b> of <figref idref="DRAWINGS">FIG. 2</figref>. Typically, all pixels in an image will have the same undesirable hue, but the hue is more noticeable in white and near-white pixels than in pixels of other colors.</p>
<heading id="h-0013" level="1">Multi-Frame White Balancing</heading>
<p id="p-0027" num="0026">In one embodiment, the first, second, and third frames are separate frames that are acquired in a pre-capture movie mode. For example, when the user presses a button to take a picture, a digital camera might continuously acquire images at a rate of 2 to 16 frames per second, as if the camera were recording a movie. However, rather than store each of the frames, the camera may only store a single image. In such an embodiment, RGB<sub>avg</sub><sub><sub2>&#x2014;</sub2></sub><sub>prod </sub>and variance values can be determined from two separate frames acquired in the pre-capture mode, and the third frame can be the frame of the stored image. Assuming the camera acquires the frames at a sufficiently high rate, such as a few frames per second or higher, variations in the three frames will be minimal and not degrade the performance of the techniques described herein. In one embodiment, the techniques described herein can be implemented in real-time logic. When implementing the techniques described herein in real time logic, the RGB<sub>avg</sub><sub><sub2>&#x2014;</sub2></sub><sub>prod </sub>value and variance value can be determined based on the first and second frames, respectively, without the first and second frames ever being saved to a permanent memory.</p>
<heading id="h-0014" level="1">Single-Frame White Balancing</heading>
<p id="p-0028" num="0027">In one embodiment, the techniques of the present invention can be implemented based on a single frame. For example, when the techniques are implemented in software that is executed after an image has been capture, the processing need not be performed in real-time. Consequently, the first, second, and third frames do not need to be separate frames but can be separate frames. In some embodiments, the first, second, and third frames described herein will be the same frame.</p>
<heading id="h-0015" level="1">Near-Gray Pixels</heading>
<p id="p-0029" num="0028">In some embodiments, near-gray pixels can be used as the basis for white balancing operation, instead of or in addition to near-white pixels. Near-gray pixels can be identified by finding the pixels for the highest RGB<sub>product </sub>that correspond to a specific sum of RGB values (RGB<sub>sum</sub>). For example, a first pixel with R, G, and B values of 100, 25, and 75 and a second pixel with R, G, and B values of 66, 68, and 66 will both have RGB<sub>sum </sub>values of 200, but the first pixel will have an RGB<sub>product </sub>of 187,500 while the second pixel will have an RGB<sub>product </sub>of 296,208. The high RGB<sub>product </sub>of the second pixel can indicate that the second pixel is a near-gray pixel. The highest RGB<sub>product </sub>for a particular RGB<sub>sum </sub>occurs when R, G, and B values are equal. Thus, the RGB<sub>product </sub>can be used to identify near-gray pixels that can be included in the subset of pixels use to calculate gain factors for the white-balancing operation, in addition to or instead of near-white pixels.</p>
<heading id="h-0016" level="1">Example Mobile Device</heading>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 3</figref> illustrates a block diagram for an example mobile device <b>300</b> in which embodiments of the present invention may be implemented. Mobile device <b>300</b> comprises a camera assembly <b>302</b>, camera and graphics interface <b>380</b>, and a communication circuit <b>390</b>. Camera assembly <b>370</b> includes camera lens <b>336</b>, image sensor <b>372</b>, and image processor <b>374</b>. Camera lens <b>336</b>, comprising a single lens or a plurality of lenses, collects and focuses light onto image sensor <b>372</b>. Image sensor <b>372</b> captures images formed by light collected and focused by camera lens <b>336</b>. Image sensor <b>372</b> may be any conventional image sensor <b>372</b>, such as a charge-coupled device (CCD) or a complementary metal oxide semiconductor (CMOS) image sensor. Image processor <b>374</b> processes raw image data captured by image sensor <b>372</b> for subsequent storage in memory <b>396</b>, output to a display <b>326</b>, and/or for transmission by communication circuit <b>390</b>. The image processor <b>374</b> may be a conventional digital signal processor programmed to process image data, which is well known in the art.</p>
<p id="p-0031" num="0030">Image processor <b>374</b> interfaces with communication circuit <b>390</b> via camera and graphics interface <b>380</b>. Communication circuit <b>390</b> comprises antenna <b>312</b>, transceiver <b>393</b>, memory <b>396</b>, microprocessor <b>392</b>, input/output circuit <b>394</b>, audio processing circuit <b>306</b>, and user interface <b>397</b>. Transceiver <b>393</b> is coupled to antenna <b>312</b> for receiving and transmitting signals. Transceiver <b>393</b> is a fully functional cellular radio transceiver, which may operate according to any known standard, including the standards known generally as the Global System for Mobile Communications (GSM), TIA/EIA-36, cdmaOne, cdma2000, UMTS, and Wideband CDMA.</p>
<p id="p-0032" num="0031">The image processor <b>374</b> may process images acquired by the sensor <b>372</b> using one or more embodiments described herein. The image processor <b>374</b> can be implemented in hardware, software, or some combination of software and hardware. For example, the image processor <b>374</b> could be implemented as part of an application specific integrated circuit (ASIC). As another example, the image processor <b>374</b> may be capable of accessing instructions that are stored on a computer readable medium and executing those instructions on a processor, in order to implement one or more embodiments of the present invention.</p>
<p id="p-0033" num="0032">Microprocessor <b>392</b> controls the operation of mobile device <b>300</b>, including transceiver <b>393</b>, according to programs stored in memory <b>396</b>. Microprocessor <b>392</b> may further execute portions or the entirety of the image processing embodiments disclosed herein. Processing functions may be implemented in a single microprocessor, or in multiple microprocessors. Suitable microprocessors may include, for example, both general purpose and special purpose microprocessors and digital signal processors. Memory <b>396</b> represents the entire hierarchy of memory in a mobile communication device, and may include both random access memory (RAM) and read-only memory (ROM). Computer program instructions and data required for operation are stored in non-volatile memory, such as EPROM, EEPROM, and/or flash memory, which may be implemented as discrete devices, stacked devices, or integrated with microprocessor <b>392</b>.</p>
<p id="p-0034" num="0033">Input/output circuit <b>394</b> interfaces microprocessor <b>392</b> with image processor <b>374</b> of camera assembly <b>370</b> via camera and graphics interface <b>380</b>. Camera and graphics interface <b>380</b> may also interface image processor <b>374</b> with user interface <b>397</b> according to any method known in the art. In addition, input/output circuit <b>394</b> interfaces microprocessor <b>392</b>, transceiver <b>393</b>, audio processing circuit <b>306</b>, and user interface <b>397</b> of communication circuit <b>390</b>. User interface <b>397</b> includes a display <b>326</b>, speaker <b>328</b>, microphone <b>338</b>, and keypad <b>340</b>. Display <b>326</b>, disposed on the back of display section, allows the operator to see dialed digits, images, called status, menu options, and other service information. Keypad <b>340</b> includes an alphanumeric keypad and may optionally include a navigation control, such as joystick control (not shown) as is well known in the art. Further, keypad <b>340</b> may comprise a full QWERTY keyboard, such as those used with palmtop computers or smart phones. Keypad <b>340</b> allows the operator to dial numbers, enter commands, and select options.</p>
<p id="p-0035" num="0034">Microphone <b>338</b> converts the user's speech into electrical audio signals. Audio processing circuit <b>306</b> accepts the analog audio inputs from microphone <b>338</b>, processes these signals, and provides the processed signals to transceiver <b>393</b> via input/output <b>394</b>. Audio signals received by transceiver <b>393</b> are processed by audio processing circuit <b>306</b>. The basic analog output signals produced by processed audio processing circuit <b>306</b> are provided to speaker <b>328</b>. Speaker <b>328</b> then converts the analog audio signals into audible signals that can be heard by the user.</p>
<p id="p-0036" num="0035">Those skilled in the art will appreciate that one or more elements shown in <figref idref="DRAWINGS">FIG. 3</figref> may be combined. For example, while the camera and graphics interface <b>380</b> is shown as a separated component in <figref idref="DRAWINGS">FIG. 3</figref>, it will be understood that camera and graphics interface <b>380</b> may be incorporated with input/output circuit <b>394</b>. Further, microprocessor <b>392</b>, input/output circuit <b>394</b>, audio processing circuit <b>306</b>, image processor <b>374</b>, and/or memory <b>396</b> may be incorporated into a specially designed application-specific integrated circuit (ASIC) <b>391</b>.</p>
<heading id="h-0017" level="1">Example Computer System</heading>
<p id="p-0037" num="0036">According to one embodiment, the techniques described herein are implemented by one or more special-purpose computing devices. The special-purpose computing devices may be hard-wired to perform the techniques, or may include digital electronic devices such as one or more application-specific integrated circuits (ASICs) or field programmable gate arrays (FPGAs) that are persistently programmed to perform the techniques, or may include one or more general purpose hardware processors programmed to perform the techniques pursuant to program instructions in firmware, memory, other storage, or a combination. Such special-purpose computing devices may also combine custom hard-wired logic, ASICs, or FPGAs with custom programming to accomplish the techniques. The special-purpose computing devices may be desktop computer systems, portable computer systems, handheld devices, networking devices or any other device that incorporates hard-wired and/or program logic to implement the techniques.</p>
<p id="p-0038" num="0037">For example, <figref idref="DRAWINGS">FIG. 4</figref> is a block diagram that illustrates a computer system <b>400</b> upon which an embodiment of the invention may be implemented. Computer system <b>400</b> includes a bus <b>402</b> or other communication mechanism for communicating information, and a hardware processor <b>404</b> coupled with bus <b>402</b> for processing information. Hardware processor <b>404</b> may be, for example, a general purpose microprocessor.</p>
<p id="p-0039" num="0038">Computer system <b>400</b> also includes a main memory <b>406</b>, such as a random access memory (RAM) or other dynamic storage device, coupled to bus <b>402</b> for storing information and instructions to be executed by processor <b>404</b>. Main memory <b>406</b> also may be used for storing temporary variables or other intermediate information during execution of instructions to be executed by processor <b>404</b>. Such instructions, when stored in storage media accessible to processor <b>404</b>, render computer system <b>400</b> into a special-purpose machine that is customized to perform the operations specified in the instructions.</p>
<p id="p-0040" num="0039">Computer system <b>400</b> further includes a read only memory (ROM) <b>408</b> or other static storage device coupled to bus <b>402</b> for storing static information and instructions for processor <b>404</b>. A storage device <b>410</b>, such as a magnetic disk or optical disk, is provided and coupled to bus <b>402</b> for storing information and instructions.</p>
<p id="p-0041" num="0040">Computer system <b>400</b> may be coupled via bus <b>402</b> to a display <b>412</b>, such as a cathode ray tube (CRT), for displaying information to a computer user. An input device <b>414</b>, including alphanumeric and other keys, is coupled to bus <b>402</b> for communicating information and command selections to processor <b>404</b>. Another type of user input device is cursor control <b>416</b>, such as a mouse, a trackball, or cursor direction keys for communicating direction information and command selections to processor <b>404</b> and for controlling cursor movement on display <b>412</b>. This input device typically has two degrees of freedom in two axes, a first axis (e.g., x) and a second axis (e.g., y), that allows the device to specify positions in a plane.</p>
<p id="p-0042" num="0041">Computer system <b>400</b> may implement the techniques described herein using customized hard-wired logic, one or more ASICs or FPGAs, firmware and/or program logic which in combination with the computer system causes or programs computer system <b>400</b> to be a special-purpose machine. According to one embodiment, the techniques herein are performed by computer system <b>400</b> in response to processor <b>404</b> executing one or more sequences of one or more instructions contained in main memory <b>406</b>. Such instructions may be read into main memory <b>406</b> from another storage medium, such as storage device <b>410</b>. Execution of the sequences of instructions contained in main memory <b>406</b> causes processor <b>404</b> to perform the process steps described herein. In alternative embodiments, hard-wired circuitry may be used in place of or in combination with software instructions.</p>
<p id="p-0043" num="0042">The term &#x201c;storage media&#x201d; as used herein refers to any media that store data and/or instructions that cause a machine to operation in a specific fashion. Such storage media may comprise non-volatile media and/or volatile media. Non-volatile media includes, for example, optical or magnetic disks, such as storage device <b>410</b>. Volatile media includes dynamic memory, such as main memory <b>406</b>. Common forms of storage media include, for example, a floppy disk, a flexible disk, hard disk, solid state drive, magnetic tape, or any other magnetic data storage medium, a CD-ROM, any other optical data storage medium, any physical medium with patterns of holes, a RAM, a PROM, and EPROM, a FLASH-EPROM, NVRAM, any other memory chip or cartridge.</p>
<p id="p-0044" num="0043">Storage media is distinct from but may be used in conjunction with transmission media. Transmission media participates in transferring information between storage media. For example, transmission media includes coaxial cables, copper wire and fiber optics, including the wires that comprise bus <b>402</b>. Transmission media can also take the form of acoustic or light waves, such as those generated during radio-wave and infra-red data communications.</p>
<p id="p-0045" num="0044">Various forms of media may be involved in carrying one or more sequences of one or more instructions to processor <b>404</b> for execution. For example, the instructions may initially be carried on a magnetic disk or solid state drive of a remote computer. The remote computer can load the instructions into its dynamic memory and send the instructions over a telephone line using a modem. A modem local to computer system <b>400</b> can receive the data on the telephone line and use an infra-red transmitter to convert the data to an infra-red signal. An infra-red detector can receive the data carried in the infra-red signal and appropriate circuitry can place the data on bus <b>402</b>. Bus <b>402</b> carries the data to main memory <b>406</b>, from which processor <b>404</b> retrieves and executes the instructions. The instructions received by main memory <b>406</b> may optionally be stored on storage device <b>410</b> either before or after execution by processor <b>404</b>.</p>
<p id="p-0046" num="0045">Computer system <b>400</b> also includes a communication interface <b>418</b> coupled to bus <b>402</b>. Communication interface <b>418</b> provides a two-way data communication coupling to a network link <b>420</b> that is connected to a local network <b>422</b>. For example, communication interface <b>418</b> may be an integrated services digital network (ISDN) card, cable modem, satellite modem, or a modem to provide a data communication connection to a corresponding type of telephone line. As another example, communication interface <b>418</b> may be a local area network (LAN) card to provide a data communication connection to a compatible LAN. Wireless links may also be implemented. In any such implementation, communication interface <b>418</b> sends and receives electrical, electromagnetic or optical signals that carry digital data streams representing various types of information.</p>
<p id="p-0047" num="0046">Network link <b>420</b> typically provides data communication through one or more networks to other data devices. For example, network link <b>420</b> may provide a connection through local network <b>422</b> to a host computer <b>424</b> or to data equipment operated by an Internet Service Provider (ISP) <b>426</b>. ISP <b>426</b> in turn provides data communication services through the world wide packet data communication network now commonly referred to as the &#x201c;Internet&#x201d; <b>428</b>. Local network <b>422</b> and Internet <b>428</b> both use electrical, electromagnetic or optical signals that carry digital data streams. The signals through the various networks and the signals on network link <b>420</b> and through communication interface <b>418</b>, which carry the digital data to and from computer system <b>400</b>, are example forms of transmission media.</p>
<p id="p-0048" num="0047">Computer system <b>400</b> can send messages and receive data, including program code, through the network(s), network link <b>420</b> and communication interface <b>418</b>. In the Internet example, a server <b>430</b> might transmit a requested code for an application program through Internet <b>428</b>, ISP <b>426</b>, local network <b>422</b> and communication interface <b>418</b>.</p>
<p id="p-0049" num="0048">The received code may be executed by processor <b>404</b> as it is received, and/or stored in storage device <b>410</b>, or other non-volatile storage for later execution.</p>
<p id="p-0050" num="0049">In the foregoing specification, embodiments of the invention have been described with reference to numerous specific details that may vary from implementation to implementation. Thus, the sole and exclusive indicator of what is the invention, and is intended by the applicants to be the invention, is the set of claims that issue from this application, in the specific form in which such claims issue, including any subsequent correction. Any definitions expressly set forth herein for terms contained in such claims shall govern the meaning of such terms as used in the claims. Hence, no limitation, element, property, feature, advantage or attribute that is not expressly recited in a claim should limit the scope of such claim in any way. The specification and drawings are, accordingly, to be regarded in an illustrative rather than a restrictive sense.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A mobile digital device, comprising:
<claim-text>a camera assembly and graphics interface;</claim-text>
<claim-text>a communication circuit;</claim-text>
<claim-text>a processor; and</claim-text>
<claim-text>a memory having code embedded therein for programming the processor to:
<claim-text>for a pixel in a digital image, calculate a product of a red component value of the pixel, a green component value of the pixel, and a blue component value of the pixel;</claim-text>
<claim-text>in response to the product being greater than a threshold value, determine that the pixel belongs to a set of near-white pixels;</claim-text>
<claim-text>determine a gain factor based on red component values of the set, green component values of the set, and blue component values of the set; and</claim-text>
<claim-text>based at least in part on the gain factor, adjust at least one of the red component value of the pixel, the green component value of the pixel, and the blue component value of the pixel, and</claim-text>
<claim-text>wherein the threshold value is based at least in part on an average RGB product for a plurality of pixels, each pixel in the plurality having an RGB product.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The mobile digital device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the gain factor is determined based at least in part on normalizing an average red component value for the set of near-white pixels, an average green component value for the set of near-white pixels, and an average blue component value for the set of near-white pixels.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The mobile digital device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the threshold value is based at least in part on a variance value, the variance value determined based at least in part on the average RGB product.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The mobile digital device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the digital image is a third frame.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The mobile digital device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the memory further comprises code embedded therein for programming the processor to normalize an average red component value for the particular set of pixels, an average green component value for the particular set of pixels, and an average blue component value for the particular set of pixels.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The mobile digital device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the threshold value is based at least in part on an average RGB product for a plurality of pixels, each pixel in the plurality having an RGB product.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The mobile digital device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the average RGB product is determined based on pixel values in a first frame, and the variance is based on pixel values in a second frame that is different than the first frame.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The mobile device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the memory further comprises code embedded therein for programming the processor to exclude from the set one or more pixels that have a red component value, green component value, or blue component value greater than a second threshold value.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. A mobile digital device, comprising:
<claim-text>a camera assembly and graphics interface;</claim-text>
<claim-text>a communication circuit;</claim-text>
<claim-text>a processor; and</claim-text>
<claim-text>a memory having code embedded therein for programming the processor to:
<claim-text>for a pixel in a digital image, calculate a product of a red component value of the pixel, a green component value of the pixel, and a blue component value of the pixel;</claim-text>
<claim-text>in response to the product being greater than a threshold value, determine that the pixel belongs to a set of near-white pixels;</claim-text>
<claim-text>determine a gain factor based on red component values of the set, green component values of the set, and blue component values of the set; and</claim-text>
<claim-text>based at least in part on the gain factor, adjust at least one of the red component value of the pixel, the green component value of the pixel, and the blue component value of the pixel, and</claim-text>
<claim-text>wherein the average RGB product is determined based on pixel values in a first frame, and the variance is based on pixel values in a second frame that is different than the first frame.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The mobile digital device of <b>9</b>, wherein the digital image is a third frame.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The mobile digital device of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the gain factor is determined based at least in part on normalizing an average red component value for the set of near-white pixels, an average green component value for the set of near-white pixels, and an average blue component value for the set of near-white pixels.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The mobile digital device of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the threshold value is based at least in part on a variance value, the variance value determined based at least in part on the average RGB product.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The mobile digital device of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the memory further comprises code embedded therein for programming the processor to normalize an average red component value for the particular set of pixels, an average green component value for the particular set of pixels, and an average blue component value for the particular set of pixels.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The mobile device of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the memory further comprises code embedded therein for programming the processor to exclude from the set one or more pixels that have a red component value, green component value, or blue component value greater than a second threshold value.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. A mobile digital device, comprising:
<claim-text>a camera assembly and graphics interface;</claim-text>
<claim-text>a communication circuit;</claim-text>
<claim-text>a processor; and</claim-text>
<claim-text>a memory having code embedded therein for programming the processor to:
<claim-text>for a pixel in a digital image, calculate a product of a red component value of the pixel, a green component value of the pixel, and a blue component value of the pixel;</claim-text>
<claim-text>in response to the product being greater than a threshold value, determine that the pixel belongs to a set of near-white pixels;</claim-text>
<claim-text>determine a gain factor based on red component values of the set, green component values of the set, and blue component values of the set;</claim-text>
<claim-text>based at least in part on the gain factor, adjust at least one of the red component value of the pixel, the green component value of the pixel, and the blue component value of the pixel; and</claim-text>
<claim-text>exclude from the set one or more pixels that have a red component value, green component value, or blue component value greater than a second threshold value.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The mobile digital device of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the gain factor is determined based at least in part on normalizing an average red component value for the set of near-white pixels, an average green component value for the set of near-white pixels, and an average blue component value for the set of near-white pixels.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The mobile digital device of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the threshold value is based at least in part on a variance value, the variance value determined based at least in part on the average RGB product.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The mobile digital device of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the digital image is a third frame.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The mobile digital device of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the memory further comprises code embedded therein for programming the processor to normalize an average red component value for the particular set of pixels, an average green component value for the particular set of pixels, and an average blue component value for the particular set of pixels.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The mobile digital device of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the average RGB product is determined based on pixel values in a first frame, and the variance is based on pixel values in a second frame that is different than the first frame.</claim-text>
</claim>
</claims>
</us-patent-grant>
