<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08622548-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08622548</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13299219</doc-number>
<date>20111117</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<us-term-of-grant>
<us-term-extension>114</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>A</section>
<class>61</class>
<subclass>B</subclass>
<main-group>3</main-group>
<subgroup>14</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>A</section>
<class>61</class>
<subclass>B</subclass>
<main-group>3</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>351206</main-classification>
<further-classification>351210</further-classification>
<further-classification>351246</further-classification>
</classification-national>
<invention-title id="d2e53">3D retinal disruptions detection using optical coherence tomography</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>7301644</doc-number>
<kind>B2</kind>
<name>Knighton et al.</name>
<date>20071100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>356479</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>7480058</doc-number>
<kind>B2</kind>
<name>Zhao et al.</name>
<date>20090100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>356479</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>7505142</doc-number>
<kind>B2</kind>
<name>Knighton et al.</name>
<date>20090300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>356479</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>7744221</doc-number>
<kind>B2</kind>
<name>Wei et al.</name>
<date>20100600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>356479</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>7782464</doc-number>
<kind>B2</kind>
<name>Mujat et al.</name>
<date>20100800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2007/0115481</doc-number>
<kind>A1</kind>
<name>Toth et al.</name>
<date>20070500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2007/0291277</doc-number>
<kind>A1</kind>
<name>Everett et al.</name>
<date>20071200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2008/0175465</doc-number>
<kind>A1</kind>
<name>Jiang et al.</name>
<date>20080700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2008/0187204</doc-number>
<kind>A1</kind>
<name>Reeves et al.</name>
<date>20080800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2009/0198094</doc-number>
<kind>A1</kind>
<name>Fenster et al.</name>
<date>20090800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2011/0103658</doc-number>
<kind>A1</kind>
<name>Davis et al.</name>
<date>20110500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382128</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2011/0243415</doc-number>
<kind>A1</kind>
<name>Yonezawa et al.</name>
<date>20111000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382131</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2012/0327368</doc-number>
<kind>A1</kind>
<name>Williams et al.</name>
<date>20121200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>351221</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>WO</country>
<doc-number>WO-2010/071898</doc-number>
<kind>A2</kind>
<date>20100600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>WO</country>
<doc-number>WO-2010/080576</doc-number>
<kind>A1</kind>
<date>20100700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00016">
<othercit>PCT International Search Report and the Written Opinion dated Mar. 20, 2012, in related International Appl. No. PCT/US2011/061252.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00017">
<othercit>De Bruin et al., &#x201c;In Vivo Three-Dimensional Imaging of Neovascular Age-Related Macular Degeneration Using Optical Frequency Domain Imaging at 1050 nm&#x201d;, <i>Investigative Ophthalmology </i>&#x26; <i>Visual Science</i>, Oct. 2008, vol. 49, No. 10, pp. 4545-4552.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00018">
<othercit>Z. Yehoshua et al., &#x201c;Comparison of Drusen Area Detected by Spectral Domain OCT and Color Fundus Photography&#x201d;, ARVO 2010, Presentation Abstract, Paper 2274, Apr. 3, 2013.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00019">
<othercit>S.R. Freeman et al., &#x201c;Optical Coherence Tomography&#x2014;Raster Scanning and Manual Segmentation in Determining Drusen Volume in Age-Related Macular Degeneration&#x201d;, Retina, Dec. 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00020">
<othercit>N. Jain et al., &#x201c;Quantitative Comparison of Drusen Segmented on SD-OCT versus Drusen Delineated on Color Fundus Photographs&#x201d;, IOVS, vol. 51, No. 10, Oct. 2010.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00021">
<othercit>K. Yi et al., &#x201c;Spectral domain optical coherence tomography for quantitative evaluation of drusen and associated structural changes in non-neovascular age-related macular degeneration&#x201d;, Br J Ophthalmol., 93(2): 176-181, Feb. 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00022">
<othercit>S. Farsiu et al., &#x201c;Fast Detection and Segmentation of Drusen in Retinal Optical Coherence Tomography Images&#x201d;, Proc. of Photonics West, San Jose, CA, Feb. 15, 2008.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00023">
<othercit>J.A. Noble et al., &#x201c;Ultrasound Image Segmentation: A Survey&#x201d;, IEEE Transactions on Medical Imaging, vol. 25, No. 8, pp. 987-1010, Apr. 15, 2006.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00024">
<othercit>N. Otsu, &#x201c;A Threshold Selection Method from Gray-Level Histograms&#x201d;, IEEE Transactions on Systems, Man, and Cybernetics, vol. 9, No. 1, pp. 62-66, 1979, Jun. 1, 1979.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00025">
<othercit>L. Vincent et al., &#x201c;Watersheds in Digital Spaces: an efficient algorithm based on immersion simulations&#x201d;, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 13, No. 6, pp. 583-598, Jun. 1991.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00026">
<othercit>Y. Cheng, &#x201c;Mean shift, mode seeking and clustering&#x201d;, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 17, No. 8, Aug. 1995.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00027">
<othercit>R. Adams et al., &#x201c;Seeded Region Growing&#x201d;, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 16, pp. 641-647, Jun. 1994.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00028">
<othercit>M. Szkulmowski et al., &#x201c;Analysis of posterior retinal layers in spectral optical coherence tomography images of the normal retina and retinal pathologies&#x201d;, Journal of Biomedical Optics 12(4), Jul.-Aug. 2007.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00029">
<othercit>R. Leitgeb et al., &#x201c;Real-time measurement of in-vitro and in-vivo blood flow with Fourier domain optical coherence tomography&#x201d;, SPIE 5316, 226-232, Jul. 1, 2004.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00030">
<othercit>M. Wojtkowski et al., &#x201c;Real-time and static in vivo ophthalmic imaging by spectral optical coherence tomography&#x201d;, SPIE 5314, 126-131, Jul. 2004.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00031">
<othercit>C. Hitzenberger et al., &#x201c;Three-dimensional imaging of the human retina by high-speed optical coherence tomography&#x201d;, Optics Express, vol. 11, Issue 21, pp. 2753-2761, Oct. 20, 2003.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00032">
<othercit>International Search Report and Written Opinion mailed Dec. 23, 2010, in International Application No. PCT/US2010/053603.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00033">
<othercit>International Preliminary Report on Patentability mailed May 10, 2012, in International Application No. PCT/US2010/053603.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00034">
<othercit>International Preliminary Report on Patentability mailed May 30, 2013, in related International Application No. PCT/US2011/061252.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>26</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>351200-246</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>11</number-of-drawing-sheets>
<number-of-figures>11</number-of-figures>
</figures>
<us-related-documents>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>61414805</doc-number>
<date>20101117</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120127427</doc-number>
<kind>A1</kind>
<date>20120524</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Guo</last-name>
<first-name>Cheng-en</first-name>
<address>
<city>Santa Clara</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Jang</last-name>
<first-name>Ben</first-name>
<address>
<city>Cupertino</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Li</last-name>
<first-name>Wenjing</first-name>
<address>
<city>Fremont</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Guo</last-name>
<first-name>Cheng-en</first-name>
<address>
<city>Santa Clara</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Jang</last-name>
<first-name>Ben</first-name>
<address>
<city>Cupertino</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Li</last-name>
<first-name>Wenjing</first-name>
<address>
<city>Fremont</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Haynes and Boone, LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Optovue, Inc.</orgname>
<role>02</role>
<address>
<city>Fremont</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Hasan</last-name>
<first-name>Mohammed</first-name>
<department>2872</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">System and method for 3D retinal disruption/elevation detection, measurement and presentation using Optical Coherence Tomography (OCT) are provided. The present invention is capable of detecting and measuring the abnormal changes of retinal layers (retinal disruptions), caused by retinal diseases, such as hard drusen, soft drusen, Pigment Epithelium Detachment (PED), Choroidal Neovascularization (CNV), Geographic Atrophy (GA), intra retinal fluid space, and exudates etc. The presentations of the results are provided with quantitative measurements of disruptions in retina and can be used for diagnosis and treatment of retinal diseases.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="103.04mm" wi="201.00mm" file="US08622548-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="284.06mm" wi="162.90mm" file="US08622548-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="225.55mm" wi="208.03mm" file="US08622548-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="160.78mm" wi="169.16mm" file="US08622548-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="202.44mm" wi="180.00mm" file="US08622548-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="152.82mm" wi="149.61mm" file="US08622548-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="201.76mm" wi="186.35mm" file="US08622548-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="117.35mm" wi="205.23mm" file="US08622548-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="209.80mm" wi="202.10mm" file="US08622548-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="207.01mm" wi="192.62mm" file="US08622548-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="133.77mm" wi="164.93mm" file="US08622548-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="254.25mm" wi="126.41mm" file="US08622548-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application claims priority to U.S. Provisional Application 61/414,805, filed on Nov. 17, 2010, which is herein incorporated by reference in its entirety.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">BACKGROUND</heading>
<p id="p-0003" num="0002">1. Field of Invention</p>
<p id="p-0004" num="0003">The embodiments described herein relates to methods and systems to detect and measure the retinal disruption/elevation of optical coherence tomography (OCT) data, as well as to present the detection and measurement results using 3D OCT data.</p>
<p id="p-0005" num="0004">2. Background State of the Arts</p>
<p id="p-0006" num="0005">Optical Coherence Tomography (OCT) has been an important modality for imaging eyes and facilitating ophthalmologists to diagnose and treat subjects with different eye diseases, retinal diseases in particular. The importance of OCT to the field of ophthalmology has also dramatically increased since Fourier Domain OCT (FD-OCT) became commercially available. FD-OCT has much higher scanning speed and higher resolution than the traditional Time Domain OCT (TD-OCT) technologies.</p>
<p id="p-0007" num="0006">One of the major pathologic changes for retinal subjects is retinal layers disruption from their normal locations, especially around the Retinal Pigment Epithelium (RPE) and Photoreceptor Inner Segment/Outer Segment (PR-IS/OS) area. Quantitative measurements of such disruptions provide important information for ophthalmologists to diagnose and treat patients.</p>
<p id="p-0008" num="0007">Previous methods using 3D OCT data follow the same scheme of first segmenting retinal layers, and then detecting the disruption (e.g. drusen) by comparing the segmented layers with expected referenced layers or with some layers which are elevated from the segmented layers by some constants. The referenced layers are often generated by fitting some smooth surfaces to the segmented layers, assuming the layers are not disrupted by any disease or pathology. In general, the presence of a disruption is determined by only comparing two 2D surfaces; this means the original 3D OCT data is not fully utilized after the layer segmentations have been performed. Such scheme has at least four major drawbacks. First, such detection methods are error prone because they are highly dependent on results of layer segmentations. If the 2D surface segmentation is not optimal, the disruption detection will be directly affected and likely produce inaccurate results. Second, to reduce noise effects associated with OCT data, layer segmentation often employs smoothing operation which can likely introduce the problem of scale. Excessive smoothing (such as the case with a large smoothing scale) will likely reduce details in desired features, while insufficient smoothing (with a small smoothing scale) will likely be inadequate to reduce noise effectively to generate optimal layer segmentations. Third, methods assuming constant elevations from segmented layers are less clinically meaningful because disruptions often occur locally with different and unpredictable sizes. Finally, a majority of existing methods only detect disruptions above the referenced layers, and any disruptions under the referenced layers are ignored. Since disruptions can occur above and below the reference layers of interest, it is important to devise a method to detect and measure disruptions in both scenarios.</p>
<heading id="h-0003" level="1">SUMMARY</heading>
<p id="p-0009" num="0008">This Summary is provided to briefly indicate the nature and substance of the invention. It is submitted with the understanding that it will not be used to interpret or limit the scope or meaning of the claims.</p>
<p id="p-0010" num="0009">In accordance with some embodiments of the present invention, an imaging apparatus includes an optical source, an x-y scanner receiving light from the optical source and directing it onto a sample, a detector receiving reflected light from the scanner, and a computer receiving a signal from the detector and providing a 3D data set containing voxels of a sample, the computer further executing instructions for processing the 3D data set, identifying one or more 3D seeds from the 3D data set, performing image processing to obtain characteristics of 3D disruptions from the 3D seeds, generating measurements for the 3D disruptions, and displaying the results of the 3D disruptions.</p>
<p id="p-0011" num="0010">An image processing method according to some embodiments of the present invention includes acquiring a 3D data set utilizing an OCT system, processing the 3D data set, identifying one or more 3D seeds from the 3D data set, performing image processing to obtain characteristics of 3D disruptions utilizing the 3D seeds, generating measurements for the 3D disruptions, and displaying the results of the 3D disruptions.</p>
<p id="p-0012" num="0011">These and other embodiments are further discussed below with respect to the following figures.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 1</figref> is an exemplary flowchart of some embodiments of the present invention.</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 2</figref> is an example of a clinically useful presentation scheme for a disruption report.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 3</figref> shows an example of a color fundus photograph with drusen.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 4</figref> shows examples of retinal disruptions in en face images with different offsets in the z directions.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 5</figref> is an example of an Intelligent Fundus Image (IFI) of some embodiments of the present invention.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 6</figref> is an exemplary presentation scheme for a progression report.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 7</figref> is a schematic illustration of an Optical Coherence Tomography (OCT) scanner.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 8</figref><i>a</i>-<i>d </i>show example images used for 3D disruption seeds generation: (a) Adaptive Seed Search Image (AS SI) using elevation map between inner segment and outer segment (IS/OS) and Retinal Pigment Epithelium (RPE) fit used for 3D disruption seeds generation (b) ASSI to generate drusen/Pigment Epithelium Detachment (PED) 3D disruption seeds; (c) ASSI to generate geographic atrophy 3D disruption seeds; and (d) example 3D disruption seeds for Geographic Atrophy (GA) detection.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 9</figref><i>a</i>-<i>d </i>show exemplary images of disruption region: (a) before disruption post processing; (b) after disruption post processing; (c) user defined sensitivity at 0.5; and (d) user defined sensitivity at 1.0.</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 10</figref> is an exemplary plot of the relationship between the size of the disruption and its distance to the center of the fovea.</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 11</figref> shows exemplary displays of 3D disruption clouds (a) at RPE-fit layer, and (b) at RPE layer.</p>
<p id="p-0024" num="0023">Where appropriate, elements having the same or similar functions have the same element designation. Figures are not to scale and do not illustrate relative sizes.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0025" num="0024">The aspects and embodiments of the invention disclosed herein relate to a computer-aided detection and measurement system and method to detect and measure the retinal disruption (or elevation), as well as to present the results using the OCT 3D volume data.</p>
<p id="p-0026" num="0025">In some embodiments, the retinal disruptions are detected by 3D region growing methods with different constraints, such as 3D shape, size, intensity and texture, 3D labeled mask generation, and Intelligent Fundus Image (IFI) construction.</p>
<p id="p-0027" num="0026">In some embodiments, the 3D disruption seeds for the 3D region growing are obtained by a method which contains retinal layer segmentation of Inner Limiting Membrane (ILM), Inner Plexiform Layer (IPL), Outer Plexiform Layer (OPL), Photoreceptor Inner Segment/Outer Segment (IS/OS), and Retinal Pigment Epithelium (RPE) layers, normal RPE layer construction, IS/OS elevation map construction, Adaptive Seed Search Image (ASSI) construction, and 3D disruption seeds detection from all of the above information as well as the OCT 3D volume data.</p>
<p id="p-0028" num="0027">In some embodiments, based on the detected disruption, the following quantitative measurements are performed: the number of disruptions, the 3D boundary of each disruption, the size (diameter, area and volume) of each disruption, the distribution of the disruptions, the sum of the disruptions in size in defined regions, e.g. the standard ETDRS (Early Treatment Diabetic Retinopathy Study) sectors, and the change of these measurements over time.</p>
<p id="p-0029" num="0028">In some embodiments, an interactive Graphical User Interface (GUI) is provided to display the above measurements as well as for changing the intermediate results, e.g. segmented retinal layers, disruptions seeds, and disruption boundaries in 3D, to correct some errors from the auto-method to obtain more accurate results. In some embodiments, progression analysis is performed and the report is provided when multiple datasets are available.</p>
<p id="p-0030" num="0029">The approaches discussed herein can be implemented on a device for measuring samples using optical coherence tomography (OCT). One such device has been commercially available in the US and internationally by the assignee herein under the trademark RTVue&#xae;. A more compact version of such device has also been commercially available in the US and internationally by the assignee herein under the trademark iVue&#xae;. Both the RTVue&#xae; and the iVue&#xae; are frequency domain OCT system with a broadband light source and a spectrometer, and acquire OCT data sets with both high definition scan(s) and lower resolution data cubes within a short period of time capable of clinical interpretation and diagnosis by clinicians. Embodiments described in this disclosure can be applied to any imaging devices and are not limited to the OCT technology discussed above.</p>
<p id="p-0031" num="0030">A system and method for retinal disruption detection, measurement and presentation using a 3D data set acquired using an OCT scanner is disclosed. Embodiments of the present invention can be utilized to facilitate diagnosis and treatment of retinal diseases with quantitative measurements of disruptions in retina.</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 7</figref> illustrates an example of an OCT imager <b>700</b> that can be utilized in 3D retinal disruption detection according to some embodiments of the present invention. OCT imager <b>700</b> includes light source <b>701</b> supplying light to coupler <b>703</b>, which directs the light through the sampling arm to XY scan <b>704</b> and through the reference arm to optical delay <b>705</b>. XY scan <b>704</b> scans the light across eye <b>709</b> and collects the reflected light from eye <b>709</b>. Light reflected from eye <b>709</b> is captured in XY scan <b>704</b> and combined with light reflected from optical delay <b>705</b> in coupler <b>703</b> to generate an interference signal. The interference signal is coupled into detector <b>702</b>. OCT imager <b>700</b> can be a time domain OCT imager, in which case depth (or A-scans) are obtained by scanning optical delay <b>705</b>, or a Fourier domain imager, in which case detector <b>702</b> is a spectrometer that captures the interference signal as a function of wavelength. In either case, the OCT A-scans are captured by computer <b>708</b>. Collections of A-scans taken along an XY pattern are utilized in computer <b>708</b> to generate 3-D OCT data sets. Computer <b>708</b> can also be utilized to process the 3-D OCT data sets into 2-D images according to some embodiments of the present invention. Computer <b>708</b> can be any device capable of processing data and may include any number of processors or microcontrollers with associated data storage such as memory or fixed storage media and supporting circuitry.</p>
<p id="p-0033" num="0032">In further illustration, <figref idref="DRAWINGS">FIG. 1</figref> is an exemplary flowchart of some embodiments of the present invention. In the flowchart illustrated in <figref idref="DRAWINGS">FIG. 1</figref>, there are ten steps of the present retinal disruption detection, measurement and presentation method, namely, 1) 3D disruption seeds detection <b>100</b>, including five sub-steps <b>101</b>-<b>105</b>; 2) 3D region growing <b>106</b>; 3) 3D labeled mask generation <b>107</b>; 4) Intelligent Fundus Image (IFI) construction <b>108</b>; 5) disruption region post-processing <b>109</b>; 6) optional disruption sensitivity calculation <b>110</b>; 7) measurements assessment <b>111</b>; 8) optional interactive presentation <b>112</b>; 9) final presentation and report <b>113</b>, and 10) optional progression analysis <b>114</b>.</p>
<p id="h-0006" num="0000">3D Disruption Seeds Detection</p>
<p id="p-0034" num="0033">The first step <b>100</b> of the flowchart in <figref idref="DRAWINGS">FIG. 1</figref> is to facilitate the detection of 3D seeds of potential disruptions in the retina, wherein disruptions seeds are detected and identified for subsequent 3D region growing to determine a final 3D disruption volume in the 3D data set. As shown in <figref idref="DRAWINGS">FIG. 1</figref>, step <b>100</b> includes five sub-steps for preparing and identifying the seeds and they are described in detailed below.</p>
<p id="p-0035" num="0034">In sub-step <b>101</b>, segmentation for different retinal layers of a subject eye, such as Inner Limiting Membrane (ILM), Inner Plexiform Layer (IPL), Outer Plexiform Layer (OPL), Photoreceptor Inner Segment/Outer Segment (PR-IS/OS), Retinal Pigment Epithelium (RPE), choroid boundaries, or other layers of interest, can be performed. Layer segmentation is commonly performed on measurement data acquired using the OCT technology and numerous methods have been used to achieve retinal layer segmentation in OCT data set. Some well-known methods are graph-cut, active contour model (snake), level set theory, and dynamic programming (see for example, J. A. Noble et. al., &#x201c;Ultrasound Image Segmentation: A Survey&#x201d;, <i>IEEE Transactions on Medical Imaging</i>, vol. 25, no. 8, pp. 987-1010, 2006).</p>
<p id="p-0036" num="0035">The second sub-step <b>102</b> is to determine a segmentation curve or surface as a representation of a normal retinal layer, e.g., RPE layer, location by assuming the layer is not disrupted by any disease or pathology; hence, points of disruption suspects on the layer are not considered. For example, this representation of a normal RPE layer location is called a &#x201c;RPE-fit&#x201d;. In some embodiments, a RPE-fit is assumed to be convex and smooth in local segments and free of disruptions. After excluding the suspected pathological segments, the RPE-fit can be fitted by low order polynomials to represent a normal RPE layer. For instance, second order or third order polynomials are sufficient to achieve an ideal RPE-fit layer. The idea of fitting the RPE surface to the actual RPE from SD-OCT image volume was presented soon after the SD-OCT was developed and methods for line or surface fitting are well-known in the art (see for example, M. Szkulmowski et. Al, &#x201c;Analysis of posterior retinal layers in spectral optical coherence tomography images of the normal retina and retinal pathologies&#x201d;, Journal of Biomedical Optics 12(4), 2007).</p>
<p id="p-0037" num="0036">The third sub-step <b>103</b> is to create an elevation map from the layers of interest. For example, an elevation map of the IS/OS can be created to help identify seed candidates needed for subsequent 3D retinal disruption detection. In some embodiments, the IS/OS segmented layer from step <b>101</b> is compared to the RPE-fit generated in step <b>102</b> to create an elevation map of IS/OS. <figref idref="DRAWINGS">FIG. 8</figref><i>a </i>shows an example of an elevation map between IS/OS and RPE-fit that can be used to detect the 3D disruption seeds for drusens with further processing step in accordance with some embodiments of this invention.</p>
<p id="p-0038" num="0037">The fourth sub-step <b>104</b> is to identify seed candidates by evaluating the OCT data distribution for each A-scan between the IS/OS segmented layer step <b>101</b> and the RPE-fit layer step <b>102</b>, or below the RPE-fit layer, depending on the type of disruptions. The evaluation of each A-scan helps confine the search range for the seed candidates and results in an image, called an &#x201c;Adaptive Seed Search Image&#x201d; (AS SI). The ASSI enhances the bright regions in the image for seed candidates that can be used to identify potential locations of retina disruptions with further processing.</p>
<p id="p-0039" num="0038">In some embodiments, sub-step <b>104</b> can be performed without performing sub-step <b>103</b>. For example, to detect GA, the 3D disruption seeds can be detected directly from an ASSI constructed below the RPE-fit layer. The elevation map constructed between IS/OS to RPE fit in step <b>103</b> can later be used to further refine the 3D disruption detection constraints and the GA detection results.</p>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. 8</figref><i>b </i>is an example ASSI by performing further image processing, such as edge detection, on an en face image to enhance the disruption regions <b>810</b> used in the seeds detection step <b>105</b>. <figref idref="DRAWINGS">FIG. 8</figref><i>c </i>is another example ASSI generated by performing further image processing, such as commonly known smoothing filter(s), on an en face image to enhance the disruption regions <b>820</b> used to detect a seed in the case of GA disruption. <figref idref="DRAWINGS">FIG. 8</figref><i>d </i>is an exemplary 2D disruption seeds image generated by locating the regions with value higher than a selected threshold in <figref idref="DRAWINGS">FIG. 8</figref><i>c</i>.</p>
<p id="p-0041" num="0040">In some embodiments, local intensity maxima can be extracted from the IS/OS elevation map and the x-y positions of these local maxima can then be considered as the x-y positions of the seed candidates of retinal disruptions for further 3D seed detection in step <b>105</b>. The local intensity maxima can be detected using commonly used 2D image segmentation algorithm, such as, Otsu adaptive thresholding (Otsu, N, &#x201c;a threshold selection method from gray-level histograms&#x201d;, IEEE Transactions on Systems, Man, and Cybernetics, Vol. 9, No. 1, pp. 62-66, 1979), a marker controlled watershed transform (Vincent, L and Pierre, S, &#x201c;Watersheds in Digital Spaces: an efficient algorithm based on immersion simulations&#x201d;, IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 13, No. 6, pp. 583-598, 1991.), or a kernel based mean shift clustering (Cheng, Yizong, &#x201c;Mean shift, mode seeking and clustering&#x201d;, IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 17, No. 8, 1995). In some embodiments, the resulting local intensity maxima (brightest points) can be extracted from the ASSI obtained in step <b>104</b> and their respective x-y positions of these local maxima can be selected as the 2D seed candidates of 3D retinal disruptions segmentation.</p>
<p id="p-0042" num="0041">The last sub-step <b>105</b> of step <b>100</b> is to detect 3D disruption seeds by incorporating the 3D OCT data set, the 2D seed candidates from the elevation map of IS/OS map construction step <b>103</b>, and/or the 2D seed candidates from the ASSI construction step <b>104</b>. The previous steps demonstrate the steps to detect the x-y positions of the seed candidates according to an embodiment in the exemplary flowchart in <figref idref="DRAWINGS">FIG. 1</figref>. To obtain seed candidates in a 3D data set, the z positions of the seed candidates need to be determined, for example, by searching the seed candidates from steps <b>103</b> and <b>104</b> along the A-scan direction with the x-y positions identified in steps <b>103</b> and <b>104</b>. In some embodiments, 3D voxels with the most expected intensity values can be selected to be the 3D seeds. Other common known methods for a person of ordinary skills in the art can be used to identify the z positions using the x-y positions obtained from previous steps. This method effectively determines the x, y and z locations of the 3D seeds to facilitate 3D retinal disruption detection.</p>
<p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. 4</figref> shows examples of retinal disruptions in en face images with different offsets in the z directions. Different techniques have been used to generate en face images (see for example U.S. application Ser. No. 12/909,648). Image <b>410</b> shows an en face image obtained from the entire volume of a 3D data set. Image <b>420</b> shows an image with 55 um offset from the RPE-fit. Drusens <b>425</b> are indicated as the bright spots or brighter boundary surrounding darker area in the image <b>420</b>. Image <b>430</b> is an image from the same 3D data set used to generate image <b>410</b> with 54 um offset from the RPE-fit. Drusens <b>435</b> are again the bright spots in the image <b>430</b>. Image <b>440</b> is another image using the same 3D data set with 74 um offset from the RPE-fit, with drusens <b>445</b> as the bring spots. The characteristics of the drusens <b>425</b>, <b>435</b> and <b>445</b> changes from images <b>420</b>, <b>430</b> and <b>440</b>, with different offset amounts from the RPE-fit. Using this simple approach as shown in <figref idref="DRAWINGS">FIG. 4</figref>, the detection of the actual drusens is likely to be error prone and non-reproducible because the characteristics of drusens in the images are highly dependent on the amount of offsets as shown herein. Therefore, 3D segmentation methods used for 3D retinal disruption detection can significantly improve clinical usefulness of the OCT data.</p>
<p id="p-0044" num="0043">The embodiments disclosed herein do not simply use 2D segmented surfaces to identify the disruptions directly, but utilize the 2D segmented surfaces as intermediate results to identify local 3D seeds. Based on the local 3D seeds from determined in step <b>105</b>, 3D region growing techniques can then be applied to detect areas of interest more reliably, especially disruptions of various sizes and locations. Therefore, the accuracy of 2D surface segmentation is less critical as compared to some current methods discussed above.</p>
<p id="h-0007" num="0000">3D Region Growing</p>
<p id="p-0045" num="0044">In the exemplary flowchart illustrated in <figref idref="DRAWINGS">FIG. 1</figref>, the next step, step <b>106</b>, is to perform 3D region growing in the 3D OCT data set. Segmentation methods using 2D region growing are well known in the field (see for example, Adam and Bischof, &#x201c;Seeded Region Growing,&#x201d; <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, vol. 16, pp. 641-647, 1994). 3D region growing in step <b>106</b> can be performed using the 3D seeds identified in step <b>105</b> with region growing constraints. Some examples of region growing constraints are 3D shape and size determined by clinical understanding of pathologies and image properties of the OCT data set. For instance, many of the retinal disruptions/pathologies, such as, hard and soft drusen, PED, and CNV, form a half-spherical shape above a normal RPE location (RPE-fit) in most cases. Also, image characteristics of the OCT data set, such as area of high reflectivity and salt-pepper texture features, are commonly observed in hard drusen formation. A 3D region growing method utilizing these constraints improves the robustness and accuracy of the region growing process and substantially reduces some commonly known leaking problem often encountered in region growing techniques. In some embodiments, 3D region growing step <b>106</b> utilizing the above mentioned constraints using the 3D seeds identified in step <b>105</b> is capable of detecting disruptions with diameter as small as 60 um.</p>
<p id="p-0046" num="0045">Alternatively, in situations where processing time is critical, the 3D disruption regions can be detected using other 3D segmentation techniques other than the 3D region growing method. In some embodiments, 3D segmentation can be performed by interpolating multiple 2D cross-sectional regions at different z distance based on the 3D seeds identified in step <b>105</b>, similar to piecing together volume using contour map. Some commonly used interpolation methods, such as, linearly, bi-linear, b-spline interpolation can be employed. In some other embodiments, to further reduce process time and computation burden, enhanced 2D retinal disruption can be obtained using the ASSI. For example, ASSI can be constructed from a certain clinically meaningful pre-selected range of z positions, in addition to the adaptive local seed identifications discussed in steps <b>101</b> to <b>105</b>. A 2D region segmentation algorithm (for example, marker-controlled watershed segmentation) can then be applied to generate a 2D counterpart of the 3D labeled disruption mask as in step <b>107</b>. In some embodiments, for drusen and PED type of disruption, the ASSI can be constructed from 100 um to 30 um above the RPE-fit layer. For Geographic Atrophy (GA) type of disruption, the ASSI can be constructed from 100 um to 400 um below the RPE-fit layer. Additional ASSI image can also be constructed from 100 um above IS/OS layer to IS/OS layer as reference of reflectivity.</p>
<p id="h-0008" num="0000">3D Labeled Mask Generation</p>
<p id="p-0047" num="0046">The next step <b>107</b> is to label voxels of each of the 3D connected objects segmented using the 3D region growing step <b>106</b>. For example, the labeling procedure can be as discussed below. First, all voxels filled by the 3D region growing step <b>106</b> can be initially assigned a mask value N larger than the maximum allowable number of 3D connected objects identified in step <b>106</b> (e.g., N=255, assuming there is less than 255 3D connected objects in the 3D data set); while voxels not filled in step <b>106</b> are assigned values of 0. Next, an iterative process can be used to label each of the 3D connected objects with a unique value. The iterative process can have k iterative steps, where 1&#x2266;k&#x2266;K, and K is the total number of 3D connected objects from step <b>106</b>. A commonly used flood-fill algorithm can also be applied to a randomly selected voxel having a mask value N to search for all mask voxels connected to this selected voxel. New mask values k will be assigned to these connected voxels having mask values N previously. After this first iteration, k will be incremented (e.g. k=k+1) and the flood-fill algorithm will be applied to another randomly selected voxel having a mask value N to search for voxels connected to this randomly selected voxel. The mask values of these voxels will then be updated to the incremented k value. This iteration process continues and stops at the K<sup>th </sup>iteration, where there is no more voxel with the mask value initially assigned mask value N. This process labels the K total number of 3D connected objects in the 3D data set to facilitate further processing steps <b>108</b>-<b>110</b> and to evaluate subsequent disruption measurements in step <b>111</b>.</p>
<p id="h-0009" num="0000">Intelligent Fundus Image (IFI) Construction</p>
<p id="p-0048" num="0047">Based on the 3D labeled mask inform step <b>107</b>, an Intelligent Fundus Image (IFI) can be constructed in step <b>108</b>. 3D data smoothing and enhancement techniques such as nonlinear filtering and histogram equalization can be applied, depending on the application at hand, to one or more specific 3D connected objects to generate the optimal fundus image representation. An example of such IFI is shown in <figref idref="DRAWINGS">FIG. 5</figref>. The IFI image as shown in <figref idref="DRAWINGS">FIG. 5</figref> resolves the problem of the sensitivity of different arbitrary offset amounts illustrated in <figref idref="DRAWINGS">FIG. 4</figref>. The IFI image as a result of the process described above produces constant and more accurate retinal disruptions characteristics. Image <b>510</b> is an image generated from the same 3D data set used for image <b>410</b>. The characteristics of drusens <b>520</b> are more pronounced and can be more clearly identified than drusens <b>425</b>, <b>435</b>, or <b>445</b> illustrated in images <b>420</b>, <b>430</b>, and <b>440</b>, respectively, of <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="h-0010" num="0000">Disruption Region Post-Processing</p>
<p id="p-0049" num="0048">In some embodiments, disruption region post-processing step <b>109</b> can be applied to the disruption labeled mask to remove or minimize the impacts of motion artifacts, such as eye movement during the data acquisition, false positive regions near disc area or due to size and shape, or optical and electronic noise. For example, eye motion can be detected using edge information and histogram distribution of the IFI image. A motion probability map can be constructed and a probability of motion artifacts can then be assigned to each labeled region so that regions with higher motion probability are removed from the disruption image. <figref idref="DRAWINGS">FIGS. 9</figref><i>a</i>-<i>b </i>show examples of disruption regions before (<figref idref="DRAWINGS">FIG. 9</figref><i>a</i>) and after (<figref idref="DRAWINGS">FIG. 9</figref><i>b</i>) the post processing. In <figref idref="DRAWINGS">FIG. 9</figref><i>b</i>, the artifacts due to eye movement, too small and too slim clusters, and false positive clusters associated with the optic disc area in <figref idref="DRAWINGS">FIG. 9</figref><i>a </i>were removed. In some embodiments, instead of removing the motion and other artifacts region, the artifacts could be displayed as different shades or colors to make them more distinguishable for easier interpretation.</p>
<p id="h-0011" num="0000">Disruption Sensitivity Calculation</p>
<p id="p-0050" num="0049">In accordance with some embodiments of the present invention, an optional step <b>110</b> can be implemented to enhance the retinal disruption detection results; a disruption sensitivity metric can be defined and assigned to each disruption region or volume. A sensitivity value can be set in a range reasonable to the user, such as a range of [0.0, 1.0] or [0%, 100%]. User can adjust this parameter to display disruption region or volume at a desire sensitivity level. A high sensitivity value can lead to a larger number of disruptions with smaller sizes to be detected, and vice versa. Alternatively, a user defined fixed sensitivity value can be selected by default. The sensitivity value can be assigned, manually or automatically, using information from the 3D OCT data set, such as, the height of the elevation, the area of the disruption, the volume of the disruption, or combination of these metrics. For instance, if the user wants to visualize large drusens or PED, a lower sensitivity value, such as 0.5 out of 1.0, can be selected. On the contrary, if the user wants to visualize all possible disruptions, a sensitivity value of 1.0 out of 1.0 could be selected. <figref idref="DRAWINGS">FIG. 9</figref><i>c</i>-<i>d </i>show examples of detected disruption regions with different sensitivity values (sensitivity value at 0.5 and 1.0, respectively).</p>
<p id="h-0012" num="0000">Disruption Measurements</p>
<p id="p-0051" num="0050">3D regions of retinal disruptions detection can be achieved as described in steps <b>100</b>-<b>110</b> described above. The next step, step <b>111</b> of <figref idref="DRAWINGS">FIG. 1</figref>, is to obtain quantified measurements and perform image processing to identify characteristics and properties of the disruptions in a manner to aid in the diagnosis of a disease or a disease state. The measurements of the 3D retinal disruptions are clinically meaningful; and the image processing helps the user to evaluate the identified conditions in a clinically useful manner. In some embodiments of the present invention, the following measurements and image processing can be performed in step <b>111</b>. Such measurements may include but are not limited to the following:
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0051">The number of disruptions;</li>
        <li id="ul0002-0002" num="0052">The 3D boundary of each disruption;</li>
        <li id="ul0002-0003" num="0053">The size (diameter) of each disruption;</li>
        <li id="ul0002-0004" num="0054">The area of each disruption;</li>
        <li id="ul0002-0005" num="0055">The volume of each disruption;</li>
        <li id="ul0002-0006" num="0056">The distribution of the size in the categories of small (&#x3c;63 um), intermediate (63 um&#x2dc;124 um), large (125 um&#x2dc;249 um), very large (250 um&#x2dc;499 um), super large (&#x3e;=500 um);</li>
        <li id="ul0002-0007" num="0057">The distribution of the volume, in the same size categories defined above;</li>
        <li id="ul0002-0008" num="0058">The total size of the disruptions in any user specified area;</li>
        <li id="ul0002-0009" num="0059">The total volume of the disruptions in any user specified area;</li>
        <li id="ul0002-0010" num="0060">The total size of the disruptions in the standard ETDRS (Early Treatment Diabetic Retinopathy Study) sectors;</li>
        <li id="ul0002-0011" num="0061">Total circle (6 mm diameter circle);</li>
        <li id="ul0002-0012" num="0062">Center circle (1 mm diameter circle);</li>
        <li id="ul0002-0013" num="0063">Inner ring (1 mm&#x2dc;3 mm ring);</li>
        <li id="ul0002-0014" num="0064">Out ring (3 mm&#x2dc;6 mm ring);</li>
        <li id="ul0002-0015" num="0065">Inner Tempo;</li>
        <li id="ul0002-0016" num="0066">Inner Superior;</li>
        <li id="ul0002-0017" num="0067">Inner Nasal;</li>
        <li id="ul0002-0018" num="0068">Inner Inferior;</li>
        <li id="ul0002-0019" num="0069">Out Tempo;</li>
        <li id="ul0002-0020" num="0070">Outer Superior;</li>
        <li id="ul0002-0021" num="0071">Outer Nasal;</li>
        <li id="ul0002-0022" num="0072">Outer Inferior;</li>
        <li id="ul0002-0023" num="0073">The total volume of the disruptions in the standard ETDRS (Early Treatment Diabetic Retinopathy Study) sectors as defined above; and</li>
        <li id="ul0002-0024" num="0074">Disruption area percentage in the standard ETDRS (Early Treatment Diabetic Retinopathy Study) sectors as defined above.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0052" num="0075">In addition to calculating these numerical measurements, they can be displayed in a clinically meaningful manner. For example, distances of disruption centers to a reference point, such as the fovea, can be calculated and plotted in relation to some measurements, such as diameter, area, or volume of the disruptions. <figref idref="DRAWINGS">FIG. 10</figref> shows an exemplary plot of the size of the disruption versus its distance to fovea.</p>
<p id="h-0013" num="0000">Interactive Presentation</p>
<p id="p-0053" num="0076">An intuitive and user-friendly Graphical User Interface (GUI) providing user interaction with the 3D disruption regions can be employed in some embodiments of the present invention. An interactive GUI presentation step <b>112</b> of the 3D retinal disruption segmentation steps <b>106</b> to <b>110</b> and the measurements performed in step <b>111</b> can be incorporated in the exemplary flowchart in <figref idref="DRAWINGS">FIG. 1</figref>. An example of an intuitive and user-friendly user interaction could allow a user to modify the layer segmentation results form step <b>101</b> to better identify the region of interest. Another example of such user interaction can allow the user to modify, add, or remove 3D disruption seeds inform step <b>105</b> for customized results. Also, such user interaction can allow the user to modify, add, remove, or emphasize the resulting 3D disruption segmentation from step <b>106</b>. The interactive presentation and GUI allow the user to verify and confirm the detection results to better meet the user's clinical needs. From the retinal layer segmentation to the generation of measurements and image processing of the detected 3D disruption from steps <b>100</b>-<b>111</b> can be evaluated again when the user chooses to make a modification through the interactive GUI presentation step <b>112</b>.</p>
<p id="h-0014" num="0000">Final Presentation and Report</p>
<p id="p-0054" num="0077">After the user has verified and confirmed the 3D retinal disruption results and measurements, a clinically useful detection report can be generated in step <b>113</b>. <figref idref="DRAWINGS">FIG. 2</figref> illustrates an example of a clinically useful presentation scheme for a disruption results report. Two IFI <b>200</b> and <b>202</b> are displayed in this sample report. IFI <b>200</b> is an IFI with a ETDRS grid overlay with the detected retinal disruptions highlighted in different colors. A color scheme is used in some embodiments to help easily identify characteristics of the disruptions. Some clinically useful characteristics are area, volume and depth. In this instance in <figref idref="DRAWINGS">FIG. 2</figref>, the color scheme of green <b>203</b>, purple <b>204</b> and red <b>205</b> are used to designate the size of the disruption for intermediate, large and very large size, respectively. IFI <b>202</b> is an IFI with another ETDRS grid overlay with an area percentage indicated in each of the ETDRS grid region. IFI <b>202</b> is a quantitative display to indicate the percentage of area affected in each of the ETDRS region. It is clinically useful to be able to identify the location and concentration of disruption in each of the sensitive ETDRS region. Region <b>206</b> is one ETDRS region where <b>1</b>% in area of the total number of drusens is located. Image <b>207</b> is one example of B-scan image where retinal layers are segmented and displayed (first red: ILM, green: IPL, cyan: RPE, second red: RPE-fit). Region <b>208</b> is the disruption regions (pink bumps) detected for the corresponding B-scan image. Additionally, some commonly used clinical information can be displayed or integrated in such a report, namely, total disruption number, disruption number for each size category (small, intermediate, large, very large, and super large), total disruption area, disruption area for each ETDRS sectors, total disruption volume, disruption volume for each ETDRS sectors, disruption area percentage for each ETDRS sectors, color ETDRS region by the disruption area percentage, overlay the above colored ETDRS region onto the IFI image <b>202</b>, overlay the above colored ETDRS region onto the color fundus photograph, and mark the disruption boundaries on to in image, with different color for the boundary to distinguish the size category <b>200</b>. <figref idref="DRAWINGS">FIG. 3</figref> shows an example color fundus photograph commonly used by clinicians to identify one form of retinal disruptions (drusens). In photograph <b>300</b>, drusens <b>302</b> are displayed in yellowish color. However, this traditional presentation or report is less clinically useful than the quantitative, objective and more accurate 3D retinal disruption detection method and presentation discussed above.</p>
<p id="p-0055" num="0078">Alternatively, the final presentation of the 3D disruptions can be rendered as &#x201c;3D disruption clouds&#x201d; in a 3D display interface. The 3D OCT scan could be displayed in full or at certain layers depending on the user's selection. The 3D disruption clouds can be rendered as pseudo or semi-transparent color volume to present of 3D location and shape. <figref idref="DRAWINGS">FIG. 11</figref><i>a </i>shows an exemplary 3D disruption clouds display at the RPE-fit layer, and <figref idref="DRAWINGS">FIG. 11</figref><i>b </i>shows another example at the RPE layer.</p>
<p id="h-0015" num="0000">Progression Analysis</p>
<p id="p-0056" num="0079">In addition to a single clinically useful final report, it is often more advantageous for a clinician to be able to monitor and effectively track a condition or a disease state in the field of ophthalmology. Some embodiments of the present invention include an integrated progression report generated in reporting step <b>114</b>, which compares and presents clinically useful information from longitudinal exams acquired within or across multiple visits. <figref idref="DRAWINGS">FIG. 6</figref> is an exemplary presentation scheme for a progression report; it shows an example of a progression analysis for two data sets acquired in two different points in time, points <b>600</b> and <b>610</b> (retinal disruption <b>620</b> segmented with boundaries). The number of longitudinal data sets can be extended from two to a reasonable number that would be clinically meaningful. When multiple datasets are available (often from multiple patient visits), progression analysis can be performed and the progression report can then be generated. When multiple data sets are available, the analysis from steps <b>100</b> to <b>113</b> of disruption 3D seeds detection will be adapted accordingly to use the multiple datasets. Using multiple dataset can further help validate and enhance robustness of the 3D disruption detection method, especially when the disease state does not change significantly between visits. Some common and clinically meaningful display and representation of measurements can be used in the progression report, for example, change in total disruption number, change in disruption number for each size category, change in disruption area, change in disruption area for each ETDRS sectors, change in disruption volume, change in disruption area percentage for each ETDRS sectors, and change in the disruption region, overlaid onto IFI images, with different colors for the disruption areas only in one data, and disruption areas in both datasets. Examples of some of these useful information are shown in graphs <b>630</b> and <b>640</b>. A similar report can be created for other disruption presentation and measurement discussed above by a person with ordinary skill in the art within the scope of the present invention.</p>
<p id="p-0057" num="0080">As is demonstrated from the analysis of data obtained from each of the prior art techniques, none of them provide a complete, reliable, or accurate analysis of the 3D retinal disruption. Each fails to reliably determine one or more measurement or segmentation in an accurate and reproducible manner.</p>
<p id="p-0058" num="0081">The above examples are provided in order to demonstrate and further illustrate certain embodiments and aspects of the present invention and are not to be construed as limiting the scope thereof. In the description above, reference is made primarily to the eye as the object. This has to be understood as merely a way to help the description and not as a restriction of the application of the present invention. As such, where the term &#x201c;eye&#x201d; is used, a more general transparent and scattering object or organ may be sought instead. Similarly, embodiments described herein use drusen and RPE-disruption as example demonstration and the same embodiments can be applied to retinal disruption generally. Although various embodiments that incorporate the teachings of the present invention have been illustrated and described in detail herein, a person of ordinary skill in the art can readily device other various embodiments that incorporate the teachings of this subject invention.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>We claim:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. An imaging apparatus, comprising:
<claim-text>an optical source;</claim-text>
<claim-text>an x-y scanner receiving light from the optical source and directing it onto a sample;</claim-text>
<claim-text>a detector receiving reflected light from the scanner; and</claim-text>
<claim-text>a computer receiving a signal from the detector and executing code stored in a memory, the code providing instructions for
<claim-text>providing a 3D data set containing voxels of a sample,</claim-text>
<claim-text>dentifying one or more 3D seeds from the 3D data set by using an adaptive seed search image (ASSI),</claim-text>
<claim-text>obtaining characteristics of 3D disruptions based on the 3D seeds,</claim-text>
<claim-text>generating measurements for the 3D disruptions, and</claim-text>
<claim-text>displaying the results of the 3D disruptions.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein identifying one or more 3D seeds from the 3D data set includes:
<claim-text>segmenting at least one retinal layer in the 3D data set to obtain one or more segmented retinal layers;</claim-text>
<claim-text>generating a notinal Retinal Pigment Epithelium (RPE) layer from the at least one retinal layer in the 3D data set to obtain a RPE fit surface; and</claim-text>
<claim-text>calculating a distance between points on the one or more segmented retinal layers to points on the RPE fit surface to obtain at least one elevation map.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The apparatus of <claim-ref idref="CLM-00002">claim 2</claim-ref>, further including reducing the 3D data set between the one or more segmented retinal layers into a 2D representation to obtain an en face image.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The apparatus of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the en face image is processed by applying an image processing filter to obtain the ASSI, the processing filter being chosen from a set of filters consisting of an edge detection filter, a smoothing filter, and the combination thereof.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The apparatus of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the ASSI can be generated by combining information of the elevation map and the en face image.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The apparatus of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein identifying one or more 3D seeds from the 3D data set includes a 3D segmentation computation.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The apparatus of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the 3D segmentation computation includes a 3D volume construction method by interpolating two or more 2D cross-sectional contour images from the 3D dataset.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The apparatus of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the 3D segmentation computation includes a 3D region growing method using region growing constraints.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the ASSI is an elevation map.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein generating measurements for the 3D disruptions is interactive.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein displaying the results of the 3D disruptions includes providing plots of one or more 3D disruption characteristics in relation to at least one anatomical feature of an eye.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further including a progression analysis to evaluate and display 3D disruption results from the 3D data set obtained from different points in time.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further including a 3D display of 3D disruption clouds overlaying a segmented layer of interest in a 3D display interface.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. An image processing method, comprising:
<claim-text>acquiring a 3D data set containing voxels of a sample from an Optical Coherent Tomography (OCT) system;</claim-text>
<claim-text>identifying one or more 3D seeds from the 3D data set by using an adaptive seed search image (ASSI);</claim-text>
<claim-text>obtaining characteristics of 3D disruptions based on the 3D seeds;</claim-text>
<claim-text>generating measurements for the 3D disruptions; and</claim-text>
<claim-text>displaying results of the 3D disruptions.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein identifying one or more 3D seeds from the 3D data set includes:
<claim-text>segmenting at least one retinal layer in the 3D data set to obtain one or more segmented retinal layers;</claim-text>
<claim-text>generating a normal Retinal Pigment Epithelium (RPE) layer from the at least one retinal layer in the 3D data set to obtain a RPE fit surface; and</claim-text>
<claim-text>calculating a distance between points on the one or more segmented retinal layers to points on the RPE fit surface to obtain at least one elevation map.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, further including reducing the 3D data set between the one or more segmented retinal layers into a 2D representation to obtain an en face image.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the en face image is processed by applying an image processing filter to obtain the ASSI, the processing filter being chosen from a set of filters consisting of an edge detection filter, a smoothing filter, and the combination thereof.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the ASSI can be generated by combining information of the elevation map and the en face image.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein identifying one or more 3D seeds from the 3D data set includes a 3D segmentation computation.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The method of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the 3D segmentation computation includes a 3D volume construction method by interpolating two or more 2D cross-sectional contour images from the 3D dataset.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The method of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the 3D segmentation computation includes a 3D region growing method using region growing constraints.</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the ASSI is an elevation map.</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein generating measurements for the 3D disruptions is interactive.</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein displaying the results of the 3D disruptions includes providing plots of one or more 3D disruption characteristics in relation to at least one anatomical feature of an eye.</claim-text>
</claim>
<claim id="CLM-00025" num="00025">
<claim-text>25. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, further including a progression analysis to evaluate and display 3D disruption results from the 3D data set obtained from different points in time.</claim-text>
</claim>
<claim id="CLM-00026" num="00026">
<claim-text>26. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, further including a 3D display of 3D disruption clouds overlaying a segmented layer of interest in a 3D display interface.</claim-text>
</claim>
</claims>
</us-patent-grant>
