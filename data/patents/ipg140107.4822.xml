<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625915-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625915</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13802977</doc-number>
<date>20130314</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>JP</country>
<doc-number>09-150656</doc-number>
<date>19970609</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<disclaimer>
<text>This patent is subject to a terminal disclaimer.</text>
</disclaimer>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>36</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382233</main-classification>
</classification-national>
<invention-title id="d2e69">Recording medium having recorded thereon coded information using plus and/or minus rounding of images</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5054103</doc-number>
<kind>A</kind>
<name>Yasuda et al.</name>
<date>19911000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5481553</doc-number>
<kind>A</kind>
<name>Suzuki et al.</name>
<date>19960100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5486876</doc-number>
<kind>A</kind>
<name>Lew et al.</name>
<date>19960100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5585963</doc-number>
<kind>A</kind>
<name>Suzuki</name>
<date>19961200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5604494</doc-number>
<kind>A</kind>
<name>Murakami et al.</name>
<date>19970200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>5659365</doc-number>
<kind>A</kind>
<name>Wilkinson</name>
<date>19970800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>5737022</doc-number>
<kind>A</kind>
<name>Yamaguchi et al.</name>
<date>19980400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>5740283</doc-number>
<kind>A</kind>
<name>Meeker</name>
<date>19980400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>5905542</doc-number>
<kind>A</kind>
<name>Linzer</name>
<date>19990500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6008852</doc-number>
<kind>A</kind>
<name>Nakaya</name>
<date>19991200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6058410</doc-number>
<kind>A</kind>
<name>Sharangpani</name>
<date>20000500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6282243</doc-number>
<kind>B1</kind>
<name>Kazui et al.</name>
<date>20010800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>6295376</doc-number>
<kind>B1</kind>
<name>Nakaya</name>
<date>20010900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382236</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>6643409</doc-number>
<kind>B1</kind>
<name>Nakaya</name>
<date>20031100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2002/0186771</doc-number>
<kind>A1</kind>
<name>Nakaya et al.</name>
<date>20021200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>EP</country>
<doc-number>0712249</doc-number>
<date>19960500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>EP</country>
<doc-number>0735769</doc-number>
<date>19961000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>EP</country>
<doc-number>0797357</doc-number>
<date>19970900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>JP</country>
<doc-number>6214754</doc-number>
<date>19940800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>JP</country>
<doc-number>9200763</doc-number>
<date>19970700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>JP</country>
<doc-number>9252470</doc-number>
<date>19970900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>JP</country>
<doc-number>1098729</doc-number>
<date>19980400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>JP</country>
<doc-number>11069345</doc-number>
<date>19990300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00024">
<othercit>M. Iwashashi, &#x201c;A Motion Compensation Technique for Down-Scaled Pictures in Layered Coding&#x201d;, IEICE Trans. Commun., vol. E77-B, No. 8, Aug. 1994, pp. 10071012.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00025">
<othercit>&#x201c;Video Coding for Low Bit Rate Communication&#x201d;, ITU-T, International Telecommunication Union, Infrastructure of Audiovisual Services-Coding of Moving Video, Feb. 1998.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00026">
<othercit>&#x201c;Motion-Compensation Prediction Mode and Motion Vector Detection Method&#x201d;, The Journal of the Institute of Television Engineers of Japan, vol. 49, No. 4, 4, Apr. 1995, pp. 445-448.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00027">
<othercit>Y. Nakaya, et al., &#x201c;Avoidance of Rounding Error Accumulation in Motion Compensation with Half Pel Accuracy&#x201d;, Proceedings of the 1998 IEICE General Conference, D-11-44, Mar. 27-30, 1998, Tokai University, Hiratsuka, Japan.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00028">
<othercit>M. Chatel, &#x201c;Classical versus Transparent IP Proxies&#x201d;, Network Working Group, Mar. 1999, (web page) http://www.iedtf.org/rfc1919.txt., (Accessed May 9, 2000).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00029">
<othercit>&#x201c;Nevod Adds Native Support for Multi-player Game including Diablo/Starcraft/Battle.net and Activision/Battlezone to its NAT1000 Internet Sharing Product Line&#x201d;, Nevod, Inc., Press Release, Dec. 14, 1998, http://www.nevod.com/products/nat1000<sub>&#x2014;</sub>95.html.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00030">
<othercit>F. Langa ed., &#x201c;High-speed surfing&#x201d;, Windows Magazine, n 1002, Feb. 1, 1999.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00031">
<othercit>F. Langa ed., &#x201c;Easy, Low-cost Web Access&#x201d;, Windows Magazine, n 1006a, Jun. 15, 1999.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00032">
<othercit>&#x201c;Ositis Software Announces the Launch of WinProxy 3.0, The Complete Internet Sharing Solution&#x201d;, Ositis Software, Press Release, Aug. 10, 1999.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00033">
<othercit>&#x201c;Windows 2000 Network Address Translator&#x201d;, Microsoft Corporation, Apr. 23, 1999.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00034">
<othercit>&#x201c;Windows NT Network Address Translator&#x201d;, Microsoft Corporation, Oct. 3, 1998.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00035">
<othercit>K. Egevang et al., &#x201c;The IP Network Address Translator (NAT)&#x201d;, Network Working Group, May 1994, (web page) http://www.jetf.org/rfc163.txt, (Accessed Jan. 22, 2000).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>1</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382232-251</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382300</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>37524001-24029</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>13</number-of-drawing-sheets>
<number-of-figures>17</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>13447581</doc-number>
<date>20120416</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>8406544</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13802977</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>13090566</doc-number>
<date>20110420</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>8160373</doc-number>
<date>20120417</date>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13447581</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>12886011</doc-number>
<date>20100920</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>7936935</doc-number>
<date>20110503</date>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13090566</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>12696230</doc-number>
<date>20100129</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>7801381</doc-number>
<date>20100921</date>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>12886011</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>12342808</doc-number>
<date>20081223</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>7676100</doc-number>
<date>20100309</date>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>12696230</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>11770900</doc-number>
<date>20070629</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>7471836</doc-number>
<date>20081230</date>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>12342808</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>10623589</doc-number>
<date>20030722</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>7251369</doc-number>
<date>20070731</date>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>11770900</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>09513688</doc-number>
<date>20000225</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>6631214</doc-number>
<date>20031007</date>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>10623589</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>09093194</doc-number>
<date>19980608</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>6295376</doc-number>
<date>20010925</date>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>09513688</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20130195194</doc-number>
<kind>A1</kind>
<date>20130801</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Nakaya</last-name>
<first-name>Yuichiro</first-name>
<address>
<city>Suginami-ku</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Nakaya</last-name>
<first-name>Yuichiro</first-name>
<address>
<city>Suginami-ku</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Antonelli, Terry, Stout &#x26; Kraus, LLP.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Hitachi, Ltd.</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Sherali</last-name>
<first-name>Ishrat I</first-name>
<department>2667</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Computer-readable medium having an image decoding program performing: storing a reference image of a previously decoded image; receiving coded information including motion vector and rounding method information specifying a rounding method for synthesizing a prediction image of a currently decoded image; and synthesizing the prediction image via motion compensation using the motion vector information and reference image, using a positive and negative rounding method for interpolating pixel intensity values; wherein interpolation of pixel intensity values uses a rounding method specified by the rounding method information included in the encoded current P frame bitstream; wherein the rounding method information is not received from the encoded I frame bitstream; and wherein the rounding method information is included in a header section of the coded information of the currently decoded image.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="132.59mm" wi="150.71mm" file="US08625915-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="140.89mm" wi="152.57mm" orientation="landscape" file="US08625915-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="138.85mm" wi="123.11mm" orientation="landscape" file="US08625915-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="118.45mm" wi="111.68mm" file="US08625915-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="47.58mm" wi="109.30mm" file="US08625915-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="134.03mm" wi="160.61mm" file="US08625915-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="145.29mm" wi="147.32mm" file="US08625915-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="97.37mm" wi="145.12mm" file="US08625915-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="115.06mm" wi="148.51mm" file="US08625915-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="140.38mm" wi="157.90mm" file="US08625915-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="92.12mm" wi="167.72mm" file="US08625915-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="133.77mm" wi="108.80mm" file="US08625915-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="141.65mm" wi="148.34mm" file="US08625915-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="136.23mm" wi="113.71mm" file="US08625915-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application is a continuation application of application Ser. No. 13/447,581, filed Apr. 16, 2012, which is a continuation application of application Ser. No. 13/090,566, filed Apr. 20, 2011 (now U.S. Pat. No. 8,160,373), which is a continuation application of application Ser. No. 12/886,011, filed Sep. 20, 2010 (now U.S. Pat. No. 7,936,935), which is a continuation of application Ser. No. 12/696,230, filed Jan. 29, 2010 (now U.S. Pat. No. 7,801,381), which is a continuation of application Ser. No. 12/342,808, filed Dec. 23, 2008 (now U.S. Pat. No. 7,676,100), which is a continuation of application Ser. No. 11/770,900, filed Jun. 29, 2007 (now U.S. Pat. No. 7,471,836), which is a continuation of application Ser. No. 10/623,589, filed Jul. 22, 2003 (now U.S. Pat. No. 7,251,369), which is a continuation of application Ser. No. 09/513,688 filed Feb. 25, 2000 (now U.S. Pat. No. 6,631,214), which is a continuation of application Ser. No. 09/093,194, filed Jun. 8, 1998 (now U.S. Pat. No. 6,295,376), the contents of which are hereby incorporated herein by reference in their entirety.</p>
<p id="p-0003" num="0002">This application is also related to application Ser. No. 09/514,287, filed Feb. 28, 2000 (now U.S. Pat. No. 6,560,367); application Ser. No. 09/516,207, filed Feb. 29, 2000 (now U.S. Pat. No. 6,529,632); application Ser. No. 09/516,245, filed Mar. 1, 2000 (now U.S. Pat. No. 6,643,409); application Ser. No. 09/875,932, filed Jun. 8, 2001 (now U.S. Pat. No. 6,574,371); application Ser. No. 09/875,930, filed Jun. 8, 2001 (now U.S. Pat. No. 6,650,781); application Ser. No. 09/875,872, filed Jun. 8, 2001 (now U.S. Pat. No. 6,567,558); application Ser. No. 09/875,929, filed Jun. 8, 2001 (now U.S. Pat. No. 6,584,227); application Ser. No. 09/875,928, filed Jun. 8, 2001 (now U.S. Pat. No. 6,606,419); application Ser. No. 10/623,669, filed Jul. 22, 2003 (now U.S. Pat. No. 6,909,809); application Ser. No. 10/623,531, filed Jul. 22, 2003 (now U.S. Pat. No. 6,915,013); application Ser. No. 10/623,668, filed Jul. 22, 2003 (now U.S. Pat. No. 6,868,185); application Ser. No. 10/623,506, filed Jul. 22, 2003 (now U.S. Pat. No. 6,876,769); application Ser. No. 10/902,042, filed Jul. 30, 2004 (now U.S. Pat. No. 7,184,601); application Ser. No. 10/901,959, filed Jul. 30, 2004 (now U.S. Pat. No. 7,200,274); application Ser. No. 10/901,960, filed Jul. 30, 2004 (now U.S. Pat. No. 7,248,742); application Ser. No. 10/901,964, filed Jul. 30, 2004 (now U.S. Pat. No. 7,072,518); application Ser. No. 10/902,040, filed Jul. 30, 2004 (now U.S. Pat. No. 7,233,704); application Ser. No. 10/902,041, filed Jul. 30, 2004 (now U.S. Pat. No. 7,236,635); application Ser. No. 11/770,912, filed Jun. 29, 2007 (now U.S. Pat. No. 7,471,837); application Ser. No. 11/770,923, filed Jun. 29, 2007 (now U.S. Pat. No. 7,466,864); application Ser. No. 11/770,932, filed Jun. 29, 2007 (now U.S. Pat. No. 7,421,133); application Ser. No. 11/770,937, filed Jun. 29, 2007 (now U.S. Pat. No. 7,424,161); application Ser. No. 11/770,953, filed Jun. 29, 2007 (now U.S. Pat. No. 7,426,307); application Ser. No. 12/342,787, filed Dec. 23, 2008; application Ser. No. 12/342,884, filed Dec. 23, 2008; application Ser. No. 12/344,617, filed Dec. 29, 2008; application Ser. No. 12/344,619, filed Dec. 29, 2008; application Ser. No. 12/344,621, filed Dec. 29, 2008; application Ser. No. 12/344,625, filed Dec. 29, 2008; application Ser. No. 12/344,626, filed Dec. 29, 2008, all of which, like the present application, are continuations of application Ser. No. 09/093,194, filed Jun. 8, 1998 (now U.S. Pat. No. 6,295,376). This application relates to and claims priority from Japanese Patent Application No. 9-150656, filed on Jun. 9, 1997. The entirety of the contents and subject matter of all of the above is incorporated herein by reference.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0004" num="0003">1. Field of the Invention</p>
<p id="p-0005" num="0004">The present invention relates to an image sequence coding and decoding method which performs interframe prediction using quantized values for chrominance or luminance intensity.</p>
<p id="p-0006" num="0005">2. Related Art</p>
<p id="p-0007" num="0006">In high efficiency coding of image sequences, interframe prediction (motion compensation) by utilizing the similarity of adjacent frames over time, is known to be a highly effective technique for data compression. Today's most frequently used motion compensation method is block matching with half pixel accuracy, which is used in international standards H.263, MPEG1, and MPEG2. In this method, the image to be coded is segmented into blocks and the horizontal and vertical components of the motion vectors of these blocks are estimated as integral multiples of half the distance between adjacent pixels. This process is described using the following equation: [Equation 1]
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i>(<i>x,y</i>)=<i>R</i>(<i>x+u</i><sub>i</sub><i>,y+v</i><sub>i</sub>(<i>x,y</i>)&#x3b5;<i>B</i><sub>i</sub>,0<i>&#x2266;i&#x3c;N</i>&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0008" num="0007">where P(x, y) and R(x, y) denote the sample values (luminance or chrominance intensity) of pixels located at coordinates (x, y) in the predicted image P of the current frame and the reference image (decoded image of a frame which has been encoded before the current frame) R, respectively. &#x201c;x&#x201d; and &#x201c;y&#x201d; are integers, and it is assumed that all the pixels are located at points where the coordinate values are integers. Additionally it is assumed that the sample values of the pixels are quantized to non-negative integers. N, Bi, and (ui, vi) denote the number of blocks in the image, the set of pixels included in the i-th block of the image, and the motion vectors of the i-th block, respectively.</p>
<p id="p-0009" num="0008">When the values for &#x201c;ui&#x201d; and &#x201c;vi&#x201d; are not integers, it is necessary to find the intensity value at the point where no pixels actually exist in the reference image. Currently, bilinear interpolation using the adjacent four pixels is the most frequently used method for this process. This interpolation method is described using the following equation:</p>
<p id="p-0010" num="0009">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mo>[</mo>
        <mrow>
          <mi>Equation</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>2</mn>
        </mrow>
        <mo>]</mo>
      </mrow>
    </mtd>
    <mtd>
      <mstyle>
        <mspace width="0.3em" height="0.3ex"/>
      </mstyle>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>R</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mrow>
                <mi>x</mi>
                <mo>+</mo>
                <mfrac>
                  <mi>p</mi>
                  <mi>d</mi>
                </mfrac>
              </mrow>
              <mo>,</mo>
              <mrow>
                <mrow>
                  <mi>y</mi>
                  <mo>+</mo>
                  <mfrac>
                    <mi>q</mi>
                    <mi>d</mi>
                  </mfrac>
                </mrow>
                <mo>=</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mrow>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <mi>d</mi>
                          <mo>-</mo>
                          <mi>q</mi>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                      <mo>&#x2062;</mo>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <mrow>
                            <mrow>
                              <mo>(</mo>
                              <mrow>
                                <mi>d</mi>
                                <mo>-</mo>
                                <mi>p</mi>
                              </mrow>
                              <mo>)</mo>
                            </mrow>
                            <mo>&#x2062;</mo>
                            <mrow>
                              <mi>R</mi>
                              <mo>&#x2061;</mo>
                              <mrow>
                                <mo>(</mo>
                                <mrow>
                                  <mi>x</mi>
                                  <mo>,</mo>
                                  <mi>y</mi>
                                </mrow>
                                <mo>)</mo>
                              </mrow>
                            </mrow>
                          </mrow>
                          <mo>+</mo>
                          <mrow>
                            <mi>pR</mi>
                            <mo>&#x2061;</mo>
                            <mrow>
                              <mo>(</mo>
                              <mrow>
                                <mrow>
                                  <mi>x</mi>
                                  <mo>+</mo>
                                  <mn>1</mn>
                                </mrow>
                                <mo>,</mo>
                                <mi>y</mi>
                              </mrow>
                              <mo>)</mo>
                            </mrow>
                          </mrow>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                    <mo>+</mo>
                    <mrow>
                      <mrow>
                        <mi>q</mi>
                        <mo>&#x2061;</mo>
                        <mrow>
                          <mo>(</mo>
                          <mrow>
                            <mrow>
                              <mrow>
                                <mo>(</mo>
                                <mrow>
                                  <mi>d</mi>
                                  <mo>-</mo>
                                  <mi>p</mi>
                                </mrow>
                                <mo>)</mo>
                              </mrow>
                              <mo>&#x2062;</mo>
                              <mrow>
                                <mi>R</mi>
                                <mo>&#x2061;</mo>
                                <mrow>
                                  <mo>(</mo>
                                  <mrow>
                                    <mi>x</mi>
                                    <mo>,</mo>
                                    <mrow>
                                      <mi>y</mi>
                                      <mo>+</mo>
                                      <mn>1</mn>
                                    </mrow>
                                  </mrow>
                                  <mo>)</mo>
                                </mrow>
                              </mrow>
                            </mrow>
                            <mo>+</mo>
                            <mi>pR</mi>
                          </mrow>
                          <mo>)</mo>
                        </mrow>
                      </mrow>
                      <mo>&#x2062;</mo>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <mrow>
                            <mi>x</mi>
                            <mo>+</mo>
                            <mn>1</mn>
                          </mrow>
                          <mo>,</mo>
                          <mrow>
                            <mi>y</mi>
                            <mo>+</mo>
                            <mn>1</mn>
                          </mrow>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>//</mo>
        <msup>
          <mi>d</mi>
          <mn>2</mn>
        </msup>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>2</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0011" num="0010">where &#x201c;d&#x201d; is a positive integer, and &#x201c;p&#x201d; and &#x201c;q&#x201d; are smaller than &#x201c;d&#x201d; but not smaller than zero &#x201c;0&#x201d;. &#x201c;//&#x201d; denotes integer division which rounds the result of normal division (division using real numbers) to the nearest integer.</p>
<p id="p-0012" num="0011">An example of the structure of an H.263 video encoder is shown in <figref idref="DRAWINGS">FIG. 1</figref>. As the coding algorithm, H.263 adopts a hybrid coding method (adaptive interframe/intraframe coding method) which is a combination of block matching and DCT (discrete cosine transform). A subtractor <b>102</b> calculates the difference between the input image (current frame base image) <b>101</b> and the output image <b>113</b> (related later) of the interframe/intraframe coding selector <b>119</b>, and then outputs an error image <b>103</b>. This error image is quantized in a quantizer <b>105</b> after being converted into DCT coefficients in a DCT converter <b>104</b> and then forms quantized DCT coefficients <b>106</b>. These quantized DCT coefficients are transmitted through the communication channel, while at the same time used to synthesize the interframe predicted image in the encoder.</p>
<p id="p-0013" num="0012">The procedure for synthesizing the predicted image is explained next. The above mentioned quantized DCT coefficients <b>106</b> forms the reconstructed error image <b>110</b> (same as the reconstructed error image on the receive side) after passing through a dequantizer <b>108</b> and inverse DCT converter <b>109</b>. This reconstructed error image and the output image <b>113</b> of the interframe/intraframe coding selector <b>119</b> is added at the adder <b>111</b> and the decoded image <b>112</b> of the current frame (same image as the decoded image of current frame reconstructed on the receiver side) is obtained. This image is stored in a frame memory <b>114</b> and delayed for a time equal to the frame interval. Accordingly, at the current point, the frame memory <b>114</b> outputs the decoded image <b>115</b> of the previous frame. This decoded image of the previous frame and the original image <b>101</b> of the current frame are input to the block matching section <b>116</b> and block matching is performed between these images. In the block matching process, the original image of the current frame is segmented into multiple blocks, and the predicted image <b>117</b> of the current frame is synthesized by extracting the section most resembling these blocks from the decoded image of the previous frame. In this process, it is necessary to estimate the motion between the prior frame and the current frame for each block. The motion vector for each block estimated in the motion estimation process is transmitted to the receiver side as motion vector data <b>120</b>.</p>
<p id="p-0014" num="0013">On the receiver side, the same prediction image as on the transmitter side is synthesized using the motion vector information and the decoding image of the previous frame. The prediction image <b>117</b> is input along with a &#x201c;0&#x201d; signal <b>118</b> to the interframe/intraframe coding selector <b>119</b>. This switch <b>119</b> selects interframe coding or intraframe coding by selecting either of these inputs. Interframe coding is performed when the prediction image <b>117</b> is selected (this case is shown in <figref idref="DRAWINGS">FIG. 2</figref>). On the other hand when the &#x201c;0&#x201d; signal is selected, intraframe coding is performed since the input image itself is converted, to a DCT coefficients and output to the communication channel.</p>
<p id="p-0015" num="0014">In order for the receiver side to correctly reconstruct the coded image, the receiver must be informed whether intraframe coding or interframe coding was performed on the transmitter side. Consequently, an identifier flag <b>121</b> is output to the communication circuit. Finally, an H.263 coded bitstream <b>123</b> is acquired by multiplexing the quantized DCT coefficients, motion vectors, the and interframe/intraframe identifier flag information in a multiplexer <b>122</b>.</p>
<p id="p-0016" num="0015">The structure of a decoder <b>200</b> for receiving the coded bit stream output from the encoder of <figref idref="DRAWINGS">FIG. 1</figref> is shown in <figref idref="DRAWINGS">FIG. 2</figref>. The H.263 coded bit stream <b>217</b> that is received is demultiplexed into quantized DCT coefficients <b>201</b>, motion vector data <b>202</b>, and an interframe/intraframe identifier flag <b>203</b> in the demultiplexer <b>216</b>. The quantized DCT coefficients <b>201</b> become a decoded error image <b>206</b> after being processed by an inverse quantizer <b>204</b> and inverse DCT converter <b>205</b>. This decoded error image is added to the output image <b>215</b> of the interframe/intraframe coding selector <b>214</b> in an adder <b>207</b> and the sum of these images is output as the decoded image <b>208</b>. The output of the interframe/intraframe coding selector is switched according to the interframe/intraframe identifier flag <b>203</b>. A prediction image <b>212</b> utilized when performing interframe encoding is synthesized in the prediction image synthesizer <b>211</b>. In this synthesizer, the position of the blocks in the decoded image <b>210</b> of the prior frame stored in frame memory <b>209</b> is shifted according to the motion vector data <b>202</b>. On the other hand, for intraframe coding, the interframe/intraframe coding selector outputs the &#x201c;0&#x201d; signal <b>213</b> as is.</p>
<heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0017" num="0016">The image encoded by H.263 is comprised of a luminance plane (&#x201c;Y&#x201d; plane) containing luminance information, and two chrominance planes (&#x201c;U&#x201d; plane and &#x201c;V&#x201d; plane) containing chrominance information.</p>
<p id="p-0018" num="0017">At this time, characteristically, when the image has 2m pixels in the horizontal direction and 2n pixels in the vertical direction (&#x201c;m&#x201d; and &#x201c;n&#x201d; are positive integers), the Y plane has 2m pixels horizontally and 2n pixels vertically, the U and V planes have m pixels horizontally and n pixels vertically.</p>
<p id="p-0019" num="0018">The low resolution on the chrominance plane is due to the fact that the human visual system has a comparatively dull visual faculty with respect to spatial variations in chrominance. Having such image as an input, H.263 performs coding and decoding in block units referred to as macroblocks.</p>
<p id="p-0020" num="0019">The structure of a macroblock is shown in <figref idref="DRAWINGS">FIG. 3</figref>. The macroblock is comprised of three blocks; a Y block, U block and V block. The size of the Y block <b>301</b> containing the luminance information is 16&#xd7;16 pixels, and the size of the U block <b>302</b> and V block <b>303</b> containing the chrominance information is 8&#xd7;8 pixels.</p>
<p id="p-0021" num="0020">In H.263, half pixel accuracy block matching is applied to each block. Accordingly, when the estimated motion vector is defined as (u, v), u and v are both integral multiples of half the distance between pixels. In other words, &#xbd; is used as the minimum unit. The configuration of the interpolation method used for the intensity values (hereafter the intensity values for &#x201c;luminance&#x201d; and &#x201c;chrominance&#x201d; are called by the general term &#x201c;intensity value&#x201d;) is shown in <figref idref="DRAWINGS">FIG. 4</figref>. When performing the interpolation described in equation 2, the quotients of division are rounded off to the nearest integer, and further, when the quotient has a half integer value (i.e. 0.5 added to an integer), rounding off is performed to the next integer in the direction away from zero. In other words, in <figref idref="DRAWINGS">FIG. 4</figref>, when the intensity values for 401, 402, 403, 404 are respectively La, Lb, Lc, and Ld (La, Lb, Lc, and Ld are non-negative integers), the interpolated intensity values Ia, Ib, Ic, and Id (Ia, Ib, Ic, and Id are non-negative integers) at positions <b>405</b>, <b>406</b>, <b>407</b>, <b>408</b> are expressed by the following equation:</p>
<p id="p-0022" num="0021">[Equation 3]
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>Ib</i>=[(<i>La+Lb+</i>1)/2]<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>Ic</i>=[(<i>La+Lc+</i>1)/2]<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>Id</i>=[(<i>La+Lb+Lc+Ld+</i>2)/4]&#x2003;&#x2003;(3)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0023" num="0022">where &#x201c;[ ]&#x201d; denotes truncation to the nearest integer towards zero &#x201c;0&#x201d; (i.e. the fractional part is discarded). The expectation of the errors caused by this rounding to integers is estimated as follows: It is assumed that the probability that the intensity value at positions <b>405</b>, <b>406</b>, <b>407</b>, and <b>408</b> of <figref idref="DRAWINGS">FIG. 4</figref> is used is all 25 percent. When finding the intensity value Ia for position <b>405</b>, the rounding error will clearly be zero &#x201c;0&#x201d;. Also, when finding the intensity value Ib for position <b>406</b>, the error will be zero &#x201c;0&#x201d; when La+Lb is an even number, and when an odd number the error is &#xbd;. If the probability that La+Lb will be an even number and an odd number is both 50 percent, then the expectation for the error will be 0&#xd7;&#xbd;+&#xbd;&#xd7;&#xbd;=&#xbc;. Further, when finding the intensity value Ic for position <b>407</b>, the expectation for the error is &#xbc; as for Ib. When finding the intensity value Id for position <b>408</b>, the error when the residual of La+Lb+Lc+Ld divided by four are 0, 1, 2, and 3 are respectively 0, &#x2212;&#xbc;, &#xbd;, and &#xbc;.</p>
<p id="p-0024" num="0023">If we assume that the probability that the residual is 0, 1, 2, and 3 is all equal (i.e. 25 percent), the expectation for the error is 0&#xd7;&#xbc;&#x2212;&#xbc;&#xd7;&#xbc;+&#xbd;&#xd7;&#xbc;+&#xbc;&#xd7;&#xbc;=&#x215b;. As described above, assuming that the possibility that the intensity value at positions <b>405</b>-<b>408</b> being used are all equal, the final expectation for the error is 0&#xd7;&#xbc;+&#xbc;&#xd7;&#xbc;+&#xbc;&#xd7;&#xbc;+&#x215b;&#xd7;&#xbc;= 5/32. This indicates that each time motion compensation is performed by means of block matching, an error of 5/32 occurs in the pixel intensity value. Generally in low rate coding, sufficient number of bits cannot be used for the encoding of the interframe error difference so that the quantized step size of the DCT coefficient is prone to be large. Accordingly, errors occurring due to motion compensation are corrected only when it is very large. When interframe encoding is performed continuously without performing intraframe coding under such environment, the errors tend to accumulate and cause bad effects on the reconstructed image.</p>
<p id="p-0025" num="0024">Just as explained above, the number of pixels is about half (&#xbd;) in both the vertical and horizontal direction on the chrominance plane. Therefore, for the motion vectors of the U block and V block, half (&#xbd;) the value of the motion vector for the Y block is used for the vertical and horizontal components. Since the horizontal and vertical components of the motion vector for the Y block motion vector are integral multiples of &#xbd;, the motion vector components for the U and V blocks will appear as integral multiples of &#xbc; (quarter pixel accuracy) if ordinary division is implemented. However, due to the high computational complexity of the intensity interpolation process for motion vectors with quarter &#xbc; pixel accuracy, the motion vectors for U and V blocks are rounded to half &#xbd; pixel accuracy in H.263.</p>
<p id="p-0026" num="0025">The rounding method utilized in H.263 is as follows: According to the definition described above, (u, v) denotes the motion vector of the macroblock (which is equal to the motion vector for the Y block). Assuming that r is an integer and s is a non-negative integer smaller than 4, u/2 can be rewritten as u/2=r+s/4. When s is 0 or 2, no rounding is required since u/2 is already an integral multiple of &#xbd;. However when s is equal to 1 or 3, the value of s is rounded to 2. By increasing the possibility that s takes the value of 2 using this rounding method, the filtering effect of motion compensation can be emphasized. When the probability that the value of s prior to rounding is 0, 1, 2, and 3 are all percent, the probability that s will be 0 or 2 after rounding will respectively be 25 percent and 75 percent. The above explained process related to the horizontal component u of the motion vector is also applied to the vertical component v. Accordingly, in the U block and V block, the probability for using the intensity value of the 401 position is &#xbc;&#xd7;&#xbc;= 1/16, and the probability for using the intensity value of the 402 and 403 positions is both &#xbc;&#xd7;&#xbe;= 3/16, while the probability for using the intensity value of position <b>404</b> is &#xbe;&#xd7;&#xbe;= 9/16. By utilizing the same method as above, the expectation for the error of the intensity value is 0&#xd7; 1/16+&#xbc;&#xd7; 3/16+&#xbc;&#xd7; 3/16+&#x215b;&#xd7; 9/16= 21/128.</p>
<p id="p-0027" num="0026">Just as explained above for the Y block, when interframe encoding is continuously performed, the problem of accumulated errors occurs. As related above, for image sequence coding and decoding methods in which interframe prediction is performed and luminance or chrominance intensity is quantized, the problem of accumulated rounding errors occurs. This rounding error is generated when the luminance or chrominance intensity value is quantized during the generation of the interframe prediction image.</p>
<p id="p-0028" num="0027">In view of the above problems, it is therefore an object of this invention, to improve the quality of the reconstructed image by preventing error accumulation.</p>
<p id="p-0029" num="0028">In order to achieve the above object, the accumulation of errors is prevented by limiting the occurrence of errors or performing an operation to cancel out errors that have occurred.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram showing the layout of the H.263 image encoder.</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram showing the layout of the H.263 image decoder.</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 3</figref> is a drawing showing the structure of the macro block.</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 4</figref> is a drawing showing the interpolation process of intensity values for block matching with half pixel accuracy.</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 5</figref> is a drawing showing a coded image sequence.</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 6</figref> is a block diagram showing a software image encoding device.</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 7</figref> is a block diagram showing a software image decoding device.</p>
<p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. 8</figref> is a flow chart showing an example of processing in the software image encoding device.</p>
<p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. 9</figref> is a flow chart showing an example of the coding mode decision processing for the software image encoding device.</p>
<p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. 10</figref> is a flow chart showing an example of motion estimation and motion compensation processing in the software image encoding device.</p>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. 11</figref> is a flow chart showing the processing in the software image decoding device.</p>
<p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. 12</figref> is a flow chart showing an example of motion compensation processing in the software image decoding device.</p>
<p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. 13</figref> is a drawing showing an example of a storage media on which an encoded bit stream generated by an encoding method that outputs bit streams including I, P+ and P&#x2212; frames is recorded.</p>
<p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. 14</figref> is a set of drawings showing specific examples of devices using an encoding method where P+ and P&#x2212; frames coexist.</p>
<p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. 15</figref> is a drawing showing an example of a storage media on which an encoded bit stream generated by an encoding method the outputs bit streams including I, B, P+, and P&#x2212; frames is recorded.</p>
<p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. 16</figref> is a block diagram showing an example of a block matching unit included in a device using an encoding method where P+ and P&#x2212; frames coexist.</p>
<p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. 17</figref> is a block diagram showing the prediction image synthesizer included in a device for decoding bit streams encoded by an encoding method where P+ and P&#x2212; frames coexist.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading>
<p id="p-0047" num="0046">First, in which circumstances the accumulated rounding errors as described in the &#x201c;Related Art&#x201d; occur must be considered. An example of an image sequences encoded by coding methods which can perform both unidirectional prediction and bidirectional prediction such as in MPEG.1, MPEG.2 and H.263 is shown in <figref idref="DRAWINGS">FIG. 5</figref>.</p>
<p id="p-0048" num="0047">An image <b>501</b> is a frame-coded by means of intraframe coding and is referred to as an I frame. In contrast, images <b>503</b>, <b>505</b>, <b>507</b>, <b>509</b> are called P frames and are coded by unidirectional interframe coding by using the previous I or P frame as the reference image. Accordingly, when for instance encoding image <b>505</b>, image <b>503</b> is used as the reference image and interframe prediction is performed. Images <b>502</b>, <b>504</b>, <b>506</b> and <b>508</b> are called B frames and bidirectional interframe prediction is performed utilizing the previous and subsequent I or P frame. The B frame is characterized by not being used as a reference image when interframe prediction is performed. Since motion compensation is not performed in I frames, the rounding error caused by motion compensation will not occur. In contrast, not only is motion compensation performed in the P frames but the P frame is also used as a reference image by other P or B frames so that it may be a cause leading to accumulated rounding errors. In the B frames on the other hand, motion compensation is performed so that the effect of accumulated rounding errors appears in the reconstructed image. However, due to the fact that B frames are not used as reference images, B frames cannot be a source of accumulated rounding errors. Thus, if accumulated rounding errors can be prevented in the P frame, then the bad effects of rounding errors can be alleviated in the overall image sequence. In H.263 a frame for coding a P frame and a B frame exists and is called a PB frame (For instance, frames <b>503</b> and <b>504</b> can both be encoded as a PB frame). If the combined two frames are viewed as separate frames, then the same principle as above can be applied. In other words, if countermeasures are taken versus rounding errors for the P frame part within a PB frame, then the accumulation of errors can be prevented.</p>
<p id="p-0049" num="0048">Rounding errors occur during interpolation of intensity values when a value obtained from normal division (division whose operation result is a real number) is a half (&#xbd;) integer (0.5 added to an integer) and this result is then rounded up to the next integer in the direction away from zero. For instance, when dividing by 4 to find an interpolated intensity value is performed, the rounding errors for the cases when the residual is 1 and 3 have equal absolute values but different signs. Consequently, the rounding errors caused by these two cases are canceled when the expectation for the rounding errors is calculated (in more general words, when dividing by a positive integer d&#x2032; is performed, the rounding errors caused by the cases when the residual is t and d&#x2032;&#x2212;t are cancelled). However, when the residual is 2, in other words when the result of normal division is a half integer, the rounding error cannot be canceled and leads to accumulated errors.</p>
<p id="p-0050" num="0049">To solve this problem, a method that allows the usage of two rounding methods can be used. The two rounding methods used here are: a rounding method that rounds half (&#xbd;) integers away from zero (0); and a rounding method that rounds half (&#xbd;) integers towards zero (0). By combining the usage of these two rounding methods, the rounding errors can be canceled. Hereafter, the rounding method that rounds the result of normal division to the nearest integer and rounds half integer values away from 0 is called &#x201c;positive rounding&#x201d;. Additionally, the rounding method that rounds the result of normal division to the nearest integer and rounds half (&#xbd;) integer values towards zero (0) is called &#x201c;negative rounding&#x201d;. The process of positive rounding used in block matching with half (&#xbd;) pixel accuracy is shown in Equation 3. When negative rounding is used instead, this equation can be rewritten as shown below.</p>
<p id="p-0051" num="0050">[Equation 4]
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>Ia=Ib </i><?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>Ib</i>=[(<i>La+Lb</i>)/2]<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>Ic</i>=[(<i>La+Lc</i>)/2]<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>Id</i>=[(<i>La+Lb+Lc+Ld+</i>1)/4]&#x2003;&#x2003;4<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0052" num="0051">Hereafter motion compensation methods that performs positive and negative rounding for the synthesis of interframe prediction images are called &#x201c;motion compensation using positive rounding&#x201d; and &#x201c;motion compensation using negative rounding&#x201d;, respectively. Furthermore, for P frames which use block matching with half (&#xbd;) pixel accuracy for motion compensation, a frame that uses positive rounding is called a &#x201c;P+ frame&#x201d; and a frame that uses negative rounding is called a &#x201c;P&#x2212; frame&#x201d; (under this definition, the P frames in H.263 are all P+ frames). The expectation for the rounding errors in P+ and P&#x2212; frames have equal absolute values but different signs. Accordingly, the accumulation of rounding errors can be prevented when P+ frames and P&#x2212; frames are alternately located along the time axis.</p>
<p id="p-0053" num="0052">In the example in <figref idref="DRAWINGS">FIG. 5</figref>, if the frames <b>503</b> and <b>507</b> are set as P+ frames and the frames <b>505</b> and <b>509</b> are set as P&#x2212; frames, then this method can be implemented. The alternate occurrence of P+ frames and P&#x2212; frames leads to the usage of a P+ frame and a P&#x2212; frame in the bidirectional prediction for B frames. Generally, the average of the forward prediction image (i.e. the prediction image synthesized by using frame <b>503</b> when frame <b>504</b> in <figref idref="DRAWINGS">FIG. 5</figref> is being encoded) and the backward prediction image (i.e. the prediction image synthesized by using frame <b>505</b> when frame <b>504</b> in <figref idref="DRAWINGS">FIG. 5</figref> is being encoded) is frequently used for synthesizing the prediction image for B frames. This means that using a P+ frame (which has a positive value for the expectation of the rounding error) and a P&#x2212; frame (which has a negative value for the expectation of the rounding error) in bidirectional prediction for a B frame is effective in canceling out the effects of rounding errors. Just as related above, the rounding process in the B frame will not be a cause of error accumulation. Accordingly, no problem will occur even if the same rounding method is applied to all the B frames. For instance, no serious degradation of decoded images is caused even if motion compensation using positive rounding is performed for all of the B frames <b>502</b>, <b>504</b>, <b>506</b>, and <b>508</b> in <figref idref="DRAWINGS">FIG. 5</figref>. Preferably only one type of rounding is performed for a B frame, in order to simplify the B frame decoding process.</p>
<p id="p-0054" num="0053">A block matching section <b>1600</b> of an image encoder according to the above described motion compensation method utilizing multiple rounding methods is shown in <figref idref="DRAWINGS">FIG. 16</figref>. Numbers identical to those in other drawings indicate the same part. By substituting the block matching section <b>116</b> of <figref idref="DRAWINGS">FIG. 1</figref> with <b>1600</b>, multiple rounding methods can be used. Motion estimation processing between the input image <b>101</b> and the decoded image of the previous frame is performed in a motion estimator <b>1601</b>. As a result, motion information <b>120</b> is output. This motion information is utilized in the synthesis of the prediction image in a prediction image synthesizer <b>1603</b>.</p>
<p id="p-0055" num="0054">A rounding method determination device <b>1602</b> determines whether to use positive rounding or negative rounding as the rounding method for the frame currently being encoded. Information <b>1604</b> relating to the rounding method that was determined is input to the prediction image synthesizer <b>1603</b>. In this prediction image synthesizer <b>1603</b>, a prediction image <b>117</b> is synthesized and output based on the rounding method determined by means of information <b>1604</b>. In the block matching section <b>116</b> in <figref idref="DRAWINGS">FIG. 1</figref>, there are no items equivalent to <b>1602</b>, <b>1604</b> of <figref idref="DRAWINGS">FIG. 16</figref>, and the prediction image is synthesized only by positive rounding. Also, the rounding method <b>1605</b> determined at the block matching section can be output, and this information can then be multiplexed into the bit stream and be transmitted.</p>
<p id="p-0056" num="0055">A prediction image synthesizer <b>1700</b> of an image decoder which can decode bit streams generated by a coding method using multiple rounding methods is shown in <figref idref="DRAWINGS">FIG. 17</figref>. Numbers identical to those in other drawings indicate the same part. By substituting the prediction image synthesizer <b>211</b> of <figref idref="DRAWINGS">FIG. 2</figref> by <b>1700</b>, multiple rounding methods can be used. In the rounding method determination device <b>1701</b>, the rounding method appropriate for prediction image synthesis in the decoding process is determined. In order to carry out decoding correctly, the rounding method selected here must be the same as the rounding method that was selected for encoding.</p>
<p id="p-0057" num="0056">For instance the following rule can be shared between the encoder and decoder: When the current frame is a P frame and the number of P frames (including the current frame) counted from the most recent I frame is odd, then the current frame is a P+ frame. When this number is even, then the current frame is a P&#x2212; frame. If the rounding method determination device on the encoding side (For instance, <b>1602</b> in <figref idref="DRAWINGS">FIG. 16</figref>) and the rounding method determination device <b>1701</b> conform to this common rule, then the images can correctly be decoded. The prediction image is synthesized in the prediction image synthesizer <b>1703</b> using motion information <b>202</b>, decoding image <b>210</b> of the prior frame, and information <b>1702</b> related to the rounding method determined as just described. This prediction image <b>212</b> is output and then used for the synthesis of the decoded image.</p>
<p id="p-0058" num="0057">As an alternative to the above mentioned case, a case where the information related to the rounding method is multiplexed in the transmitted bit stream can also be considered (such bit stream can be generated at the encoder by outputting the information <b>1605</b> related to the rounding method from the block matching section depicted in <figref idref="DRAWINGS">FIG. 16</figref>). In such case, the rounding method determiner device <b>1701</b> is not used, and information <b>1704</b> related to the rounding method extracted from the encoded bit stream is used at the prediction image synthesizer <b>1703</b>.</p>
<p id="p-0059" num="0058">Besides the image encoder and the image decoder utilizing the custom circuits and custom chips of the conventional art as shown in <figref idref="DRAWINGS">FIG. 1</figref> and <figref idref="DRAWINGS">FIG. 2</figref>, this invention can also be applied to software image encoders and software image decoders utilizing general-purpose processors. A software image encoder <b>600</b> and a software image decoder <b>700</b> are shown in <figref idref="DRAWINGS">FIG. 6</figref> and <figref idref="DRAWINGS">FIG. 7</figref>. In the software image encoder <b>600</b>, an input image <b>601</b> is first stored in the input frame memory <b>602</b> and the general-purpose processor <b>603</b> loads information from here and performs encoding. The program for driving this general-purpose processor is loaded from a storage device <b>608</b> which can be a hard disk, floppy disk, etc. and stored in a program memory <b>604</b>. This general purpose processor also uses a process memory <b>605</b> to perform the encoding. The encoding information output by the general-purpose processor is temporarily stored in the output buffer <b>606</b> and then output as an encoded bit stream <b>607</b>.</p>
<p id="p-0060" num="0059">A flowchart for the encoding software (recording medium readable by computer) is shown in <figref idref="DRAWINGS">FIG. 8</figref>. The process starts in <b>801</b>, and the value 0 is assigned to variable N in <b>802</b>. Next, in <b>803</b> and <b>804</b>, the value 0 is assigned to N when the value for N is 100. N is a counter for the number of frames. 1 is added for each one frame whose processing is complete, and values from 0 to 99 are allowed when performing coding. When the value for N is 0, the current frame is an I frame. When N is an odd number, the current frame is a P+ frame, and when an even number other than 0, the current frame is a P&#x2212; frame. When the upper limit for the value of N is 99, it means that one I frame is coded after 99 P frames (P+ frames or P&#x2212; frames) are coded. By always inserting one I frame in a certain number of coded frames, the following benefits can be obtained: (a) Error accumulation due to a mismatch between encoder and decoder processing can be prevented (for instance, a mismatch in the computation of DCT); and (b) The processing load for acquiring the reproduced image of the target frame from the coded data (random access) is reduced. The optimal N value varies when the encoder performance or the environment where the encoder is used are changed. It does not mean, therefore, that the value of N must always be 100.</p>
<p id="p-0061" num="0060">The process for determining the rounding method and coding mode for each frame is performed in <b>805</b> and the flowchart with details of this operation is shown in <figref idref="DRAWINGS">FIG. 9</figref>. First of all, whether N is a zero (0) or not is checked in <b>901</b>. If N is 0, then &#x2018;I&#x2019; is output as distinction information of the prediction mode, to the output buffer in <b>902</b>. This means that the image to be coded is will be coded as an I frame. Here, &#x201c;output to the output buffer&#x201d; means that after being stored in the output buffer, the information is output to an external device as a portion of the coded bit stream. When N is not 0, then whether N is an odd or even number is identified in <b>904</b>. When N is an odd number, &#x2018;+&#x2019; is output to the output buffer as the distinction information for the rounding method in <b>905</b>, and the image to be coded will be coded as a P+ frame. On the other hand, when N is an even number, &#x2018;&#x2212;&#x2019; is output to the output buffer as the distinction information for the rounding method in <b>906</b>, and the image to be coded will be coded as a P&#x2212; frame.</p>
<p id="p-0062" num="0061">The process again returns to <figref idref="DRAWINGS">FIG. 8</figref>, where after determining the coding mode in <b>805</b>, the input image is stored in the frame memory A in <b>806</b>. The frame memory A referred to here signifies a portion of the memory zone (for instance, the memory zone maintained in the memory of <b>605</b> in <figref idref="DRAWINGS">FIG. 6</figref>) of the software encoder. In <b>807</b>, it is checked whether the frame currently being coded is an I frame. When not identified as an I frame, motion estimation and motion compensation is performed in <b>808</b>.</p>
<p id="p-0063" num="0062">The flowchart in <figref idref="DRAWINGS">FIG. 10</figref> shows details of this process performed in <b>808</b>. First of all, in <b>1001</b>, motion estimation is performed between the images stored in frame memories A and B (just as written in the final part of this paragraph, the decoded image of the prior frame is stored in frame memory B). The motion vector for each block is found, and this motion vector is sent to the output buffer. Next, in <b>1002</b>, whether or not the current frame is a P+ frame is checked. When the current frame is a P+ frame, the prediction image is synthesized in <b>1003</b> utilizing positive rounding and this prediction image is stored in frame memory C. On the other hand, when the current frame is a P&#x2212; frame, the prediction image is synthesized in <b>1004</b> utilizing negative rounding and this prediction image is stored in the frame memory C. Next, in <b>1005</b>, the differential image between frame memories A and C is found and stored in frame memory A.</p>
<p id="p-0064" num="0063">Here, the process again returns to <figref idref="DRAWINGS">FIG. 8</figref>. Prior to starting the processing in <b>809</b>, the input image is stored in frame memory A when the current frame is an I frame, and the differential image between the input image and the prediction image is stored in frame memory A when the current frame is a P frame (P+ or P&#x2212; frame). In <b>809</b>, DCT is applied to the image stored in frame memory A, and the DCT coefficients calculated here are sent to the output buffer after being quantized. In <b>810</b>, inverse quantization is performed to the quantized DCT coefficients and inverse DCT is applied. The image obtained by applying inverse DCT is stored in frame memory B. Next in <b>811</b>, it is checked again whether the current frame is an I frame. When the current frame is not an I frame, the images stored in frame memory B and C are added and the result is stored in frame memory B. The coding process of a frame ends here, and the image stored in frame memory B before going into 813 is the reconstructed image of this frame (this image is identical with the one obtained at the decoding side). In <b>813</b>, it is checked whether the frame whose coding has just finished is the final frame in the sequence. If this is true, the coding process ends. If this frame is not the final frame, 1 is added to N in <b>814</b>, and the process again returns to <b>803</b> and the coding process for the next frame starts.</p>
<p id="p-0065" num="0064">A software decoder <b>700</b> is shown in <figref idref="DRAWINGS">FIG. 7</figref>. After the coded bit stream <b>701</b> is temporarily stored in the input buffer <b>702</b>, this bit stream is then loaded into the general-purpose processor <b>703</b>. The program for driving this general-purpose processor is loaded from a storage device <b>708</b> which can be a hard disk, floppy disk, etc. and stored in a program memory <b>704</b>. This general-purpose processor also uses a process memory <b>605</b> to perform the decoding. The decoded image obtained by the decoding process is temporarily stored in the output frame memory <b>706</b> and then sent out as the output image <b>707</b>.</p>
<p id="p-0066" num="0065">A flowchart of the decoding software for the software decoder <b>700</b> shown in <figref idref="DRAWINGS">FIG. 7</figref> is shown in <figref idref="DRAWINGS">FIG. 11</figref>. The process starts in <b>1101</b>, and it is checked in <b>1102</b> whether input information is present. If there is no input information, the decoding process ends in <b>1103</b>. When input information is present, distinction information of the prediction mode is input in <b>1104</b>. The word &#x201c;input&#x201d; used here means that the information stored in the input buffer (for instance <b>702</b> of <figref idref="DRAWINGS">FIG. 7</figref>) is loaded by the general-purpose processor. In <b>1105</b>, it is checked whether the encoding mode distinction information is &#x201c;I&#x201d;. When not &#x201c;I&#x201d;, the distinction information for the rounding method is input and synthesis of the interframe prediction image is performed in <b>1107</b>.</p>
<p id="p-0067" num="0066">A flowchart showing details of the operation in <b>1107</b> is shown in <figref idref="DRAWINGS">FIG. 12</figref>. In <b>1201</b>, a motion vector is input for each block. Then, in <b>1202</b>, it is checked whether the distinction information for the rounding method loaded in <b>1106</b> is a &#x201c;+&#x201d;. When this information is &#x201c;+&#x201d;, the frame currently being decoded is a P+ frame. In this case, the prediction image is synthesized using positive rounding in <b>1203</b>, and the prediction image is stored in frame memory D. Here, frame memory D signifies a portion of the memory zone of the software decoder (for instance, this memory zone is obtained in the processing memory <b>705</b> in <figref idref="DRAWINGS">FIG. 7</figref>). When the distinction information of the rounding method is not &#x201c;+&#x201d;, the current frame being decoded is a P&#x2212; frame. The prediction image is synthesized using negative rounding in <b>1204</b> and this prediction image is stored in frame memory D. At this point, if a P+ frame is decoded as a P&#x2212; frame due to some type of error, or conversely if a P&#x2212; frame is decoded as a P+ frame, the correct prediction image is not synthesized in the decoder and the quality of the decoded image deteriorates.</p>
<p id="p-0068" num="0067">After synthesizing the prediction image, the operation returns to <figref idref="DRAWINGS">FIG. 11</figref> and the quantized DCT coefficients is input in <b>1108</b>. Inverse quantization and inverse DCT is then applied to these coefficients and the resulting image is stored in frame memory E. In <b>1109</b>, it is checked again whether the frame currently being decoded is an I frame. If the current frame is not an I frame, images stored in frame memory D and E are added in <b>1110</b> and the resulting sum image is stored in frame memory E. The image stored in frame memory E before starting the process in <b>1111</b> is the reconstructed image. This image stored in frame memory E is output to the output frame memory (for instance, <b>706</b> in <figref idref="DRAWINGS">FIG. 7</figref>) in <b>1111</b>, and then output from the decoder as the reconstructed image. The decoding process for a frame is completed here and the process for the next frame starts by returning to <b>1102</b>.</p>
<p id="p-0069" num="0068">When a software based on the flowchart shown in <figref idref="DRAWINGS">FIGS. 8-12</figref> is run in the software image encoders or decoders, the same effect as when custom circuits and custom chips are utilized are obtained.</p>
<p id="p-0070" num="0069">A storage media (recording media) with the bit stream generated by the software encoder <b>601</b> of <figref idref="DRAWINGS">FIG. 6</figref> being recorded is shown in <figref idref="DRAWINGS">FIG. 13</figref>. It is assumed that the algorithms shown in the flowcharts of <figref idref="DRAWINGS">FIGS. 8-10</figref> is used in the software encoder. Digital information is recorded concentrically on a recording disk <b>1301</b> capable of recording digital information (for instance magnetic disks, optical disk, etc.). A portion <b>1302</b> of the information recorded on this digital disk includes: prediction mode distinction information <b>1303</b>, <b>1305</b>, <b>1308</b>, <b>1311</b>, and <b>1314</b>; rounding method distinction information <b>1306</b>, <b>1309</b>, <b>1312</b>, and <b>1315</b>; and motion vector and DCT coefficient information <b>1304</b>, <b>1307</b>, <b>1310</b>, <b>1313</b>, and <b>1316</b>. Information representing &#x2018;I&#x2019; is recorded in <b>1303</b>, &#x2018;P&#x2019; is recorded in <b>1305</b>, <b>1308</b>, <b>1311</b>, and <b>1314</b>, &#x2018;+&#x2019; is recorded in <b>1306</b>, and <b>1312</b>, and &#x2018;&#x2212;&#x2019; is recorded in <b>1309</b>, and <b>1315</b>. In this case, &#x2018;I&#x2019; and &#x2018;+&#x2019; can be represented by a single bit of zero (0), and &#x2018;P&#x2019; and &#x2018;&#x2212;&#x2019; can be represented by a single bit of one (1). Using this representation, the decoder can correctly interpret the recorded information and the correct reconstructed image is synthesized. By storing a coded bit stream in a storage media using the method described above, the accumulation of rounding errors is prevented when the bit stream is read and decoded.</p>
<p id="p-0071" num="0070">A storage media with the bit stream of the coded data of the image sequence shown in <figref idref="DRAWINGS">FIG. 5</figref> being recorded is shown in <figref idref="DRAWINGS">FIG. 15</figref>. The recorded bit stream includes information related to P+, P&#x2212;, and B frames. In the same way as in <b>1301</b> of <figref idref="DRAWINGS">FIG. 13</figref>, digital information is recorded concentrically on a record disk <b>1501</b> capable for recording digital information (for instance, magnetic disks, optical disks, etc.). A portion <b>1502</b> of the digital information recorded on this digital disk includes: prediction mode distinction information <b>1503</b>, <b>1505</b>, <b>1508</b>, <b>1510</b>, and <b>1513</b>; rounding method distinction information <b>1506</b>, and <b>1512</b>; and motion vector and DCT coefficient information <b>1504</b>, <b>1507</b>, <b>1509</b>, <b>1511</b>, and <b>1514</b>. Information representing &#x2018;I&#x2019; is recorded in <b>1503</b>, &#x2018;P&#x2019; is recorded in <b>1505</b>, and <b>1510</b>, &#x2018;B&#x2019; is recorded in <b>1508</b>, and <b>1513</b>, &#x2018;+&#x2019; is recorded in <b>1505</b>, and &#x2018;&#x2212;&#x2019; is recorded in <b>1511</b>. In this case, &#x2018;I&#x2019;, &#x2018;P&#x2019; and &#x2018;B&#x2019; can be represented respectively by two bit values 00, 01, and 10, and &#x2018;+&#x2019; and is &#x2018;&#x2212;&#x2019; can be represented respectively by one bit values 0 and 1. Using this representation, the decoder can correctly interpret the recorded information and the correct reconstructed is synthesized.</p>
<p id="p-0072" num="0071">In <figref idref="DRAWINGS">FIG. 15</figref>, information related to frame <b>501</b> (I frame) in <figref idref="DRAWINGS">FIG. 5</figref> is <b>1503</b> and <b>1504</b>, information related to <b>502</b> (B frame) is <b>1508</b> and <b>1509</b>, information related to frame <b>503</b> (P+ frame) is <b>1505</b> and <b>1507</b>, information related to frame <b>504</b> (B frame) is <b>1513</b> and <b>1514</b>, and information related to frame <b>505</b> (P&#x2212; frame) is <b>1510</b> and <b>1512</b>. When coding image sequences are coded using B frames, the transmission order and display order of frames are usually different. This is because the previous and subsequent reference images need to be coded before the prediction image for the B frame is synthesized. Consequently, in spite of the fact that the frame <b>502</b> is displayed before frame <b>503</b>, information related to frame <b>503</b> is transmitted before information related to frame <b>502</b>.</p>
<p id="p-0073" num="0072">As described above, there is no need to use multiple rounding methods for B frames since motion compensation in B frames do not cause accumulation of rounding errors. Therefore, as shown in this example, information that specifies rounding methods (e.g. &#x2018;+&#x2019; and &#x2018;&#x2212;&#x2019;) is not transmitted for B frames. Thus for instance, even if only positive rounding is applied to B frames, the problem of accumulated rounding errors does not occur. By storing coded bit streams containing information related to B frames in a storage media in the way described above, the occurrence of accumulated rounding errors can be prevented when this bit stream is read and decoded.</p>
<p id="p-0074" num="0073">Specific examples of coders and decoders using the coding method described in this specification is shown in <figref idref="DRAWINGS">FIG. 14</figref>. The image coding and decoding method can be utilized by installing image coding and decoding software into a computer <b>1401</b>. This software is recorded in some kind of storage media (CD-ROM, floppy disk, hard disk, etc.) <b>1412</b>, loaded into a computer and then used. Additionally, the computer can be used as an image communication terminal by connecting the computer to a communication lines. It is also possible to install the decoding method described in this specification into a player device <b>1403</b> that reads and decodes the coded bit stream recorded in a storage media <b>1402</b>. In this case, the reconstructed image signal can be displayed on a television monitor <b>1404</b>. The device <b>1403</b> can be used only for reading the coded bit stream, and in this case, the decoding device can be installed in the television monitor <b>1404</b>. It is well known that digital data transmission can be realized using satellites and terrestrial waves. A decoding device can also be installed in a television receiver <b>1405</b> capable of receiving such digital transmissions. Also, a decoding device can also be installed inside a set top box <b>1409</b> connected to a satellite/terrestrial wave antenna, or a cable <b>1408</b> of a cable television system, so that the reconstructed images can be displayed on a television monitor <b>1410</b>. In this case, the decoding device can be incorporated in the television monitor rather than in the set top box, as in the case of <b>1404</b>. The layout of a digital satellite broadcast system is shown in <b>1413</b>, <b>1414</b> and <b>1415</b>. The video information in the coded bit stream is transmitted from a broadcast station <b>1413</b> to a communication or broadcast satellite <b>1414</b>. The satellite receives this information, sends it to a home <b>1415</b> having equipment for receiving satellite broadcast programs, and the video information is reconstructed and displayed in this home using devices such as a television receiver or a set top box.</p>
<p id="p-0075" num="0074">Digital image communication using mobile terminals <b>1406</b> has recently attracted considerable attention, due to the fact that image communication at very low bit rates has become possible. Digital portable terminals can be categorized in the following three types: a transceiver having both an encoder and decoder; a transmitter having only an encoder; and a receiver having only a decoder.</p>
<p id="p-0076" num="0075">An encoding device can be installed in a video camera recorder <b>1407</b>. The camera can also be used just for capturing the video signal and this signal can be supplied to a custom encoder <b>1411</b>. All of the devices or systems shown in this drawing can be equipped with the coding and/or decoding method described in this specification. By using this coding and/or decoding method in these devices or systems, images of higher quality compared with those images obtained using conventional technologies can be obtained. The following variations are clearly included within the scope of this invention.</p>
<p id="p-0077" num="0076">(i) A prerequisite of the above described principle was the use of block matching as a motion compensation method. However, this invention is further capable of being applied to all image sequence coding and decoding methods in which motion compensation is performed by taking a value for the vertical and horizontal components of the pixel motion vector that is other than an integer multiple of the sampling period in the vertical and horizontal directions of the pixel, and then finding by interpolation, the intensity value of a position where the sample value is not present. Thus for instance, the global motion compensation listed in Japanese Patent Application No. 8-60572 published as Japanese Patent Application Laid-Open No. 9-252470 and the warping prediction listed in Japanese Patent Application No. 8-249601 published as Japanese Patent Application Laid-Open No. 10-98729 are applicable to the method of this invention.</p>
<p id="p-0078" num="0077">(ii) The description of the invention only mentioned the case where a value integral multiple of &#xbd; was taken for the horizontal and vertical components of the motion vector. However, this invention is also generally applicable to methods in which integral multiples of 1/d (d is a positive integer and also an even number) are allowed for the horizontal and vertical components of the motion vector. However, when d becomes large, the divisor for division in bilinear interpolation (square of &#x201c;d&#x201d;, see Equation 2) also becomes large, so that in contrast, the probability of results from normal division reaching a value of 0.5 become low. Accordingly, when performing only positive rounding, the absolute value of the expectation for rounding errors becomes small and the bad effects caused by accumulated errors become less conspicuous. Also applicable to the method of this invention, is a motion compensation method where for instance, the d value is variable, both positive rounding and negative rounding are used when d is smaller than a fixed value, and only positive rounding or only negative rounding is used when the value of d is larger than a fixed value.</p>
<p id="p-0079" num="0078">(iii) As mentioned in the &#x201c;Related Art&#x201d; section, when DCT is utilized as an error coding method, the adverse effects from accumulated rounding errors are prone to appear when the quantized step size of the DCT coefficient is large. However a method is also applicable to the invention, in which, when the quantization step size of DCT coefficients is larger than a threshold value then both positive rounding and negative rounding are used. When the quantization step size of the DCT coefficients is smaller than the threshold value then only positive rounding or only negative rounding is used.</p>
<p id="p-0080" num="0079">(iv) In cases where error accumulations occur on the luminance plane and cases where error accumulations occur on the chrominance plane, the bad effects on the reconstructed images are generally more serious in the case of error accumulations on the chrominance plane. This is due to the fact that rather than cases where the image darkens or lightens slightly, cases where overall changes in the image color happen are more conspicuous. However, a method is also applicable to this invention in which both positive rounding and negative rounding are used for the chrominance signal, and only positive rounding or negative rounding is used for the luminance signal.</p>
<p id="p-0081" num="0080">As described in the &#x201c;Related Art&#x201d; section, &#xbc; pixel accuracy motion vectors obtained by halving the &#xbd; pixel accuracy motion vectors are rounded to &#xbd; pixel accuracy in H.263. However by adding certain changes to this method, the absolute expectation value for rounding errors can be reduced. In H.263 that was mentioned in the related art, a value which is half the horizontal or vertical components of the motion vector for the luminance plane is expressed as r+s/4 (r is an integer, s is an integer less than 4 and not smaller than 0), and when s is 1 or 3, a rounding operation is performed to obtain a 2. This operation can be changed as follows: When s is 1, a rounding operation is performed to obtain a zero &#x201c;0&#x201d;, and when s is 3 a 1 is be added to r to make s a &#x201c;0&#x201d;. By performing these operations, the number of times that the intensity values at positions <b>406</b>-<b>408</b> in <figref idref="DRAWINGS">FIG. 4</figref> is definitely reduced (probability that horizontal and vertical components of motion vector will be an integer become high) so that the absolute expectation value for the rounding error becomes small. However, even if the size of the error occurring in this method can be limited, the accumulation of errors cannot be completely prevented.</p>
<p id="p-0082" num="0081">(v) The invention described in this specification is applicable to a method that obtains the final interframe prediction image by averaging the prediction images obtained by different motion compensation methods. For example, in the method described in Japanese Patent Application No. 8-3616 published as Japanese Patent Application Laid-Open No. 9-200763, interframe prediction images obtained by the following two methods are averaged: block matching in which a motion vector is assigned to each 16&#xd7;16 pixel block; and block matching in which a motion vector is assigned to each 8&#xd7;8 pixel blocks. In this method, rounding is also performed when calculating the average of the two prediction images. When only positive rounding is continuously performed in this averaging operation, a new type of rounding error accumulates. This problem can be solved by using multiple rounding methods for this averaging operation. In this method, negative rounding is performed in the averaging operation when positive rounding is performed in block matching. Conversely, positive rounding is used for the averaging when negative rounding is used for block matching. By using different rounding methods for averaging and block matching, the rounding errors from two different sources is cancelled within the same frame.</p>
<p id="p-0083" num="0082">(vi) When utilizing a method that alternately locates P+ frames and P&#x2212; frames along the time axis, the encoder or the decoder needs to determine whether the currently processed P frame is a P+ frame or a P&#x2212; frame. The following is an example of such identification method: A counter counts the number of P frames after the most recently coded or decoded I frame, and the current P frame is a P+ frame when the number is odd, and a P&#x2212; frame when the number is even (this method is referred to as an implicit scheme). There is also a method for instance, that writes into the header section of the coded image information, information to identify whether the currently coded P frame at the encoder is a P+ frame or a P&#x2212; frame (this method is referred to as an explicit scheme). Compared with the implicit method, this method is well able to withstand transmission errors, since there is no need to count the number of P frames.</p>
<p id="p-0084" num="0083">Additionally, the explicit method has the following advantages: As described in the &#x201c;Related Art&#x201d; section, past encoding standards (such as MPEG-1 or MPEG-2) use only positive rounding for motion compensation. This means for instance that the motion estimation/motion compensation devices (for example equivalent to <b>106</b> in <figref idref="DRAWINGS">FIG. 1</figref>) for MPEG-1/MPEG-2 on the market are not compatible with coding methods that use both P+ frames and P&#x2212; frames. It is assumed that there is a decoder which can decode bit streams generated by a coding method that uses P+ frames and P&#x2212; frames. In this case if the decoder is based on the above mentioned implicit method, then it will be difficult to develop an encoder that generates bit streams that can be correctly decoded by the above mentioned decoder, using the above mentioned motion estimation/compensation device for MPEG-1/MPEG-2.</p>
<p id="p-0085" num="0084">However, if the decoder is based on the above mentioned explicit method, this problem can be solved. An encoder using an MFEG-1/MPEG-2 motion estimation/motion compensation device can continuously send P+ frames, by continuously writing rounding method distinction information indicating positive rounding into the frame information header. When this is performed, a decoder based on the explicit method can correctly decode the bit stream generated by this encoder. Of course, it should be more likely in such case that the accumulation of rounding errors occurs, since only P+ frames are present. However, error accumulation is not a serious problem in cases where the encoder uses only small values as the quantization step size for the DCT coefficients (an example for such coders is a custom encoder used only for high rate coding). In addition to this interoperability between past standards, the explicit method further have the following advantages: (a) the equipment cost for high rate custom encoders and coders not prone to rounding error accumulation due to frequent insertion of I frames can be reduced by installing only positive or negative rounding as the pixel value rounding method for motion compensation; and (b) the above encoders not prone to rounding error accumulation have the advantage in that there is no need to decide whether to code the current frame as a P+ or P&#x2212; frame, and the processing is simplified.</p>
<p id="p-0086" num="0085">(vii) The invention described in this specification is applicable to coding and decoding methods that applies filtering accompanying rounding to the interframe prediction images. For instance, in the international standard H.261 for image sequence coding, a low-pass filter (called a &#x201c;loop filter&#x201d;) is applied to block signals whose motion vectors are not zero (0) in interframe prediction images. Also, in H.263, filters can be used to smooth out discontinuities on block boundaries (blocking artifacts). All of these filters perform weighted averaging to pixel intensity values and rounding is then performed on the averaged intensity values. Even for these cases, selective use of positive rounding and negative rounding is effective for preventing error accumulation.</p>
<p id="p-0087" num="0086">(viii) Besides I P+P&#x2212; P+ P&#x2212; . . . , various methods for mixing P+ frames and P&#x2212; frames such as I P+ P+ P&#x2212; P&#x2212; P+ P+ . . . , or I P+ P&#x2212; P&#x2212; P+ P+ . . . are applicable to the method of this invention. For instance, using a random number generator that outputs 0 and 1 both at a probability of 50 percent, the encoder can code a P+ and P&#x2212; frame when the output is 0 and 1, respectively. In any case, the less the difference in probability that P+ frames and P&#x2212; frames occur in a certain period of time, the less the rounding error accumulation is prone to occur. Further, when the encoder, is allowed to mix P+ frames and P&#x2212; frames by an arbitrary method, the encoder and decoder must operate based on the explicit method and not with the implicit method described above. Accordingly, the explicit method is superior when viewed from the perspective of allowing flexibility configuration for the encoder and decoder.</p>
<p id="p-0088" num="0087">(ix) The invention described in this specification does not limit the pixel value interpolation method to bilinear interpolation. Interpolation methods for intensity values can generally be described by the following equation:</p>
<p id="p-0089" num="0088">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mo>[</mo>
        <mrow>
          <mi>Equation</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>5</mn>
        </mrow>
        <mo>]</mo>
      </mrow>
    </mtd>
    <mtd>
      <mstyle>
        <mspace width="0.3em" height="0.3ex"/>
      </mstyle>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>R</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mrow>
                <mi>x</mi>
                <mo>+</mo>
                <mi>r</mi>
              </mrow>
              <mo>,</mo>
              <mrow>
                <mi>y</mi>
                <mo>+</mo>
                <mi>s</mi>
              </mrow>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mi>T</mi>
          <mo>(</mo>
          <mrow>
            <munderover>
              <mo>&#x2211;</mo>
              <mrow>
                <mi>j</mi>
                <mo>=</mo>
                <mrow>
                  <mo>-</mo>
                  <mi>x</mi>
                </mrow>
              </mrow>
              <mi>x</mi>
            </munderover>
            <mo>&#x2062;</mo>
            <mrow>
              <munderover>
                <mo>&#x2211;</mo>
                <mrow>
                  <mi>j</mi>
                  <mo>=</mo>
                  <mrow>
                    <mo>-</mo>
                    <mi>x</mi>
                  </mrow>
                </mrow>
                <mi>x</mi>
              </munderover>
              <mo>&#x2062;</mo>
              <mrow>
                <mrow>
                  <mi>h</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mrow>
                        <mi>r</mi>
                        <mo>-</mo>
                        <mi>j</mi>
                      </mrow>
                      <mo>,</mo>
                      <mrow>
                        <mi>s</mi>
                        <mo>-</mo>
                        <mi>k</mi>
                      </mrow>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>&#x2062;</mo>
                <mrow>
                  <mi>R</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mrow>
                        <mi>x</mi>
                        <mo>+</mo>
                        <mi>j</mi>
                      </mrow>
                      <mo>,</mo>
                      <mrow>
                        <mi>y</mi>
                        <mo>+</mo>
                        <mi>k</mi>
                      </mrow>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>5</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where, r and s are real numbers, h(r, s) is a function for interpolating the real numbers, and T(z) is a function for rounding the real number z. The definitions of R (x, y), x, and y are the same as in Equation 4.
</p>
<p id="p-0090" num="0089">Motion compensation utilizing positive rounding is performed when T (z) is a function representing positive rounding, and motion compensation utilizing negative rounding is performed when the function representing negative rounding. This invention is applicable to interpolation methods that can be described using Equation 5. For instance, bilinear interpolation can be described by defining h(r, s) as shown below.</p>
<p id="p-0091" num="0090">
<maths id="MATH-US-00003" num="00003">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mo>[</mo>
        <mrow>
          <mi>Equation</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>6</mn>
        </mrow>
        <mo>]</mo>
      </mrow>
    </mtd>
    <mtd>
      <mstyle>
        <mspace width="0.3em" height="0.3ex"/>
      </mstyle>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mtable>
        <mtr>
          <mtd>
            <mrow>
              <mrow>
                <mrow>
                  <mi>h</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mi>r</mi>
                      <mo>,</mo>
                      <mi>s</mi>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>=</mo>
                <mrow>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mn>1</mn>
                      <mo>-</mo>
                      <mrow>
                        <mo>&#xf603;</mo>
                        <mi>r</mi>
                        <mo>&#xf604;</mo>
                      </mrow>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                  <mo>&#x2062;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mn>1</mn>
                      <mo>-</mo>
                      <mrow>
                        <mo>&#xf603;</mo>
                        <mi>s</mi>
                        <mo>&#xf604;</mo>
                      </mrow>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
              <mo>,</mo>
            </mrow>
          </mtd>
          <mtd>
            <mrow>
              <mrow>
                <mn>0</mn>
                <mo>&#x2264;</mo>
                <mrow>
                  <mo>&#xf603;</mo>
                  <mi>r</mi>
                  <mo>&#xf604;</mo>
                </mrow>
                <mo>&#x2264;</mo>
                <mn>1</mn>
              </mrow>
              <mo>,</mo>
              <mrow>
                <mn>0</mn>
                <mo>&#x2264;</mo>
                <mrow>
                  <mo>&#xf603;</mo>
                  <mi>s</mi>
                  <mo>&#xf604;</mo>
                </mrow>
                <mo>&#x2264;</mo>
                <mn>1</mn>
              </mrow>
              <mo>,</mo>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mrow>
              <mn>0</mn>
              <mo>,</mo>
            </mrow>
          </mtd>
          <mtd>
            <mrow>
              <mi>otherwise</mi>
              <mo>.</mo>
            </mrow>
          </mtd>
        </mtr>
      </mtable>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>6</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0092" num="0091">However, if for instance h(r,s) is defined as shown below,</p>
<p id="p-0093" num="0092">
<maths id="MATH-US-00004" num="00004">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mo>[</mo>
        <mrow>
          <mi>Equation</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>7</mn>
        </mrow>
        <mo>]</mo>
      </mrow>
    </mtd>
    <mtd>
      <mstyle>
        <mspace width="0.3em" height="0.3ex"/>
      </mstyle>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mtable>
        <mtr>
          <mtd>
            <mrow>
              <mrow>
                <mrow>
                  <mi>h</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mi>r</mi>
                      <mo>,</mo>
                      <mi>s</mi>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>=</mo>
                <mrow>
                  <mn>1</mn>
                  <mo>-</mo>
                  <mrow>
                    <mo>&#xf603;</mo>
                    <mi>r</mi>
                    <mo>&#xf604;</mo>
                  </mrow>
                  <mo>-</mo>
                  <mrow>
                    <mo>&#xf603;</mo>
                    <mi>s</mi>
                    <mo>&#xf604;</mo>
                  </mrow>
                </mrow>
              </mrow>
              <mo>,</mo>
            </mrow>
          </mtd>
          <mtd>
            <mrow>
              <mrow>
                <mn>0</mn>
                <mo>&#x2264;</mo>
                <mrow>
                  <mrow>
                    <mo>&#xf603;</mo>
                    <mi>r</mi>
                    <mo>&#xf604;</mo>
                  </mrow>
                  <mo>+</mo>
                  <mrow>
                    <mo>&#xf603;</mo>
                    <mi>s</mi>
                    <mo>&#xf604;</mo>
                  </mrow>
                </mrow>
                <mo>&#x2264;</mo>
                <mn>1</mn>
              </mrow>
              <mo>,</mo>
              <mrow>
                <mi>rs</mi>
                <mo>&#x3c;</mo>
                <mn>0</mn>
              </mrow>
              <mo>,</mo>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mrow>
              <mrow>
                <mn>1</mn>
                <mo>-</mo>
                <mrow>
                  <mo>&#xf603;</mo>
                  <mi>r</mi>
                  <mo>&#xf604;</mo>
                </mrow>
              </mrow>
              <mo>,</mo>
            </mrow>
          </mtd>
          <mtd>
            <mrow>
              <mrow>
                <mrow>
                  <mo>&#xf603;</mo>
                  <mi>r</mi>
                  <mo>&#xf604;</mo>
                </mrow>
                <mo>&#x2265;</mo>
                <mrow>
                  <mo>&#xf603;</mo>
                  <mi>s</mi>
                  <mo>&#xf604;</mo>
                </mrow>
              </mrow>
              <mo>,</mo>
              <mrow>
                <mrow>
                  <mo>&#xf603;</mo>
                  <mi>r</mi>
                  <mo>&#xf604;</mo>
                </mrow>
                <mo>&#x2264;</mo>
                <mn>1</mn>
              </mrow>
              <mo>,</mo>
              <mrow>
                <mi>rs</mi>
                <mo>&#x2265;</mo>
                <mn>0</mn>
              </mrow>
              <mo>,</mo>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mrow>
              <mrow>
                <mn>1</mn>
                <mo>-</mo>
                <mrow>
                  <mo>&#xf603;</mo>
                  <mi>s</mi>
                  <mo>&#xf604;</mo>
                </mrow>
              </mrow>
              <mo>,</mo>
            </mrow>
          </mtd>
          <mtd>
            <mrow>
              <mrow>
                <mrow>
                  <mo>&#xf603;</mo>
                  <mi>s</mi>
                  <mo>&#xf604;</mo>
                </mrow>
                <mo>&#x3e;</mo>
                <mrow>
                  <mo>&#xf603;</mo>
                  <mi>r</mi>
                  <mo>&#xf604;</mo>
                </mrow>
              </mrow>
              <mo>,</mo>
              <mrow>
                <mrow>
                  <mo>&#xf603;</mo>
                  <mi>s</mi>
                  <mo>&#xf604;</mo>
                </mrow>
                <mo>&#x2264;</mo>
                <mn>1</mn>
              </mrow>
              <mo>,</mo>
              <mrow>
                <mi>rs</mi>
                <mo>&#x3e;</mo>
                <mn>0</mn>
              </mrow>
              <mo>,</mo>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mrow>
              <mn>0</mn>
              <mo>,</mo>
            </mrow>
          </mtd>
          <mtd>
            <mrow>
              <mi>otherwise</mi>
              <mo>.</mo>
            </mrow>
          </mtd>
        </mtr>
      </mtable>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>7</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0094" num="0093">then an interpolation method different from bilinear interpolation is implemented but the invention is still applicable.</p>
<p id="p-0095" num="0094">(x) The invention described in this specification does not limit the coding method for error images to DCT (discrete cosine transform). For instance, wavelet transform (for example, N. Antonioni, et. al, &#x201c;Image Coding Using Wavelet Transform&#x201d; IEEE Trans. Image Processing, vol. 1, no. 2, April 1992) and Walsh-Hadamard transform (for example, A. N. Netravalli and B. G. Haskell, &#x201c;Digital Pictures&#x201d;, Plenum Press, 1998) are also applicable to this invention.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08625915-20140107-M00001.NB">
<img id="EMI-M00001" he="16.26mm" wi="76.20mm" file="US08625915-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US08625915-20140107-M00002.NB">
<img id="EMI-M00002" he="13.80mm" wi="76.20mm" file="US08625915-20140107-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00003" nb-file="US08625915-20140107-M00003.NB">
<img id="EMI-M00003" he="12.36mm" wi="76.20mm" file="US08625915-20140107-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00004" nb-file="US08625915-20140107-M00004.NB">
<img id="EMI-M00004" he="20.15mm" wi="76.20mm" file="US08625915-20140107-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A computer-readable medium having stored thereon an image decoding program which, when executed by a computer, performs operations comprising:
<claim-text>storing a reference image which is a previously decoded image;</claim-text>
<claim-text>receiving coded information including motion vector information and rounding method information specifying a rounding method used in synthesizing a prediction image of a currently decoded image; and</claim-text>
<claim-text>synthesizing the prediction image by performing motion compensation using the motion vector information and the reference image;</claim-text>
<claim-text>wherein the synthesizing a prediction image is performable using a positive rounding method and a negative rounding method for interpolating intensity values of pixels;</claim-text>
<claim-text>wherein the interpolation of intensity values of pixels is performed using a rounding method specified by the rounding method information;</claim-text>
<claim-text>wherein the interpolation is performed using the rounding method specified by rounding method information included in the encoded bitstream of the current frame when the current frame is a P frame;</claim-text>
<claim-text>wherein the rounding method information is not received from the encoded bitstream of the current frame when the current frame is an I frame;</claim-text>
<claim-text>and wherein the rounding method information is included in a header section of the coded information of the currently decoded image. </claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
