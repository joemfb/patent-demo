<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625913-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625913</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13352171</doc-number>
<date>20120117</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>JP</country>
<doc-number>2011-027845</doc-number>
<date>20110210</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>179</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>36</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>40</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>46</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>11</main-group>
<subgroup>20</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382233</main-classification>
<further-classification>382260</further-classification>
<further-classification>348441</further-classification>
</classification-national>
<invention-title id="d2e71">Image processing apparatus and image processing method</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>2009/0040374</doc-number>
<kind>A1</kind>
<name>Kobayashi</name>
<date>20090200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>348448</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>2009/0073192</doc-number>
<kind>A1</kind>
<name>Kobayashi</name>
<date>20090300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345643</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>2009/0310018</doc-number>
<kind>A1</kind>
<name>Sakashita et al.</name>
<date>20091200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348448</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>2010/0020230</doc-number>
<kind>A1</kind>
<name>Suzuki</name>
<date>20100100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348441</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>2010/0098349</doc-number>
<kind>A1</kind>
<name>Arashima et al.</name>
<date>20100400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382263</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2011/0170009</doc-number>
<kind>A1</kind>
<name>Uemura et al.</name>
<date>20110700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348598</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2011/0267424</doc-number>
<kind>A1</kind>
<name>Koike</name>
<date>20111100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348 43</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>JP</country>
<doc-number>2009-044460</doc-number>
<date>20090200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>7</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382232-233</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382244</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382260</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348441</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348448</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348598</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>6</number-of-drawing-sheets>
<number-of-figures>6</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120207399</doc-number>
<kind>A1</kind>
<date>20120816</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Mizuno</last-name>
<first-name>Ryosuke</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Mizuno</last-name>
<first-name>Ryosuke</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Fitzpatrick, Cella, Harper &#x26; Scinto</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Canon Kabushiki Kaisha</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Liu</last-name>
<first-name>Li</first-name>
<department>2665</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A low frequency component image L[i] and high frequency component-emphasized image SH[i] are generated from an image A[i]. Lossy compression processing is performed for the low frequency component image L[i] to generate a compressed image C[i] and store it in a memory. A compressed image C[i&#x2212;1] is decoded to generate a decoded image L&#x2032;[i&#x2212;1]. The compressed image C[i] is decoded to generate a decoded image L&#x2032;[i]. A difference image E[i] between the decoded image L&#x2032;[i] and the low frequency component image L[i] is generated. The low frequency component image L[i], decoded image L&#x2032;[i&#x2212;1], and difference image E[i] are composited at a predetermined ratio to generate a composite image SL[i]. The high frequency component-emphasized image SH[i] and composite image SL[i] are output as subframe images of the i-th frame.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="135.30mm" wi="229.36mm" file="US08625913-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="231.06mm" wi="157.65mm" orientation="landscape" file="US08625913-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="241.89mm" wi="102.19mm" orientation="landscape" file="US08625913-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="231.73mm" wi="147.49mm" orientation="landscape" file="US08625913-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="252.73mm" wi="153.92mm" orientation="landscape" file="US08625913-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="259.16mm" wi="182.71mm" file="US08625913-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="255.02mm" wi="149.78mm" orientation="landscape" file="US08625913-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0002" num="0001">1. Field of the Invention</p>
<p id="p-0003" num="0002">The present invention relates to a technique for image display.</p>
<p id="p-0004" num="0003">2. Description of the Related Art</p>
<p id="p-0005" num="0004">Recently, image display apparatuses with various display units such as a liquid crystal display panel have been put into practical use, including a TV receiver and PC monitor. However, when pursuit for a moving object (way of viewing in which a moving object is pursued by the line of sight on the movie display) is performed on a hold type display apparatus typified by a liquid crystal display apparatus, a motion blur corresponding to the optical output period is observed.</p>
<p id="p-0006" num="0005">As a technique for reducing such a motion blur, a &#x201c;spatial frequency division method&#x201d; has been proposed. The &#x201c;spatial frequency division method&#x201d; is a method of doubling the 60-Hz frame rate of an input image signal to 120 Hz, gathering spatial high frequency components of an image regarding a motion blur to one subframe to decrease those of the other subframe, and then displaying the image. In the output image, spatial high frequency components are localized in one subframe, suppressing the motion blur.</p>
<p id="p-0007" num="0006">In Japanese Patent Laid-Open No. 2009-044460, the frame rate of input image data is doubled. Then, the input image data is divided into spatial high frequency component-emphasized image data and low frequency component image data, and these image data are switched and displayed for respective subframes. At this time, low frequency component image data is formed from the average value of low frequency component image data respectively generated from immediately preceding and succeeding subframes which sandwich the subframe of the low frequency component image data. The use of the average value can reduce a temporal shift of the barycenter on the display of high frequency component-emphasized image data and low frequency component image data. In Japanese Patent Laid-Open No. 2009-044460, a motion blur can be suppressed, and a ghost and tailing blur visually perceived due to a temporal shift of the barycenter can be reduced further.</p>
<p id="p-0008" num="0007">In Japanese Patent Laid-Open No. 2009-044460, low frequency component image data to be displayed is formed from the average value of low frequency component image data that is respectively generated from immediately preceding and succeeding subframes that sandwich the subframe of the low frequency component image data, as described above. Obtaining the average value of low frequency component image data requires low frequency component image data generated from input image data of at least one immediately preceding subframe. A memory is therefore needed to store low frequency component image data.</p>
<p id="p-0009" num="0008">When low frequency component image data is lossily compressed to compress the memory band, the compression and decoding generate a compression error, degrading the image quality. In the &#x201c;spatial frequency division method&#x201d;, the compression error amount changes depending on the spatial frequency characteristic. For example, even input image data having the same tone value may be visually perceived to have different tone values within the region due to the difference of the frequency characteristic.</p>
<heading id="h-0002" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0010" num="0009">The present invention has been made to solve the above problems, and provides a technique for suppressing degradation of the image quality caused when a low frequency component image is lossily compressed.</p>
<p id="p-0011" num="0010">According to one aspect of the present invention, there is provided an image processing apparatus comprising: a unit that generates, from an input image of an i-th (i=1, 2, . . . ) frame, an image of a low frequency component in the image of the i-th frame as a low frequency component image i; a unit that generates, from the image of the i-th frame, a high frequency component-emphasized image i which emphasizes a high frequency component in the image of the i-th frame; a unit that performs lossy compression processing for the low frequency component image i to generate a compressed image i and store the generated compressed image i in a memory; a unit that decodes a compressed image (i&#x2212;1) stored in the memory to generate a decoded image (i&#x2212;1); a unit that decodes the compressed image i to generate a decoded image i; a unit that generates a difference image between the decoded image i and the low frequency component image i; a generation unit that composites the low frequency component image i, the decoded image (i&#x2212;1), and the difference image at a predetermined ratio to generate a composite image; and an output unit that outputs the high frequency component-emphasized image i and the composite image as subframe images of the i-th frame.</p>
<p id="p-0012" num="0011">According to another aspect of the present invention, there is provided an image processing apparatus comprising: a unit that generates, from an input image of an i-th (i=1, 2, . . . ) frame, an image of a low frequency component in the image of the i-th frame as a low frequency component image i; a unit that generates, from the image of the i-th frame, a high frequency component-emphasized image i which emphasizes a high frequency component in the image of the i-th frame; a unit that performs lossy compression processing for the low frequency component image i to generate a compressed image i and store the generated compressed image i in a memory; a unit that decodes a compressed image (i&#x2212;1) stored in the memory to generate a decoded image (i&#x2212;1); a unit that decodes the compressed image i to generate a decoded image i; a unit that generates a difference image between the decoded image i and the low frequency component image i; a unit that generates an image E<b>1</b> having a pixel value obtained by multiplying a pixel value of each pixel forming the difference image by r (0&#x3c;r&#x3c;1), and an image E<b>2</b> having a pixel value obtained by multiplying a pixel value of each pixel forming the difference image by (1&#x2212;r); a unit that composites the low frequency component image i, the decoded image (i&#x2212;1), and the image E<b>1</b> at a predetermined ratio to generate a composite image; and a unit that outputs an image obtained by compositing the high frequency component-emphasized image i and the image E<b>2</b>, and the composite image as subframe images of the i-th frame.</p>
<p id="p-0013" num="0012">According to still another aspect of the present invention, there is provided an image processing method to be performed by an image processing apparatus, comprising: a step of generating, from an input image of an i-th (i=1, 2, . . . ) frame, an image of a low frequency component in the image of the i-th frame as a low frequency component image i; a step of generating, from the image of the i-th frame, a high frequency component-emphasized image i which emphasizes a high frequency component in the image of the i-th frame; a step of performing lossy compression processing for the low frequency component image i to generate a compressed image i and store the generated compressed image i in a memory; a step of decoding a compressed image (i&#x2212;1) stored in the memory to generate a decoded image (i&#x2212;1); a step of decoding the compressed image i to generate a decoded image i; a step of generating a difference image between the decoded image i and the low frequency component image i; a generation step of compositing the low frequency component image i, the decoded image (i&#x2212;1), and the difference image at a predetermined ratio to generate a composite image; and an output step of outputting the high frequency component-emphasized image i and the composite image as subframe images of the i-th frame.</p>
<p id="p-0014" num="0013">According to yet still another aspect of the present invention, there is provided an image processing method to be performed by an image processing apparatus, comprising the steps of: generating, from an input image of an i-th (i=1, 2, . . . ) frame, an image of a low frequency component in the image of the i-th frame as a low frequency component image i; generating, from the image of the i-th frame, a high frequency component-emphasized image i which emphasizes a high frequency component in the image of the i-th frame; performing lossy compression processing for the low frequency component image i to generate a compressed image i and store the generated compressed image i in a memory; decoding a compressed image (i&#x2212;1) stored in the memory to generate a decoded image (i&#x2212;1); decoding the compressed image i to generate a decoded image i; generating a difference image between the decoded image i and the low frequency component image i; generating an image E<b>1</b> having a pixel value obtained by multiplying a pixel value of each pixel forming the difference image by r (0&#x3c;r&#x3c;1), and an image E<b>2</b> having a pixel value obtained by multiplying a pixel value of each pixel forming the difference image by (1&#x2212;r); compositing the low frequency component image i, the decoded image (i&#x2212;1), and the image E<b>1</b> at a predetermined ratio to generate a composite image; and outputting an image obtained by compositing the high frequency component-emphasized image i and the image E<b>2</b>, and the composite image as subframe images of the i-th frame.</p>
<p id="p-0015" num="0014">Further features of the present invention will become apparent from the following description of exemplary embodiments with reference to the attached drawings.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram exemplifying the functional arrangement of an image processing apparatus;</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 2</figref> is a table showing the time series relationship between an input frame image and an output subframe image;</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 3</figref> is a block diagram exemplifying the functional arrangement of an image processing apparatus;</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 4</figref> is a block diagram exemplifying the functional arrangement of an image processing apparatus;</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 5</figref> is a flowchart showing processing to be performed by the image processing apparatus; and</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 6</figref> is a block diagram exemplifying the functional arrangement of an image processing apparatus.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DESCRIPTION OF THE EMBODIMENTS</heading>
<p id="p-0022" num="0021">Embodiments of the present invention will now be described with reference to the accompanying drawings. Note that the following embodiments are merely examples of concretely practicing the present invention, and are detailed examples of arrangements defined in the scope of appended claims.</p>
<heading id="h-0005" level="1">First Embodiment</heading>
<p id="p-0023" num="0022">An image processing apparatus according to the first embodiment obtains an output frame rate double the input frame rate by decomposing the image of each input frame into two subframe images and outputting the two subframe images within the one-frame period. At this time, the &#x201c;spatial frequency division method&#x201d; suppresses a motion blur, and also reduces a ghost and tailing blur visually perceived due to a temporal shift of the barycenter on the display.</p>
<p id="p-0024" num="0023">First, an image processing apparatus according to the first embodiment will be explained with reference to the block diagram of <figref idref="DRAWINGS">FIG. 1</figref>. An image of the i-th (i=1, 2, . . . ) frame (i-th image input to the image processing apparatus) will be referred to as A[i].</p>
<p id="p-0025" num="0024">Upon receiving the image A[i], a filter unit <b>100</b> generates, from the image A[i], an image of a low frequency component in the image A[i] (in the image) as a low frequency component image i, and a high frequency component-emphasized image i which emphasizes a high frequency component in the image A[i]. The low frequency component image i generated from the image A[i] will be referred to as L[i], and the high frequency component-emphasized image i generated from the image A[i] will be referred to as SH[i].</p>
<p id="p-0026" num="0025">The low frequency component image is generated by, for example, performing low-pass filter processing for the image A[i]. The generation method is not particularly limited. The high frequency component-emphasized image is generated according to, for example, the following equation:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>SH[i]=</i>2&#xd7;<i>A[i]&#x2212;L[i]</i><?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0027" num="0026">The filter unit <b>100</b> sends the generated high frequency component-emphasized image SH[i] to a switching unit <b>107</b>, and the low frequency component image L[i] to a decoding prediction unit <b>104</b>, blending unit <b>106</b>, compression error amount calculation unit <b>105</b>, and compression encoding unit <b>101</b>.</p>
<p id="p-0028" num="0027">The compression encoding unit <b>101</b> performs lossless compression processing for the low frequency component image L[i], generating a compressed image i. The compressed image i will be referred to as C[i]. The compression encoding unit <b>101</b> stores the generated compressed image C[i] in a memory unit <b>102</b>.</p>
<p id="p-0029" num="0028">A decoding unit <b>103</b> reads out a compressed image C[i&#x2212;1] from the memory unit <b>102</b> and decodes it, generating a decoded image (i&#x2212;1). The decoded image i will be referred to as L&#x2032;[i]. The decoding unit <b>103</b> sends the generated decoded image L&#x2032;[i&#x2212;1] to the blending unit <b>106</b>.</p>
<p id="p-0030" num="0029">The decoding prediction unit <b>104</b> performs lossless compression processing for the low frequency component image L[i], generating the compressed image C[i]. Further, the decoding prediction unit <b>104</b> decodes the generated compressed image C[i], generating the decoded image L&#x2032;[i]. The lossless compression processing and decoding processing are almost the same as those performed by the compression encoding unit <b>101</b> and decoding unit <b>103</b>, respectively. The decoding prediction unit <b>104</b> sends the generated decoded image L&#x2032;[i] to the compression error amount calculation unit <b>105</b>.</p>
<p id="p-0031" num="0030">The compression error amount calculation unit <b>105</b> generates a difference image (compression error amount) E[i] between the low frequency component image L[i] sent from the filter unit <b>100</b> and the decoded image L&#x2032;[i] sent from the decoding prediction unit <b>104</b> in accordance with the following equation:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>E[i]=L[i]&#x2212;L&#x2032;[i]</i><?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0032" num="0031">When no compression is executed, L&#x2032;[i]=L[i] and E[i]=0. The compression error amount calculation unit <b>105</b> sends the generated difference image E[i] to the blending unit <b>106</b>.</p>
<p id="p-0033" num="0032">The blending unit <b>106</b> composites the low frequency component image L[i], decoded image L&#x2032;[i&#x2212;1], and difference image E[i] at a predetermined ratio, generating a composite image SL[i]. For example, the composite image SL[i] is generated according to the following equation using &#x3b1; (0&#x2266;&#x3b1;&#x2266;1) and &#x3b2; (0&#x2266;&#x3b2;&#x2266;1):
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>SL[i]=L&#x2032;[i&#x2212;</i>1]&#xd7;&#x3b1;+<i>L[i]&#xd7;</i>(1&#x2212;&#x3b1;)+<i>E[i]&#xd7;&#x3b2;</i><?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0034" num="0033">That is, the blending unit <b>106</b> composites an image obtained by multiplying the pixel value of each pixel forming the decoded image L&#x2032;[i&#x2212;1] by &#x3b1;, an image obtained by multiplying the pixel value of each pixel forming the low frequency component image L[i] by (1&#x2212;&#x3b1;), and an image obtained by multiplying the pixel value of each pixel forming the difference image E[i] by &#x3b2;. The blending unit <b>106</b> sends the generated composite image SL[i] to the switching unit <b>107</b>.</p>
<p id="p-0035" num="0034">The switching unit <b>107</b> alternately switches and outputs, as subframe images of the i-th frame, the high frequency component-emphasized image SH[i] sent from the filter unit <b>100</b> and the composite image SL[i] sent from the blending unit <b>106</b>. For example, the switching unit <b>107</b> outputs the composite image SL[i] first and then the high frequency component-emphasized image SH[i]. As subframe images of the i-th frame, the composite image SL[i] and high frequency component-emphasized image SH[i] can be output in the order named. The output destination is a display apparatus such as a CRT or liquid crystal screen, but is not particularly limited.</p>
<p id="p-0036" num="0035">Hence, the &#x201c;spatial frequency division method&#x201d; can suppress a motion blur, and reduce a ghost and tailing blur visually perceived due to a temporal shift of the barycenter on the display.</p>
<p id="p-0037" num="0036">Next, the time series relationship between an input frame image and an output subframe image will be explained with reference to <figref idref="DRAWINGS">FIG. 2</figref>. The &#x201c;subframe number&#x201d; on the first line is a number assigned to each output subframe image, and the number is 1, 2, . . . in the output order.</p>
<p id="p-0038" num="0037">The second line from the top indicates an input frame image. The third line from the top indicates an output subframe image. The fourth line from the top indicates details of the output subframe image.</p>
<p id="p-0039" num="0038">For example, when the image A[i&#x2212;1] of the (i&#x2212;1)th frame is input, a subframe image SL[i&#x2212;1] having the subframe number=1 is output, as represented by the third line from the top. As represented by the fourth line from the top, SL[i&#x2212;1] is &#x201c;L&#x2032;[i&#x2212;2]&#xd7;&#x3b1;+L[i&#x2212;1]&#xd7;(1&#x2212;&#x3b1;)+E[i&#x2212;1]&#xd7;&#x3b2;&#x201d;. After outputting SL[i&#x2212;1], a subframe image SH[i&#x2212;1] having the subframe number=2 is output, as represented by the third line from the top. As represented by the fourth line from the top, SH[i&#x2212;1] is &#x201c;2&#xd7;A[i&#x2212;1]&#x2212;L[i&#x2212;1]&#x201d;. In this manner, subframe images are output sequentially.</p>
<p id="p-0040" num="0039">L[i] generated from A[i] is ideally distributed to output images in accordance with the coefficient &#x3b1; for the subframe number=3 and 5 in the table shown in <figref idref="DRAWINGS">FIG. 2</figref>. However, when the memory band is compressed, a compression error is generated for the subframe number=5. The compression error amount can be expressed as E[i]&#xd7;&#x3b2;. If the coefficient &#x3b2; has the same value as that of the coefficient &#x3b1;, E[i]&#xd7;&#x3b2; has been composited in advance for the subframe number=3.</p>
<p id="p-0041" num="0040">That is, in the embodiment, the compression error amount is composited to an image of the second preceding subframe and output. An output tone value integrated in the unit time can be adjusted to be almost equal to an ideal value obtained when no memory band is compressed. Especially in a still image, compression error amounts generated in respective frames are equal. Thus, the compression error amount E[i]&#xd7;&#x3b2; for the subframe number=3 and the compression error amount E[i+1]&#xd7;&#x3b2; composited for the subframe number=5 become equal. That is, the contents of the output image become equal to an ideal output of L[i]&#xd7;&#x3b1;+L[i+1]&#xd7;(1&#x2212;&#x3b1;).</p>
<p id="p-0042" num="0041">The coefficient &#x3b2; has the same value as that of the coefficient &#x3b1;. The coefficient &#x3b2; determines the degree at which the compression error amount is composited to an image of the second preceding subframe. The coefficient &#x3b2; is desirably set to satisfy &#x3b2;&#x2266;&#x3b1;.</p>
<p id="p-0043" num="0042">According to the first embodiment, the compression error amount upon compression of the memory band is calculated in advance and composited to an image. An output tone value integrated in the unit time can be adjusted to be almost equal to an ideal value obtained when no memory band is compressed. Particularly when the input image is a still image, the output tone value can be set equal to the ideal value. This can suppress degradation of the image quality caused when a low frequency component image is lossily compressed. At this time, no compression error amount need be held for frame delay, so a memory for storing the compression error amount is not required.</p>
<p id="p-0044" num="0043">Note that lossy compression processing in the compression encoding unit <b>101</b> is arbitrary. For example, a compression encoding method based on a decoding result prediction value, like DPCM (Differential Pulse Code Modulation), does not require the decoding prediction unit <b>104</b>, and a prediction value used in compression is employed as L&#x2032;[i].</p>
<heading id="h-0006" level="1">Second Embodiment</heading>
<p id="p-0045" num="0044">In the first embodiment, the compression error amount E[i] is composited to an image of the second preceding subframe consequently. Particularly when the input image is a still image, the output tone value can be made equal to the ideal value. However, the input image is displayed upon compositing the compression error amount to the second subframe preceding a subframe which actually generates an error. When the input image is a movie, a ghost or stain may be visually perceived near the moving object owing to a temporal shift of the display.</p>
<p id="p-0046" num="0045">In the second embodiment, the compression error amount E[i] is distributed and composited to an image of an immediately preceding subframe and an image of the second preceding subframe. Hence, the compression error amount can be composited to a display image temporally closest to a subframe which actually generates an error.</p>
<p id="p-0047" num="0046">The functional arrangement of an image processing apparatus according to the second embodiment will be exemplified with reference to the block diagram of <figref idref="DRAWINGS">FIG. 3</figref>. In <figref idref="DRAWINGS">FIG. 3</figref>, the same reference numerals as those in <figref idref="DRAWINGS">FIG. 1</figref> denote the same building elements, and a description thereof will not be repeated.</p>
<p id="p-0048" num="0047">A distribution processing unit <b>300</b> divides E[i] calculated by a compression error amount calculation unit <b>105</b> into E<b>1</b>[<i>i</i>] and E<b>2</b>[<i>i</i>] (E[i]=E<b>1</b>[<i>i</i>]+E<b>2</b>[<i>i</i>]). For example, the distribution processing unit <b>300</b> generates an image E<b>1</b>[<i>i</i>] having a pixel value obtained by multiplying the pixel value of each pixel forming the image E[i] by r (0&#x3c;r&#x3c;1), and an image E<b>2</b>[<i>i</i>] having a pixel value obtained by multiplying the pixel value of each pixel forming the image E[i] by (1&#x2212;r). The distribution processing unit <b>300</b> sends E<b>1</b>[<i>i</i>] to a blending unit <b>106</b> and E<b>2</b>[<i>i</i>] to an adder <b>301</b>.</p>
<p id="p-0049" num="0048">The adder <b>301</b> generates, as SH&#x2032;[i], an image obtained by compositing E<b>2</b>[<i>i</i>] to SH[i] from a filter unit <b>100</b>. The adder <b>301</b> sends the generated image SH&#x2032;[i] to the blending unit <b>106</b>.</p>
<p id="p-0050" num="0049">The blending unit <b>106</b> composites the low frequency component image L[i], decoded image L&#x2032;[i&#x2212;1], and difference image E<b>1</b>[<i>i</i>] at a predetermined ratio, generating a composite image SL[i]. For example, the composite image SL[i] is generated according to the following equation using &#x3b1; (0&#x2266;&#x3b1;&#x2266;1) and &#x3b2; (0&#x2266;&#x3b2;&#x2266;1):
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>SL[i]=L&#x2032;[i&#x2212;</i>1]&#xd7;&#x3b1;+<i>L[i]&#xd7;</i>(1&#x2212;&#x3b1;)+<i>E</i>1[<i>i]&#xd7;&#x3b2;</i><?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0051" num="0050">The blending unit <b>106</b> sends the generated composite image SL[i] to a switching unit <b>107</b>. The switching unit <b>107</b> alternately switches and outputs, as subframe images of the i-th frame, the image SH&#x2032;[i] sent from the adder <b>301</b> and the composite image SL[i] sent from the blending unit <b>106</b>. For example, the switching unit <b>107</b> outputs the composite image SL[i] first and then the image SH&#x2032;[i]. The output destination is a display apparatus such as a CRT or liquid crystal screen, but is not particularly limited.</p>
<p id="p-0052" num="0051">According to the second embodiment, the compression error amount E[i] is distributed and composited to an image of an immediately preceding subframe and an image of the second preceding subframe. The compression error amount can be composited to a display image temporally closest to a subframe which actually generates a compression error. This can reduce a ghost or stain near a moving object that is visually perceived owing to a temporal shift of the display of the compression error amount.</p>
<heading id="h-0007" level="1">Third Embodiment</heading>
<p id="p-0053" num="0052">In the first and second embodiments, the blending unit <b>106</b> and adder <b>301</b> execute image composition. However, the composite image may deviate from the displayable data value allowable range depending on the magnitude and coefficient of the compression error amount. The third embodiment can suppress degradation of the image quality even when the composite image deviates from the displayable data value allowable range.</p>
<p id="p-0054" num="0053">The arrangement of an image processing apparatus according to the third embodiment will be described with reference to the block diagram of <figref idref="DRAWINGS">FIG. 4</figref>. In <figref idref="DRAWINGS">FIG. 4</figref>, the same reference numerals as those in <figref idref="DRAWINGS">FIG. 1</figref> denote the same building elements, and a description thereof will not be repeated.</p>
<p id="p-0055" num="0054">A composite image SL[i] may deviate from the displayable data value allowable range depending on the compression error amount E[i] to be composited by a blending unit <b>106</b> or the coefficient &#x3b2;. When the composite image SL[i] deviates from the data value allowable range, a saturation processing unit <b>400</b> corrects the composite image SL[i] to fall within the allowable range, and outputs the corrected composite image SL[i] to a switching unit <b>107</b>. Also, the saturation processing unit <b>400</b> outputs, to a saturation composition unit <b>401</b>, a correction amount S[i] which has been added to or subtracted from the input in the saturation processing. The saturation composition unit <b>401</b> composites part or all of the correction amount S[i] to the high frequency component-emphasized image SH[i] based on a predetermined coefficient, and outputs the resultant high frequency component-emphasized image SH[i] to the switching unit <b>107</b>.</p>
<p id="p-0056" num="0055">Processing to be performed by the image processing apparatus according to the third embodiment will be explained with reference to <figref idref="DRAWINGS">FIG. 5</figref> which is a flowchart showing this processing. In step S<b>1</b>, a filter unit <b>100</b> acquires an image A[i]. In step S<b>2</b>, the filter unit <b>100</b> generates a low frequency component image L[i] and high frequency component-emphasized image SH[i] from the image A[i].</p>
<p id="p-0057" num="0056">In step S<b>3</b>, a decoding prediction unit <b>104</b> performs lossless compression processing for the low frequency component image L[i], generating a compressed image C[i]. Further, the decoding prediction unit <b>104</b> decodes the generated compressed image C[i], generating a decoded image L&#x2032;[i].</p>
<p id="p-0058" num="0057">In step S<b>4</b>, a compression error amount calculation unit <b>105</b> calculates E[i]=L[i]&#x2212;L&#x2032;[i], generating a difference image (compression error amount) E[i] between the low frequency component image L[i] sent from the filter unit <b>100</b> and the decoded image L&#x2032;[i] sent from the decoding prediction unit <b>104</b>.</p>
<p id="p-0059" num="0058">In step S<b>5</b>, a compression encoding unit <b>101</b> performs lossless compression processing for the low frequency component image L[i], generating a compressed image C[i]. In step S<b>6</b>, the compression encoding unit <b>101</b> stores the generated compressed image C[i] in a memory unit <b>102</b>.</p>
<p id="p-0060" num="0059">In step S<b>7</b>, a decoding unit <b>103</b> reads out a compressed image C[i&#x2212;1] from the memory unit <b>102</b>. In step S<b>8</b>, the decoding unit <b>103</b> decodes the compressed image C[i&#x2212;1], generating a decoded image L&#x2032;[i&#x2212;1].</p>
<p id="p-0061" num="0060">In step S<b>9</b>, the blending unit <b>106</b> composites the low frequency component image L[i], decoded image L&#x2032;[i&#x2212;1], and difference image E[i] at a predetermined ratio in the same way as in the first embodiment, generating a composite image SL[i].</p>
<p id="p-0062" num="0061">In step S<b>10</b>, the saturation processing unit <b>400</b> determines whether the pixel value of each pixel forming the composite image SL[i] falls within the allowable range. For example, the saturation processing unit <b>400</b> determines whether the pixel value falls within the range of pixel values displayable by a display apparatus. If the saturation processing unit <b>400</b> determines that the pixel value falls within the allowable range, the process advances to step S<b>13</b>; if NO, to step S<b>11</b>.</p>
<p id="p-0063" num="0062">In step S<b>11</b>, the saturation processing unit <b>400</b> corrects the composite image SL[i] so that the pixel value of each pixel forming the composite image SL[i] falls within the allowable range. For example, the saturation processing unit <b>400</b> updates the composite image SL[i] by subtracting a predetermined correction amount S[i] from the pixel value of each pixel forming the composite image SL[i].</p>
<p id="p-0064" num="0063">In step S<b>12</b>, the saturation composition unit <b>401</b> updates the high frequency component-emphasized image SH[i] by adding the predetermined correction amount S[i] to the pixel value of each pixel forming the high frequency component-emphasized image SH[i]. Instead of the predetermined correction amount S[i], d (0&#x3c;d&#x3c;1)&#xd7;S[i] may be added.</p>
<p id="p-0065" num="0064">In step S<b>13</b>, the switching unit <b>107</b> alternately switches and outputs, as subframe images of the i-th frame, the high frequency component-emphasized image SH[i] and composite image SL[i].</p>
<p id="p-0066" num="0065">The third embodiment is also applicable to the second embodiment. More specifically, when the high frequency component-emphasized image SH&#x2032;[i] obtained by the adder <b>301</b> deviates from the displayable data value allowable range, the above-described saturation processing is performed even for an output from the adder <b>301</b> in the same way. The third embodiment can obtain the same effects as those of the above-described embodiments, and can suppress degradation of the image quality caused by saturation of display data.</p>
<heading id="h-0008" level="1">Fourth Embodiment</heading>
<p id="p-0067" num="0066">The functional arrangement of an image processing apparatus according to the fourth embodiment will be described with reference to the block diagram of <figref idref="DRAWINGS">FIG. 6</figref>. In <figref idref="DRAWINGS">FIG. 6</figref>, the same reference numerals as those in <figref idref="DRAWINGS">FIG. 1</figref> denote the same building elements, and a description thereof will not be repeated.</p>
<p id="p-0068" num="0067">Upon receiving an image A[i], a low-pass filter unit <b>600</b> performs low-pass filter processing for the image A[i] by, for example, cutting off (filtering) a lower limit spatial frequency given by a predetermined constant from the image A[i], thereby generating a low frequency component image L[i].</p>
<p id="p-0069" num="0068">A blending unit <b>106</b> composites the low frequency component image L[i] and an image L&#x2032;[i&#x2212;1] decoded by a decoding unit <b>103</b> at a predetermined ratio, generating a composite image SL[i]. For example, the blending unit <b>106</b> calculates SL[i]=L[i]&#xd7;(1&#x2212;&#x3b1;)+L&#x2032;[i&#x2212;1]&#xd7;&#x3b1;.</p>
<p id="p-0070" num="0069">A calculation unit <b>601</b> calculates SH[i]=2&#xd7;A[i]&#x2212;SL[i], generating a high frequency component-emphasized image SH[i]. Needless to say, processing for obtaining the high frequency component-emphasized image SH[i] is not limited to this.</p>
<p id="p-0071" num="0070">A switching unit <b>107</b> alternately switches and outputs, as subframe images of the i-th frame, the high frequency component-emphasized image SH[i] and composite image SL[i]. For example, the switching unit <b>107</b> outputs the composite image SL[i] first and then the high frequency component-emphasized image SH[i]. As subframe images of the i-th frame, the composite image SL[i] and high frequency component-emphasized image SH[i] can be output in the order named. The output destination is a display apparatus such as a CRT or liquid crystal screen, but is not particularly limited.</p>
<p id="p-0072" num="0071">According to the fourth embodiment, the high frequency component-emphasized image SH[i] is generated based on the composite image SL[i] containing the compression error amount. Although the fourth embodiment is different from the above-described embodiments in the method of generating the high frequency component-emphasized image SH[i], a compression error amount-composited output result can be obtained. That is, an output tone value integrated in the unit time can be adjusted to be almost equal to an ideal value, suppressing degradation of the image quality caused when the low frequency component image L[i] is lossily compressed.</p>
<p id="p-0073" num="0072">In the above description, the input image is an image input upon dividing one frame into two subframes to double the frame rate. However, the input image is arbitrary. As described above, the present invention can suppress degradation of the image quality caused when low frequency component image data is lossily compressed.</p>
<heading id="h-0009" level="1">Fifth Embodiment</heading>
<p id="p-0074" num="0073">Although the respective units shown in <figref idref="DRAWINGS">FIGS. 1</figref>, <b>3</b>, <b>4</b>, and <b>6</b> may be formed from hardware, the units except for the memory unit <b>102</b> may be formed from software (computer program). In this case, the software is installed in a PC (Personal Computer) including the memory unit <b>102</b>, and a control unit such as a CPU in the PC executes the software. The PC implements the functions of the respective units shown in <figref idref="DRAWINGS">FIGS. 1</figref>, <b>3</b>, <b>4</b>, and <b>6</b>.</p>
<heading id="h-0010" level="1">Other Embodiments</heading>
<p id="p-0075" num="0074">Aspects of the present invention can also be realized by a computer of a system or apparatus (or devices such as a CPU or MPU) that reads out and executes a program recorded on a memory device to perform the functions of the above-described embodiment(s), and by a method, the steps of which are performed by a computer of a system or apparatus by, for example, reading out and executing a program recorded on a memory device to perform the functions of the above-described embodiment(s). For this purpose, the program is provided to the computer for example via a network or from a recording medium of various types serving as the memory device (for example, computer-readable medium).</p>
<p id="p-0076" num="0075">While the present invention has been described with reference to exemplary embodiments, it is to be understood that the invention is not limited to the disclosed exemplary embodiments. The scope of the following claims is to be accorded the broadest interpretation so as to encompass all such modifications and equivalent structures and functions.</p>
<p id="p-0077" num="0076">This application claims the benefit of Japanese Patent Application No. 2011-027845 filed Feb. 10, 2011, which is hereby incorporated by reference herein in its entirety.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. An image processing apparatus comprising:
<claim-text>a unit that generates, from an input image of an i-th (i=1, 2, . . . ) frame, an image of a low frequency component in the image of the i-th frame as a low frequency component image i;</claim-text>
<claim-text>a unit that generates, from the image of the i-th frame, a high frequency component-emphasized image i which emphasizes a high frequency component in the image of the i-th frame;</claim-text>
<claim-text>a unit that performs lossy compression processing for the low frequency component image i to generate a compressed image i and store the generated compressed image i in a memory;</claim-text>
<claim-text>a unit that decodes a compressed image (i&#x2212;1) stored in the memory to generate a decoded image (i&#x2212;1);</claim-text>
<claim-text>a unit that decodes the compressed image i to generate a decoded image i;</claim-text>
<claim-text>a unit that generates a difference image between the decoded image i and the low frequency component image i;</claim-text>
<claim-text>a generation unit that composites the low frequency component image i, the decoded image (i&#x2212;1), and the difference image at a predetermined ratio to generate a composite image; and</claim-text>
<claim-text>an output unit that outputs the high frequency component-emphasized image i and the composite image as subframe images of the i-th frame.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising an updating unit that, when a pixel value of each pixel forming the composite image falls outside a predetermined allowable range, corrects the pixel value so as to fall within the predetermined allowable range, and adds an amount of the correction to a pixel value of each pixel forming the high frequency component-emphasized image i,
<claim-text>wherein said output unit outputs, as subframe images of the i-th frame, the high frequency component-emphasized image i and the composite image each of which is updated by said updating unit.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said generation unit generates the composite image by compositing an image obtained by multiplying a pixel value of each pixel forming the decoded image (i&#x2212;1) by &#x3b1; (0&#x2266;&#x3b1;&#x2266;1), an image obtained by multiplying a pixel value of each pixel forming the low frequency component image i by (1&#x2212;&#x3b1;), and an image obtained by multiplying a pixel value of each pixel forming the difference image by &#x3b2; (0&#x2266;&#x3b2;&#x2266;1).</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. An image processing apparatus comprising:
<claim-text>a unit that generates, from an input image of an i-th (i=1, 2, . . . ) frame, an image of a low frequency component in the image of the i-th frame as a low frequency component image i;</claim-text>
<claim-text>a unit that generates, from the image of the i-th frame, a high frequency component-emphasized image i which emphasizes a high frequency component in the image of the i-th frame;</claim-text>
<claim-text>a unit that performs lossy compression processing for the low frequency component image i to generate a compressed image i and store the generated compressed image i in a memory;</claim-text>
<claim-text>a unit that decodes a compressed image (i&#x2212;1) stored in the memory to generate a decoded image (i&#x2212;1);</claim-text>
<claim-text>a unit that decodes the compressed image i to generate a decoded image i;</claim-text>
<claim-text>a unit that generates a difference image between the decoded image i and the low frequency component image i;</claim-text>
<claim-text>a unit that generates an image E<b>1</b> having a pixel value obtained by multiplying a pixel value of each pixel forming the difference image by r (0&#x3c;r&#x3c;1), and an image E<b>2</b> having a pixel value obtained by multiplying a pixel value of each pixel forming the difference image by (1&#x2212;r);</claim-text>
<claim-text>a unit that composites the low frequency component image i, the decoded image (i&#x2212;1), and the image E<b>1</b> at a predetermined ratio to generate a composite image; and</claim-text>
<claim-text>a unit that outputs an image obtained by compositing the high frequency component-emphasized image i and the image E<b>2</b>, and the composite image as subframe images of the i-th frame.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. An image processing method to be performed by an image processing apparatus, comprising:
<claim-text>a step of generating, from an input image of an i-th (i=1, 2, . . . ) frame, an image of a low frequency component in the image of the i-th frame as a low frequency component image i;</claim-text>
<claim-text>a step of generating, from the image of the i-th frame, a high frequency component-emphasized image i which emphasizes a high frequency component in the image of the i-th frame;</claim-text>
<claim-text>a step of performing lossy compression processing for the low frequency component image i to generate a compressed image i and store the generated compressed image i in a memory;</claim-text>
<claim-text>a step of decoding a compressed image (i&#x2212;1) stored in the memory to generate a decoded image (i&#x2212;1);</claim-text>
<claim-text>a step of decoding the compressed image i to generate a decoded image i;</claim-text>
<claim-text>a step of generating a difference image between the decoded image i and the low frequency component image i;</claim-text>
<claim-text>a generation step of compositing the low frequency component image i, the decoded image (i&#x2212;1), and the difference image at a predetermined ratio to generate a composite image; and</claim-text>
<claim-text>an output step of outputting the high frequency component-emphasized image i and the composite image as subframe images of the i-th frame.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. An image processing method to be performed by an image processing apparatus, comprising the steps of:
<claim-text>generating, from an input image of an i-th (i=1, 2, . . . ) frame, an image of a low frequency component in the image of the i-th frame as a low frequency component image i;</claim-text>
<claim-text>generating, from the image of the i-th frame, a high frequency component-emphasized image i which emphasizes a high frequency component in the image of the i-th frame;</claim-text>
<claim-text>performing lossy compression processing for the low frequency component image i to generate a compressed image i and store the generated compressed image i in a memory;</claim-text>
<claim-text>decoding a compressed image (i&#x2212;1) stored in the memory to generate a decoded image (i&#x2212;1);</claim-text>
<claim-text>decoding the compressed image i to generate a decoded image i;</claim-text>
<claim-text>generating a difference image between the decoded image i and the low frequency component image i;</claim-text>
<claim-text>generating an image E<b>1</b> having a pixel value obtained by multiplying a pixel value of each pixel forming the difference image by r (0&#x3c;r&#x3c;1), and an image E<b>2</b> having a pixel value obtained by multiplying a pixel value of each pixel forming the difference image by (1&#x2212;r);</claim-text>
<claim-text>compositing the low frequency component image i, the decoded image (i&#x2212;1), and the image E<b>1</b> at a predetermined ratio to generate a composite image; and</claim-text>
<claim-text>outputting an image obtained by compositing the high frequency component-emphasized image i and the image E<b>2</b>, and the composite image as subframe images of the i-th frame.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. A non-transitory computer-readable storage medium storing a computer program for causing a computer to function as each unit of an image processing apparatus defined in <claim-ref idref="CLM-00001">claim 1</claim-ref>. </claim-text>
</claim>
</claims>
</us-patent-grant>
