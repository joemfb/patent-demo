<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625819-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625819</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12169386</doc-number>
<date>20080708</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<us-term-of-grant>
<us-term-extension>1151</us-term-extension>
<disclaimer>
<text>This patent is subject to a terminal disclaimer.</text>
</disclaimer>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>R</subclass>
<main-group>3</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>381110</main-classification>
</classification-national>
<invention-title id="d2e55">Method and device for voice operated control</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>6618073</doc-number>
<kind>B1</kind>
<name>Lambert et al.</name>
<date>20030900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348 1408</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>2005/0058313</doc-number>
<kind>A1</kind>
<name>Victorian et al.</name>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>381315</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>2007/0033029</doc-number>
<kind>A1</kind>
<name>Sakawaki</name>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704233</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>2007/0086600</doc-number>
<kind>A1</kind>
<name>Boesen</name>
<date>20070400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>381 79</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>2007/0237341</doc-number>
<kind>A1</kind>
<name>Laroche</name>
<date>20071000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00006">
<othercit>Office Action for U.S. Appl. No. 12/102,555, filed Apr. 14, 2008, mailed Mar. 14, 2012.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00007">
<othercit>Office Action for U.S. Appl. No. 12/102,555, filed Apr. 14, 2008, mailed Dec. 5, 2012.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00008">
<othercit>Office Action for U.S. Appl. No. 12/102,555, filed Apr. 14, 2008, mailed Sep. 27, 2011.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>17</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>381110</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>381 92</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>381107</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>381 59</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>381 57</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>381 56</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>381309</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>381315</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>381380</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>381122</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>381328</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>381322</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>381318</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>381375</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>700 94</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704260</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704233</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704231</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704226</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704228</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>379419</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>37942801</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>379430</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>37943301</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>37943302</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>8</number-of-drawing-sheets>
<number-of-figures>9</number-of-figures>
</figures>
<us-related-documents>
<continuation-in-part>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>12102555</doc-number>
<date>20080414</date>
</document-id>
<parent-status>PENDING</parent-status>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>12169386</doc-number>
</document-id>
</child-doc>
</relation>
</continuation-in-part>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60911691</doc-number>
<date>20070413</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20090010456</doc-number>
<kind>A1</kind>
<date>20090108</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Goldstein</last-name>
<first-name>Steven Wayne</first-name>
<address>
<city>Delray Beach</city>
<state>FL</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Usher</last-name>
<first-name>John</first-name>
<address>
<city>Montreal</city>
<country>CA</country>
</address>
</addressbook>
<residence>
<country>CA</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Boillot</last-name>
<first-name>Marc Andre</first-name>
<address>
<city>Plantation</city>
<state>FL</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Goldstein</last-name>
<first-name>Steven Wayne</first-name>
<address>
<city>Delray Beach</city>
<state>FL</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Usher</last-name>
<first-name>John</first-name>
<address>
<city>Montreal</city>
<country>CA</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Boillot</last-name>
<first-name>Marc Andre</first-name>
<address>
<city>Plantation</city>
<state>FL</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Personics Holdings, Inc</orgname>
<role>02</role>
<address>
<city>Boca Raton</city>
<state>FL</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Lao</last-name>
<first-name>Lun-See</first-name>
<department>2655</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Methods and devices for voice operated control are provided. The method can include measuring an ambient sound received from at least one Ambient Sound Microphone, measuring an internal sound received from at least one Ear Canal Microphone, detecting a spoken voice from a wearer of the earpiece based on an analysis of the ambient sound and the internal sound, and controlling at least one voice operation of the earpiece if the presence of spoken voice is detected. The analysis can be a non-difference comparison such as a correlation analysis, a cross-correlation analysis, and a coherence analysis.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="156.89mm" wi="149.01mm" file="US08625819-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="213.53mm" wi="176.11mm" file="US08625819-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="194.90mm" wi="169.25mm" file="US08625819-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="167.64mm" wi="149.01mm" file="US08625819-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="186.27mm" wi="173.65mm" file="US08625819-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="141.56mm" wi="139.36mm" file="US08625819-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="190.58mm" wi="161.97mm" file="US08625819-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="152.32mm" wi="155.36mm" file="US08625819-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="194.06mm" wi="160.61mm" file="US08625819-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This Application is a Continuation in Part application of application Ser. No. 12/102,555, filed 14 Apr. 2008, which claims the priority benefit of Provisional Application No. 60/911,691 filed on Apr. 13, 2007, the entire disclosures of which are incorporated herein by reference.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">FIELD</heading>
<p id="p-0003" num="0002">The present invention pertains to sound processing using earpieces, and more particularly, to a device and method for controlling operation of an earpiece based on voice activity.</p>
<heading id="h-0003" level="1">BACKGROUND</heading>
<p id="p-0004" num="0003">It can be difficult to communicate using an earpiece or earphone device in the presence of high-level background sounds. The earpiece microphone can pick up environmental sounds such as traffic, construction, and nearby conversations that can degrade the quality of the communication experience. In the presence of babble noise, where numerous talkers are simultaneously speaking, the earpiece does not adequately discriminate between voices in the background and the voice of the user operating the earpiece.</p>
<p id="p-0005" num="0004">Although audio processing technologies can adequately suppress noise, the earpiece is generally sound agnostic and cannot differentiate sounds. Thus, a user desiring to speak into the earpiece may be competing with other people's voices in his or her proximity that are also captured by the microphone of the earpiece.</p>
<p id="p-0006" num="0005">A need therefore exists for a method and device of personalized voice operated control.</p>
<heading id="h-0004" level="1">SUMMARY</heading>
<p id="p-0007" num="0006">Embodiments in accordance with the present invention provide a method and device for voice operated control.</p>
<p id="p-0008" num="0007">In a first embodiment, an earpiece can include an Ambient Sound Microphone (ASM) configured to capture ambient sound, an Ear Canal Microphone (ECM) configured to capture internal sound in an ear canal, and a processor operatively coupled to the ASM and the ECM. The processor can detect a spoken voice generated by a wearer of the earpiece based on an analysis of the ambient sound measured at the ASM and the internal sound measured at the ECM.</p>
<p id="p-0009" num="0008">A voice operated control (VOX) operatively coupled to the processor can control a mixing of the ambient sound and the internal sound for producing a mixed signal. The VOX can control at least one among a voice monitoring system, a voice dictation system, and a voice recognition system. The VOX can manage a delivery of the mixed signal based on one or more aspects of the spoken voice, such as a volume level, a voicing level, and a spectral shape of the spoken voice. The VOX can further control a second mixing of the audio content and the mixed signal delivered to the ECR. A transceiver operatively coupled to the processor can transmit the mixed signal to at least one among a cell phone, a media player, a portable computing device, and a personal digital assistant.</p>
<p id="p-0010" num="0009">In a second embodiment, an earpiece can include an Ambient Sound Microphone (ASM) configured to capture ambient sound, an Ear Canal Microphone (ECM) configured to capture internal sound in an ear canal, an Ear Canal Receiver (ECR) operatively coupled to the processor and configured to deliver audio content to the ear canal, and a processor operatively coupled to the ASM, the ECM and the ECR. The processor can detect a spoken voice generated by a wearer of the earpiece based on an analysis of the ambient sound measured at the ASM and the internal sound measured at the ECM.</p>
<p id="p-0011" num="0010">A voice operated control (VOX) operatively coupled to the processor can mix the ambient sound and the internal sound to produce a mixed signal. The VOX can control the mix based on one or more aspects of the audio content and the spoken voice, such as a volume level, a voicing level, and a spectral shape of the spoken voice. The one or more aspects of the audio content can include at least one among a spectral distribution, a duration, and a volume of the audio content. The audio content can be provided via a phone call, a voice message, a music signal, an alarm or an auditory warning. The VOX can include a level detector for comparing a sound pressure level (SPL) of the ambient sound and the internal sound, a correlation unit for assessing a correlation of the ambient sound and the internal sound for detecting the spoken voice, a coherence unit for determining whether the spoken voice originates from the wearer, or a spectral analysis unit for detecting whether spectral portions of the spoken voice are similar in the ambient sound and the internal sound.</p>
<p id="p-0012" num="0011">In a third embodiment, a dual earpiece can include a first earpiece and a second earpiece. The first earpiece can include a first Ambient Sound Microphone (ASM) configured to capture a first ambient sound, and a first Ear Canal Microphone (ECM) configured to capture a first internal sound in an ear canal. The second earpiece can include a second Ambient Sound Microphone (ASM) configured to capture a second ambient sound, a second Ear Canal Microphone (ECM) configured to capture a second internal sound in an ear canal, and a processor operatively coupled to the first earpiece and the second earpiece. The processor can detect a spoken voice generated by a wearer of the earpiece based on an analysis of at least one of the first and second ambient sound and at least one of the first and second internal sound. A voice operated control (VOX) operatively coupled to the processor, the first earpiece, and the second earpiece, can control a mixing of at least one of the first and second ambient sound and at least one of the first and second internal sound for producing a mixed signal.</p>
<p id="p-0013" num="0012">The dual earpiece can further include a first Ear Canal Receiver (ECR) in the first earpiece for receiving audio content from an audio interface, and a second ECR in the second earpiece for receiving the audio content. The VOX can control a second mixing of the mixed signal with the audio content to produce a second mixed signal and control a delivery of the second mixed signal to the first ECR and the second ECR. For instance, the VOX can receive the first ambient sound from the first earpiece and the second internal sound from the second earpiece for controlling the mixing.</p>
<p id="p-0014" num="0013">In a fourth embodiment, a method for voice operable control suitable for use with an earpiece can include the steps of measuring an ambient sound received from at least one Ambient Sound Microphone (ASM), measuring an internal sound received from at least one Ear Canal Microphone (ECM), detecting a spoken voice from a wearer of the earpiece based on an analysis of the ambient sound and the internal sound, and controlling at least one voice operation of the earpiece if the presence of spoken voice is detected. The analysis can be non-difference comparison such as a correlation, a coherence, cross-correlation, or a signal ratio. For example in at least one exemplary embodiment the ratio of a measured first and second sound signal can be used to determine the presence of a user's voice. For example if a ratio of first signal/second signal or vice versa is above or below a set value, for example if an ECM measures a second signal at 90 dB and an ASM measures a first signal at 80 dB, then the ratio 90 dB/80 dB&#x3e;1 would be indicative of a user generated sound (e.g., voice). At least one exemplary embodiment could also use the log of the ratio or a difference of the logs. In one arrangement, the step of detecting a spoken voice is performed only if an absolute sound pressure level of the ambient sound or the internal sound is above a predetermined threshold. The method can further include performing a level comparison analysis of a first ambient sound captured from a first ASM in a first earpiece and a second ambient sound captured from a second ASM in a second earpiece. In another configuration, the level comparison analysis can be between a first internal sound captured from a first ECM in a first earpiece and a second internal sound captured from a second ECM in a second earpiece.</p>
<p id="p-0015" num="0014">In a fifth embodiment, a method for voice operable control suitable for use with an earpiece can include measuring an ambient sound received from at least one Ambient Sound Microphone (ASM), measuring an internal sound received from at least one Ear Canal Microphone (ECM), performing a cross correlation between the ambient sound and the internal sound, declaring a presence of spoken voice from a wearer of the earpiece if a peak of the cross correlation is within a predetermined amplitude range and a timing of the peak is within a predetermined time range, and controlling at least one voice operation of the earpiece if the presence of spoken voice is detected. For instance, the voice operated control can manage a voice monitoring system, a voice dictation system, or a voice recognition system. The spoken voice can be declared if the peak and the timing of the cross correlation reveals that the spoken voice arrives at the at least one ECM before the at least one ASM.</p>
<p id="p-0016" num="0015">In one configuration, the cross correlation can be performed between a first ambient sound within a first earpiece and a first internal sound within the first earpiece. In another configuration, the cross correlation can be performed between a first ambient sound within a first earpiece and a second internal sound within a second earpiece. In yet another configuration, the cross correlation can be performed either between a first ambient sound within a first earpiece and a second ambient sound within a second earpiece, or between a first internal sound within a first earpiece and a second internal sound within a second earpiece.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 1</figref> is a pictorial diagram of an earpiece in accordance with an exemplary embodiment;</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram of the earpiece in accordance with an exemplary embodiment;</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 3</figref> is a flowchart of a method for voice operated control in accordance with an exemplary embodiment;</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 4</figref> is a block diagram for mixing sounds responsive to voice operated control in accordance with an exemplary embodiment;</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 5</figref> is a flowchart for a voice activated switch based on level differences in accordance with an exemplary embodiment;</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 6</figref> is a block diagram of a voice activated switch using inputs from level and cross correlation in accordance with an exemplary embodiment;</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 7</figref> is a flowchart for a voice activated switch based on cross correlation in accordance with an exemplary embodiment;</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 8</figref> is a flowchart for a voice activated switch based on cross correlation using a fixed delay method in accordance with an exemplary embodiment; and</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 9</figref> is a flowchart for a voice activated switch based on cross correlation and coherence analysis using inputs from different earpieces in accordance with an exemplary embodiment.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0006" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0026" num="0025">The following description of at least one exemplary embodiment is merely illustrative in nature and is in no way intended to limit the invention, its application, or uses.</p>
<p id="p-0027" num="0026">Processes, techniques, apparatus, and materials as known by one of ordinary skill in the relevant art may not be discussed in detail but are intended to be part of the enabling description where appropriate, for example the fabrication and use of transducers.</p>
<p id="p-0028" num="0027">In all of the examples illustrated and discussed herein, any specific values, for example the sound pressure level change, should be interpreted to be illustrative only and non-limiting. Thus, other examples of the exemplary embodiments could have different values.</p>
<p id="p-0029" num="0028">Note that similar reference numerals and letters refer to similar items in the following figures, and thus once an item is defined in one figure, it may not be discussed for following figures.</p>
<p id="p-0030" num="0029">Note that herein when referring to correcting or preventing an error or damage (e.g., hearing damage), a reduction of the damage or error and/or a correction of the damage or error are intended.</p>
<p id="p-0031" num="0030">At least one exemplary embodiment of the invention is directed to an earpiece for voice operated control. Reference is made to <figref idref="DRAWINGS">FIG. 1</figref> in which an earpiece device, generally indicated as earpiece <b>100</b>, is constructed and operates in accordance with at least one exemplary embodiment of the invention. As illustrated, earpiece <b>100</b> depicts an electro-acoustical assembly <b>113</b> for an in-the-ear acoustic assembly, as it would typically be placed in the ear canal <b>131</b> of a user <b>135</b>. The earpiece <b>100</b> can be an in the ear earpiece, behind the ear earpiece, receiver in the ear, open-fit device, or any other suitable earpiece type. The earpiece <b>100</b> can be partially or fully occluded in the ear canal, and is suitable for use with users having healthy or abnormal auditory functioning.</p>
<p id="p-0032" num="0031">Earpiece <b>100</b> includes an Ambient Sound Microphone (ASM) <b>111</b> to capture ambient sound, an Ear Canal Receiver (ECR) <b>125</b> to deliver audio to an ear canal <b>131</b>, and an Ear Canal Microphone (ECM) <b>123</b> to assess a sound exposure level within the ear canal. The earpiece <b>100</b> can partially or fully occlude the ear canal <b>131</b> to provide various degrees of acoustic isolation. The assembly is designed to be inserted into the user's ear canal <b>131</b>, and to form an acoustic seal with the walls <b>129</b> of the ear canal at a location <b>127</b> between the entrance <b>117</b> to the ear canal <b>131</b> and the tympanic membrane (or ear drum) <b>133</b>. Such a seal is typically achieved by means of a soft and compliant housing of assembly <b>113</b>. Such a seal can create a closed cavity <b>131</b> of approximately 5 cc between the in-ear assembly <b>113</b> and the tympanic membrane <b>133</b>. As a result of this seal, the ECR (speaker) <b>125</b> is able to generate a full range bass response when reproducing sounds for the user. This seal also serves to significantly reduce the sound pressure level at the user's eardrum <b>133</b> resulting from the sound field at the entrance to the ear canal <b>131</b>. This seal is also a basis for a sound isolating performance of the electro-acoustic assembly.</p>
<p id="p-0033" num="0032">Located adjacent to the ECR <b>125</b>, is the ECM <b>123</b>, which is acoustically coupled to the (closed or partially closed) ear canal cavity <b>131</b>. One of its functions is that of measuring the sound pressure level in the ear canal cavity <b>131</b> as a part of testing the hearing acuity of the user as well as confirming the integrity of the acoustic seal and the working condition of the earpiece <b>100</b>. In one arrangement, the ASM <b>111</b> is housed in the assembly <b>113</b> to monitor sound pressure at the entrance to the occluded or partially occluded ear canal <b>131</b>. All transducers shown can receive or transmit audio signals to a processor <b>121</b> that undertakes audio signal processing and provides a transceiver for audio via the wired or wireless communication path <b>119</b>.</p>
<p id="p-0034" num="0033">The earpiece <b>100</b> can actively monitor a sound pressure level both inside and outside an ear canal <b>131</b> and enhance spatial and timbral sound quality while maintaining supervision to ensure safe sound reproduction levels. The earpiece <b>100</b> in various embodiments can conduct listening tests, filter sounds in the environment, monitor warning sounds in the environment, present notification based on identified warning sounds, maintain constant audio content to ambient sound levels, and filter sound in accordance with a Personalized Hearing Level (PHL).</p>
<p id="p-0035" num="0034">The earpiece <b>100</b> can generate an Ear Canal Transfer Function (ECTF) to model the ear canal <b>131</b> using ECR <b>125</b> and ECM <b>123</b>, as well as an Outer Ear Canal Transfer function (OETF) using ASM <b>111</b>. For instance, the ECR <b>125</b> can deliver an impulse within the ear canal <b>131</b> and generate the ECTF via cross correlation of the impulse with the impulse response of the ear canal <b>131</b>. The earpiece <b>100</b> can also determine a sealing profile with the user's ear to compensate for any leakage. It also includes a Sound Pressure Level Dosimeter to estimate sound exposure and recovery times. This permits the earpiece <b>100</b> to safely administer and monitor sound exposure to the ear.</p>
<p id="p-0036" num="0035">Referring to <figref idref="DRAWINGS">FIG. 2</figref>, a block diagram <b>200</b> of the earpiece <b>100</b> in accordance with an exemplary embodiment is shown. As illustrated, the earpiece <b>100</b> can include the processor <b>121</b> operatively coupled to the ASM <b>111</b>, ECR <b>125</b>, and ECM <b>123</b> via one or more Analog to Digital Converters (ADC) <b>202</b> and Digital to Analog Converters (DAC) <b>203</b>. The processor <b>121</b> can utilize computing technologies such as a microprocessor, Application Specific Integrated Chip (ASIC), and/or digital signal processor (DSP) with associated storage memory <b>208</b> such as Flash, ROM, RAM, SRAM, DRAM or other like technologies for controlling operations of the earpiece device <b>100</b>. The processor <b>121</b> can also include a clock to record a time stamp.</p>
<p id="p-0037" num="0036">As illustrated, the earpiece <b>100</b> can include a voice operated control (VOX) module <b>201</b> to provide voice control to one or more subsystems, such as a voice recognition system, a voice dictation system, a voice recorder, or any other voice related processor. The VOX <b>201</b> can also serve as a switch to indicate to the subsystem a presence of spoken voice and a voice activity level of the spoken voice. The VOX <b>201</b> can be a hardware component implemented by discrete or analog electronic components or a software component. In one arrangement, the processor <b>121</b> can provide functionality of the VOX <b>201</b> by way of software, such as program code, assembly language, or machine language.</p>
<p id="p-0038" num="0037">The memory <b>208</b> can also store program instructions for execution on the processor <b>121</b> as well as captured audio processing data. For instance, memory <b>208</b> can be off-chip and external to the processor <b>121</b>, and include a data buffer to temporarily capture the ambient sound and the internal sound, and a storage memory to save from the data buffer the recent portion of the history in a compressed format responsive to a directive by the processor. The data buffer can be a circular buffer that temporarily stores audio sound at a current time point to a previous time point. It should also be noted that the data buffer can in one configuration reside on the processor <b>121</b> to provide high speed data access. The storage memory <b>208</b> can be non-volatile memory such as SRAM to store captured or compressed audio data.</p>
<p id="p-0039" num="0038">The earpiece <b>100</b> can include an audio interface <b>212</b> operatively coupled to the processor <b>121</b> and VOX <b>201</b> to receive audio content, for example from a media player, cell phone, or any other communication device, and deliver the audio content to the processor <b>121</b>. The processor <b>121</b> responsive to detecting voice operated events from the VOX <b>202</b> can adjust the audio content delivered to the ear canal. For instance, the processor <b>121</b> (or VOX <b>201</b>) can lower a volume of the audio content responsive to detecting an event for transmitting the acute sound to the ear canal. The processor <b>121</b> by way of the ECM <b>123</b> can also actively monitor the sound exposure level inside the ear canal and adjust the audio to within a safe and subjectively optimized listening level range based on voice operating decisions made by the VOX <b>201</b>.</p>
<p id="p-0040" num="0039">The earpiece <b>100</b> can further include a transceiver <b>204</b> that can support singly or in combination any number of wireless access technologies including without limitation Bluetooth&#x2122;, Wireless Fidelity (WiFi), Worldwide Interoperability for Microwave Access (WiMAX), and/or other short or long range communication protocols. The transceiver <b>204</b> can also provide support for dynamic downloading over-the-air to the earpiece <b>100</b>. It should be noted also that next generation access technologies can also be applied to the present disclosure.</p>
<p id="p-0041" num="0040">The location receiver <b>232</b> can utilize common technology such as a common GPS (Global Positioning System) receiver that can intercept satellite signals and therefrom determine a location fix of the earpiece <b>100</b>.</p>
<p id="p-0042" num="0041">The power supply <b>210</b> can utilize common power management technologies such as replaceable batteries, supply regulation technologies, and charging system technologies for supplying energy to the components of the earpiece <b>100</b> and to facilitate portable applications. A motor (not shown) can be a single supply motor driver coupled to the power supply <b>210</b> to improve sensory input via haptic vibration. As an example, the processor <b>121</b> can direct the motor to vibrate responsive to an action, such as a detection of a warning sound or an incoming voice call.</p>
<p id="p-0043" num="0042">The earpiece <b>100</b> can further represent a single operational device or a family of devices configured in a master-slave arrangement, for example, a mobile device and an earpiece. In the latter embodiment, the components of the earpiece <b>100</b> can be reused in different form factors for the master and slave devices.</p>
<p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. 3</figref> is a flowchart of a method <b>300</b> for voice operated control in accordance with an exemplary embodiment. The method <b>300</b> can be practiced with more or less than the number of steps shown and is not limited to the order shown. To describe the method <b>300</b>, reference will be made to <figref idref="DRAWINGS">FIG. 4</figref> and components of <figref idref="DRAWINGS">FIG. 1</figref> and <figref idref="DRAWINGS">FIG. 2</figref>, although it is understood that the method <b>300</b> can be implemented in any other manner using other suitable components. The method <b>300</b> can be implemented in a single earpiece, a pair of earpieces, headphones, or other suitable headset audio delivery device.</p>
<p id="p-0045" num="0044">The method <b>300</b> can start in a state wherein the earpiece <b>100</b> has been inserted in an ear canal <b>131</b> of a wearer. As shown in step <b>302</b>, the earpiece <b>100</b> can measure ambient sounds in the environment received at the ASM <b>111</b>. Ambient sounds correspond to sounds within the environment such as the sound of traffic noise, street noise, conversation babble, or any other acoustic sound. Ambient sounds can also correspond to industrial sounds present in an industrial setting, such as factory noise, lifting vehicles, automobiles, and robots to name a few.</p>
<p id="p-0046" num="0045">During the measuring of ambient sounds in the environment, the earpiece <b>100</b> also measures internal sounds, such as ear canal levels, via the ECM <b>123</b> as shown in step <b>304</b>. The internal sounds can include ambient sounds passing through the earpiece <b>100</b> as well as spoken voice generated by a wearer of the earpiece <b>100</b>. Although the earpiece <b>100</b> when inserted in the ear can partially of fully occlude the ear canal <b>131</b>, the earpiece <b>100</b> may not completely attenuate the ambient sound. The passive aspect of the earpiece <b>100</b>, due to the mechanical and sealing properties, can provide upwards of a 22 dB noise reduction. Portions of ambient sounds higher than the noise reduction level may still pass through the earpiece <b>100</b> into the ear canal <b>131</b> thereby producing residual sounds. For instance, high energy low frequency sounds may not be completely attenuated. Accordingly, residual sound may be resident in the ear canal <b>131</b> producing internal sounds that can be measured by the ECM <b>123</b>. Internal sounds can also correspond to audio content and spoken voice when the user is speaking and/or audio content is delivered by the ECR <b>125</b> to the ear canal <b>131</b> by way of the audio interface <b>212</b>.</p>
<p id="p-0047" num="0046">At step <b>306</b>, the processor <b>121</b> compares the ambient sound and the internal sound to determine if the wearer (i.e., the user <b>135</b> wearing the earpiece <b>100</b>) of the earpiece <b>100</b> is speaking. That is, the processor <b>121</b> determines if the sound received at the ASM <b>111</b> and ECM <b>123</b> corresponds to the wearer's voice or to other voices in the wearer's environment. Notably, the enclosed air chamber (&#x2dc;5 cc volume) within the user's ear canal <b>131</b> due to the occlusion of the earpiece <b>100</b> causes a build up of sound waves when the wearer speaks. Accordingly, the ECM <b>123</b> picks up the wearer's voice in the ear canal <b>131</b> when the wearer is speaking even though the ear canal is occluded. The processor <b>121</b>, by way of one or more non-difference comparison approaches, such as correlation analysis, cross-correlation analysis, and coherence analysis determines whether the sound captured at the ASM <b>111</b> and ECM <b>123</b> corresponds to the wearer's voice or ambient sounds in the environment, such as other users talking in a conversation. The processor <b>121</b> can also identify a voicing level from the ambient sound and the internal sound. The voicing level identifies a degree of intensity and periodicity of the sound. For instance, a vowel is highly voiced due to the periodic vibrations of the vocal cords and the intensity of the air rushing through the vocal cords from the lungs. In contrast, unvoiced sounds such as fricatives and plosives have a low voicing level since they are produced by rushing non-periodic air waves and are relatively short in duration.</p>
<p id="p-0048" num="0047">If at step <b>308</b>, spoken voice from the wearer of the earpiece <b>100</b> is detected, the earpiece <b>100</b> can proceed to control a mixing of the ambient sound received at the ASM <b>111</b> with the internal sound received at the ECM <b>123</b>, as shown in step <b>310</b>, and in accordance with the block diagram <b>400</b> of <figref idref="DRAWINGS">FIG. 4</figref>. If spoken voice from the wearer is not detected, the method <b>300</b> can proceed back to step <b>302</b> and step <b>304</b> to monitor ambient and internal sounds. The VOX <b>201</b> can also generate a voice activity flag declaring the presence of spoken voice by the wearer of the earpiece <b>100</b>, which can be passed to other subsystems.</p>
<p id="p-0049" num="0048">As shown in <figref idref="DRAWINGS">FIG. 4</figref>, the first mixing <b>402</b> can include adjusting the gain of the ambient sound and internal sound, and with respect to background noise levels. For instance, the VOX <b>201</b> upon deciding that the sound captured at the ASM <b>111</b> and ECM <b>123</b> originates from the wearer of the earpiece <b>100</b> can combine the ambient sound and the internal sound with different gains to produce a mixed signal. The mixed signal can apply weightings more towards the ambient sound or internal sound depending on the background noise level, the wearer's vocalization level, or spectral characteristics. The mixed signal can thus include sound waves from the wearer's voice captured at the ASM <b>111</b> and also sound waves captured internally in the wearer's ear canal generated via bone conduction.</p>
<p id="p-0050" num="0049">Briefly referring to <figref idref="DRAWINGS">FIG. 4</figref>, a block diagram <b>400</b> for voice operated control is shown. The VOX <b>201</b> can include algorithmic modules <b>402</b> for a non-difference comparison such as correlation, cross-correlation, and coherence. The VOX <b>201</b> applies one or more of these decisional approaches, as will be further described ahead, for determining if the ambient sound and internal sound correspond to the wearer's spoken voice. In the decisional process, the VOX <b>201</b> can prior to the first mixing <b>404</b> assign mixing gains (&#x3b1;) and (1&#x2212;&#x3b1;) to the ambient sound signal from the ASM <b>111</b> and the internal sound signal from the ECM <b>123</b>. These mixing gains establish how the ambient sound signals and internal sound signals are combined for further processing.</p>
<p id="p-0051" num="0050">In one arrangement based on correlation, the processor <b>121</b> determines if the internal sound captured at the ECM <b>123</b> arrives before the ambient sound at the ASM <b>111</b>. Since the wearer's voice is generated via bone conduction in the ear canal <b>131</b>, it travels a shorter distance than an acoustic wave emanating from the wearer's mouth to the ASM <b>111</b> at the wearer's ear. The VOX <b>201</b> can analyze the timing of one or more peaks in a cross correlation between the ambient sound and the internal sound to determine whether the sound originates from the ear canal <b>131</b>, thus indicating that the wearer's spoken voice generated the sound. Whereas, sounds generated external to the ear canal <b>131</b>, such as those of neighboring talkers, reach the ASM <b>111</b> before passing through the earpiece <b>100</b> into the wearer's ear canal <b>131</b>. A spectral comparison of the ambient sound and internal sound can also be performed to determine the origination point of the captured sound.</p>
<p id="p-0052" num="0051">In another arrangement based on level detection, the processor <b>121</b> determines if either the ambient sound or internal sound exceeds a predetermined threshold, and if so, compares a Sound Pressure Level (SPL) between the ambient sound and internal sound to determine if the sound originates from the wearer's voice. In general, the SPL at the ECM <b>123</b> is higher than the SPL at the ASM <b>111</b> if the wearer of the earpiece <b>100</b> is speaking. Accordingly, a first metric in determining whether the sound captured at the ASM <b>111</b> and ECM <b>123</b> is to compare the SPL levels at both microphones.</p>
<p id="p-0053" num="0052">In another arrangement based on spectral distribution, a spectrum analysis can be performed on audio frames to assess the voicing level. The spectrum analysis can reveal peaks and valleys of vowels characteristic of voiced sounds. Most vowels are represented by three to four formants which contain a significant portion of the audio energy. Formants are due to the shaping of the air passageway (e.g., throat, tongue, and mouth) as the user &#x2018;forms&#x2019; speech sounds. The voicing level can be assigned based on the degree of formant peaking and bandwidth.</p>
<p id="p-0054" num="0053">The threshold metric can be first employed so as to minimize the amount of processing required to continually monitor sounds in the wearer's environment before performing the comparison. The threshold establishes the level at which a comparison between the ambient sound and internal sound is performed. The threshold can also be established via learning principles, for example, wherein the earpiece <b>100</b> learns when the wearer is speaking and his or her speaking level in various noisy environments. For instance, the processor <b>121</b> can record background noise estimates from the ASM <b>111</b> while simultaneously monitoring the wearer's speaking level at the ECM <b>123</b> to establish the wearer's degree of vocalization relative to the background noise.</p>
<p id="p-0055" num="0054">Returning back to <figref idref="DRAWINGS">FIG. 3</figref>, at step <b>312</b>, the VOX <b>201</b> can deliver the mixed signal to a portable communication device, such as a cell phone, personal digital assistant, voice recorder, laptop, or any other networked or non-networked system component (see also <figref idref="DRAWINGS">FIG. 4</figref>). Recall the VOX <b>201</b> can generate the mixed signal in view of environmental conditions, such as the level of background noise. So, in high background noises, the mixed signal can include more of the internal sound from the wearer's voice generated in ear canal <b>131</b> and captured at the ECM <b>123</b> than the ambient sound with the high background noise. In a quiet environment, the mixed signal can include more of the ambient sound captured at the ASM <b>111</b> than the wearer's voice generated in ear canal <b>131</b>. The VOX <b>201</b> can also apply various spectral equalizations to account for the differences in spectral timbre from the ambient sound and the internal sound based on the voice activity level and/or mixing scheme.</p>
<p id="p-0056" num="0055">As shown in optional step <b>314</b>, the VOX <b>201</b> can also record the mixed signal for further analysis by a voice processing system. For instance, the earpiece <b>100</b> having identified voice activity levels previously at step <b>308</b> can pass a command to another module such as a voice recognition system, a voice dictation system, a voice recorder, or any other voice processing module. The recording of the mixed signal at step <b>314</b> allows the processor <b>121</b>, or voice processing system receiving the mixed signal to analyze the mixed signal for information, such as voice commands or background noises. The voice processing system can thus examine a history of the mixed signal from the recorded information.</p>
<p id="p-0057" num="0056">The earpiece <b>100</b> can also determine whether the sound corresponds to a spoken voice of the wearer even when the wearer is listening to music, engaged in a phone call, or receiving audio via other means. Moreover, the earpiece <b>100</b> can adjust the internal sound generated within the ear canal <b>131</b> to account for the audio content being played to the wearer while the wearer is speaking. As shown in step <b>316</b>, the VOX <b>201</b> can determine if audio content is being delivered to the ECR <b>125</b> in making the determination of spoken voice. Recall, audio content such as music is delivered to the ear canal <b>131</b> via the ECR <b>125</b> which plays the audio content to the wearer of the earpiece <b>100</b>. If at step <b>318</b>, the earpiece <b>100</b> is delivering audio content to the user, the VOX <b>201</b> at step <b>320</b> can control a second mixing of the mixed signal with the audio content to produce a second mixed signal (see second mixer <b>406</b> of <figref idref="DRAWINGS">FIG. 4</figref>). This second mixing provides loop-back from the ASM <b>111</b> and the ECM <b>123</b> of the wearer's own voice to allow the wearer to hear themselves when speaking in the presence of audio content delivered to the ear canal <b>131</b> via the ECR <b>125</b>. If audio content is not playing, the method <b>300</b> can proceed back to step <b>310</b> to control the mixing of the wearer's voice (i.e., speaker voice) between the ASM <b>111</b> and the ECM <b>123</b>.</p>
<p id="p-0058" num="0057">Upon mixing the mixed signal with the audio content, the VOX <b>201</b> can deliver the second mixed signal to the ECR <b>125</b> as indicated in step <b>322</b> (see also <figref idref="DRAWINGS">FIG. 4</figref>). In such regard, the VOX <b>201</b> permits the wearer to monitor his or her own voice and simultaneously hear the audio content. The method can end after step <b>322</b>. Notably, the second mixing can also include soft muting of the audio content during the duration of voice activity detection, and resuming audio content playing during non-voice activity or after a predetermined amount of time. The VOX <b>201</b> can further amplify or attenuate the spoken voice based on the level of the audio content if the wearer is speaking at a higher level and trying to overcome the audio content they hear. For instance, the VOX <b>201</b> can compare and adjust a level of the spoken voice with respect to a previously calculated (e.g., via learning) level.</p>
<p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. 5</figref> is a flowchart <b>500</b> for a voice activated switch based on level differences in accordance with an exemplary embodiment. The flowchart <b>500</b> can include more or less than the number of steps shown and is not limited to the order of the steps. The flowchart <b>500</b> can be implemented in a single earpiece, a pair of earpieces, headphones, or other suitable headset audio delivery device.</p>
<p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. 5</figref> illustrates an arrangement wherein the VOX <b>201</b> uses as its inputs the ambient sound microphone (ASM) signals from the left (L) <b>578</b> and right (R) <b>582</b> earphone devices, and the Ear Canal Microphone (ECM) signals from the left (L) <b>580</b> and right (R) <b>584</b> signals. The ASM and ECM signals are amplified with amplifiers <b>575</b>, <b>577</b>, <b>579</b>, <b>581</b> before being filtered using Band Pass Filters (BPFs) <b>583</b>, <b>585</b>, <b>587</b>, <b>589</b>, which can have the same frequency response. The filtering can use analog or digital electronics, as may the subsequent signal strength comparator <b>588</b> of the filtered and amplified ASM and ECM signals from the left and right earphone devices. The VOX <b>201</b> determines that when the filtered ECM signal level exceeds the filtered ASM signal level by an amount determined by the reference difference unit <b>586</b>, decision units <b>590</b>, <b>591</b> deem that user-generated voice is present. The VOX <b>201</b> introduces a further decision unit <b>592</b> that takes as its input the outputs of decision units <b>590</b>, <b>591</b> from both the left and right earphone devices, which can be combined into a single functional unit. As an example, the decision unit <b>592</b> can be either an AND or OR logic gate, depending on the operating mode selected with (optional) user-input <b>598</b>. The output decision <b>594</b> operates the VOX <b>201</b> in a voice communication system, for example, allowing the user's voice to be transmitted to a remote individual (e.g. using radio frequency communications) or for the user's voice to be recorded.</p>
<p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. 6</figref> is a block diagram <b>600</b> of a voice activated switch using inputs from level and cross correlation in accordance with an exemplary embodiment. The block diagram <b>600</b> can include more or less than the number of steps shown and is not limited to the order of the steps. The block diagram <b>600</b> can be implemented in a single earpiece, a pair of earpieces, headphones, or other suitable headset audio delivery device.</p>
<p id="p-0062" num="0061">As illustrated, the voice activated switch <b>600</b> uses both the level-based detection method <b>670</b> described in <figref idref="DRAWINGS">FIG. 5</figref> and also a correlation-based method <b>672</b> described ahead in <figref idref="DRAWINGS">FIG. 7</figref>. The decision unit <b>699</b> can be either an AND or OR logic gate, depending on the operating mode selected with (optional) user-input <b>698</b>. The decision unit <b>699</b> can generate a voice activated on or off decision <b>691</b>.</p>
<p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. 7</figref> is a flowchart <b>700</b> for a voice activated switch based on cross correlation in accordance with an exemplary embodiment. The flowchart <b>700</b> can include more or less than the number of steps shown and is not limited to the order of the steps. The flowchart <b>700</b> can be implemented in a single earpiece, a pair of earpieces, headphones, or other suitable headset audio delivery device.</p>
<p id="p-0064" num="0063">A cross-correlation between two signals is a measure of their similarity. In general, a cross-correlation between ASM and ECM signals is defined according to the following equation:</p>
<p id="p-0065" num="0064">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mrow>
            <mrow>
              <mi>X</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mrow>
                <mi>Corr</mi>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>n</mi>
                    <mo>,</mo>
                    <mn>1</mn>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mrow>
            <mo>=</mo>
            <mrow>
              <munderover>
                <mo>&#x2211;</mo>
                <mrow>
                  <mi>n</mi>
                  <mo>=</mo>
                  <mn>0</mn>
                </mrow>
                <mi>N</mi>
              </munderover>
              <mo>&#x2062;</mo>
              <mrow>
                <mrow>
                  <mi>ASM</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mi>n</mi>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>&#x2062;</mo>
                <mrow>
                  <mi>ECM</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mi>n</mi>
                      <mo>-</mo>
                      <mn>1</mn>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
          <mo>,</mo>
          <mstyle>
            <mtext>
</mtext>
          </mstyle>
          <mo>&#x2062;</mo>
          <mrow>
            <mi>Where</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mtext>:</mtext>
            </mstyle>
          </mrow>
        </mrow>
        <mo>&#x2062;</mo>
        <mstyle>
          <mtext>
</mtext>
        </mstyle>
        <mo>&#x2062;</mo>
        <mrow>
          <mrow>
            <mi>l</mi>
            <mo>=</mo>
            <mn>0</mn>
          </mrow>
          <mo>,</mo>
          <mn>1</mn>
          <mo>,</mo>
          <mn>2</mn>
          <mo>,</mo>
          <mrow>
            <mi>&#x2026;</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.8em" height="0.8ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>N</mi>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>1</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0066" num="0065">Where: ASM(n) is the n<sup>th </sup>sample of the ASM signal, and ECM(n&#x2212;1) is the (n&#x2212;1)<sup>th </sup>sample of the ECM signal.</p>
<p id="p-0067" num="0066">Using a non-difference comparison approach such as cross-correlation (or correlation and coherence) between the ASM and ECM signals to determine user voice activity is more reliable than taking the level difference of the ASM and ECM signals. Using the cross-correlation rather than a level differencing approach significantly reduces &#x201c;False-positives&#x201d; which may occur due to user non-speech body noise, such as teeth chatter; sneezes, coughs, etc. Furthermore, such non-speech user generated noise would generate a larger sound level in the ear canal (i.e. and a higher ECM signal level) than on the outside of the same ear canal (i.e. and a lower ASM signal level). Therefore, a VOX system that relies on level difference between the ASM and the ECM is often &#x201c;tricked&#x201d; into falsely determining that user voice was present.</p>
<p id="p-0068" num="0067">False-positive speech detection can use unnecessary radio bandwidth for single-duplex voice communication systems. Furthermore, false positive user voice activity can be dangerous, for instance with an emergency worker in the field whose incoming voice signal from a remote location may be muted in response to a false-positive VOX decision. Thus, minimizing false positives using a non-difference comparison approach is beneficial to protecting the user from harm.</p>
<p id="p-0069" num="0068">Single-lag auto-correlation is sufficient when only a single audio signal is available for analysis, but can provide false-positives both when the input signal is from an ECM (for instance, voice sounds such as murmurs or humming will trigger the VOX), or when the input signal is from an ASM (in such a case, voice sounds from ambient sound sources such as other individuals or reproduced sound from loudspeakers will trigger the VOX).</p>
<p id="p-0070" num="0069">Like Correlation and Cross-Correlation, a coherence function is also a measure of similarity between two signals and is a non-difference comparison approach, defined as:</p>
<p id="p-0071" num="0070">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <msubsup>
          <mi>&#x3b3;</mi>
          <mi>xy</mi>
          <mn>2</mn>
        </msubsup>
        <mo>=</mo>
        <mfrac>
          <msup>
            <mrow>
              <mo>&#xf603;</mo>
              <mrow>
                <msub>
                  <mi>G</mi>
                  <mi>xy</mi>
                </msub>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>(</mo>
                  <mi>f</mi>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mo>&#xf604;</mo>
            </mrow>
            <mn>2</mn>
          </msup>
          <mrow>
            <mrow>
              <msub>
                <mi>G</mi>
                <mi>xx</mi>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mi>f</mi>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>&#x2062;</mo>
            <mrow>
              <msub>
                <mi>G</mi>
                <mi>yy</mi>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mi>f</mi>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
        </mfrac>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>2</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
Where G<sub>xy </sub>is the cross-spectrum of two signals (e.g. the ASM and ECM signals), and can be calculated by first computing the cross-correlation in equation (1), applying a window function (e.g. Hanning window), and transforming the result to the frequency domain (e.g. via an FFT). G<sub>xx </sub>or G<sub>yy </sub>is the auto-power spectrum of either the ASM or ECM signals, and can be calculated by first computing the auto-correlation (using equation 1, but where the two input signals are both from either the ASM or ECM and transforming the result to the frequency domain. The coherence function gives a frequency-dependant vector between 0 and 1, where a high coherence at a particular frequency indicates a high degree of coherence at this frequency, and can therefore be used to only analyze those speech frequencies in the ASM and ECM signals (e.g. in the 300 Hz-3 kHz range), whereby a high coherence indicates voice activity (e.g. a coherence greater than 0.7).
</p>
<p id="p-0072" num="0071">As illustrated, there are two parallel paths for the left and right earphone devices. For each earphone device, the inputs are the filtered ASM and ECM signals. In the first path, the left (L) ASM signal <b>788</b> is passed to a gain function <b>775</b> and band-pass filtered by BPF <b>783</b>. The left (L) ECM signal <b>780</b> is also passed to a gain function <b>777</b> and band-pass filtered by BPF <b>785</b>. In the second path, the right (R) ASM signal <b>782</b> is passed to a gain function <b>779</b> and band-pass filtered by BPF <b>787</b>. The right (R) ECM signal <b>784</b> is also passed to a gain function <b>781</b> and band-pass filtered by BPF <b>789</b>. The filtering can be performed in the time domain or digitally using frequency or time domain filtering. A cross correlation or coherence between the gain scaled and band-pass filtered signals is then calculated at unit <b>795</b>.</p>
<p id="p-0073" num="0072">Upon calculating the cross correlation, decision unit <b>796</b> undertakes analysis of the cross-correlation vector to determine a peak and the lag at which this peak occurs for each path. An optional &#x201c;learn mode&#x201d; unit <b>799</b> is used to train the decision unit <b>796</b> to be robust to detect the user voice, and lessen the chance of false positives (i.e. predicting user voice when there is none) and false negatives (i.e. predicting no user voice when there is user voice). In this learn mode, the user is prompted to speak (e.g. using a user-activated voice or non-voice audio command and/or visual command using a display interface on a remote control unit), and the VOX <b>201</b> records the calculated cross-correlation and extracts the peak value and lag at which this peak occurs. The lag and (optionally) peak value for this reference measurement in &#x201c;learn mode&#x201d; is then recorded to computer memory and is used to compare other cross-correlation measurements. If the lag-time for the peak cross-correlation measurement matches the reference lag value, or another pre-determined value, then the decision unit <b>796</b> outputs a &#x201c;user voice active&#x201d; message (e.g. represented by a logical 1, or soft decision between 0 and 1) to the second decision unit <b>720</b>. In some embodiments, the decision unit <b>720</b> can be an OR gate or AND gate; as determined by the particular operating mode <b>722</b> (which may be user defined or pre-defined). The decision unit <b>720</b> can generate a voice activated on or off decision <b>724</b>.</p>
<p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. 8</figref> is a flowchart <b>800</b> for a voice activated switch based on cross correlation using a fixed delay method in accordance with an exemplary embodiment. The flowchart <b>800</b> can include more or less than the number of steps shown and is not limited to the order of the steps. The flowchart <b>800</b> can be implemented in a single earpiece, a pair of earpieces, headphones, or other suitable headset audio delivery device</p>
<p id="p-0075" num="0074">Flowchart <b>800</b> provides an overview of a multi-band analysis of cross-correlation platform. In one arrangement, the cross-correlation can use a fixed-delay cross-correlation method. The logic output of the different band-pass filters (<b>810</b>-<b>816</b>) are fed into decision unit <b>896</b> for both the left earphone device (via band-pass filters <b>810</b>, <b>812</b>) and the right earphone device (via band-pass filters <b>814</b>, <b>816</b>). The decision unit <b>896</b> can be a simple logical AND unit, or an OR unit (this is because depending on the particular vocalization of the user, e.g. a sibilant fricative or a voiced vowel, the lag of the peak in the cross-correlation analysis may be different for different frequencies). The particular configuration of the decision unit <b>896</b> can be configured by the operating mode <b>822</b>, which may be user-defined or pre-defined. The dual decision unit <b>820</b> in the preferred embodiment is a logical AND gate, though may be an OR gate, and returns a binary decision to the VOX on or off decision <b>824</b>.</p>
<p id="p-0076" num="0075"><figref idref="DRAWINGS">FIG. 9</figref> is a flowchart <b>900</b> for a voice activated switch based on cross correlation and coherence analysis using inputs from different earpieces in accordance with an exemplary embodiment. The flowchart <b>900</b> can include more or less than the number of steps shown and is not limited to the order of the steps. The flowchart <b>900</b> can be implemented in a single earpiece, a pair of earpieces, headphones, or other suitable headset audio delivery device.</p>
<p id="p-0077" num="0076">Flowchart <b>900</b> is a variation of flowchart <b>700</b> where instead of comparing the ASM and ECM signals of the same earphone device, the ASM signals of different earphone devices are compared, and alternatively or additionally, the ECM signals of different earphone devices are also compared. As illustrated, there are two parallel paths for the left and right earphone device. For each earphone device, the inputs are the filtered ASM and ECM signals. In the first path, the left (L) ASM signal <b>988</b> is passed to a gain function <b>975</b> and band-pass filtered by BPF <b>983</b>. The right (R) ASM signal <b>980</b> is also passed to a gain function <b>977</b> and band-pass filtered by BPF <b>985</b>. The filtering can be performed in the time domain or digitally using frequency or time domain filtering. In the second path, the left (L) ECM signal <b>982</b> is passed to a gain function <b>979</b> and band-pass filtered by BPF <b>987</b>. The right (R) ECM signal <b>984</b> is also passed to a gain function <b>981</b> and band-pass filtered by BPF <b>989</b>.</p>
<p id="p-0078" num="0077">A cross correlation or coherence between the gain scaled and band-pass filtered signals is then calculated at unit <b>996</b> for each path. Upon calculating the cross correlation, decision unit <b>996</b> undertakes analysis of the cross-correlation vector to determine a peak and the lag at which this peak occurs. The decision unit <b>996</b> searches for a high coherence or a correlation with a maxima at lag zero to indicate that the origin of the sound source is equidistant to the input sound sensors. If the lag-time for the peak a cross-correlation measurement matches a reference lag value, or another pre-determined value, then the decision unit <b>996</b> outputs a &#x201c;user voice active&#x201d; message (e.g. represented by a logical 1, or soft decision between 0 and 1) to the second decision unit <b>920</b>. In some embodiments, the decision unit <b>920</b> can be an OR gate or AND gate; as determined by the particular operating mode <b>922</b> (which may be user defined or pre-defined). The decision unit <b>920</b> can generate a voice activated on or off decision <b>924</b>. An optional &#x201c;learn mode&#x201d; unit <b>999</b> is used to train decision units <b>996</b>, similar to learn mode unit <b>799</b> described above with respect to <figref idref="DRAWINGS">FIG. 7</figref>.</p>
<p id="p-0079" num="0078">While the present invention has been described with reference to exemplary embodiments, it is to be understood that the invention is not limited to the disclosed exemplary embodiments. The scope of the following claims is to be accorded the broadest interpretation so as to encompass all modifications, equivalent structures and functions of the relevant exemplary embodiments. Thus, the description of the invention is merely exemplary in nature and, thus, variations that do not depart from the gist of the invention are intended to be within the scope of the exemplary embodiments of the present invention. Such variations are not to be regarded as a departure from the spirit and scope of the present invention.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08625819-20140107-M00001.NB">
<img id="EMI-M00001" he="16.93mm" wi="76.20mm" file="US08625819-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US08625819-20140107-M00002.NB">
<img id="EMI-M00002" he="7.79mm" wi="76.20mm" file="US08625819-20140107-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. An earpiece, comprising:
<claim-text>an Ambient Sound Microphone (ASM) configured to capture ambient sound;</claim-text>
<claim-text>an Ear Canal Microphone (ECM) configured to capture internal sound in an ear canal;</claim-text>
<claim-text>an Ear Canal Receiver (ECR) configured to deliver audio content to the ear canal;</claim-text>
<claim-text>a processor operatively coupled to the ASM, the ECM and the ECR, where the processor is configured to detect a spoken voice generated by a wearer of the earpiece based on a non-difference comparison of the ambient sound captured by the ASM and the internal sound captured by the ECM, where an output signal indicating that the spoken voice is detected is generated when the non-difference comparison is greater than a threshold; and</claim-text>
<claim-text>a voice operated control (VOX) operatively coupled to receive the output signal of the processor and configured to mix the ambient sound and the internal sound to produce a mixed signal and control production of the mixed signal based on at least one of one or more aspects of the audio content or one or more aspects of the spoken voice,</claim-text>
<claim-text>wherein the VOX is configured to increase a first gain of one of the ambient sound and the internal sound and decrease a second gain of a remaining one of the ambient sound and the internal sound, such that the mixed signal includes a combination of the ambient sound and the internal sound, and</claim-text>
<claim-text>wherein the non-difference comparison is at least one among a correlation, a cross-correlation and a comparison that uses coherence.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The earpiece of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more aspects of the spoken voice include at least one of a volume level, a voicing level, or a spectral shape of the spoken voice.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The earpiece of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more aspects of the audio content include at least one of a spectral distribution, a duration, or a volume of the audio content.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The earpiece of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the VOX comprises:
<claim-text>a level detector configured to compare a sound pressure level (SPL) of the ambient sound and the internal sound for detecting the spoken voice;</claim-text>
<claim-text>a correlation unit configured to access the correlation of the ambient sound and the internal sound for detecting the spoken voice;</claim-text>
<claim-text>a coherence unit configured to determine whether the spoken voice originates from the wearer; and</claim-text>
<claim-text>a spectral analysis unit configured to detect whether spectral portions of the spoken voice are similar in the ambient sound and the internal sound.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. A dual earpiece, comprising:
<claim-text>a first earpiece comprising:
<claim-text>a first Ambient Sound Microphone (ASM) configured to measure a first ambient sound, and</claim-text>
<claim-text>a first Ear Canal Microphone (ECM) configured to measure a first internal sound in a first ear canal;</claim-text>
</claim-text>
<claim-text>a second earpiece comprising:
<claim-text>a second Ambient Sound Microphone (ASM) configured to measure a second ambient sound, and</claim-text>
<claim-text>a second Ear Canal Microphone (ECM) configured to measure a second internal sound in a second ear canal;</claim-text>
</claim-text>
<claim-text>a processor operatively coupled to the first earpiece and the second earpiece, where the processor is configured to detect a spoken voice generated by a wearer of the dual earpiece based on a non-difference comparison of at least one of the first and second ambient sounds and at least one of the first and second internal sounds, where the spoken voice of the wearer is detected when the non-difference comparison is greater than a threshold; and</claim-text>
<claim-text>a voice operated control (VOX) configured to mix a first signal and a second signal to produce a mixed signal and control production of the mixed signal based on one or more aspects of the spoken voice, the first signal including at least one of the first and second ambient sounds, the second signal including at least one of the first and second internal sounds,</claim-text>
<claim-text>wherein the VOX is configured to increase a first gain of one of the first signal and the second signal and decrease a second gain of a remaining one of the first signal and the second signal, such that the mixed signal includes a combination of the first signal and the second signal, and</claim-text>
<claim-text>wherein the non-difference comparison is at least one among a correlation, a cross-correlation and a comparison that uses coherence.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The dual earpiece of <claim-ref idref="CLM-00005">claim 5</claim-ref>, further comprising:
<claim-text>a first Ear Canal Receiver (ECR) in the first earpiece for receiving first audio content from an audio interface; and</claim-text>
<claim-text>a second ECR in the second earpiece for receiving a second audio content, wherein the VOX controls a further mixing of the mixed signal with at least one of the first and second audio content to produce a further mixed signal and controls a delivery of the further mixed signal to at least one of the first ECR and the second ECR.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The dual earpiece of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the VOX receives the first ambient sound from the first earpiece and the second internal sound from the second earpiece for controlling the mixing.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The dual earpiece of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the dual earpiece is coupled to a remote device, the remote device including at least one microphone configured to measure at least one acoustic signal, the non-difference comparison being further determined using the at least one acoustic signal.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The dual earpiece of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the remote device includes at least one of an earpiece, a cell phone, a media player, a portable computing device or a personal digital assistant.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. A method for voice operated control, the method comprising the steps of:
<claim-text>measuring a first sound received from a first microphone (FM);</claim-text>
<claim-text>measuring a second sound received from a second microphone (SM);</claim-text>
<claim-text>detecting a spoken voice based on a non-difference comparison of the first and second sounds where the spoken voice is detected when the non-difference comparison exceeds a threshold;</claim-text>
<claim-text>mixing the first sound and the second sound to produce a mixed signal and controlling the production of the mixed signal based on one or more aspects of the spoken voice, by increasing a first gain of one of the first sound and the second sound and decreasing a second gain of a remaining one of the first sound and the second sound, such that the mixed signal includes a combination of the first sound and the second sound; and</claim-text>
<claim-text>controlling at least one voice operation if the spoken voice is detected,</claim-text>
<claim-text>wherein the non-difference comparison is at least one among a correlation, a cross-correlation and a comparison that uses coherence.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the step of detecting the spoken voice is performed if an absolute sound pressure level of the first sound or the second sound is above a predetermined threshold.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, further comprising performing a further non-difference comparison of the first sound measured by the first microphone in a first earpiece and a third sound measured by a third microphone in a second earpiece.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, further comprising performing a further non-difference comparison of the second sound measured by the second microphone in a first earpiece and an additional sound measured by an additional microphone in a second earpiece.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the first sound is received from the first microphone in an earpiece and the second sound is received from the second microphone in a remote device.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the remote device includes at least one of a further earpiece, a cell phone, a media player, a portable computing device or a personal digital assistant.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref> wherein the earpiece and the further earpiece are configured to be worn by a same individual.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref> wherein the earpiece and the further earpiece are configured to be worn by different individuals. </claim-text>
</claim>
</claims>
</us-patent-grant>
