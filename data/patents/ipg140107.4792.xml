<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625885-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625885</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13036982</doc-number>
<date>20110228</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<rule-47-flag/>
<us-term-of-grant>
<us-term-extension>194</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>62</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382156</main-classification>
<further-classification>382155</further-classification>
<further-classification>382157</further-classification>
<further-classification>382158</further-classification>
<further-classification>382159</further-classification>
</classification-national>
<invention-title id="d2e55">Methods and systems for data analysis and feature recognition</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5267328</doc-number>
<kind>A</kind>
<name>Gouge</name>
<date>19931100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5319719</doc-number>
<kind>A</kind>
<name>Nakazawa et al.</name>
<date>19940600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5826261</doc-number>
<kind>A</kind>
<name>Spencer</name>
<date>19981000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5892838</doc-number>
<kind>A</kind>
<name>Brady</name>
<date>19990400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6067371</doc-number>
<kind>A</kind>
<name>Gouge et al.</name>
<date>20000500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6122396</doc-number>
<kind>A</kind>
<name>King et al.</name>
<date>20000900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6373984</doc-number>
<kind>B1</kind>
<name>Gouge et al.</name>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6396939</doc-number>
<kind>B1</kind>
<name>Hu et al.</name>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6463163</doc-number>
<kind>B1</kind>
<name>Kresch</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6574378</doc-number>
<kind>B1</kind>
<name>Lim</name>
<date>20030600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6601059</doc-number>
<kind>B1</kind>
<name>Fries</name>
<date>20030700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6985612</doc-number>
<kind>B2</kind>
<name>Hahn</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>7092548</doc-number>
<kind>B2</kind>
<name>Laumeyer et al.</name>
<date>20060800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>7162076</doc-number>
<kind>B2</kind>
<name>Liu</name>
<date>20070100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>7203360</doc-number>
<kind>B2</kind>
<name>Lee et al.</name>
<date>20070400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>7295700</doc-number>
<kind>B2</kind>
<name>Schiller et al.</name>
<date>20071100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>7362892</doc-number>
<kind>B2</kind>
<name>Lewis et al.</name>
<date>20080400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>7751602</doc-number>
<kind>B2</kind>
<name>Collins et al.</name>
<date>20100700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>2002/0168657</doc-number>
<kind>A1</kind>
<name>Chen et al.</name>
<date>20021100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>2002/0176113</doc-number>
<kind>A1</kind>
<name>Edgar</name>
<date>20021100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>2004/0219517</doc-number>
<kind>A1</kind>
<name>Ecker et al.</name>
<date>20041100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>2004/0240712</doc-number>
<kind>A1</kind>
<name>Rowe et al.</name>
<date>20041200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382124</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>2005/0025355</doc-number>
<kind>A1</kind>
<name>Simard et al.</name>
<date>20050200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>2005/0049498</doc-number>
<kind>A1</kind>
<name>Roche et al.</name>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>2006/0024669</doc-number>
<kind>A1</kind>
<name>Bogoch et al.</name>
<date>20060200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>2006/0045512</doc-number>
<kind>A1</kind>
<name>Imamura et al.</name>
<date>20060300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>2006/0285010</doc-number>
<kind>A1</kind>
<name>Wang et al.</name>
<date>20061200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>2007/0047785</doc-number>
<kind>A1</kind>
<name>Jang et al.</name>
<date>20070300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>2007/0081743</doc-number>
<kind>A1</kind>
<name>Kim</name>
<date>20070400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>2007/0140540</doc-number>
<kind>A1</kind>
<name>McLaren et al.</name>
<date>20070600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00031">
<othercit>Reddick et al., &#x201c;Automated segmentation and classification of multispectral magnetic resonance images of brain using artificial neural networks&#x201d;, IEEE Transactions on Medical Imaging, vol. 16, No. 6, pp. 911-917, 1997.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00032">
<othercit>Keller et al., &#x201c;Pulse-coupled neural networks for medical image analysis&#x201d;, Proc. SPIE 3722, Applications and Science of Computational Intelligence II, 444, 1999.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00033">
<othercit>&#x201c;Artificial Neural Network,&#x201d; Wikipedia, http:/en.wikipedia.org/wiki/artificial<sub>&#x2014;</sub>neural<sub>&#x2014;</sub>network; pp. 1-10, Aug. 2005.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00034">
<othercit>Stergiou et al., &#x201c;Neural Networks,&#x201d; http://www.doc.ic.ac.uk/&#x2dc;nd/surprise<sub>&#x2014;</sub>96/journal/wol14/cs11/report.html, pp. 1-29, Aug. 2005.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00035">
<othercit>Rowe et al., &#x201c;Detection of Antibody to Avian Influenza A (H5N1) Virus in Human Serum by Using a Combination of Serologic Assays,&#x201d; Journal of Clinical Microbiology, Apr. 1999 p. 937-943.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00036">
<othercit>Sung et al., &#x201c;Example-Based Learning for View-Based Human Face Detection&#x201d;, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 20, No. 1, Jan. 1998.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>18</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382155-160</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>90</number-of-drawing-sheets>
<number-of-figures>104</number-of-figures>
</figures>
<us-related-documents>
<continuation-in-part>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>12597096</doc-number>
<date>20090925</date>
</document-id>
<parent-status>ABANDONED</parent-status>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13036982</doc-number>
</document-id>
</child-doc>
</relation>
</continuation-in-part>
<continuation-in-part>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>11689361</doc-number>
<date>20070321</date>
</document-id>
<parent-status>ABANDONED</parent-status>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>12597096</doc-number>
</document-id>
</child-doc>
</relation>
</continuation-in-part>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60743711</doc-number>
<date>20060323</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120141021</doc-number>
<kind>A1</kind>
<date>20120607</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Brinson, Jr.</last-name>
<first-name>Robert M.</first-name>
<address>
<city>Rome</city>
<state>GA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Middleton</last-name>
<first-name>Nicholas Levi</first-name>
<address>
<city>Highlands Ranch</city>
<state>CO</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Donaldson</last-name>
<first-name>Bryan Glenn</first-name>
<address>
<city>Cumming</city>
<state>GA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Brinson, Jr.</last-name>
<first-name>Robert M.</first-name>
<address>
<city>Rome</city>
<state>GA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Middleton</last-name>
<first-name>Nicholas Levi</first-name>
<address>
<city>Highlands Ranch</city>
<state>CO</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Donaldson</last-name>
<first-name>Bryan Glenn</first-name>
<address>
<city>Cumming</city>
<state>GA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Lowe Graham Jones PLLC</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Intelliscience Corporation</orgname>
<role>02</role>
<address>
<city>Atlanta</city>
<state>GA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Bhatnagar</last-name>
<first-name>Anand</first-name>
<department>2668</department>
</primary-examiner>
<assistant-examiner>
<last-name>Park</last-name>
<first-name>Soo</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Systems and methods for automated pattern recognition and object detection. The method can be rapidly developed and improved using a minimal number of algorithms for the data content to fully discriminate details in the data, while reducing the need for human analysis. The system includes a data analysis system that recognizes patterns and detects objects in data without requiring adaptation of the system to a particular application, environment, or data content. The system evaluates the data in its native form independent of the form of presentation or the form of the post-processed data.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="151.98mm" wi="234.02mm" file="US08625885-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="184.57mm" wi="172.38mm" file="US08625885-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="167.89mm" wi="170.43mm" file="US08625885-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="140.38mm" wi="97.37mm" file="US08625885-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="159.60mm" wi="94.23mm" file="US08625885-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="144.86mm" wi="78.82mm" file="US08625885-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="231.31mm" wi="182.63mm" file="US08625885-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="214.04mm" wi="132.67mm" file="US08625885-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="158.75mm" wi="71.54mm" file="US08625885-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="215.31mm" wi="96.77mm" file="US08625885-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="236.47mm" wi="154.43mm" file="US08625885-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="220.98mm" wi="158.75mm" file="US08625885-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="229.45mm" wi="170.43mm" file="US08625885-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="190.16mm" wi="158.75mm" file="US08625885-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="222.00mm" wi="158.75mm" file="US08625885-20140107-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="188.64mm" wi="158.75mm" file="US08625885-20140107-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00016" num="00016">
<img id="EMI-D00016" he="222.00mm" wi="158.75mm" file="US08625885-20140107-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00017" num="00017">
<img id="EMI-D00017" he="208.45mm" wi="158.75mm" file="US08625885-20140107-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00018" num="00018">
<img id="EMI-D00018" he="148.51mm" wi="158.75mm" file="US08625885-20140107-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00019" num="00019">
<img id="EMI-D00019" he="125.73mm" wi="158.75mm" file="US08625885-20140107-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00020" num="00020">
<img id="EMI-D00020" he="158.75mm" wi="111.84mm" file="US08625885-20140107-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00021" num="00021">
<img id="EMI-D00021" he="181.36mm" wi="158.75mm" file="US08625885-20140107-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00022" num="00022">
<img id="EMI-D00022" he="202.10mm" wi="158.75mm" file="US08625885-20140107-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00023" num="00023">
<img id="EMI-D00023" he="133.27mm" wi="134.54mm" file="US08625885-20140107-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00024" num="00024">
<img id="EMI-D00024" he="72.39mm" wi="77.55mm" file="US08625885-20140107-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00025" num="00025">
<img id="EMI-D00025" he="126.24mm" wi="130.05mm" file="US08625885-20140107-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00026" num="00026">
<img id="EMI-D00026" he="122.43mm" wi="139.02mm" file="US08625885-20140107-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00027" num="00027">
<img id="EMI-D00027" he="132.00mm" wi="123.02mm" file="US08625885-20140107-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00028" num="00028">
<img id="EMI-D00028" he="124.97mm" wi="116.67mm" file="US08625885-20140107-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00029" num="00029">
<img id="EMI-D00029" he="151.21mm" wi="86.53mm" file="US08625885-20140107-D00029.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00030" num="00030">
<img id="EMI-D00030" he="151.21mm" wi="83.99mm" file="US08625885-20140107-D00030.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00031" num="00031">
<img id="EMI-D00031" he="232.58mm" wi="171.11mm" orientation="landscape" file="US08625885-20140107-D00031.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00032" num="00032">
<img id="EMI-D00032" he="244.77mm" wi="174.33mm" orientation="landscape" file="US08625885-20140107-D00032.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00033" num="00033">
<img id="EMI-D00033" he="149.35mm" wi="102.53mm" file="US08625885-20140107-D00033.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00034" num="00034">
<img id="EMI-D00034" he="102.53mm" wi="97.37mm" file="US08625885-20140107-D00034.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00035" num="00035">
<img id="EMI-D00035" he="101.85mm" wi="93.56mm" file="US08625885-20140107-D00035.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00036" num="00036">
<img id="EMI-D00036" he="94.23mm" wi="90.34mm" file="US08625885-20140107-D00036.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00037" num="00037">
<img id="EMI-D00037" he="92.29mm" wi="89.75mm" file="US08625885-20140107-D00037.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00038" num="00038">
<img id="EMI-D00038" he="93.56mm" wi="73.66mm" file="US08625885-20140107-D00038.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00039" num="00039">
<img id="EMI-D00039" he="242.91mm" wi="182.03mm" orientation="landscape" file="US08625885-20140107-D00039.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00040" num="00040">
<img id="EMI-D00040" he="137.16mm" wi="133.94mm" file="US08625885-20140107-D00040.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00041" num="00041">
<img id="EMI-D00041" he="139.70mm" wi="128.19mm" file="US08625885-20140107-D00041.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00042" num="00042">
<img id="EMI-D00042" he="133.27mm" wi="115.32mm" file="US08625885-20140107-D00042.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00043" num="00043">
<img id="EMI-D00043" he="130.73mm" wi="109.56mm" file="US08625885-20140107-D00043.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00044" num="00044">
<img id="EMI-D00044" he="124.97mm" wi="124.97mm" file="US08625885-20140107-D00044.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00045" num="00045">
<img id="EMI-D00045" he="129.46mm" wi="114.72mm" file="US08625885-20140107-D00045.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00046" num="00046">
<img id="EMI-D00046" he="128.19mm" wi="123.02mm" file="US08625885-20140107-D00046.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00047" num="00047">
<img id="EMI-D00047" he="131.40mm" wi="117.94mm" file="US08625885-20140107-D00047.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00048" num="00048">
<img id="EMI-D00048" he="126.92mm" wi="112.78mm" file="US08625885-20140107-D00048.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00049" num="00049">
<img id="EMI-D00049" he="128.78mm" wi="123.70mm" file="US08625885-20140107-D00049.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00050" num="00050">
<img id="EMI-D00050" he="199.90mm" wi="100.58mm" file="US08625885-20140107-D00050.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00051" num="00051">
<img id="EMI-D00051" he="99.99mm" wi="90.34mm" file="US08625885-20140107-D00051.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00052" num="00052">
<img id="EMI-D00052" he="93.56mm" wi="106.34mm" file="US08625885-20140107-D00052.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00053" num="00053">
<img id="EMI-D00053" he="99.99mm" wi="86.53mm" file="US08625885-20140107-D00053.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00054" num="00054">
<img id="EMI-D00054" he="95.50mm" wi="81.36mm" file="US08625885-20140107-D00054.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00055" num="00055">
<img id="EMI-D00055" he="90.34mm" wi="84.58mm" file="US08625885-20140107-D00055.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00056" num="00056">
<img id="EMI-D00056" he="99.31mm" wi="90.34mm" file="US08625885-20140107-D00056.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00057" num="00057">
<img id="EMI-D00057" he="94.83mm" wi="88.39mm" file="US08625885-20140107-D00057.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00058" num="00058">
<img id="EMI-D00058" he="91.02mm" wi="91.61mm" file="US08625885-20140107-D00058.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00059" num="00059">
<img id="EMI-D00059" he="85.85mm" wi="74.34mm" file="US08625885-20140107-D00059.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00060" num="00060">
<img id="EMI-D00060" he="94.23mm" wi="81.36mm" file="US08625885-20140107-D00060.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00061" num="00061">
<img id="EMI-D00061" he="202.52mm" wi="93.56mm" file="US08625885-20140107-D00061.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00062" num="00062">
<img id="EMI-D00062" he="239.01mm" wi="172.38mm" file="US08625885-20140107-D00062.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00063" num="00063">
<img id="EMI-D00063" he="233.93mm" wi="169.84mm" file="US08625885-20140107-D00063.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00064" num="00064">
<img id="EMI-D00064" he="229.45mm" wi="178.14mm" file="US08625885-20140107-D00064.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00065" num="00065">
<img id="EMI-D00065" he="125.56mm" wi="165.35mm" file="US08625885-20140107-D00065.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00066" num="00066">
<img id="EMI-D00066" he="191.01mm" wi="153.84mm" orientation="landscape" file="US08625885-20140107-D00066.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00067" num="00067">
<img id="EMI-D00067" he="193.55mm" wi="159.60mm" orientation="landscape" file="US08625885-20140107-D00067.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00068" num="00068">
<img id="EMI-D00068" he="242.23mm" wi="167.22mm" file="US08625885-20140107-D00068.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00069" num="00069">
<img id="EMI-D00069" he="235.80mm" wi="161.46mm" file="US08625885-20140107-D00069.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00070" num="00070">
<img id="EMI-D00070" he="228.77mm" wi="173.06mm" file="US08625885-20140107-D00070.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00071" num="00071">
<img id="EMI-D00071" he="129.46mm" wi="178.82mm" file="US08625885-20140107-D00071.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00072" num="00072">
<img id="EMI-D00072" he="231.31mm" wi="172.38mm" orientation="landscape" file="US08625885-20140107-D00072.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00073" num="00073">
<img id="EMI-D00073" he="237.74mm" wi="176.19mm" file="US08625885-20140107-D00073.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00074" num="00074">
<img id="EMI-D00074" he="234.53mm" wi="168.57mm" file="US08625885-20140107-D00074.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00075" num="00075">
<img id="EMI-D00075" he="126.92mm" wi="168.57mm" file="US08625885-20140107-D00075.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00076" num="00076">
<img id="EMI-D00076" he="243.50mm" wi="174.92mm" orientation="landscape" file="US08625885-20140107-D00076.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00077" num="00077">
<img id="EMI-D00077" he="237.07mm" wi="167.22mm" file="US08625885-20140107-D00077.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00078" num="00078">
<img id="EMI-D00078" he="235.20mm" wi="161.46mm" file="US08625885-20140107-D00078.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00079" num="00079">
<img id="EMI-D00079" he="233.26mm" wi="167.89mm" file="US08625885-20140107-D00079.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00080" num="00080">
<img id="EMI-D00080" he="237.07mm" wi="182.03mm" file="US08625885-20140107-D00080.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00081" num="00081">
<img id="EMI-D00081" he="237.07mm" wi="184.57mm" orientation="landscape" file="US08625885-20140107-D00081.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00082" num="00082">
<img id="EMI-D00082" he="233.93mm" wi="177.55mm" orientation="landscape" file="US08625885-20140107-D00082.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00083" num="00083">
<img id="EMI-D00083" he="126.92mm" wi="178.14mm" file="US08625885-20140107-D00083.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00084" num="00084">
<img id="EMI-D00084" he="233.93mm" wi="175.60mm" orientation="landscape" file="US08625885-20140107-D00084.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00085" num="00085">
<img id="EMI-D00085" he="235.80mm" wi="180.68mm" orientation="landscape" file="US08625885-20140107-D00085.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00086" num="00086">
<img id="EMI-D00086" he="233.26mm" wi="185.84mm" orientation="landscape" file="US08625885-20140107-D00086.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00087" num="00087">
<img id="EMI-D00087" he="233.26mm" wi="174.33mm" file="US08625885-20140107-D00087.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00088" num="00088">
<img id="EMI-D00088" he="230.04mm" wi="172.38mm" file="US08625885-20140107-D00088.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00089" num="00089">
<img id="EMI-D00089" he="235.20mm" wi="183.90mm" orientation="landscape" file="US08625885-20140107-D00089.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00090" num="00090">
<img id="EMI-D00090" he="239.01mm" wi="178.14mm" orientation="landscape" file="US08625885-20140107-D00090.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">PRIORITY CLAIM</heading>
<p id="p-0002" num="0001">This application is a continuation-in-part of U.S. patent application Ser. No. 12/567,096 as filed on Aug. 25, 2009, and U.S. patent application Ser. No. 11/689,361 as filed on Mar. 21, 2007, both of which claim priority to U.S. Provisional Patent Application 60/743,711 as filed on Mar. 23, 2006. These patent applications are incorporated herein by reference.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">FIELD OF THE INVENTION</heading>
<p id="p-0003" num="0002">The present invention, in various embodiments, relates generally to the field of data analysis and more particularly to pattern and object recognition within multispectral and hyperspectral digital data sets.</p>
<heading id="h-0003" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0004" num="0003">With the increasing use of computers and computerized technology, the amount of information represented digitally has become enormous. Analysis of these vast quantities of digital data generally involves the recognition of known patterns.</p>
<p id="p-0005" num="0004">Traditionally, information originating in a digital format is ultimately analyzed through manual review by a person who often requires substantial training. In order for people to efficiently interact with volumes of digital data, the information must typically be converted into a visual, audible, or other human-perceivable representation. However, during the process of translating digital data from its raw form into a convenient output format, some information may be lost or misinterpreted. Moreover, the data is often processed and/or filtered for presentation prior to analysis thereby resulting in the loss of significant information from the original data. While humans can be trained to analyze many different types of data, manual human analysis is generally more expensive with regard to time and accuracy than automated systems. Additionally, errors are often introduced due to the inherent limitations of human perception and attention span. Frequently, the data contains more detail than human senses can discern, and it is documented that human repetition begets errors.</p>
<p id="p-0006" num="0005">To address the innate shortcomings of human analysis, many automated data analysis and pattern recognition systems have been developed and subsequently improved upon. However, most of these solutions are highly data-specific and/or processing intensive. The data inputs that a pattern recognition system can handle are often fixed and limited by design such that applicability is restricted to a specific data modality; otherwise stated, the system by which the data is evaluated is tightly coupled to the specific data source it is designed to evaluate. Hence, improvements across a broad range of systems are very difficult.</p>
<p id="p-0007" num="0006">Furthermore, within many systems, pattern and feature recognition is processing-intensive. For example, image analysis commonly uses complex algorithms to find geometric shapes, edges, etc., for the purpose of characterizing or classifying features of interest; this requires multitudes of algorithms to be processed. The time to discover, develop, and implement each algorithm causes an incremental delay in deploying or improving the system.</p>
<p id="p-0008" num="0007">Thus, there still remains substantial room for improvement in the arena of automated data analysis and pattern recognition systems.</p>
<heading id="h-0004" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0009" num="0008">In a preferred embodiment of the present invention, the data analysis and feature recognition system as described herein provides an automated pattern recognition and object detection system that can be rapidly developed and improved upon using a relatively minimal number of simple evaluation algorithms that function to capture primary or more fundamental relationships between data elements in order to fully discriminate features and objects within the data allthewhile reducing the need for human analysis. Advantageously, this limited set of algorithms can be implemented quickly in a specific data modality or across multiple modalities. Hence, the system as described herein is designed so as not to be limited by a specific data modality, submodality, etc., or by the incomplete knowledge of its developers.</p>
<p id="p-0010" num="0009">In one aspect of the present invention, the system recognizes patterns and detects objects within data without requiring adaptation of said system to a particular application, environment, or data content. The system evaluates the data in its native form independent of the form of presentation or the form of the post-processed data.</p>
<p id="p-0011" num="0010">In one aspect of the present invention, the system analyzes data from any and all multispectral and hyperspectral data modalities within all data types. Example data modalities include multi-band imagery, acoustics, and as yet undiscovered modalities. Within multispectral and hyperspectral imagery, there exists still and moving images with applications in the fields of medicine, homeland security, natural resources, geology, agriculture, food sciences, meteorology, space, military, digital rights management, and others. Within acoustic, there exists single and multi-channel audio sound, ultrasound-continuous stream, seismic, and SONAR with applications in the fields of medicine, homeland security, military, natural resources, geology, space, digital rights management, and others. Examples of other digital data streams include radar, scent, tactile, financial market and statistical data, mechanical pressure, environmental data, taste, harmonics, chemical analysis, electrical impulses, text, and others. Some data modalities are combinations of other modalities, such as video with sound or multiple forms of a single modality such as where multiple images of different types are taken of the same sample, for example correlated MRI and CT imaging or combined SAR, photograph, and IR imagery. Improvements made in the common system benefit all modalities.</p>
<p id="p-0012" num="0011">In still other aspects of the present invention, the system provides an automated functionality that operates on the full resolution of the native data. The results are produced in a timely manner thereby alleviating the tedium of preliminary human analysis. Moreover, in one embodiment the system can be programmed to automatically alert the operator or user to examine a data set(s) requiring closer attention.</p>
<p id="p-0013" num="0012">In additional aspects of the present invention, the method includes receiving a first multispectral or hyperspectral data set; selecting and analyzing individual bands of the data set(s) using a series of evaluation algorithms; generating an algorithm value cache of the results set of the first algorithmic training of the first data set; receiving a second multispectral or hyperspectral data set containing data bands of the same type as the first; processing the second data set using the same series of evaluation algorithms as is used to evaluate the first data set; generating a second algorithm value cache of the results set of the algorithmic processing on the second data set; comparing the algorithm value cache from the training results of the first data set with the algorithm value cache from the processing results of the second data set; and performing a processing action based upon the generated match result.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0014" num="0013">The preferred and alternative embodiments of the present invention are described in detail herein with reference to the following drawings.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 1</figref> shows an overview of one embodiment of an example data analysis and feature recognition system;</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 2</figref> shows one embodiment of an example system for executing data analysis and feature recognition;</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 3</figref> shows an example method for using a data analysis and feature recognition system;</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 4</figref> shows an example method for creating a datastore;</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 5</figref> shows an example method for creating a known feature;</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 6</figref> shows an example method for modifying a synaptic web by training or untraining a known feature;</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 7</figref> shows an example method for generating an algorithm value cache;</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 8</figref> shows an example method for training a known feature;</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 9</figref> shows an example method for creating a collection of synaptic training paths from positive and negative training value sets;</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 10</figref> shows an example method for removing negative training value sets from the collection of training paths;</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 11</figref> shows an example method for creating and following a synaptic path from a training synaptic path array;</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 12</figref> shows an example method for associating a synaptic path with a known feature;</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 13</figref> shows an example method for untraining a known feature;</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 14</figref> shows an example method for using a set of algorithm values to retrieve a synaptic leaf in the synaptic web;</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 15</figref> shows an example method for dissociating a synaptic leaf from a known feature;</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 16</figref> shows an example method for identifying a known feature(s);</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 17</figref> shows an example method for determining if a known feature(s) is identified within the data set or selection therein;</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 18</figref> shows an example method for evaluating threshold detection;</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 19</figref> shows an example method for evaluating cluster detection;</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 20</figref> shows an example method for evaluating threshold and cluster detection;</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 21</figref> shows an example method for processing the known feature(s) identified for a given data set or selection therein;</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 22</figref> shows an example method for performing the known feature action(s)-on-detection;</p>
<p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. 23</figref> shows an example ten-data element by ten-data element data array of a grey-scale image;</p>
<p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. 24</figref> shows the adjacent data element TDA;</p>
<p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. 25</figref> shows an example ten-data element by ten-data element data array containing the resultant algorithm values after evaluation of the original data array of <figref idref="DRAWINGS">FIG. 23</figref> with the mean algorithm using the adjacent data element TDA;</p>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. 26</figref> shows an example ten-data element by ten-data element data array containing the resultant algorithm values after evaluation of the original data array of <figref idref="DRAWINGS">FIG. 23</figref> with the median algorithm using the adjacent data element TDA;</p>
<p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. 27</figref> shows an example ten-data element by ten-data element data array containing the resultant algorithm values after evaluation of the original data array of <figref idref="DRAWINGS">FIG. 23</figref> with the spread of values algorithm using the adjacent data element TDA;</p>
<p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. 28</figref> shows an example ten-data element by ten-data element data array containing the resultant algorithm values after evaluation of the original data array of <figref idref="DRAWINGS">FIG. 23</figref> with the standard deviation algorithm using the adjacent data element TDA;</p>
<p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. 29</figref> shows an example synaptic web containing a single synaptic path built from the algorithmic evaluation of TDE (2, 2) of the <figref idref="DRAWINGS">FIG. 23</figref> data array with the mean, median, spread of values, and standard deviation algorithms using the adjacent data element TDA;</p>
<p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. 30</figref> shows an example synaptic web containing two synaptic paths built from the algorithmic evaluation of TDE (2, 2) and TDE (3, 2) of the <figref idref="DRAWINGS">FIG. 23</figref> data array with the mean, median, spread of values, and standard deviation algorithms using the adjacent data element TDA;</p>
<p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. 31</figref> shows an example synaptic web containing eleven synaptic paths built from the algorithm evaluation of TDEs (2, 2) through (9, 2) and TDEs (2, 3) through (4, 3) of the <figref idref="DRAWINGS">FIG. 23</figref> data array with the mean, median, spread of values, and standard deviation algorithms using the adjacent data element TDA;</p>
<p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. 32</figref> shows an example synaptic web containing sixteen synaptic paths built from the algorithmic evaluation of TDEs (2, 2) through (9, 2) and TDEs (2, 3) through (9, 3) of the <figref idref="DRAWINGS">FIG. 23</figref> data array with the mean, median, spread of values, and standard deviation algorithms using the adjacent data element TDA;</p>
<p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. 33</figref> shows an example synaptic web containing one synaptic path that terminates in a synaptic leaf having multiple known features;</p>
<p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. 34</figref> shows an example six-data element by six-data element data array of a grey-scale image;</p>
<p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. 35</figref> shows an example six-data element by six-data element data array containing the resultant algorithm values after evaluation of the original data array of <figref idref="DRAWINGS">FIG. 34</figref> with the mean algorithm using the adjacent data element TDA;</p>
<p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. 36</figref> shows an example six-data element by six-data element data array containing the resultant algorithm values after evaluation of the original data array of <figref idref="DRAWINGS">FIG. 34</figref> with the median algorithm using the adjacent data element TDA;</p>
<p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. 37</figref> shows an example six-data element by six-data element data array containing the resultant algorithm values after evaluation of the original data array of <figref idref="DRAWINGS">FIG. 34</figref> with the spread of values algorithm using the adjacent data element TDA;</p>
<p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. 38</figref> shows an example six-data element by six-data element data array containing the resultant algorithm values after evaluation of the original data array of <figref idref="DRAWINGS">FIG. 34</figref> with the standard deviation algorithm using the adjacent data element TDA;</p>
<p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. 39</figref> shows an example synaptic web wherein the algorithm processing results of the first valid data element of <figref idref="DRAWINGS">FIG. 34</figref> are compared to the values of the existing partial synaptic web trained from the algorithmic evaluation of the <figref idref="DRAWINGS">FIG. 23</figref> data array in an effort to identify a known feature(s) within the data array of <figref idref="DRAWINGS">FIG. 34</figref>;</p>
<p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. 40</figref> shows an example ten-data element by ten-data element data array selection of Band 13 as sourced from an AVIRIS hyperspectral data set;</p>
<p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. 41</figref> shows an example ten-data element by ten-data element data array selection of Band 20 as sourced from an AVIRIS hyperspectral data set;</p>
<p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. 42</figref> shows an example ten-data element by ten-data element data array selection of Band 173 as sourced from an AVIRIS hyperspectral data set;</p>
<p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. 43</figref> shows an example ten-data element by ten-data element data array selection of Band 200 as sourced from an AVIRIS hyperspectral data set;</p>
<p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. 44</figref> shows an example ten-data element by ten-data element data array selection of Band 13 containing the resultant algorithm values after evaluation of the original data array of <figref idref="DRAWINGS">FIG. 40</figref> with the mean algorithm using the adjacent data element TDA;</p>
<p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. 45</figref> shows an example ten-data element by ten-data element data array selection of Band 13 containing the resultant algorithm values after evaluation of the original data array of <figref idref="DRAWINGS">FIG. 40</figref> with the spread of values algorithm using the adjacent data element TDA;</p>
<p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. 46</figref> shows an example ten-data element by ten-data element data array selection of Band 20 containing the resultant algorithm values after evaluation of the original data array of <figref idref="DRAWINGS">FIG. 41</figref> with the mean algorithm using the adjacent data element TDA;</p>
<p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. 47</figref> shows an example ten-data element by ten-data element data array selection of Band 20 containing the resultant algorithm values after evaluation of the original data array of <figref idref="DRAWINGS">FIG. 41</figref> with the spread of values algorithm using the adjacent data element TDA;</p>
<p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. 48</figref> shows an example ten-data element by ten-data element data array selection of Band 173 containing the resultant algorithm values after evaluation of the original data array of <figref idref="DRAWINGS">FIG. 42</figref> with the value algorithm using the adjacent data element TDA;</p>
<p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. 49</figref> shows an example ten-data element by ten-data element data array selection of Band 200 containing the resultant algorithm values after evaluation of the original data array of <figref idref="DRAWINGS">FIG. 43</figref> with the value algorithm masked with 0xFFFC using the adjacent data element TDA;</p>
<p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. 50</figref> shows an example synaptic web containing two synaptic paths built from the algorithmic evaluation of TDE (2, 2) and TDE (3, 2) of the <figref idref="DRAWINGS">FIGS. 40-43</figref> data arrays, which represent data selections within Bands 13, 20, 173, and 200, respectively, of the AVIRIS hyperspectral data set, with the mean, spread of values, value, and value masked with 0xFFFC algorithms using the adjacent data element TDA;</p>
<p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. 51</figref> shows an example six-data element by six-data element data array selection of Band 13 as sourced from an AVIRIS hyperspectral data set;</p>
<p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. 52</figref> shows an example six-data element by six-data element data array selection of Band 20 as sourced from an AVIRIS hyperspectral data set;</p>
<p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. 53</figref> shows an example six-data element by six-data element data array selection of Band 173 as sourced from an AVIRIS hyperspectral data set;</p>
<p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. 54</figref> shows an example six-data element by six-data element data array selection of Band 200 as sourced from an AVIRIS hyperspectral data set;</p>
<p id="p-0069" num="0068"><figref idref="DRAWINGS">FIG. 55</figref> shows an example six-data element by six-data element data array selection of Band 13 containing the resultant algorithm values after evaluation of the original data array of <figref idref="DRAWINGS">FIG. 51</figref> with the mean algorithm using the adjacent data element TDA;</p>
<p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. 56</figref> shows an example six-data element by six-data element data array selection of Band 13 containing the resultant algorithm values after evaluation of the original data array of <figref idref="DRAWINGS">FIG. 51</figref> with the spread of values algorithm using the adjacent data element TDA;</p>
<p id="p-0071" num="0070"><figref idref="DRAWINGS">FIG. 57</figref> shows an example six-data element by six-data element data array selection of Band 20 containing the resultant algorithm values after evaluation of the original data array of <figref idref="DRAWINGS">FIG. 52</figref> with the mean algorithm using the adjacent data element TDA;</p>
<p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. 58</figref> shows an example six-data element by six-data element data array selection of Band 20 containing the resultant algorithm values after evaluation of the original data array of <figref idref="DRAWINGS">FIG. 52</figref> with the spread of values algorithm using the adjacent data element TDA;</p>
<p id="p-0073" num="0072"><figref idref="DRAWINGS">FIG. 59</figref> shows an example six-data element by six-data element data array selection of Band 173 containing the resultant algorithm values after evaluation of the original data array of <figref idref="DRAWINGS">FIG. 53</figref> with the value algorithm using the adjacent data element TDA;</p>
<p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. 60</figref> shows an example six-data element by six-data element data array selection of Band 200 containing the resultant algorithm values after evaluation of the original data array of <figref idref="DRAWINGS">FIG. 54</figref> with the value algorithm masked with 0xFFFC using the adjacent data element TDA;</p>
<p id="p-0075" num="0074"><figref idref="DRAWINGS">FIG. 61</figref> shows an example synaptic web wherein the algorithm processing results of the first valid data element (2, 2) of <figref idref="DRAWINGS">FIGS. 51-54</figref> are compared to the values of the existing partial synaptic web trained from the algorithmic evaluation of the <figref idref="DRAWINGS">FIGS. 40-43</figref> data arrays in an effort to identify a known feature(s) within the data arrays of <figref idref="DRAWINGS">FIGS. 51-54</figref>;</p>
<p id="p-0076" num="0075"><figref idref="DRAWINGS">FIG. 62</figref> shows a screenshot of the &#x201c;Start&#x201d; tab or introduction screen of the &#x201c;New SyntelliBase Wizard&#x201d;;</p>
<p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. 63</figref> shows a screenshot of the &#x201c;Required&#x201d; tab of the &#x201c;New SyntelliBase Wizard&#x201d;;</p>
<p id="p-0078" num="0077"><figref idref="DRAWINGS">FIG. 64</figref> shows a screenshot of the expanded &#x201c;Submodality&#x201d; combination box of the &#x201c;Required&#x201d; tab of the &#x201c;New SyntelliBase Wizard&#x201d;;</p>
<p id="p-0079" num="0078"><figref idref="DRAWINGS">FIG. 65</figref> shows a screenshot of the &#x201c;Optional&#x201d; tab of the &#x201c;New SyntelliBase Wizard&#x201d;;</p>
<p id="p-0080" num="0079"><figref idref="DRAWINGS">FIG. 66</figref> shows a screenshot of the &#x201c;Target Data Shape&#x201d; tab of the &#x201c;New SyntelliBase Wizard&#x201d;;</p>
<p id="p-0081" num="0080"><figref idref="DRAWINGS">FIG. 67</figref> shows a screenshot of the &#x201c;Summary&#x201d; tab of the &#x201c;New SyntelliBase Wizard&#x201d;;</p>
<p id="p-0082" num="0081"><figref idref="DRAWINGS">FIG. 68</figref> shows a screenshot of the &#x201c;Summary&#x201d; tab of the &#x201c;New SyntelliBase Wizard;</p>
<p id="p-0083" num="0082"><figref idref="DRAWINGS">FIG. 69</figref> shows a screenshot of one embodiment of a user-interface for a data analysis and feature recognition system as it after datastore creation is complete;</p>
<p id="p-0084" num="0083"><figref idref="DRAWINGS">FIG. 70</figref> shows a screenshot of one embodiment of an application to accomplish data analysis and feature recognition wherein the &#x201c;Grey Adjacent Pixels&#x201d; TDA is expanded to show a listing of all available evaluation algorithms;</p>
<p id="p-0085" num="0084"><figref idref="DRAWINGS">FIG. 71</figref> shows a screenshot of the &#x201c;Start&#x201d; tab of the &#x201c;New Known Feature Wizard&#x201d;;</p>
<p id="p-0086" num="0085"><figref idref="DRAWINGS">FIG. 72</figref> shows a screenshot of the &#x201c;Identification&#x201d; tab of the &#x201c;New Known Feature Wizard&#x201d;;</p>
<p id="p-0087" num="0086"><figref idref="DRAWINGS">FIG. 73</figref> shows a screenshot of the &#x201c;Identification&#x201d; tab of the &#x201c;New Known Feature Wizard&#x201d; wherein the known feature &#x201c;Method&#x201d; of detection combination box is expanded;</p>
<p id="p-0088" num="0087"><figref idref="DRAWINGS">FIG. 74</figref> shows a screenshot of the &#x201c;Training Counts&#x201d; tab of the &#x201c;New Known Feature Wizard&#x201d;;</p>
<p id="p-0089" num="0088"><figref idref="DRAWINGS">FIG. 75</figref> shows a screenshot of the &#x201c;Cluster Range&#x201d; tab of the &#x201c;New Known Feature Wizard&#x201d;;</p>
<p id="p-0090" num="0089"><figref idref="DRAWINGS">FIG. 76</figref> shows a screenshot of the &#x201c;Actions&#x201d; tab of the &#x201c;New Known Feature Wizard&#x201d;;</p>
<p id="p-0091" num="0090"><figref idref="DRAWINGS">FIG. 77</figref> shows a screenshot of the &#x201c;Summary&#x201d; tab of the &#x201c;New Known Feature Wizard&#x201d;;</p>
<p id="p-0092" num="0091"><figref idref="DRAWINGS">FIG. 78</figref> shows a screenshot of one embodiment of an application to accomplish data analysis and feature recognition wherein the workspace is loaded with a sample image &#x201c;forest.bmp&#x201d; containing a user-defined region of interest of the feature &#x201c;Forest&#x201d;;</p>
<p id="p-0093" num="0092"><figref idref="DRAWINGS">FIG. 79</figref> shows a screenshot of the &#x201c;Start&#x201d; tab or introduction screen of the &#x201c;Train Known Feature Wizard&#x201d;;</p>
<p id="p-0094" num="0093"><figref idref="DRAWINGS">FIG. 80</figref> shows a screenshot of the &#x201c;Known Features&#x201d; tab of the &#x201c;Train Known Feature Wizard&#x201d;;</p>
<p id="p-0095" num="0094"><figref idref="DRAWINGS">FIG. 81</figref> shows a screenshot of the &#x201c;Method&#x201d; tab of the &#x201c;Train Known Feature Wizard&#x201d;;</p>
<p id="p-0096" num="0095"><figref idref="DRAWINGS">FIG. 82</figref> shows a screenshot of the &#x201c;Summary&#x201d; tab of the &#x201c;Train Known Feature Wizard&#x201d;;</p>
<p id="p-0097" num="0096"><figref idref="DRAWINGS">FIG. 83</figref> shows a screenshot of the &#x201c;Results Summary&#x201d; dialog box, which displays the results of known feature training of the feature &#x201c;Forest&#x201d;;</p>
<p id="p-0098" num="0097"><figref idref="DRAWINGS">FIG. 84</figref> shows a screenshot of one embodiment of an application to accomplish data analysis and feature recognition wherein the workspace is loaded with the sample image &#x201c;island.bmp&#x201d; containing a user-defined region of interest of the feature &#x201c;Forest&#x201d;;</p>
<p id="p-0099" num="0098"><figref idref="DRAWINGS">FIG. 85</figref> shows a screenshot of the &#x201c;Results Summary&#x201d; dialog box, which displays the results of known feature training of the feature &#x201c;Forest&#x201d;;</p>
<p id="p-0100" num="0099"><figref idref="DRAWINGS">FIG. 86</figref> shows a screenshot of the &#x201c;Start&#x201d; tab or introduction screen of the &#x201c;Process Known Feature Wizard&#x201d;;</p>
<p id="p-0101" num="0100"><figref idref="DRAWINGS">FIG. 87</figref> shows a screenshot of the &#x201c;Known Features&#x201d; tab of the &#x201c;Process Known Feature Wizard&#x201d; wherein the known feature &#x201c;Forest&#x201d; is selected for processing;</p>
<p id="p-0102" num="0101"><figref idref="DRAWINGS">FIG. 88</figref> shows a screenshot of the &#x201c;Significance&#x201d; tab of the &#x201c;Process Known Feature Wizard&#x201d;;</p>
<p id="p-0103" num="0102"><figref idref="DRAWINGS">FIG. 89</figref> shows a screenshot of the &#x201c;Training Counts&#x201d; tab of the &#x201c;Process Known Feature Wizard&#x201d;;</p>
<p id="p-0104" num="0103"><figref idref="DRAWINGS">FIG. 90</figref> shows a screenshot of the &#x201c;Cluster Range&#x201d; tab of the &#x201c;Process Known Feature Wizard&#x201d;;</p>
<p id="p-0105" num="0104"><figref idref="DRAWINGS">FIG. 91</figref> shows a screenshot of the &#x201c;Summary&#x201d; tab of the &#x201c;Process Known Feature Wizard&#x201d;;</p>
<p id="p-0106" num="0105"><figref idref="DRAWINGS">FIG. 92</figref> shows a screenshot of the &#x201c;Results Summary&#x201d; dialog box displaying the results of known feature processing of the feature &#x201c;Forest&#x201d; within the sample image &#x201c;dam.bmp&#x201d;;</p>
<p id="p-0107" num="0106"><figref idref="DRAWINGS">FIG. 93</figref> shows a screenshot of one embodiment of an application to accomplish data analysis and feature recognition wherein the workspace is loaded with the sample image &#x201c;dam.bmp&#x201d; and only the processed image layer &#x201c;Forest,&#x201d; which represents the results of known feature processing, is selected for viewing;</p>
<p id="p-0108" num="0107"><figref idref="DRAWINGS">FIG. 94</figref> shows a screenshot of one embodiment of an application to accomplish data analysis and feature recognition wherein the workspace is loaded with the sample image &#x201c;dam.bmp,&#x201d; and the image layer &#x201c;Composite,&#x201d; which is comprised of the sample image &#x201c;dam.bmp&#x201d; layered with the processed image layer &#x201c;Forest,&#x201d; is selected for viewing;</p>
<p id="p-0109" num="0108"><figref idref="DRAWINGS">FIG. 95</figref> shows a screenshot of the &#x201c;Results Summary&#x201d; dialog box displaying the results of known feature processing of the feature &#x201c;Forest&#x201d; within the sample image &#x201c;island.bmp&#x201d;;</p>
<p id="p-0110" num="0109"><figref idref="DRAWINGS">FIG. 96</figref> shows a screenshot of one embodiment of an application to accomplish data analysis and feature recognition wherein the workspace is loaded with the sample image &#x201c;island.bmp&#x201d; and only the processed image layer &#x201c;Forest,&#x201d; which represents the results of known feature processing, is available for viewing;</p>
<p id="p-0111" num="0110"><figref idref="DRAWINGS">FIG. 97</figref> shows a screenshot of one embodiment of an application to accomplish data analysis and feature recognition wherein the workspace is loaded with the sample image &#x201c;island.bmp,&#x201d; and the image layer &#x201c;Composite,&#x201d; which is comprised of the sample image &#x201c;island.bmp&#x201d; layered with the processed image layer &#x201c;Forest,&#x201d; is selected for viewing;</p>
<p id="p-0112" num="0111"><figref idref="DRAWINGS">FIG. 98</figref> shows a screenshot of one embodiment of an application to accomplish data analysis and feature recognition wherein the workspace is loaded with the sample image &#x201c;island.bmp&#x201d; containing a user-defined region of interest of the feature &#x201c;Water&#x201d;;</p>
<p id="p-0113" num="0112"><figref idref="DRAWINGS">FIG. 99</figref> shows a screenshot of the &#x201c;Results Summary&#x201d; dialog box, which displays the results of known feature training of the feature &#x201c;Water&#x201d;;</p>
<p id="p-0114" num="0113"><figref idref="DRAWINGS">FIG. 100</figref> shows a screenshot of the &#x201c;Known Features&#x201d; tab of the &#x201c;Process Known Feature Wizard&#x201d; wherein the known features &#x201c;Forest&#x201d; and &#x201c;Water&#x201d; are selected for processing in the sample image &#x201c;island.bmp&#x201d;;</p>
<p id="p-0115" num="0114"><figref idref="DRAWINGS">FIG. 101</figref> shows a screenshot of the &#x201c;Summary&#x201d; tab of the &#x201c;Process Known Feature Wizard&#x201d;;</p>
<p id="p-0116" num="0115"><figref idref="DRAWINGS">FIG. 102</figref> shows a screenshot of the &#x201c;Results Summary&#x201d; dialog box displaying the results of known feature processing of features &#x201c;Forest&#x201d; and &#x201c;Water&#x201d; within the sample image &#x201c;island.bmp&#x201d;;</p>
<p id="p-0117" num="0116"><figref idref="DRAWINGS">FIG. 103</figref> shows a screenshot of one embodiment of an application to accomplish data analysis and feature recognition wherein the workspace is loaded the sample image &#x201c;island.bmp&#x201d; and only the processed image layer &#x201c;Water,&#x201d; which represents the results of known feature processing, is selected for viewing; and</p>
<p id="p-0118" num="0117"><figref idref="DRAWINGS">FIG. 104</figref> shows a screenshot of one embodiment of an application to accomplish data analysis and feature recognition wherein the workspace is loaded with the sample image &#x201c;island.bmp,&#x201d; and the image layer &#x201c;Composite,&#x201d; which is comprised of the sample image &#x201c;island.bmp&#x201d; layered with the processed image layers &#x201c;Forest&#x201d; and &#x201c;Water,&#x201d; is selected for viewing.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0006" level="1">DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENT</heading>
<p id="p-0119" num="0118">Although several of the following embodiments and examples of a data analysis and feature recognition system are described with reference to specific data types, such as multispectral and hyperspectral image data, the invention is not limited to analysis of these data types. The methods and systems as described herein can be used to recognize discrete features in one or a plurality of data sets or any other collection of information that can be represented in a quantifiable datastore.</p>
<p id="p-0120" num="0119">The embodiments of a data analysis and feature recognition system described herein generally involve the analysis and organization of digital multispectral or hyperspectral data streams for the purpose of learning and repeatedly recognizing patterns and objects within the data. In some embodiments, the data organization structure used by the system involves a web (hereafter &#x201c;synaptic web&#x201d;) of interconnected data fields used to describe the data elements of a defined object or feature.</p>
<p id="p-0121" num="0120">As used herein, the term &#x201c;datastore&#x201d; carries its traditional meaning and refers to any software or hardware element capable of at least temporarily storing data. In one embodiment, the datastores referred to herein contain a plurality of known features represented by a plurality of synaptic webs with each synaptic web comprised of a plurality of synaptic leaves joined by synaptic paths.</p>
<p id="p-0122" num="0121">As used herein, the term &#x201c;target data element&#x201d; (TDE) refers to a discrete portion of a larger data set in a given data stream or medium that is being evaluated for characteristics using evaluation algorithms and a given target data area. A TDE can be any size appropriate for a particular data type, modality, submodality, etc. For example, in an image data set, a TDE can consist of a single pixel. In several embodiments and regardless of size, a TDE is a &#x201c;point&#x201d; that is evaluated in a single discrete step before processing moves to the next valid TDE in the data set or selection therein.</p>
<p id="p-0123" num="0122">As used herein, the term &#x201c;target data area&#x201d; (TDA) refers to an ordered collection of data elements immediately surrounding a TDE. In one embodiment, the size and shape of the TDA vary depending upon the type of data or medium that is evaluated, user specifications, and/or industry- or system-acceptable standards and functions to define the member data elements available for inclusion during evaluation of a given TDE.</p>
<p id="p-0124" num="0123">As used herein, the term &#x201c;known feature&#x201d; (KF) refers to an element of data representing an entity, item, object, pattern, or other discretely definable piece of information known to be present in a particular data set during training. At the time of processing, the system searches a new data set for one or more of the previously trained known features.</p>
<p id="p-0125" num="0124">As used herein, the term &#x201c;synaptic web,&#x201d; which is also embodied herein in the form of an algorithm datastore, refers to an organizational structure for storing information about discrete features, patterns, objects, or other known data sets in an implementation such as a rooted, fixed depth tree, a binary tree, or some other acceptable user-specified, preset, or automatically determined structure. A synaptic web advantageously allows the information about the known feature(s) to be quickly added and an unknown data set to be quickly evaluated in order to identify any known features contained therein.</p>
<p id="p-0126" num="0125">As used herein, the term &#x201c;synaptic leaf&#x201d; refers to a terminal node in a synaptic web that represents a plurality of known features identified by the set of algorithm values used to reach the leaf.</p>
<p id="p-0127" num="0126">As used herein, the term &#x201c;synaptic path&#x201d; refers to an evaluation pathway built from a plurality of algorithm values resulting from the analysis of a given TDE with a set of evaluation algorithms and a TDA. The synaptic path is used to reach a synaptic leaf based upon algorithmic calculations for a TDE.</p>
<p id="p-0128" num="0127">As used herein, the term &#x201c;training event&#x201d; refers to the process of associating a plurality of algorithm values and data patterns from a given data set or selection therein to a particular known feature by establishing and/or updating synaptic paths and synaptic leaves as stored in a user-specified, preset, or automatically determined organizational storage structure, such as a synaptic web or algorithm datastore.</p>
<p id="p-0129" num="0128">As used herein, the term &#x201c;algorithm&#x201d; carries its traditional meaning and refers without limitation to any series of repeatable steps resulting in a discrete value. In one embodiment, an algorithm is any mathematical calculation. In several embodiments, various algorithms are performed on a TDE in relation to a previously defined TDA in order to produce a single, meaningful value.</p>
<p id="p-0130" num="0129">As used herein, the term &#x201c;positive training value sets&#x201d; refers to the sets of algorithm values that are located within the area of data trained as the user-defined known feature.</p>
<p id="p-0131" num="0130">As used herein, the term &#x201c;negative training value sets&#x201d; refers to the sets of algorithm values that are located outside the area of data trained as the user-defined known feature and that are typically used to identify training values unique to the positive training value sets.</p>
<p id="p-0132" num="0131">As used herein, the term &#x201c;area training&#x201d; refers to a process used during a training event whereby each set of algorithm values found in a positive training value set (i.e., all data patterns within the current selection) is used to generate synaptic paths for the known feature.</p>
<p id="p-0133" num="0132">As used herein, the term &#x201c;area untraining&#x201d; refers to a process used during an untraining event whereby each set of algorithm values found in a positive training value set (i.e., all data patterns within the current selection) is dissociated from the known feature.</p>
<p id="p-0134" num="0133">As used herein, the term &#x201c;relative adjusted training&#x201d; refers to a process used during a training event whereby each set of algorithm values found in a negative training value set nullifies one matching set of algorithm values found in the positive training value set. The remaining positive training value sets can then be used to generate synaptic paths (i.e., training entry value sets) for the known feature.</p>
<p id="p-0135" num="0134">As used herein, the term &#x201c;absolute adjusted training&#x201d; refers to a process used during a training event whereby each set of algorithm values found in a negative training value set nullifies all matching sets of algorithm values found in the positive training value set. The remaining positive training value sets can then be used to generate synaptic paths (i.e., training entry value sets) for the known feature.</p>
<p id="p-0136" num="0135">As used herein, the term &#x201c;hit detection&#x201d; refers to a method for determining whether a known feature is present in a test data set based upon matching a synaptic path encountered during processing with any path in the synaptic web that is trained for the known feature.</p>
<p id="p-0137" num="0136">As used herein, the term &#x201c;cluster detection&#x201d; refers to a method of determining whether a known feature is present in a test data set based upon both hit detection and the detection of a specified number of additional known feature hits within a pre-defined &#x201c;cluster range&#x201d; of the TDE.</p>
<p id="p-0138" num="0137">As used herein, the term &#x201c;cluster range&#x201d; refers to a set of data elements surrounding a given centralized data element or TDE over which a known feature is evaluated. In one embodiment, the cluster range is a number representing an actual physical distance, in the sense of radius or norm in, over which the known feature operates, while in an alternate embodiment it represents a mathematical relationship between the known feature hit locations. In either embodiment, the cluster range is dictated by the topology and dimensionality of the data set or selection therein that is being processed.</p>
<p id="p-0139" num="0138">As used herein, the term &#x201c;threshold detection&#x201d; refers to a method for determining whether a known feature is present in a test data set based upon both hit detection and the number of times the synaptic path, which is used in hit detection, is trained as the known feature.</p>
<p id="p-0140" num="0139">As used herein, the term &#x201c;(data) modality&#x201d; retains its traditional meaning and refers to one of the various different forms or formats of digital data that can be processed. For example, image data represents one modality, while sound data represents another. In addition to describing data types that conform to one or more human sensory modalities, the term is also intended to encompass data types and formats that might have little or no relation to the human senses. For example, financial data, demographic data, and literary data also represent modalities within the meaning of the term as used herein.</p>
<p id="p-0141" num="0140">As used herein, the term &#x201c;(data) submodality&#x201d; refers to a sub-classification of a data modality. In some embodiments, a submodality refers to one of the applications or sources for the data that can affect how the data is processed. For example, X-ray and satellite photography are submodalities of the imaging modality. Moreover, systems manufactured by different vendors (e.g., GENERAL ELECTRIC, SIEMENS) but used for producing X-ray images can vary enough in their data formats to require distinction into different submodalities.</p>
<p id="p-0142" num="0141"><figref idref="DRAWINGS">FIG. 1</figref> shows one embodiment of an overview of an example data analysis and feature recognition system that is configured to accept an original source data set <b>80</b> or selection therein containing one or a plurality of known and pre-identified features <b>81</b> (e.g., a known pattern, shape, object, or entity). The system is generally configured such that the user &#x201c;trains&#x201d; <b>82</b> said system to recognize the known feature <b>81</b> via the execution of one or a plurality of evaluation algorithms in association with a particular TDA. These algorithms and the TDA are used in concert to assess <b>83</b> the representative data of the given data set selection <b>80</b> in order to identify unique sets of algorithm values and data patterns characterizing the feature(s) <b>81</b>. In one embodiment, once training of all known features <b>81</b> is complete, the training values defining said features are stored <b>84</b> in an organizational structure, such as a synaptic web <b>85</b>, which is comprised of a plurality of synaptic leaves interconnected by a plurality of synaptic paths, or an algorithm datastore. In one embodiment, once the system has been trained for a known feature(s) <b>81</b>, a new data set selection <b>86</b> containing an unknown set of features <b>87</b> is presented to the system for subsequent analysis <b>88</b>. The same pluralities of evaluation algorithms and the same TDA as are used during preliminary known feature training are then called to evaluate <b>89</b> the new data set selection. The resultant algorithmically determined data values and patterns are subsequently compared <b>90</b> to the previously identified and stored algorithm values and data patterns <b>85</b> for the purpose of positively identifying any previously trained known features contained therein. In one embodiment, the results of this known feature processing exercise are then stored in another organizational storage structure, such as a known feature data output overlay, which is sized and addressed in the same manner as the original data set or selection therein, for retrieval at a future time if needed. In one embodiment, once a known feature(s) is found in the new data set, the system notifies <b>91</b> the user as to the identification of said feature(s) and/or presents a representation <b>92</b> (e.g., a graphical image, an audible sound) of the known feature(s) to the user.</p>
<p id="p-0143" num="0142"><figref idref="DRAWINGS">FIG. 2</figref> shows an example system <b>100</b> for executing data analysis and feature recognition. In one embodiment, the system <b>100</b> includes a single computer <b>101</b>. In an alternate embodiment, the system <b>100</b> includes a computer <b>101</b> in communication with a plurality of other computers <b>103</b>. In an alternate embodiment, the computer <b>101</b> is connected with a plurality of other computers <b>103</b>, a server <b>104</b>, a datastore <b>106</b>, and/or a network <b>108</b>, such as an intranet or the Internet. In yet another embodiment, a bank of servers, a wireless device, a cellular telephone, and/or another data capture/entry device(s) is used in place of the computer <b>101</b>. In one embodiment, a data storage device <b>106</b> stores a data analysis and feature recognition datastore. The datastore <b>106</b> can be stored locally at the computer <b>101</b> or at any remote location while remaining retrievable by the computer <b>101</b>. In one embodiment, an application program, which can create the datastore <b>106</b>, is run by the server <b>104</b> or by the computer <b>101</b>. In one embodiment, the computer <b>101</b> or server <b>104</b> includes an application program(s) that trains a known feature and/or identifies a previously defined known feature in digital data media. In one embodiment, the medium is one or a plurality of image pixels or at least one sound recording sample.</p>
<p id="p-0144" num="0143"><figref idref="DRAWINGS">FIG. 3</figref> shows a method formed in accordance with an embodiment of the present invention. The method initializes at block <b>200</b>, and at block <b>202</b> a datastore is created. This is described in more detail with reference to <figref idref="DRAWINGS">FIGS. 4-5</figref>. At block <b>204</b>, a known feature is trained or untrained into the datastore. This is described in more detail with reference to <figref idref="DRAWINGS">FIGS. 6-15</figref>. At block <b>206</b>, the known feature is identified. This is described in more detail with reference to <figref idref="DRAWINGS">FIGS. 16-20</figref>. At block <b>208</b>, an associated known feature action(s)-on-detection is performed. This is described in more detail with reference to <figref idref="DRAWINGS">FIGS. 21-22</figref>. At block <b>210</b>, the method for creation and processing of a known feature(s) is complete.</p>
<p id="p-0145" num="0144"><figref idref="DRAWINGS">FIG. 4</figref> shows an example method <b>202</b> for creating a datastore. The method <b>202</b> initializes at block <b>212</b>, and at block <b>214</b> a plurality of datastore properties are assigned. In one embodiment, the datastore properties include declaration of the data modality and/or submodality. In one embodiment, at block <b>216</b> a known feature(s) is created. This is described in more detail with reference to <figref idref="DRAWINGS">FIG. 5</figref>. In one embodiment, at block <b>218</b> a TDA shape is assigned. The TDA used during data analysis and feature recognition can exist in any applicable geometric configuration, shape, or pattern, can contain any number of applicable dimensions, and can consist of any number of data elements. One example of a TDA to be used in association with the multispectral or hyperspectral imaging modality is a pattern of near and far neighboring pixels surrounding a single, centralized TDE. In another example, the TDA exists in a starburst-like pattern of eight, three-pixel radials surrounding a TDE (hereafter &#x201c;starburst 8&#xd7;3 TDA&#x201d;).</p>
<p id="p-0146" num="0145">In one embodiment, at block <b>220</b> of <figref idref="DRAWINGS">FIG. 4</figref> the TDA evaluation algorithms are selected. In one embodiment, the methods and systems for data analysis and feature recognition as disclosed herein draw from a plurality of acceptable user-specified, preset, or automatically determined evaluation algorithms in order to characterize the original data set or selection therein. In an alternate embodiment, the TDA evaluation algorithms to be used are determined based upon the TDA selected for use. In another embodiment, an automated process is used to decipher the &#x201c;best available&#x201d; TDA and/or evaluation algorithm(s) for use in the analysis of the original data set or selection therein. Here, the &#x201c;best available&#x201d; TDA and/or evaluation algorithm(s) possess any of the following characteristics, which are important or relevant to evaluation of the subject data set or selection therein, including inter alia increased efficiency, productivity, and/or specificity and reduced cost with regard to time and/or processing requirements. In yet another embodiment, the evaluation algorithm(s) are masked, divided, etc., so as to decrease the specificity of the resultant algorithm values and patterns.</p>
<p id="p-0147" num="0146">At block <b>222</b> of <figref idref="DRAWINGS">FIG. 4</figref>, the datastore <b>106</b> is saved to the computer <b>101</b> or the network <b>108</b>, and the method <b>202</b> is complete. Blocks <b>214</b>, <b>216</b>, and the combination of blocks <b>218</b> and <b>220</b> can be executed in any order.</p>
<p id="p-0148" num="0147"><figref idref="DRAWINGS">FIG. 5</figref> shows an example method <b>216</b> for creating a known feature. The method <b>216</b> initializes at block <b>224</b>, and at block <b>226</b> a name for the known feature is entered into the system by the user. In one embodiment, at block <b>228</b> the known feature method of detection attribute is assigned. In one embodiment, the method of detection is selected as hit detection, while in alternate embodiments cluster detection, threshold detection, or cluster and threshold detection are selected. In one embodiment, at block <b>230</b> of <figref idref="DRAWINGS">FIG. 4</figref> the known feature processing action-on-detection is assigned In one embodiment, this processing action is the method of notification used to alert the user when a known feature is positively identified for a given data element within the subject data set or selection therein. In one embodiment, the user chooses to execute no processing action; to play a user-specified, preset, or automatically determined sound; to paint pluralities of activated data elements a user-specified, preset, or automatically determined color; or to execute another acceptable user-specified, preset, or automatically determined action. At block <b>232</b>, the method <b>216</b> is complete. Blocks <b>226</b>, <b>228</b>, and <b>230</b> can be executed in any order.</p>
<p id="p-0149" num="0148"><figref idref="DRAWINGS">FIG. 6</figref> shows an example method <b>204</b> for modifying a synaptic web by training or untraining a known feature. In one embodiment, the method <b>204</b> initializes at block <b>234</b>, and at block <b>236</b> an algorithm value cache is generated. This is described in more detail with reference to <figref idref="DRAWINGS">FIG. 7</figref>. At block <b>238</b>, a region of interest (hereafter &#x201c;ROI&#x201d;), which is an area of data within the original data set or selection therein that is known to contain the feature to be trained or untrained, is defined by the user. At block <b>240</b>, the positive training value sets are retrieved. In one embodiment, at block <b>242</b> a decision is made as to whether the user is performing adjusted training of the feature. In some circumstances, limitations in the ability of the user to finely tune a region of interest can cause some of the positive training value sets to actually contain portions of the data that the user knows should not be trained as the current known feature. Such occurrences are handled by adjusted training, which can be selected by the user as the method for training. In a given multispectral or hyperspectral image, this area outside the region of interest is often the background or noise area that the user does not wish to train as the known feature. By identifying these negative training value sets, those sets of algorithm values that are not associated with the current known feature but exist within the region of interest can be removed. If YES at block <b>242</b>, at block <b>244</b> the negative training value sets are retrieved, and the method <b>204</b> proceeds to block <b>246</b>. If NO at block <b>242</b>, the method <b>204</b> proceeds to block <b>246</b>.</p>
<p id="p-0150" num="0149">In one embodiment, at block <b>246</b> of <figref idref="DRAWINGS">FIG. 6</figref> a decision is made as to whether the user is training a known feature. If YES at block <b>246</b>, at block <b>248</b> the known feature is trained. This is described in more detail with reference to <figref idref="DRAWINGS">FIGS. 8-12</figref>. In one embodiment, at block <b>250</b> the number of unique synaptic paths (i.e., algorithm values and data patterns) added to and/or updated in the synaptic web is reported, and the method <b>204</b> is complete.</p>
<p id="p-0151" num="0150">If NO at block <b>246</b> of <figref idref="DRAWINGS">FIG. 6</figref>, at block <b>252</b> a known feature is untrained. This is described in more detail with reference to <figref idref="DRAWINGS">FIGS. 13-15</figref>. In one embodiment, at block <b>254</b> the number of unique synaptic paths removed from the synaptic web is reported, and the method <b>204</b> is complete. Blocks <b>236</b> and <b>238</b> can be executed in any order. Block <b>240</b> and the combination of blocks <b>242</b> and <b>244</b> can be executed in any order.</p>
<p id="p-0152" num="0151"><figref idref="DRAWINGS">FIG. 7</figref> shows an example method <b>236</b> for generating and populating an algorithm value cache. In one embodiment, an algorithm value cache, which is sized and addressed in the same manner as the original data set or selection therein, consists of a data array to store the numerical results of the evaluation algorithm training or analysis processes. The method <b>236</b> initializes at block <b>256</b>, and at block <b>258</b> an algorithm value cache is initialized. At block <b>260</b>, the first TDE in the current data set or selection therein is retrieved. At block <b>262</b>, the algorithm values are calculated on the TDA for the TDE. At block <b>264</b>, the resultant algorithm values are stored in the algorithm value cache at the location specified by the subject TDE. At block <b>266</b>, a decision is made as to whether there are more TDEs to analyze in the current data set or selection therein. If YES at block <b>266</b>, at block <b>268</b> the next TDE in the data set or selection therein is retrieved, and the method <b>236</b> returns to block <b>262</b>. If NO at block <b>266</b>, the completed algorithm cache is returned, and the method <b>236</b> is complete.</p>
<p id="p-0153" num="0152"><figref idref="DRAWINGS">FIG. 8</figref> shows an example method <b>248</b> for training a known feature. The method <b>248</b> initializes at block <b>272</b>, and at block <b>274</b> a known feature to be trained is accepted from the user. At block <b>276</b>, the training synaptic path array is developed from the positive and negative training value sets, which are determined at <figref idref="DRAWINGS">FIG. 6</figref>. This is described in more detail with reference to <figref idref="DRAWINGS">FIGS. 9-10</figref>. At block <b>278</b>, a new synaptic path is created and followed. This is described in more detail with reference to <figref idref="DRAWINGS">FIG. 11</figref>. At block <b>280</b>, the synaptic path is associated with a known feature. This is described in more detail with reference to <figref idref="DRAWINGS">FIG. 12</figref>. At block <b>282</b>, a decision is made as to whether there are more entries remaining in the training synaptic path array. If YES at block <b>282</b>, the method <b>248</b> returns to block <b>278</b>. If NO at block <b>282</b>, at block <b>284</b> the training counts are updated. In one embodiment, this is accomplished by sorting the added or changed synaptic leaves. At block <b>286</b>, the method <b>248</b> is complete. Blocks <b>274</b> and <b>276</b> can be executed in any order. Blocks <b>282</b> and <b>284</b> can be executed in any order.</p>
<p id="p-0154" num="0153"><figref idref="DRAWINGS">FIG. 9</figref> shows an example method <b>276</b> for developing a training synaptic path array from positive and negative training value sets, which are determined at <figref idref="DRAWINGS">FIG. 6</figref>. The method <b>276</b> initializes at block <b>288</b>, and at block <b>290</b> a training type and the positive and negative training value sets are retrieved. In one embodiment, the known feature training types include: area training, which functions to associate all data patterns within the current data set or selection therein to a given known feature; area untraining, which functions to dissociate all data patterns within the current data set or selection therein from a given known feature; absolute adjusted training, which takes any occurrence of a data pattern for a given known feature that is found outside the current data set or selection therein and removes all occurrences of the same data pattern that are found inside the current data set or selection therein; and relative adjusted training, which takes each occurrence of a data pattern for a given known feature that is found outside the current data set or selection therein and nullifies only one occurrence of the same data pattern that is found inside the current data set or selection therein. In the preferred embodiment, the user selects the method of training that is optimal for the specific data type, modality, submodality, etc., and sample quality.</p>
<p id="p-0155" num="0154">At block <b>292</b> of <figref idref="DRAWINGS">FIG. 9</figref>, the positive training value sets are assigned to the training synaptic path array. At block <b>294</b>, a decision is made as to whether the user is performing adjusted training. If YES at block <b>294</b>, at block <b>296</b> all the negative training value sets are removed from the training synaptic path array. This is described in more detail with reference to <figref idref="DRAWINGS">FIG. 10</figref>. The method <b>276</b> then proceeds to block <b>298</b>. If NO at block <b>294</b>, at block <b>298</b> the method <b>276</b> for developing the training synaptic path array is complete.</p>
<p id="p-0156" num="0155"><figref idref="DRAWINGS">FIG. 10</figref> shows an example method <b>296</b> for removing the negative training value sets from the training synaptic path array; otherwise stated, <figref idref="DRAWINGS">FIG. 10</figref> shows an example method <b>296</b> for performing adjusted training. In one embodiment, relative or absolute adjusted training, as described previously at <figref idref="DRAWINGS">FIG. 9</figref>, are available for use. The method <b>296</b> initializes at block <b>300</b>, and at block <b>302</b> a synaptic path in a set of negative training value sets is selected. At block <b>304</b>, a decision is made as to whether the training type is set to absolute adjusted training. If YES at block <b>304</b>, at block <b>306</b> all training synaptic paths matching the current training synaptic path are removed from the training synaptic path array, and the method <b>296</b> proceeds to block <b>310</b>. If NO at block <b>304</b>, at block <b>308</b>, one training synaptic path matching the current training synaptic path is removed from the training synaptic path array, and the method <b>296</b> proceeds to block <b>310</b>.</p>
<p id="p-0157" num="0156">At block <b>310</b> of <figref idref="DRAWINGS">FIG. 10</figref>, a decision is made as to whether there are more training synaptic paths remaining in the set of negative training values. If YES at block <b>310</b>, at block <b>312</b>, the next training synaptic path in the set of negative training values is selected, and the method <b>296</b> returns to block <b>304</b>. If NO at block <b>310</b>, at block <b>314</b> the method <b>296</b> is complete.</p>
<p id="p-0158" num="0157"><figref idref="DRAWINGS">FIG. 11</figref> shows an example method <b>278</b> for creating and following a synaptic path from a training synaptic path array. The method <b>278</b> initializes at block <b>316</b>, and at block <b>318</b> the current node (hereafter &#x201c;Current_Node&#x201d;) is set to a root node of the synaptic web. At block <b>320</b>, an algorithm value (hereafter &#x201c;Current_Alg_Val&#x201d;) in a synaptic path is selected. At block <b>322</b>, a decision is made as to whether the Current_Node has a next node (hereafter &#x201c;Next_Node&#x201d;) link for the Current_Alg_Val. If YES at block <b>322</b>, the method <b>278</b> proceeds to block <b>326</b>. If NO at block <b>322</b>, at block <b>324</b> a new node (hereafter &#x201c;New_Node&#x201d;) is created, and the Current_Node is linked to the New_Node via the Current_Alg_Val. The method <b>278</b> then proceeds to block <b>326</b>.</p>
<p id="p-0159" num="0158">At block <b>326</b> of <figref idref="DRAWINGS">FIG. 11</figref>, the Current_Node is set to the Next_Node. At block <b>328</b>, a decision is made as to whether there are more algorithm values remaining in the synaptic path. If YES at block <b>328</b>, the method <b>278</b> returns to block <b>320</b>. If NO at block <b>328</b>, at block <b>330</b> the resulting synaptic leaf (hereafter &#x201c;Returned_Leaf&#x201d;) is returned from the end of the synaptic path, and the method <b>278</b> is complete.</p>
<p id="p-0160" num="0159"><figref idref="DRAWINGS">FIG. 12</figref> shows an example method <b>280</b> for associating the synaptic path with a known feature. The method <b>280</b> initializes at block <b>332</b>, and at block <b>334</b> a current synaptic leaf (hereafter &#x201c;Current_Leaf&#x201d;) is set to the Returned_Leaf (from block <b>330</b> of <figref idref="DRAWINGS">FIG. 11</figref>). At block <b>336</b>, a decision is made as to whether the Current_Leaf contains the index value of the trained known feature. If YES at block <b>336</b>, at block <b>338</b> the hit count of the Current_Leaf is updated, and the method <b>280</b> proceeds to block <b>346</b>. If NO at block <b>336</b>, at block <b>340</b> a decision is made as to whether the Current_Leaf has a valid next synaptic leaf (hereafter &#x201c;Next_Leaf&#x201d;). If YES at block <b>340</b>, at block <b>342</b> the Next_Leaf is set to the Current_Leaf, and the method <b>280</b> returns to block <b>336</b>. If NO at block <b>340</b>, at block <b>344</b> a new synaptic leaf (hereafter &#x201c;New_Leaf&#x201d;), which contains the index value of the trained known feature, is created and linked to the Current_Leaf. At block <b>346</b>, the method <b>280</b> is complete.</p>
<p id="p-0161" num="0160"><figref idref="DRAWINGS">FIG. 13</figref> shows an example method <b>252</b> for untraining a known feature. The method <b>252</b> initializes at block <b>348</b>, and at block <b>350</b> a known feature to be untrained and a plurality of positive training value sets, which are determined at <figref idref="DRAWINGS">FIG. 6</figref>, are retrieved. At block <b>352</b>, a set of positive training values is selected. At block <b>354</b>, the synaptic path for the set of positive training values is followed. This is described in more detail with reference to <figref idref="DRAWINGS">FIG. 14</figref>. At block <b>356</b>, a decision is made as to whether the synaptic path exists within the training synaptic path array. If YES at block <b>356</b>, at block <b>358</b> the synaptic path is dissociated from the known feature. This is described in more detail with reference to <figref idref="DRAWINGS">FIG. 15</figref>. The method <b>252</b> then proceeds to block <b>360</b>. If NO at block <b>356</b>, at block <b>360</b> a decision is made as to whether there are more positive training value sets remaining in the plurality of positive training value sets retrieved at block <b>350</b>. If YES at block <b>360</b>, the method <b>252</b> returns to block <b>352</b>. If NO at block <b>360</b>, at block <b>362</b> the method <b>252</b> is complete.</p>
<p id="p-0162" num="0161"><figref idref="DRAWINGS">FIG. 14</figref> shows an example method <b>354</b> for following a synaptic path for a set of positive training values in order to identify a synaptic leaf based upon a set of algorithm values. The method <b>354</b> initializes at block <b>364</b>, and at block <b>366</b> the Current_Node is set to a root node of the synaptic web. At block <b>368</b>, the Current_Alg_Val for the Current_Node of the synaptic path is selected. At block <b>370</b>, a decision is made as to whether the Current_Node has a Next_Node link for the Current_Alg_Val. If YES at block <b>370</b>, at block <b>372</b> the Next_Node is set to the Current_Node, and the method <b>354</b> proceeds to block <b>376</b>. If NO at block <b>370</b>, at block <b>374</b> the synaptic path does not exist, and the method <b>354</b> is complete.</p>
<p id="p-0163" num="0162">At block <b>376</b> of <figref idref="DRAWINGS">FIG. 14</figref>, a decision is made as to whether there are more algorithm values remaining in the synaptic path. If YES at block <b>376</b>, the method <b>354</b> returns to block <b>368</b>. If NO at block <b>376</b>, at block <b>378</b> the resulting synaptic leaf (hereafter &#x201c;Returned_Leaf&#x201d;) is returned at the end of the synaptic path, and the method <b>354</b> is complete.</p>
<p id="p-0164" num="0163"><figref idref="DRAWINGS">FIG. 15</figref> shows an example method <b>358</b> for dissociating a synaptic leaf from a known feature. The method <b>358</b> initializes at block <b>380</b>, and at block <b>382</b> the Current_Leaf is set to the Returned_Leaf (from block <b>378</b> of <figref idref="DRAWINGS">FIG. 14</figref>). At block <b>384</b>, a decision is made as to whether the Current_Leaf contains the index value of the trained known feature. If YES at block <b>384</b>, at block <b>386</b> the Current_Leaf is removed from the synaptic path, and the method <b>358</b> proceeds to block <b>392</b>. If NO at block <b>384</b>, at block <b>388</b> a decision is made as to whether the Current_Leaf has a valid Next_Leaf. If YES at block <b>388</b>, at block <b>390</b> the Next_Leaf is set to the Current_Leaf, and the method <b>358</b> returns to block <b>384</b>. If NO at block <b>388</b>, at block <b>392</b> the method <b>358</b> is complete.</p>
<p id="p-0165" num="0164"><figref idref="DRAWINGS">FIG. 16</figref> shows an example method <b>206</b> for identifying a known feature in a given data set or selection therein. In one embodiment, the method <b>206</b> initializes at block <b>394</b>, and at block <b>396</b> an algorithm value cache is generated. This is described in more detail with reference to <figref idref="DRAWINGS">FIG. 7</figref>. At block <b>398</b>, a region of interest within the data set or selection therein is defined by the user. At block <b>400</b>, the first TDE in the data set or selection therein is selected. At block <b>402</b>, a decision is made as to whether the TDE is located within the selected region of interest. If YES at block <b>402</b>, the method <b>206</b> proceeds to block <b>404</b>. If NO at block <b>402</b>, the method <b>206</b> proceeds to block <b>416</b>.</p>
<p id="p-0166" num="0165">At block <b>404</b> of <figref idref="DRAWINGS">FIG. 16</figref>, the algorithm values for the TDE are retrieved from the algorithm value cache; if the algorithm value cache is unavailable, the algorithm values are calculated for the TDE. At block <b>406</b>, the synaptic web is queried with the algorithm values. (Reference <figref idref="DRAWINGS">FIG. 14</figref>.) At block <b>408</b>, a decision is made as to whether a synaptic path exists for the algorithm values. If YES at block <b>408</b>, at block <b>410</b> a determination is made as to whether any known features hit in the synaptic leaf. This is described in more detail with reference to <figref idref="DRAWINGS">FIGS. 17-20</figref>. The method <b>206</b> then proceeds to block <b>412</b>. If NO at block <b>408</b>, at block <b>412</b> a decision is made as to whether there are more TDEs remaining in the region of interest. If YES at block <b>412</b>, at block <b>414</b> the next TDE in the region of interest is retrieved, and the method <b>206</b> returns to block <b>402</b>. If NO at block <b>412</b>, at block <b>416</b> the known feature(s) identified within the region of interest is returned, and the method <b>206</b> is complete. Blocks <b>396</b> and <b>398</b> can be executed in any order.</p>
<p id="p-0167" num="0166"><figref idref="DRAWINGS">FIG. 17</figref> shows an example method <b>410</b> for determining if a known feature hits for a synaptic leaf. The method <b>410</b> initializes at block <b>418</b>, and at block <b>420</b> the first known feature found for the current leaf (i.e., the current algorithm value set) is retrieved. At block <b>422</b>, a decision is made as to whether the known feature is selected by the user for identification. If YES at block <b>422</b>, the method <b>410</b> proceeds to block <b>424</b>. If NO at block <b>422</b>, the method <b>410</b> proceeds to block <b>442</b>.</p>
<p id="p-0168" num="0167">At block <b>424</b> of <figref idref="DRAWINGS">FIG. 17</figref>, a decision is made as to whether the method of known feature detection is set to hit detection. If YES at block <b>424</b>, at block <b>426</b> the known feature is added to the list of identified features for the current synaptic leaf, and the method <b>410</b> proceeds to block <b>440</b>. If NO at block <b>424</b>, at block <b>428</b> a decision is made as to whether the method of known feature detection is set to threshold detection. If YES at block <b>428</b>, at block <b>430</b> the known feature is checked for a threshold hit. This is described in more detail with reference to <figref idref="DRAWINGS">FIG. 18</figref>. The method <b>410</b> then proceeds to block <b>438</b>. If NO at block <b>428</b>, at block <b>432</b> a decision is made as to whether the method of known feature detection is set to cluster detection. If YES at block <b>432</b>, at block <b>434</b> the known feature is checked for a cluster hit. This is described in more detail with reference to <figref idref="DRAWINGS">FIG. 19</figref>. The method <b>410</b> then proceeds to block <b>438</b>. If NO at block <b>432</b>, at block <b>436</b> the known feature is checked for a threshold and cluster hit. This is described in more detail with reference to <figref idref="DRAWINGS">FIG. 20</figref>. The method <b>410</b> then proceeds to block <b>438</b>.</p>
<p id="p-0169" num="0168">At block <b>438</b> of <figref idref="DRAWINGS">FIG. 17</figref>, a decision is made as to whether the known feature is positively identified (i.e. hits) at the current data element location. If YES at block <b>438</b>, the method <b>410</b> returns to block <b>426</b>. If NO at block <b>438</b>, the method <b>410</b> proceeds to block <b>440</b>.</p>
<p id="p-0170" num="0169">At block <b>440</b> of <figref idref="DRAWINGS">FIG. 17</figref>, a decision is made as to whether the method of known feature detection is set to process only the most significant known feature found for the given data element. Since it is possible following known feature processing for multiple known features to be identified at any given data element location, known features can be processed in different ways. When determining whether a particular known feature positively activates for a given data element, the system can use any known feature that hits for the given data element or only the known feature trained most often (i.e., the most significant known feature) for the given data element. If YES at block <b>440</b>, the method <b>410</b> proceeds to block <b>446</b>. If NO at block <b>440</b>, the method <b>410</b> proceeds to block <b>442</b>.</p>
<p id="p-0171" num="0170">At block <b>442</b> of <figref idref="DRAWINGS">FIG. 17</figref>, a decision is made as to whether there are more known features associated with the current synaptic leaf. If YES at block <b>442</b>, at block <b>444</b> the next known feature found for the current synaptic leaf is retrieved, and the method <b>410</b> returns to block <b>422</b>. If NO at block <b>442</b>, at block <b>446</b> the method <b>410</b> is complete.</p>
<p id="p-0172" num="0171"><figref idref="DRAWINGS">FIG. 18</figref> shows an example method <b>430</b> for determining whether a known feature is identified via a threshold hit. The method <b>430</b> initializes at block <b>448</b>, and at block <b>450</b> a decision is made as to whether the processing thresholds are set. The processing threshold value defines the minimum number of times a known feature must be associated with the synaptic path during training in order for said feature to be positively identified at the given data element location. The associated limit value defines the maximum number of times a known feature must be associated with the synaptic path during training in order for said feature to be positively identified at the given data element location. If YES at block <b>450</b>, the method <b>430</b> proceeds to block <b>452</b>. If NO at block <b>450</b>, the method <b>430</b> proceeds to block <b>454</b>.</p>
<p id="p-0173" num="0172">At block <b>452</b> of <figref idref="DRAWINGS">FIG. 18</figref>, a decision is made as to whether the known feature hit count on the synaptic leaf is between the processing minimum and maximum hit count values. If YES at block <b>452</b>, the method <b>430</b> proceeds to block <b>456</b>. If NO at block <b>452</b>, the method <b>430</b> proceeds to block <b>458</b>.</p>
<p id="p-0174" num="0173">At block <b>454</b> of <figref idref="DRAWINGS">FIG. 18</figref>, a decision is made as to whether the known feature hit count on the synaptic leaf is between the known feature minimum and maximum hit count values. If YES at block <b>454</b>, at block <b>456</b> a value of TRUE is returned, and the method <b>430</b> is complete. If NO at block <b>454</b>, at block <b>458</b> a value of FALSE is returned, and the method <b>430</b> is complete.</p>
<p id="p-0175" num="0174"><figref idref="DRAWINGS">FIG. 19</figref> shows an example method <b>434</b> for determining whether a known feature is identified via a cluster hit. The method <b>434</b> initializes at block <b>460</b>, and at block <b>462</b> a decision is made as to whether the processing cluster range value is set. In one embodiment, the cluster range value defines how far, in each applicable direction and dimension (e.g., X, Y, and Z for three-dimensional imagery), from where a known feature is identified that another positive identification (i.e., &#x201c;hit&#x201d;) of the same known feature must also be located in order for the given known feature to be positively identified for a given data element. In one embodiment, this cluster range value, which can be user-specified, preset, or automatically determined, refers to the actual physical distance in which the known features operate; in an alternate embodiment, the value simply represents some mathematical relationship between the known features. Typically, the cluster range value of a particular known feature defaults to a value of zero to yield a cluster area containing a single data element. A cluster range value set to one results in a cluster area containing all the data elements, located in each applicable direction and dimension, within one unit of the subject data element. For linear data this includes three data elements&#x2014;one before and one following the subject data element. For two-dimensional data, this includes a square of eight data elements surrounding the subject data element. For three-dimensional data, this includes a cubical cluster of 26 data elements surrounding the subject data element.</p>
<p id="p-0176" num="0175">If YES at block <b>462</b> of <figref idref="DRAWINGS">FIG. 19</figref>, at block <b>464</b> a cluster check based upon the processing cluster range value is performed, and the method <b>434</b> proceeds to block <b>468</b>. If NO at block <b>462</b>, at block <b>466</b> a cluster check based upon the known feature cluster range value is performed, and the method <b>434</b> proceeds to block <b>468</b>.</p>
<p id="p-0177" num="0176">At block <b>468</b> of <figref idref="DRAWINGS">FIG. 19</figref>, a decision is made as to whether a cluster of known features is found. If YES at block <b>468</b>, at block <b>470</b> a value of TRUE is returned, and the method <b>434</b> is complete. If NO at block <b>468</b>, at block <b>472</b> a value of FALSE is returned, and the method <b>434</b> is complete.</p>
<p id="p-0178" num="0177"><figref idref="DRAWINGS">FIG. 20</figref> shows an example method <b>436</b> for determining whether a known feature is identified via a threshold and cluster hit. The method <b>436</b> initializes at block <b>474</b>, and at block <b>476</b> the known feature is checked for a threshold hit. This is described in more detail with reference to <figref idref="DRAWINGS">FIG. 18</figref>. At block <b>478</b>, a decision is made as to whether a threshold hit is present. If YES at block <b>478</b>, the method <b>436</b> proceeds to block <b>480</b>. If NO at block <b>478</b>, the method <b>436</b> proceeds to block <b>486</b>.</p>
<p id="p-0179" num="0178">At block <b>480</b> of <figref idref="DRAWINGS">FIG. 20</figref>, the known feature is checked for a cluster hit. This is described in more detail with reference to <figref idref="DRAWINGS">FIG. 19</figref>. At block <b>482</b>, a decision is made as to whether a cluster hit is present. If YES at block <b>482</b>, at block <b>484</b> a value of TRUE is returned, and the method <b>436</b> is complete. If NO at block <b>482</b>, at block <b>486</b> a value of FALSE is returned, and the method <b>436</b> is complete. The combination of blocks <b>476</b> and <b>478</b> and the combination of blocks <b>480</b> and <b>482</b> can be executed in any order.</p>
<p id="p-0180" num="0179"><figref idref="DRAWINGS">FIG. 21</figref> shows an example method <b>208</b> for processing the known feature(s) identified for a given data set or selection therein. The method <b>208</b> initializes at block <b>488</b>, and at block <b>490</b> the first TDE within the data set or selection therein is retrieved. At block <b>492</b>, a decision is made as to whether the TDE is located within the user-specified region of interest. If YES at block <b>492</b>, the method <b>208</b> proceeds to block <b>494</b>. If NO at block <b>492</b>, the method <b>208</b> proceeds to block <b>502</b>.</p>
<p id="p-0181" num="0180">At block <b>494</b> of <figref idref="DRAWINGS">FIG. 21</figref>, the list of known features identified for the subject TDE is retrieved. At block <b>496</b>, the associated action(s)-on-detection for the list of features are performed. This is described in more detail with reference to <figref idref="DRAWINGS">FIG. 22</figref>. At block <b>498</b>, a decision is made as to whether there are more TDEs remaining in the region of interest. If YES at block <b>498</b>, at block <b>500</b> the next TDE within the data set or selection therein is retrieved, and the method <b>208</b> returns to block <b>492</b>. If NO at block <b>498</b>, at block <b>502</b> the method <b>208</b> is complete.</p>
<p id="p-0182" num="0181"><figref idref="DRAWINGS">FIG. 22</figref> shows an example method <b>496</b> for performing the action(s) associated with the list of identified known features. The method <b>496</b> initializes at block <b>504</b>, and at block <b>506</b> the first known feature is retrieved from the list of known features identified for a given TDE. At block <b>508</b>, a decision is made as to whether the known feature action-on-detection is set to play a user-specified, preset, or automatically determined sound. If YES at block <b>508</b>, the method <b>496</b> proceeds to block <b>510</b>. If NO at block <b>508</b>, the method <b>496</b> proceeds to block <b>514</b>.</p>
<p id="p-0183" num="0182">At block <b>510</b> of <figref idref="DRAWINGS">FIG. 22</figref>, a decision is made as to whether the sound has been played by the system at least once before. If YES at block <b>510</b>, the method <b>496</b> proceeds to block <b>518</b>. If NO at block <b>510</b>, at block <b>512</b> the sound specified by the known feature action-on-detection attribute is played by the system, and the method <b>496</b> proceeds to block <b>518</b>.</p>
<p id="p-0184" num="0183">At block <b>514</b> of <figref idref="DRAWINGS">FIG. 22</figref>, a decision is made as to whether the known feature action-on-detection is set to paint a data element(s) a user-specified, preset, or automatically determined color. If YES at block <b>514</b>, at block <b>516</b> the data element color at the TDE location is set to the color specified by the known feature action-on-detection attribute, and the method <b>496</b> proceeds to block <b>518</b>. If NO at block <b>514</b>, the method <b>496</b> proceeds to block <b>518</b>.</p>
<p id="p-0185" num="0184">At block <b>518</b> of <figref idref="DRAWINGS">FIG. 22</figref>, a decision is made as to whether there are more known features in the list of identified known features for the subject TDE. If YES at block <b>518</b>, at block <b>520</b> the next known feature in the list of identified known features for a given TDE is retrieved, and the method <b>496</b> returns to block <b>508</b>. If NO at block <b>518</b>, at block <b>522</b> the method <b>496</b> for known feature action-on-detection processing is complete. Additional known feature actions or combinations of actions are possible as needed or specified by alternate embodiments of the present invention. The actions can be checked and/or executed in any order.</p>
<p id="p-0186" num="0185">For illustrative purposes, the use of a data analysis and feature recognition system as described herein is accomplished with reference to the following examples. <figref idref="DRAWINGS">FIGS. 23-39</figref> depict known feature training and subsequent recognition within two-dimensional data sets, while <figref idref="DRAWINGS">FIGS. 40-61</figref> depict known feature training and recognition within hyperspectral data sets. It is important to note that data analysis and feature recognition using the system of the present invention is not limited to applicability in the imagery data type alone. The methods and systems as described herein can be used to evaluate discrete features in any single or multidimensional data set or any other collection of information that can be represented in a quantifiable datastore.</p>
<p id="p-0187" num="0186">In each of the data arrays of the following examples, the X-location coordinate for each data element of the data array is indicated by the corresponding label in the column header, and the Y-location coordinate for each data element is indicated by the corresponding label in the row header as is common in the art. In one embodiment, the numbers shown within each data array are the grey-scale data element values of the original image selection, while in an alternate embodiment, the numbers represent some other quantifiable characteristic (i.e., location, brightness, elevation, etc.) of the data elements. The numbers shown are the data element values that are analyzed during known feature training and/or recognition.</p>
<p id="p-0188" num="0187"><figref idref="DRAWINGS">FIG. 23</figref> shows an example data array for a two-dimensional, grey-scale, ten-data element (i.e., pixel) by ten-data element image selection of a known feature. The image selection contains a total of 100 data elements.</p>
<p id="p-0189" num="0188"><figref idref="DRAWINGS">FIG. 24</figref> shows the adjacent data element TDA, which exists as a localized grouping of the eight adjacent data elements surrounding a single, centralized TDE. In this example, use of the adjacent data element TDA ensures that at some point during training, the data values of every data element in the original data set are considered. In an alternate embodiment, the adjacent data element TDA does not necessarily represent the preferred TDA embodiment for all data types, modalities, submodalities, etc. As shown in the data arrays of <figref idref="DRAWINGS">FIGS. 23</figref>, <b>34</b>, <b>40</b>-<b>43</b>, and <b>51</b>-<b>54</b>, the first and last rows and columns of data values are highlighted because the data elements contained therein do not abut the requisite data elements as required for complete analysis in accordance with the adjacent data element TDA; as such, these data elements are considered &#x201c;invalid&#x201d; during data analysis. This concept is further exemplified in the post-analyzed data arrays of <figref idref="DRAWINGS">FIGS. 25-28</figref>, <b>35</b>-<b>38</b>, <b>44</b>-<b>49</b>, and <b>55</b>-<b>60</b> wherein the data elements of the first and last rows and columns are devoid of algorithm values.</p>
<p id="p-0190" num="0189">In the example of <figref idref="DRAWINGS">FIGS. 23-39</figref>, the TDA evaluation algorithms used to accomplish known feature training include: the mean algorithm (hereafter &#x201c;Alg1&#x201d;), the median algorithm (hereafter &#x201c;Alg2&#x201d;), the spread of values algorithm (hereafter &#x201c;Alg3&#x201d;), and the standard deviation algorithm (hereafter &#x201c;Alg4&#x201d;). In an alternate embodiment, selection of the TDA to be used during known feature training automatically determines which evaluation algorithms are used. In another embodiment, selection of a given TDA or selection of a given evaluation algorithm determines all subsequent algorithms to be used throughout known feature training.</p>
<p id="p-0191" num="0190">For this example, algorithmic analysis (i.e., known feature training) of the <figref idref="DRAWINGS">FIG. 23</figref> data array begins at any valid, user-specified, preset, or automatically determined data element. In one embodiment, analysis with Alg1 using the adjacent data element TDA initializes at TDE (2, 2) and returns a value of 153, count 1. This resultant value 153 is stored in an algorithm value cache, which is sized and addressed in the same manner as the <figref idref="DRAWINGS">FIG. 23</figref> data array, at the corresponding TDE location (2, 2).</p>
<p id="p-0192" num="0191">In one embodiment, TDE (2, 2) of the <figref idref="DRAWINGS">FIG. 23</figref> data array is then processed with Alg2 using the adjacent data element TDA. A value of 159, count 1, is returned and is stored in the algorithm value cache at the corresponding TDE location (2, 2).</p>
<p id="p-0193" num="0192">In one embodiment, TDE (2, 2) of the <figref idref="DRAWINGS">FIG. 23</figref> data array is then processed with Alg3 using the adjacent data element TDA. A value of 217, count 1, is returned and is stored in the algorithm value cache at the corresponding TDE location (2, 2).</p>
<p id="p-0194" num="0193">In one embodiment, TDE (2, 2) of the <figref idref="DRAWINGS">FIG. 23</figref> data array is then processed with Alg4 using the adjacent data element TDA. A value of 64, count 1, is returned and is stored in the algorithm value cache at the corresponding TDE location (2, 2).</p>
<p id="p-0195" num="0194">In one embodiment, following the evaluation of TDE (2, 2) of the <figref idref="DRAWINGS">FIG. 23</figref> data array with Alg1, Alg2, Alg3, and Alg4 using the adjacent data element TDA, known feature training proceeds to the next valid TDE and then continues until the collection of valid TDEs in the data array is exhausted or processing is terminated. In an alternate embodiment, all valid TDEs of the <figref idref="DRAWINGS">FIG. 23</figref> data array are analyzed with a single evaluation algorithm using the adjacent data element TDA prior to initialization of analysis with any subsequent algorithms.</p>
<p id="p-0196" num="0195"><figref idref="DRAWINGS">FIG. 25</figref> shows an example data array for the ten-data element by ten-data element image selection (as shown in the <figref idref="DRAWINGS">FIG. 23</figref> data array) after analysis with Alg1 (i.e., the mean algorithm) using the adjacent data element TDA. The first valid data element is (2, 2), and the resultant Alg1 value 153, count 1, is used further starting at <figref idref="DRAWINGS">FIG. 29</figref>.</p>
<p id="p-0197" num="0196"><figref idref="DRAWINGS">FIG. 26</figref> shows an example data array for the ten-data element by ten-data element image selection (as shown in the <figref idref="DRAWINGS">FIG. 23</figref> data array) after analysis with Alg2 (i.e., the median algorithm) using the adjacent data element TDA. The first valid data element is (2, 2), and the resultant Alg2 value 159, count 1, is used further starting at <figref idref="DRAWINGS">FIG. 29</figref>.</p>
<p id="p-0198" num="0197"><figref idref="DRAWINGS">FIG. 27</figref> shows an example data array for the ten-data element by ten-data element image selection (as shown in the <figref idref="DRAWINGS">FIG. 23</figref> data array) after analysis with Alg3 (i.e., the spread of values algorithm) using the adjacent data element TDA. The first valid data element is (2, 2), and the resultant Alg3 value 217, count 1, is used further starting at <figref idref="DRAWINGS">FIG. 29</figref>.</p>
<p id="p-0199" num="0198"><figref idref="DRAWINGS">FIG. 28</figref> shows an example data array for the ten-data element by ten-data element image selection (as shown in the <figref idref="DRAWINGS">FIG. 23</figref> data array) after analysis with Alg4 (i.e., the standard deviation algorithm) using the adjacent data element TDA. The first valid data element is (2, 2) and the resultant Alg4 value 64, count 1, is used further starting at <figref idref="DRAWINGS">FIG. 29</figref>.</p>
<p id="p-0200" num="0199"><figref idref="DRAWINGS">FIG. 29</figref> shows one embodiment of an example synaptic web containing a single synaptic path formed after algorithm evaluation of the first valid TDE (2, 2) of the <figref idref="DRAWINGS">FIG. 23</figref> data array with Alg1, Alg2, Alg3, and Alg4 using the adjacent data element TDA. The first node <b>600</b> shows the resultant algorithm value 153, count 1, of algorithmic analysis of TDE (2 2) with Alg1 using the adjacent data element TDA. A count of one signifies the number of times during known feature training that evaluation of the original data array of <figref idref="DRAWINGS">FIG. 23</figref> with Alg1 results in a value of 153; the count value is incremented each time the resultant algorithm value 153 is found after evaluation of a TDE with Alg1. The second node <b>602</b> of the <figref idref="DRAWINGS">FIG. 29</figref> synaptic web shows the resultant algorithm value 159, count 1, after algorithmic analysis of TDE (2, 2) with Alg2 using the adjacent data element TDA. The third node <b>604</b> shows the resultant algorithm value 217, count 1, after algorithmic analysis of TDE (2, 2) with Alg3 using the adjacent data element TDA. Finally, the fourth node <b>606</b> shows the resultant algorithm value 64, count 1, after algorithmic analysis of TDE (2, 2) with Alg4 using the adjacent data element TDA. Following this synaptic path leads to a synaptic leaf <b>608</b> containing a known feature (hereafter &#x201c;KF1&#x201d;). Since this is the first time this synaptic path is established (i.e., identified) for KF1, the count for said known feature is also one. In this example, the synaptic leaf <b>608</b> is the first synaptic leaf of the synaptic web.</p>
<p id="p-0201" num="0200"><figref idref="DRAWINGS">FIG. 30</figref> shows one embodiment of an example synaptic web containing two synaptic paths formed after algorithmic evaluation of TDE (2, 2) and TDE (3, 2) of the <figref idref="DRAWINGS">FIG. 23</figref> data array with Alg1, Alg2, Alg3, and Alg4 using the adjacent data element TDA. The synaptic leaf <b>610</b> results from the algorithmic evaluation of TDE (3, 2). After the analysis of TDE (2, 2) and (3, 2) of the original data array of <figref idref="DRAWINGS">FIG. 23</figref>, there are two different synaptic paths that identify the same known feature, &#x201c;KF1.&#x201d;</p>
<p id="p-0202" num="0201"><figref idref="DRAWINGS">FIG. 31</figref> shows one embodiment of an example synaptic web containing eleven synaptic paths formed after algorithmic evaluation TDEs (2, 2) through (9, 2) and TDEs (2, 3) through (4, 3) of the <figref idref="DRAWINGS">FIG. 23</figref> data array with Alg1, Alg2, Alg3, and Alg4 using the adjacent data element TDA. The data elements are analyzed from left to right within the data array rows, and the corresponding resultant algorithm values are presented from left to right within the synaptic web. There are no repeat algorithm values calculated as a result of Alg1, and accordingly, for each data element evaluated in the original data array of <figref idref="DRAWINGS">FIG. 23</figref>, a new synaptic path terminating in a new synaptic leaf is added to the synaptic web.</p>
<p id="p-0203" num="0202"><figref idref="DRAWINGS">FIG. 32</figref> shows one embodiment of an example synaptic web containing sixteen synaptic paths formed after algorithmic evaluation of TDEs (2, 2) through (9, 2) and TDEs (2, 3) through (9, 3) of the <figref idref="DRAWINGS">FIG. 23</figref> data array with Alg1, Alg2, Alg3, and Alg4 using the adjacent data element TDA. Again, the data elements are analyzed from left to right within the data array rows, and the corresponding resultant algorithm values are presented from left to right within the synaptic web. In this example, as analysis proceeds through the TDEs and each synaptic path is built, a repeat in the resulting Alg1 value of 151 is calculated at TDE (8, 2) and TDE (5, 3). Since this value of 151 for Alg1 is encountered twice, the associated value count is incremented to two. As the algorithmic analysis continues, the synaptic path associated with the Alg1 value of 151 branches into separate synaptic paths because the resulting Alg2 values for TDE (8, 2) and TDE (5, 2) are different.</p>
<p id="p-0204" num="0203"><figref idref="DRAWINGS">FIG. 33</figref> shows one embodiment of an example synaptic web containing a single synaptic path that results in a synaptic leaf with two known features <b>612</b> and <b>614</b>. When multiple known features are associated with a single synaptic path, said features are stored in a sorted list that is ordered by each feature hit count value. The known feature that is most often associated with the synaptic pattern appears first in the list, and it is followed by the other known features, which are in order by decreasing hit count values. In the case of a tie, the first known feature associated with the synaptic path appears first in the list.</p>
<p id="p-0205" num="0204">In one embodiment, once training of one or a plurality of known features from the original ten-data element by ten-data element data array of <figref idref="DRAWINGS">FIG. 23</figref> is complete and the training values are stored in the synaptic web (which is partially shown in <figref idref="DRAWINGS">FIG. 32</figref>), a new data set selection containing one or a plurality of unknown features is presented to the system for analysis and known feature recognition.</p>
<p id="p-0206" num="0205"><figref idref="DRAWINGS">FIG. 34</figref> shows an example data array for a two-dimensional, grey-scale, six-data element (i.e., pixel) by six-data element image selection of a known feature. In the preferred embodiment, it is presumed that this example image selection is obtained using the same type of data capture device as is used to obtain the example data array of <figref idref="DRAWINGS">FIG. 23</figref>. The image selection contains a total of thirty-six data elements.</p>
<p id="p-0207" num="0206">It is important to note that the same evaluation algorithms and the same TDA as are used during preliminary known feature training (as described with relation to <figref idref="DRAWINGS">FIGS. 23-32</figref>) must be employed to evaluate the new data set selection (as shown in <figref idref="DRAWINGS">FIG. 34</figref>). For this example, the adjacent data element TDA (as shown in <figref idref="DRAWINGS">FIG. 24</figref>) and the mean algorithm (i.e., Alg1), the median algorithm (i.e., Alg2), the spread of values algorithm (i.e., Alg3), and the standard deviation algorithm (i.e., Alg4) are used to accomplish known feature recognition.</p>
<p id="p-0208" num="0207">For this example, algorithmic processing of the <figref idref="DRAWINGS">FIG. 34</figref> data array begins at any valid, user-specified, preset, or automatically determined data element. In one embodiment, processing with Alg1 using the adjacent data element TDA initializes at TDE (2, 2) and returns a value of 164, count 1. This resultant value 164 is stored in an algorithm value cache, which is sized and addressed in the same manner as the <figref idref="DRAWINGS">FIG. 34</figref> data array, at the corresponding TDE location (2, 2).</p>
<p id="p-0209" num="0208">In one embodiment, TDE (2, 2) of the <figref idref="DRAWINGS">FIG. 34</figref> data array is then processed with Alg2 using the adjacent data element TDA. A value of 152, count 1, is returned and is stored in the algorithm value cache at the corresponding TDE location (2, 2).</p>
<p id="p-0210" num="0209">In one embodiment, TDE (2, 2) of the <figref idref="DRAWINGS">FIG. 34</figref> data array is then processed with Alg3 using the adjacent data element TDA. A value of 179, count 1, is returned and is stored in the algorithm value cache at the corresponding TDE location (2, 2).</p>
<p id="p-0211" num="0210">In one embodiment, TDE (2, 2) of the <figref idref="DRAWINGS">FIG. 34</figref> data array is then processed with Alg4 using the adjacent data element TDA. A value of 55, count 1, is returned and is stored in the algorithm value cache at the corresponding TDE location (2, 2).</p>
<p id="p-0212" num="0211">In one embodiment, following the evaluation of TDE (2, 2) of the <figref idref="DRAWINGS">FIG. 34</figref> data array with Alg1, Alg2, Alg3, and Alg4 using the adjacent data element TDA, processing proceeds to the next valid TDE and then continues until the collection of valid TDEs in the data array is exhausted or training is terminated. In an alternate embodiment, all valid TDEs of the <figref idref="DRAWINGS">FIG. 34</figref> data array are processed with a single evaluation algorithm using the adjacent data element TDA prior to initialization of processing with any subsequent algorithms.</p>
<p id="p-0213" num="0212"><figref idref="DRAWINGS">FIG. 35</figref> shows an example data array for the six-data element by six-data element image selection (as shown in the <figref idref="DRAWINGS">FIG. 34</figref> data array) after analysis with Alg1 (i.e., the mean algorithm) using the adjacent data element TDA. The first valid data element is (2, 2), and the resultant Alg1 value 164, count 1, is used further starting at <figref idref="DRAWINGS">FIG. 39</figref>.</p>
<p id="p-0214" num="0213"><figref idref="DRAWINGS">FIG. 36</figref> shows an example data array for the six-data element by six-data element image selection (as shown in the <figref idref="DRAWINGS">FIG. 34</figref> data array) after analysis with Alg2 (i.e., the median algorithm) using the adjacent data element TDA. The first valid data element is (2, 2), and the resultant Alg2 value 152, count 1, is used further starting at <figref idref="DRAWINGS">FIG. 39</figref>.</p>
<p id="p-0215" num="0214"><figref idref="DRAWINGS">FIG. 37</figref> shows an example data array for the six-data element by six-data element image selection (as shown in the <figref idref="DRAWINGS">FIG. 34</figref> data array) after analysis with Alg3 (i.e., the spread of values algorithm) using the adjacent data element TDA. The first valid data element is (2, 2), and the resultant Alg3 value 179, count 1, is used further starting at <figref idref="DRAWINGS">FIG. 39</figref>.</p>
<p id="p-0216" num="0215"><figref idref="DRAWINGS">FIG. 38</figref> shows an example data array for the six-data element by six-data element image selection (as shown in the <figref idref="DRAWINGS">FIG. 34</figref> data array) after analysis with Alg4 (i.e., the standard deviation algorithm) using the adjacent data element TDA. The first valid data element is (2, 2), and the resultant Alg4 value 55, count 1, is used further starting at <figref idref="DRAWINGS">FIG. 39</figref>.</p>
<p id="p-0217" num="0216"><figref idref="DRAWINGS">FIG. 39</figref> shows one embodiment of an example synaptic web wherein the algorithm processing results of the first valid data element (2, 2) of the <figref idref="DRAWINGS">FIG. 34</figref> data array (as shown in the data arrays of <figref idref="DRAWINGS">FIGS. 35-38</figref>) are compared to the existing partial synaptic web (as originally shown in <figref idref="DRAWINGS">FIG. 32</figref>), which is trained from the algorithmic evaluation of the <figref idref="DRAWINGS">FIG. 23</figref> data array (as shown in the data arrays of <figref idref="DRAWINGS">FIGS. 25-28</figref>). Referencing the previously trained synaptic web of <figref idref="DRAWINGS">FIG. 32</figref>, the list of Alg1 values is searched for the value 164, and a node for the second algorithm (i.e., Alg2) is found. At the second node, a search for the Alg2 value of 152 is conducted, and a node for the third algorithm (i.e., Alg3) is found. At the third node, a search for the Alg3 value of 179 is conducted, and a node for the fourth algorithm (i.e., Alg4) is found. At the fourth node, a search for the Alg4 value of 55 is also found. Thus, the first valid data element (2, 2) of the sample data array of <figref idref="DRAWINGS">FIG. 34</figref> is identified as being consistent with KF1 following the known synaptic path <b>616</b> as highlighted.</p>
<p id="p-0218" num="0217">In one embodiment, the results of the known feature processing example (as described in <figref idref="DRAWINGS">FIGS. 34-39</figref>) are stored in another organizational storage structure, such as a known feature data output overlay, which is sized and addressed in the same manner as the original data set selection of <figref idref="DRAWINGS">FIG. 34</figref>, for retrieval at a future time if needed. In one embodiment, once a known feature(s) is identified in a new data set, the system notifies the user as to the identification of the feature(s) and/or presents the user with a representation (e.g., a graphical image or an audible sound) associated with the known feature(s).</p>
<p id="p-0219" num="0218">For further illustrative purposes, the use of a data analysis and feature recognition system as described herein is accomplished with reference to a hyperspectral data example sourced from NASA's Airborne Visible/Infrared Imaging Spectrometer (referred to hereafter as &#x201c;AVIRIS&#x201d;), which collected data in 224 contiguous spectral bands with a bandwidth of 0.10 &#x3bc;m. The AVIRIS sensor was utilized to collect data and spectrally analyze the hydrothermal mineral alterations in rocks of the Cuprite mining district in western Nevada, USA. For the example as shown in <figref idref="DRAWINGS">FIGS. 40-61</figref>, four bands (i.e., Band 13, Band 20, Band 173, and Band 200) of this hyperspectral data set are analyzed using one embodiment of the system of the present invention.</p>
<p id="p-0220" num="0219"><figref idref="DRAWINGS">FIG. 40</figref> shows an example data array representing a selection of Band 13, which is located at 500.5 nanometers on the visible spectrum and is hereafter referred to as &#x201c;B13,&#x201d; of the AVIRIS hyperspectral data set. In this example, B13 is represented as a two-dimensional, ten-data element by ten-data element selection of a known feature. The data selection has a total of 100 data elements.</p>
<p id="p-0221" num="0220"><figref idref="DRAWINGS">FIG. 41</figref> shows an example data array representing a selection of Band 20, which is located at 558.7 nanometers on the visible spectrum and is hereafter referred to as &#x201c;B20,&#x201d; of the AVIRIS hyperspectral data set. In this example, B20 is represented by a two-dimensional, ten-data element by ten-data element selection of a known feature. The data selection has a total of 100 data elements.</p>
<p id="p-0222" num="0221"><figref idref="DRAWINGS">FIG. 42</figref> shows an example data array representing Band 173, which is located at 2,000.5 nanometers on the infrared spectrum and is hereafter referred to as &#x201c;B173,&#x201d; of the AVIRIS hyperspectral data set. In this example, B173 is represented by a two-dimensional, ten-data element by ten-data element selection of a known feature. The data selection has a total of 100 data elements. For the purpose of simplicity, only the values for data elements (2, 2) and (3, 2) are shown; the remaining data element values are represented by the symbol &#x201c;X&#x201d; within this data array.</p>
<p id="p-0223" num="0222"><figref idref="DRAWINGS">FIG. 43</figref> shows an example data array representing Band 200, which is located at 2,270.2 nanometers on the infrared spectrum and is hereafter referred to as &#x201c;B200,&#x201d; of the AVIRIS hyperspectral data set. In this example, Band 200 is represented by a two-dimensional, ten-data element by ten-data element selection of a known feature. The data selection has a total of 100 data elements. For the purpose of simplicity, only the values for data elements (2, 2) and (3, 2) are shown; the remaining data element values are represented by the symbol &#x201c;X&#x201d; within this data array.</p>
<p id="p-0224" num="0223">In the example of <figref idref="DRAWINGS">FIGS. 40-61</figref>, the adjacent data element TDA as shown in <figref idref="DRAWINGS">FIG. 24</figref> is used to accomplish known feature training. In addition, the TDA evaluation algorithms to be used to accomplish known feature training include: the mean algorithm (hereafter &#x201c;Alg1&#x201d;) and the spread of values algorithm (hereafter &#x201c;Alg3&#x201d;), which are used to analyze B13 (as shown in the <figref idref="DRAWINGS">FIG. 40</figref> data array) and B20 (as shown in the <figref idref="DRAWINGS">FIG. 41</figref> data array) of the AVIRIS hyperspectral data set; the value algorithm (hereafter &#x201c;Alg5&#x201d;), which is used to analyze B173 (as shown in the <figref idref="DRAWINGS">FIG. 42</figref> data array); and the value algorithm with a mask of 0xFFFC (hereafter &#x201c;Alg6&#x201d;), which is used to analyze B200 (as shown in the <figref idref="DRAWINGS">FIG. 43</figref> data array). In an alternate embodiment, selection of the TDA to be used during known feature training automatically determines which evaluation algorithms are used. In another embodiment, selection of a given TDA or selection of a given evaluation algorithm determines all subsequent algorithms to be used throughout known feature training.</p>
<p id="p-0225" num="0224">For this example, algorithmic analysis of a selection of B13 as shown in the <figref idref="DRAWINGS">FIG. 40</figref> data array begins at any valid, user-specified, preset, or automatically determined data element. In one embodiment, analysis with Alg1 (which is hereafter designated with relation to B13 as &#x201c;B13Alg1&#x201d;) using the adjacent data element TDA initializes at TDE (2, 2) and returns a value of 153, count 1. This resultant value 153 is stored in an algorithm value cache, which is sized and addressed in the same manner as the <figref idref="DRAWINGS">FIG. 40</figref> data array, at the corresponding TDE location (2, 2).</p>
<p id="p-0226" num="0225">In one embodiment, TDE (2, 2) of B13 as shown in the <figref idref="DRAWINGS">FIG. 40</figref> data array is then processed with Alg3 (which is hereafter designated with relation to B13 as &#x201c;B13Alg3&#x201d;) using the adjacent data element TDA. A value of 217, count 1, is returned and is stored in the algorithm value cache at the corresponding TDE location (2, 2).</p>
<p id="p-0227" num="0226">In one embodiment, following the evaluation of TDE (2, 2) of the <figref idref="DRAWINGS">FIG. 40</figref> data array with B13Alg1 and B13Alg3 using the adjacent data element TDA, known feature training proceeds to the next valid TDE and then continues until the collection of valid TDEs in the data array is exhausted or processing is terminated. In an alternate embodiment, all valid TDEs of the <figref idref="DRAWINGS">FIG. 40</figref> data array are processed with a single evaluation algorithm using the adjacent data element TDA prior to initialization of training with any subsequent algorithms.</p>
<p id="p-0228" num="0227"><figref idref="DRAWINGS">FIG. 44</figref> shows an example data array for the ten-data element by ten-data element selection of B13 (as shown in the <figref idref="DRAWINGS">FIG. 40</figref> data array) after analysis with B13Alg1 (i.e., the mean algorithm) using the adjacent data element TDA. The first valid data element is (2, 2), and the resultant B13Alg1 value 153, count 1, is used further starting at <figref idref="DRAWINGS">FIG. 50</figref>.</p>
<p id="p-0229" num="0228"><figref idref="DRAWINGS">FIG. 45</figref> shows an example data array for the ten-data element by ten-data element image selection of B13 (as shown in the <figref idref="DRAWINGS">FIG. 40</figref> data array) after analysis with B13Alg3 (i.e., the spread of values algorithm) using the adjacent data element TDA. The first valid data element is (2, 2), and the resultant B13Alg3 value 217, count 1, is used further starting at <figref idref="DRAWINGS">FIG. 50</figref>.</p>
<p id="p-0230" num="0229">For this example, algorithmic analysis of a selection of B20 as shown in the <figref idref="DRAWINGS">FIG. 41</figref> data array begins at any valid, user-specified, preset, or automatically determined data element. In one embodiment, analysis with Alg1 (which is hereafter designated with relation to B20 as &#x201c;B20Alg1&#x201d;) using the adjacent data element TDA initializes at TDE (2, 2) and returns a value of 120, count 1. This resultant value 120 is stored in an algorithm value cache, which is sized and addressed in the same manner as the <figref idref="DRAWINGS">FIGS. 40-41</figref> data arrays, at the corresponding TDE location (2, 2).</p>
<p id="p-0231" num="0230">In one embodiment, TDE (2, 2) of B20 as shown in the <figref idref="DRAWINGS">FIG. 41</figref> data array is then processed with Alg3 (which is hereafter designated with relation to B20 as &#x201c;B20Alg3&#x201d;) using the adjacent data element TDA. A value of 116, count 1, is returned and is stored in the algorithm value cache at the corresponding TDE location (2, 2).</p>
<p id="p-0232" num="0231">In one embodiment, following the evaluation of TDE (2, 2) of the <figref idref="DRAWINGS">FIG. 41</figref> data array with B20Alg1 and B20Alg3 using the adjacent data element TDA, known feature training proceeds to the next valid TDE and then continues until the collection of valid TDEs in the data array is exhausted or processing is terminated. In an alternate instance, all valid TDEs of the <figref idref="DRAWINGS">FIG. 41</figref> data array are processed with a single evaluation algorithm using the adjacent data element TDA prior to initialization of processing with any subsequent algorithms.</p>
<p id="p-0233" num="0232"><figref idref="DRAWINGS">FIG. 46</figref> shows an example data array for the ten-data element by ten-data element selection of B20 (as shown in the <figref idref="DRAWINGS">FIG. 41</figref> data array) after analysis with B20Alg1 (i.e., the mean algorithm) using the adjacent data element TDA. The first valid data element is (2, 2), and the resultant B20Alg1 value 120, count 1, is used further starting at <figref idref="DRAWINGS">FIG. 50</figref>.</p>
<p id="p-0234" num="0233"><figref idref="DRAWINGS">FIG. 47</figref> shows an example data array for the ten-data element by ten-data element image selection of B20 (as shown in the <figref idref="DRAWINGS">FIG. 41</figref> data array) after analysis with B20Alg3 (i.e., the spread of values algorithm) using the adjacent data element TDA. The first valid data element is (2, 2) and the resultant B20Alg3 value 111, count 1, is used further starting at <figref idref="DRAWINGS">FIG. 50</figref>.</p>
<p id="p-0235" num="0234">For this example, algorithmic analysis of a selection of B173 as shown in the <figref idref="DRAWINGS">FIG. 42</figref> data array begins at any valid, user-specified, preset, or automatically determined data element. In one embodiment, processing with Alg5 (which is hereafter designated with relation to B173 as &#x201c;B173Alg5&#x201d;) using the adjacent data element TDA initializes at TDE (2, 2) and returns a value of 84, count 1. This resultant value 84 is stored in an algorithm value cache, which is sized and addressed in the same manner as the <figref idref="DRAWINGS">FIGS. 40-42</figref> data arrays, at the corresponding TDE location (2, 2).</p>
<p id="p-0236" num="0235">In one embodiment, following the evaluation of TDE (2, 2) of the <figref idref="DRAWINGS">FIG. 42</figref> data array with B173Alg5 using the adjacent data element TDA, known feature training proceeds to the next valid TDE and then continues until the collection of valid TDEs in the data array is exhausted or processing is terminated.</p>
<p id="p-0237" num="0236"><figref idref="DRAWINGS">FIG. 48</figref> shows an example data array for the ten-data element by ten-data element selection of B173 (as shown in the <figref idref="DRAWINGS">FIG. 42</figref> data array) after analysis with B173Alg5 (i.e., the value algorithm) using the adjacent data element TDA. The first valid data element is (2, 2), and the resultant B173Alg5 value 84, count 1, is used further starting at <figref idref="DRAWINGS">FIG. 50</figref>.</p>
<p id="p-0238" num="0237">For this example, algorithmic processing of a selection of B200 as shown in the <figref idref="DRAWINGS">FIG. 43</figref> data array begins at any valid, user-specified, preset, or automatically determined data element. In one embodiment, analysis with Alg6 (which is hereafter designated with relation to B200 as &#x201c;B200Alg6&#x201d;) using the adjacent data element TDA initializes at TDE (2, 2) and returns a value of 124, count 1. This resultant value 124 is stored in an algorithm value cache, which is sized and addressed in the same manner as the <figref idref="DRAWINGS">FIGS. 40-43</figref> data arrays, at the corresponding TDE location (2, 2).</p>
<p id="p-0239" num="0238">In one embodiment, following the evaluation of TDE (2, 2) of the <figref idref="DRAWINGS">FIG. 43</figref> data array with B200Alg6 using the adjacent data element TDA, known feature training proceeds to the next valid TDE and then continues until the collection of valid TDEs in the data array is exhausted or processing is terminated.</p>
<p id="p-0240" num="0239"><figref idref="DRAWINGS">FIG. 49</figref> shows an example data array for the ten-data element by ten-data element selection of B200 (as shown in the <figref idref="DRAWINGS">FIG. 43</figref> data array) after analysis with B200Alg6 (i.e., the value algorithm masked with 0xFFFC) using the adjacent data element TDA. The first valid data element is (2, 2), and the resultant B200Alg6 value 124, count 1, is used further starting at <figref idref="DRAWINGS">FIG. 50</figref>.</p>
<p id="p-0241" num="0240"><figref idref="DRAWINGS">FIG. 50</figref> shows one embodiment of an example synaptic web containing two synaptic paths formed after algorithmic evaluation of TDE (2, 2) and TDE (3, 2) of the <figref idref="DRAWINGS">FIG. 40</figref> data array (which represents a data selection from B13 of the AVIRIS hyperspectral data set) with B13Alg1 and B13Alg3; TDE (2, 2) and TDE (3, 2) of the <figref idref="DRAWINGS">FIG. 41</figref> data array (which represents a data selection from B20 of the AVIRIS hyperspectral data set) with B20Alg1 and B20Alg3; TDE (2, 2) and TDE (3, 2) of the <figref idref="DRAWINGS">FIG. 42</figref> data array (which represents a data selection from B173 of the AVIRIS hyperspectral data set) with B173Alg5; and TDE (2, 2) and TDE (3, 2) of the <figref idref="DRAWINGS">FIG. 43</figref> data array (which represents a data selection from B200 of the AVIRIS hyperspectral data set) with B200Alg6 using the adjacent data element TDA. After analyzing TDE (2, 2) and (3, 2), there are two different synaptic paths that terminate with two synaptic leaves <b>618</b> and <b>620</b>, each of which identify the same known feature, &#x201c;KF1.&#x201d;</p>
<p id="p-0242" num="0241">In one embodiment, once training of one or a plurality of known features from the original ten-data element by ten-data element data arrays (as shown in <figref idref="DRAWINGS">FIGS. 40-43</figref>) is complete and the training values are stored in the synaptic web (as shown in <figref idref="DRAWINGS">FIG. 50</figref>), a new data set selection containing one or a plurality of unknown features is presented to the system for analysis and known feature recognition.</p>
<p id="p-0243" num="0242"><figref idref="DRAWINGS">FIG. 51</figref> shows an example data array representing a selection of B13 of the AVIRIS hyperspectral data set. In this example, B13 is represented as a two-dimensional, six-data element by six-data element selection of a known feature. In one embodiment, it is presumed that this example data selection is obtained using the same type of data capture device as is used to obtain the example data array of <figref idref="DRAWINGS">FIG. 40</figref>. The image selection has a total of thirty-six data elements.</p>
<p id="p-0244" num="0243"><figref idref="DRAWINGS">FIG. 52</figref> shows an example data array representing a selection of B20 of the AVIRIS hyperspectral data set. In this example, B20 is represented as a two-dimensional, six-data element by six-data element selection of a known feature. In one embodiment, it is presumed that this example data selection is obtained using the same type of data capture device as is used to obtain the example data array of <figref idref="DRAWINGS">FIG. 41</figref>. The image selection has a total of thirty-six data elements.</p>
<p id="p-0245" num="0244"><figref idref="DRAWINGS">FIG. 53</figref> shows an example data array representing a selection of B173 of the AVIRIS hyperspectral data set. In this example, B173 is represented as a two-dimensional, six-data element by six-data element selection of a known feature. In one embodiment, it is presumed that this example data selection is obtained using the same type of data capture device as is used to obtain the example data array of <figref idref="DRAWINGS">FIG. 42</figref>. The image selection has a total of thirty-six data elements.</p>
<p id="p-0246" num="0245"><figref idref="DRAWINGS">FIG. 54</figref> shows an example data array representing a selection of B200 of the AVIRIS hyperspectral data set. In this example, B200 is represented as a two-dimensional, six-data element by six-data element selection of a known feature. In one embodiment, it is presumed that this example data selection is obtained using the same type of data capture device as is used to obtain the example data array of <figref idref="DRAWINGS">FIG. 43</figref>. The image selection has a total of thirty-six data elements.</p>
<p id="p-0247" num="0246">As previously noted, the same evaluation algorithms and the same TDA as are used during preliminary known feature training (as described with relation to <figref idref="DRAWINGS">FIGS. 40-50</figref>) must be employed to evaluate the new data set selections (as shown in <figref idref="DRAWINGS">FIGS. 51-54</figref>). For this example, the adjacent data element TDA (as shown in <figref idref="DRAWINGS">FIG. 24</figref>) and the mean algorithm, the spread of values algorithm, the value algorithm, and the value algorithm masked with 0xFFFC are used to accomplish known feature recognition.</p>
<p id="p-0248" num="0247">For this example, algorithmic processing of a selection of B13 as shown in the <figref idref="DRAWINGS">FIG. 51</figref> data array begins at any valid, user-specified, preset, or automatically determined data element. In one embodiment, processing with B13Alg1 using the adjacent data element TDA initializes at TDE (2, 2) and returns a value of 153, count 1. This resultant value 153 is stored in an algorithm value cache, which is sized and addressed in the same manner as the <figref idref="DRAWINGS">FIG. 51</figref> data array, at the corresponding TDE location (2, 2).</p>
<p id="p-0249" num="0248">In one embodiment, TDE (2, 2) of B13 as shown in the <figref idref="DRAWINGS">FIG. 51</figref> data array is then processed with B13Alg3 using the adjacent data element TDA. A value of 217, count 1, is returned and is stored in the algorithm value cache at the corresponding TDE location (2, 2).</p>
<p id="p-0250" num="0249">In one embodiment, following the evaluation of TDE (2, 2) of the <figref idref="DRAWINGS">FIG. 51</figref> data array with B13Alg1 and B13Alg3 using the adjacent data element TDA, processing proceeds to the next valid TDE and then continues until the collection of valid TDEs in the data array is exhausted or processing is terminated. In an alternate embodiment, all valid TDEs of the <figref idref="DRAWINGS">FIG. 51</figref> data array are processed with a single evaluation algorithm using the adjacent data element TDA prior to initialization of processing with any subsequent algorithms.</p>
<p id="p-0251" num="0250"><figref idref="DRAWINGS">FIG. 55</figref> shows an example data array for the six-data element by six-data element selection of B13 (as shown in the <figref idref="DRAWINGS">FIG. 51</figref> data array) after analysis with B13Alg1 (i.e., the mean algorithm) using the adjacent data element TDA. The first valid data element is (2, 2), and the resultant B13Alg1 value 153, count 1, is used further starting at <figref idref="DRAWINGS">FIG. 61</figref>.</p>
<p id="p-0252" num="0251"><figref idref="DRAWINGS">FIG. 56</figref> shows an example data array for the six-data element by six-data element selection of B13 (as shown in the <figref idref="DRAWINGS">FIG. 51</figref> data array) after analysis with B13Alg3 (i.e., the spread of values algorithm) using the adjacent data element TDA. The first valid data element is (2, 2), and the resultant B13Alg3 value 217, count 1, is used further starting at <figref idref="DRAWINGS">FIG. 61</figref>.</p>
<p id="p-0253" num="0252">For this example, algorithmic processing of a selection of B20 as shown in the <figref idref="DRAWINGS">FIG. 52</figref> data array begins at any valid, user-specified, preset, or automatically determined data element. In one embodiment, processing with B20Alg1 using the adjacent data element TDA initializes at TDE (2, 2) and returns a value of 120, count 1. This resultant value 120 is stored in an algorithm value cache, which is sized and addressed in the same manner as the <figref idref="DRAWINGS">FIGS. 51-52</figref> data arrays, at the corresponding TDE location (2, 2).</p>
<p id="p-0254" num="0253">In one embodiment, TDE (2, 2) of B13 as shown in the <figref idref="DRAWINGS">FIG. 52</figref> data array is then processed with B20Alg3 using the adjacent data element TDA. A value of 111, count 1, is returned and is stored in the algorithm value cache at the corresponding TDE location (2, 2).</p>
<p id="p-0255" num="0254">In one embodiment, following the evaluation of TDE (2, 2) of the <figref idref="DRAWINGS">FIG. 52</figref> data array with B20Alg1 and B20Alg3 using the adjacent data element TDA, processing proceeds to the next valid TDE and then continues until the collection of valid TDEs in the data array is exhausted or processing is terminated. In an alternate instance, all valid TDEs of the <figref idref="DRAWINGS">FIG. 52</figref> data array are processed with a single evaluation algorithm using the adjacent data element TDA prior to initialization of processing with any subsequent algorithms.</p>
<p id="p-0256" num="0255"><figref idref="DRAWINGS">FIG. 57</figref> shows an example data array for the six-data element by six-data element selection of B20 (as shown in the <figref idref="DRAWINGS">FIG. 52</figref> data array) after analysis with B20Alg1 (i.e., the mean algorithm) using the adjacent data element TDA. The first valid data element is (2, 2), and the resultant B20Alg1 value 120 count 1, is used further starting at <figref idref="DRAWINGS">FIG. 61</figref>.</p>
<p id="p-0257" num="0256"><figref idref="DRAWINGS">FIG. 58</figref> shows an example data array for the six-data element by six-data element selection of B20 (as shown in the <figref idref="DRAWINGS">FIG. 52</figref> data array) after analysis with B20Alg3 (i.e., the spread of values algorithm) using the adjacent data element TDA. The first valid data element is (2, 2), and the resultant B20Alg3 value 143, count 1, is used further starting at <figref idref="DRAWINGS">FIG. 61</figref>.</p>
<p id="p-0258" num="0257">For this example, algorithmic processing of a selection of B173 as shown in the <figref idref="DRAWINGS">FIG. 53</figref> data array begins at any valid, user-specified, preset, or automatically determined data element. In one embodiment, processing with B173Alg5 using the adjacent data element TDA initializes at TDE (2, 2) and returns a value of 84, count 1. This resultant value 84 is stored in an algorithm value cache, which is sized and addressed in the same manner as the <figref idref="DRAWINGS">FIGS. 51-53</figref> data arrays, at the corresponding TDE location (2, 2).</p>
<p id="p-0259" num="0258">In one embodiment, following the evaluation of TDE (2, 2) of the <figref idref="DRAWINGS">FIG. 53</figref> data array with B173Alg5 using the adjacent data element TDA, processing proceeds to the next valid TDE and then continues until the collection of valid TDEs in the data array is exhausted or processing is terminated.</p>
<p id="p-0260" num="0259"><figref idref="DRAWINGS">FIG. 59</figref> shows an example data array for the six-data element by six-data element selection of B173 (as shown in the <figref idref="DRAWINGS">FIG. 53</figref> data array) after analysis with B173Alg5 (i.e., the value algorithm) using the adjacent data element TDA. The first valid data element is (2, 2), and the resultant B173Alg5 value 84, count 1, is used further starting at <figref idref="DRAWINGS">FIG. 61</figref>.</p>
<p id="p-0261" num="0260">For this example, algorithmic processing of a selection of Band 200 as shown in the <figref idref="DRAWINGS">FIG. 54</figref> data array begins at any valid, user-specified, preset, or automatically determined data element. In one embodiment, processing with B200Alg6 using the adjacent data element TDA initializes at TDE (2, 2) and returns a value of 124, count 1. This resultant value 124 is stored in an algorithm value cache, which is sized and addressed in the same manner as the <figref idref="DRAWINGS">FIGS. 51-54</figref> data arrays, at the corresponding TDE location (2, 2).</p>
<p id="p-0262" num="0261">In one embodiment, following the evaluation of TDE (2, 2) of the <figref idref="DRAWINGS">FIG. 54</figref> data array with B200Alg6 using the adjacent data element TDA, processing proceeds to the next valid TDE and then continues until the collection of valid TDEs in the data array is exhausted or processing is terminated.</p>
<p id="p-0263" num="0262"><figref idref="DRAWINGS">FIG. 60</figref> shows an example data array for the six-data element by six-data element selection of B200 (as shown in the <figref idref="DRAWINGS">FIG. 54</figref> data array) after analysis with B200Alg6 (i.e., the value algorithm masked with 0xFFFC) using the adjacent data element TDA. The first valid data element is (2, 2), and the resultant B200Alg6 value 124, count 1, is used further starting at <figref idref="DRAWINGS">FIG. 61</figref>.</p>
<p id="p-0264" num="0263"><figref idref="DRAWINGS">FIG. 61</figref> shows one embodiment of an example synaptic web wherein the algorithm processing results of the first valid data element (2, 2) of the <figref idref="DRAWINGS">FIGS. 51-54</figref> data arrays (as shown in the data arrays of <figref idref="DRAWINGS">FIGS. 55-60</figref>) are compared to the existing partial synaptic web (as originally shown in <figref idref="DRAWINGS">FIG. 50</figref>), which is trained from the algorithmic evaluation of the data arrays of <figref idref="DRAWINGS">FIGS. 40-43</figref> (as previously described in <figref idref="DRAWINGS">FIGS. 44-49</figref>). Referencing the previously trained synaptic web of <figref idref="DRAWINGS">FIG. 50</figref>, the list of B13Alg1 values is searched for the value 153, and a node for the second algorithm (i.e., B13Alg3) is found. In the second node, a search for the B13Alg3 value of 217 is conducted, and a node for the third algorithm (i.e., B20Alg1) is found. In the third node, a search for the B20Alg1 value of 120 is conducted, and a node for the fourth algorithm (i.e., B20Alg3) is found. In the fourth node, a search for the B20Alg3 value of 111 is conducted, and a node for the fifth algorithm (i.e., B173Alg5) is found. In the fifth node, a search for the B173Alg5 value of 84 is conducted, and a node for the sixth algorithm (i.e., B200Alg6) is found. In the sixth node, a search for the B200Alg6 value of 124 is also found. Thus, the first valid data element (2, 2) of the sample data arrays of <figref idref="DRAWINGS">FIGS. 55-60</figref> is identified as being consistent with KF1 following the known synaptic path <b>622</b> as highlighted.</p>
<p id="p-0265" num="0264">The screenshots presented in <figref idref="DRAWINGS">FIGS. 62-104</figref> represent one embodiment of a user interface for data analysis and feature recognition in the multispectral and/or hyperspectral imagery modality; infinite alternatives exist.</p>
<p id="p-0266" num="0265"><figref idref="DRAWINGS">FIG. 62</figref> is a screenshot <b>800</b> showing the &#x201c;Start&#x201d; tab <b>804</b> or introduction screen of the &#x201c;New SyntelliBase Wizard,&#x201d; which guides the user through the step-by-step creation and/or editing of a datastore (which is embodied herein as a synaptic web and which may be referred to hereafter as a &#x201c;SyntelliBase&#x201d;). In the top right corner of the wizard is a button <b>802</b> to close and exit the wizard. In one embodiment, the user navigates through the wizard using the tab controls at the top of each wizard dialog box or the &#x201c;Cancel <b>808</b>,&#x201d; &#x201c;Back <b>810</b>,&#x201d; &#x201c;Next <b>812</b>,&#x201d; and &#x201c;Finish <b>814</b>&#x201d; buttons at the bottom. This general layout for the wizard as described herein is prevalent throughout most wizard dialog boxes.</p>
<p id="p-0267" num="0266"><figref idref="DRAWINGS">FIG. 63</figref> is a screenshot <b>800</b> showing the &#x201c;Required&#x201d; tab <b>804</b> of the &#x201c;New SyntelliBase Wizard&#x201d; wherein the initial values defining the datastore are set by the user. The type of digital data to be processed is identified by the &#x201c;Modality&#x201d; combination box <b>820</b>, which contains a listing of data modalities specifying the digital data stream format, and the &#x201c;Submodality&#x201d; combination box <b>822</b>, which contains a series of data submodalities specifying the use of the data or specific application of the data modality. While the SyntelliBase name can be changed later and at any time, the modality and submodality information cannot. Also located on this tab, the user decides whether or not to turn on &#x201c;Logging&#x201d; <b>824</b>.</p>
<p id="p-0268" num="0267"><figref idref="DRAWINGS">FIG. 64</figref> is a screenshot <b>800</b> showing the expanded &#x201c;Submodality&#x201d; combination box <b>822</b> of the &#x201c;Required&#x201d; tab <b>804</b> of the &#x201c;New SyntelliBase Wizard.&#x201d; In one embodiment, the &#x201c;Submodality&#x201d; combination box <b>822</b> contains a configurable list of data sub-classifications (i.e., submodalities) that are currently available for selection within the previously selected digital data &#x201c;Modality.&#x201d; Assigning a data submodality enables a user to address differences in digital data within a given data modality.</p>
<p id="p-0269" num="0268"><figref idref="DRAWINGS">FIG. 65</figref> is a screenshot <b>800</b> showing the &#x201c;Optional&#x201d; tab <b>804</b> of the &#x201c;New SyntelliBase Wizard&#x201d; wherein descriptive demographic parameters for information relating to the machine &#x201c;Vendor&#x201d; name <b>830</b>, &#x201c;Machine&#x201d; type <b>832</b>, machine &#x201c;Model&#x201d; <b>834</b>, &#x201c;Trainer&#x201d; name <b>836</b>, and datastore &#x201c;Use&#x201d; <b>838</b> are optionally entered into text boxes by the user. In one embodiment, this information is used to categorize datastores received and/or stored by a network.</p>
<p id="p-0270" num="0269"><figref idref="DRAWINGS">FIG. 66</figref> is a screenshot <b>800</b> showing the &#x201c;Target Data Shape&#x201d; tab <b>804</b> of the &#x201c;New SyntelliBase Wizard&#x201d; wherein the target data shape <b>840</b> (i.e., TDA) and a set of evaluation algorithms <b>842</b> corresponding to the selected target data shape are selected by the user. The &#x201c;Target Data Shape&#x201d; combination box <b>840</b> provides a listing of available TDAs that are used to determine how data immediately surrounding a given TDE is collected for analysis by the evaluation algorithms. In this example, the &#x201c;Target Data Shape&#x201d; box <b>840</b> is set to the &#x201c;Grey Adjacent Pixels&#x201d; TDA, which exists as a square of the nine adjacent pixels surrounding a single, central pixel (i.e., TDE). In one embodiment, the process of selecting the evaluation algorithms applicable to the current processing run begins by choosing a specific TDA shape, while in an alternate embodiment the evaluation algorithms are selected independently from the TDA. In this example a series of three evaluation algorithms <b>842</b>, &#x201c;Algorithm 2,&#x201d; &#x201c;Algorithm 3,&#x201d; and &#x201c;Algorithm 4,&#x201d; are selected by the user. It is a combination of the analysis results for the selected evaluation algorithms, not just the analysis results for a single evaluation algorithm, using the &#x201c;Grey Adjacent Pixel&#x201d; target data shape that is employed for training and processing by the synaptic web.</p>
<p id="p-0271" num="0270"><figref idref="DRAWINGS">FIG. 67</figref> is a screenshot <b>800</b> showing the &#x201c;Summary&#x201d; tab <b>804</b> of the &#x201c;New SyntelliBase Wizard&#x201d; wherein the current datastore properties and settings are displayed in a table <b>850</b> for easy user reference and review. In one embodiment, the selections can be edited by selecting the &#x201c;Back&#x201d; button and confirmed by selecting the &#x201c;Finish&#x201d; button. In this example, the data &#x201c;Modality&#x201d; is &#x201c;Imaging2D&#x201d; <b>851</b>; the data &#x201c;Submodality&#x201d; is &#x201c;X-Ray&#x201d; <b>852</b>; and &#x201c;Logging&#x201d; is &#x201c;True&#x201d; <b>854</b>.</p>
<p id="p-0272" num="0271"><figref idref="DRAWINGS">FIG. 68</figref> is a screenshot <b>800</b> showing the &#x201c;Summary&#x201d; tab <b>804</b> of the &#x201c;New SyntelliBase Wizard&#x201d; wherein the table <b>850</b>, which displays the current datastore properties and settings for easy user reference and review, is scrolled down. In this example, the &#x201c;Target Data Shape&#x201d; is &#x201c;Grey Adjacent Pixels&#x201d; <b>860</b> and the number of evaluation &#x201c;Algorithms&#x201d; selected is &#x201c;7&#x201d; <b>862</b>.</p>
<p id="p-0273" num="0272"><figref idref="DRAWINGS">FIG. 69</figref> is a screenshot <b>900</b> showing one embodiment of a user-interface for a data analysis and feature recognition system that is used to accomplish known feature training and identification and as it exists after the completion of datastore creation; infinite alternatives exist. In one embodiment, the application contains some or all of the following user-interface features: a menu bar(s) <b>910</b>, which is known in the art; sets of icons <b>914</b> representing toolbar options designed to provide easy access to the most commonly used application operations while also allowing customization of user preferences; and an application workspace <b>926</b>. The left-most panel <b>916</b> of the user interface, titled &#x201c;SyntelliBases&#x201d; <b>912</b>, contains information including any associated attributes and user settings about any currently open datastores. In this example, there is one datastore, &#x201c;syntellibase1&#x201d; <b>918</b>, opened in the &#x201c;Imaging2D&#x201d; data modality; a set of known features, when defined and trained, are stored in the &#x201c;Known Features&#x201d; folder <b>920</b>; and the &#x201c;Grey Adjacent Pixels&#x201d; TDA <b>924</b> is selected to train &#x201c;syntellibase1.&#x201d; The right-most panel <b>926</b> is a workspace wherein any currently open images, which can be used to train the opened datastore(s) and then subsequently processed to identify the presence of any trained known features, are displayed.</p>
<p id="p-0274" num="0273"><figref idref="DRAWINGS">FIG. 70</figref> is a screenshot showing one embodiment of a user-interface for a data analysis and feature recognition system wherein the &#x201c;Grey Adjacent Pixels&#x201d; TDA <b>924</b> attribute of &#x201c;syntellibase1&#x201d; is expanded to reveal a listing of all the associated evaluation algorithms that can be selected for use in conjunction with the current TDA. In this example, the selected evaluation algorithms, which include &#x201c;Algorithm 8,&#x201d; &#x201c;Algorithm 9,&#x201d; &#x201c;Algorithm 10,&#x201d; &#x201c;Algorithm 19,&#x201d; &#x201c;Algorithm 20,&#x201d; &#x201c;Algorithm 26,&#x201d; and &#x201c;Algorithm 27,&#x201d; are designated by filled-in icons.</p>
<p id="p-0275" num="0274"><figref idref="DRAWINGS">FIG. 71</figref> is a screenshot <b>950</b> showing the &#x201c;Start&#x201d; tab <b>952</b> or introduction screen of the &#x201c;New Known Feature Wizard,&#x201d; which guides the user through the step-by-step creation and/or editing of a known feature. The layout of the &#x201c;New Known Feature Wizard&#x201d; is described in more detail with reference to <figref idref="DRAWINGS">FIG. 62</figref>.</p>
<p id="p-0276" num="0275"><figref idref="DRAWINGS">FIG. 72</figref> is a screenshot <b>950</b> showing the &#x201c;Identification&#x201d; tab <b>952</b> of the &#x201c;New Known Feature Wizard&#x201d; wherein the user assigns a known feature name, which functions to distinguish the current feature from other features, and a known feature processing method of detection, which defines how the known feature is identified in the data set. Also on the &#x201c;Identification&#x201d; tab <b>952</b>, the user decides whether the entire data selection is to be processed or if processing terminates when the system identifies a single occurrence of the current known feature. In one example, the option to stop data processing upon the first found occurrence of the known feature is beneficial if the system identifies an anomaly that should immediately trigger an event such as stopping a production line for quality control purposes. In such a situation, the user checks the option to &#x201c;Stop processing on first found occurrence.&#x201d; Otherwise, the entire data selection is processed. In this example, the known feature &#x201c;Name&#x201d; <b>960</b> is entered into the textbox as &#x201c;Forest,&#x201d; the known feature &#x201c;Method&#x201d; of detection <b>962</b> combination box is set to &#x201c;HitDetection,&#x201d; and the option to &#x201c;Stop processing on first found occurrence&#x201d; <b>964</b> is unchecked.</p>
<p id="p-0277" num="0276"><figref idref="DRAWINGS">FIG. 73</figref> is a screenshot <b>950</b> showing the &#x201c;Identification&#x201d; tab <b>952</b> of the &#x201c;New Known Feature Wizard&#x201d; wherein the known feature &#x201c;Method&#x201d; of detection combination box <b>962</b> is expanded to reveal a listing of all available known feature methods of identification. In one embodiment, there are four types of processing methods of detection available for selection: &#x201c;Hit Detection,&#x201d; &#x201c;Cluster&#x201d; detection, &#x201c;Threshold&#x201d; detection, and &#x201c;Cluster and Threshold&#x201d; detection.</p>
<p id="p-0278" num="0277">In one embodiment, when hit detection is selected as the known feature processing method, a positive identification of a known feature is made when any of the trained entries in the SyntelliBase is matched. In one example, this method is useful to show the user all of the possible locations that a known feature is identified.</p>
<p id="p-0279" num="0278">In one embodiment, when a known feature is trained, the data patterns associated with the feature are likely to be encountered multiple times. The number of times the pattern is encountered is stored in the synaptic web as the feature's &#x201c;hit count.&#x201d; In one embodiment, a known feature with cluster detection as its processing method is recognized if the pattern is identified as a trained entry in the synaptic web, and the known feature is identified a given number of times for other surrounding data elements within the given area. In one example, cluster detection is useful to prevent isolated data elements from being erroneously identified as the known feature and has further applicability when the user is attempting to filter out spurious hits.</p>
<p id="p-0280" num="0279">In one embodiment, threshold detection identifies a known feature if said feature is present for a given synaptic path assuming said path is trained as this known feature a minimum number of times (i.e., threshold) and is not trained more than a maximum number of times (i.e., limit). Otherwise stated, the threshold method of known feature detection recognizes a known feature if the pattern is identified as a trained entry in the synaptic web and that entry has been trained as this known feature a number of times greater than a given threshold but less than a given limit. In one example, threshold detection ensures that patterns that have seldom been associated with a feature cannot be used to cause a misidentification and is useful to filter out under-trained or over-trained known feature hits.</p>
<p id="p-0281" num="0280">In one embodiment, the cluster and threshold detection processing method uses a combination of both the clustering and thresholding methodologies as previously described. Accordingly, a known feature is identified if said feature is present for a given synaptic path assuming said path is trained as this known feature a minimum number of times greater than the threshold value but less than a given limit value (i.e., thresholding filter), and then if the known feature is identified a given number of times for other surrounding data elements within the given area (i.e., clustering filter).</p>
<p id="p-0282" num="0281"><figref idref="DRAWINGS">FIG. 74</figref> is a screenshot <b>950</b> showing the &#x201c;Training Counts&#x201d; tab <b>952</b> of the &#x201c;New Known Feature Wizard&#x201d; wherein the threshold detection parameters are set by the user. In one embodiment, the user selects a &#x201c;Threshold&#x201d; value representing the minimum number of times the given known feature must be associated with a synaptic path during training in order for said known feature to be positively identified for the given synaptic path. In one embodiment, increasing the threshold value guarantees that only recurring synaptic paths with a number of occurrences higher than the threshold value are used in processing thus ensuring a higher level of confidence in the eventual identification of the known feature. In one embodiment, the user selects a &#x201c;Limit&#x201d; value representing the maximum number of times the given known feature can be associated with the synaptic path during training in order for said known feature to be positively identified for the given synaptic path. In one embodiment, processing with an upper limit can identify those patterns that may be producing erroneous known feature identification. In this example, the &#x201c;Threshold&#x201d; value <b>970</b> is set to the default value of &#x201c;1,&#x201d; and the &#x201c;Limit&#x201d; value <b>974</b> is set to the default value of &#x201c;2,147,483,647.&#x201d;</p>
<p id="p-0283" num="0282"><figref idref="DRAWINGS">FIG. 75</figref> is a screenshot <b>950</b> showing the &#x201c;Cluster Range&#x201d; tab <b>952</b> of the &#x201c;New Known Feature Wizard&#x201d; wherein the cluster detection parameters are set. In cluster detection, as well as in cluster and threshold detection, the known feature has an associated &#x201c;dimension value&#x201d; or &#x201c;cluster range value&#x201d; <b>982</b>, which defines how far, in each applicable dimension <b>980</b> of the multispectral or hyperspectral data set, from where a known feature is identified that other positive identifications (i.e., hits) of the same known feature must also be identified in order for the known feature to be positively identified for a given data element (i.e., TDE). In one embodiment, the user selects the &#x201c;Cluster Count&#x201d; value <b>984</b>, which is the number of times the same known feature must occur within the defined cluster area (as specified by the &#x201c;dimension value&#x201d; or &#x201c;cluster range value&#x201d;) in order for the known feature to be positively identified (i.e., &#x201c;hit&#x201d;) for the current data element. In one example, a cluster range value of zero yields a cluster area containing a single data element, while in another example, a cluster range value of one yields a cluster area containing all the data elements within one unit of the subject data element. In one embodiment, adjustment of the cluster range values is useful if the data is known to have different vertical and horizontal scales.</p>
<p id="p-0284" num="0283"><figref idref="DRAWINGS">FIG. 76</figref> is a screenshot <b>950</b> showing the &#x201c;Actions&#x201d; tab <b>952</b> of the &#x201c;New Known Feature Wizard&#x201d; wherein the system action output is selected by the user. In one embodiment, the &#x201c;Action&#x201d; combination box <b>990</b> contains a listing of the available system actions-on-detection, which are the possible responses of the system and/or methods of user notification that can occur when a known feature is positively identified for a given data element within the data set. In one embodiment, the possible output options are &#x201c;No Action&#x201d; (i.e., do nothing), &#x201c;System Sounds&#x201d; (i.e., play a user-specified, preset, or automatically determined system sound), and &#x201c;Paint&#x201d; (i.e., paint the data element identified as the known feature a user-specified, preset, or automatically determined system color in an output layer).</p>
<p id="p-0285" num="0284"><figref idref="DRAWINGS">FIG. 77</figref> is a screenshot <b>950</b> showing the &#x201c;Summary&#x201d; tab <b>952</b> of the &#x201c;New Known Feature Wizard&#x201d; wherein the current known feature properties and settings are displayed for easy user reference and review. In this example, the known feature &#x201c;Name&#x201d; <b>1000</b> is &#x201c;Forest&#x201d;; the &#x201c;Method&#x201d; of known feature detection is &#x201c;Hit Detection&#x201d; <b>1002</b>; the &#x201c;Threshold&#x201d; value <b>1004</b> is &#x201c;1&#x201d;; the &#x201c;Limit&#x201d; value is &#x201c;2,147,483,647&#x201d; <b>1006</b>; the &#x201c;Cluster Range&#x201d; value <b>1008</b> sets the &#x201c;X&#x201d; dimension value to &#x201c;0,&#x201d; the &#x201c;Y&#x201d; dimension value to &#x201c;0,&#x201d; and the &#x201c;Cluster Count&#x201d; value to &#x201c;1&#x201d;; the known feature &#x201c;Action&#x201d; on detection is &#x201c;Paint&#x201d; <b>1010</b>; and the action &#x201c;Data&#x201d; value <b>1012</b> is &#x201c;ForestGreen.&#x201d;</p>
<p id="p-0286" num="0285">In one embodiment and after one or a plurality of datastores and one or a plurality of known features are created and/or edited, one medium or a plurality of media, which are to be used to train the datastore(s) by associating the user-specified known feature(s) with the data patterns in a given selection(s), are retrieved and loaded into the system of the present invention. In one embodiment, multiple training sessions are required to achieve reliable feature identification. In one embodiment, selections are used to include or exclude areas of interest within a selected image medium for training or processing purposes.</p>
<p id="p-0287" num="0286"><figref idref="DRAWINGS">FIG. 78</figref> is a screenshot <b>900</b> showing one embodiment of an application <b>1024</b> to accomplish data analysis and feature recognition. The user-interface features are previously described with reference to <figref idref="DRAWINGS">FIG. 69</figref>. For this example, the application workspace <b>926</b> is loaded with three sample images: the image &#x201c;forest.bmp&#x201d; <b>1020</b> showing an area of forest, the image &#x201c;dam.bmp,&#x201d; and the image &#x201c;island.bmp.&#x201d; Within the sample image <b>1020</b> is a user-defined selection of forest <b>1028</b>, which is delimited by a black outlined polygon drawn using a selection tool as is common in the art. This selection area (referred to hereafter as the &#x201c;region of interest&#x201d;) is to be trained as the known feature &#x201c;Forest&#x201d; into the datastore &#x201c;syntellibase1&#x201d; using the TDA and evaluation algorithms as selected at <figref idref="DRAWINGS">FIG. 66</figref>. In one embodiment, the panel directly to the left of the sample image <b>1020</b> contains a listing of the layers <b>1026</b> (i.e., processed known features) that are currently available for viewing. In one embodiment, the area <b>916</b> located to the left of the workspace contains a listing of any opened datastores and their associated attributes, such as a listing of trained known features and the user-specified TDA, and the right-most panel of the application contains a &#x201c;Gallery&#x201d; of thumbnail images <b>1030</b> that are currently loaded into the system. In one embodiment, mouse position and color values <b>1022</b>, which are based upon the current cursor location as is common in the art, are shown at the bottom right corner of the screen.</p>
<p id="p-0288" num="0287"><figref idref="DRAWINGS">FIG. 79</figref> is a screenshot <b>1100</b> showing the &#x201c;Start&#x201d; tab <b>1110</b> or introduction screen of the &#x201c;Train Known Feature Wizard,&#x201d; which guides the user through the step-by-step process of training or untraining one or a plurality of selected known features. The layout of the &#x201c;Train Known Feature Wizard&#x201d; is described in more detail with reference to <figref idref="DRAWINGS">FIG. 62</figref>.</p>
<p id="p-0289" num="0288"><figref idref="DRAWINGS">FIG. 80</figref> is a screenshot <b>1100</b> showing the &#x201c;Known Features&#x201d; tab <b>1110</b> of the &#x201c;Train Known Feature Wizard&#x201d; wherein the one or a plurality of previously created known features are selected for training by the user. In one embodiment, the user opts to simultaneously train one or a plurality of known features into one or a plurality of datastores if available. In this example, the listing of available datastores <b>1120</b> contains &#x201c;syntellibase1&#x201d;; the known feature &#x201c;Forest&#x201d; <b>1122</b> is selected for training into &#x201c;syntellibase1&#x201d;; and the known feature &#x201c;Water&#x201d; <b>1124</b> is not selected for training into &#x201c;syntellibase1.&#x201d;</p>
<p id="p-0290" num="0289"><figref idref="DRAWINGS">FIG. 81</figref> is a screenshot <b>1100</b> showing the &#x201c;Method&#x201d; tab <b>1110</b> of the &#x201c;Train Known Feature Wizard&#x201d; wherein the method of known feature training is selected by the user. In one embodiment, the known feature training methodologies include: &#x201c;Area&#x201d; training <b>1130</b>, in which all data patterns within the current data selection are associated with the known feature; area &#x201c;Untrain&#x201d; <b>1132</b>, in which all the data patterns within the current data selection are dissociated from the known feature; &#x201c;Absolute&#x201d; adjusted training <b>1134</b>, in which any occurrence of a feature data pattern found outside the current data selection causes all occurrences of the same data pattern inside the data selection to be removed; and &#x201c;Relative&#x201d; adjusted training <b>1136</b>, in which each occurrence of the feature data pattern found outside the current data selection nullifies one occurrence of the data pattern found inside the current data selection. In the preferred embodiment, the user selects the method of known feature training that is optimal for the specific data modality, data submodality, and data sample quality currently being processed. For this example, the method selected to train the known feature &#x201c;Forest&#x201d; into datastore &#x201c;syntellibase1&#x201d; is &#x201c;Area&#x201d; training.</p>
<p id="p-0291" num="0290"><figref idref="DRAWINGS">FIG. 82</figref> is a screenshot <b>1100</b> showing the &#x201c;Summary&#x201d; tab <b>1110</b> of the &#x201c;Train Known Feature Wizard&#x201d; wherein the current known feature training options are displayed for easy user reference and review. In this example, the number of &#x201c;Known Features&#x201d; <b>1140</b> selected for training is one, and the &#x201c;Method&#x201d; of known feature training <b>1142</b> is &#x201c;Area.&#x201d; In one embodiment, clicking the &#x201c;Finish&#x201d; button on the &#x201c;Summary&#x201d; tab of the &#x201c;Train Known Feature Wizard&#x201d; initializes known feature training or untraining based upon user selections.</p>
<p id="p-0292" num="0291"><figref idref="DRAWINGS">FIG. 83</figref> is a screenshot <b>1200</b> showing the &#x201c;Results Summary&#x201d; dialog box, which displays the results <b>1210</b> of known feature training or untraining in tabular format. In this example, area training of the region of interest containing the feature &#x201c;Forest&#x201d; (as defined at <figref idref="DRAWINGS">FIG. 78</figref>) into the datastore &#x201c;syntellibase1&#x201d; results in the identification of &#x201c;30,150&#x201d; new data patterns representing said known feature and &#x201c;0&#x201d; new data paths, which is the number of times the known feature &#x201c;Forest&#x201d; is associated with an existing data pattern from another known feature. In addition, there are &#x201c;0&#x201d; updated data patterns in the datastore; otherwise stated, the number of times the known feature &#x201c;Forest&#x201d; is associated with a data pattern already known for said feature is zero. In one embodiment, the user can elect not to see the results summary dialog box by unselecting the checkbox labeled &#x201c;Always show results summary.&#x201d;</p>
<p id="p-0293" num="0292">In one embodiment, these new and updated data patterns are generated as a result of executing the previously selected evaluation algorithms (as defined at <figref idref="DRAWINGS">FIG. 66</figref>) on the data element values within the previously selected region of interest (i.e., the selected area of forest as shown in the sample image of <figref idref="DRAWINGS">FIG. 78</figref>) using the training process as described in <figref idref="DRAWINGS">FIGS. 23-32</figref>. The algorithm values for each data element are calculated and taken as a set; those values generate a data pattern associated with the known feature in the synaptic web. Within the region of interest, 30,150 data patterns are associated with the known feature &#x201c;Forest.&#x201d;</p>
<p id="p-0294" num="0293"><figref idref="DRAWINGS">FIG. 84</figref> is a screenshot <b>900</b> showing one embodiment of an application to accomplish data analysis and feature recognition. The user-interface features are previously described with reference to <figref idref="DRAWINGS">FIG. 69</figref>. For this example, the application workspace is loaded with three sample images of interest: the image &#x201c;island.bmp&#x201d; and two other images. The sample image &#x201c;island.bmp&#x201d; <b>1252</b> shows light shaded areas of forest and a dark shaded area of water. Also within the sample image <b>1252</b> is a user-defined selection of forest, which is delimited by a black outlined polygon drawn using a selection tool as is common in the art. This region of interest is to be trained as the known feature &#x201c;Forest&#x201d; into the datastore &#x201c;syntellibase1&#x201d; using the TDA and evaluation algorithms as selected at <figref idref="DRAWINGS">FIG. 66</figref>. In one embodiment, the panel <b>1250</b> directly to the left of the sample image <b>1252</b> contains a listing of layers (i.e., processed known features) that are currently available for viewing. In one embodiment, the area <b>916</b> located to the left of the workspace contains a listing of any opened datastores and their associated attributes, such as a listing of trained known features <b>920</b> and the user-specified TDA.</p>
<p id="p-0295" num="0294"><figref idref="DRAWINGS">FIG. 85</figref> is a screenshot <b>1200</b> showing the &#x201c;Results Summary&#x201d; dialog box, which displays the results <b>1210</b> of known feature training in tabular format. For this example, area training of the user-defined region of interest containing the feature &#x201c;Forest&#x201d; (as defined in <figref idref="DRAWINGS">FIG. 84</figref>) into the datastore &#x201c;syntellibase1&#x201d; results in the identification of &#x201c;8,273&#x201d; new data patterns representing said known feature and &#x201c;0&#x201d; new data paths. In addition, there are &#x201c;2,301&#x201d; updated data patterns in the datastore; otherwise stated, the number of times the known feature &#x201c;Forest&#x201d; is associated with a data pattern already known for said feature is 2,301.</p>
<p id="p-0296" num="0295">In one embodiment, these new and updated data patterns are generated as a result of executing the previously selected evaluation algorithms (as defined at <figref idref="DRAWINGS">FIG. 66</figref>) on the data element values within the previously selected region of interest (i.e., the selected area of forest as shown the sample image of <figref idref="DRAWINGS">FIG. 84</figref>) using the training process as described in <figref idref="DRAWINGS">FIGS. 23-32</figref>. Within the region of interest, 2,301 data patterns, which were previously associated with the known feature &#x201c;Forest,&#x201d; are updated in the synaptic web. In addition, 8,273 data patterns, which were not previously associated with the known feature &#x201c;Forest,&#x201d; are created.</p>
<p id="p-0297" num="0296">In one embodiment, once training of one or a plurality of user-specified known features is complete and a collection of synaptic paths is stored in association with said feature(s), the feature(s) is considered to be &#x201c;trained.&#x201d; Once a known feature is sufficiently trained, in one embodiment the system can be tasked to identify the trained known feature(s) in the same or a different sample data set(s).</p>
<p id="p-0298" num="0297"><figref idref="DRAWINGS">FIG. 86</figref> is a screenshot <b>1300</b> showing the &#x201c;Start&#x201d; tab <b>1310</b> or introduction screen of the &#x201c;Process Known Feature Wizard,&#x201d; which guides the user through the step-by-step process of identifying one or a plurality of selected known features. The layout of the &#x201c;Process Known Feature Wizard&#x201d; is described in more detail with reference to <figref idref="DRAWINGS">FIG. 62</figref>. In one embodiment, the wizard allows a new selection of digital data to be processed using the previously trained datastore(s) in order to determine if one or a plurality of trained known features are present.</p>
<p id="p-0299" num="0298"><figref idref="DRAWINGS">FIG. 87</figref> is a screenshot <b>1300</b> showing the &#x201c;Known Features&#x201d; tab <b>1310</b> of the &#x201c;Process Known Feature Wizard&#x201d; wherein one or a plurality of previously created datastores and known features are selected for processing by the user. In one embodiment, the user opts to simultaneously process one or a plurality of known features within any or all of the opened and trained datastores. For this example, the listing of available datastores <b>1320</b> contains &#x201c;syntellibase1&#x201d; <b>1322</b>, and the known feature &#x201c;Forest&#x201d; is selected for processing.</p>
<p id="p-0300" num="0299"><figref idref="DRAWINGS">FIG. 88</figref> is a screenshot <b>1300</b> showing the &#x201c;Significance&#x201d; tab <b>1310</b> of the &#x201c;Process Known Feature Wizard&#x201d; wherein the user can optionally override the significance processing options as previously set at <figref idref="DRAWINGS">FIG. 72</figref>. In one embodiment, multiple known features can be identified by a similar set of algorithm values, and said features are stored in a list that is ordered by the training hit counts of each feature. The feature most often associated with a particular algorithm value set is the first feature in the list and is called the &#x201c;most significant feature&#x201d; for that data value path. Because some data value sets are common to more than one known feature, the &#x201c;Significance&#x201d; tab of the &#x201c;Process Known Feature Wizard&#x201d; presents the user with a choice related to assignment of the action to be taken among said features. In one embodiment, all known features matching a particular data value path are used when the user selects the option button <b>1330</b>, &#x201c;Use any known feature trained for a data point.&#x201d; In an alternate embodiment, only the known feature trained most often to a particular data value path is used when the user selects the option button <b>1332</b>, &#x201c;Use the known feature trained most often.&#x201d; In this example, the option button <b>1330</b> is selected.</p>
<p id="p-0301" num="0300"><figref idref="DRAWINGS">FIG. 89</figref> is a screenshot <b>1300</b> showing the &#x201c;Training Counts&#x201d; tab <b>1310</b> of the &#x201c;Process Known Feature Wizard&#x201d; wherein the user can optionally override the training count values, as previously set at <figref idref="DRAWINGS">FIG. 74</figref>, for a single known feature processing run. In one embodiment, the user selects a &#x201c;Threshold&#x201d; value <b>1340</b> representing the minimum number of times the given known feature must be associated with the synaptic path during training in order to be positively identified for the given synaptic path. In one embodiment, a user selects a &#x201c;Limit&#x201d; value <b>1342</b> representing the maximum number of times the given known feature can be associated with the synaptic path during training in order to be positively identified for the given synaptic path. It is important to note that for the threshold processing overrides to work here, the known feature must already be set to threshold processing or to cluster and threshold processing; this setting is determined during known feature creation as shown at <figref idref="DRAWINGS">FIG. 73</figref>.</p>
<p id="p-0302" num="0301"><figref idref="DRAWINGS">FIG. 90</figref> is a screenshot <b>1300</b> showing the &#x201c;Cluster Range&#x201d; tab <b>1310</b> of the &#x201c;Process Known Feature Wizard&#x201d; wherein the user can optionally override the cluster detection parameters, as previously set at <figref idref="DRAWINGS">FIG. 75</figref>, for a single known feature processing run. In one embodiment, the user sets the cluster area (i.e., cluster range) size by entering X, Y, Z, etc., &#x201c;Dimension Values&#x201d; <b>1352</b> for each of the &#x201c;Dimensions&#x201d; <b>1350</b> of the multispectral or hyperspectral data set. In one embodiment, the user selects the &#x201c;Cluster Count&#x201d; value <b>1354</b>, which is the number of times the same known feature must occur within the defined cluster area for the known feature to be positively identified for the current data element. It is important to note that for the cluster processing overrides to work here, the known feature must already be set to cluster processing or cluster and threshold processing; this setting is determined during known feature creation as shown at <figref idref="DRAWINGS">FIG. 73</figref>.</p>
<p id="p-0303" num="0302"><figref idref="DRAWINGS">FIG. 91</figref> is a screenshot <b>1300</b> showing the &#x201c;Summary&#x201d; tab <b>1310</b> of the &#x201c;Process Known Feature Wizard&#x201d; wherein the current known feature processing options are displayed for easy user reference and review. For this example, the number of &#x201c;Known &#x201c;Features&#x201d; <b>1360</b> selected for processing is &#x201c;1&#x201d;; the &#x201c;Threshold&#x201d; override value is &#x201c;0&#x201d; <b>1362</b>, the &#x201c;Limit&#x201d; override value <b>1364</b> is &#x201c;100,000&#x201d;, the &#x201c;Significance&#x201d; processing option override value is &#x201c;Use any known feature trained for a data point&#x201d; <b>1366</b>; and the &#x201c;Cluster Range&#x201d; override value <b>1368</b> sets the &#x201c;X&#x201d; dimension cluster range value to &#x201c;0,&#x201d; the &#x201c;Y&#x201d; dimension cluster range value to &#x201c;0,&#x201d; and the &#x201c;Cluster Count&#x201d; value to &#x201c;1.&#x201d;</p>
<p id="p-0304" num="0303"><figref idref="DRAWINGS">FIG. 92</figref> is a screenshot <b>1400</b> showing the &#x201c;Results Summary&#x201d; dialog box, which displays the results of known feature processing in tabular format. For this example, the sample image &#x201c;dam.bmp&#x201d; is processed using the trained datastore &#x201c;syntellibase1&#x201d; <b>1402</b>, and the known feature &#x201c;Forest&#x201d; is identified a total of &#x201c;131,656&#x201d; times using &#x201c;31,556&#x201d; possible data patterns <b>1404</b> representing said known feature. In addition, the known &#x201c;Feature Action&#x201d; <b>1406</b>, which is to paint one or a plurality of data elements identified as the known feature &#x201c;Forest&#x201d; the color &#x201c;ForestGreen,&#x201d; is executed by the system.</p>
<p id="p-0305" num="0304">It is important to note that the same evaluation algorithms and the same TDA as are used during known feature training, as described in more detail with reference to <figref idref="DRAWINGS">FIGS. 78-83</figref>, must also be used during known feature processing. The algorithmically determined data values and patterns resulting from known feature processing are then compared to the previously trained, identified, and stored algorithm values and data patterns resulting from known feature training for the purpose of positively identifying any previously trained known features contained therein. In one embodiment, when the same algorithm set is executed and returns the same set of values, the same data pattern is developed as was developed in training, and the known feature associated with the data pattern is identified.</p>
<p id="p-0306" num="0305"><figref idref="DRAWINGS">FIG. 93</figref> is a screenshot <b>900</b> showing one embodiment of an application to accomplish data analysis and feature recognition. The user-interface features are previously described with reference to <figref idref="DRAWINGS">FIG. 69</figref>. For this example, the application workspace is loaded with two sample images: the image &#x201c;forest.bmp,&#x201d; and the image &#x201c;dam.bmp.&#x201d; The processed image layer &#x201c;Forest&#x201d; <b>1410</b>, which is shown as the only visible layer over the image &#x201c;dam.bmp,&#x201d; represents the results of known feature processing as described in <figref idref="DRAWINGS">FIGS. 86-92</figref> and contains a total of 131,656 data elements identified as the known feature &#x201c;Forest&#x201d; and painted the color &#x201c;ForestGreen&#x201d; (which appears as the color black). Accordingly, the processed image layer &#x201c;Forest&#x201d; <b>1410</b> contains data patterns matching the 31,556 data patterns identified as the known feature &#x201c;Forest.&#x201d;</p>
<p id="p-0307" num="0306"><figref idref="DRAWINGS">FIG. 94</figref> is a screenshot <b>900</b> showing one embodiment of an application to accomplish data analysis and feature recognition. The user-interface features are previously described with reference to <figref idref="DRAWINGS">FIG. 69</figref>. For this example, the application workspace is loaded with two sample images: the image &#x201c;forest.bmp&#x201d; and the image &#x201c;dam.bmp.&#x201d; The image layer &#x201c;Composite&#x201d; <b>1420</b>, which is comprised of the sample image &#x201c;dam.bmp&#x201d; and the processed image layer &#x201c;Forest,&#x201d; represents the results of known feature processing as described in <figref idref="DRAWINGS">FIGS. 86-92</figref> and contains a total of 131,656 data elements identified as the known feature &#x201c;Forest&#x201d; and painted the color &#x201c;ForestGreen&#x201d; (which appears as black). Accordingly, the processed image layer &#x201c;Forest&#x201d; contains data patterns matching the 31,556 data patterns previously identified as the known feature &#x201c;Forest.&#x201d;</p>
<p id="p-0308" num="0307"><figref idref="DRAWINGS">FIG. 95</figref> is a screenshot <b>1400</b> showing the &#x201c;Results Summary&#x201d; dialog box, which displays the results of known feature processing in tabular format. In this example, the sample image &#x201c;island.bmp&#x201d; (as shown in <figref idref="DRAWINGS">FIG. 84</figref>) is processed using the trained datastore &#x201c;syntellibase1&#x201d; <b>1402</b>, and the known feature &#x201c;Forest&#x201d; is identified a total of &#x201c;89,818&#x201d; times using &#x201c;17,999&#x201d; possible data patterns <b>1404</b> representing said known feature. In addition, the known &#x201c;Feature Action&#x201d; <b>1406</b>, which is to &#x201c;Paint&#x201d; one or a plurality of data elements identified as the known feature &#x201c;Forest&#x201d; the color &#x201c;ForestGreen,&#x201d; is executed by the system.</p>
<p id="p-0309" num="0308"><figref idref="DRAWINGS">FIG. 96</figref> is a screenshot <b>900</b> showing one embodiment of an application to accomplish data analysis and feature recognition. The user-interface features are previously described with reference to <figref idref="DRAWINGS">FIG. 69</figref>. For this example, the application workspace is loaded with three sample images: the image &#x201c;forest.bmp,&#x201d; the image &#x201c;island.bmp,&#x201d; and another image. The processed image layer &#x201c;Forest&#x201d; <b>1430</b>, which is shown as the only visible layer over the image &#x201c;island.bmp,&#x201d; represents the results of known feature processing as summarized in <figref idref="DRAWINGS">FIG. 95</figref> and contains a total of 89,818 data elements identified as the known feature &#x201c;Forest&#x201d; and painted the color &#x201c;ForestGreen&#x201d; (which appears as the color black). Accordingly, the processed image layer &#x201c;Forest&#x201d; <b>1430</b> contains data patterns matching the 17,999 data patterns identified as the known feature &#x201c;Forest.&#x201d; Also shown in this example, the solid block of the color &#x201c;ForestGreen&#x201d; (which appears as a solid block of black) present within the processed image layer &#x201c;Forest&#x201d; <b>1430</b> represents the user-selected region of interest containing the feature &#x201c;Forest&#x201d; within the original sample image &#x201c;island.bmp&#x201d; (as shown in <figref idref="DRAWINGS">FIG. 84</figref>); this region of interest is subsequently area trained by the system as the known feature &#x201c;Forest.&#x201d;</p>
<p id="p-0310" num="0309"><figref idref="DRAWINGS">FIG. 97</figref> is a screenshot <b>900</b> showing one embodiment of an application to accomplish data analysis and feature recognition. The user-interface features are previously described with reference to <figref idref="DRAWINGS">FIG. 69</figref>. For this example, the application workspace is loaded with three sample images: &#x201c;forest.bmp,&#x201d; &#x201c;island.bmp,&#x201d; and another image. The image layer &#x201c;Composite&#x201d; <b>1440</b>, which is comprised of the sample image &#x201c;island.bmp&#x201d; and the processed image layer &#x201c;Forest,&#x201d; represents the results of known feature processing as summarized in <figref idref="DRAWINGS">FIG. 95</figref> and contains a total of 89,818 data elements identified as the known feature &#x201c;Forest&#x201d; and painted the color &#x201c;ForestGreen&#x201d; (which appears as black). Accordingly the processed image layer &#x201c;Forest&#x201d; contains data patterns matching the 17,999 data patterns previously identified as the known feature &#x201c;Forest.&#x201d;</p>
<p id="p-0311" num="0310"><figref idref="DRAWINGS">FIG. 98</figref> is a screenshot <b>900</b> showing one embodiment of an application to accomplish data analysis and feature recognition. The user-interface features are previously described with reference to <figref idref="DRAWINGS">FIG. 69</figref>. For this example, the application workspace is loaded with three sample images: the image &#x201c;dam.bmp,&#x201d; the image &#x201c;forest.bmp,&#x201d; and the image &#x201c;island.bmp.&#x201d; The sample image &#x201c;island.bmp&#x201d; <b>1450</b> shows light shaded areas of forest and a dark shaded area of water. Also within the sample image <b>1450</b> is a user-defined selection of water, which is delimited by a black outlined polygon drawn using a selection tool as is common in the art. This region of interest is to be trained as the known feature &#x201c;Water&#x201d; into the datastore &#x201c;syntellibase1&#x201d; using the TDA and evaluation algorithms as selected at <figref idref="DRAWINGS">FIG. 66</figref>.</p>
<p id="p-0312" num="0311"><figref idref="DRAWINGS">FIG. 99</figref> is a screenshot <b>1460</b> showing the &#x201c;Results Summary&#x201d; dialog box, which displays the results of known feature training in tabular format. For this example, area training of the user-defined region of interest containing the feature &#x201c;Water&#x201d; (as defined in <figref idref="DRAWINGS">FIG. 98</figref>) into the datastore &#x201c;syntellibase1&#x201d; results in the identification of &#x201c;1&#x201d; new data pattern representing said known feature and &#x201c;0&#x201d; new data paths In addition, there are &#x201c;0&#x201d; updated data patterns in the datastore; otherwise stated, the number of times the known feature &#x201c;Water&#x201d; is associated with a data pattern already known for said feature is zero.</p>
<p id="p-0313" num="0312"><figref idref="DRAWINGS">FIG. 100</figref> is a screenshot <b>1500</b> showing the &#x201c;Known Features&#x201d; tab <b>1510</b> of the &#x201c;Process Known Feature Wizard&#x201d; wherein one or a plurality of previously created datastores and known features are selected for processing by the user. For this example, the listing of available datastores <b>1512</b> contains &#x201c;syntellibase1,&#x201d; and the known features &#x201c;Forest&#x201d; and &#x201c;Water&#x201d; are selected for processing.</p>
<p id="p-0314" num="0313"><figref idref="DRAWINGS">FIG. 101</figref> is a screenshot <b>1500</b> showing the &#x201c;Summary&#x201d; tab <b>1510</b> of the &#x201c;Process Known Feature Wizard&#x201d; wherein the current known feature processing options are displayed for easy user reference and review. For this example, the number of &#x201c;Known Features&#x201d; <b>1522</b> selected for processing is &#x201c;2&#x201d;; the &#x201c;Threshold&#x201d; override value is &#x201c;0&#x201d; <b>1524</b>; the &#x201c;Limit&#x201d; override value <b>1526</b> is &#x201c;100,000&#x201d;; the &#x201c;Significance&#x201d; processing option override value is, &#x201c;Use any known feature trained for a data point&#x201d; <b>1528</b>; and the &#x201c;Cluster Range&#x201d; override value <b>1530</b> sets the &#x201c;X&#x201d; dimension cluster range value to &#x201c;0,&#x201d; the &#x201c;Y&#x201d; dimension cluster range value to &#x201c;0,&#x201d; and the &#x201c;Cluster Count&#x201d; value to &#x201c;0.&#x201d;</p>
<p id="p-0315" num="0314"><figref idref="DRAWINGS">FIG. 102</figref> is a screenshot <b>1540</b> showing the &#x201c;Results Summary&#x201d; dialog box, which displays the results of known feature processing in tabular format. For this example, the sample image &#x201c;island.bmp&#x201d; is processed using the trained datastore &#x201c;syntellibase1&#x201d; <b>1542</b>. The known feature &#x201c;Forest&#x201d; is identified at total of &#x201c;89,818&#x201d; times using &#x201c;17,999&#x201d; possible data patterns <b>1544</b> representing said known feature, and the known &#x201c;Feature Action&#x201d; <b>1546</b>, which is to &#x201c;Paint&#x201d; one or a plurality of data elements identified as the known feature &#x201c;Forest&#x201d; the color &#x201c;ForestGreen,&#x201d; &#x201c;Forest,&#x201d; is executed by the system. In addition, the known feature &#x201c;Water&#x201d; is identified a total of &#x201c;45,467&#x201d; times using &#x201c;1&#x201d; possible data pattern <b>1548</b> representing said known feature, and the known &#x201c;Feature Action&#x201d; <b>1550</b>, which is to paint one or a plurality of data elements identified as the known feature &#x201c;Water&#x201d; the color &#x201c;Blue,&#x201d; is executed by the system.</p>
<p id="p-0316" num="0315"><figref idref="DRAWINGS">FIG. 103</figref> is a screenshot <b>900</b> showing one embodiment of an application to accomplish data analysis and feature recognition. The user-interface features are previously described with reference to <figref idref="DRAWINGS">FIG. 69</figref>. For this example, the application workspace is loaded with three sample images: the image &#x201c;forest.bmp,&#x201d; the image &#x201c;island.bmp,&#x201d; and another image. The processed image layer &#x201c;Water&#x201d; <b>1570</b>, which is shown as the only visible layer over the image &#x201c;island.bmp,&#x201d; represents the results of known feature processing as described in <figref idref="DRAWINGS">FIGS. 100-102</figref> and contains a total of 45,467 data elements identified as the known feature &#x201c;Water&#x201d; and painted the color &#x201c;Blue&#x201d; (which appears as black horizontal lines). Accordingly, the processed image layer &#x201c;Water&#x201d; <b>1570</b> contains data patterns matching the &#x201c;1&#x201d; data pattern identified as the known feature &#x201c;Water.&#x201d;</p>
<p id="p-0317" num="0316"><figref idref="DRAWINGS">FIG. 104</figref> is a screenshot <b>900</b> showing one embodiment of an application to accomplish data analysis and feature recognition. The user-interface features are previously described with reference to <figref idref="DRAWINGS">FIG. 69</figref>. For this example, the application workspace is loaded with three sample images: the image &#x201c;forest.bmp,&#x201d; the image &#x201c;island.bmp,&#x201d; and another image. The image layer &#x201c;Composite&#x201d; <b>1580</b>, which is comprised of the sample image &#x201c;island.bmp,&#x201d; the processed image layer &#x201c;Forest,&#x201d; and the processed image layer &#x201c;Water,&#x201d; represents the result of known feature processing as described in <figref idref="DRAWINGS">FIGS. 95-103</figref>. Within the image layer &#x201c;Composite&#x201d; <b>1580</b>, there are a total of 89,818 data elements identified as the known feature &#x201c;Forest&#x201d; and painted the color &#x201c;ForestGreen&#x201d; (which appears as black) and <b>45</b>,<b>467</b> data elements identified as the known feature &#x201c;Water&#x201d; and painted the color &#x201c;Blue&#x201d; (which appears as black horizontal lines). The data elements of the image layer &#x201c;Composite&#x201d; that remain unpainted (white) represent the data elements that are not trained as either the known feature &#x201c;Forest&#x201d; or the known feature &#x201c;Water.&#x201d; For example, the area <b>1590</b>, which appears to contain the known feature &#x201c;Water&#x201d; (as shown in <figref idref="DRAWINGS">FIG. 84</figref>), actually contains data patterns that do not match the data patterns associated with either the known feature &#x201c;Forest&#x201d; or the known feature &#x201c;Water&#x201d;; these unidentified data patterns most likely represent areas of shallow water or shoreline.</p>
<p id="p-0318" num="0317">While the preferred embodiment of the invention has been illustrated and described, as noted above, many changes can be made without departing from the spirit and scope of the invention. Accordingly, the scope of the invention is not limited by the disclosure of the preferred embodiment. Instead, the invention should be determined entirely by reference to the claims that follow.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>The embodiments of the invention in which an exclusive property or privilege is claimed are defined as follows:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A system for data analysis and feature recognition comprising:
<claim-text>a computer processor;</claim-text>
<claim-text>a display in communication with the computer processor;</claim-text>
<claim-text>a data store in communication with the computer processor, the data store containing a series of algorithms; and</claim-text>
<claim-text>a memory in communication with the computer processor and containing stored programming instructions operable by the computer processor, the stored programming instruction, when operated by the computer processor, causing the computer processor to:
<claim-text>train for the presence of the feature within a first digital data set based on the series of algorithms, comprising:
<claim-text>calculating a first value for a first target data element within the first digital data set using a first algorithm from the series of algorithms;</claim-text>
<claim-text>calculating a second value for the first target data element within the first digital data set using a second algorithm from the series of algorithms;</claim-text>
<claim-text>repeating the steps of calculating the first value and calculating the second value for a plurality of additional target data elements within the first digital data set using the first algorithm and the second algorithm from the series of algorithms;</claim-text>
<claim-text>defining a plurality of synaptic paths, each one of the plurality of synaptic paths being defined by the calculated first values and calculated second values for each of the target data elements; and</claim-text>
<claim-text>associating the feature with one or more of the synaptic paths, based upon a cluster value, the cluster value representing a number of times the feature must occur within a defined cluster area for the feature to be associated with the target data element; and</claim-text>
</claim-text>
<claim-text>identify the feature in a second digital data set based on the series of algorithms, comprising:
<claim-text>calculating a first value for a first target data element within the second digital data set using the first algorithm from the series of algorithms;</claim-text>
<claim-text>calculating a second value for the first target data element within the second digital data set using the second algorithm from the series of algorithms;</claim-text>
<claim-text>repeating the steps of calculating the first value and calculating the second value for a plurality of additional target data elements within the second digital data set using the first algorithm and the second algorithm from the series of algorithms; and</claim-text>
<claim-text>determining whether the feature is present in the second digital data set by comparing the calculated first values and second values from the second digital data set with the defined plurality of synaptic paths associated with the feature.</claim-text>
</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:
<claim-text>the stored programming instructions causing the computer processor to train for the presence of the feature further cause the computer processor to:</claim-text>
<claim-text>(i) calculate a third value for the first target data element within the first digital data set using a third algorithm from the series of algorithms;</claim-text>
<claim-text>(ii) repeat the step of calculating the third value for the plurality of additional target data elements within the first digital data set; and</claim-text>
<claim-text>(iii) define the plurality of synaptic paths by the calculated first values, calculated second values, and calculated third values for each of the target data elements;</claim-text>
<claim-text>and further wherein the stored programming instructions causing the computer processor to identify the feature in the second digital data set cause the processor to:</claim-text>
<claim-text>(i) calculate a third value for the first target data element within the second digital data set using the third algorithm from the series of algorithms;</claim-text>
<claim-text>(ii) repeat the step of calculating the third value for the plurality of additional target data elements within the first digital data set; and</claim-text>
<claim-text>(iii) determine whether the feature is present in the second digital data set by comparing the calculated first values, calculated second values, and calculated third values from the second digital data set with the defined plurality of synaptic paths associated with the feature.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first digital data set is a multispectral data set of a first type and the second digital data set is a multispectral data set of the first type.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The system of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the first target data element, the second target data element, and the plurality of additional target data elements from the first digital data set are each contained within a common band within the first digital data set.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The system of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the defined cluster extends across multiple bands within the multispectral data set.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the stored programming instructions further cause the processor to create a known feature data store, the known feature data store containing unique synaptic paths associated with the feature.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The system of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the known feature data store further comprises a count value for each of a plurality of unique synaptic paths, the count value indicating a number of instances in which the feature has been found to be associated with each one of the unique synaptic paths.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the synaptic paths comprise a plurality of nodes, each of the nodes representing a calculated value from a different one of the series of algorithms.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the stored programming instructions further cause the processor to perform an action comprising one of sounding an alarm or displaying an indicator when the processor determines that the feature is present in the second digital data set.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first digital data set is a multispectral data set comprising a plurality of subsets of sensor data, the plurality of subsets of sensor data comprising contiguous bands within a range of a frequency spectrum.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the range of the frequency spectrum comprises the visible and the infrared portions of the frequency spectrum, and further wherein the plurality of subsets of sensor data lie within a plurality of different bands within the visible/infrared portion of the frequency spectrum.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein each of the synaptic paths comprises a plurality of nodes, the plurality of nodes representing calculated values corresponding to target data elements from a plurality of subsets of sensor data.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. A system for data analysis and feature recognition comprising:
<claim-text>a computer processor;</claim-text>
<claim-text>a display in communication with the computer processor;</claim-text>
<claim-text>a data store in communication with the computer processor, the data store containing a series of algorithms;</claim-text>
<claim-text>a first digital data set in communication with the processor, the first digital data set comprising a plurality of subsets, each of the plurality of subsets being drawn from a different band of a frequency spectrum;</claim-text>
<claim-text>a second digital data set in communication with the processor, the second digital data set comprising a plurality of subsets, each of the plurality of subsets being drawn from a different band of a frequency spectrum; and</claim-text>
<claim-text>a memory in communication with the computer processor and containing stored programming instructions operable by the computer processor, the stored programming instruction, when operated by the computer processor, causing the computer processor to:
<claim-text>train for the presence of the feature within the first digital data set based on the series of algorithms, comprising:
<claim-text>calculating a first value for a first target data element within a first subset from the plurality of subsets within the first digital data set using one of the algorithms from the series of algorithms;</claim-text>
<claim-text>calculating a second value for a second target data element within a second subset from the plurality of subsets within the first digital data set using one of the algorithms from the series of algorithms;</claim-text>
<claim-text>repeating the steps of calculating the first value and calculating the second value for a plurality of additional target data elements within first subset and the second subset from the first digital data set;</claim-text>
<claim-text>defining a plurality of synaptic paths, each one of the plurality of synaptic paths being defined by the calculated first values and calculated second values for each of the target data elements; and</claim-text>
<claim-text>associating the feature with one or more of the synaptic paths, based upon a cluster value, the cluster value representing a number of times the feature must occur within a defined cluster area for the feature to be associated with the target data element; and</claim-text>
</claim-text>
<claim-text>identify the feature in the second digital data set based on the series of algorithms, comprising:
<claim-text>calculating a first value for a first target data element within a first subset from the plurality of subsets within the second digital data set using one of the algorithms from the series of algorithms;</claim-text>
<claim-text>calculating a second value for a second target data element within a second subset from the plurality of subsets within the second digital data set using one of the algorithms from the series of algorithms;</claim-text>
<claim-text>repeating the steps of calculating the first value and calculating the second value for a plurality of additional target data elements within the second digital data set using the first algorithm and the second algorithm from the series of algorithms; and</claim-text>
<claim-text>determining whether the feature is present in the second digital data set by comparing the calculated first values and second values from the second digital data set with the defined plurality of synaptic paths associated with the feature.</claim-text>
</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref> wherein each of the synaptic paths comprises a plurality of nodes, the plurality of nodes representing calculated values corresponding to target data elements from a plurality of subsets of sensor data.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein:
<claim-text>the stored programming instructions causing the computer processor to train for the presence of the feature further cause the computer processor to:</claim-text>
<claim-text>(i) calculate a third value for a third target data element within a third subset from the plurality of subsets within the first digital data set using one of the algorithms from the series of algorithms;</claim-text>
<claim-text>(ii) repeat the step of calculating the third value for the plurality of additional target data elements within the first digital data set; and</claim-text>
<claim-text>(iii) define the plurality of synaptic paths by the calculated first values, calculated second values, and calculated third values for each of the target data elements;</claim-text>
<claim-text>and further wherein the stored programming instructions causing the computer processor to identify the feature in the second digital data set cause the processor to:</claim-text>
<claim-text>(i) calculate a third value for a third target data element within a third subset from the plurality of subsets within the second digital data set using one of the algorithms from the series of algorithms;</claim-text>
<claim-text>(ii) repeat the step of calculating the third value for the plurality of additional target data elements within the first digital data set; and</claim-text>
<claim-text>(iii) determine whether the feature is present in the second digital data set by comparing the calculated first values, calculated second values, and calculated third values from the second digital data set with the defined plurality of synaptic paths associated with the feature.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The system of <claim-ref idref="CLM-00015">claim 15</claim-ref> wherein the first value, the second value, and the third value are each calculated by using the same algorithm from the series of algorithms.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the first value, the second value, and the third value are each calculated by using a different algorithm from the series of algorithms.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the defined cluster extends across multiple bands within the multispectral data set.</claim-text>
</claim>
</claims>
</us-patent-grant>
