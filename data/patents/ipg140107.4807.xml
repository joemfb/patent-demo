<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625900-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625900</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13412952</doc-number>
<date>20120306</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>TW</country>
<doc-number>95150066 A</doc-number>
<date>20061229</date>
</priority-claim>
</priority-claims>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20130101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>3</main-group>
<subgroup>033</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classifications-cpc>
<main-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>00154</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
</main-cpc>
</classifications-cpc>
<classification-national>
<country>US</country>
<main-classification>382187</main-classification>
<further-classification>382119</further-classification>
<further-classification>345179</further-classification>
</classification-national>
<invention-title id="d2e61">Video-based biometric signature data collection</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>4189743</doc-number>
<kind>A</kind>
<name>Schure et al.</name>
<date>19800200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348577</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6011873</doc-number>
<kind>A</kind>
<name>Desai et al.</name>
<date>20000100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382245</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6044165</doc-number>
<kind>A</kind>
<name>Perona et al.</name>
<date>20000300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6064751</doc-number>
<kind>A</kind>
<name>Smithies et al.</name>
<date>20000500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6100538</doc-number>
<kind>A</kind>
<name>Ogawa</name>
<date>20000800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6377249</doc-number>
<kind>B1</kind>
<name>Mumford</name>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345179</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6543935</doc-number>
<kind>B2</kind>
<name>Balla et al.</name>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6543945</doc-number>
<kind>B2</kind>
<name>Watanabe et al.</name>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6714311</doc-number>
<kind>B2</kind>
<name>Hashimoto</name>
<date>20040300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6718060</doc-number>
<kind>B1</kind>
<name>Yokota et al.</name>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>7257255</doc-number>
<kind>B2</kind>
<name>Pittel</name>
<date>20070800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>7279646</doc-number>
<kind>B2</kind>
<name>Xu</name>
<date>20071000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>7961917</doc-number>
<kind>B2</kind>
<name>Black</name>
<date>20110600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2002/0015159</doc-number>
<kind>A1</kind>
<name>Hashimoto</name>
<date>20020200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2002/0176225</doc-number>
<kind>A1</kind>
<name>O'Hara et al.</name>
<date>20021100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>361683</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2002/0176577</doc-number>
<kind>A1</kind>
<name>Xu</name>
<date>20021100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>380258</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>2003/0021331</doc-number>
<kind>A1</kind>
<name>Balla et al.</name>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>2003/0095708</doc-number>
<kind>A1</kind>
<name>Pittel</name>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382187</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>2003/0210946</doc-number>
<kind>A1</kind>
<name>De Schrijver</name>
<date>20031100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>2005/0180618</doc-number>
<kind>A1</kind>
<name>Black</name>
<date>20050800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>2007/0200121</doc-number>
<kind>A1</kind>
<name>Lankhorst et al.</name>
<date>20070800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>257 79</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>2010/0283766</doc-number>
<kind>A1</kind>
<name>Shieh</name>
<date>20101100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>WO</country>
<doc-number>9514286</doc-number>
<kind>A1</kind>
<date>19950500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>WO</country>
<doc-number>9916013</doc-number>
<kind>A1</kind>
<date>19990400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>WO</country>
<doc-number>0227461</doc-number>
<kind>A1</kind>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>WO</country>
<doc-number>0232151</doc-number>
<kind>A2</kind>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00027">
<othercit>International Search Report regarding International Application No. PCT/EP2007/061871, dated May 29, 2008, 5 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00028">
<othercit>Final Office Action regarding U.S. Appl. No. 11/938,817, dated Mar. 29, 2012, 16 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00029">
<othercit>Amendment Pursuant to Request for Continued Examination regarding U.S. Appl. No. 11/938,817, dated May 25, 2012, 13 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00030">
<othercit>George, &#x201c;Biometric Verification in Dynamic Writing,&#x201d; Proceedings of SPIE, Wavelet and Independent Component Analysis Applications IX, 4738:125-132, Apr. 2002.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00031">
<othercit>Pacut et al., &#x201c;Recognition of Human Signatures,&#x201d; Proceedings of IJCNN '01, International Joint Conference on Neural Networks, 2:1560-1564, Jul. 2001.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00032">
<othercit>Zhao et al., &#x201c;On-Line Signature Verification by Adaptively Weighted DP Matching,&#x201d;IEIXE Trans. Inf. &#x26; Syst., E79-D (5):535-541, May 1996.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00033">
<othercit>International Search Report dated May 29, 2008, regarding Application No. PCT/EP2007/06871, 5 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00034">
<othercit>USPTO Office Action dated Nov. 7, 2011, regarding U.S. Appl. No. 11/938,817, 16 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00035">
<othercit>Response to Office Action dated Jan. 31, 2012, regarding U.S. Appl. No. 11/938,817, 14 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>9</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>None</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>7</number-of-drawing-sheets>
<number-of-figures>9</number-of-figures>
</figures>
<us-related-documents>
<division>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>11938817</doc-number>
<date>20071113</date>
</document-id>
<parent-status>PENDING</parent-status>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13412952</doc-number>
</document-id>
</child-doc>
</relation>
</division>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120206420</doc-number>
<kind>A1</kind>
<date>20120816</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Shieh</last-name>
<first-name>Kelvin</first-name>
<address>
<city>Tapei</city>
<country>TW</country>
</address>
</addressbook>
<residence>
<country>TW</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Shieh</last-name>
<first-name>Kelvin</first-name>
<address>
<city>Tapei</city>
<country>TW</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Yee &#x26; Associates, P.C.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
<agent sequence="02" rep-type="attorney">
<addressbook>
<last-name>Toub</last-name>
<first-name>Libby Z.</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Internatinoal Business Machines Corporation</orgname>
<role>02</role>
<address>
<city>Armonk</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Vu</last-name>
<first-name>Kim</first-name>
<department>2666</department>
</primary-examiner>
<assistant-examiner>
<last-name>Hu</last-name>
<first-name>Fred</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Video-based handwriting input for biometric signature data collection is provided. The invention is implemented by finding all of the relative positions among all light points of light sources when all light sources mounted on a stylus are turned ON. An image is captured including effective strokes and digital data of pen pressure with a digital camera when a user writes on a writing surface with a stylus having a light source indicating x-y coordinates of strokes. The images are time-stamped. The stylus includes a micro-switch. When the stylus touches the writing surface, the micro-switch is turned on to let the light source indicating x-y coordinates of strokes turn ON and generate a light point. Effective stroke images and digital data of pen pressure are extracted by comparing the captured images with the light point on the stylus.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="248.41mm" wi="179.75mm" file="US08625900-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="240.45mm" wi="185.25mm" file="US08625900-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="254.68mm" wi="189.23mm" file="US08625900-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="233.85mm" wi="140.72mm" orientation="landscape" file="US08625900-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="252.14mm" wi="187.03mm" file="US08625900-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="251.71mm" wi="180.93mm" file="US08625900-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="186.52mm" wi="169.42mm" file="US08625900-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="181.69mm" wi="164.17mm" file="US08625900-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application claims priority to Taiwan Patent Application No. 95150066 entitled &#x201c;VIDEO-BASED BIOMETRIC SIGNATURE DATA COLLECTING METHOD AND APPARATUS&#x201d;, filed on Dec. 29, 2006, which is incorporated herein by reference and assigned to the assignee herein.</p>
<heading id="h-0002" level="1">FIELD OF INVENTION</heading>
<p id="p-0003" num="0002">The present invention generally relates to a handwriting input method and apparatus, and more particularly, to a handwriting input method and apparatus by capturing image data of real-time handwriting strokes and biometric data (e.g. pen pressure) with a digital camera for recognition or authentication.</p>
<heading id="h-0003" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0004" num="0003">With the fast development of computer technologies, there appear to be many information processing devices for accepting users' handwriting input with a handwriting recognition subsystem, such as personal digital assistants PDA or hand portable computers HPC. A handwriting recognition subsystem tends to be useful in the environment of inputting text into a small mobile device like a PDA, or inputting hard-to-enter characters like Chinese into a computer. Users can input handwritten data and symbols into computers by means of pen-like devices. Corresponding to this, there appear to be many handwriting character recognition devices, which can recognize a user's handwriting input.</p>
<p id="p-0005" num="0004">In the field of handwriting input, two approaches to handwriting character recognition are: on-line character recognition (OLCR) and optical character recognition (OCR). The OCR approach is sometimes also referred to as off-line handwriting recognition. In general, the on-line character recognition (OLCR) technique employs dynamic handwriting information, while the off-line handwriting recognition employs static handwriting information. All OCR systems generally use an input device, such as an optical scanner, for reading text from existing documents into a computer, such as to an image file, and process the image file by data perceiving and data re-construction (e.g. analyze the patterns and identify the characters they represent) to produce a text file for editing or other use later. Relative to the OLCR technique, as the OCR technique cannot obtain real-time dynamic handwriting information such as stroke direction, stroke order, pen tip pressure or speed, etc., as features, the recognition rate will be affected.</p>
<p id="p-0006" num="0005">The OLCR technique uses a stylus as a handwriting input device to write characters one by one on a digitizing tablet, and then recognizes these characters by a recognition software. In addition to strokes, OLCR technique employs dynamic handwriting information, such as stroke direction, stroke order, tip pressure or speed, etc., as features, it provides generally better recognition accuracy, and is used widely for current handwriting input devices. See the publications entitled, &#x201c;On-line signature verification by adaptively weighted Dmatching,&#x201d; Authors: Zhao, P. (CADIX Inc., Tokyo, Japan); Higashi, A.; Sato, Y. Source: IEICE Transactions on Information and Systems, Vol. E79-D, No. 5, May 1996, p. 535-541; &#x201c;Recognition of human signatures,&#x201d; Authors: Pacut, A. (Warsaw Univ. of Technol., Poland); Czajka, A. Source: IJCNN&#x2032;OI. International Joint Conference on Neural Networks, Proceedings (Cat. No. 01CH37222), 2001, pt. 2, p. 1560-4 vol. 2; and &#x201c;Biometric verification in dynamic writing,&#x201d; Authors: George, S. E. (Sch. of Comput. &#x26; Inf. Sci., Univ. of South Australia, Adelaide, SA, Australia) Source: Proceedings of the SPIE&#x2014;The International Society for Optical Engineering, v 4738, 2002, p 125-32.</p>
<p id="p-0007" num="0006">One of the applications of handwriting recognition today is signature recognition for biometric identification and/or verification, most typically used for retail or safeguarding applications, etc. In these applications, the underlying method is to consider the writing pressure (i.e. pen pressure) of a stylus or a pen on a writing surface, in addition to the sequence of x,y-coordinates, as biometric information of a person is considered as a basis for authentication. The authentication technique based on signature verification always utilizes a pressure-sensitive pen and a tablet to record a user's signature. Signature verification then compares the user's signature against a stored signature sample corresponding to the same user, and determines true or false to identify the user.</p>
<p id="p-0008" num="0007">At present, conventional handwriting input devices utilizing OLCR technique usually request a touch-sensitive pad (e.g. digitizing tablet) which incorporates either magnetic sensor or pressure sensor to sense and record the pen strokes that are touching the pad surface. The conventional digitizing tablet usually has a wire connecting an external smart stylus.</p>
<p id="p-0009" num="0008">The IBM's ThinkScribe&#x2122; is a device integrating a handwriting digitizer having a digitizing tablet with a traditional paper-based recording system. The digitizing tablet includes an active area capable of receiving electromagnetic signals from a radio frequency coupled stylus. This device records a user's handwriting input in strokes and associated timing and can reproduce the user's handwriting input according to the original timing information. A user may write the documents to be transmitted on the digitizing tablet or paper. The digitizing tablet generates a data flow representative of strokes and the associated events, and records the data flow in a nonvolatile memory. The associated events in the data flow may be generally categorized as being either automatically generated by the input device or as being user-invoked. Automatically generated events are events which occur and are detected and recorded without specific input from the user. For example, there may be defined a pen-down event which indicates that the stylus was brought into contact with the writing surface and a pen-up event which indicates that the stylus was lifted from the writing surface. An &#x201c;ink trace&#x201d; may thus be defined as a series of pen coordinates recorded between a pen-down and a pen-up event.</p>
<p id="p-0010" num="0009">All of the input devices mentioned above require a touch-sensitive pad which incorporates either a magnetic sensor or pressure sensor to sense and record the pen strokes that are touching the pad's surface. The pad may be provided at an additional cost to an existing PDA or a personal computer. The pad is also large in size which either is difficult to carry, or it occupies the screen area when it is built onto a PDA and in operation. The pad usually has a wire connecting the pad to the computer, and a wire connecting the pen to the pad. The situation sometimes is a hassle. Besides, considering the identification or verification of the user of a low-cost and low computational power device, such as a mobile phone or a PDA, our choices would then be limited, since it would be unreasonable to attach a high-cost device to collect biometric data for identification or verification, or to adopt a method which needs a lot of computation power to identify or verify the identity of the user. Thus, there is a need to provide a low-cost method and device for signature recognition or authentication which is capable of providing more accurate biometric data (signature data) to a mobile phone or a PDA which only needs reasonable computation power.</p>
<p id="p-0011" num="0010">In other types of handwriting recognition systems, a pure digital camera input may be used for the sake of handwriting recognition, however, the processing is complicated and the results may not be good. For instance, disclosed in U.S. Pat. No. 6,044,165, assigned to California Institute of Technology, is a technique that uses a digital camera which monitors movement of a writing implement relative to a writing surface, and associated processing hardware which processes the output of the camera to track that movement. However, there are no disclosures to collect biometric data for identification or verification utilizing a digital camera.</p>
<p id="p-0012" num="0011">As many computer systems (e.g. notebook PC, pervasive device, PDA etc.) are increasingly entering the market equipped with an embedded digital camera of relatively high resolution, it would be advantageous to provide digital video data with the digital camera for use in the handwriting recognition process in such pervasive devices. Accordingly, there is a need to provide an easier low-cost solution to collect biometric data (e.g. signature data) for recognition or authentication which enables a user to write on a paper without a touch-sensitive pad and a wire connecting the sensor to a computer (or a pervasive device and the like) and a wire connecting a stylus (or a pen) to the pad, but equipped with a low-cost digital camera functioning to overcome the known drawbacks mentioned above.</p>
<heading id="h-0004" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0013" num="0012">It is thus an object of the present invention to provide a simple, low-cost, wireless method and apparatus of collecting biometric data (e.g. signature data) for recognition or authentication which enables a user to write on a paper without a touch sensitive pad provided for a computer system (such as Notebook and PDA etc.) equipped with an embedded digital camera. The purpose of the present invention is not the identification/verification algorithm itself, but the data acquisition method.</p>
<p id="p-0014" num="0013">This invention discloses the retrieval of an accumulated locus (e.g. effective strokes) of the light point of a stylus, not an ink locus itself, and pen pressure data. Thus, it is another object of the present invention to provide a method and apparatus for handwriting input with better recognition accuracy which can be used for a real paper environment or for a non-paper environment on which there is not any ink locus.</p>
<p id="p-0015" num="0014">The present invention discloses a method and an apparatus, as well as a computer recording medium for collecting biometric data (signature data) for recognition or authentication. The invention is implemented by finding all relative positions among all light points of light sources when all light sources mounted on a stylus are turned ON; capturing an image including effective strokes and digital data of pen pressure with a digital camera, when a user writes on a writing surface with a stylus having a light source (e.g. LED) indicating x-y coordinates of strokes, said images being time-stamped; wherein the stylus comprises a micro-switch, and when the stylus touches the writing surface, the micro-switch is turned on to let the light source indicate x-y coordinates of strokes turn ON and generate a &#x201c;light point&#x201d;; and extracting effective stroke images and digital data of pen pressure by comparing the captured images with the light point of the light source indicating x-y coordinates of strokes on the stylus. Moreover, the invention discloses a low-cost, wireless method and apparatus for handwriting input which enables a user to write on a paper without a touch-sensitive pad provided for a computer system (such as Notebook and PDA etc.) equipped with an embedded digital camera.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION OF THE INVENTION</heading>
<p id="h-0006" num="0000">Terminology:</p>
<p id="p-0016" num="0015">The following terms will be used throughout the detailed description:</p>
<p id="p-0017" num="0016">Strokes&#x2014;the track of movement of a pen or stylus. The strokes comprise text, marks, lines, and/or shapes written on or in proximity to a writing surface or a surface of a paper, including the solid lines and broken lines shown in <figref idref="DRAWINGS">FIG. 7</figref>.</p>
<p id="p-0018" num="0017">Ink traces&#x2014;the ink traces comprise text written on a surface of a paper, as the solid lines shown in <figref idref="DRAWINGS">FIG. 7</figref>, indicative of the effective strokes, i.e. character itself.</p>
<p id="p-0019" num="0018">Biometric&#x2014;measurable biological characteristics. Biometrics is the study of automated methods for uniquely recognizing humans based on physiological or behavioral characteristics of an individual. There are several biometrics methods to identify or verify a person. Some examples of physiological characteristics include, but are not limited to, fingerprint, facial features, iris pattern, retinal pattern or hand geometry, etc., while examples of behavioral characteristics include signature pattern, voice characteristics, typing pattern or keystroke dynamics, etc.</p>
<p id="p-0020" num="0019">The following will describe the embodiment of the present invention. The disclosed embodiment is only for illustration, thus it will be understood by those skilled in the art that there are many modifications and changes made therein without departing from the spirit and scope of the invention. Throughout the appended drawings, like features are identified by like reference numerals.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 1</figref> &#x26; <figref idref="DRAWINGS">FIG. 2</figref> respectively illustrate different information processing systems implementing the present invention. In <figref idref="DRAWINGS">FIG. 1</figref>, a PDA <b>100</b> or a similar pervasive computing device includes an embedded digital camera <b>110</b> and a strokes/pen pressure retrieving module (not shown). The digital camera <b>110</b> can be a charge coupled device (CCD) or complementary metal oxide semiconductor (CMOS) digital camera or an infrared camera.</p>
<p id="p-0022" num="0021">When a user writes with a stylus or pen <b>130</b> on a writing surface or a paper <b>140</b>, the digital camera <b>110</b> will capture images including real-time strokes. The strokes/pen pressure retrieving module includes the function of producing time-stamped dynamic video data output corresponding to the images of the real-time strokes of the movement of the stylus <b>130</b>. Similarly, when the user writes on the writing surface, the module includes the function of producing time-stamped digital data representing pen pressure corresponding to the images of the real-time strokes from the images captured by the digital camera <b>110</b>. There is a light source <b>181</b> (e.g. LED) mounted beside the tip <b>150</b> of the stylus <b>130</b> for the module to extract the images of the real-time effective strokes from the images captured by the digital camera <b>110</b>. A PDA or a similar pervasive device can receive the images captured by the digital camera <b>110</b>, and then extract the images of the real-time effective strokes for recognition according to the strokes/pen pressure retrieving module of the present invention shown in <figref idref="DRAWINGS">FIG. 5</figref>.</p>
<p id="p-0023" num="0022">Similarly, the computer system <b>200</b> of <figref idref="DRAWINGS">FIG. 2</figref> includes a host <b>210</b>, screen <b>220</b> and a strokes/pen pressure retrieving module (not shown). There is a digital camera <b>110</b> embedded on screen <b>220</b> to capture images of real-time strokes for producing time-stamped dynamic video data and time-stamped digital data representing pen pressure corresponding to the images of the real-time strokes.</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 3</figref> shows stylus <b>130</b> used for the invention. The stylus <b>130</b> includes a micro-switch <b>160</b>, a pressure sensor <b>162</b>, an analog to digital converter (A/D converter) <b>164</b>, a micro-controller <b>166</b>, a memory <b>168</b>, a battery <b>170</b>, and <b>4</b> light sources, each of which is located at a different position (e.g. LEDs <b>181</b>, <b>182</b>, <b>183</b> &#x26; <b>184</b>).</p>
<p id="p-0025" num="0024">The pressure sensor <b>162</b> associated with the tip <b>150</b> of stylus <b>130</b> may preferably be a low-cost piezoelectric device which converts the pen pressure of the stylus tip to voltage signals representing pen pressure. A/D converter <b>164</b> converts the voltage signals representing pen pressure to digital data of pen pressure for micro-controller <b>166</b>.</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 4</figref> depicts the flowchart of a monitor module in micro-controller <b>166</b> embedded in stylus <b>130</b> according to the present invention. Step <b>401</b> determines if stylus <b>130</b> touches the writing surface. When stylus <b>130</b> is brought into contact with the writing surface, micro-switch <b>160</b> is turned ON, light source (LED) <b>181</b> indicating x, y coordinates of strokes, which is located at the first location on stylus <b>130</b>, is energized by battery <b>170</b> and is also turned ON to generate a &#x201c;light point&#x201d;. At the same time, as micro-switch <b>160</b> is ON, an ON signal <b>161</b> is generated to inform micro-controller <b>166</b> for sending out a sampling signal <b>167</b> to A/D converter <b>164</b> and starting periodically to sample the output voltage signal of piezoelectric sensor <b>162</b> corresponding to the pen pressure and to receive the digital data of pen pressure converted by A/D converter <b>164</b>, such as 8-bit digital data (Step <b>403</b>). The frequency of sampling signal <b>167</b> is the same as the frequency of taking images with digital camera <b>110</b> embedded in PDA <b>100</b> or computer system <b>200</b>, respectively shown in <figref idref="DRAWINGS">FIG. 1</figref> or <figref idref="DRAWINGS">FIG. 2</figref>. The preferred frequency of the sampling signal is about 30 Hz. At step <b>405</b>, micro-controller <b>166</b> will compress/encode the digital data of pen pressure via a conduit such as the conventional differential pulse code modulation (DPCM) to be preferably 3 bits and store the digital data in memory <b>168</b>, e.g. first-in-first-out (FIFO). There are other compressing/encoding techniques available, e.g. ADPCM (Adapter Differential PCM) or ADM (Adaptive Delta Modulation) etc. Finally, at step <b>407</b>, through the ON/OFF switching of the second, third, and fourth light sources (LED <b>182</b>, <b>183</b>, <b>184</b>) for transmission of pen pressure data, respectively located at different locations on stylus <b>130</b>, the compressed/encoded digital data of pen pressure is further transmitted in the form of light signals to PDA <b>100</b> or computer system <b>200</b> for reading.</p>
<p id="p-0027" num="0026">Step <b>409</b> determines whether stylus <b>130</b> is lifted off of the writing surface. If not determined to have occurred yet, repeat steps <b>403</b>, <b>405</b>, and <b>407</b>. When the stylus lifts off of the writing surface, micro-switch <b>160</b> is turned OFF. The battery <b>170</b> does not energize the light source (LED) <b>181</b>, light source <b>181</b> is turned OFF and extinguished, and then the light point disappears. Simultaneously, micro-controller <b>166</b> will also stop sampling the output voltage signal of piezoelectric sensor <b>162</b>.</p>
<p id="p-0028" num="0027">The light sources (LEDs <b>181</b>, <b>182</b>, <b>183</b>, <b>184</b>) having different locations as mentioned above may be replaced with LEDs having different colors (e.g. White, Red, Green or Blue color). The compressed/encoded digital data of pen pressure is transmitted to PDA <b>100</b> or computer system <b>200</b> through the ON/OFF switching of the light sources <b>182</b>, <b>183</b>, <b>184</b> having different colors. The light sources for transmission of compressed/encoded digital data of pen pressure are not only limited to 3 as mentioned in the embodiment herein. There may be different numbers of light sources adopted for different applications and resolution needs.</p>
<p id="p-0029" num="0028">The stylus <b>130</b> is basically a pen (inked pen) having an ink cartridge <b>152</b>, which can write on real paper, or a plastic stylus not having an ink cartridge <b>152</b>, which cannot write on real paper to leave ink traces. The light source <b>181</b> is preferably positioned beside the tip <b>150</b> of the stylus. The light point presented by light source <b>181</b> is provided as a reference for the handwriting input module to extract desired real-time effective strokes from images captured by digital camera <b>110</b>. The light source <b>181</b> is not limited to be mounted in the vicinity of the tip <b>150</b>, when implementing the present invention. In addition, a plastic stylus, which cannot write on real paper to leave ink traces, may directly provide the light source <b>181</b> (e.g. LED light) positioned at the tip <b>150</b>.</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 5</figref> depicts the flowchart of a handwriting input process embodiment which may be a program that is embodied in software or firmware and that is executed by PDA <b>100</b> or computer system <b>200</b> according to the present invention. A-A&#x2032; portion shown in the <figref idref="DRAWINGS">FIG. 5</figref> is the strokes/pen pressure retrieving module according to the present invention. At first, at step <b>410</b>, a user performs a step of defining a target area prior to handwriting input to define a target area on a writing surface (<b>140</b>) for the user's handwriting input. The target area is the effective area for digital camera (<b>110</b>) capturing images. As shown in <figref idref="DRAWINGS">FIG. 6</figref>, the user can interact with a computer system via screen (<b>220</b>) to define the target area on the writing surface. Moreover, the image of a rectangle on the writing surface taken by the digital camera will become a trapezoid. Accordingly, after defining the target area on the writing surface, a simple mathematical transformation can be performed to transform the trapezoid back into a rectangle (<b>190</b>) shown on the screen (<b>220</b>). The mathematical transformation is easily accomplished by those persons skilled in the art. As such, after obtaining the images of strokes, a mathematical transformation for the images of strokes is required as depicted in the subsequent step <b>433</b>. Actually, it should be noted that as the transformation can be performed in the subsequent handwriting recognition process, the mathematical transformation is not required during the handwriting input process.</p>
<p id="p-0031" num="0030">Then, an initializing step <b>420</b> is performed. The user firstly turns ON the light source <b>181</b> representing x, y coordinates and the other light sources <b>182</b>, <b>183</b>, <b>184</b> located at different locations for representing digital data of pen pressure. The strokes/pen pressure retrieving module then finds all of the relative positions among all light points of light sources.</p>
<p id="p-0032" num="0031">When the user contacts the target area of the writing surface with the stylus <b>130</b> having a light source <b>181</b>, the light source <b>181</b> is turned ON to generate a light point. Through the light point, the images captured by the digital camera <b>110</b> will be processed according to the present invention to produce video data corresponding to the images of effective strokes and the digital data of pen pressure (step <b>430</b>).</p>
<p id="p-0033" num="0032">The step <b>430</b> of obtaining the images of effective strokes and digital data of pen pressure is divided into 3 sub-steps, which are depicted as follows:</p>
<p id="p-0034" num="0033">At step <b>431</b>, a step of capturing and time-stamping images is performed. The digital camera takes images generally at the rate of 30 frames per second. In addition to a sequence of images within the target area of the writing surface captured by the digital camera, the images are time-stamped. Next, at step <b>432</b>, a step of extracting images of effective strokes and digital data representing pen pressure from the captured images is performed to filter out all irrelevant images, all not related to the strokes (e.g. images of hand and stylus), and leave only the locus of the light point of the light source <b>181</b> on the stylus, i.e. &#x201c;effective strokes&#x201d; and the digital data representing pen pressure by comparing the captured images of frames with the light point of the light source <b>181</b> on the stylus. For example, the &#x201c;intensity&#x201d; of the light point of the stylus <b>130</b> can be set as a &#x201c;threshold&#x201d; value to filter out those captured images of which the intensity is below the threshold, and to leave only the locus of the light point on the stylus, i.e. &#x201c;effective strokes&#x201d; and the digital data representing pen pressure. Moreover, if the light source is an infrared light source, similarly the &#x201c;intensity&#x201d; of the infrared can be set as a &#x201c;threshold&#x201d; value to filter out those captured images of which the intensity is below the threshold, and to leave only the locus of the light point on the stylus, i.e. &#x201c;effective strokes&#x201d; and the digital data representing pen pressure.</p>
<p id="p-0035" num="0034">As the relative positions among light sources <b>181</b>-<b>184</b> have been found during initializing, the sequence of light sources <b>182</b>-<b>184</b> from most significant bit (MSB) to least significant bit (LSB) can be determined to obtain digital data. For example, if the outputs of light sources <b>182</b>-<b>184</b> for representing digital data of pen pressure are (ON, OFF, ON) in sequence, it will be recognized as (1, 0, 1).</p>
<p id="p-0036" num="0035">At step <b>433</b>, a mathematical transformation and normalization step of the images of effective strokes is performed, similar to the descriptions related to <figref idref="DRAWINGS">FIG. 6</figref> mentioned above, to transform and normalize the time-stamped images of effective strokes in order to correct skew of the images of effective strokes when the digital camera takes images. Similarly, as the transformation can be performed in the subsequent handwriting recognition process, step <b>433</b> is not required during the handwriting input process. Additionally, if step <b>405</b> of compressing and encoding the digital data of pen pressure has been performed within stylus <b>130</b>, the corresponding step of decompressing and decoding the compressed/encoded digital data of pen pressure will be also performed at the step <b>433</b>.</p>
<p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. 7</figref> depicts a sequence of image frames taken by the digital camera. As the digital camera takes images at the rate of 30 frames per second, only parts of frames are shown in <figref idref="DRAWINGS">FIG. 7</figref>. The digital camera captures all images shown in the view field within the target area. Accordingly, if the user writes on a writing surface, e.g. a paper, with an inked pen/stylus having a light source, which can write on real paper, there should be images of ink traces, the stylus, the light point on the stylus, and the user's hand actually shown in each frame.</p>
<p id="p-0038" num="0037">For convenience in understanding, the traces of the movement of the light point in each frame of <figref idref="DRAWINGS">FIG. 7</figref> are depicted as an &#x201c;accumulated locus&#x201d;. The strokes shown in <figref idref="DRAWINGS">FIG. 7</figref> include solid lines and broken lines, in which the ink traces of the pen represented by the solid lines overlap or parallel the accumulated locus of the light point on the stylus, and the broken lines represent the accumulated locus of the tip <b>150</b> on the stylus when the pen is lifted from the writing surface (that is, the light source <b>181</b> is turned OFF, and then the light point disappears). Accordingly, each frame of <figref idref="DRAWINGS">FIG. 7</figref> will include the images of ink traces, the stylus, the &#x201c;accumulated locus&#x201d; of the light point on the stylus, and the user's hand, in which the &#x201c;accumulated locus&#x201d; of the light point on the stylus constitutes the desired &#x201c;effective strokes&#x201d;.</p>
<p id="p-0039" num="0038">The images of each frame will actually not include the images of the broken lines in <figref idref="DRAWINGS">FIG. 7</figref>, and the image of the broken lines in each frame will only include the extinguished (OFF) light source <b>181</b> beside the tip <b>150</b> of the stylus or pen <b>130</b>, located at a certain position of the broken lines of each frame (not light point, as the pen is lifted from the writing surface, the light source <b>181</b> is turned OFF, and then the light point disappears). By utilizing the light point on the stylus, the present invention can be performed to easily filter out the images of ink traces, the stylus other than the light point, and the user's hand, and only leave the video data of the accumulated focus of the light point into storage. After performing step <b>432</b>, the video data stored in storage corresponds to the &#x201c;effective strokes&#x201d;, which are constituted by the accumulated focus of the light point on the stylus, i.e. the solid lines in the frame <b>9</b> of <figref idref="DRAWINGS">FIG. 7</figref>. The ink traces on the paper correspond to the effective strokes described herein.</p>
<p id="p-0040" num="0039">If the user writes on a non-paper writing surface with a stylus/pen having the light source <b>181</b>, there are no ink traces of strokes. Accordingly, frames taken by the digital camera will only include the stylus, the light point, and the image of the user's hand, without ink traces as shown in <figref idref="DRAWINGS">FIG. 7</figref>. The accumulated locus of the light point on the stylus constitutes the desired effective strokes.</p>
<p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. 8</figref> further depicts an example of performing step <b>430</b> in which there are no results of light sources <b>182</b>-<b>184</b> representing digital data of pen pressure shown. When writing &#x201c;I&#x201d; on a writing surface, the images of effective strokes <b>700</b> captured by digital camera <b>110</b> include solid lines (&#x201c;I&#x201d;) shown in <figref idref="DRAWINGS">FIG. 8</figref>, i.e. segments <b>1</b>,<b>3</b>, and <b>5</b> after completing step <b>430</b>. In fact, the solid lines (&#x201c;I&#x201d;) and the broken lines are the accumulated locus of the light source <b>181</b> beside the tip <b>150</b> of the stylus respectively when the stylus is brought into contact with the writing surface and when the stylus is lifted off the writing surface. The periods <b>2</b> and <b>4</b> indicate the periods when the light point is OFF or extinguished when the stylus is lifted off of the writing surface, and correspond to the segments <b>2</b> and <b>4</b> of the broken lines of the images of strokes. Accordingly, the &#x201c;intensity&#x201d; of the light point of the stylus <b>130</b> is set as a &#x201c;threshold&#x201d; value to filter out those captured strokes images of which the intensity is below the threshold (e.g. the images of the user's hand and the stylus, and segments <b>2</b> and <b>4</b> of the broken lines), and leave only the locus of the light point on the stylus, i.e. &#x201c;effective strokes&#x201d; as the solid lines &#x201c;I&#x201d; shown in <figref idref="DRAWINGS">FIG. 8</figref>. Similarly, the effective strokes shown in <figref idref="DRAWINGS">FIG. 7</figref> are processed after step <b>430</b>, the result of which will appear as shown in <figref idref="DRAWINGS">FIG. 9</figref> in which there are only the solid lines from <figref idref="DRAWINGS">FIG. 7</figref> stored in storage.</p>
<p id="p-0042" num="0041">The effective strokes and the digital data of pen pressure obtained after performing step <b>430</b> will be fed to a handwriting recognition engine for recognition (Step <b>460</b>). The object of the present invention is to provide effective strokes of handwriting input and the digital data of pen pressure for recognition, but not the recognition engine itself. Therefore, the handwriting recognition engine can be any suitable handwriting recognition system.</p>
<p id="p-0043" num="0042">The present invention is to retrieve the accumulated locus (e.g. effective strokes) of a light point on a stylus, other than the ink locus. Therefore, the method of handwriting input according to the present invention can be used for a real paper environment or for a non-paper environment on which there are no ink traces of strokes.</p>
<p id="p-0044" num="0043">Although the above illustrates the preferred embodiment of the present invention, it is realized that the present invention can be implemented in hardware, software or any combination thereof. Therefore, there exist other changes in form and details. For example, as an alternative, the mentioned light sources <b>182</b>-<b>184</b> on the stylus representing digital data of pen pressure can be arranged to have colors different from that of the light source <b>181</b> indicating x-y coordinates of strokes (e.g. as the preferred alternative embodiment of the present invention, the light sources <b>182</b>-<b>184</b> are respectively Red, Green and Blue LEDs, while the light source <b>181</b> is a White LED). As such, the effective strokes can be directly filtered out via the white light, while the desired digital data of pen pressure can further be filtered out respectively via red, green and blue lights. Accordingly, the initializing step <b>420</b> shown in <figref idref="DRAWINGS">FIG. 5</figref> mentioned above can be omitted.</p>
<p id="p-0045" num="0044">It is important to note that those of ordinary skill in the art will appreciate that the processes of the present invention are capable of being distributed in the form of a computer readable medium of instructions and a variety of forms and that the present invention applies equally regardless of the particular type of signal bearing media actually used to carry out the distribution. Examples of computer readable media include recordable-type media, such as a floppy disk, a hard disk drive, a RAM, CD-ROMs, DVD-ROMs, and transmission-type media, such as digital and analog communications links, wired or wireless communications links using transmission forms, such as, for example, radio frequency and light wave transmissions. The computer readable media may take the form of coded formats that are decoded for actual use in a particular data processing system.</p>
<p id="p-0046" num="0045">The foregoing is illustrative of the present invention and is not to be construed as limiting thereof. Although a few exemplary embodiments of this invention have been described, those skilled in the art will readily appreciate that many modifications are possible in the exemplary embodiments without materially departing from the novel teachings and advantages of this invention. Accordingly, all such modifications are intended to be included within the scope of this invention as defined in the claims. In the claims, means-plus-function clauses, where used, are intended to cover the structures described herein as performing the recited function and not only structural equivalents but also equivalent structures. Therefore, it is to be understood that the foregoing is illustrative of the present invention and is not to be construed as limited to the specific embodiments disclosed, and that modifications to the disclosed embodiments, as well as other embodiments, are intended to be included within the scope of the appended claims. The invention is defined by the following claims, with equivalents of the claims to be included therein.</p>
<heading id="h-0007" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0047" num="0046">In order to further understand the invention and advantages thereof, the following detailed description of the disclosed embodiment will be considered in conjunction with the accompanying drawings, in which:</p>
<p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. 1</figref> &#x26; <figref idref="DRAWINGS">FIG. 2</figref> respectively illustrate different information processing systems implementing the present invention;</p>
<p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. 3</figref> shows a stylus <b>130</b> used for the present invention;</p>
<p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. 4</figref> depicts the flowchart of a monitor module in a micro-controller <b>166</b> embedded in the stylus <b>130</b> according to the present invention;</p>
<p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. 5</figref> depicts the flowchart of a handwriting input process embodiment according to the present invention;</p>
<p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. 6</figref> depicts that the user can interact with a computer system via screen <b>220</b> to define the target area on the writing surface;</p>
<p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. 7</figref> depicts a sequence of image frames taken by a digital camera;</p>
<p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. 8</figref> depicts an example of performing step <b>430</b>; and</p>
<p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. 9</figref> depicts the results of processing the effective strokes shown in <figref idref="DRAWINGS">FIG. 7</figref> via step <b>430</b>, in which there are only the solid lines in <figref idref="DRAWINGS">FIG. 7</figref> stored in storage.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method of retrieving pen pressure for transmission, the method comprising:
<claim-text>periodically sampling an electrical signal representing the pen pressure outputted from a pressure sensor disposed on a pen device when writing with the pen device, in response to an ON signal from a micro-switch disposed on the pen device when the pen device touches a writing surface;</claim-text>
<claim-text>receiving digital data of pen pressure converted by an A/D converter, wherein a micro-controller compresses and encodes the digital data of pen pressure outputted from the A/D converter from a first number of bits of digital data to a second number of bits of compressed and encoded digital data that is less than the first number of bits; and</claim-text>
<claim-text>outputting the compressed and encoded digital data of pen pressure via a plurality of light sources on the pen device each with a different color, wherein each of the second number of bits of compressed and encoded digital data corresponds to a different one of the plurality of light sources on the pen device each with the different color.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first number of bits of digital data of pen pressure is 8 bits.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the second number of bits of compressed and encoded digital data of pen pressure is 3 bits.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the pressure sensor is a piezoelectric device.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the micro-controller compresses and encodes the digital data of pen pressure using one of DPCM, ADPCM or ADM.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a number of the plurality of light sources each with the different color for the outputting of the compressed and encoded digital data of pen pressure is 3.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a light source indicating x-y coordinates of strokes and the plurality of light sources each with the different color for the outputting of the compressed and encoded digital data of pen pressure are LEDs of different colors.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the electrical signal representing the pen pressure outputted from the pressure sensor disposed on the pen device is sampled at a frequency of 30 hertz.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the micro-controller transmits the compressed and encoded digital data of pen pressure as light signals using the plurality of light sources each with the different color. </claim-text>
</claim>
</claims>
</us-patent-grant>
