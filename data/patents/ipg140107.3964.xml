<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625033-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625033</doc-number>
<kind>B1</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12697880</doc-number>
<date>20100201</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<us-term-of-grant>
<us-term-extension>922</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>5</main-group>
<subgroup>14</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>348701</main-classification>
<further-classification>348700</further-classification>
<further-classification>382181</further-classification>
<further-classification>382224</further-classification>
</classification-national>
<invention-title id="d2e53">Large-scale matching of audio and video</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>4811399</doc-number>
<kind>A</kind>
<name>Landell et al.</name>
<date>19890300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6236758</doc-number>
<kind>B1</kind>
<name>Sodagar et al.</name>
<date>20010500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6585521</doc-number>
<kind>B1</kind>
<name>Obrador</name>
<date>20030700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6766523</doc-number>
<kind>B2</kind>
<name>Herley</name>
<date>20040700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6782186</doc-number>
<kind>B1</kind>
<name>Covell et al.</name>
<date>20040800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6944632</doc-number>
<kind>B2</kind>
<name>Stern</name>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>7043473</doc-number>
<kind>B1</kind>
<name>Rassool et al.</name>
<date>20060500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>7174293</doc-number>
<kind>B2</kind>
<name>Kenyon et al.</name>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>7386479</doc-number>
<kind>B2</kind>
<name>Mizuno</name>
<date>20080600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>8069176</doc-number>
<kind>B1</kind>
<name>Ioffe et al.</name>
<date>20111100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>707747</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>8184953</doc-number>
<kind>B1</kind>
<name>Covell et al.</name>
<date>20120500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>386248</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>8238669</doc-number>
<kind>B2</kind>
<name>Covell et al.</name>
<date>20120800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382224</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2002/0133499</doc-number>
<kind>A1</kind>
<name>Ward et al.</name>
<date>20020900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2003/0033223</doc-number>
<kind>A1</kind>
<name>Mizuno</name>
<date>20030200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2003/0093790</doc-number>
<kind>A1</kind>
<name>Logan et al.</name>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2004/0025174</doc-number>
<kind>A1</kind>
<name>Cerrato</name>
<date>20040200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>2004/0031058</doc-number>
<kind>A1</kind>
<name>Reisman</name>
<date>20040200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>2004/0128682</doc-number>
<kind>A1</kind>
<name>Liga et al.</name>
<date>20040700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>2004/0199387</doc-number>
<kind>A1</kind>
<name>Wang et al.</name>
<date>20041000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>2005/0086682</doc-number>
<kind>A1</kind>
<name>Burges et al.</name>
<date>20050400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>2005/0096920</doc-number>
<kind>A1</kind>
<name>Matz et al.</name>
<date>20050500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>2005/0147256</doc-number>
<kind>A1</kind>
<name>Peters et al.</name>
<date>20050700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>2005/0193016</doc-number>
<kind>A1</kind>
<name>Seet et al.</name>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>2005/0283792</doc-number>
<kind>A1</kind>
<name>Swix et al.</name>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>2006/0080356</doc-number>
<kind>A1</kind>
<name>Burges et al.</name>
<date>20060400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>2007/0124756</doc-number>
<kind>A1</kind>
<name>Covell et al.</name>
<date>20070500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>2007/0130580</doc-number>
<kind>A1</kind>
<name>Covell et al.</name>
<date>20070600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>2007/0143778</doc-number>
<kind>A1</kind>
<name>Covell et al.</name>
<date>20070600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>2008/0090551</doc-number>
<kind>A1</kind>
<name>Gidron et al.</name>
<date>20080400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>2008/0263041</doc-number>
<kind>A1</kind>
<name>Cheung</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00031">
<othercit>U.S. Appl. No. 11/765,292, filed Jun. 19, 2007, 39 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00032">
<othercit>U.S. Appl. No. 12/042,138, filed Mar. 4, 2008, 38 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00033">
<othercit>U.S. Appl. No. 12/237,397, filed Sep. 25, 2008, 34 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00034">
<othercit>U.S. Appl. No. 12/536,907, filed Aug. 6, 2009, 51 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00035">
<othercit>U.S. Appl. No. 12/569,827, filed Sep. 29, 2009, 56 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00036">
<othercit>U.S. Appl. No. 11/468,265, filed Aug. 29, 2006, Covell et al.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00037">
<othercit>U.S. Appl. No. 11/766,594, filed Jun. 21, 2007, Baluja et al.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00038">
<othercit>Baluja, S., et al., &#x201c;Content Fingerprinting Using Wavelets,&#x201d; 3rd European Conference on Visual Media Production (CVMP 2006), Part of the 2nd Multimedia Conference, 2006, 10 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00039">
<othercit>Baluja, S., et al., &#x201c;Learning &#x201c;Forgiving&#x201d; Hash Functions: Algorithms and Large Scale Tests,&#x201d; Proc. Int. Joint Conf. Artif. Intell, 2007, pp. 2663-2669.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00040">
<othercit>Baluja, S., et al., &#x201c;Waveprint: Efficient wavelet-based audio fingerprinting,&#x201d; Pattern Recognition, 2008, 3 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00041">
<othercit>Burges, C., et al., &#x201c;Duplicate Detection and Audio Thumbnails with Audio Fingerprinting&#x201d; Technical Report MSR-TR-2004-19, Microsoft Corporation, 2004, 5 pages [online] [retrieved on Nov. 21, 2006] Retrieved from the internet: &#x3c;URL: www.research.microsoft.com/&#x2dc;cburges/tech<sub>&#x2014;</sub>reports/tr-2004-19.pdf&#x3e;.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00042">
<othercit>Burges, C., et al., &#x201c;Using Audio Fingerprinting for Duplicate Detection and Thumbnail Generation,&#x201d; Microsoft Research, Microsoft Corporation, Mar. 2005, 4 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00043">
<othercit>Cano, P., et al., &#x201c;A Review of Algorithms for Audio Fingerprinting&#x201d; In Proc. of the IEEE MMSP, St. Thomas, Virgin Islands, 2002, 5 pages, [online] [retrieved on Nov. 21, 2006] Retrieved from the Internet: &#x3c;URL: www.iua.upf.es/mtg/publications/MMSP-2002-pcano.pdf&#x3e;.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00044">
<othercit>Cohen et al., &#x201c;Finding Interesting Associations without Support Pruning,&#x201d; IEEE Transactions on Knowledge and Data Engineering, 2001, 12 pages, can be retrieved from the internet: &#x3c;URL: www.dbis.informatik.huberlin.de/dbisold/lehre/WS0405/kDD/paper/CDFG<sub>&#x2014;</sub>00.pdf&#x3e;.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00045">
<othercit>Covell, M., et al., &#x201c;Advertisement Detection and Replacement using Acoustic and Visual Repetition,&#x201d; IEEE 8<sup>th </sup>Workshop on Multimedia Signal Processing, Oct. 2006, 6 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00046">
<othercit>Covell, M., et al., &#x201c;LSH Banding for Large-Scale Retrieval With Memory and Recall Constraints,&#x201d; International Conference on Acoustics, Speech and Signal Processing (ICASSP-2009), 4 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00047">
<othercit>Dean, J., et al., &#x201c;MapReduce: Simplified Data Processing on Large Clusters,&#x201d; OSDI'04: Sixth Symposium on Operating System Design and Implementation, San Francisco, CA, Dec. 2004, 13 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00048">
<othercit>Haitsma, J., et al., &#x201c;A Highly Robust Audio Fingerprinting System&#x201d; IRCAM, 2002, 9 pages, [online] [retrieved on Nov. 16, 2006], Retrieved from the Internet: &#x3c;URL: www.ismir2002.ismir.net/proceedings/02-FP04-2.pdf&#x3e;.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00049">
<othercit>Jacobs et al., &#x201c;Fast Multiresolution Image Querying&#x201d; International Conference on Computer Graphics and Interactive Techniques, ACM, 1995, 10 pages, [online] [retrieved on Nov. 21, 2006]. Retrieved from the Internet: &#x3c;URL: www.grail.cs.washington.edu/projects/query/mrquery.pdf&#x3e;.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00050">
<othercit>Ke et al., &#x201c;Computer Vision for Music Identification&#x201d; IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2005, 8 pages, [online] [retrieved on Nov. 21, 2006], Retrieved from the Internet: &#x3c;URL: www.cs.cmu.edu/&#x2dc;yke/musicretrieval/cvpr2005-mr.pdf&#x3e;.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00051">
<othercit>Sadlier et al., &#x201c;Automatic TV Advertisement Detection from MPEG Bitstream,&#x201d; 2001, Pattern Recognition, vol. 35, Issue 12, pp. 2719-2726.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00052">
<othercit>Stollnitz et al., &#x201c;Wavelets for Computer Graphics: A Primer, Part 1,&#x201d; University of Washington, 1995, 8 pages, [online] [retrieved on Nov. 21, 2006]. Retrieved from the Internet: &#x3c;URL: www.grail.cs.washington.edu/pub/stoll/wavelet1.pdf&#x3e;.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00053">
<othercit>Stollnitz et al., &#x201c;Wavelets for Computer Graphics: A Primer, Part 2,&#x201d; University of Washington, 1995, 9 pages, [online] [retrieved on Nov. 21, 2006]. Retrieved from the Internet: &#x3c;URL: www.grail.cs.washington.edu/pub/stoll/wavelet2.pdf&#x3e;.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00054">
<othercit>Viola et al., &#x201c;Robust Real-Time Object Detection,&#x201d; Int. J. Computer Vision, 2002, 25 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00055">
<othercit>Wang, &#x201c;The Shazam Music Recognition Service,&#x201d; <i>Communications of the ACM</i>, Aug. 2006, 49(8): 5 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00056">
<othercit>Yang, C. &#x201c;MACS: Music Audio Characteristic Sequence Indexing for Similarity Retrieval&#x201d;, Oct. 2001, pp. 21-24, New Paltz, New York.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00057">
<othercit>&#x201c;Community&#x201d; definition, Oxford English Dictionary, 17 pages, [online] [retrieved on Apr. 27, 2009] Retrieved from the internet &#x3c;URL:http://dictionary.oed.com/cgi/entry/50045241?single=1&#x26;query<sub>&#x2014;</sub>type=word&#x26;queryword=community&#x3e;.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00058">
<othercit>&#x201c;Compression&#x201d; definition, Oxford English Dictionary, 4 pages, [online] [retrieved on Apr. 27, 2009] Retrieved from the internet &#x3c;URL:http://dictionary.oed.com/cgi/entry/50045890?single=1&#x26;query<sub>&#x2014;</sub>type=word&#x26;queryword=compression&#x3e;.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00059">
<othercit>&#x201c;Database&#x201d; definition, Oxford English Dictionary, 2 pages, [online] [retrieved on Apr. 27, 2009] Retrieved from the internet &#x3c;URL:http://dictionary.oed.com/cgi/entry/50057772?single=1&#x26;query<sub>&#x2014;</sub>type=word&#x26;queryword=database&#x3e;.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00060">
<othercit>&#x201c;Encrypt&#x201d; definition, Oxford English Dictionary, 1 page, [online] [retrieved on Apr. 27, 2009] Retrieved from the internet &#x3c;URL:http://dictionary.oed.com/cgi/entry/00292459?single=1&#x26;query<sub>&#x2014;</sub>type=word&#x26;queryword=encrypt&#x3e;.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00061">
<othercit>&#x201c;Shazam Experience Music&#x201d; SHAZAM Entertainment, 2002-2007, 2 pages, [online] [Retrieved on May 30, 2007]. Retrieved from the Internet: &#x3c;URL: www.shazam.com/music/portal/sp/s/media-type/html/user/anon/page/default/template/Myhome/music.html&#x3e;.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00062">
<othercit>&#x201c;Shazam Entertainment Brings Music Recognition to Windows Mobile 5.0 Powered Smartphones,&#x201d; Shazam Entertainment, Apr. 6, 2006, 1 page, [online][retrieved on Nov. 16, 2006]. Retrieved from the Internet: &#x3c;URL: www..shazam.com/music/portal/sp/s/media-type/html/user/anon/page/default/template/pages/p/company<sub>&#x2014;</sub>release30.html&#x3e;.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00063">
<othercit>&#x201c;CS276 Information Retrieval and Web Mining,&#x201d; STANFORD, 2005, 8 pages, [online] [retrieved on Nov. 16, 2006]. Retrieved from the Internet: &#x3c;URL: www.stanford.edu/class/cs276/handouts/lecture19.pdf&#x3e;.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00064">
<othercit>&#x201c;Data Mining: Associations,&#x201d; STANFORD, 2002, 11 pages, [online] [retrieved on Nov. 16, 2006]. Retrieved from the Internet: &#x3c;URL: www.stanford.edu/class/cs206/cs206-2.pdf&#x3e;.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00065">
<othercit>International Preliminary Report on Patentability, Application No. PCT/US06/45549 mailed Jun. 12, 2008, 7 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00066">
<othercit>European Search Report, EP Application No. 08 15 3719 mailed Sep. 26, 2008, 8 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00067">
<othercit>International Preliminary Report on Patentability, Application No. PCT/US06/45551 mailed Apr. 2, 2009, 11 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00068">
<othercit>PCT International Search Report for PCT/US2006/045549 dated Oct. 9, 2007, 10 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00069">
<othercit>PCT International Search Report for PCT/US2006/045551 dated Jul. 21, 2008, 20 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>31</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>348700</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348701</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382224</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382181</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>6</number-of-drawing-sheets>
<number-of-figures>9</number-of-figures>
</figures>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Marwood</last-name>
<first-name>David</first-name>
<address>
<city>San Leandro</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Keysers</last-name>
<first-name>Daniel</first-name>
<address>
<city>Adliswil</city>
<country>CH</country>
</address>
</addressbook>
<residence>
<country>CH</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Tucker</last-name>
<first-name>Richard</first-name>
<address>
<city>Zurich</city>
<country>CH</country>
</address>
</addressbook>
<residence>
<country>CH</country>
</residence>
</us-applicant>
<us-applicant sequence="004" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Postelnicu</last-name>
<first-name>Gheorghe</first-name>
<address>
<city>Zurich</city>
<country>CH</country>
</address>
</addressbook>
<residence>
<country>CH</country>
</residence>
</us-applicant>
<us-applicant sequence="005" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Covell</last-name>
<first-name>Michele</first-name>
<address>
<city>Palo Alto</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Marwood</last-name>
<first-name>David</first-name>
<address>
<city>San Leandro</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Keysers</last-name>
<first-name>Daniel</first-name>
<address>
<city>Adliswil</city>
<country>CH</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Tucker</last-name>
<first-name>Richard</first-name>
<address>
<city>Zurich</city>
<country>CH</country>
</address>
</addressbook>
</inventor>
<inventor sequence="004" designation="us-only">
<addressbook>
<last-name>Postelnicu</last-name>
<first-name>Gheorghe</first-name>
<address>
<city>Zurich</city>
<country>CH</country>
</address>
</addressbook>
</inventor>
<inventor sequence="005" designation="us-only">
<addressbook>
<last-name>Covell</last-name>
<first-name>Michele</first-name>
<address>
<city>Palo Alto</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Fenwick &#x26; West LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Google Inc.</orgname>
<role>02</role>
<address>
<city>Mountain View</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Pe</last-name>
<first-name>Geepy</first-name>
<department>2488</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Large-scale matching of videos is performed by matching a set of probe videos against a set of reference videos to determine if they are visually and/or aurally similar. The visual and audio fingerprints of all probe videos and reference videos are divided into subfingerprints, which are divided into LSH bands. The LSH bands of the probe videos are sorted in one list, and the LSH bands of the reference videos are sorted in another list. Then, the two sorted lists are linearly scanned for matching LSH bands. The matching LSH bands are sorted by probe video ID, and each probe video ID is searched to find matches between probe videos and reference videos. Further, an incremental matching process identifies matches as groups of new probe videos and/or new reference videos are added, without unnecessary repetition of matching old probe videos against old reference videos.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="205.06mm" wi="163.07mm" file="US08625033-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="242.82mm" wi="155.53mm" file="US08625033-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="186.77mm" wi="179.58mm" file="US08625033-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="223.69mm" wi="171.37mm" orientation="landscape" file="US08625033-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="231.31mm" wi="164.08mm" file="US08625033-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="246.46mm" wi="197.70mm" file="US08625033-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="229.28mm" wi="167.39mm" file="US08625033-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND</heading>
<p id="p-0002" num="0001">1. Field of the Invention</p>
<p id="p-0003" num="0002">This invention generally relates to matching audio and video files against a set of reference audio and video files.</p>
<p id="p-0004" num="0003">2. Description of the Related Art</p>
<p id="p-0005" num="0004">The sharing of video content on websites has developed into a worldwide phenomenon. On average, over 10,000 videos are posted to video sharing websites every day, and this number is increasing as the tools and opportunities for capturing video become easy to use and more widespread.</p>
<p id="p-0006" num="0005">It is desirable to identify audio and video files that are aurally or visually similar to other audio or video files in a collection. These audio or video file matches may be used to improve search results of users searching for particular content on the video sharing website, they may be used to recommend additional audio or video files that are similar to a file a user has already played, or they may be used to identify duplicates or near duplicates of content to content rights holders.</p>
<heading id="h-0002" level="1">SUMMARY</heading>
<p id="p-0007" num="0006">Embodiments of the invention include methods, computer-readable storage media, and computer systems for performing large-scale matching of audio and video files to other audio and video files from a collection. A probe set of videos is matched against a reference set of videos by determining if they are either visually or aurally similar. The visual and audio fingerprints of all probe videos and reference videos are divided into subfingerprints, which are themselves divided into Locality Sensitive Hashing (&#x201c;LSH&#x201d;) bands. The LSH bands of the probe videos are sorted into a list. The LSH bands of the reference videos are sorted into a second list. Then, the two sorted lists are linearly scanned for matching LSH bands between the probe videos and the reference videos. Matching LSH bands are then sorted by probe video IDs. Each probe video ID is searched for reference videos containing many matching LSH bands grouped by offset. The matches are stored for use in improving search results, identifying similar content, and identifying duplicates or near duplicates of content to rights holders.</p>
<p id="p-0008" num="0007">In another embodiment, an incremental process is used to identify matches as groups of new probe videos and/or new reference videos are added to the existing collection. All new probe videos are matched against all old reference videos. All (new and old) probe videos are matched against all new reference videos. Then, the identified matches from these two steps are added to the results of the previous full collection matching process (all old probe videos against all old reference videos) to identify the complete set of matches for all probe videos and all reference videos. Thus, the set of probe videos and reference videos can be iteratively built up without unnecessary and costly repetitive matching of old probe videos against old reference videos.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. 1A</figref> is a conceptual illustration of a video fingerprint divided into subfingerprints for storage in memory of a computer system. <figref idref="DRAWINGS">FIG. 1B</figref> is a conceptual illustration of a subfingerprint divided into LSH bands for storage in memory of a computer system.</p>
<p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. 2</figref> is a high-level block diagram of a computing environment according to one embodiment.</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 3</figref> is a high-level block diagram illustrating an example of a computer for use as a batch manager, a sort distributor, and/or a sort machine of <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 4</figref> is an example method of performing a batch match of probe videos against reference videos according to one embodiment.</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIGS. 5A-5B</figref> illustrate two options for dividing the tasks of matching probe videos and reference videos into batches for incremental matching. <figref idref="DRAWINGS">FIG. 5C</figref> is a series of frames illustrating how a collection of probe videos and reference videos can grow incrementally by scanning batches of the new probe videos against the old reference videos and matching all (new and old) probe videos against all new reference videos.</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 6</figref> is an example method of performing incremental matching according to one embodiment.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<p id="p-0015" num="0014">The figures depict an embodiment of the present invention for purposes of illustration only. One skilled in the art will readily recognize from the following description that alternative embodiments of the structures and methods illustrated herein may be employed without departing from the principles of the invention described herein.</p>
<heading id="h-0004" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0016" num="0015">Video sharing websites may desire to match a first set of videos against a second set of videos for a variety of purposes. For example, matches may be used to improve search results of users searching for particular content on the video sharing website because they may be used to remove duplicates or near duplicates from the search results, they may be used to recommend additional audio or video files that are similar to a file a user has already selected or played, or they may be used to identify duplicates or near duplicates of content.</p>
<p id="p-0017" num="0016">For purposes of description, the first set of videos will be referred to herein as &#x201c;probe videos&#x201d; and the second set of videos will be referred to herein as &#x201c;reference videos.&#x201d; Probe videos are, for example, a set of user-generated content uploaded to a video sharing website, and reference videos are a set of videos against which the probe videos are matched.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIGS. 1A and 1B</figref> illustrate the pre-processing steps by which probe videos and reference videos are prepared in order to begin the matching process. <figref idref="DRAWINGS">FIG. 1A</figref> is an illustration of a video fingerprint <b>100</b> divided into subfingerprints <b>101</b>A, <b>101</b>B. In one embodiment, an entire fingerprint <b>100</b> is determined for each probe video and for each reference video according to any of various techniques known to those of skill in the art, for example by applying a hash-based fingerprint function to a bit sequence of the video file. Additional examples of audio fingerprinting are discussed in S. Baluja, M. Covell, &#x201c;Waveprint: Efficient wavelet-based audio fingerprinting,&#x201d; Pattern Recognition, 2008, which is incorporated herein by reference. The entire fingerprint <b>100</b> may be determined separately for the audio portion of the video and the visual portion of the video, and matches may be separately determined for audio and video. In one embodiment, the subfingerprints <b>101</b>A, <b>101</b>B each represent about a quarter second of the original video, wherein each quarter second comprises approximately 100 bytes of data.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 1B</figref> is an illustration of a subfingerprint divided into LSH bands <b>110</b>A, <b>110</b>B. Locality Sensitive Hashing (LSH) is commonly used technique to divide subfingerprints <b>101</b> into manageable portions for matching purposes. In one embodiment, each LSH band <b>110</b>A, <b>110</b>B, etc., comprises 4 bytes from the 100 bytes of data in the subfingerprint <b>101</b>. In one embodiment, implicit in each LSH band is the time when it occurs within the video, referred to herein as the &#x201c;time offset&#x201d; as measured from the beginning of the video. In one implementation, the LSH band is represented by a numerical value, but other representations of LSH values can also be used, provided that the representation allows sorting. Further details regarding LSH banding can be found in M. Covell and S. Baluja, &#x201c;LSH Banding For Large-Scale Retrieval With Memory and Recall Constraints,&#x201d; <i>International Conference on Acoustics, Speech and Signal Processing </i>(<i>ICASSP</i>-2009), which is incorporated herein by reference.</p>
<p id="h-0005" num="0000">1. System Overview</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 2</figref> is a high-level block diagram of a computing environment <b>200</b> according to one embodiment of the system for performing large-scale matching of audio and video. The computing environment <b>200</b> includes fingerprint and video ID storage <b>220</b>, a batch manager <b>222</b>, at least one sort distributor <b>224</b> controlling at least one sort machine <b>225</b>, and match storage <b>229</b>.</p>
<p id="p-0021" num="0020">The fingerprint and video ID storage <b>220</b> is a storage location for the fingerprints <b>100</b>, subfingerprints <b>101</b>, and LSH bands <b>110</b>A, <b>110</b>B of all probe and reference videos. Each fingerprint <b>100</b>, subfingerprint <b>101</b>, and LSH band <b>110</b>A, <b>110</b>B are keyed to a unique identifier for the particular video from which it was generated. These unique identifiers are referred to herein as video IDs. The video IDs are also stored in storage <b>220</b>.</p>
<p id="p-0022" num="0021">The batch manager <b>222</b> manages the batches of a set of probe videos to be matched against a set of reference videos. The batch manager receives the LSH bands of all probe and reference videos to be matched from fingerprint and video ID storage <b>220</b>. Through communications with the sort distributor <b>224</b>, the batch manager processes the batches of probe videos to be matched against reference videos and outputs the matches. The batch manager includes a sort distributor interface <b>2221</b>, a batch matcher <b>2222</b>, and a probe-reference pairwise comparison module <b>2223</b>.</p>
<p id="p-0023" num="0022">The sort distributor interface <b>2221</b> manages the communications between the batch manager <b>222</b> and at least one sort distributor <b>224</b>. The batch manager <b>222</b> uses the sort distributor interface <b>2221</b> to instruct the sort distributor <b>224</b> to sort the LSH bands of all probe videos and, separately, the LSH bands of all reference videos, which will be described in greater detail below.</p>
<p id="p-0024" num="0023">One or more sort distributors <b>224</b> receive the LSH bands of all probe and reference videos that will be compared in the batch by the batch manager <b>222</b>. Each sort distributor <b>224</b> executes a sorting function over one or more computers, referred to herein as sort machines <b>225</b>, to process a large dataset. Examples of distributed sorters include MapReduce, developed by Google Inc., and Hadoop, developed as an open-source Apache product. An example implementation of MapReduce is presented in J. Dean and S. Ghemawat, &#x201c;MapReduce: Simplified Data Processing on Large Clusters,&#x201d; OSDI'04: Sixth Symposium on Operating System Design and Implementation, San Francisco, Calif., December, 2004, which is incorporated herein by reference. In one implementation, a first sort distributor <b>224</b> instructs a first group of sort machines <b>225</b> to sort the LSH bands of all probe videos in the batch, and a second sort distributor <b>224</b> instructs a second group of sort machines <b>225</b> to sort the LSH bands of all reference videos in the batch. In one embodiment, each sort distributor <b>224</b> distributes the sorting tasks across multiple sort machines <b>225</b> that are working in parallel. Thus, a sort distributor <b>224</b> can more quickly obtain the sorted list of LSH bands, for example from all probe videos in the batch or from all reference videos in the batch. The one or more sort distributors <b>224</b> returns the sorted lists of LSH bands from all probe videos in the batch and the sorted lists of LSH bands from all reference videos in the batch to the batch manager <b>222</b> though the sort distributor interface <b>2221</b>.</p>
<p id="p-0025" num="0024">Referring back to the batch manager <b>222</b>, the batch manager also includes a batch matcher <b>2222</b>. The batch matcher <b>2222</b> instructs the sort distributor <b>224</b> and the sort machines <b>225</b> to perform a linear scan of the sorted LSH bands to identify LSH bands that appear both in a probe video and a reference video and output each probe video ID along with the associated reference LSH bands and video IDs. The sort distributor <b>224</b> and sort machines <b>225</b> sort this output by probe video ID, which produces, for each probe video ID, all reference video IDs, LSH bands, and offsets that match it.</p>
<p id="p-0026" num="0025">The probe-reference pairwise comparison module <b>2223</b> reviews the results of the common LSH bands found by the batch matcher <b>2222</b> for each pair of probe and reference videos found to share LSH bands to determine whether a probe video matches a reference video. In one embodiment, a threshold of approximately 50 matches of LSH bands are used to determine that a probe video matches a reference video. A match is stronger if the matching LSB hands are clumped in subfingerprints and if they are clumped by offset. A probe video may be determined to match more than one reference video. For example, the first portion of a probe video may be similar to a first reference video, and a second portion of the probe video may be similar to a second reference video. In implementations where the audio portion of a video is analyzed for matches separately from a visual portion of a video, the audio portion of a probe video may match a first reference video and the visual portion of a video may match a second reference video. In one implementation, if a match is determined in an audio segment, greater leniency may be applied by the probe-reference pairwise comparison module <b>2223</b> toward finding a video match to the same reference video for borderline cases. The results of the pairwise comparison are output to match storage <b>229</b>.</p>
<p id="p-0027" num="0026">The match storage <b>229</b> stores the matches for any probe. Specifically, for any probe, the match storage <b>229</b> stores the reference videos and the time offset ranges over which the probe video matches the reference videos in audio, video, or both for use in any of the applications described below.</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 3</figref> is a high-level block diagram illustrating an example of a computer for use as a batch manager <b>222</b>, a sort distributor <b>224</b>, and/or a sort machine <b>225</b> of <figref idref="DRAWINGS">FIG. 2</figref>. Illustrated are a processor <b>302</b> coupled to a bus <b>304</b>. Also coupled to the bus <b>304</b> are a memory <b>306</b>, a storage device <b>308</b>, a keyboard <b>310</b>, a graphics adapter <b>312</b>, a pointing device <b>314</b>, and a network adapter <b>316</b>. A display <b>318</b> is coupled to the graphics adapter <b>312</b>.</p>
<p id="p-0029" num="0028">The processor <b>302</b> may be any general-purpose processor. The storage device <b>308</b> is, in one embodiment, a hard disk drive but can also be any other device capable of storing data, such as a writeable compact disk (CD) or DVD, or a solid-state memory device. The memory <b>306</b> may be, for example, firmware, read-only memory (ROM), non-volatile random access memory (NVRAM), and/or RAM, and holds instructions and data used by the processor <b>302</b>. The pointing device <b>314</b> may be a mouse, track ball, or other type of pointing device, and is used in combination with the keyboard <b>310</b> to input data into the computer <b>300</b>. The graphics adapter <b>312</b> displays images and other information on the display <b>318</b>. The network adapter <b>316</b> couples the computer <b>300</b> to the network (not shown). In one embodiment, the network is the Internet. The network can also utilize dedicated or private communications links that are not necessarily part of the Internet.</p>
<p id="p-0030" num="0029">As is known in the art, the computer <b>300</b> is adapted to execute computer program modules. As used herein, the term &#x201c;module&#x201d; refers to computer program logic and/or data for providing the specified functionality. A module can be implemented in hardware, firmware, and/or software. In one embodiment, the modules are stored on the storage device <b>308</b>, loaded into the memory <b>306</b>, and executed by the processor <b>302</b>. The computer <b>300</b> is configured to perform the specific functions and operations by various modules, for example as detailed in <figref idref="DRAWINGS">FIGS. 4 and 6</figref>, and thereby operates as a particular computer under such program control. The types of computers <b>300</b> utilized by the entities of <figref idref="DRAWINGS">FIG. 2</figref> can vary depending upon the embodiment and the processing power utilized by the entity.</p>
<p id="h-0006" num="0000">2. Full Collection Matching</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 4</figref> is an example method <b>400</b> of performing a batch match of probe videos against reference videos according to one embodiment. In one implementation, the method <b>400</b> is used to conduct a full collection scan for matches of a large collection of probe videos against a large collection of reference videos. The example below illustrates the method <b>400</b> by finding visual matches between videos, however matches can be found between aurally similar videos by executing the method <b>400</b>, for example, in parallel, to match audio.</p>
<p id="p-0032" num="0031">In <b>401</b>, the LSH bands of the probe videos are determined. In a pre-processing step, Locality Sensitive Hashing is used to determine a value corresponding to each small portion of the probe video. The LSH bands of the probe videos may be stored in fingerprint and video ID storage <b>220</b> so as to avoid the unnecessary repetition of this pre-processing step. Then, the LSH bands corresponding to the probe videos are obtained from fingerprint and video ID storage <b>220</b>. In <b>402</b>, the LSH bands of the probe videos are sorted by the LSH band value using a distributed sort, for example, as executed by a sort distributor <b>224</b>.</p>
<p id="p-0033" num="0032">Similarly, in <b>403</b>, the LSH bands of the reference videos are determined. The LSH bands of the reference videos may also be stored in and thereafter obtained from fingerprint and video ID storage <b>220</b>. In <b>404</b>, the LSH bands of the probe videos are sorted by LSH band value using a distributed sort, for example, as executed by the same or another sort distributor <b>224</b>. In some implementations, steps <b>401</b> and <b>402</b> execute at an overlapping time with steps <b>403</b> and <b>404</b>.</p>
<p id="p-0034" num="0033">In <b>405</b>, a linear scan is made of the sorted lists of LSH bands of the probe videos and reference videos for matches, for example by batch matcher <b>2222</b> of the batch manager <b>222</b>. Because the LSH bands are sorted, a linear scan can be made much more quickly than executing single lookups of LSH bands from probe videos across all the LSH bands of reference videos. Another advantage is that each matched LSH band can associate many probe video IDs with many reference video IDs, thus avoiding duplicating work. Thus, although the investment to sort the LSH bands in steps <b>402</b> and <b>404</b> is significant, the sorting ultimately accelerates the process of determining matches in <b>405</b>.</p>
<p id="p-0035" num="0034">In <b>406</b>, the matching LSH bands are sorted by probe video ID. For each probe video ID, the matching reference videos can be determined. Accordingly, in <b>407</b>, the matches can be output by the batch manager <b>222</b> for storage, for example in match storage <b>229</b> for subsequent use in any of the applications described below.</p>
<p id="p-0036" num="0035">In one implementation for very large scale matching of probe videos against reference videos, the total number of probes are divided into smaller groups for processing against the total number of reference videos for easier handling. For example, a list of 400 million probe videos can be divided into groups of approximately 10 million videos each. Alternatively or additionally, the total number of reference videos can be divided into smaller groups for processing against the total number of probe videos. However, in one embodiment, dividing probe videos into groups is used rather than dividing reference videos into groups because there is an overhead cost to compare one probe against any number of references. The overhead cost is related to retrieving further probe fingerprint information from storage. Thus, dividing the probes into groups results in a lower processing cost than dividing the references into groups that must be compared against each probe (which incurs the overhead cost to compare each probe against each group separately, resulting in a higher total overhead cost).</p>
<p id="h-0007" num="0000">3. Incremental Matching</p>
<p id="p-0037" num="0036">Once an initial full collection match has been completed, for example, by following the method <b>400</b> described with reference to <figref idref="DRAWINGS">FIG. 4</figref>, it becomes important to periodically update the recognized matches as additional probe videos and reference videos are added to the collection. The process of updating the matches as new probe videos and reference videos are added is reference to herein as incremental matching. <figref idref="DRAWINGS">FIG. 5A</figref> and <figref idref="DRAWINGS">FIG. 5B</figref> illustrate two options for dividing the tasks of matching probe videos and reference videos into batches for incremental matching. <figref idref="DRAWINGS">FIG. 5C</figref> is a series of frames illustrating how a collection of probe videos and reference videos can grow incrementally by scanning batches of the new probe videos against the old reference videos and matching all (new and old) probe videos against all new reference videos.</p>
<p id="p-0038" num="0037"><figref idref="DRAWINGS">FIGS. 5A and 5B</figref> illustrate options for batching probe videos and reference videos to process matches. The vertical extent of <figref idref="DRAWINGS">FIGS. 5A and 5B</figref> represents the probe videos including a comparatively small number of new probe videos compared to the existing previously matched probe videos. The horizontal extent represents the reference videos, including a comparatively small number of new reference videos compared to the existing previously matched reference videos. Rectangle <b>551</b> illustrates the body of pairwise comparisons between the old probes and old references that was completed as part of the previous full collection matching. Referring to <figref idref="DRAWINGS">FIG. 5A</figref>, rectangle <b>552</b> represents a batch of all new probes compared against all old references, and rectangle <b>553</b> represents a batch of all probes (old and new) compared against all new references. As another option illustrated in <figref idref="DRAWINGS">FIG. 5B</figref>, rectangle <b>554</b> represents a batch of all new probes compared against all references (old and new), and rectangle <b>555</b> represents all old probes matched against all new references. Note that the combined pairwise comparisons between rectangles <b>551</b>, <b>552</b>, and <b>553</b> of <figref idref="DRAWINGS">FIG. 5A</figref> is the same as the pairwise comparisons between rectangles <b>551</b>, <b>554</b>, and <b>555</b> of <figref idref="DRAWINGS">FIG. 5B</figref>.</p>
<p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. 5C</figref> illustrates how incremental matching through batches can be used to build up a collection of probe videos and reference videos over time, without repetitive comparisons. As new probe videos are added to the vertical extent of the rectangle and new references are added to the horizontal extent of the rectangle and the additional batches are compared, the combined results represent the new baseline to which further new probe videos and new reference videos can be added. As a result, rectangle <b>551</b> representing the collection of probe videos that have been compared against all reference videos grows over time.</p>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. 6</figref> is an example method <b>600</b> of performing incremental matching after an initial full collection matching process has been completed, according to one embodiment. By performing the method <b>600</b> iteratively, the collection of probe videos and reference videos is built up over time, without incurring the costs of comparing a probe video to a reference video more than once.</p>
<p id="p-0041" num="0040">In <b>601</b>, new probe videos and/or new reference videos are received by the video sharing website. As described above, the new probe videos and new reference videos are pre-processed to determine LSH bands corresponding to each portion of the video, and the LSH bands may be stored, for example in fingerprint and video ID storage <b>220</b> along with the LSH bands from the old probe videos and old reference videos that were already compared in the full collection matching process.</p>
<p id="p-0042" num="0041">In <b>602</b>, all new probe videos are matched against all old reference videos. This batch corresponds to rectangle <b>552</b> of <figref idref="DRAWINGS">FIG. 5A</figref>. In one embodiment, the matching is completed in the same way as described above with reference to the method <b>400</b> of <figref idref="DRAWINGS">FIG. 4</figref> for completing the full collection matching.</p>
<p id="p-0043" num="0042">In <b>603</b>, all (new and old) probe videos are matched against all new reference videos. This batch corresponds to rectangle <b>553</b> of <figref idref="DRAWINGS">FIG. 5A</figref>. In one embodiment, this matching also is completed in the same way as described above with reference to the method <b>400</b> of <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0044" num="0043">In <b>604</b>, the matches from steps <b>602</b> and <b>603</b> are added to the results of the previous full collection matching process (i.e., matches from all old probe videos against all old reference videos corresponding to rectangle <b>551</b> of <figref idref="DRAWINGS">FIG. 5A</figref>) to identify the complete set of matches for all probe videos against all reference videos. Optionally, the matches from the full collection matching and steps <b>602</b> and <b>603</b> can be outputted to storage for subsequent use in any of the applications described below.</p>
<p id="p-0045" num="0044">In <b>605</b>, all received probe and reference videos from <b>601</b> are marked as old because all probe videos have been compared against all reference videos, and the work need not be repeated. Instead, on the next iteration of the method <b>600</b>, the matches from steps <b>602</b> and <b>603</b> are added to the results of the previous iteration of the method <b>600</b> in order to build up over time the collection of probe videos and reference videos that have been compared against each other.</p>
<p id="h-0008" num="0000">4. Applications for Large Scale Matching of Audio and Video</p>
<p id="p-0046" num="0045">Many applications exist for systems, methods, and computer-program products for large scale matching of audio and video described herein. Specifically, the matches of probe videos against a set of reference videos can be used to improve search results, identify similar content, and identify duplicates or near duplicates of content for rights holders, among others.</p>
<p id="p-0047" num="0046">In one application, the matches between probe videos and reference videos can be used to improve search results on a video sharing website. For example, partial or full duplicates of videos can be removed from search results to enable a user to more quickly find desired videos. In a related application, by understanding which videos constitute matches to a popular video, the video sharing website may return the highest quality video among the matches to the popular video (as judged, for example, by the longest length of the video) when a popular video is requested.</p>
<p id="p-0048" num="0047">In another application, the matches between a probe video and reference videos can be used to suggest similar content to a user who selects or views the probe video. For example, the credits portion of videos may remain the same across various episodes of the videos by the same creator, regardless of who uploads the videos to the video sharing website. Thus, the matches to one episode that a user has enjoyed can lead the user to other episodes. Similarly, a user who has viewed one video with an audio track can be presented with options suggesting other different videos that have a matching audio track.</p>
<p id="p-0049" num="0048">In another application, the most popular new probe videos are prioritized for matching against new and existing reference videos. For example, the top 10% of new probe videos as measured by number of times they have been viewed are prioritized to be matched first against all new and existing reference videos. This reduces the time required to complete the match process for videos that are being frequently accessed.</p>
<p id="p-0050" num="0049">In another application, the information regarding which probe videos match reference videos can be used to share metadata among the matching videos. By sharing metadata among matching videos, less effort needs to be expended to manually label or propagate labels across matching videos. Additionally, sharing metadata across matches can also be used to improve search and recommendation capabilities of the video sharing website.</p>
<p id="p-0051" num="0050">In another application, the information regarding which probe videos match a reference video can be used to identify duplicates or near duplicates of content for rights holders in the reference video. As a result, the rights holders can determine whether the matching probe videos are acceptable or if additional follow-up actions are desired.</p>
<p id="h-0009" num="0000">5. Additional Configuration Considerations</p>
<p id="p-0052" num="0051">The present invention has been described in particular detail with respect to several possible embodiments. Those of skill in the art will appreciate that the invention may be practiced in other embodiments. The particular naming of the components, capitalization of terms, the attributes, data structures, or any other programming or structural aspect is not mandatory or significant, and the mechanisms that implement the invention or its features may have different names, formats, or protocols. Also, the particular division of functionality between the various system components described herein is merely exemplary, and not mandatory; functions performed by a single system component may instead be performed by multiple components, and functions performed by multiple components may instead performed by a single component.</p>
<p id="p-0053" num="0052">Some portions of above description present the features of the present invention in terms of algorithms and symbolic representations of operations on information. These algorithmic descriptions and representations are the means used by those skilled in the data processing arts to most effectively convey the substance of their work to others skilled in the art. These operations, while described functionally or logically, are understood to be implemented by computer programs. Furthermore, it has also proven convenient at times, to refer to these arrangements of operations as modules or by functional names, without loss of generality.</p>
<p id="p-0054" num="0053">Unless specifically stated otherwise as apparent from the above discussion, it is appreciated that throughout the description, discussions utilizing terms such as &#x201c;determining&#x201d; or the like, refer to the action and processes of a computer system, or similar electronic computing device, that manipulates and transforms data represented as physical (electronic) quantities within the computer system memories or registers or other such information storage, transmission or display devices.</p>
<p id="p-0055" num="0054">Certain aspects of the present invention include process steps and instructions described herein in the form of an algorithm. It should be noted that the process steps and instructions of the present invention could be embodied in software, firmware or hardware, and when embodied in software, could be downloaded to reside on and be operated from different platforms used by real time network operating systems.</p>
<p id="p-0056" num="0055">The present invention also relates to an apparatus for performing the operations herein. This apparatus may be specially constructed for the required purposes, or it may comprise a general-purpose computer selectively activated or reconfigured by a computer program stored on a computer readable medium that can be accessed by the computer and run by a computer processor. Such a computer program may be stored in a computer readable storage medium, such as, but is not limited to, any type of disk including floppy disks, optical disks, CD-ROMs, magnetic-optical disks, read-only memories (ROMs), random access memories (RAMs), EPROMs, EEPROMs, magnetic or optical cards, application specific integrated circuits (ASICs), or any type of media suitable for storing electronic instructions, and each coupled to a computer system bus. Furthermore, the computers referred to in the specification may include a single processor or may be architectures employing multiple processor designs for increased computing capability.</p>
<p id="p-0057" num="0056">In addition, the present invention is not described with reference to any particular programming language. It is appreciated that a variety of programming languages may be used to implement the teachings of the present invention as described herein, and any references to specific languages are provided for enablement and best mode of the present invention.</p>
<p id="p-0058" num="0057">The present invention is well suited to a wide variety of computer network systems over numerous topologies. Within this field, the configuration and management of large networks comprise storage devices and computers that are communicatively coupled to dissimilar computers and storage devices over a network, such as the Internet.</p>
<p id="p-0059" num="0058">Finally, it should be noted that the language used in the specification has been principally selected for readability and instructional purposes, and may not have been selected to delineate or circumscribe the inventive subject matter. Accordingly, the disclosure of the present invention is intended to be illustrative, but not limiting, of the scope of the invention.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method of performing large-scale matching of a set of probe videos to a set of reference videos, the method comprising:
<claim-text>accessing locality sensitive hashing (LSH) bands of the set of probe videos, each LSH band of the set of probe videos having a band value and corresponding to a portion of a subfingerprint of a respective probe video of the set of probe videos;</claim-text>
<claim-text>sorting the LSH bands of the set of probe videos into a first sorted list based on the band values of the LSH bands of the set of probe videos;</claim-text>
<claim-text>accessing LSH bands of the set of reference videos, each LSH band of the set of reference videos having a band value and corresponding to a portion of a subfingerprint of a respective reference video of the set of reference videos;</claim-text>
<claim-text>sorting the LSH bands of the set of reference videos into a second sorted list based on the band values of the LSH bands of the set of reference videos;</claim-text>
<claim-text>linearly scanning the first and second sorted lists of LSH bands for matches; and</claim-text>
<claim-text>outputting the matches for storage.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein accessing LSH bands of the set of probe videos and accessing LSH bands of the set of reference videos comprise retrieving the LSH bands of the set of probe videos and the set of reference videos from a storage device.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each LSH band of the set of probe videos corresponds to audio of a respective probe video of the set of probe videos, and each LSH band of the set of reference videos corresponds to audio of a respective reference video of the set of reference videos, and the matches comprise audio matches between the probe videos and reference videos.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each LSH band of the set of probe videos corresponds to a visual portion of a respective probe video of the set of probe videos, and each LSH band of the set of reference videos corresponds to a visual portion of a respective reference video of the set of reference videos, and the matches comprise visual matches between the probe videos and reference videos.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein sorting the LSH bands of the set of probe videos and sorting the LSH bands of the set of reference videos are both performed by executing a distributed sorter.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>receiving new probe videos and new reference videos;</claim-text>
<claim-text>matching the new probe videos against the set of reference videos; and</claim-text>
<claim-text>matching all probe videos against the new reference videos, wherein all probe videos comprises the set of probe videos and the new probe videos.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the matches are used to remove duplicates from search results.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the matches are used to suggest similar content.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the set of probe videos comprises approximately the top ten percent of probe videos by popularity.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising sharing metadata among the matches.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the matches are used to identify duplicates or near duplicates of reference videos for rights holders in the reference videos.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. A non-transitory computer-readable storage medium storing computer program instructions executable by a processor for large-scale matching of a set of probe videos to a set of reference videos, the computer program instructions comprising instructions for:
<claim-text>accessing locality sensitive hashing (LSH) bands of the set of probe videos, each LSH band of the set of probe videos having a band value and corresponding to a portion of a subfingerprint of a respective probe video of the set of probe videos;</claim-text>
<claim-text>sorting the LSH bands of the set of probe videos into a first sorted list based on the band values of the LSH bands of the set of probe videos;</claim-text>
<claim-text>accessing LSH bands of the set of reference videos, each LSH band of the set of reference videos having a band value and corresponding to a portion of a subfingerprint of a respective reference video of the set of reference videos;</claim-text>
<claim-text>sorting the LSH bands of the set of reference videos into a second sorted list based on the band values of the LSH bands of the set of reference videos;</claim-text>
<claim-text>linearly scanning the first and second sorted lists of LSH bands for matches; and</claim-text>
<claim-text>outputting the matches for storage.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the instructions for accessing LSH bands of the set of probe videos and accessing LSH bands of the set of reference videos comprise instructions for retrieving the LSH bands of the set of probe videos and the set of reference videos from a storage device.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein each LSH band of the set of probe videos corresponds to audio of a respective probe video of the set of probe videos, and each LSH band of the set of reference videos corresponds to audio of a respective reference video of the set of reference videos, and the matches comprise audio matches between the probe videos and reference videos.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein each LSH band of the set of probe videos corresponds to a visual portion of a respective probe video of the set of probe videos, and each LSH band of the set of reference videos corresponds to a visual portion of a respective reference video of the set of reference videos, and the matches comprise visual matches between the probe videos and reference videos.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the instructions for sorting the LSH bands of the set of probe videos and sorting the LSH bands of the set of reference videos are both instructions for executing a distributed sorter.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the instructions further comprise instructions for:
<claim-text>receiving new probe videos and new reference videos;</claim-text>
<claim-text>matching the new probe videos against the set of reference videos; and</claim-text>
<claim-text>matching all probe videos against the new reference videos, wherein all probe videos comprises the set of probe videos and the new probe videos.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the matches are used to remove duplicates from search results.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the matches are used to suggest similar content.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the set of probe videos comprises approximately the top ten percent of probe videos by popularity.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the instructions further comprise instructions for sharing metadata among the matches.</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the matches are used to identify duplicates or near duplicates of reference videos for rights holders in the reference videos.</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. A system for large-scale matching of a set of probe videos to a set of reference videos, the system comprising:
<claim-text>a storage for locality sensitive hashing (LSH) bands of the set of probe videos, each LSH band of the set of probe videos corresponding to a portion of a subfingerprint of a respective probe video of the set of probe videos, and for LSH bands of the set of reference videos, each LSH band of the set of reference videos having a band value and corresponding to a portion of a subfingerprint of a respective reference video of the set of reference videos;</claim-text>
<claim-text>a batch manager for accessing the LSH bands of the set of probe videos and the LSH bands of the set of reference videos;</claim-text>
<claim-text>a sort distributor for sorting the LSH bands of the set of probe videos into a first sorted list based on the band values of the LSH bands of the set of probe videos and for sorting the LSH bands of the set of reference videos into a second sorted list based on the band values of the LSH bands of the set of reference videos;</claim-text>
<claim-text>a batch matcher for instructing the sort distributor to perform a linear scan of the first and second sorted lists of LSH bands for matches; and</claim-text>
<claim-text>a pairwise comparison module for determining matches between probe videos and reference videos.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. The system of <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein accessing the LSH bands of the set of probe videos and the LSH bands of the set of reference videos comprises retrieving the LSH bands of the set of probe videos and the set of reference videos from a storage device.</claim-text>
</claim>
<claim id="CLM-00025" num="00025">
<claim-text>25. The system of <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein each LSH band of the set of probe videos corresponds to audio of a respective probe video of the set of probe videos, and each LSH band of the set of reference videos corresponds to audio of a respective reference video of the set of reference videos, and the matches comprise audio matches between the probe videos and reference videos.</claim-text>
</claim>
<claim id="CLM-00026" num="00026">
<claim-text>26. The system of <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein each LSH band of the set of probe videos corresponds to a visual portion of a respective probe video of the set of probe videos, and each LSH band of the set of reference videos corresponds to a visual portion of a respective reference video of the set of reference videos, and the matches comprise visual matches between the probe videos and reference videos.</claim-text>
</claim>
<claim id="CLM-00027" num="00027">
<claim-text>27. The system of <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein the matches are used to remove duplicates from search results.</claim-text>
</claim>
<claim id="CLM-00028" num="00028">
<claim-text>28. The system of <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein the matches are used to suggest similar content.</claim-text>
</claim>
<claim id="CLM-00029" num="00029">
<claim-text>29. The system of <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein the set of probe videos comprises approximately the top ten percent of probe videos by popularity.</claim-text>
</claim>
<claim id="CLM-00030" num="00030">
<claim-text>30. The system of <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein the matches are used to share metadata among the matches.</claim-text>
</claim>
<claim id="CLM-00031" num="00031">
<claim-text>31. The system of <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein the matches are used to identify duplicates or near duplicates of reference videos for rights holders in the reference videos.</claim-text>
</claim>
</claims>
</us-patent-grant>
