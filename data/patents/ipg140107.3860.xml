<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08624927-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08624927</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12690386</doc-number>
<date>20100120</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>JP</country>
<doc-number>P2009-015137</doc-number>
<date>20090127</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>703</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>09</class>
<subclass>G</subclass>
<main-group>5</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>345660</main-classification>
<further-classification>345661</further-classification>
<further-classification>345665</further-classification>
<further-classification>345668</further-classification>
<further-classification>345156</further-classification>
<further-classification>715863</further-classification>
</classification-national>
<invention-title id="d2e71">Display apparatus, display control method, and display control program</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5917460</doc-number>
<kind>A</kind>
<name>Kodama</name>
<date>19990600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345  8</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6864912</doc-number>
<kind>B1</kind>
<name>Mahaffey et al.</name>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348 61</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>2002/0089467</doc-number>
<kind>A1</kind>
<name>Hara</name>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345  4</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>2003/0080937</doc-number>
<kind>A1</kind>
<name>Light</name>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345156</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>2003/0210258</doc-number>
<kind>A1</kind>
<name>Williams</name>
<date>20031100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2006/0164382</doc-number>
<kind>A1</kind>
<name>Kulas et al.</name>
<date>20060700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345156</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2007/0273712</doc-number>
<kind>A1</kind>
<name>O'Mullan et al.</name>
<date>20071100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2007/0279591</doc-number>
<kind>A1</kind>
<name>Wezowski et al.</name>
<date>20071200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>351208</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2008/0199049</doc-number>
<kind>A1</kind>
<name>Daly</name>
<date>20080800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382107</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2008/0276196</doc-number>
<kind>A1</kind>
<name>Tang</name>
<date>20081100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715800</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2009/0103621</doc-number>
<kind>A1</kind>
<name>Numata et al.</name>
<date>20090400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524016</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2009/0109130</doc-number>
<kind>A1</kind>
<name>Murphy et al.</name>
<date>20090400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345 31</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2009/0271732</doc-number>
<kind>A1</kind>
<name>Kondo et al.</name>
<date>20091000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715781</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2009/0295832</doc-number>
<kind>A1</kind>
<name>Takatsuka et al.</name>
<date>20091200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345659</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2010/0125816</doc-number>
<kind>A1</kind>
<name>Bezos</name>
<date>20100500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715863</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2010/0253825</doc-number>
<kind>A1</kind>
<name>Horie</name>
<date>20101000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>34833301</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>2011/0006978</doc-number>
<kind>A1</kind>
<name>Yuan</name>
<date>20110100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345156</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>2011/0018875</doc-number>
<kind>A1</kind>
<name>Arahari et al.</name>
<date>20110100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345420</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>2011/0090149</doc-number>
<kind>A1</kind>
<name>Larsen et al.</name>
<date>20110400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345158</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>2011/0175932</doc-number>
<kind>A1</kind>
<name>Yu et al.</name>
<date>20110700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345661</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>2011/0254865</doc-number>
<kind>A1</kind>
<name>Yee et al.</name>
<date>20111000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345661</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>2011/0304557</doc-number>
<kind>A1</kind>
<name>Wilburn et al.</name>
<date>20111200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345173</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>2012/0017147</doc-number>
<kind>A1</kind>
<name>Mark</name>
<date>20120100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345158</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>2012/0287163</doc-number>
<kind>A1</kind>
<name>Djavaherian</name>
<date>20121100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345667</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>JP</country>
<doc-number>7 44143</doc-number>
<date>19950200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>JP</country>
<doc-number>2005-25170</doc-number>
<date>20050100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>JP</country>
<doc-number>2005 284487</doc-number>
<date>20051000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>JP</country>
<doc-number>2006 14223</doc-number>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>JP</country>
<doc-number>2007-36802</doc-number>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>JP</country>
<doc-number>2007 164814</doc-number>
<date>20070600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>JP</country>
<doc-number>2007 220010</doc-number>
<date>20070800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>JP</country>
<doc-number>2008 65837</doc-number>
<date>20080300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00033">
<document-id>
<country>JP</country>
<doc-number>2008 77655</doc-number>
<date>20080400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00034">
<document-id>
<country>JP</country>
<doc-number>2008 146164</doc-number>
<date>20080600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00035">
<document-id>
<country>JP</country>
<doc-number>2008-294815</doc-number>
<date>20081200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00036">
<document-id>
<country>TW</country>
<doc-number>544637</doc-number>
<date>20030800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00037">
<document-id>
<country>WO</country>
<doc-number>WO 2006/097722</doc-number>
<date>20060900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00038">
<othercit>Search Report issued in corresponding European application No. 10250071.7.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00039">
<othercit>Varona, J. et al., &#x201c;Hands-free vision-based interface for computer accessibility&#x201d;, Journal of Network and Computer Applications, Academic Press, New York, NY, US, vol. 31, No. 4, 1 Nov. 2008, pp. 357-374, XP023316579.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>11</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>345  4</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345  8</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345156</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345158</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345661</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345659</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345660</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345665</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345668</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345173</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345667</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345 31</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>715863</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>715800</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>715781</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348 61</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>34833301</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382107</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>15</number-of-drawing-sheets>
<number-of-figures>26</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20100188426</doc-number>
<kind>A1</kind>
<date>20100729</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Ohmori</last-name>
<first-name>Kenta</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Kawakami</last-name>
<first-name>Takashi</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Ohmori</last-name>
<first-name>Kenta</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Kawakami</last-name>
<first-name>Takashi</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Frommer Lawrence &#x26; Haug</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
<agent sequence="02" rep-type="attorney">
<addressbook>
<last-name>Frommer</last-name>
<first-name>William S.</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Sony Corporation</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
<assignee>
<addressbook>
<orgname>Sony Mobile Communications, Inc.</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Harrison</last-name>
<first-name>Chante</first-name>
<department>2679</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A display apparatus includes a display unit displaying information on a display screen, a distance detector detecting a distance from the display screen to the face of an operator, and a controller causing the information to be enlarged and reduced on the display screen in response to a change of the detected distance.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="146.64mm" wi="156.38mm" file="US08624927-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="171.70mm" wi="156.63mm" file="US08624927-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="148.34mm" wi="143.09mm" file="US08624927-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="218.44mm" wi="175.01mm" file="US08624927-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="200.66mm" wi="180.93mm" file="US08624927-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="240.11mm" wi="114.47mm" file="US08624927-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="225.72mm" wi="116.16mm" file="US08624927-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="238.84mm" wi="110.83mm" file="US08624927-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="244.77mm" wi="142.41mm" file="US08624927-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="241.47mm" wi="131.91mm" file="US08624927-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="251.04mm" wi="138.85mm" file="US08624927-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="237.83mm" wi="157.56mm" file="US08624927-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="243.08mm" wi="155.28mm" file="US08624927-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="245.11mm" wi="142.41mm" file="US08624927-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="256.96mm" wi="145.71mm" file="US08624927-20140107-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="248.41mm" wi="141.48mm" file="US08624927-20140107-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0002" num="0001">1. Field of the Invention</p>
<p id="p-0003" num="0002">The present invention relates to a display apparatus and, more particularly, to a mobile terminal with a display screen and a display control method.</p>
<p id="p-0004" num="0003">2. Description of the Related Art</p>
<p id="p-0005" num="0004">Some mobile terminals with a display screen, such as portable telephone terminals, have a function of changing the size of characters and the like presented on the display screen. This function enables information to be presented in a legible size to an operator even with weak eyesight.</p>
<p id="p-0006" num="0005">Japanese Unexamined Patent Application Publication No. 2006-14223 discloses a technique of changing the size of text on a display screen in response to an operation of an operation key on a portable terminal and also adjusting the display contents of the text to conform to the display format of the text.</p>
<p id="p-0007" num="0006">Japanese Unexamined Patent Application Publication No. 2008-65837 discloses a technique of detecting a movement direction and movement amount of a display unit by using a triaxial acceleration sensor incorporated near the display unit on a portable electronic device to enlarge or reduce display information on the display screen at a display magnification ratio according to the movement amount.</p>
<p id="p-0008" num="0007">Japanese Unexamined Patent Application Publication No. 2005-284487 proposes a technique of promptly determining a face image from a captured image with a small amount of calculation.</p>
<heading id="h-0002" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0009" num="0008">To change a character size as disclosed in Japanese Unexamined Patent Application Publication No. 2006-14223, the operator typically operates a specific operation key. This disadvantageously makes the operating method of the mobile terminal even more complex.</p>
<p id="p-0010" num="0009">Moreover, in addition to changing the enlargement/reduction ratio according to the contents to be displayed, it may be desired to view details of, for example, a map by enlarging the contents or to widely view the entirety by reducing the contents. In such cases, it may be bothersome to perform an enlarging or reducing operation each time to enlarge or reduce the map.</p>
<p id="p-0011" num="0010">According to the technique disclosed in Japanese Unexamined Patent Application Publication No. 2008-65837, the display information can be enlarged or reduced only by moving a hand that holds the device in the vertical direction, making it possible to mitigate the bother of the manual operation by the operator. With this technique, however, the movement of the device itself is detected by detecting an acceleration in only one axial (Z-axis) direction of the triaxial acceleration sensor. Therefore, a relative positional relation between the operator and the device is not considered. For example, when the operator moves his/her face closer to or away from the device without moving the device, the display magnification ratio remains unchanged. Moreover, depending on the operator's posture, such as when the operator holds and operates the device while lying down, a local relation between the axial direction and vertical axial direction of the device is changed, thereby possibly causing a malfunction in the same operator's operation.</p>
<p id="p-0012" num="0011">It is desirable to provide a technique of easily and intuitively enlarging or reducing display information irrespectively of the operator's posture.</p>
<p id="p-0013" num="0012">A display apparatus according to an embodiment of the present invention includes display means for displaying information on a display screen, distance detecting means for detecting a distance from the display screen to the face of an operator, and control means for causing the information to be enlarged or reduced on the display screen in response to a change of the detected distance. The distance from the display screen to the face of the operator is changed by relatively moving the face of the operator with respect to the display screen. Therefore, bothersome operations, such as key operations and menu operations, can be omitted in an enlarging or reducing operation.</p>
<p id="p-0014" num="0013">More specifically, the distance detecting means includes imaging means for capturing an image of the face of the operator who is viewing the display screen, and the control means can detect the distance from the display screen to the face of the operator by detecting a ratio of a face image of the operator to the captured image.</p>
<p id="p-0015" num="0014">The control means determines a magnification ratio of the image according to a ratio or difference in detection size of the face image with respect to its reference size. Alternatively, the control means continues to cause the image to be enlarged or reduced successively or stepwise according to a sign of the difference in detection size of the face image with respect to the reference size, and stops changing the magnification ratio when the difference becomes 0.</p>
<p id="p-0016" num="0015">A display control method according to another embodiment of the present invention includes the steps of displaying information on a display screen, detecting a distance from the display screen to the face of an operator, and causing the information to be enlarged or reduced on the display screen in response to a change of the detected distance.</p>
<p id="p-0017" num="0016">According to still another embodiment of the present invention, a display control program that causes a computer to perform the steps described above can be provided.</p>
<p id="p-0018" num="0017">According to the embodiments of the present invention, the operator changes a relative position of his/her face with respect to the display device, thereby allowing easy and intuitive enlargement or reduction of display irrespectively of the operator's posture. Therefore, it is possible to freely make a transition between an at-a-glance display and a detailed display of the display contents. User interface's operability can then be improved.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 1</figref> schematically depicts the structure of a mobile terminal according to an embodiment of the present invention;</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 2</figref> schematically depicts the hardware structure of the mobile terminal in <figref idref="DRAWINGS">FIG. 1</figref>;</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 3</figref> is a functional block diagram depicting main functions of the mobile terminal in the embodiment of the present invention;</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 4</figref> is a block diagram depicting an example of the structure of a face distance detecting unit in the embodiment of the present invention;</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIGS. 5A and 5B</figref> depict a first example of a relation between a captured image and an operator's face image included in the captured image in the embodiment of the present invention;</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIGS. 6A and 6B</figref> depict a second example of the relation between the captured image and the operator's face image included in the captured image in the embodiment of the present invention;</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIGS. 7A and 7B</figref> depict a third example of the relation between the captured image and the operator's face image included in the captured image in the embodiment of the present invention;</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIGS. 8A and 8B</figref> depict an example when a magnification ratio of display information on a display screen of the mobile terminal in the case in <figref idref="DRAWINGS">FIGS. 5A and 5B</figref> is changed in a hands-free manner;</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIGS. 9A and 9B</figref> depict an example when a magnification ratio of display information on the display screen of the mobile terminal in the case in <figref idref="DRAWINGS">FIGS. 6A and 6B</figref> is changed in a hands-free manner;</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIGS. 10A and 10B</figref> depict an example when a magnification ratio of display information in the display screen of the mobile terminal in the case in <figref idref="DRAWINGS">FIGS. 7A and 7B</figref> is changed in a hands-free manner;</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIGS. 11A and 11B</figref> are graphs depicting two examples of the magnification ratio of the display information to a face distance in an embodiment of the present invention;</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIGS. 12A and 12B</figref> are graphs depicting other two examples of the magnification ratio of the display information to the face distance in an embodiment of the present invention;</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIGS. 13A and 13B</figref> depict an example when the display contents are moved in a hands-free manner in an embodiment of the present invention;</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIGS. 14A and 14B</figref> depict an example continued from <figref idref="DRAWINGS">FIGS. 13A and 13B</figref>; and</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIGS. 15A and 15B</figref> depict an example continued from <figref idref="DRAWINGS">FIGS. 14A and 14B</figref>.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading>
<p id="p-0034" num="0033">Preferred embodiments of the present invention are described in detail below with reference to the drawings.</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 1</figref> schematically depicts the structure of a mobile terminal <b>100</b> according to an embodiment of the present invention. This mobile terminal <b>100</b> is assumed to be, for example, a portable telephone terminal, and is also simply referred to below as a terminal.</p>
<p id="p-0036" num="0035">The mobile terminal <b>100</b> displays arbitrary display information, such as text and images, on a display screen <b>110</b>. The mobile terminal <b>100</b> also obtains from an imaging unit <b>104</b> an image of the face (a face image) of an operator <b>10</b> (that is, a user) viewing this display screen <b>110</b>. In an embodiment of the present invention, the imaging unit <b>104</b> is equivalent to a device, such as a digital camera, that captures an image of the operator viewing the display screen. The imaging unit <b>104</b> may be an imaging portion, for capturing an image of a subject ahead of the operator, which is rotated to an operator side, or may be another imaging portion with a lens fixedly oriented to the operator for videophones or the like. However, to effectively detect the movement of the face of the operator in a direction parallel to the display screen as described below, the imaging unit <b>104</b> preferably allows wide-angle capturing to some degree.</p>
<p id="p-0037" num="0036">Furthermore, although not shown in <figref idref="DRAWINGS">FIG. 1</figref>, the mobile terminal <b>100</b> typically includes an operation unit with various operation keys and others provided to portable telephone terminals or the like. The mobile terminal <b>100</b> is not particularly restricted to be of, for example, a foldable, slidable, or straight type.</p>
<p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. 2</figref> schematically depicts the hardware structure of the mobile terminal <b>100</b> in <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0039" num="0038">In addition to the imaging unit <b>104</b>, the mobile terminal <b>100</b> includes a CPU <b>101</b>, a storage unit <b>102</b>, a display unit <b>103</b>, an operation unit <b>105</b>, and a unique function unit <b>106</b>.</p>
<p id="p-0040" num="0039">The CPU <b>101</b> configures a controller according to an embodiment of the present invention. The CPU <b>101</b> controls the entire mobile terminal <b>100</b> by executing various programs including a system program in the terminal apparatus and a display control program according to an embodiment of the present invention, and performs various processes.</p>
<p id="p-0041" num="0040">The storage unit <b>102</b> stores programs to be executed by the CPU <b>101</b> and necessary data, and can include an internal storage devices, such as a ROM, a RAM, a flash memory, and an HDD, as well as a removable storage medium. Display information, described below, and captured image information obtained by the imaging unit <b>104</b> are also stored in the storage unit <b>102</b>.</p>
<p id="p-0042" num="0041">The display unit <b>103</b> displays arbitrary display information on the display screen <b>110</b> depicted in <figref idref="DRAWINGS">FIG. 1</figref>, and includes a display device, such as an LCD or an organic EL.</p>
<p id="p-0043" num="0042">The imaging unit <b>104</b> can capture an image of the face of the operator <b>10</b>, as described with reference to <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0044" num="0043">The operation unit <b>105</b> receives an input operation from the operator, as described with reference to <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0045" num="0044">The unique function unit <b>106</b> performs unique functions provided to each mobile terminal, and is at least any one of, for example, a music replaying unit, an image replaying unit, a motion-picture replaying unit, a GPS function unit, and a non-contact IC function unit.</p>
<p id="p-0046" num="0045">Units included in a typical portable telephone terminal, such as a wireless communication unit, a vibration generating unit, an LED light-emitting unit, and a power supply unit, are omitted in <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. 3</figref> is a functional block diagram depicting main functions of the mobile terminal <b>100</b> in the embodiment of the present invention.</p>
<p id="p-0048" num="0047">A control unit <b>300</b> includes a face distance detecting unit <b>310</b>, a face position detecting unit <b>315</b>, a display information changing unit <b>320</b>, and an image display instructing unit <b>330</b>.</p>
<p id="p-0049" num="0048">The face distance detecting unit <b>310</b> detects a distance (face distance) from the mobile terminal (display screen) to the face of the operator, and stores the detected distance in a face distance information holding unit <b>370</b> in the storage unit <b>102</b> as operator's face distance information <b>371</b>.</p>
<p id="p-0050" num="0049">The storage unit <b>102</b> also includes a display information holding unit <b>360</b> that stores display information <b>361</b> containing characters and images to be eventually presented to the operator with the use of the display unit <b>103</b>.</p>
<p id="p-0051" num="0050">The face position detecting unit <b>315</b> detects face position information <b>381</b> representing a relative position of the face of the operator with respect to the mobile terminal (display screen), and stores the detected face position information <b>381</b> in a face position information holding unit <b>380</b>.</p>
<p id="p-0052" num="0051">The display information changing unit <b>320</b> changes the display information <b>361</b> based on the operator's face distance information <b>371</b>.</p>
<p id="p-0053" num="0052">The image display instructing unit <b>330</b> instructs the display unit <b>103</b> to display the display information <b>361</b> changed by the display information changing unit <b>320</b>.</p>
<p id="p-0054" num="0053">The display unit <b>103</b> displays the image as instructed by the image display instructing unit <b>330</b>.</p>
<p id="p-0055" num="0054">An example of a principle and technique of detecting a face distance by the face distance detecting unit <b>310</b> is described with reference to <figref idref="DRAWINGS">FIG. 4</figref>, although not particularly restrictive.</p>
<p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. 4</figref> is a block diagram depicting an example of the structure of the face distance detecting unit <b>310</b>.</p>
<p id="p-0057" num="0056">The face distance detecting unit <b>310</b> includes the imaging unit <b>104</b> and a control unit <b>300</b><i>a</i>. A captured image <b>341</b> obtained by the imaging unit <b>104</b> is stored in a captured image holding unit <b>340</b> in the storage unit <b>102</b>. The captured image <b>341</b> preferably includes an operator's face image <b>341</b><i>a. </i></p>
<p id="p-0058" num="0057">The control unit <b>300</b><i>a </i>includes a face rectangle detecting unit <b>311</b>, a face distance extracting unit <b>312</b>, and a face position extracting unit <b>313</b>.</p>
<p id="p-0059" num="0058">The face rectangle detecting unit <b>311</b> detects a rectangle (for example, a rectangle including eyes or a rectangle including both eyes and a nose) corresponding to a portion of the face of the operator in the captured image <b>341</b> obtained by the imaging unit <b>104</b>, and obtains face rectangle information <b>351</b>. The face rectangle information <b>351</b> is stored in a face rectangle information holding unit <b>350</b> in the storage unit <b>102</b>. The face rectangle information <b>351</b> includes such information as coordinates of a predetermined point (for example, an upper-left point) of the face rectangle and information about the width and height of the face rectangle. In place of the width and height information, coordinates of diagonal points of the face rectangle may be used.</p>
<p id="p-0060" num="0059">To detect a face rectangle, a related technique as disclosed in Japanese Unexamined Patent Application Publication No. 2005-284487 described above can be used. For example, a large amount of information about faces are learned in advance and constructed as dictionary data for face detection. Next, the input captured image <b>341</b> is compared with the information stored in the dictionary for face detection while being enlarged or reduced, and an operation of determining any similar portion is repeated to detect a face rectangle. By using this technique, information about face elements, such as eyes, a nose, and a mouth, can be extracted from the captured image <b>341</b>, and their positions and consequently a face region, facial expressions, and others can be rapidly detected as face rectangle information. The face elements may include a frame of eyeglasses. The face rectangle information reflects the position and the distance of the face of the operator with respect to the imaging unit <b>104</b> and changes of the position and the distance, as described later.</p>
<p id="p-0061" num="0060">The face distance extracting unit <b>312</b> detects a distance from the face of the operator with respect to the display screen <b>110</b> in a relative size (ratio) of the face image <b>341</b><i>a </i>with respect to the captured image <b>341</b>. Here, &#x201c;relative&#x201d; indicates that moving the face from or to the terminal standing still is equivalent to moving the terminal from or to the face standing still.</p>
<p id="p-0062" num="0061">The face position extracting unit <b>313</b> detects a relative position of the face of the operator with respect to the display screen <b>110</b> in the position of the face image <b>341</b><i>a </i>with respect to the captured image <b>341</b>.</p>
<p id="p-0063" num="0062">In this manner, according to the technique of detecting a face distance by using the face image of the operator, not only the size of the face image with respect to the captured image but also the position thereof can be detected.</p>
<p id="p-0064" num="0063"><figref idref="DRAWINGS">FIGS. 5A</figref>, <b>5</b>B, <b>6</b>A, <b>6</b>B, <b>7</b>A, and <b>7</b>B depict relations between the captured image <b>341</b> and the face image <b>341</b><i>a </i>of the operator included in that captured image <b>341</b>. With these relations, the distance and position of the face of the operator with respect to the portable terminal can be recognized based on the captured image <b>341</b>, which is now described below. Each of <figref idref="DRAWINGS">FIGS. 5A</figref>, <b>6</b>A, and <b>7</b>A depicts a positional relation between the portable terminal and the operator viewed from top. Each of <figref idref="DRAWINGS">FIGS. 5B</figref>, <b>6</b>B, and <b>7</b>B depicts the captured image <b>341</b> stored in the captured image holding unit <b>340</b> in the states depicted in <figref idref="DRAWINGS">FIGS. 5A</figref>, <b>6</b>A, and <b>7</b>A, respectively. From the captured image <b>341</b>, face rectangle information is detected by using the related technique as described above.</p>
<p id="p-0065" num="0064">With reference to <figref idref="DRAWINGS">FIGS. 5A</figref>, <b>5</b>B, <b>6</b>A, <b>6</b>B, <b>7</b>A, and <b>7</b>B, a technique of finding a face distance and face position of the operator based on a positional relation between the captured image <b>341</b> and the face rectangle corresponding to the operator's face image <b>341</b><i>a </i>is described below.</p>
<p id="p-0066" num="0065">First, as depicted in <figref idref="DRAWINGS">FIG. 5A</figref>, it is assumed that (the face of) the operator <b>10</b> is positioned straight in front of the display screen <b>110</b>. In this case, as depicted in <figref idref="DRAWINGS">FIG. 5B</figref>, the operator's face image <b>341</b><i>a </i>included in the captured image <b>341</b> is positioned at the center of the captured image <b>341</b>. Here, when the display screen <b>110</b> and the imaging unit <b>104</b> are shifted in position, even if the face is positioned straight in front of the display screen <b>110</b>, the face image <b>341</b><i>a </i>in the captured image <b>341</b> obtained by the imaging unit <b>104</b> may be shifted from the center. In this case, an amount of positional shift of the face image <b>341</b><i>a </i>with respect to the captured image <b>341</b> can be corrected.</p>
<p id="p-0067" num="0066">By contrast, when the operator <b>10</b> relatively moves from the state shown in <figref idref="DRAWINGS">FIG. 5A</figref> to left with respect to the display screen <b>110</b>, the operator's face image <b>341</b><i>a </i>included in the captured image <b>341</b> moves from the state shown in <figref idref="DRAWINGS">FIG. 5B</figref> to a left side in the captured image.</p>
<p id="p-0068" num="0067">In this manner, by detecting the position of the operator's face image <b>341</b><i>a </i>included in the captured image <b>341</b>, the direction in which the operator <b>10</b> is positioned with respect to the imaging unit <b>104</b> or the display screen <b>110</b> and also to the mobile terminal <b>100</b> can be determined.</p>
<p id="p-0069" num="0068">Next, when the operator <b>10</b> gets closer to the imaging unit <b>104</b> as depicted in <figref idref="DRAWINGS">FIG. 6A</figref>, the size (face size) of the operator's face image <b>341</b><i>a </i>included in the captured image <b>341</b> (more specifically, for example, a distance between operator's eyes on the captured image <b>341</b>) is large with respect to the captured image <b>341</b>, as depicted in <figref idref="DRAWINGS">FIG. 6B</figref>. In practice, by detecting a relative dimension of the face size with respect to the captured image <b>341</b> using a face detector, the distance from the operator <b>10</b> to the imaging unit <b>104</b> can be determined. In other words, a change of the distance can be detected based on a change of the face size. Here, the width of the face rectangle can be used substantially as a distance between eyes, which is an indicator of the face size. Alternatively, the distance can be similarly determined based on face size information obtained by using a distance between elements configuring a face, such as a nose, a mouth, and eyebrows, and the size of the entire face included in the captured image <b>341</b>.</p>
<p id="p-0070" num="0069">Furthermore, when the operator <b>10</b> moves away from the imaging unit <b>104</b> as depicted in <figref idref="DRAWINGS">FIG. 7A</figref>, the distance between operator's eyes in the operator's face image <b>341</b><i>a </i>included in the captured image <b>341</b> is relatively small with respect to the captured image <b>341</b>, as depicted in <figref idref="DRAWINGS">FIG. 7B</figref>.</p>
<p id="p-0071" num="0070">In this manner, by detecting a relative dimension of the distance between operator's eyes included in the captured image <b>341</b> using the face detector, the distance (face distance) from the operator to the imaging unit (and also the display screen) can be determined.</p>
<p id="p-0072" num="0071"><figref idref="DRAWINGS">FIGS. 8A</figref>, <b>8</b>B, <b>9</b>A, <b>9</b>B, <b>10</b>A, and <b>10</b>B depict examples when the magnification ratio of display information on the display screen of the mobile terminal <b>100</b> in the respective cases in <figref idref="DRAWINGS">FIGS. 5A</figref>, <b>5</b>B, <b>6</b>A, <b>6</b>B, <b>7</b>A, and <b>7</b>B is changed in a hands-free manner. Here, &#x201c;hands-free&#x201d; indicates that the operator is free from any key operation or menu operation, and does not indicate that both hands are completely available (normally, at least one hand is used to hold the mobile terminal <b>100</b>). That is, each of <figref idref="DRAWINGS">FIGS. 8A</figref>, <b>9</b>A, and <b>10</b>A depicts a positional relation between the operator <b>10</b> and the mobile terminal <b>100</b>. Each of <figref idref="DRAWINGS">FIGS. 8B</figref>, <b>9</b>B, and <b>10</b>B depicts a display example of the same display contents. The display contents in this example represent text (that is, characters). As is evident from these drawings, in comparison with the case in <figref idref="DRAWINGS">FIG. 8A</figref> where the face distance of the operator <b>10</b> is a standard intermediate distance, when the face distance becomes short because the operator <b>10</b> gets closer to the mobile terminal <b>100</b> as depicted in <figref idref="DRAWINGS">FIG. 9A</figref>, the text (that is, the character size) is displayed as being enlarged as depicted in <figref idref="DRAWINGS">FIG. 9B</figref>, compared with the case in <figref idref="DRAWINGS">FIG. 8B</figref>. Also, in comparison with the case in <figref idref="DRAWINGS">FIG. 8A</figref>, when the face distance becomes long because the operator <b>10</b> moves away from the mobile terminal <b>100</b> as depicted in <figref idref="DRAWINGS">FIG. 10A</figref>, the text is displayed as being reduced as depicted in <figref idref="DRAWINGS">FIG. 10B</figref>, compared with the case in <figref idref="DRAWINGS">FIG. 8B</figref>.</p>
<p id="p-0073" num="0072">In this manner, when the operator <b>10</b> relatively moves his/her face closer to the mobile terminal <b>100</b>, the display contents (display image) are displayed as being enlarged. When the operator <b>10</b> moves his/her face away from the mobile terminal <b>100</b>, the display contents are displayed as being reduced.</p>
<p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. 11A</figref> is a graph depicting a relation between a magnification ratio M and a face distance D. The face distance D changes between a predetermined short distance Da and a predetermined long distance Db, their center being an intermediate reference distance Dr. When the face distance D changes from the reference distance Dr to the short distance Da, the magnification ratio M successively changes from 1 to Ma (Ma&#x3e;1). When the face distance D changes from the reference distance Dr to the long distance Db, the magnification ratio M successively changes from 1 to Mb (Mb&#x3c;1). The magnification ratio M may change not successively but stepwise.</p>
<p id="p-0075" num="0074">In this manner, in the present embodiment, the magnification ratio of the display image is determined according to the difference (or ratio) of a measurement value with respect to the reference value of the face distance. However, a variable range from the short distance Da to the long distance Db is restricted by itself. For example, after the face is moved closest to the screen from an intermediate face distance to a position represented by the short distance Da where the screen can be visually viewed and the display image is enlarged to a size with a magnification ratio Ma<b>1</b>, when the display image is desired to be further enlarged, it is difficult to move the face closer. In such cases, a specific face change (for example, closing one or both of eyes) is used. With such a face change being detected, even when the face distance is changed, the current magnification ratio for the display contents can be maintained. For example, as depicted in <figref idref="DRAWINGS">FIG. 11B</figref>, to further enlarge the display contents after the magnification ratio becomes Ma<b>1</b> with the face distance Da, the face is returned to the reference distance Dr with the specific face change, and then the face is moved closer again without a face change. With this, a larger magnification ratio Ma<b>2</b> can be obtained.</p>
<p id="p-0076" num="0075">The similar operation can also be performed for reduction. That is, when the display image is desired to be further reduced after the face is moved away from the screen to the distance Db to reduce the display image to a magnification ratio Mb<b>1</b>, the face is returned to the reference distance Dr with the specific face change, and then the face is again moved away without a face change. With this, a smaller magnification ratio Mb<b>2</b> can be obtained.</p>
<p id="p-0077" num="0076">The specific face change is not restricted to closing eye(s). An arbitrary face change recognizable from the captured face image can be used. For example, the specific face change may be opening the mouth.</p>
<p id="p-0078" num="0077">In another technique of enlarging the variable ranges, as depicted in <figref idref="DRAWINGS">FIG. 12A</figref>, when the face distance is the predetermined reference distance Dr (as depicted in <figref idref="DRAWINGS">FIG. 12A</figref>, a certain range of neutral zone may be provided), a magnification ratio change velocity V is set at 0 (that is, the magnification ratio is maintained at the current value). While the face distance D is shorter than the reference distance Dr, a magnification ratio change velocity is set at a predetermined positive value Va, and enlargement is continued. Conversely, while the face distance D is larger than the reference distance Dr, the magnification ratio change velocity is set at a predetermined negative value Vb, and reduction is continued. In this case, the operator normally places his/her face near the reference distance Dr. When the operator desires to enlarge the display contents, the operator moves his/her face closer to the terminal. With this, the display contents are enlarged successively or stepwise at the magnification ratio change velocity Va. When the face distance is returned to the vicinity of reference distance Dr with enlargement to a desired magnification ratio, enlargement is stopped, and the magnification ratio at that moment is maintained. Conversely, when the display contents are desired to be reduced, the face is moved away from the terminal. With this, the display contents are reduced successively or stepwise at the magnification ratio change velocity Vb. When the face distance is returned to the original with reduction at a desired magnification ratio, reduction is stopped, and the magnification ratio at that moment is maintained.</p>
<p id="p-0079" num="0078">The reference distance Dr and its neutral zone may each have a predetermined fixed value, or may be arbitrarily set by the operator. Also, as depicted in <figref idref="DRAWINGS">FIG. 12B</figref>, the magnification ratio change velocity may be constant irrespectively of a change of the face distance, or the change velocity may be changed according to an absolute value of the difference between the face distance and the reference value of the face distance (the change velocity may be increased as the absolute value is larger).</p>
<p id="p-0080" num="0079">While the operation of enlarging/reducing the display contents have been described above, the display contents can also be moved in a hands-free manner. Such an example is described with reference to <figref idref="DRAWINGS">FIGS. 13A</figref>, <b>13</b>B, <b>14</b>A, <b>14</b>B, <b>15</b>A, and <b>15</b>B. Each of <figref idref="DRAWINGS">FIGS. 13A</figref>, <b>14</b>A, and <b>15</b>A depicts a positional relation between the operator <b>10</b> and the mobile terminal <b>100</b>. Each of <figref idref="DRAWINGS">FIGS. 13B</figref>, <b>14</b>B, and <b>15</b>B depicts a display example of display information (here, a map) in each case.</p>
<p id="p-0081" num="0080">When the map is displayed at a magnification ratio as depicted in <figref idref="DRAWINGS">FIG. 13B</figref> in the state of the face distance as depicted in <figref idref="DRAWINGS">FIG. 13A</figref>, if the face is moved away from the mobile terminal <b>100</b> as depicted in <figref idref="DRAWINGS">FIG. 14A</figref> to increase the face distance, the magnification ratio of the map is decreased to cause a reduced display. As a result, a wide range of areas can be viewed.</p>
<p id="p-0082" num="0081">From this state, when the operator <b>10</b> moves his/her face in a right direction closer to the mobile terminal <b>100</b> as depicted in <figref idref="DRAWINGS">FIG. 15A</figref>, a face distance is detected, and a face position is also detected by the face position detecting unit <b>315</b> (<figref idref="DRAWINGS">FIG. 3</figref>) described above. With this movement of the face position, the display contents are also moved. In the example depicted in <figref idref="DRAWINGS">FIGS. 15A and 15B</figref>, when the face is moved to right while facing the mobile terminal <b>100</b>, the map is moved in a left direction on the display screen. Also, simultaneously with a decrease in the face distance, the map is enlarged for display. In this operation, by moving the face in an end direction on a side on the display screen the operator <b>10</b> desires to view, display information hidden on an end side is moved to a screen center side. To view information about a station at right on the screen in <figref idref="DRAWINGS">FIG. 14B</figref> in more detail, the operator <b>10</b> moves his/her face in a right direction closer to the terminal as depicted in <figref idref="DRAWINGS">FIG. 15A</figref> for display. As a result, the information about the station is displayed as being enlarged on the display unit <b>103</b> as depicted in <figref idref="DRAWINGS">FIG. 15B</figref>.</p>
<p id="p-0083" num="0082">Contrary to the example in <figref idref="DRAWINGS">FIGS. 15A and 15B</figref>, by moving the face to left, the map is moved in a right direction on the display screen. The similar operation can also be performed not only in a horizontal direction but also in a vertical direction.</p>
<p id="p-0084" num="0083">By determining a position and dimension of the face image together and moving the map for display while changing the magnification ratio, a large range of information can be comfortably viewed even with a small display screen of the mobile terminal. That is, a detailed display and an at-a-glance display can be intuitively switched for use in a hands-free manner.</p>
<p id="p-0085" num="0084">The variable ranges are restricted also for the movement of the face. As with enlargement and reduction, two techniques (not illustrated) are available to cope with this restriction. In a first technique, in the case where the display contents are moved according to the movement amount of the face, the movement of the display contents does not reflect the movement of the face when a state of a specific face change is detected.</p>
<p id="p-0086" num="0085">In a second technique, the display contents are continuously moved in a predetermined direction defined by a direction of a shift in the face position with respect to a reference position (typically, a front center of the terminal). That is, the face is moved from the center in a direction of a hidden portion of the display contents to be desired to be viewed, and then the face is returned to the center when the display contents are moved by a desired amount.</p>
<p id="p-0087" num="0086">Here, the movement of the display information does not continue infinitely. After an end of the display information (for example, an end of a page) appears on the display screen, the movement stops regardless an instruction for movement.</p>
<p id="p-0088" num="0087">In the example above, the display contents are enlarged when the face gets closer to the terminal, and are reduced when the face moves away from the terminal. Conversely, in another example, the display contents may be enlarged when the face moves away from the terminal, and be reduced when the face gets closer to the terminal. This can support presbyopic operators.</p>
<p id="p-0089" num="0088">As described above, according to the present embodiment, the display contents of the terminal can be easily enlarged or reduced even by an operator with weak eyesight. With a myopic operator moving his/her eyes closer to the terminal or with a presbyopic operator moving his/her eyes away from the terminal, the display information is enlarged. With this, the information displayed on the screen can be used even when the displayed information difficult to view.</p>
<p id="p-0090" num="0089">While the preferred embodiments of the present invention have been described above, various modifications and changes can be made other than those described above.</p>
<p id="p-0091" num="0090">For example, these various operations described above can be performed by providing a plurality of display modes and selectively setting any one of the display modes by the operator.</p>
<p id="p-0092" num="0091">While the face distance detecting unit uses the imaging unit in the example above, another distance detecting unit using infrared rays or ultrasonic waves may be used instead.</p>
<p id="p-0093" num="0092">Furthermore, the above description assumes the mobile terminal to be a mobile phone terminal, but the mobile terminal may not necessarily have a phone function. For example, any mobile terminal having a display screen, such as a PDA, a game machine, and a small PC, is applicable. Moreover, though the mobile terminal according to the embodiments of the present invention is preferably a portable-type mobile terminal, the mobile terminal is not limited to the portable type. The face distance detecting unit using a camera or the like can be included in a generally-used PC or a terminal installed in a public space, such as a ticket dispenser or a guide board.</p>
<p id="p-0094" num="0093">In the example above, the face distance and the face position are both detected. However, when only enlarging or reducing the display contents is performed without movement, a detector configured to detect a face position can be omitted.</p>
<p id="p-0095" num="0094">In the example above, the lens of the imaging unit is positioned above the rectangular display screen in portrait orientation. Alternatively, the lens can be positioned on a side of the display screen in landscape orientation for use. In this case, as with the case above, the face position may be corrected with an amount of shift between the center of the display screen and the imaging unit.</p>
<p id="p-0096" num="0095">Whether to apply the operation of changing the display magnification ratio based on the face distance (and the moving operation based on the face position) described above may be defined fixedly for each application or by user's settings.</p>
<p id="p-0097" num="0096">According to the embodiments of the present invention, there are also provided a computer program causing a computer to perform the functions described in the above embodiments, and a recording medium which stores the program in a computer readable manner. Examples of the recording medium for supplying the program include a magnetic recording medium (a flexible disk, a hard disk, magnetic tape, and the like), an optical disk (a magneto-optical disk such as an MO or a PD, a CD, a DVD, and the like), a semiconductor storage, paper tape, and so on.</p>
<p id="p-0098" num="0097">The present application contains subject matter related to that disclosed in Japanese Priority Patent Application JP 2009-015137 filed in the Japan Patent Office on Jan. 27, 2009, the entire content of which is hereby incorporated by reference.</p>
<p id="p-0099" num="0098">It should be understood by those skilled in the art that various modifications, combinations, sub-combinations and alterations may occur depending on design requirements and other factors insofar as they are within the scope of the appended claims or the equivalents thereof.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A display apparatus comprising:
<claim-text>display means for displaying information on a display screen;</claim-text>
<claim-text>distance detecting means for detecting a distance from the display screen to the face of an operator; and</claim-text>
<claim-text>control means for causing the information to be enlarged or educed on the display screen in response to a change of the detected distance, wherein the distance detecting means includes imaging means for capturing an image of the face of the operator who is viewing the display screen, and the control means detects the distance from the display screen to the face of the operator by detecting a ratio of a face image of the operator to the captured image,</claim-text>
<claim-text>wherein the control means determines a magnification ratio of the image according to a ratio or difference in detection size of the face image with respect to a reference size,</claim-text>
<claim-text>wherein the control means causes the magnification ratio of the image to increase when the detected distance from the display screen to the face of the operator changes in one direction from a start distance and causes the magnification ratio to decrease when the detected distance changes in an opposite direction, and</claim-text>
<claim-text>wherein the control means is configured to detect a specific face change in the image of the face of the operator, wherein in response to the specific face change, the magnification ratio is maintained at its last changed ratio even when the distance from the display screen to the face of the operator is returned toward the start distance and further, when the specific face change no longer is detected, the magnification ratio is increased or decreased again when the detected distance from the display screen to the face of the operator changes in said one or opposite direction once again.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The display apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the control means continues to cause the information to be enlarged or reduced successively or stepwise according to a sign of the difference in detection size of the face image with respect to the reference size, and stops changing the magnification ratio when the difference becomes 0.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The display apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the magnification ratio is increased as the distance is shorter.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The display apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the magnification ratio is decreased as the distance is shorter.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The display apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a first display mode and a second display mode that are selectable by the operator are provided, and the magnification ratio is increased as the distance is shorter in the first display mode and the magnification ratio is decreased as the distance is shorter in the second display mode.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The display apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the control means causes the information to be moved for display in a predetermined direction associated with a moving direction of the face in response to a change in position of the face of the operator in a direction parallel to the display screen.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The display apparatus according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the control means determines a movement amount for moving the information for display according to a difference in detected position of the face image with respect to a reference position.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The display apparatus according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the control means causes the information to be successively moved for display according to a sign of a difference in detected position of the face image with respect to the reference position, and stops movement when the difference becomes 0.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. A display control method comprising the steps of:
<claim-text>displaying information on a display screen;</claim-text>
<claim-text>detecting a distance from the display screen to the face of an operator; and</claim-text>
<claim-text>causing the information to be enlarged or reduced on the display screen in response to a change of the detected distance, wherein the step of detecting the distance includes the steps of:</claim-text>
<claim-text>capturing an image of the face of the operator who is viewing the display screen; and</claim-text>
<claim-text>detecting a ratio of a face image of the operator to the captured image,</claim-text>
<claim-text>wherein a magnification ratio of the image is determined according to a ratio or difference in detection size of the face image with respect to a reference size,</claim-text>
<claim-text>the magnification ratio of the image increases when the detected distance from the display screen to the face of the operator changes in one direction from a start distance and the magnification ratio decreases when the detected distance changes in an opposite direction, and</claim-text>
<claim-text>wherein the magnification ratio is maintained at its last changed ratio even when the distance from the display screen to the face of the operator is returned toward the start distance when a specific face change is detected in the image of the face of the operator, and further, when the specific face change no longer is detected, the magnification ratio is increased or decreased again when the detected distance from the display screen to the face of the operator changes in said one or opposite direction once again.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. A non-transitory computer readable medium causing a computer to execute the steps of:
<claim-text>displaying information on a display screen;</claim-text>
<claim-text>detecting a distance from the display screen to the face of an operator; and</claim-text>
<claim-text>causing the information to be enlarged or reduced on the display screen in response to a change of the detected distance, wherein the step of detecting the distance includes the steps of:</claim-text>
<claim-text>capturing an image of the face of the operator who is viewing the display screen; and</claim-text>
<claim-text>detecting a ratio of a face image of the operator to the captured image,</claim-text>
<claim-text>wherein a magnification ratio of the image is determined according to a ratio or difference in detection size of the face image with respect to a reference size,</claim-text>
<claim-text>the magnification ratio of the image increases when the detected distance from the display screen to the face of the operator changes in one direction from a start distance and the magnification ratio decreases when the detected distance changes in an opposite direction, and</claim-text>
<claim-text>wherein the magnification ratio is maintained at its last changed ratio even when the distance from the display screen to the face of the operator is returned toward the start distance when a specific face change is detected in the image of the face of the operator, and further, when the specific face change no longer is detected, the magnification ratio is increased or decreased again when the detected distance from the display screen to the face of the operator changes in said one or opposite direction once again.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. A display apparatus comprising:
<claim-text>a display unit displaying information on a display screen;</claim-text>
<claim-text>a distance detector detecting a distance from the display screen to the face of an operator; and</claim-text>
<claim-text>a controller causing the information to be enlarged and reduced on the display screen in response to a change of the detected distance, wherein the distance detector includes imaging means for capturing an image of the face of the operator who is viewing the display screen, and the controller detects the distance from the display screen to the face of the operator by detecting a ratio of a face image of the operator to the captured image,</claim-text>
<claim-text>wherein the controller determines a magnification ratio of the image according to a ratio or difference in detection size of the face image with respect to a reference size,</claim-text>
<claim-text>wherein the controller causes the magnification ratio of the image to increase when the detected distance from the display screen to the face of the operator changes in one direction from a start distance and causes the magnification ratio to decrease when the detected distance changes in an opposite direction, and</claim-text>
<claim-text>wherein the controller is configured to detect a specific face change in the image of the face of the operator, wherein in response to the specific face change, the magnification ratio is maintained at its last changed ratio even when the distance from the display screen to the face of the operator is returned toward the start distance and further, when the specific face change no longer is detected, the magnification ratio is increased or decreased again when the detected distance from the display screen to the face of the operator changes in said one or opposite direction once again. </claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
