<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626677-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626677</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12790532</doc-number>
<date>20100528</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<us-term-of-grant>
<us-term-extension>710</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>15</main-group>
<subgroup>18</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>706 12</main-classification>
</classification-national>
<invention-title id="d2e53">Training SVMs with parallelized stochastic gradient descent</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>2003/0158830</doc-number>
<kind>A1</kind>
<name>Kowalczyk et al.</name>
<date>20030800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>2005/0049990</doc-number>
<kind>A1</kind>
<name>Milenova et al.</name>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>2006/0112026</doc-number>
<kind>A1</kind>
<name>Graf et al.</name>
<date>20060500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>2007/0094170</doc-number>
<kind>A1</kind>
<name>Graf et al.</name>
<date>20070400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00005">
<othercit>Zhu et al (&#x201c;P-packSVM: Parallel Primal grAdient desCent Kernel SVM&#x201d; ICDM Dec. 2009).</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00006">
<othercit>Graf et al (&#x201c;Parallel Support Vector Machines: The cascade svm&#x201d; 2005).</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00007">
<othercit>Amdahl, &#x201c;Validity of the single processor approach to achieving large scale computing capabilities&#x201d;, retrieved on Apr. 17, 2010 at &#x3c;&#x3c;http://userweb.cs.utexas.edu/&#x2dc;dburger/cs395t/papers/5<sub>&#x2014;</sub>amdahl.pdf&#x3e;&#x3e;, ACM, AFIPS Joint Computer Conferences, 1967, pp. 483-485.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00008">
<othercit>Cao, Keerthi, Ong, Uvaraj, Fu, Lee, Zhang, &#x201c;Parallel Sequential Minimal Optimization for the Training of Support Vector Machines&#x201d;, retrieved on Apr. 17, 2010 at &#x3c;&#x3c;http://guppy.mpe.nus.edu.sg/&#x2dc;mpeongcj/html/%20of%20papers/parallel<sub>&#x2014;</sub>SMO<sub>&#x2014;</sub>IEEE.pdf&#x3e;&#x3e;, IEEE Transactions on Neural Networks, vol. 17, No. 4, Jul. 2006, pp. 1039-1049.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00009">
<othercit>Chang, Lin, &#x201c;LIBSVM: a Library for Support Vector Machines&#x201d;, retrieved on Apr. 17, 2010 at &#x3c;&#x3c;http://www.csie.ntu.edu.tw/&#x2dc;cjlin/libsvm&#x3e;&#x3e;, National Taiwan University Technical Report, Computer Science and Information Engineering, 2001-2004, pp. 1-30. (software available at http://www.csie.ntu.edu.tw/&#x2dc;cjlin/libsvm).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00010">
<othercit>Chang, Zhu, Wang, Bai, &#x201c;PSVM: Parallelizing Support Vector Machines on Distributed Computers&#x201d;, retrieved on Apr. 17, 2010 at &#x3c;&#x3c;http://books.nips.cc/papers/files/nips20/NIPS2007<sub>&#x2014;</sub>0435.pdf&#x3e;&#x3e;, MIT Press, Advances in Neural Information Processing Systems (NIPS), 2007, pp. 257-264.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00011">
<othercit>Collobert, Bengio, Bengio, &#x201c;A Parallel Mixture of SVMs for Very Large Scale Problems&#x201d;, retrieved on Apr. 17, 2010 at &#x3c;&#x3c;http://ronan.collobert.com/pub/matos/2002<sub>&#x2014;</sub>mixtures<sub>&#x2014;</sub>nips.pdf&#x3e;&#x3e;, MIT Press, Neural Computation , vol. 14, No. 5, 2002, pp. 1105-1114.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00012">
<othercit>Fan, &#x201c;LIBSVM Data: Classification, Regression, and Multilabel&#x201d;, retrieved on Apr. 19, 2010 at &#x3c;&#x3c;http://www.csie.ntu.edu.tw/&#x2dc;cjlin/libsvmtools/datasets/&#x3e;&#x3e;, National Taiwan University, Nov. 28, 2006, pp. 1-3.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00013">
<othercit>Ferris, Munson, &#x201c;Interior-Point Methods for Massive Support Vector Machines&#x201d;, retrieved on Apr. 17, 2010 at &#x3c;&#x3c;http://pages.cs.wisc.edu/&#x2dc;ferris/papers/siopt-svm.pdf&#x3e;&#x3e;, Society for Industrial and Applied Mathematics, SIAM Journal on Optimization, vol. 13, No. 3, 2002, pp. 783-804.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00014">
<othercit>Fine, Scheinberg, &#x201c;Efficient SVM Training Using Low-Rank Kernel Representations&#x201d;, retrieved on Apr. 17, 2010 at &#x3c;&#x3c;http://www.google.co.in/url?sa=t&#x26;source=web&#x26;ct=res&#x26;cd=1&#x26;ved=0CAYQFjAA&#x26;url=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fdownload%3Fdoi%3D10.1.1.72.2049%26rep%3Drep1%26type%3Dpdf&#x26;rct=j&#x26;q=Efficient+SVM+Training+Using+Low-Rank+Kernel+Representations&#x26;ei=ynnJS7WiAp680gShvJHGBA&#x26;usg=AFQjCNE5YcG2Vv<sub>&#x2014;</sub>yeacTrJvnS5A22y8TgQ&#x3e;&#x3e;, JMLR.org, Journal of Machine Learning Research, vol. 2, 2001, pp. 243-264.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00015">
<othercit>Hush, Kelly, Scovel, Steinwart, &#x201c;QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines&#x201d;, retrieved on Apr. 17, 2010 at &#x3c;&#x3c;http://www.google.co.in/url?sa=t&#x26;source=web&#x26;ct=res&#x26;cd=1&#x26;ved=0CAYQFjAA&#x26;url=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fdownload%3Fdoi%3D10.1.1.61.6884%26rep% 3Drep1%26type%3Dpdf&#x26;rct=j&#x26;q=&#x26;ei=43rJS4rmG4Hw0wSO06XABA&#x26;usg=AFQjCNFu4t5ly6a6lvj<sub>&#x2014;</sub>kTgqkxYXXzlKZA&#x3e;&#x3e;, JMLR.org, Journal of Machine Learning Research, vol. 7, 2006, pp. 733-769.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00016">
<othercit>Joachims, &#x201c;Learning to Classify Text Using Support Vector Machines (Abstract)&#x201d;, retrieved on Apr. 22, 2010 at &#x3c;&#x3c;http://www.cs.cornell.edu/People/tj/svmtcatbook/&#x3e;&#x3e;, Kluwer Academic Publisher, 2002, pp. 1-4.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00017">
<othercit>Joachims, &#x201c;Making Large-Scale SVM Learning Practical&#x201d;, retrieved on Apr. 17, 2010 at &#x3c;&#x3c;http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=A0337C0CDCE440FEAF9F16063CDE0182? doi=10.1.1.52.1136&#x26;rep=rep1&#x26;type=pdf&#x3e;&#x3e;, University of Dortmund, LS 8-Report 24, Jun. 1998, pp. 41-56.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00018">
<othercit>Joachims, &#x201c;Optimizing Search Engines using Clickthrough Data&#x201d;, retrieved on Apr. 17, 2010 at &#x3c;&#x3c;http://www.cs.cornell.edu/People/tj/publications/joachims<sub>&#x2014;</sub>02c.pdf&#x3e;&#x3e;, ACM, Conference on Knowledge Discovery and Data Mining (SIGKDD), 2002, pp. 133-142.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00019">
<othercit>Joachims, &#x201c;Training Linear SVMs in Linear Time&#x201d;, retrieved on Apr. 17, 2010 at &#x3c;&#x3c;http://www.cs.cornell.edu/People/tj/publications/joachims<sub>&#x2014;</sub>06a.pdf&#x3e;&#x3e;, ACM, Conference on Knowledge Discovery and Data Mining (SIGKDD), Aug. 20, 2006, pp. 217-226.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00020">
<othercit>Kakade, Shalev-Shwartz, &#x201c;Mind the Duality Gap: Logarithmic regret algorithms for online optimization&#x201d;, retrieved on Apr. 17, 2010 at &#x3c;&#x3c;http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=74690A8BD8C986458F39432E2014BB73? doi=10.1.1.142.2&#x26;rep=rep1&#x26;type=pdf&#x3e;&#x3e;, MIT Press, Advances in Neural Information Processing Systems (NIPS), Dec. 9, 2008, pp. 1457-1464.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00021">
<othercit>Kivinen, Smola, Williamson, &#x201c;Online Learning with Kernels&#x201d;, retrieved on Apr. 17, 2010 at &#x3c;&#x3c;http://users.cecs.anu.edu.au/&#x2dc;williams/papers/P149.pdf&#x3e;&#x3e;, MIT Press, Advances in Neural Information Processing Systems (NIPS), vol. 14, 2002, pp. 785-792.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00022">
<othercit>Lewis, Yang, Rose, Li, &#x201c;RCV1: A New Benchmark Collection for Text Categorization Research&#x201d;, retrieved on Apr. 17, 2010 at &#x3c;&#x3c;http://jmlr.csail.mit.edu/papers/volume5/lewis04a/lewis04a.pdf&#x3e;&#x3e;, JMLR.org, Journal of Machine Learning Research, vol. 5, 2004, pp. 361-397.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00023">
<othercit>Loosli, Canu, Bottou, &#x201c;Training Invariant Support Vector Machines using Selective Sampling&#x201d;, retrieved on Apr. 17, 2010 at &#x3c;&#x3c;http://upload.pobrun.com/40/uploads/files/publications/techreport<sub>&#x2014;</sub>lasvm.pdf&#x3e;&#x3e;, MIT Press, Large Scale Kernel Machines, 2007, pp. 301-320.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00024">
<othercit>Manning, Raghavan, Schutze, &#x201c;An Introduction to Information Retrieval&#x201d;, retrieved on Apr. 17, 2010 at &#x3c;&#x3c;http://nlp.stanford.edu/IR-book/pdf/00front.pdf&#x3e;&#x3e;, Cambridge University Press, 2009, pp. 1-37.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00025">
<othercit>Mehrotra, &#x201c;On the Implementation of a Primal-Dual Interior Point Method&#x201d;, retrieved on Apr. 19, 2010 at &#x3c;&#x3c;http://scitation.aip.org/getabs/servlet/GetabsServlet? prog=normal&#x26;id=SJOPE8000002000004000575000001&#x26;idtype=cvips&#x26;gifs=yes&#x26;ref=no&#x3e;&#x3e;, SIAM Journal on Optimization, vol. 2, No. 4, Nov. 1992, pp. 575-601.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00026">
<othercit>&#x201c;MPI Documents&#x201d;, retrieved on Apr. 19, 2010 at &#x3c;&#x3c;http://www.mpi-forum.org/docs/&#x3e;&#x3e;, 2010, pp. 1-2.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00027">
<othercit>Platt, &#x201c;Fast Training of Support Vector Machines using Sequential Minimal Optimization&#x201d;, retrieved on Apr. 17, 2010 at &#x3c;&#x3c;http://209.85.129.132/search?q=cache:5JChx7KMQqIJ:research.microsoft.com/en-us/um/people/jplatt/smo-book.ps.gz+Fast+Training+of+Support+Vector+Machines+using+Sequential+Minimal&#x26;cd=1&#x26;hl=en&#x26;ct=clnk&#x26;gl=in&#x26;client=firefox-a&#x3e;&#x3e;, MIT Press, 1999, pp. 185-208.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00028">
<othercit>Shalev-Shwartz, &#x201c;Online Learning: Theory, Algorithms, and Applications&#x201d;, retrieved on Apr. 17, 2010 at &#x3c;&#x3c;Shalev-Shwartz, Online Learning: Theory, Algorithms, and Applications&#x3e;&#x3e;, Hebrew University, Doctoral Thesis, Jul. 2007, pp. 1-162.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00029">
<othercit>Shalev-Shwartz, Singer, Srebro, &#x201c;Pegasos: Primal Estimated sub-GrAdient SOIver for SVM&#x201d;, retrieved on Apr. 17, 2010 at &#x3c;&#x3c;http://ttic.uchicago.edu/&#x2dc;shai/papers/ShalevSiSr07.pdf&#x3e;&#x3e;, ACM, Proceedings of Conference on Machine Learning (ICML), vol. 227, 2007, pp. 807-814.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00030">
<othercit>Shalev-Shwartz, Srebro, &#x201c;SVM Optimization: Inverse Dependence on Training Set Size&#x201d;, retrieved on Apr. 17, 2010 at &#x3c;&#x3c;http://icmI2008.cs.helsinki.fi/papers/266.pdf&#x3e;&#x3e;, ACM, Proceedings of Conference on Machine Learning (ICML), vol. 307, 2008, pp. 928-935.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00031">
<othercit>Woodsend, Gondzio, &#x201c;Parallel support vector machine training with nonlinear kernels&#x201d;, retrieved on Apr. 17, 2010 at &#x3c;&#x3c;http://bigml.wikispaces.com/file/view/Woodsend.pdf&#x3e;&#x3e;, Technical Report MS-07-007, School of Mathematics, The University of Edinburgh, Nov. 23, 2007, pp. 1-2.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00032">
<othercit>Zanghirati, Zanni, &#x201c;A parallel solver for large quadratic programs in training support vector machines&#x201d;, retrieved on Apr. 17, 2010 at &#x3c;&#x3c;http://dm.unife.it/gpdt/zz-parco2003.pdf&#x3e;&#x3e;, Elsevier Science Publishers Amsterdam, Parallel Computing, vol. 29, No. 4, Apr. 2003, pp. 535-551.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00033">
<othercit>Zhang, &#x201c;Solving Large Scale Linear Prediction Problems Using Stochastic Gradient Descent Algorithms&#x201d;, retrieved on Apr. 17, 2010 at &#x3c;&#x3c;http://delivery.acm.org/10.1145/1020000/1015332/p12-zhang.pdf? key1=1015332&#x26;key2=8793561721&#x26;coll=GUIDE&#x26;dl=GUIDE&#x26;CFID=84854457&#x26;CFTOKEN=19659702&#x3e;&#x3e;, ACM, Proceedings of Conference on Machine Learning (ICML), vol. 69, 2004, pp. 116-124.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>18</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>706 12</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>6</number-of-drawing-sheets>
<number-of-figures>6</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20110295774</doc-number>
<kind>A1</kind>
<date>20111201</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Chen</last-name>
<first-name>Weizhu</first-name>
<address>
<city>Beijing</city>
<country>CN</country>
</address>
</addressbook>
<residence>
<country>CN</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Wang</last-name>
<first-name>Gang</first-name>
<address>
<city>Beijing</city>
<country>CN</country>
</address>
</addressbook>
<residence>
<country>CN</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Chen</last-name>
<first-name>Zheng</first-name>
<address>
<city>Beijing</city>
<country>CN</country>
</address>
</addressbook>
<residence>
<country>CN</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Chen</last-name>
<first-name>Weizhu</first-name>
<address>
<city>Beijing</city>
<country>CN</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Wang</last-name>
<first-name>Gang</first-name>
<address>
<city>Beijing</city>
<country>CN</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Chen</last-name>
<first-name>Zheng</first-name>
<address>
<city>Beijing</city>
<country>CN</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Lee &#x26; Hayes, PLLC</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Microsoft Corporation</orgname>
<role>02</role>
<address>
<city>Redmond</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Wong</last-name>
<first-name>Lut</first-name>
<department>2129</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Techniques for training a non-linear support vector machine utilizing a stochastic gradient descent algorithm are provided. The computations of the stochastic gradient descent algorithm are parallelized via a number of processors. Calculations of the stochastic gradient descent algorithm on a particular processor may be combined according to a packing strategy before communicating the results of the calculations with the other processors.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="242.40mm" wi="189.82mm" file="US08626677-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="270.00mm" wi="191.60mm" file="US08626677-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="275.34mm" wi="192.28mm" file="US08626677-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="276.01mm" wi="121.16mm" file="US08626677-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="265.51mm" wi="147.49mm" file="US08626677-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="274.24mm" wi="185.25mm" file="US08626677-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="270.00mm" wi="170.18mm" file="US08626677-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND</heading>
<p id="p-0002" num="0001">Support vector machines (SVMs) are a type of machine learning method that can be used for classification, regression analysis, and ranking For example, based on a set of training data samples that are each associated with one category or another, SVMs may be used to predict which category a new data sample will be associated with. The data samples may be expressed as an ordered pair including a vector that indicates features of a particular data sample and a classifier that indicates the category of the particular data sample. In a particular example, the set of training data &#x3a8; may be given by:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>&#x3a8;={(<i>x</i><sub>i</sub><i>,y</i><sub>i</sub>)|<i>x</i><sub>i</sub>&#x3b5;<img id="CUSTOM-CHARACTER-00001" he="3.13mm" wi="2.12mm" file="US08626677-20140107-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/><sup>n</sup><i>,y</i><sub>i</sub>&#x3b5;{&#x2212;1,1}}<sub>i=1</sub><sup>m</sup>&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where x<sub>i </sub>is a feature vector for a particular sample, y<sub>i </sub>is the classifier of the particular sample, m is the number of samples in a set of training data, and <img id="CUSTOM-CHARACTER-00002" he="3.13mm" wi="2.12mm" file="US08626677-20140107-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/> is the set of real numbers.
</p>
<p id="p-0003" num="0002">Continuing with the classification example, SVMs construct a hyperplane having one or more dimensions that may separate data samples into two categories. An optimal solution given by the SVM is the hyperplane that provides the largest separation between vectors of the two categories. The vectors that limit the amount of separation between the two categories are often referred to as the &#x201c;support vectors.&#x201d;</p>
<p id="p-0004" num="0003">In some instances, linear hyperplanes separate data samples in the two categories. In other instances, non-linear hyperplanes separate the data samples in the two categories. When non-linear hyperplanes separate the data samples, SVMs may utilize a kernel function to map the data into a different space having higher dimensions, such as the Reproducing Kernel Hilbert Space for a Mercer kernel <img id="CUSTOM-CHARACTER-00003" he="3.13mm" wi="3.13mm" file="US08626677-20140107-P00002.TIF" alt="custom character" img-content="character" img-format="tif"/>. In this way, a linear hyperplane can be used to separate data that would otherwise be separated by a non-linear curve with complex boundaries.</p>
<p id="p-0005" num="0004">The primal form of the objective function to be solved by non-linear SVMs is given by:</p>
<p id="p-0006" num="0005">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>f</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mi>w</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mrow>
            <mfrac>
              <mi>&#x3c3;</mi>
              <mn>2</mn>
            </mfrac>
            <mo>&#x2062;</mo>
            <msubsup>
              <mrow>
                <mo>&#xf605;</mo>
                <mi>w</mi>
                <mo>&#xf606;</mo>
              </mrow>
              <mn>2</mn>
              <mn>2</mn>
            </msubsup>
          </mrow>
          <mo>+</mo>
          <mrow>
            <mfrac>
              <mn>1</mn>
              <mi>m</mi>
            </mfrac>
            <mo>&#x2062;</mo>
            <mrow>
              <munderover>
                <mo>&#x2211;</mo>
                <mrow>
                  <mi>i</mi>
                  <mo>=</mo>
                  <mn>1</mn>
                </mrow>
                <mi>m</mi>
              </munderover>
              <mo>&#x2062;</mo>
              <mrow>
                <mi>max</mi>
                <mo>&#x2062;</mo>
                <mrow>
                  <mo>{</mo>
                  <mrow>
                    <mn>0</mn>
                    <mo>,</mo>
                    <mrow>
                      <mn>1</mn>
                      <mo>-</mo>
                      <mrow>
                        <msub>
                          <mi>y</mi>
                          <mi>i</mi>
                        </msub>
                        <mo>&#x2062;</mo>
                        <mrow>
                          <mo>&#x2329;</mo>
                          <mrow>
                            <mi>w</mi>
                            <mo>,</mo>
                            <mrow>
                              <mi>&#x3d5;</mi>
                              <mo>&#x2061;</mo>
                              <mrow>
                                <mo>(</mo>
                                <msub>
                                  <mi>x</mi>
                                  <mi>i</mi>
                                </msub>
                                <mo>)</mo>
                              </mrow>
                            </mrow>
                          </mrow>
                          <mo>&#x232a;</mo>
                        </mrow>
                      </mrow>
                    </mrow>
                  </mrow>
                  <mo>}</mo>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>2</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where w is a predictor vector that is normal to the hyperplane that provides maximum separation between two classes. In addition, &#x3c3; is a regularizer weight of the regularization function
</p>
<p id="p-0007" num="0006">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mrow>
  <mfrac>
    <mi>&#x3c3;</mi>
    <mn>2</mn>
  </mfrac>
  <mo>&#x2062;</mo>
  <msubsup>
    <mrow>
      <mo>&#xf605;</mo>
      <mi>w</mi>
      <mo>&#xf606;</mo>
    </mrow>
    <mn>2</mn>
    <mn>2</mn>
  </msubsup>
</mrow>
</math>
</maths>
<br/>
that is used to make the objective function more regular or smooth. Further, the term
</p>
<p id="p-0008" num="0007">
<maths id="MATH-US-00003" num="00003">
<math overflow="scroll">
<mrow>
  <mfrac>
    <mn>1</mn>
    <mi>m</mi>
  </mfrac>
  <mo>&#x2062;</mo>
  <mrow>
    <munderover>
      <mo>&#x2211;</mo>
      <mrow>
        <mi>i</mi>
        <mo>=</mo>
        <mn>1</mn>
      </mrow>
      <mi>m</mi>
    </munderover>
    <mo>&#x2062;</mo>
    <mrow>
      <mi>max</mi>
      <mo>&#x2062;</mo>
      <mrow>
        <mo>{</mo>
        <mrow>
          <mn>0</mn>
          <mo>,</mo>
          <mrow>
            <mn>1</mn>
            <mo>-</mo>
            <mrow>
              <msub>
                <mi>y</mi>
                <mi>i</mi>
              </msub>
              <mo>&#x2062;</mo>
              <mrow>
                <mo>&#x2329;</mo>
                <mrow>
                  <mi>w</mi>
                  <mo>,</mo>
                  <mrow>
                    <mi>&#x3d5;</mi>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>(</mo>
                      <msub>
                        <mi>x</mi>
                        <mi>i</mi>
                      </msub>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                </mrow>
                <mo>&#x232a;</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
        <mo>}</mo>
      </mrow>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
<br/>
may be referred herein to as the loss function for the SVM primal objective function.
</p>
<p id="p-0009" num="0008">Training non-linear support vector machines can be resource intensive and time consuming. Many SVM training algorithms optimize a dual form of the objective function using Lagrangian multipliers. However, in some cases, these algorithms may sacrifice accuracy for speed. In addition, attempts to reduce the amount of time to train non-linear SVMs by parallelizing computations among a number of processors to optimize the dual form of the objective function have provided marginal results.</p>
<heading id="h-0002" level="1">SUMMARY</heading>
<p id="p-0010" num="0009">This disclosure describes training non-linear SVMs using parallelized stochastic gradient descent algorithms. The stochastic gradient descent algorithms train non-linear SVMs by optimizing the primal SVM objective function. In addition, the computations of the stochastic gradient descent algorithms may be parallelized on a number of processors. Calculations of the stochastic gradient descent algorithm on a particular processor may be combined according to a packing strategy before communicating the results of the calculations with the other processors.</p>
<p id="p-0011" num="0010">This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key or essential features of the claimed subject matter, nor is it intended to be used as an aid in determining the scope of the claimed subject matter.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE CONTENTS</heading>
<p id="p-0012" num="0011">The detailed description is described with reference to the accompanying Figures. In the Figures, the left-most digit(s) of a reference number identifies the Figure in which the reference number first appears. The use of the same reference numbers in different Figures indicates similar or identical items or features.</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram illustrating training of a support vector machine using a parallelized stochastic gradient descent algorithm.</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 2</figref> illustrates a system to train support vector machines using a stochastic gradient descent algorithm.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 3</figref> is a flow diagram of a method to train a non-linear support vector machine with a parallelized stochastic gradient descent algorithm and to use the trained support vector machine to characterize a new data sample.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 4</figref> is a flow diagram of a method to determine a predictor vector for the primal objective function of a support vector machine.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 5</figref> is a flow diagram of a method to determine a predictor vector for the primal objective function of a support vector machine utilizing a hash table distributed among a plurality of processors.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 6</figref> is a flow diagram of a method to implement a packing strategy when determining a predictor vector for the primal objective function of a support vector machine via a parallelized stochastic gradient descent algorithm.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 1</figref> illustrates an example framework <b>100</b> for training a non-linear support vector machine (SVM) <b>102</b> using a parallelized stochastic gradient descent algorithm <b>104</b>. The non-linear SVM <b>102</b> may also be referred to in this disclosure as a kernel SVM. The stochastic gradient descent algorithm <b>104</b> may be used to fit parameters of the primal objective function of the non-linear SVM <b>102</b>. In particular, the stochastic gradient descent algorithm <b>104</b> may determine a predictor vector <b>106</b> for the primal objective function of the non-linear SVM <b>102</b>.</p>
<p id="p-0020" num="0019">The non-linear SVM <b>102</b> may be implemented using a set of training data <b>108</b> that includes a plurality of samples <b>110</b>. Each sample <b>110</b> of the set of training data <b>108</b> may include an ordered key-value pair having a feature vector <b>112</b> as a key and a corresponding value <b>114</b>. The stochastic gradient descent algorithm <b>104</b> may be executed over a number of iterations utilizing a respective training data sample <b>110</b> for each iteration to determine the predictor vector <b>106</b> for the primal objective function of the non-linear SVM <b>102</b>. The number of iterations may represent a particular number of iterations that allow the stochastic gradient descent algorithm <b>104</b> to converge. In some cases, the number of iterations may be predetermined.</p>
<p id="p-0021" num="0020">The computations of the stochastic gradient descent algorithm <b>104</b> may implement a parallelization method <b>116</b> on a number of processors <b>118</b>(<b>1</b>) . . . <b>118</b>(N). For example, each of a plurality of processors <b>118</b> may perform a different set of calculations to execute the stochastic gradient descent algorithm <b>104</b>. The processors <b>118</b> may communicate with each other to share the results of their respective computations and derive a final result. The processors <b>118</b> may reside within a single computing device or the processors <b>118</b> may reside on a number of different computing devices. Although four processors <b>118</b> are shown in <figref idref="DRAWINGS">FIG. 1</figref>, any number of processors may be utilized to parallelize computations of the stochastic gradient descent algorithm <b>104</b>.</p>
<p id="p-0022" num="0021">In some instances, the parallelization method <b>116</b> may utilize a hash table <b>120</b>, which may also be referred to herein as a &#x201c;master hash table.&#x201d; The hash table <b>120</b> may be generated by applying a hash function to a number of data samples from the set of training data <b>108</b>. In some cases, the hash function may map feature vectors of the samples of training data <b>108</b> with their respective classifier value. The hash table <b>120</b> may be used by the stochastic gradient descent algorithm <b>104</b> to calculate the predictor vector <b>106</b> for the non-linear SVM <b>102</b>. In a particular implementation, a portion of the hash table <b>120</b> is associated with each respective processor <b>118</b>, such that each of the processors <b>118</b> utilizes a particular portion of the hash table <b>120</b> to perform calculations of the stochastic gradient descent algorithm <b>104</b>. The portion of the hash table <b>120</b> associated with each particular processor <b>118</b> may be referred to herein as a &#x201c;distributed hash table&#x201d; or a &#x201c;local distributed hash table.&#x201d;</p>
<p id="p-0023" num="0022">In addition, a packing strategy <b>122</b> may be implemented to reduce the frequency of communications between the processors <b>118</b> utilized to execute the stochastic gradient descent algorithm <b>104</b>. For example, in some cases, each processor <b>118</b> may perform calculations for each iteration of the stochastic gradient descent algorithm <b>104</b> and communicate results of one or more of the calculations to at least one other processor <b>118</b>. The packing strategy <b>122</b> may package results of a plurality of the calculations related to execution of the stochastic gradient descent algorithm <b>104</b> into a single communication that is then transmitted to one or more of the other processors <b>118</b>, rather than the processors <b>118</b> communicating with each other after performing a certain calculation.</p>
<p id="p-0024" num="0023">After training the non-linear SVM <b>102</b> according to the stochastic gradient descent algorithm <b>104</b>, a new data sample <b>124</b> having a particular feature set, X<sub>i</sub>, may be provided to the non-linear SVM <b>102</b>. The non-linear SVM <b>102</b> may be used to characterize the new data sample <b>124</b> via a classification module <b>126</b>, a regression module <b>128</b>, or a ranking module <b>130</b>. Once the new data sample <b>124</b> has been characterized, the new data sample <b>124</b> may be used in a variety of applications, such as a spam filter application <b>132</b>, an optical character recognition (OCR) application <b>134</b>, a speech recognition application <b>136</b>, a search engine application <b>138</b>, a biometrics application <b>140</b>, a combination thereof, or any of various other types of applications (not shown).</p>
<p id="p-0025" num="0024">By utilizing a parallelized stochastic gradient descent algorithm <b>104</b> to optimize the primal objective function of the non-linear SVM <b>102</b> and by utilizing a packing strategy to reduce the frequency of inter-processor communication, the time to train the non-linear SVM <b>102</b> may be reduced. In addition, utilizing the parallelized stochastic gradient descent algorithm <b>104</b> with the inter-processor communication packing strategy to train the non-linear SVM <b>102</b> may improve the accuracy of the SVM <b>102</b> and decrease the computing resources utilized to train the non-linear SVM <b>102</b>.</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 2</figref> illustrates a system <b>200</b> to train support vector machines using a stochastic gradient descent algorithm. The system <b>200</b> includes a computing device <b>202</b>. The computing device <b>202</b> may be a server computer, a mainframe computer, a personal computer, or other suitable computing device. The computing device <b>202</b> includes one or more processors <b>204</b> and memory <b>206</b>. The memory <b>206</b> is an example of computer-readable storage media and may include volatile memory, nonvolatile memory, removable memory, non-removable memory, or a combination thereof. For example, the memory <b>206</b> may include, but is not limited to, RAM, ROM, EEPROM, flash memory, one or more hard disks, solid state drives, floppy disks, optical memory (e.g., CD, DVD), or other non-transient memory technologies.</p>
<p id="p-0027" num="0026">The computing device <b>202</b> also includes one or more communication interfaces <b>208</b> to facilitate wired and/or wireless communications via a network <b>210</b> with one or more additional computing devices <b>212</b>-<b>216</b>. The network <b>210</b> may be representative of any one or combination of multiple different types of wired and wireless networks, such as the Internet, cable networks, satellite networks, wide area wireless communication networks, wireless local area networks, and public switched telephone networks (PSTN). In some cases, the additional computing devices <b>212</b>-<b>216</b> may include components similar to those of the computing device <b>202</b>.</p>
<p id="p-0028" num="0027">The memory <b>206</b> includes a support vector machine (SVM) module <b>218</b>. The SVM module <b>218</b> may characterize data samples received by the computing device <b>202</b>. For example, the SVM module <b>218</b> may classify a data sample as belonging to one of two classes, such as classifying an email as spam or not spam. In other examples, the SVM module <b>218</b> may rank a new data sample with respect to a set of samples and/or predict a value based on the new data sample by using a regression analysis. The SVM module <b>218</b> may utilize a linear SVM or a non-linear SVM to characterize data samples.</p>
<p id="p-0029" num="0028">The memory <b>206</b> also includes a stochastic gradient descent (SGD) algorithm <b>220</b>. The SGD algorithm <b>220</b> may train support vector machines using training data <b>222</b>. The training data <b>222</b> may include a number of training data samples with known values. For example, to train a classification SVM, the training data <b>222</b> may include training data samples that include a feature vector x and a classifier value y. In a particular illustration, the feature vector x may indicate a number of features of an email, such as when the email is sent, the sender of the email, length of the email, etc. and the classifier value y may indicate whether the email is spam or not spam based on the set of features.</p>
<p id="p-0030" num="0029">In an illustrative implementation, the SGD algorithm <b>220</b> may determine a predictor vector for a non-linear SVM based on a number of samples from the training data <b>222</b>. In particular, the SGD algorithm <b>220</b> may select a particular training data sample, determine the gradient of the primal SVM objective function for the particular training data sample, and update the predictor vector based on the gradient. The SGD algorithm <b>220</b> may then execute a number of additional iterations by selecting further samples from the training data <b>222</b> and updating the predictor vector after each iteration. After completing a specified number of iterations, the SGD algorithm <b>220</b> returns the predictor vector to the SVM module <b>218</b>. In some cases, the number of iterations may be predetermined and based on an estimated amount of time for the SGD algorithm <b>220</b> to execute the predetermined number of iterations and/or the computational resources to execute the predetermined number of iterations.</p>
<p id="p-0031" num="0030">The memory <b>206</b> also includes a parallelization module <b>224</b> to parallelize computations of the SGD algorithm <b>220</b>. For example, the computations of the SGD algorithm <b>220</b> may be distributed among a number of computing devices, such as the computing devices <b>202</b> and <b>212</b>-<b>216</b>. In a particular implementation, the SGD algorithm <b>220</b> utilizes a distributed hash table <b>226</b> stored in the memory <b>206</b>. The distributed hash table <b>226</b> may be produced by applying a hash function to a portion of the data samples of the training data <b>222</b>. In some cases, the distributed hash table <b>226</b> and the distributed hash tables of the additional computing devices <b>212</b>-<b>216</b> may comprise a master hash table. Alternatively, or additionally, a master hash table may be stored in a particular one of the additional computing devices <b>212</b>-<b>216</b>.</p>
<p id="p-0032" num="0031">The SGD algorithm <b>220</b> may utilize the distributed hash table <b>226</b> to perform computations to determine a local predictor vector. For example, the SGD algorithm <b>220</b> determines a local predictor vector by determining a gradient for the primal SVM objective function for each entry of the distributed hash table <b>226</b>. In turn, additional computing devices <b>212</b>-<b>216</b> may also determine a local value for the predictor vector. The parallelization module <b>224</b> facilitates the exchange of predictor vector information <b>228</b> including the local predictor vectors between the computing device <b>202</b> and the additional computing devices <b>212</b>-<b>216</b> to determine a global value for the predictor vector that is returned to the SVM module <b>218</b>.</p>
<p id="p-0033" num="0032">In addition, for each iteration of the SGD algorithm <b>220</b>, the parallelization module <b>224</b> may determine whether the distributed hash table <b>226</b> is to be updated. The parallelization module <b>224</b> may facilitate the exchange of hash table information <b>228</b> between the computing device <b>202</b> and the additional computing devices <b>212</b>-<b>216</b> to update the distributed hash table <b>226</b> and/or to update the respective distributed hash tables of the additional computing devices <b>212</b>-<b>216</b>. In a particular implementation, the parallelization module <b>224</b> may determine whether a particular training data sample is represented by an entry in the distributed hash table <b>226</b> and update the entry. In other cases, the parallelization module <b>224</b> may add a new entry to the distributed hash table <b>226</b>, where the new entry is associated with the particular training data sample. For example, when the processor <b>204</b> is utilizing fewer resources than processors of the additional computing devices <b>212</b>-<b>216</b>, then the parallelization module <b>224</b> may add a new entry to the distributed hash table <b>226</b> for the particular training data sample. Additionally, the parallelization module <b>224</b> may exchange the distributed hash table information <b>228</b> with the additional computing devices <b>212</b>-<b>216</b> to determine if updates have been made to the distributed hash tables of one or more of the additional computing devices <b>212</b>-<b>216</b> relating to the particular training data sample.</p>
<p id="p-0034" num="0033">The memory <b>206</b> also includes a packing strategy module <b>230</b> to reduce communications between the computing device <b>202</b> and the additional computing devices <b>212</b>-<b>216</b> that calculate the predictor vector for the SVM module <b>218</b>. In an illustrative implementation, the packing strategy module <b>230</b> may combine calculations of the SGD algorithm <b>220</b> to compute a local value of the predictor vector before communicating the results of the calculations with the additional computing devices <b>212</b>-<b>216</b>. For example, rather than exchanging predictor vector information <b>232</b> to determine a global predictor vector for each iteration of the SGD algorithm <b>220</b>, the packing strategy module <b>230</b> may combine calculations for determining the local predictor vector for several iterations of the SGD algorithm <b>220</b> before communicating the predictor vector information <b>232</b> with the additional computing devices <b>212</b>-<b>216</b> to determine a global predictor vector.</p>
<p id="p-0035" num="0034">In some instances, the packing strategy module <b>230</b> may pre-calculate intermediate values, such as the inner products and kernel functions, utilized by the SGD algorithm <b>220</b> to determine the local predictor vector for a number of iterations of the SGD algorithm <b>220</b>. For example, the packing strategy module <b>230</b> may pre-calculate an inner product between a predictor vector and a kernel mapping function vector for a group of iterations of the SGD algorithm <b>220</b>. In addition, the packing strategy module <b>230</b> may pre-calculate one or more kernel functions for each iteration of a group of iterations of the SGD algorithm <b>220</b>.</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIGS. 3-6</figref> show methods <b>300</b>, <b>400</b>, <b>500</b>, and <b>600</b>, respectively, to train a support vector machine with a parallelized stochastic gradient descent algorithm. The methods <b>300</b>, <b>400</b>, <b>500</b>, and <b>600</b> are illustrated as a collection of blocks in a logical flow graph, which represent a sequence of operations that can be implemented in hardware, software, or a combination thereof. In the context of software, the blocks represent computer-executable instructions that, when executed by one or more processors, perform the recited operations. Generally, computer-executable instructions include routines, programs, objects, components, data structures, and the like that perform particular functions or implement particular abstract data types. The order in which the operations are described is not intended to be construed as a limitation, and any number of the described blocks can be combined in any order and/or in parallel to implement the process.</p>
<p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. 3</figref> illustrates a method <b>300</b> to train a non-linear support vector machine with a parallelized stochastic gradient descent algorithm and to use the trained support vector machine to characterize a new data sample. At <b>302</b>, training data is received by a computing device executing a parallelized stochastic gradient descent algorithm to train a support vector machine. The computing device may include a number of processors. In some cases, the training data may include a number of samples with each sample including a feature vector. The feature vector specifies particular features of the sample. The training data samples may also include a classifier or a ranking that is associated with the respective feature vector of the sample.</p>
<p id="p-0038" num="0037">At <b>304</b>, the parallelized stochastic gradient descent algorithm trains a non-linear support vector machine. The parallelized stochastic gradient descent algorithm may be implemented by a number of processors residing on one or more computing devices. In particular, the calculations of the stochastic gradient descent algorithm may be distributed among the processors in order to reduce the amount of time to train the support vector machine. The parallelized stochastic gradient descent algorithm trains the non-linear support vector machine by providing parameters to the support vector machine, such as a predictor vector. The predictor vector can be used to solve a primal objective function of the support vector machine.</p>
<p id="p-0039" num="0038">At <b>306</b>, a new data sample is received at a computing device including the trained non-linear support vector machine. For example, the computing device may receive an email having a particular set of features, such as the sender of the email, a time that the email was sent, words or phrases in the body of the email, words or phrases of the subject line of the email, and so on. At <b>308</b>, the computing device characterizes the new data sample according to the trained non-linear support vector machine. For example, the trained support vector machine may associate a feature vector of the new data sample with a particular classification or a particular ranking. In an illustrative example, the trained non-linear support vector machine may determine whether or not an email received by the computing device is spam or not based on the features of the email.</p>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. 4</figref> illustrates a method <b>400</b> to determine a predictor vector for a primal objective function of a support vector machine. At <b>402</b>, a sample <b>404</b> is selected from training data that is utilized to train the support vector machine. The training data sample <b>404</b> includes a feature vector x and a classifier value y. At <b>406</b>, the loss of the objective function of the support vector machine is estimated as the loss for the current iteration. As mentioned previously, the loss of the SVM primal objective function, l(w), is given by:</p>
<p id="p-0041" num="0040">
<maths id="MATH-US-00004" num="00004">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mfrac>
            <mn>1</mn>
            <mi>m</mi>
          </mfrac>
          <mo>&#x2062;</mo>
          <mrow>
            <munderover>
              <mo>&#x2211;</mo>
              <mrow>
                <mi>i</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mi>m</mi>
            </munderover>
            <mo>&#x2062;</mo>
            <mrow>
              <mi>max</mi>
              <mo>&#x2062;</mo>
              <mrow>
                <mo>{</mo>
                <mrow>
                  <mn>0</mn>
                  <mo>,</mo>
                  <mrow>
                    <mn>1</mn>
                    <mo>-</mo>
                    <mrow>
                      <msub>
                        <mi>y</mi>
                        <mi>i</mi>
                      </msub>
                      <mo>&#x2062;</mo>
                      <mrow>
                        <mo>&#x2329;</mo>
                        <mrow>
                          <mi>w</mi>
                          <mo>,</mo>
                          <mrow>
                            <mi>&#x3d5;</mi>
                            <mo>&#x2061;</mo>
                            <mrow>
                              <mo>(</mo>
                              <msub>
                                <mi>x</mi>
                                <mi>i</mi>
                              </msub>
                              <mo>)</mo>
                            </mrow>
                          </mrow>
                        </mrow>
                        <mo>&#x232a;</mo>
                      </mrow>
                    </mrow>
                  </mrow>
                </mrow>
                <mo>}</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
        <mo>,</mo>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>3</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where the expression <img id="CUSTOM-CHARACTER-00004" he="3.13mm" wi="0.68mm" file="US08626677-20140107-P00003.TIF" alt="custom character" img-content="character" img-format="tif"/>w, &#x3c6;(x<sub>i</sub>)<img id="CUSTOM-CHARACTER-00005" he="3.13mm" wi="0.68mm" file="US08626677-20140107-P00004.TIF" alt="custom character" img-content="character" img-format="tif"/> is referred to herein as the inner product of the predictor vector w and the kernel mapping function vector &#x3c6;(x<sub>i</sub>). Thus, the overall loss for the primal objective function is estimated as:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>l</i>(<i>w</i>)&#x2248;<i>l</i><sub>t</sub>(<i>w</i>):=max{0,1<i>&#x2212;y</i><sub>i(t)</sub><i>&#xb7;</i><img id="CUSTOM-CHARACTER-00006" he="3.13mm" wi="0.68mm" file="US08626677-20140107-P00003.TIF" alt="custom character" img-content="character" img-format="tif"/><i>w</i>,&#x3c6;(<i>x</i><sub>i(t)</sub>)<img id="CUSTOM-CHARACTER-00007" he="3.13mm" wi="0.68mm" file="US08626677-20140107-P00004.TIF" alt="custom character" img-content="character" img-format="tif"/>}.&#x2003;&#x2003;(4)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
Additionally, in this way, the overall primal objective function can be evaluated based on the primal objective function for this iteration, which is given by:
</p>
<p id="p-0042" num="0041">
<maths id="MATH-US-00005" num="00005">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mrow>
            <mi>f</mi>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mi>w</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>&#x2248;</mo>
          <mrow>
            <msub>
              <mi>f</mi>
              <mi>t</mi>
            </msub>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mi>w</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
        </mrow>
        <mo>:=</mo>
        <mrow>
          <mrow>
            <mfrac>
              <mi>&#x3c3;</mi>
              <mn>2</mn>
            </mfrac>
            <mo>&#x2062;</mo>
            <msubsup>
              <mrow>
                <mo>&#xf605;</mo>
                <mi>w</mi>
                <mo>&#xf606;</mo>
              </mrow>
              <mn>2</mn>
              <mn>2</mn>
            </msubsup>
          </mrow>
          <mo>+</mo>
          <mrow>
            <msub>
              <mi>l</mi>
              <mi>t</mi>
            </msub>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mi>w</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>5</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0043" num="0042">At <b>408</b>, the gradient of the primal objective function for the particular iteration is calculated. In particular, the predictor vector for the primal objective function is given by:</p>
<p id="p-0044" num="0043">
<maths id="MATH-US-00006" num="00006">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mi>w</mi>
        <mo>&#x2190;</mo>
        <mrow>
          <mi>w</mi>
          <mo>-</mo>
          <mrow>
            <mfrac>
              <mn>1</mn>
              <mrow>
                <mi>&#x3c3;</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mi>t</mi>
              </mrow>
            </mfrac>
            <mo>&#x2062;</mo>
            <mrow>
              <mo>&#x2207;</mo>
              <mrow>
                <msub>
                  <mi>f</mi>
                  <mi>t</mi>
                </msub>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>(</mo>
                  <mi>w</mi>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>6</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0045" num="0044">where &#x2207;&#x192;<sub>t</sub>(w) is the gradient of the primal objective function. The gradient of the primal objective function may be calculated by:</p>
<p id="p-0046" num="0045">
<maths id="MATH-US-00007" num="00007">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mo>&#x2207;</mo>
          <mrow>
            <msub>
              <mi>f</mi>
              <mi>t</mi>
            </msub>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mi>w</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mrow>
            <mi>&#x3c3;</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>w</mi>
          </mrow>
          <mo>-</mo>
          <mrow>
            <mo>{</mo>
            <mtable>
              <mtr>
                <mtd>
                  <mrow>
                    <mn>0</mn>
                    <mo>,</mo>
                  </mrow>
                </mtd>
                <mtd>
                  <mrow>
                    <mrow>
                      <msub>
                        <mi>y</mi>
                        <mrow>
                          <mi>i</mi>
                          <mo>&#x2061;</mo>
                          <mrow>
                            <mo>(</mo>
                            <mi>t</mi>
                            <mo>)</mo>
                          </mrow>
                        </mrow>
                      </msub>
                      <mo>&#xb7;</mo>
                      <mrow>
                        <mo>&#x2329;</mo>
                        <mrow>
                          <mi>w</mi>
                          <mo>,</mo>
                          <mrow>
                            <mi>&#x3d5;</mi>
                            <mo>&#x2061;</mo>
                            <mrow>
                              <mo>(</mo>
                              <msub>
                                <mi>x</mi>
                                <mrow>
                                  <mi>i</mi>
                                  <mo>&#x2061;</mo>
                                  <mrow>
                                    <mo>(</mo>
                                    <mi>t</mi>
                                    <mo>)</mo>
                                  </mrow>
                                </mrow>
                              </msub>
                              <mo>)</mo>
                            </mrow>
                          </mrow>
                        </mrow>
                        <mo>&#x232a;</mo>
                      </mrow>
                    </mrow>
                    <mo>&#x2265;</mo>
                    <mn>1</mn>
                  </mrow>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mrow>
                    <mrow>
                      <msub>
                        <mi>y</mi>
                        <mrow>
                          <mi>i</mi>
                          <mo>&#x2061;</mo>
                          <mrow>
                            <mo>(</mo>
                            <mi>t</mi>
                            <mo>)</mo>
                          </mrow>
                        </mrow>
                      </msub>
                      <mo>&#x2062;</mo>
                      <mrow>
                        <mi>&#x3d5;</mi>
                        <mo>&#x2061;</mo>
                        <mrow>
                          <mo>(</mo>
                          <msub>
                            <mi>x</mi>
                            <mrow>
                              <mi>i</mi>
                              <mo>&#x2061;</mo>
                              <mrow>
                                <mo>(</mo>
                                <mi>t</mi>
                                <mo>)</mo>
                              </mrow>
                            </mrow>
                          </msub>
                          <mo>)</mo>
                        </mrow>
                      </mrow>
                    </mrow>
                    <mo>,</mo>
                  </mrow>
                </mtd>
                <mtd>
                  <mrow>
                    <mrow>
                      <msub>
                        <mi>y</mi>
                        <mrow>
                          <mi>i</mi>
                          <mo>&#x2061;</mo>
                          <mrow>
                            <mo>(</mo>
                            <mi>t</mi>
                            <mo>)</mo>
                          </mrow>
                        </mrow>
                      </msub>
                      <mo>&#xb7;</mo>
                      <mrow>
                        <mo>&#x2329;</mo>
                        <mrow>
                          <mi>w</mi>
                          <mo>,</mo>
                          <mrow>
                            <mi>&#x3d5;</mi>
                            <mo>&#x2061;</mo>
                            <mrow>
                              <mo>(</mo>
                              <msub>
                                <mi>x</mi>
                                <mrow>
                                  <mi>i</mi>
                                  <mo>&#x2061;</mo>
                                  <mrow>
                                    <mo>(</mo>
                                    <mi>t</mi>
                                    <mo>)</mo>
                                  </mrow>
                                </mrow>
                              </msub>
                              <mo>)</mo>
                            </mrow>
                          </mrow>
                        </mrow>
                        <mo>&#x232a;</mo>
                      </mrow>
                    </mrow>
                    <mo>&#x3c;</mo>
                    <mn>1</mn>
                  </mrow>
                </mtd>
              </mtr>
            </mtable>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>7</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0047" num="0046">At <b>410</b>, the predictor vector w is updated. In a particular implementation, when the support vector machine is non-linear and introduces kernels, the predictor vector can be evaluated as</p>
<p id="p-0048" num="0047">
<maths id="MATH-US-00008" num="00008">
<math overflow="scroll">
<mrow>
  <mrow>
    <mi>w</mi>
    <mo>=</mo>
    <mrow>
      <munderover>
        <mo>&#x2211;</mo>
        <mrow>
          <mi>i</mi>
          <mo>=</mo>
          <mn>1</mn>
        </mrow>
        <mi>m</mi>
      </munderover>
      <mo>&#x2062;</mo>
      <mrow>
        <msub>
          <mi>&#x3b1;</mi>
          <mi>i</mi>
        </msub>
        <mo>&#x2062;</mo>
        <msub>
          <mi>y</mi>
          <mi>i</mi>
        </msub>
        <mo>&#x2062;</mo>
        <mrow>
          <mi>&#x3d5;</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <msub>
              <mi>x</mi>
              <mi>i</mi>
            </msub>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mrow>
    </mrow>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
</maths>
<br/>
such that w is given by:
</p>
<p id="p-0049" num="0048">
<maths id="MATH-US-00009" num="00009">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mi>w</mi>
        <mo>&#x2190;</mo>
        <mrow>
          <mrow>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mn>1</mn>
                <mo>-</mo>
                <mfrac>
                  <mi>t</mi>
                  <mi>t</mi>
                </mfrac>
              </mrow>
              <mo>)</mo>
            </mrow>
            <mo>&#x2062;</mo>
            <mi>w</mi>
          </mrow>
          <mo>+</mo>
          <mrow>
            <mo>{</mo>
            <mtable>
              <mtr>
                <mtd>
                  <mrow>
                    <mn>0</mn>
                    <mo>,</mo>
                  </mrow>
                </mtd>
                <mtd>
                  <mrow>
                    <mrow>
                      <msub>
                        <mi>y</mi>
                        <mrow>
                          <mi>i</mi>
                          <mo>&#x2061;</mo>
                          <mrow>
                            <mo>(</mo>
                            <mi>t</mi>
                            <mo>)</mo>
                          </mrow>
                        </mrow>
                      </msub>
                      <mo>&#xb7;</mo>
                    </mrow>
                    <mo>&#x2063;</mo>
                    <mrow>
                      <mrow>
                        <mo>&#x2329;</mo>
                        <mrow>
                          <mi>w</mi>
                          <mo>,</mo>
                          <mrow>
                            <mi>&#x3d5;</mi>
                            <mo>&#x2061;</mo>
                            <mrow>
                              <mo>(</mo>
                              <msub>
                                <mi>x</mi>
                                <mrow>
                                  <mi>i</mi>
                                  <mo>&#x2061;</mo>
                                  <mrow>
                                    <mo>(</mo>
                                    <mi>t</mi>
                                    <mo>)</mo>
                                  </mrow>
                                </mrow>
                              </msub>
                              <mo>)</mo>
                            </mrow>
                          </mrow>
                        </mrow>
                        <mo>&#x232a;</mo>
                      </mrow>
                      <mo>&#x2265;</mo>
                      <mn>1</mn>
                    </mrow>
                  </mrow>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mrow>
                    <mrow>
                      <mfrac>
                        <msub>
                          <mi>y</mi>
                          <mrow>
                            <mi>i</mi>
                            <mo>&#x2061;</mo>
                            <mrow>
                              <mo>(</mo>
                              <mi>t</mi>
                              <mo>)</mo>
                            </mrow>
                          </mrow>
                        </msub>
                        <mrow>
                          <mi>&#x3c3;</mi>
                          <mo>&#x2062;</mo>
                          <mstyle>
                            <mspace width="0.3em" height="0.3ex"/>
                          </mstyle>
                          <mo>&#x2062;</mo>
                          <mi>t</mi>
                        </mrow>
                      </mfrac>
                      <mo>&#xb7;</mo>
                      <mrow>
                        <mi>&#x3d5;</mi>
                        <mo>&#x2061;</mo>
                        <mrow>
                          <mo>(</mo>
                          <msub>
                            <mi>x</mi>
                            <mrow>
                              <mi>i</mi>
                              <mo>&#x2061;</mo>
                              <mrow>
                                <mo>(</mo>
                                <mi>t</mi>
                                <mo>)</mo>
                              </mrow>
                            </mrow>
                          </msub>
                          <mo>)</mo>
                        </mrow>
                      </mrow>
                    </mrow>
                    <mo>,</mo>
                  </mrow>
                </mtd>
                <mtd>
                  <mrow>
                    <mrow>
                      <msub>
                        <mi>y</mi>
                        <mrow>
                          <mi>i</mi>
                          <mo>&#x2061;</mo>
                          <mrow>
                            <mo>(</mo>
                            <mi>t</mi>
                            <mo>)</mo>
                          </mrow>
                        </mrow>
                      </msub>
                      <mo>&#xb7;</mo>
                      <mrow>
                        <mo>&#x2329;</mo>
                        <mrow>
                          <mi>w</mi>
                          <mo>,</mo>
                          <mrow>
                            <mi>&#x3d5;</mi>
                            <mo>&#x2061;</mo>
                            <mrow>
                              <mo>(</mo>
                              <msub>
                                <mi>x</mi>
                                <mrow>
                                  <mi>i</mi>
                                  <mo>&#x2061;</mo>
                                  <mrow>
                                    <mo>(</mo>
                                    <mi>t</mi>
                                    <mo>)</mo>
                                  </mrow>
                                </mrow>
                              </msub>
                              <mo>)</mo>
                            </mrow>
                          </mrow>
                        </mrow>
                        <mo>&#x232a;</mo>
                      </mrow>
                    </mrow>
                    <mo>&#x3c;</mo>
                    <mn>1</mn>
                  </mrow>
                </mtd>
              </mtr>
            </mtable>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>8</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
After updating w according to equation 8, a projection is applied to help bring the primal objective function to a minimum. The projection is given by:
</p>
<p id="p-0050" num="0049">
<maths id="MATH-US-00010" num="00010">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mi>w</mi>
        <mo>&#x2190;</mo>
        <mrow>
          <mi>min</mi>
          <mo>&#x2062;</mo>
          <mrow>
            <mo>{</mo>
            <mrow>
              <mn>1</mn>
              <mo>,</mo>
              <mfrac>
                <mrow>
                  <mn>1</mn>
                  <mo>/</mo>
                  <msqrt>
                    <mi>&#x3c3;</mi>
                  </msqrt>
                </mrow>
                <msub>
                  <mrow>
                    <mo>&#xf605;</mo>
                    <mi>w</mi>
                    <mo>&#xf606;</mo>
                  </mrow>
                  <mn>2</mn>
                </msub>
              </mfrac>
            </mrow>
            <mo>}</mo>
          </mrow>
          <mo>&#x2062;</mo>
          <mi>w</mi>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>9</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0051" num="0050">At <b>412</b>, if another iteration is to be carried out, then the method <b>400</b> returns to <b>402</b>. Otherwise, the method <b>400</b> moves to <b>414</b> and the predictor vector w is provided to the support vector machine. An example of instructions for performing the method <b>400</b> may be expressed as:</p>
<p id="p-0052" num="0051">
<tables id="TABLE-US-00001" num="00001">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="offset" colwidth="28pt" align="left"/>
<colspec colname="1" colwidth="42pt" align="left"/>
<colspec colname="2" colwidth="147pt" align="left"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry>1.</entry>
<entry>INPUT: &#x3c3;, T , training sample space &#x3a8;</entry>
</row>
<row>
<entry/>
<entry>2.</entry>
<entry>INITIALIZE: w = 0</entry>
</row>
<row>
<entry/>
<entry>3.</entry>
<entry>FOR t = 1, 2, . . . , T</entry>
</row>
<row>
<entry/>
<entry>4.</entry>
<entry>&#x2003;&#x2003;Randomly pick up (x, y) &#x2208; &#x3a8;</entry>
</row>
<row>
<entry/>
<entry>5.</entry>
<entry>&#x2003;&#x2003;Predict y&#x2032; &#x2190; &#x2009;<img id="CUSTOM-CHARACTER-00008" he="2.46mm" wi="1.02mm" file="US08626677-20140107-P00005.TIF" alt="custom character" img-content="character" img-format="tif"/> w, &#x3c6;(x)&#x2009;<img id="CUSTOM-CHARACTER-00009" he="2.46mm" wi="1.02mm" file="US08626677-20140107-P00006.TIF" alt="custom character" img-content="character" img-format="tif"/> </entry>
</row>
<row>
<entry/>
<entry>6.</entry>
<entry>&#x2003;&#x2003;w &#x2190; (1 &#x2212; 1/t)w</entry>
</row>
<row>
<entry/>
<entry> </entry>
</row>
<row>
<entry/>
<entry>7.</entry>
<entry>&#x2003;&#x2003;
<maths id="MATH-US-00011" num="00011">
<math overflow="scroll">
<mrow>
  <mrow>
    <mrow>
      <mi>IF</mi>
      <mo>&#x2062;</mo>
      <mstyle>
        <mspace width="0.8em" height="0.8ex"/>
      </mstyle>
      <mo>&#x2062;</mo>
      <msup>
        <mi>yy</mi>
        <mi>&#x2032;</mi>
      </msup>
    </mrow>
    <mo>&#x3c;</mo>
    <mrow>
      <mn>1</mn>
      <mo>&#x2062;</mo>
      <mstyle>
        <mspace width="0.8em" height="0.8ex"/>
      </mstyle>
      <mo>&#x2062;</mo>
      <mi>THEN</mi>
      <mo>&#x2062;</mo>
      <mstyle>
        <mspace width="0.8em" height="0.8ex"/>
      </mstyle>
      <mo>&#x2062;</mo>
      <mi>w</mi>
    </mrow>
  </mrow>
  <mo>&#x2190;</mo>
  <mrow>
    <mi>w</mi>
    <mo>+</mo>
    <mrow>
      <mfrac>
        <mi>y</mi>
        <mrow>
          <mi>&#x3c3;</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mi>t</mi>
        </mrow>
      </mfrac>
      <mo>&#x2062;</mo>
      <mrow>
        <mi>&#x3d5;</mi>
        <mo>&#x2061;</mo>
        <mrow>
          <mo>(</mo>
          <mi>x</mi>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
</entry>
</row>
<row>
<entry/>
<entry> </entry>
</row>
<row>
<entry/>
<entry>8.</entry>
<entry>&#x2003;&#x2003;
<maths id="MATH-US-00012" num="00012">
<math overflow="scroll">
<mrow>
  <mi>w</mi>
  <mo>&#x2190;</mo>
  <mrow>
    <mi>min</mi>
    <mo>&#x2062;</mo>
    <mrow>
      <mo>{</mo>
      <mrow>
        <mn>1</mn>
        <mo>,</mo>
        <mfrac>
          <mrow>
            <mn>1</mn>
            <mo>/</mo>
            <msqrt>
              <mi>&#x3c3;</mi>
            </msqrt>
          </mrow>
          <msub>
            <mrow>
              <mo>&#xf605;</mo>
              <mi>w</mi>
              <mo>&#xf606;</mo>
            </mrow>
            <mn>2</mn>
          </msub>
        </mfrac>
      </mrow>
      <mo>}</mo>
    </mrow>
    <mo>&#x2062;</mo>
    <mi>w</mi>
  </mrow>
</mrow>
</math>
</maths>
</entry>
</row>
<row>
<entry/>
<entry> </entry>
</row>
<row>
<entry/>
<entry>9.</entry>
<entry>RETURN w</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0053" num="0052">In a particular implementation, the predictor vector may be expressed as w=sv where s&#x3b5;<img id="CUSTOM-CHARACTER-00010" he="3.13mm" wi="2.12mm" file="US08626677-20140107-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/> is a scalar. By expressing w in terms of a scalar s and the vector v, when calculating w the scalar is changed rather than calculating each of the coefficients of the predictor vector. Thus, the term</p>
<p id="p-0054" num="0053">
<maths id="MATH-US-00013" num="00013">
<math overflow="scroll">
<mrow>
  <mi>w</mi>
  <mo>&#x2190;</mo>
  <mrow>
    <mi>w</mi>
    <mo>+</mo>
    <mrow>
      <mfrac>
        <mi>y</mi>
        <mrow>
          <mi>&#x3c3;</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mi>t</mi>
        </mrow>
      </mfrac>
      <mo>&#x2062;</mo>
      <mrow>
        <mi>&#x3d5;</mi>
        <mo>&#x2061;</mo>
        <mrow>
          <mo>(</mo>
          <msub>
            <mi>x</mi>
            <mi>i</mi>
          </msub>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
<br/>
of equation 8 can be expressed as
</p>
<p id="p-0055" num="0054">
<maths id="MATH-US-00014" num="00014">
<math overflow="scroll">
<mrow>
  <mi>v</mi>
  <mo>&#x2190;</mo>
  <mrow>
    <mi>v</mi>
    <mo>+</mo>
    <mrow>
      <mfrac>
        <mi>y</mi>
        <mrow>
          <mi>&#x3c3;</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mi>ts</mi>
        </mrow>
      </mfrac>
      <mo>&#x2062;</mo>
      <mrow>
        <mrow>
          <mi>&#x3d5;</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <msub>
              <mi>x</mi>
              <mi>i</mi>
            </msub>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>.</mo>
      </mrow>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
<br/>
In addition, a variable norm can be introduced to store an up-to-date value of &#x2225;w&#x2225;<sub>2 </sub>for a hash table <img id="CUSTOM-CHARACTER-00011" he="3.13mm" wi="3.13mm" file="US08626677-20140107-P00007.TIF" alt="custom character" img-content="character" img-format="tif"/>. The hash table <img id="CUSTOM-CHARACTER-00012" he="3.13mm" wi="3.13mm" file="US08626677-20140107-P00007.TIF" alt="custom character" img-content="character" img-format="tif"/> may be used to store key-value pairs designated as (x<sub>i</sub>, &#x3b2;<sub>i</sub>), where x<sub>i </sub>represents a feature vector and &#x3b2;<sub>i </sub>is a value based on a hash function applied to the classifier value y<sub>i</sub>. According to this particular implementation, an example of instructions for performing the method <b>400</b> may be expressed as:
</p>
<p id="p-0056" num="0055">
<tables id="TABLE-US-00002" num="00002">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="offset" colwidth="14pt" align="left"/>
<colspec colname="1" colwidth="28pt" align="left"/>
<colspec colname="2" colwidth="175pt" align="left"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry>&#x2002;1.</entry>
<entry>INPUT: &#x3c3;, T, training sample space &#x3a8;</entry>
</row>
<row>
<entry/>
<entry>&#x2002;2.</entry>
<entry>INITIALIZE: &#x2009;<img id="CUSTOM-CHARACTER-00013" he="2.12mm" wi="2.79mm" file="US08626677-20140107-P00008.TIF" alt="custom character" img-content="character" img-format="tif"/> &#x2009;= &#x2205;&#x2009;, s = 1, norm = 0</entry>
</row>
<row>
<entry/>
<entry>&#x2002;3.</entry>
<entry>FOR t = 1, 2, . . . , T</entry>
</row>
<row>
<entry/>
<entry>&#x2002;4.</entry>
<entry>&#x2003;Randomly pick up (x, y) &#x2208; &#x3a8;</entry>
</row>
<row>
<entry/>
<entry>&#x2002;5.</entry>
<entry>&#x2003;y&#x2032; &#x2190; s&#x2009;<img id="CUSTOM-CHARACTER-00014" he="2.46mm" wi="1.02mm" file="US08626677-20140107-P00009.TIF" alt="custom character" img-content="character" img-format="tif"/> &#x3bd;, &#x3c6;(x)&#x2009;<img id="CUSTOM-CHARACTER-00015" he="2.46mm" wi="1.02mm" file="US08626677-20140107-P00010.TIF" alt="custom character" img-content="character" img-format="tif"/> &#x2009;&#x2002;by iterating all entries in &#x2009;<img id="CUSTOM-CHARACTER-00016" he="2.12mm" wi="2.79mm" file="US08626677-20140107-P00008.TIF" alt="custom character" img-content="character" img-format="tif"/> </entry>
</row>
<row>
<entry/>
<entry>&#x2002;6.</entry>
<entry>&#x2003;s &#x2190; (1 &#x2212; 1/t)s</entry>
</row>
<row>
<entry/>
<entry>&#x2002;7.</entry>
<entry>&#x2003;IF yy&#x2032; &#x3c; 1 THEN</entry>
</row>
<row>
<entry/>
<entry> </entry>
</row>
<row>
<entry/>
<entry>&#x2002;8.</entry>
<entry>&#x2003;&#x2003;
<maths id="MATH-US-00015" num="00015">
<math overflow="scroll">
<mrow>
  <mi>norm</mi>
  <mo>&#x2190;</mo>
  <mrow>
    <mi>norm</mi>
    <mo>+</mo>
    <mrow>
      <mfrac>
        <mrow>
          <mn>2</mn>
          <mo>&#x2062;</mo>
          <mi>y</mi>
        </mrow>
        <mrow>
          <mi>&#x3c3;</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mi>t</mi>
        </mrow>
      </mfrac>
      <mo>&#xb7;</mo>
      <msup>
        <mi>y</mi>
        <mi>&#x2032;</mi>
      </msup>
    </mrow>
    <mo>+</mo>
    <mrow>
      <msup>
        <mrow>
          <mo>(</mo>
          <mfrac>
            <mi>y</mi>
            <mrow>
              <mi>&#x3c3;</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mi>t</mi>
            </mrow>
          </mfrac>
          <mo>)</mo>
        </mrow>
        <mn>2</mn>
      </msup>
      <mo>&#x2062;</mo>
            <mo>&#x2062;</mo>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>x</mi>
          <mo>,</mo>
          <mi>x</mi>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
</entry>
</row>
<row>
<entry/>
<entry> </entry>
</row>
<row>
<entry/>
<entry>&#x2002;9.</entry>
<entry>&#x2003;&#x2003;IF key x is found in &#x2009;<img id="CUSTOM-CHARACTER-00017" he="2.12mm" wi="2.79mm" file="US08626677-20140107-P00008.TIF" alt="custom character" img-content="character" img-format="tif"/> ,</entry>
</row>
<row>
<entry/>
<entry> </entry>
</row>
<row>
<entry/>
<entry/>
<entry>&#x2003;&#x2003;&#x2003; 
<maths id="MATH-US-00016" num="00016">
<math overflow="scroll">
<mrow>
  <mrow>
    <mi>THEN</mi>
    <mo>&#x2062;</mo>
    <mstyle>
      <mspace width="0.8em" height="0.8ex"/>
    </mstyle>
    <mo>&#x2062;</mo>
    <mi>add</mi>
    <mo>&#x2062;</mo>
    <mstyle>
      <mspace width="0.8em" height="0.8ex"/>
    </mstyle>
    <mo>&#x2062;</mo>
    <mi>its</mi>
    <mo>&#x2062;</mo>
    <mstyle>
      <mspace width="0.8em" height="0.8ex"/>
    </mstyle>
    <mo>&#x2062;</mo>
    <mi>value</mi>
    <mo>&#x2062;</mo>
    <mstyle>
      <mspace width="0.8em" height="0.8ex"/>
    </mstyle>
    <mo>&#x2062;</mo>
    <mi>by</mi>
    <mo>&#x2062;</mo>
    <mstyle>
      <mspace width="0.8em" height="0.8ex"/>
    </mstyle>
    <mo>&#x2062;</mo>
    <mfrac>
      <mi>y</mi>
      <mrow>
        <mi>&#x3c3;</mi>
        <mo>&#x2062;</mo>
        <mstyle>
          <mspace width="0.3em" height="0.3ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mi>ts</mi>
      </mrow>
    </mfrac>
    <mo>&#x2062;</mo>
    <mstyle>
      <mspace width="0.8em" height="0.8ex"/>
    </mstyle>
    <mo>&#x2062;</mo>
    <mi>in</mi>
    <mo>&#x2062;</mo>
    <mstyle>
      <mspace width="0.8em" height="0.8ex"/>
    </mstyle>
    <mo>&#x2062;</mo>
      </mrow>
  <mo>;</mo>
</mrow>
</math>
</maths>
</entry>
</row>
<row>
<entry/>
<entry> </entry>
</row>
<row>
<entry/>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;
<maths id="MATH-US-00017" num="00017">
<math overflow="scroll">
<mrow>
  <mstyle>
    <mspace width="0.6em" height="0.6ex"/>
  </mstyle>
  <mo>&#x2062;</mo>
  <mrow>
    <mi>ELSE</mi>
    <mo>&#x2062;</mo>
    <mstyle>
      <mspace width="0.8em" height="0.8ex"/>
    </mstyle>
    <mo>&#x2062;</mo>
    <mi>add</mi>
    <mo>&#x2062;</mo>
    <mstyle>
      <mspace width="0.8em" height="0.8ex"/>
    </mstyle>
    <mo>&#x2062;</mo>
        <mo>&#x2062;</mo>
    <mstyle>
      <mspace width="0.8em" height="0.8ex"/>
    </mstyle>
    <mo>&#x2062;</mo>
    <mi>a</mi>
    <mo>&#x2062;</mo>
    <mstyle>
      <mspace width="0.8em" height="0.8ex"/>
    </mstyle>
    <mo>&#x2062;</mo>
    <mi>new</mi>
    <mo>&#x2062;</mo>
    <mstyle>
      <mspace width="0.8em" height="0.8ex"/>
    </mstyle>
    <mo>&#x2062;</mo>
    <mi>entry</mi>
    <mo>&#x2062;</mo>
    <mstyle>
      <mspace width="0.8em" height="0.8ex"/>
    </mstyle>
    <mo>&#x2062;</mo>
    <mrow>
      <mo>(</mo>
      <mrow>
        <mi>x</mi>
        <mo>,</mo>
        <mfrac>
          <mi>y</mi>
          <mrow>
            <mi>&#x3c3;</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>ts</mi>
          </mrow>
        </mfrac>
      </mrow>
      <mo>)</mo>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
</entry>
</row>
<row>
<entry/>
<entry> </entry>
</row>
<row>
<entry/>
<entry>10.</entry>
<entry>&#x2003;&#x2003;&#x2003;IF norm &#x3e; 1/&#x3c3;</entry>
</row>
<row>
<entry/>
<entry> </entry>
</row>
<row>
<entry/>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;
<maths id="MATH-US-00018" num="00018">
<math overflow="scroll">
<mrow>
  <mrow>
    <mrow>
      <mi>THEN</mi>
      <mo>&#x2062;</mo>
      <mstyle>
        <mspace width="0.8em" height="0.8ex"/>
      </mstyle>
      <mo>&#x2062;</mo>
      <mi>s</mi>
    </mrow>
    <mo>&#x2190;</mo>
    <mrow>
      <mi>s</mi>
      <mo>&#xb7;</mo>
      <mfrac>
        <mn>1</mn>
        <msqrt>
          <mrow>
            <mi>&#x3c3;</mi>
            <mo>&#xb7;</mo>
            <mi>norm</mi>
          </mrow>
        </msqrt>
      </mfrac>
    </mrow>
  </mrow>
  <mo>;</mo>
  <mrow>
    <mi>norm</mi>
    <mo>&#x2190;</mo>
    <mrow>
      <mn>1</mn>
      <mo>/</mo>
      <mi>&#x3c3;</mi>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
</entry>
</row>
<row>
<entry/>
<entry> </entry>
</row>
<row>
<entry/>
<entry>11.</entry>
<entry>RETURN s&#x3bd; by iterating all entries in &#x2009;<img id="CUSTOM-CHARACTER-00018" he="2.12mm" wi="2.79mm" file="US08626677-20140107-P00008.TIF" alt="custom character" img-content="character" img-format="tif"/> </entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
<br/>
As indicated in the instructions above, when a hash table is used to calculate the predictor vector, a process can be used to update the hash table when calculating the gradient of the primal objective function. In particular, when the training data sample <b>404</b> includes a feature vector x that is included in the distributed hash table, then the value, &#x3b2;<sub>i </sub>associated with the feature vector is updated by
</p>
<p id="p-0057" num="0056">
<maths id="MATH-US-00019" num="00019">
<math overflow="scroll">
<mrow>
  <mfrac>
    <mi>y</mi>
    <mrow>
      <mi>&#x3c3;</mi>
      <mo>&#x2062;</mo>
      <mstyle>
        <mspace width="0.3em" height="0.3ex"/>
      </mstyle>
      <mo>&#x2062;</mo>
      <mi>ts</mi>
    </mrow>
  </mfrac>
  <mo>.</mo>
</mrow>
</math>
</maths>
<br/>
When the feature vector is not included in the distributed hash table, then a new entry is added to the distributed hash table
</p>
<p id="p-0058" num="0057">
<maths id="MATH-US-00020" num="00020">
<math overflow="scroll">
<mrow>
  <mrow>
    <mo>(</mo>
    <mrow>
      <mi>x</mi>
      <mo>,</mo>
      <mfrac>
        <mi>y</mi>
        <mrow>
          <mi>&#x3c3;</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mi>ts</mi>
        </mrow>
      </mfrac>
    </mrow>
    <mo>)</mo>
  </mrow>
  <mo>.</mo>
</mrow>
</math>
</maths>
</p>
<p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. 5</figref> illustrates a method <b>500</b> to determine a predictor vector for an objective function of a support vector machine utilizing a hash table distributed among a plurality of processors. By distributing computation of the SGD algorithm among a plurality of processors, the training of an SVM via the SGD algorithm can proceed more quickly.</p>
<p id="p-0060" num="0059">At <b>502</b>, a processor selects a sample <b>504</b> from training data. The sample <b>504</b> includes a feature vector x and a classifier value y. In some cases, the same sample <b>504</b> is provided to each processor implementing the SGD algorithm. At <b>506</b>, the processor calculates a local inner product of the current value of the predictor vector w and a kernel mapping function vector &#x3c6;(x) by iterating each entry of a local distributed hash table <b>508</b>. The local distributed hash table <b>508</b> may include a subset of entries of an master hash table given by
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><img id="CUSTOM-CHARACTER-00019" he="3.13mm" wi="3.13mm" file="US08626677-20140107-P00007.TIF" alt="custom character" img-content="character" img-format="tif"/><sub>i</sub>={(<i>x</i><sub>i,j</sub>,&#x3b2;<sub>i,j</sub>)}<img id="CUSTOM-CHARACTER-00020" he="4.57mm" wi="4.23mm" file="US08626677-20140107-P00011.TIF" alt="custom character" img-content="character" img-format="tif"/>&#x2282;<img id="CUSTOM-CHARACTER-00021" he="3.13mm" wi="3.13mm" file="US08626677-20140107-P00007.TIF" alt="custom character" img-content="character" img-format="tif"/>&#x2003;&#x2003;(10)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
Each processor executing the SGD algorithm may be provided with a local distributed hash table <img id="CUSTOM-CHARACTER-00022" he="3.13mm" wi="3.13mm" file="US08626677-20140107-P00007.TIF" alt="custom character" img-content="character" img-format="tif"/><sub>i </sub>that represents a particular subset of the master hash table <img id="CUSTOM-CHARACTER-00023" he="3.13mm" wi="3.13mm" file="US08626677-20140107-P00007.TIF" alt="custom character" img-content="character" img-format="tif"/>.
</p>
<p id="p-0061" num="0060">At <b>510</b>, the processor sums the local inner product calculated by the processor with local inner products <b>512</b> from one or more additional processors via inter-processor communication to determine a global inner product for the predictor vector and the mapping space feature vector. At <b>514</b>, the processor utilizes the global inner product to determine the gradient of the primal objective function for this iteration. As part of determining the gradient of the primal objective function, a hash table update may occur at <b>516</b>. In particular, the local distributed hash table <b>508</b> may be updated at <b>518</b> when the feature vector x from the training data sample <b>504</b> is included in the local distributed hash table <b>508</b>. For example, a new value for the classifier y corresponding to the key x may be calculated at <b>516</b>.</p>
<p id="p-0062" num="0061">When the key x is not found in the local distributed hash table <b>508</b>, then, at <b>520</b>, the processor may receive an indication from an additional processor specifying that the local distributed hash table associated with the additional processor includes the key x. Thus, the local distributed hash table of the additional processor is updated. Further, when the key x is not included in the local distributed hash tables of any of the plurality of processors executing the stochastic gradient descent algorithm, a new entry may be added to one of the local hash tables at <b>522</b>. For example, the local distributed hash table associated with the processor having the most free resources may be updated by adding a new entry based on the key x and the corresponding classifier value y.</p>
<p id="p-0063" num="0062">At <b>524</b>, a local predictor vector is updated based on the dot product of the classifier value y and the inner product of the current value of the predictor vector and the kernel mapping function vector. At <b>526</b>, if there are further iterations, the method returns to <b>502</b>, otherwise, the method moves to <b>528</b>. At <b>528</b>, the local predictor vectors <b>530</b> from each processor are combined to form a global predictor vector that is returned to the support vector machine. An example of instructions for performing the method <b>500</b> may be expressed as:</p>
<p id="p-0064" num="0063">
<tables id="TABLE-US-00003" num="00003">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="center"/>
<thead>
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
<row>
<entry>FOR PROCESSOR i</entry>
</row>
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="offset" colwidth="14pt" align="left"/>
<colspec colname="1" colwidth="28pt" align="left"/>
<colspec colname="2" colwidth="175pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>&#x2002;1.</entry>
<entry>INPUT: &#x3c3;, T, training sample space &#x3a8;</entry>
</row>
<row>
<entry/>
<entry>&#x2002;2.</entry>
<entry>INITIALIZE: &#x2009;<img id="CUSTOM-CHARACTER-00024" he="2.46mm" wi="2.46mm" file="US08626677-20140107-P00012.TIF" alt="custom character" img-content="character" img-format="tif"/> <sub>i </sub>= &#x2205;&#x2009;, s = 1, norm = 0</entry>
</row>
<row>
<entry/>
<entry>&#x2002;3.</entry>
<entry>FOR t = 1, 2, . . . , T</entry>
</row>
<row>
<entry/>
<entry>&#x2002;4.</entry>
<entry>&#x2003;All processors pick up the same random (x, y) &#x2208; &#x3a8;</entry>
</row>
<row>
<entry/>
<entry>&#x2002;5.</entry>
<entry>&#x2003;y<sub>i</sub>&#x2032; &#x2190; s&#x2009;<img id="CUSTOM-CHARACTER-00025" he="2.46mm" wi="1.02mm" file="US08626677-20140107-P00013.TIF" alt="custom character" img-content="character" img-format="tif"/> &#x3bd;<sub>i</sub>, &#x3c6;(x)&#x2009;<img id="CUSTOM-CHARACTER-00026" he="2.46mm" wi="1.02mm" file="US08626677-20140107-P00014.TIF" alt="custom character" img-content="character" img-format="tif"/> &#x2009;by iterating all entries in &#x2009;<img id="CUSTOM-CHARACTER-00027" he="2.46mm" wi="2.46mm" file="US08626677-20140107-P00012.TIF" alt="custom character" img-content="character" img-format="tif"/> <sub>i</sub></entry>
</row>
<row>
<entry/>
<entry>&#x2002;6.</entry>
<entry>&#x2003;Sum up y&#x2032; &#x2190; y<sub>i</sub>&#x2032; via inter-processor communication</entry>
</row>
<row>
<entry/>
<entry>&#x2002;7.</entry>
<entry>&#x2003;s &#x2190; (1 &#x2212; 1/t)s</entry>
</row>
<row>
<entry/>
<entry>&#x2002;8.</entry>
<entry>&#x2003;IF yy&#x2032; &#x3c; 1 THEN</entry>
</row>
<row>
<entry/>
<entry> </entry>
</row>
<row>
<entry/>
<entry>&#x2002;9.</entry>
<entry>&#x2003;&#x2003;
<maths id="MATH-US-00021" num="00021">
<math overflow="scroll">
<mrow>
  <mi>norm</mi>
  <mo>&#x2190;</mo>
  <mrow>
    <mi>norm</mi>
    <mo>+</mo>
    <mrow>
      <mfrac>
        <mrow>
          <mn>2</mn>
          <mo>&#x2062;</mo>
          <mi>y</mi>
        </mrow>
        <mrow>
          <mi>&#x3c3;</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mi>t</mi>
        </mrow>
      </mfrac>
      <mo>&#xb7;</mo>
      <msup>
        <mi>y</mi>
        <mi>&#x2032;</mi>
      </msup>
    </mrow>
    <mo>+</mo>
    <mrow>
      <msup>
        <mrow>
          <mo>(</mo>
          <mfrac>
            <mi>y</mi>
            <mrow>
              <mi>&#x3c3;</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mi>t</mi>
            </mrow>
          </mfrac>
          <mo>)</mo>
        </mrow>
        <mn>2</mn>
      </msup>
      <mo>&#x2062;</mo>
            <mo>&#x2062;</mo>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>x</mi>
          <mo>,</mo>
          <mi>x</mi>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
</entry>
</row>
<row>
<entry/>
<entry> </entry>
</row>
<row>
<entry/>
<entry>10.</entry>
<entry>&#x2003;&#x2003;&#x2003;IF key x is found in &#x2009;<img id="CUSTOM-CHARACTER-00028" he="2.46mm" wi="2.46mm" file="US08626677-20140107-P00012.TIF" alt="custom character" img-content="character" img-format="tif"/> <sub>i</sub>,</entry>
</row>
<row>
<entry/>
<entry> </entry>
</row>
<row>
<entry/>
<entry/>
<entry>&#x2003;&#x2003;
<maths id="MATH-US-00022" num="00022">
<math overflow="scroll">
<mrow>
  <mrow>
    <mi>THEN</mi>
    <mo>&#x2062;</mo>
    <mstyle>
      <mspace width="0.8em" height="0.8ex"/>
    </mstyle>
    <mo>&#x2062;</mo>
    <mi>add</mi>
    <mo>&#x2062;</mo>
    <mstyle>
      <mspace width="0.8em" height="0.8ex"/>
    </mstyle>
    <mo>&#x2062;</mo>
    <mi>its</mi>
    <mo>&#x2062;</mo>
    <mstyle>
      <mspace width="0.8em" height="0.8ex"/>
    </mstyle>
    <mo>&#x2062;</mo>
    <mi>value</mi>
    <mo>&#x2062;</mo>
    <mstyle>
      <mspace width="0.8em" height="0.8ex"/>
    </mstyle>
    <mo>&#x2062;</mo>
    <mi>by</mi>
    <mo>&#x2062;</mo>
    <mstyle>
      <mspace width="0.8em" height="0.8ex"/>
    </mstyle>
    <mo>&#x2062;</mo>
    <mrow>
      <mfrac>
        <mi>y</mi>
        <mrow>
          <mi>&#x3c3;</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mi>t</mi>
        </mrow>
      </mfrac>
      <mo>/</mo>
      <mi>s</mi>
    </mrow>
    <mo>&#x2062;</mo>
    <mstyle>
      <mspace width="0.8em" height="0.8ex"/>
    </mstyle>
    <mo>&#x2062;</mo>
    <mi>in</mi>
    <mo>&#x2062;</mo>
    <mstyle>
      <mspace width="0.8em" height="0.8ex"/>
    </mstyle>
    <mo>&#x2062;</mo>
    <msub>
            <mi>i</mi>
    </msub>
  </mrow>
  <mo>;</mo>
</mrow>
</math>
</maths>
</entry>
</row>
<row>
<entry/>
<entry> </entry>
</row>
<row>
<entry/>
<entry>11.</entry>
<entry>&#x2003;&#x2003;&#x2003;IF no processor reports the existence</entry>
</row>
<row>
<entry/>
<entry/>
<entry>&#x2003;&#x2003;Find a least occupied processor j and add &#x2009;<img id="CUSTOM-CHARACTER-00029" he="2.46mm" wi="2.46mm" file="US08626677-20140107-P00012.TIF" alt="custom character" img-content="character" img-format="tif"/> <sub>j</sub></entry>
</row>
<row>
<entry/>
<entry> </entry>
</row>
<row>
<entry/>
<entry/>
<entry>&#x2003;&#x2003;
<maths id="MATH-US-00023" num="00023">
<math overflow="scroll">
<mrow>
  <mi>a</mi>
  <mo>&#x2062;</mo>
  <mstyle>
    <mspace width="0.8em" height="0.8ex"/>
  </mstyle>
  <mo>&#x2062;</mo>
  <mi>new</mi>
  <mo>&#x2062;</mo>
  <mstyle>
    <mspace width="0.8em" height="0.8ex"/>
  </mstyle>
  <mo>&#x2062;</mo>
  <mi>entry</mi>
  <mo>&#x2062;</mo>
  <mstyle>
    <mspace width="0.8em" height="0.8ex"/>
  </mstyle>
  <mo>&#x2062;</mo>
  <mrow>
    <mo>(</mo>
    <mrow>
      <mi>x</mi>
      <mo>,</mo>
      <mrow>
        <mfrac>
          <mi>y</mi>
          <mrow>
            <mi>&#x3c3;</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>t</mi>
          </mrow>
        </mfrac>
        <mo>/</mo>
        <mi>s</mi>
      </mrow>
    </mrow>
    <mo>)</mo>
  </mrow>
</mrow>
</math>
</maths>
</entry>
</row>
<row>
<entry/>
<entry> </entry>
</row>
<row>
<entry/>
<entry>12.</entry>
<entry>&#x2003;&#x2003;&#x2003;IF norm &#x3e; 1/&#x3c3;</entry>
</row>
<row>
<entry/>
<entry> </entry>
</row>
<row>
<entry/>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;
<maths id="MATH-US-00024" num="00024">
<math overflow="scroll">
<mrow>
  <mrow>
    <mrow>
      <mi>THEN</mi>
      <mo>&#x2062;</mo>
      <mstyle>
        <mspace width="0.8em" height="0.8ex"/>
      </mstyle>
      <mo>&#x2062;</mo>
      <mi>s</mi>
    </mrow>
    <mo>&#x2190;</mo>
    <mrow>
      <mi>s</mi>
      <mo>&#xb7;</mo>
      <mfrac>
        <mn>1</mn>
        <msqrt>
          <mrow>
            <mi>&#x3c3;</mi>
            <mo>&#xb7;</mo>
            <mi>norm</mi>
          </mrow>
        </msqrt>
      </mfrac>
    </mrow>
  </mrow>
  <mo>;</mo>
  <mrow>
    <mi>norm</mi>
    <mo>&#x2190;</mo>
    <mrow>
      <mn>1</mn>
      <mo>/</mo>
      <mi>&#x3c3;</mi>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
</entry>
</row>
<row>
<entry/>
<entry> </entry>
</row>
<row>
<entry/>
<entry>13.</entry>
<entry>&#x2003;RETURN s&#x3bd; by iterating all entries in &#x2009;<img id="CUSTOM-CHARACTER-00030" he="2.46mm" wi="2.46mm" file="US08626677-20140107-P00012.TIF" alt="custom character" img-content="character" img-format="tif"/> <sub>1</sub>, . . . H<sub>p</sub></entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. 6</figref> illustrates a method <b>600</b> to implement a packing strategy when determining a predictor vector for a primal objective function of a support vector machine via a parallelized stochastic gradient descent algorithm. At <b>602</b>, a number of iterations are determined for which to combine calculations of a stochastic gradient descent algorithm. For example, the stochastic gradient descent algorithm may reduce the number of communications between processors by combining results of the calculations of a number of iterations that is less than or equal to the total number of iterations before the processors exchange the results.</p>
<p id="p-0066" num="0065">At <b>604</b>, a number of data samples <b>606</b> from a training data set are selected corresponding to the number of iterations for which calculations are to be combined. For example, if calculations from 100 iterations are to be combined, then 100 data samples are chosen from the training data set. At <b>608</b>, the inner product of the predictor vector w and the kernel mapping function vector &#x3c6;(x) for each iteration in the number of iterations is pre-calculated. In some cases, the inner product for a subsequent iteration is calculated based on the inner product from one or more previous iterations. For example, the inner product for a third iteration may be pre-calculated based on the inner product of the second iteration, the inner product of the first iteration, or a combination thereof. In addition, at <b>610</b>, at least one kernel function <img id="CUSTOM-CHARACTER-00031" he="3.13mm" wi="3.13mm" file="US08626677-20140107-P00002.TIF" alt="custom character" img-content="character" img-format="tif"/>(x, x) for each iteration is pre-calculated. The kernel function for a particular subsequent iteration may be based on kernel functions for one or more previous iterations. An example packing strategy may be illustrated as:</p>
<p id="p-0067" num="0066">
<tables id="TABLE-US-00004" num="00004">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="center"/>
<thead>
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry><chemistry id="CHEM-US-00001" num="00001">
<img id="EMI-C00001" he="48.34mm" wi="69.00mm" file="US08626677-20140107-C00001.TIF" alt="embedded image" img-content="table" img-format="tif"/>
</chemistry>
</entry>
</row>
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
<br/>
where &#x25aa; represent coefficients calculated for each iteration.
</p>
<p id="p-0068" num="0067">At <b>612</b>, the inner products for each of the number of iterations are transmitted to a number of additional processors in one communication and the inner products from the processors executing the stochastic gradient descent algorithm are summed at <b>614</b>. At <b>616</b>, the gradient for the primal objective function is calculated for each of the number of iterations utilizing the pre-calculated inner products from <b>608</b>. During the calculation of the gradient, an update of the local distributed hash table may take place for each iteration. In a particular implementation, the local distributed hash table update may take place according to a process including <b>516</b>-<b>522</b> of <figref idref="DRAWINGS">FIG. 5</figref>.</p>
<p id="p-0069" num="0068">At <b>618</b>, a local predictor vector is updated for each of the number of iterations based on the gradient calculated for each respective iteration. In conjunction with the illustrated packing strategy, the predictor vector for a particular iteration, w<sub>t</sub>, can be calculated according to:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>w</i><sub>t</sub><i>=a</i><sub>t</sub><i>w</i><sub>t&#x2212;1</sub><i>+b</i><sub>t</sub>&#x3c6;(<i>x</i><sub>t</sub>)&#x2003;&#x2003;(11)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where the coefficients a<sub>t </sub>and b<sub>t </sub>are represented in the packing strategy illustrated above as &#x25aa;. At <b>620</b>, if further iterations remain, the method returns to <b>602</b> such that calculations for a new group of iterations can be performed. At <b>622</b>, when no iterations remain, the final local predictor vectors for each processor are combined to determine a global predictor vector that is returned to the support vector machine.
</p>
<p id="p-0070" num="0069">An example of instructions for performing the method <b>600</b> may be expressed as:</p>
<p id="p-0071" num="0070">
<tables id="TABLE-US-00005" num="00005">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="center"/>
<thead>
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
<row>
<entry>PROCESSOR i</entry>
</row>
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="21pt" align="left"/>
<colspec colname="2" colwidth="196pt" align="left"/>
<tbody valign="top">
<row>
<entry>1.</entry>
<entry>INPUT: &#x3c3;, T , r, training sample space &#x3a8;</entry>
</row>
<row>
<entry>2.</entry>
<entry>INITIALIZE: &#x2009;<img id="CUSTOM-CHARACTER-00032" he="2.79mm" wi="2.46mm" file="US08626677-20140107-P00015.TIF" alt="custom character" img-content="character" img-format="tif"/> <sub>i </sub>= &#x2205;&#x2009;, s = 1, norm = 0</entry>
</row>
<row>
<entry>3.</entry>
<entry>FOR t = 1, 2, . . . , T/r</entry>
</row>
<row>
<entry>4.</entry>
<entry>&#x2003;Randomly pick up r samples (x<sub>1</sub>, y<sub>1</sub>) . . . (x<sub>r</sub>, y<sub>r</sub>) &#x2208; &#x3a8;. Ensure all</entry>
</row>
<row>
<entry/>
<entry>processors receive the same samples.</entry>
</row>
<row>
<entry>5.</entry>
<entry>&#x2003;FOR k = 1, . . . r Do</entry>
</row>
<row>
<entry>6.</entry>
<entry>&#x2003;&#x2003;y<sub>i,k</sub>&#x2032; &#x2190; s&#x2009;<img id="CUSTOM-CHARACTER-00033" he="2.46mm" wi="1.02mm" file="US08626677-20140107-P00016.TIF" alt="custom character" img-content="character" img-format="tif"/> &#x3bd;<sub>k</sub>, &#x3c6;(x<sub>i</sub>)&#x2009;<img id="CUSTOM-CHARACTER-00034" he="2.46mm" wi="1.02mm" file="US08626677-20140107-P00017.TIF" alt="custom character" img-content="character" img-format="tif"/> &#x2009;by iterating all entries in &#x2009;<img id="CUSTOM-CHARACTER-00035" he="2.79mm" wi="2.46mm" file="US08626677-20140107-P00015.TIF" alt="custom character" img-content="character" img-format="tif"/> <sub>i</sub></entry>
</row>
<row>
<entry>7.</entry>
<entry>&#x2003;Communicate with other processors to get y<sub>k</sub>&#x2032; = &#x3a3;<sub>i</sub>y<sub>i,k</sub>&#x2032;</entry>
</row>
<row>
<entry>8.</entry>
<entry>&#x2003;Calculate pair<sub>i,j </sub>= &#x2009;<img id="CUSTOM-CHARACTER-00036" he="2.79mm" wi="2.79mm" file="US08626677-20140107-P00018.TIF" alt="custom character" img-content="character" img-format="tif"/> (x<sub>i</sub>, x<sub>j</sub>) in distribution</entry>
</row>
<row>
<entry>9.</entry>
<entry>&#x2003;LocalSet &#x2190; &#x2205;</entry>
</row>
<row>
<entry>10.</entry>
<entry>&#x2003;&#x2003;FOR k = 1, . . . r Do</entry>
</row>
<row>
<entry>11.</entry>
<entry>&#x2003;&#x2003;&#x2003;s &#x2190; (1 &#x2212; 1/t)s</entry>
</row>
<row>
<entry>12.</entry>
<entry>&#x2003;&#x2003;&#x2003;FOR l = k + 1 . . . r Do y<sub>l</sub>&#x2032; &#x2190; (1 &#x2212; 1/t)y<sub>l</sub>&#x2032;</entry>
</row>
<row>
<entry>13.</entry>
<entry>&#x2003;&#x2003;&#x2003;IF y<sub>k</sub>y<sub>k</sub>&#x2032; &#x3c; 1 THEN</entry>
</row>
<row>
<entry> </entry>
</row>
<row>
<entry>14.</entry>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;
<maths id="MATH-US-00025" num="00025">
<math overflow="scroll">
<mrow>
  <mi>norm</mi>
  <mo>&#x2190;</mo>
  <mrow>
    <mi>norm</mi>
    <mo>+</mo>
    <mrow>
      <mfrac>
        <mrow>
          <mn>2</mn>
          <mo>&#x2062;</mo>
          <msub>
            <mi>y</mi>
            <mi>k</mi>
          </msub>
        </mrow>
        <mrow>
          <mi>&#x3c3;</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mi>t</mi>
        </mrow>
      </mfrac>
      <mo>&#xb7;</mo>
      <msubsup>
        <mi>y</mi>
        <mi>k</mi>
        <mi>&#x2032;</mi>
      </msubsup>
    </mrow>
    <mo>+</mo>
    <mrow>
      <msup>
        <mrow>
          <mo>(</mo>
          <mfrac>
            <msub>
              <mi>y</mi>
              <mi>k</mi>
            </msub>
            <mrow>
              <mi>&#x3c3;</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mi>t</mi>
            </mrow>
          </mfrac>
          <mo>)</mo>
        </mrow>
        <mn>2</mn>
      </msup>
      <mo>&#x2062;</mo>
      <msub>
        <mi>pair</mi>
        <mrow>
          <mi>k</mi>
          <mo>,</mo>
          <mi>k</mi>
        </mrow>
      </msub>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
</entry>
</row>
<row>
<entry> </entry>
</row>
<row>
<entry>15.</entry>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;
<maths id="MATH-US-00026" num="00026">
<math overflow="scroll">
<mrow>
  <mi>LocalSet</mi>
  <mo>&#x2190;</mo>
  <mrow>
    <mi>LocalSet</mi>
    <mo>&#x22c3;</mo>
    <mrow>
      <mo>{</mo>
      <mrow>
        <mo>(</mo>
        <mrow>
          <msub>
            <mi>x</mi>
            <mi>k</mi>
          </msub>
          <mo>,</mo>
          <mrow>
            <mfrac>
              <msub>
                <mi>y</mi>
                <mi>k</mi>
              </msub>
              <mrow>
                <mi>&#x3c3;</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mi>t</mi>
              </mrow>
            </mfrac>
            <mo>/</mo>
            <mi>s</mi>
          </mrow>
        </mrow>
        <mo>)</mo>
      </mrow>
      <mo>}</mo>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
</entry>
</row>
<row>
<entry> </entry>
</row>
<row>
<entry>16.</entry>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;
<maths id="MATH-US-00027" num="00027">
<math overflow="scroll">
<mrow>
  <mrow>
    <mi>For</mi>
    <mo>&#x2062;</mo>
    <mstyle>
      <mspace width="0.8em" height="0.8ex"/>
    </mstyle>
    <mo>&#x2062;</mo>
    <mi>l</mi>
  </mrow>
  <mo>=</mo>
  <mrow>
    <mrow>
      <mi>k</mi>
      <mo>+</mo>
      <mrow>
        <mn>1</mn>
        <mo>&#x2062;</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mi>&#x2026;</mi>
        <mo>&#x2062;</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mi>r</mi>
        <mo>&#x2062;</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mi>DO</mi>
        <mo>&#x2062;</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <msubsup>
          <mi>y</mi>
          <mi>l</mi>
          <mi>&#x2032;</mi>
        </msubsup>
      </mrow>
    </mrow>
    <mo>&#x2190;</mo>
    <mrow>
      <msubsup>
        <mi>y</mi>
        <mi>l</mi>
        <mi>&#x2032;</mi>
      </msubsup>
      <mo>+</mo>
      <mrow>
        <mfrac>
          <msub>
            <mi>y</mi>
            <mi>k</mi>
          </msub>
          <mrow>
            <mi>&#x3c3;</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>t</mi>
          </mrow>
        </mfrac>
        <mo>&#xb7;</mo>
        <msub>
          <mi>pair</mi>
          <mrow>
            <mi>k</mi>
            <mo>,</mo>
            <mi>l</mi>
          </mrow>
        </msub>
      </mrow>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
</entry>
</row>
<row>
<entry> </entry>
</row>
<row>
<entry>17.</entry>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;IF norm &#x3e; 1/&#x3c3; THEN</entry>
</row>
<row>
<entry> </entry>
</row>
<row>
<entry>18.</entry>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;
<maths id="MATH-US-00028" num="00028">
<math overflow="scroll">
<mrow>
  <mrow>
    <mi>s</mi>
    <mo>&#x2190;</mo>
    <mrow>
      <mi>s</mi>
      <mo>&#xb7;</mo>
      <mfrac>
        <mn>1</mn>
        <msqrt>
          <mrow>
            <mi>&#x3c3;</mi>
            <mo>&#xb7;</mo>
            <mi>norm</mi>
          </mrow>
        </msqrt>
      </mfrac>
    </mrow>
  </mrow>
  <mo>;</mo>
  <mrow>
    <mi>norm</mi>
    <mo>&#x2190;</mo>
    <mrow>
      <mn>1</mn>
      <mo>/</mo>
      <mi>&#x3c3;</mi>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
</entry>
</row>
<row>
<entry> </entry>
</row>
<row>
<entry>19.</entry>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;FOR l = k + 1 . . . r Do y<sub>l</sub>&#x2032; &#x2190; (1 &#x2212; 1/t)y<sub>l</sub>&#x2032;</entry>
</row>
<row>
<entry>20.</entry>
<entry>&#x2003;&#x2003;Update &#x2009;<img id="CUSTOM-CHARACTER-00037" he="2.79mm" wi="2.46mm" file="US08626677-20140107-P00015.TIF" alt="custom character" img-content="character" img-format="tif"/> <sub>i </sub>according to LocalSet, for those elements reported</entry>
</row>
<row>
<entry/>
<entry>not existed in &#x2009;<img id="CUSTOM-CHARACTER-00038" he="2.79mm" wi="2.46mm" file="US08626677-20140107-P00015.TIF" alt="custom character" img-content="character" img-format="tif"/> <sub>1 </sub>. . . &#x2009;<img id="CUSTOM-CHARACTER-00039" he="2.79mm" wi="2.46mm" file="US08626677-20140107-P00015.TIF" alt="custom character" img-content="character" img-format="tif"/> <sub>p</sub>,</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;add them to the least occupied processors.</entry>
</row>
<row>
<entry>21.</entry>
<entry/>
</row>
<row>
<entry>22.</entry>
<entry>&#x2003;RETURN s&#x3bd;<sub>i </sub>by iterating all entries in &#x2009;<img id="CUSTOM-CHARACTER-00040" he="2.79mm" wi="2.46mm" file="US08626677-20140107-P00015.TIF" alt="custom character" img-content="character" img-format="tif"/> <sub>1</sub>, . . . &#x2009;<img id="CUSTOM-CHARACTER-00041" he="2.79mm" wi="2.46mm" file="US08626677-20140107-P00015.TIF" alt="custom character" img-content="character" img-format="tif"/> <sub>p</sub></entry>
</row>
<row>
<entry namest="1" nameend="2" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0072" num="0071">Although the coefficients a<sub>t </sub>and b<sub>t </sub>are calculated for each iteration and are not pre-calculated according to the packing strategy, pre-calculating the inner products <img id="CUSTOM-CHARACTER-00042" he="3.13mm" wi="0.68mm" file="US08626677-20140107-P00003.TIF" alt="custom character" img-content="character" img-format="tif"/>w<sub>t</sub>, &#x3c6;(x<sub>t</sub>)<img id="CUSTOM-CHARACTER-00043" he="3.13mm" wi="0.68mm" file="US08626677-20140107-P00004.TIF" alt="custom character" img-content="character" img-format="tif"/> and packing the inner products from a number of iterations into a single communication reduces the frequency of communications between processors during execution of the stochastic gradient descent algorithm. In addition, pre-calculating the Kernel functions <img id="CUSTOM-CHARACTER-00044" he="3.13mm" wi="3.13mm" file="US08626677-20140107-P00002.TIF" alt="custom character" img-content="character" img-format="tif"/>(x, x) for each iteration can save time when calculating the predictor vector for each iteration. In this way, the packing strategy reduces the amount of time to calculate the predictor vector for the support vector machine and processing resources utilized when calculating the predictor vector for each iteration are reduced.</p>
<p id="h-0005" num="0000">Conclusion</p>
<p id="p-0073" num="0072">Although the subject matter has been described in language specific to structural features and/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather, the specific features and acts described above are disclosed as example forms of implementing the claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-chemistry idref="CHEM-US-00001" cdx-file="US08626677-20140107-C00001.CDX" mol-file="US08626677-20140107-C00001.MOL"/>
<us-math idrefs="MATH-US-00001" nb-file="US08626677-20140107-M00001.NB">
<img id="EMI-M00001" he="8.47mm" wi="76.20mm" file="US08626677-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US08626677-20140107-M00002.NB">
<img id="EMI-M00002" he="5.67mm" wi="76.20mm" file="US08626677-20140107-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00003" nb-file="US08626677-20140107-M00003.NB">
<img id="EMI-M00003" he="8.47mm" wi="76.20mm" file="US08626677-20140107-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00004" nb-file="US08626677-20140107-M00004.NB">
<img id="EMI-M00004" he="8.47mm" wi="76.20mm" file="US08626677-20140107-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00005" nb-file="US08626677-20140107-M00005.NB">
<img id="EMI-M00005" he="5.67mm" wi="76.20mm" file="US08626677-20140107-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00006" nb-file="US08626677-20140107-M00006.NB">
<img id="EMI-M00006" he="6.35mm" wi="76.20mm" file="US08626677-20140107-M00006.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00007" nb-file="US08626677-20140107-M00007.NB">
<img id="EMI-M00007" he="7.45mm" wi="76.20mm" file="US08626677-20140107-M00007.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00008" nb-file="US08626677-20140107-M00008.NB">
<img id="EMI-M00008" he="8.47mm" wi="76.20mm" file="US08626677-20140107-M00008.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00009" nb-file="US08626677-20140107-M00009.NB">
<img id="EMI-M00009" he="9.91mm" wi="76.20mm" file="US08626677-20140107-M00009.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00010" nb-file="US08626677-20140107-M00010.NB">
<img id="EMI-M00010" he="8.81mm" wi="76.20mm" file="US08626677-20140107-M00010.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00011" nb-file="US08626677-20140107-M00011.NB">
<img id="EMI-M00011" he="5.67mm" wi="36.32mm" file="US08626677-20140107-M00011.TIF" alt="embedded image " img-content="table" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00012" nb-file="US08626677-20140107-M00012.NB">
<img id="EMI-M00012" he="8.47mm" wi="25.40mm" file="US08626677-20140107-M00012.TIF" alt="embedded image " img-content="table" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00013" nb-file="US08626677-20140107-M00013.NB">
<img id="EMI-M00013" he="5.67mm" wi="76.20mm" file="US08626677-20140107-M00013.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00014" nb-file="US08626677-20140107-M00014.NB">
<img id="EMI-M00014" he="5.67mm" wi="76.20mm" file="US08626677-20140107-M00014.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00015" nb-file="US08626677-20140107-M00015.NB">
<img id="EMI-M00015" he="6.01mm" wi="39.88mm" file="US08626677-20140107-M00015.TIF" alt="embedded image " img-content="table" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00016" nb-file="US08626677-20140107-M00016.NB">
<img id="EMI-M00016" he="5.67mm" wi="37.42mm" file="US08626677-20140107-M00016.TIF" alt="embedded image " img-content="table" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00017" nb-file="US08626677-20140107-M00017.NB">
<img id="EMI-M00017" he="5.67mm" wi="38.44mm" file="US08626677-20140107-M00017.TIF" alt="embedded image " img-content="table" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00018" nb-file="US08626677-20140107-M00018.NB">
<img id="EMI-M00018" he="7.03mm" wi="40.98mm" file="US08626677-20140107-M00018.TIF" alt="embedded image " img-content="table" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00019" nb-file="US08626677-20140107-M00019.NB">
<img id="EMI-M00019" he="5.67mm" wi="76.20mm" file="US08626677-20140107-M00019.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00020" nb-file="US08626677-20140107-M00020.NB">
<img id="EMI-M00020" he="5.67mm" wi="76.20mm" file="US08626677-20140107-M00020.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00021" nb-file="US08626677-20140107-M00021.NB">
<img id="EMI-M00021" he="6.01mm" wi="39.88mm" file="US08626677-20140107-M00021.TIF" alt="embedded image " img-content="table" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00022" nb-file="US08626677-20140107-M00022.NB">
<img id="EMI-M00022" he="5.67mm" wi="40.22mm" file="US08626677-20140107-M00022.TIF" alt="embedded image " img-content="table" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00023" nb-file="US08626677-20140107-M00023.NB">
<img id="EMI-M00023" he="5.67mm" wi="24.72mm" file="US08626677-20140107-M00023.TIF" alt="embedded image " img-content="table" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00024" nb-file="US08626677-20140107-M00024.NB">
<img id="EMI-M00024" he="7.03mm" wi="40.98mm" file="US08626677-20140107-M00024.TIF" alt="embedded image " img-content="table" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00025" nb-file="US08626677-20140107-M00025.NB">
<img id="EMI-M00025" he="6.01mm" wi="39.54mm" file="US08626677-20140107-M00025.TIF" alt="embedded image " img-content="table" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00026" nb-file="US08626677-20140107-M00026.NB">
<img id="EMI-M00026" he="5.67mm" wi="38.44mm" file="US08626677-20140107-M00026.TIF" alt="embedded image " img-content="table" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00027" nb-file="US08626677-20140107-M00027.NB">
<img id="EMI-M00027" he="5.67mm" wi="44.79mm" file="US08626677-20140107-M00027.TIF" alt="embedded image " img-content="table" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00028" nb-file="US08626677-20140107-M00028.NB">
<img id="EMI-M00028" he="7.03mm" wi="33.53mm" file="US08626677-20140107-M00028.TIF" alt="embedded image " img-content="table" img-format="tif"/>
</us-math>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method comprising:
<claim-text>training, by at least one computing device including a plurality of processors, a non-linear support vector machine with a stochastic gradient descent algorithm using a training data set, wherein training the non-linear support vector machine includes executing the stochastic gradient descent algorithm for a number of iterations and wherein at least a portion of results of computations of a plurality of iterations of the number of iterations are combined prior to an exchange of results of the computations of the plurality of iterations by at least a subset of the plurality of processors; and</claim-text>
<claim-text>parallelizing, by the at least one computing device, at least a portion of the computations of the stochastic gradient descent algorithm on the plurality of processors.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein training the non-linear support vector machine includes determining a predictor vector that minimizes a primal objective function of the non-linear support vector machine, the primal objective function of the non-linear support vector machine including a regularization portion and a loss portion.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each iteration of the stochastic gradient descent algorithm includes:
<claim-text>selecting a random data sample from the training data set, the random data sample having a set of features;</claim-text>
<claim-text>calculating an inner product of the predictor vector and a kernel mapping function vector;</claim-text>
<claim-text>approximating a total value of the loss portion of the primal objective function based on a value of the loss portion for the random data sample;</claim-text>
<claim-text>calculating a gradient of the primal objective function; and</claim-text>
<claim-text>updating the predictor vector.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein a hash table is utilized to calculate the inner product for the predictor vector and the kernel mapping function vector.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein parallelizing at least a portion of the computations of the stochastic gradient descent algorithm includes distributing entries of the hash table to each of the plurality of processors.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the number of iterations is predetermined.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>receiving a new data sample after the non-linear support vector machine has been trained, the new data sample including a set of features; and</claim-text>
<claim-text>characterizing the new data sample according to the set of features of the new data sample.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein characterizing the new data sample includes at least one of: determining a classification for the new data sample, determining a ranking of the new data sample, or predicting an output corresponding to the new data sample.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a first computing device includes a first portion of the plurality of processors and a second computing device includes a second portion of the plurality of processors.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. One or more computer-readable storage memory including instructions that, when executed by a plurality of processors, perform acts comprising:
<claim-text>training, by the plurality of processors, a non-linear support vector machine with a stochastic gradient descent algorithm using a training data set, wherein training the non-linear support vector machine includes executing the stochastic gradient descent algorithm for a number of iterations and wherein at least a portion of results of computations of a plurality of iterations of the number of iterations are combined prior to an exchange of results of the computations of the plurality of iterations by at least a subset of the plurality of processors; and</claim-text>
<claim-text>parallelizing, by the at least one computing device, at least a portion of the computations of the stochastic gradient descent algorithm on the plurality of processors.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The one or more computer-readable storage memory of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein training the non-linear support vector machine includes determining a predictor vector that minimizes a primal objective function of the non-linear support vector machine, the primal objective function of the non-linear support vector machine including a regularization portion and a loss portion.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The one or more computer-readable storage memory of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein each iteration of the stochastic gradient descent algorithm includes:
<claim-text>selecting a random data sample from the training data set, the random data sample having a set of features;</claim-text>
<claim-text>calculating an inner product of the predictor vector and a kernel mapping function vector;</claim-text>
<claim-text>approximating a total value of the loss portion of the primal objective function based on a value of the loss portion for the random data sample;</claim-text>
<claim-text>calculating a gradient of the primal objective function; and</claim-text>
<claim-text>updating the predictor vector.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The one or more computer-readable storage memory of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein a hash table is utilized to calculate the inner product for the predictor vector and the kernel mapping function vector.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The one or more computer-readable storage memory of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein parallelizing at least a portion of the computations of the stochastic gradient descent algorithm includes distributing entries of the hash table to each of the plurality of processors.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. A computing system comprising:
<claim-text>a plurality of processors;</claim-text>
<claim-text>memory accessible by at least one of the plurality of processors, the memory storing instructions that, when executed by the plurality of processor, perform acts comprising:</claim-text>
<claim-text>training a non-linear support vector machine with a stochastic gradient descent algorithm using a training data set, wherein training the non-linear support vector machine includes executing the stochastic gradient descent algorithm for a number of iterations and wherein at least a portion of results of computations of a plurality of iterations of the number of iterations are combined prior to an exchange of results of the computations of the plurality of iterations by at least a subset of the plurality of processors; and</claim-text>
<claim-text>parallelizing at least a portion of the computations of the stochastic gradient descent algorithm on the plurality of processors.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The computing system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, the acts further comprising:
<claim-text>receiving a new data sample after the non-linear support vector machine has been trained, the new data sample including a set of features; and</claim-text>
<claim-text>characterizing the new data sample according to the set of features of the new data sample.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The computing system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein characterizing the new data sample includes at least one of: determining a classification for the new data sample, determining a ranking of the new data sample, or predicting an output corresponding to the new data sample.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The computing system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein a first computing device includes a first portion of the plurality of processors and a second computing device includes a second portion of the plurality of processors. </claim-text>
</claim>
</claims>
</us-patent-grant>
