<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625752-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625752</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>11680058</doc-number>
<date>20070228</date>
</document-id>
</application-reference>
<us-application-series-code>11</us-application-series-code>
<us-term-of-grant>
<us-term-extension>2057</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>M</subclass>
<main-group>11</main-group>
<subgroup>06</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>379 8818</main-classification>
<further-classification>704270</further-classification>
</classification-national>
<invention-title id="d2e53">Closed-loop command and response system for automatic communications between interacting computer systems over an audio communications channel</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>3937889</doc-number>
<kind>A</kind>
<name>Bell, III et al.</name>
<date>19760200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>370494</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>3978288</doc-number>
<kind>A</kind>
<name>Bruckner et al.</name>
<date>19760800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>380253</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>4060694</doc-number>
<kind>A</kind>
<name>Suzuki et al.</name>
<date>19771100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>704252</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>4071888</doc-number>
<kind>A</kind>
<name>Owens</name>
<date>19780100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>704258</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>4117263</doc-number>
<kind>A</kind>
<name>Yeh</name>
<date>19780900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>704267</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>4196311</doc-number>
<kind>A</kind>
<name>Hoven</name>
<date>19800400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>370458</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>4327251</doc-number>
<kind>A</kind>
<name>Fomenko et al.</name>
<date>19820400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>704270</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>4612416</doc-number>
<kind>A</kind>
<name>Emerson et al.</name>
<date>19860900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>379 8818</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>4614841</doc-number>
<kind>A</kind>
<name>Babecki et al.</name>
<date>19860900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>379 9329</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>4790003</doc-number>
<kind>A</kind>
<name>Kepley et al.</name>
<date>19881200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>379 8818</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>4837798</doc-number>
<kind>A</kind>
<name>Cohen et al.</name>
<date>19890600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>379 8814</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>4935954</doc-number>
<kind>A</kind>
<name>Thompson et al.</name>
<date>19900600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>379 8804</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>5299240</doc-number>
<kind>A</kind>
<name>Iwahashi et al.</name>
<date>19940300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>375240</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>5471534</doc-number>
<kind>A</kind>
<name>Utter</name>
<date>19951100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>381  4</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>5528728</doc-number>
<kind>A</kind>
<name>Matsuura et al.</name>
<date>19960600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>704232</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>5659599</doc-number>
<kind>A</kind>
<name>Arumainayagam et al.</name>
<date>19970800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>5687220</doc-number>
<kind>A</kind>
<name>Finnigan</name>
<date>19971100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>5822405</doc-number>
<kind>A</kind>
<name>Astarabadi</name>
<date>19981000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>379 8804</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>5915001</doc-number>
<kind>A</kind>
<name>Uppaluru</name>
<date>19990600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>379 8822</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>6052442</doc-number>
<kind>A</kind>
<name>Cooper et al.</name>
<date>20000400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>379 8819</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>6088428</doc-number>
<kind>A</kind>
<name>Trandal et al.</name>
<date>20000700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>379 8802</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>6157705</doc-number>
<kind>A</kind>
<name>Perrone</name>
<date>20001200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>379 8801</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>6173042</doc-number>
<kind>B1</kind>
<name>Wu</name>
<date>20010100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>379 8804</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>6208966</doc-number>
<kind>B1</kind>
<name>Bulfer</name>
<date>20010300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>704251</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>6263052</doc-number>
<kind>B1</kind>
<name>Cruze</name>
<date>20010700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>379 8818</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>6295341</doc-number>
<kind>B1</kind>
<name>Muller</name>
<date>20010900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>379 8818</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>6321194</doc-number>
<kind>B1</kind>
<name>Berestesky</name>
<date>20011100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704232</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>6324499</doc-number>
<kind>B1</kind>
<name>Lewis et al.</name>
<date>20011100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>704233</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>6327345</doc-number>
<kind>B1</kind>
<name>Jordan</name>
<date>20011200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>379 8802</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>6330308</doc-number>
<kind>B1</kind>
<name>Cheston, III et al.</name>
<date>20011200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>379 8804</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>US</country>
<doc-number>6335962</doc-number>
<kind>B1</kind>
<name>Ali et al.</name>
<date>20020100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>379 8811</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>US</country>
<doc-number>6335963</doc-number>
<kind>B1</kind>
<name>Bosco</name>
<date>20020100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>379 8812</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00033">
<document-id>
<country>US</country>
<doc-number>6337977</doc-number>
<kind>B1</kind>
<name>Ranta</name>
<date>20020100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>455413</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00034">
<document-id>
<country>US</country>
<doc-number>6339591</doc-number>
<kind>B1</kind>
<name>Migimatsu</name>
<date>20020100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>370352</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00035">
<document-id>
<country>US</country>
<doc-number>6341160</doc-number>
<kind>B2</kind>
<name>Tverskoy et al.</name>
<date>20020100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>379 8813</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00036">
<document-id>
<country>US</country>
<doc-number>6341264</doc-number>
<kind>B1</kind>
<name>Kuhn et al.</name>
<date>20020100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>704255</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00037">
<document-id>
<country>US</country>
<doc-number>6345250</doc-number>
<kind>B1</kind>
<name>Martin</name>
<date>20020200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>704260</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00038">
<document-id>
<country>US</country>
<doc-number>6345254</doc-number>
<kind>B1</kind>
<name>Lewis et al.</name>
<date>20020200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>704275</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00039">
<document-id>
<country>US</country>
<doc-number>6347134</doc-number>
<kind>B1</kind>
<name>Sherwood et al.</name>
<date>20020200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00040">
<document-id>
<country>US</country>
<doc-number>6366882</doc-number>
<kind>B1</kind>
<name>Bijl et al.</name>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00041">
<document-id>
<country>US</country>
<doc-number>6377922</doc-number>
<kind>B2</kind>
<name>Brown et al.</name>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00042">
<document-id>
<country>US</country>
<doc-number>6418203</doc-number>
<kind>B1</kind>
<name>Marcie</name>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>379 9001</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00043">
<document-id>
<country>US</country>
<doc-number>6442242</doc-number>
<kind>B1</kind>
<name>McAllister et al.</name>
<date>20020800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>379 671</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00044">
<document-id>
<country>US</country>
<doc-number>6453252</doc-number>
<kind>B1</kind>
<name>Laroche</name>
<date>20020900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00045">
<document-id>
<country>US</country>
<doc-number>6487278</doc-number>
<kind>B1</kind>
<name>Skladman et al.</name>
<date>20021100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>379 8813</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00046">
<document-id>
<country>US</country>
<doc-number>6493696</doc-number>
<kind>B1</kind>
<name>Chazin</name>
<date>20021200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00047">
<document-id>
<country>US</country>
<doc-number>6529585</doc-number>
<kind>B2</kind>
<name>Ng et al.</name>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00048">
<document-id>
<country>US</country>
<doc-number>6563912</doc-number>
<kind>B1</kind>
<name>Dorfman et al.</name>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>379 8813</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00049">
<document-id>
<country>US</country>
<doc-number>6564292</doc-number>
<kind>B2</kind>
<name>Wei Loon et al.</name>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711111</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00050">
<document-id>
<country>US</country>
<doc-number>6724867</doc-number>
<kind>B1</kind>
<name>Henderson</name>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>379 8822</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00051">
<document-id>
<country>US</country>
<doc-number>6731724</doc-number>
<kind>B2</kind>
<name>Wesemann et al.</name>
<date>20040500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00052">
<document-id>
<country>US</country>
<doc-number>6748360</doc-number>
<kind>B2</kind>
<name>Pitman et al.</name>
<date>20040600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00053">
<document-id>
<country>US</country>
<doc-number>6751298</doc-number>
<kind>B2</kind>
<name>Bhogal et al.</name>
<date>20040600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00054">
<document-id>
<country>US</country>
<doc-number>6775360</doc-number>
<kind>B2</kind>
<name>Davidson et al.</name>
<date>20040800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00055">
<document-id>
<country>US</country>
<doc-number>6781962</doc-number>
<kind>B1</kind>
<name>Williams et al.</name>
<date>20040800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00056">
<document-id>
<country>US</country>
<doc-number>6865258</doc-number>
<kind>B1</kind>
<name>Polcyn</name>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00057">
<document-id>
<country>US</country>
<doc-number>6973426</doc-number>
<kind>B1</kind>
<name>Schier et al.</name>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00058">
<document-id>
<country>US</country>
<doc-number>7003456</doc-number>
<kind>B2</kind>
<name>Gillick et al.</name>
<date>20060200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00059">
<document-id>
<country>US</country>
<doc-number>7006967</doc-number>
<kind>B1</kind>
<name>Kahn et al.</name>
<date>20060200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00060">
<document-id>
<country>US</country>
<doc-number>7010485</doc-number>
<kind>B1</kind>
<name>Baumgartner et al.</name>
<date>20060300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00061">
<document-id>
<country>US</country>
<doc-number>7024359</doc-number>
<kind>B2</kind>
<name>Chang et al.</name>
<date>20060400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00062">
<document-id>
<country>US</country>
<doc-number>7027773</doc-number>
<kind>B1</kind>
<name>McMillin</name>
<date>20060400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>455 412</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00063">
<document-id>
<country>US</country>
<doc-number>7092496</doc-number>
<kind>B1</kind>
<name>Maes et al.</name>
<date>20060800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00064">
<document-id>
<country>US</country>
<doc-number>7113572</doc-number>
<kind>B2</kind>
<name>Holmes</name>
<date>20060900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00065">
<document-id>
<country>US</country>
<doc-number>7170979</doc-number>
<kind>B1</kind>
<name>Byrne et al.</name>
<date>20070100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>379 8818</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00066">
<document-id>
<country>US</country>
<doc-number>7194752</doc-number>
<kind>B1</kind>
<name>Kenyon et al.</name>
<date>20070300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00067">
<document-id>
<country>US</country>
<doc-number>7236932</doc-number>
<kind>B1</kind>
<name>Grajski</name>
<date>20070600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00068">
<document-id>
<country>US</country>
<doc-number>7460654</doc-number>
<kind>B1</kind>
<name>Jenkins et al.</name>
<date>20081200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00069">
<document-id>
<country>US</country>
<doc-number>7562012</doc-number>
<kind>B1</kind>
<name>Wold et al.</name>
<date>20090700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00070">
<document-id>
<country>US</country>
<doc-number>7668710</doc-number>
<kind>B2</kind>
<name>Doyle</name>
<date>20100200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00071">
<document-id>
<country>US</country>
<doc-number>7672843</doc-number>
<kind>B2</kind>
<name>Srinivasan et al.</name>
<date>20100300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00072">
<document-id>
<country>US</country>
<doc-number>2002/0049590</doc-number>
<kind>A1</kind>
<name>Yoshino et al.</name>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00073">
<document-id>
<country>US</country>
<doc-number>2002/0112007</doc-number>
<kind>A1</kind>
<name>Wood et al.</name>
<date>20020800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00074">
<document-id>
<country>US</country>
<doc-number>2002/0152071</doc-number>
<kind>A1</kind>
<name>Chaiken et al.</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00075">
<document-id>
<country>US</country>
<doc-number>2002/0178003</doc-number>
<kind>A1</kind>
<name>Gehrke et al.</name>
<date>20021100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00076">
<document-id>
<country>US</country>
<doc-number>2002/0178004</doc-number>
<kind>A1</kind>
<name>Chang et al.</name>
<date>20021100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00077">
<document-id>
<country>US</country>
<doc-number>2003/0026392</doc-number>
<kind>A1</kind>
<name>Brown et al.</name>
<date>20030200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00078">
<document-id>
<country>US</country>
<doc-number>2003/0048881</doc-number>
<kind>A1</kind>
<name>Trajkovic</name>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00079">
<document-id>
<country>US</country>
<doc-number>2003/0115045</doc-number>
<kind>A1</kind>
<name>Harris et al.</name>
<date>20030600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00080">
<document-id>
<country>US</country>
<doc-number>2003/0120493</doc-number>
<kind>A1</kind>
<name>Gupta</name>
<date>20030600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00081">
<document-id>
<country>US</country>
<doc-number>2003/0128820</doc-number>
<kind>A1</kind>
<name>Hirschberg et al.</name>
<date>20030700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00082">
<document-id>
<country>US</country>
<doc-number>2003/0169857</doc-number>
<kind>A1</kind>
<name>Akhteruzzaman et al.</name>
<date>20030900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00083">
<document-id>
<country>US</country>
<doc-number>2003/0233231</doc-number>
<kind>A1</kind>
<name>Fellenstein et al.</name>
<date>20031200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>14</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>379 8801- 8828</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704270</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>19</number-of-drawing-sheets>
<number-of-figures>21</number-of-figures>
</figures>
<us-related-documents>
<division>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>10403350</doc-number>
<date>20030327</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>7330538</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>11680058</doc-number>
</document-id>
</child-doc>
</relation>
</division>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60368644</doc-number>
<date>20020328</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20070140440</doc-number>
<kind>A1</kind>
<date>20070621</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Dunsmuir</last-name>
<first-name>Martin R. M.</first-name>
<address>
<city>Waianae</city>
<state>HI</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Dunsmuir</last-name>
<first-name>Martin R. M.</first-name>
<address>
<city>Waianae</city>
<state>HI</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<last-name>Inouye</last-name>
<first-name>Patrick J. S.</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
<agent sequence="02" rep-type="attorney">
<addressbook>
<last-name>Wittman</last-name>
<first-name>Krista A.</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
<agent sequence="03" rep-type="attorney">
<addressbook>
<last-name>Kisselev</last-name>
<first-name>Leonid</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Intellisist, Inc.</orgname>
<role>02</role>
<address>
<city>Bellevue</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Nguyen</last-name>
<first-name>Duc</first-name>
<department>2651</department>
</primary-examiner>
<assistant-examiner>
<last-name>Mohammed</last-name>
<first-name>Assad</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A system and method for enabling two computer systems to communicate over an audio communications channel, such as a voice telephony connection. Such a system includes a software application that enables a user's computer to call, interrogate, download, and manage a voicemail account stored on a telephone company's computer, without human intervention. A voicemail retrieved from the telephone company's computer can be stored in a digital format on the user's computer. In such a format, the voicemail can be readily archived, or even distributed throughout a network, such as the Internet, in a digital form, such as an email attachment. Preferably a computationally efficient audio recognition algorithm is employed by the user's computer to respond to and navigate the automated audio menu of the telephone company's computer.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="196.09mm" wi="241.22mm" file="US08625752-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="187.03mm" wi="161.12mm" orientation="landscape" file="US08625752-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="249.26mm" wi="172.21mm" orientation="landscape" file="US08625752-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="227.25mm" wi="181.27mm" orientation="landscape" file="US08625752-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="245.28mm" wi="184.49mm" orientation="landscape" file="US08625752-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="254.34mm" wi="194.56mm" orientation="landscape" file="US08625752-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="226.82mm" wi="152.15mm" orientation="landscape" file="US08625752-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="236.56mm" wi="191.01mm" orientation="landscape" file="US08625752-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="228.52mm" wi="150.54mm" orientation="landscape" file="US08625752-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="154.77mm" wi="158.75mm" orientation="landscape" file="US08625752-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="242.06mm" wi="187.45mm" orientation="landscape" file="US08625752-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="210.31mm" wi="166.37mm" orientation="landscape" file="US08625752-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="227.92mm" wi="158.75mm" orientation="landscape" file="US08625752-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="222.33mm" wi="169.59mm" orientation="landscape" file="US08625752-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="242.65mm" wi="178.05mm" orientation="landscape" file="US08625752-20140107-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="250.19mm" wi="188.04mm" orientation="landscape" file="US08625752-20140107-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00016" num="00016">
<img id="EMI-D00016" he="255.69mm" wi="181.86mm" orientation="landscape" file="US08625752-20140107-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00017" num="00017">
<img id="EMI-D00017" he="153.75mm" wi="138.85mm" orientation="landscape" file="US08625752-20140107-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00018" num="00018">
<img id="EMI-D00018" he="216.15mm" wi="193.55mm" orientation="landscape" file="US08625752-20140107-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00019" num="00019">
<img id="EMI-D00019" he="250.87mm" wi="188.04mm" orientation="landscape" file="US08625752-20140107-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application is a divisional application based on prior copending patent application Ser. No. 10/403,350, filed on Mar. 27, 2003, which itself is based on a prior provisional application Ser. No. 60/368,644, filed on Mar. 28, 2002, the benefit of the filing dates of which are hereby claimed under 35 U.S.C. &#xa7;119(e) and 35 U.S.C. &#xa7;120.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">FIELD OF THE INVENTION</heading>
<p id="p-0003" num="0002">The present invention pertains to a method and apparatus that allows two computer systems to communicate over an audio communications channel, including enabling a two-way exchange of multimedia messages.</p>
<heading id="h-0003" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0004" num="0003">A voice response (VR) system allows a human user to listen to spoken information generated by a computer system. The user enters dual tone multi-frequency (DTMF) tones, or speaks commands, to navigate through the functions of such a VR system.</p>
<p id="p-0005" num="0004">The implementation of VR systems that respond to tones or spoken commands is well known, but these systems are designed with the assumption that humans will be providing the commands to a computer over a communication link. Furthermore, these systems are typically designed to use human speech in the form of stored audio files that are played over the telephone line in order to communicate with the outside world. Communication with VR systems is thus normally via an analog interface. U.S. Pat. Nos. 4,071,888 and 4,117,263 are representative of basic patents in the field of VR systems. Modern VR systems are largely similar to the centralized systems described in these patents.</p>
<p id="p-0006" num="0005">In contrast to VR systems, electronic mail (email) employs digital electronic signals for communications between users. Messages are encoded as numbers and sent from place to place over digital computer networks. Furthermore, email can be used to exchange voice messages in the form of digital audio files. However, the interface between email software systems and the underlying network is digital&#x2014;not analog.</p>
<p id="p-0007" num="0006">As a result of this analog-digital interface dichotomy, there is currently virtually no integration between voicemail and email. Since voicemail is the most common application of VR systems today, it is the best example. Accessing a voicemail system using a telephone handset, a user may listen to commands and send DTMF (Touchtone&#xae;) responses in order to listen to, save, forward, and delete their voicemail messages. However, commercial voicemail systems have a limited message capacity (both in time and space), and the lack of a digital interface in voicemail systems makes integration of voicemail with email and digital audio difficult. Not only is voicemail management using traditional dial-in systems cumbersome, it can be expensive, as cellular and mobile phone users must often incur the user peak-rate phone charges to access their voicemail. In addition, if the user has multiple telephones with voicemail accounts then each voicemail account must be checked with a separate phone call, and the user must manage each voicemail box separately. Voicemail is therefore a transient, untrustworthy, and cumbersome medium for communication.</p>
<p id="p-0008" num="0007">Note that email and voicemail systems both use a &#x201c;store &#x26; forward&#x201d; model for message delivery. It would thus be desirable to construct a bridge between them (allowing voicemail to reach the Internet and Internet audio messages to reach the phone system), which should enable a number of applications of great utility to be implemented. For example, if voicemail messages were available on a user's computer in digital form and freely available for distribution via email, then several advantages to users of voicemail systems would result. For example, such a system would enable the following benefits: (1) voicemail messages could be captured securely and permanently, just like any other type of computer file; (2) voicemail messages could be distributed and used wherever digital audio files are used, in particular, for transmission to remote locations via email (note the cost of retrieving email remotely is far lower than the long distance charges or peak roaming charges that may be incurred to make calls to voicemail); and, (3) because no direct connection is required to a modem, except at one location (the server), users would be able to receive voicemail on non-telephone devices, i.e., with the same devices used for email.</p>
<p id="p-0009" num="0008">The prior art identifies the value of integrating voicemail with computers and in particular, personal computers (PCs). U.S. Pat. No. 6,339,591, for example, describes a system for sending voicemail messages over the Internet, using proprietary methods (i.e., not email). The most likely configuration that might be used to integrate voicemail with the computer network would effect this integration at the centralized voicemail switch. In such a system, because voicemail messages are stored as digital audio files on the voicemail switch and because that switch is on the computer network, those voicemail messages might then be made available to computers on the network.</p>
<p id="p-0010" num="0009">U.S. Pat. No. 5,822,405 discloses a method of using a PC or other device equipped with a special modem to retrieve voicemail over a telephone line and store each message in a file on the computer; however, this patent makes no mention of digital distribution of the voicemail messages retrieved. This patent comes close to solving the central problem of interacting between a computer and a VR system, namely the need to use speech recognition in many cases, but room for improvement exists. For example, improvements can be made in the analysis of the audio signals received by a user's computer, and no utility is provided in this prior art patent for the digital distribution of the retrieved messages.</p>
<p id="p-0011" num="0010">Where voicemail messages are to be saved for later use in a conventional voicemail system, the voicemail messages are kept stored within the voicemail system. For example, U.S. Pat. Nos. 6,295,341; 4,327,251; 6,337,977; and 6,341,160 describe such systems. Even when computers are employed, the messages are generally kept in the answering device (as disclosed in U.S. Pat. No. 6,052,442). U.S. Pat. No. 6,335,963 even teaches that email be employed for notifying a user of voicemail, but not for delivery of the messages themselves.</p>
<p id="p-0012" num="0011">There is much use made of voice recognition in VR applications, but in almost all these applications, voice recognition is used by a computer to recognize the content of a human voice speaking on the telephone (e.g., as taught in U.S. Pat. Nos. 6,335,962; 6,330,308; 6,208,966; 5,822,405; and 4,060,694). Such human voice recognition techniques are computationally expensive. Readily available human voice recognition applications compare real-time spoken words against a stored dictionary. Because of variations in the human spoken word and variations in the quality of the communications channels, the comparison of a spoken word with a dictionary of words must take into account variations in both the length and the spectral characteristics of the human speech being recognized. Thus, solving the problem of human speech recognition in real-time consumes significant computational resources, which effectively limits the applications of human speech recognition used in conjunction with fast, relatively expensive, computers. Where non-standard audio recognition methods are used, they are typically restricted to narrow applications, as disclosed in U.S. Pat. Nos. 6,324,499; 6,321,194, and 6,327,345.</p>
<p id="p-0013" num="0012">It should be noted that VR systems often emulate (i.e., &#x201c;speak&#x201d;) the human voice, but do not produce it. Instead, they use stored audio files that are played over the telephone communication link. Therefore, the speech that these VR systems produce is identically spoken every time it is played. The recognition of repetitive identical audio signatures is, in fact, a much simpler problem to solve than the problem of recognizing actual spoken human voice produced by a variety of speakers. It would be preferable to provide a system employing such techniques for recognizing stored audio file speech, thereby enhancing computational performance and enabling less expensive processors to be employed.</p>
<p id="p-0014" num="0013">Another issue with conventional voice-recognition methods applied to VR applications is that the recognition of whole words and phrases can involve considerable latency. In VR applications, it is preferable to keep recognition latency to a minimum to avoid lost audio and poor response. Reduced processing overhead within the application will allow latency to be reduced within the recognition system.</p>
<p id="p-0015" num="0014">In the prior art, voice recognition is always proceeded by a learning step, where the recognizing computer system processes speech audio to build a recognizer library. Many VR and voice recognition inventions include such a learning process, which may be used to teach the computer what to say, what tones to send, or what words to recognize (e.g., as disclosed in U.S. Pat. Nos. 6,345,250; 6,341,264; and 5,822,405). It should be noted that in the prior art, when a system is learning words to be recognized, the learning method is independent of the context of the audio being learned. That is to say, the recognition method stands alone and can distinguish between a word being recognized and all other words (at least theoretically). It would thus be desirable to provide a computer-driven VR system wherein the learning method is simplified to take into account the invariant nature of the messages and the known context of their expression, to require fewer computational resources to be employed.</p>
<p id="p-0016" num="0015">Much prior art in the field of automatic control of VR systems with a computer depends upon the calling computer knowing the context of the VR system at all times. For example, the application described in U.S. Pat. No. 6,173,042 assumes that the VR system works identically every time, and that tones can be input to the VR system at any time. The prior art recognizes that the context of recognition is important (e.g., as disclosed in U.S. Pat. No. 6,345,254). It would be desirable to provide a programming language to describe VR interactions, which includes a syntax powerful enough to express such context in a general manner.</p>
<p id="p-0017" num="0016">Many VR control applications (such as described in U.S. Pat. No. 5,822,405) use some form of interpreted programming language to tell the application how to drive the remote VR system. In the prior art however, the scripting language is of a very restricted syntax, specific to its application (for example, voicemail retrieval). In order to build a general purpose VR response system, it would be helpful to have a programming language that is sufficiently powerful to address a wide range of VR applications (e.g., retrieval of stock quotes, airline times, or data from an online banking application).</p>
<p id="p-0018" num="0017">Another aspect of the learning process that can have a major impact on its efficiency is the user interface (UI). A UI that is too generalized may result in complex manipulations of the interface being required to achieve full control of the learning process. Such a situation arises often when the learning portion of an invention's embodiment is performed with a general purpose tool, as is in U.S. Pat. No. 5,822,405. It would be desirable to provide a computer-driven VR system, wherein the UI is specifically adapted to enable easy navigation and control of all of the aspects of the VR system, including any learning method required.</p>
<p id="p-0019" num="0018">A different issue with conventional voice recognition methods applied to VR applications, is that the recognition of whole words and phrases can involve considerable latency. It would be desirable to provide a computer-driven VR system, wherein recognition latency is kept to a minimum to avoid lost audio content and poor response.</p>
<p id="p-0020" num="0019">When designing a VR control application (such as described in U.S. Pat. No. 5,822,405) it may be necessary to develop some form of interpreted programming language, to tell the application how to drive the remote VR system. In the prior art, however, the scripting language is of a very restricted syntax, specific to its application (for example, voicemail retrieval). In order to build a general purpose VR response system, it would be desirable to employ a programming language that is sufficiently powerful and more general in nature to address a wide range of VR applications (e.g., retrieval of stock quotes, airline times, or for accessing data in an online banking application). If a bridge such as that noted above can be built between voicemail and the Internet, it would make voicemail as easy to review, author, and send, as email. Voicemail, originating in the telephone system, might be integrated directly with messages created entirely on the Internet using an audio messaging application.</p>
<p id="p-0021" num="0020">Many integrated messaging systems have been built. These systems seek to integrate some combination of voicemail, text messaging, and email into one interface. However, the prior art with respect to unified messaging (UM) is exclusively concerned with creating a closed universe within which the system operates. Such systems, although at times elegant, do not cater to users who have a need to access voicemail from different voicemail systems (such as from home and from work), through an Internet connection. For example, U.S. Pat. No. 6,263,052 archives the voice messages within the voicemail system. It would be desirable to enable the voicemail messages to be available on the computer network, thereby enabling a user to reply to those messages offline, and to forward the reply to the original caller using email, or to make a voicemail response that is delivered by the computer system. If integrated messaging systems could interface directly with any VR system over the public service telephone network (PSTN), then UM would become easier to apply, and would also become more useful.</p>
<p id="p-0022" num="0021">Often after voicemail messages are received, a user will wish to reply to such messages. It is convenient for the user to be able to reply to the voicemail at their leisure, and have the reply forwarded to the original sender as another voicemail. Such a system is described in U.S. Pat. No. 6,263,052.</p>
<p id="p-0023" num="0022">In the prior art it is assumed that if two computers are to communicate with each other they will do so using some form of digital encoding, and that if they are using a telephone line to communicate they will modulate a signal on that line with an audio signal that follows the structure of the digital sequence they wish to communicate. U.S. Pat. Nos. 4,196,311 and 3,937,889 are exemplary of such art. On the other hand, humans communicate with each other over the telephone using analog, not digital, communications. However, if two computer systems, each equipped with voice recognition and the ability to communicate using analog voice communications, were placed in communication with each other in a peer-to-peer configuration, a useful form of two-way communication might result. If the recognition of audio from one computer can drive a program on the other computer, which can in turn send audio responses to the first computer, then secure encoded communications might be effected by use of a normal telephone voice call.</p>
<p id="p-0024" num="0023">Clearly, it would be desirable to provide a software system, running on a suitably equipped computer, which can be flexibly programmed and easily taught to navigate a VR system using audio signature recognition and which can download chosen audio segments to the computer system as digital audio files. Such a system will preferably enable the automatic scheduled retrieval of audio files from the VR system and enable these files to be automatically forwarded via email to the intended recipient, over the Internet.</p>
<p id="p-0025" num="0024">It would further be desirable for digital audio files to be played over the telephone system and to leave voicemail messages that can be played directly by the recipient. Yet another desirable feature of such a system would be the use of computationally efficient waveform recognition algorithms to maximize the number of telephone lines that can be simultaneously supported by one computer.</p>
<p id="p-0026" num="0025">It would still be further desirable to provide flexible interfaces, functions, and programming language to enable general purpose applications to interface with the VR retrieval and forwarding system. Such a system would automatically recognize duplicate audio files (i.e., files which have been downloaded twice from the same VR system), and provide means for the user to prepare digital audio files as replies to received messages, or as new voice messages, and to have those digital audio files delivered via email or over the phone line, to the intended recipient.</p>
<p id="p-0027" num="0026">Further desirable features of such a system would include means for teaching the software to recognize new audio signatures and to incorporate them into a program script, and such learning processes should be enabled both locally (at a computer with a modem), and remotely (by employing a computer and a modem receiving commands via email from a remote computer). It would further be desirable to provide a system that enables two computers to communicate over an audio communications channel, to achieve an audio encoded computer-to-computer communications system.</p>
<heading id="h-0004" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0028" num="0027">The present invention is directed to a system and method for enabling two computer systems to communicate over an audio communications channel, such as a voice telephony connection. Another aspect of the invention is directed to an Internet and telephony service utilizing the method of the present invention.</p>
<p id="p-0029" num="0028">One of a number of preferred embodiments of this invention is directed to the use of a VR management application to automate interaction with a VR system. In a preferred implementation, the VR management application resides on a server, and multiple users can access the VR management application remotely. Users teach the VR management application how to access one or more VR systems associated with each of the users. For each audio command prompt likely to be issued by the VR system, the VR management application learns to recognize the audio command prompt, and how to respond to that audio command prompt. A user can then instruct the VR management application to automatically interact with the VR system to achieve a result, based upon a desired level of interaction. In a preferred embodiment, the interaction includes retrieving the user's voicemail. The VR management application will establish a logical connection with the VR system, receive audio communications from the VR system, and compare each communication with the audio command prompts that were previously learned. The VR management application provides the appropriate responses and receives additional audio communications, until a desired level of interaction is achieved. When the desired level of interaction is retrieving voicemail, a user is preferably enabled to receive such voicemail either via email, via a network location, or via a telephone.</p>
<p id="p-0030" num="0029">In a preferred embodiment, the learning process includes generating a discrete Fourier transform (DFT) based on at least a portion of each audio command prompt to be learned. When the VR management application automatically interacts with a VR system, at least one DFT will be generated, based on the audio communication received from the VR system. Each learned DFT will be compared with the newly generated DFT to recognize the command prompt corresponding to the audio received.</p>
<p id="p-0031" num="0030">Another aspect of the present invention is a computationally efficient method of recognizing an audio signal. The method requires that a plurality of known DFTs be provided, each known DFT corresponding to a specific audio signal. At least one unknown DFT is generated for each audio signal to be recognized. The at least one unknown DFT is compared to each known DFT, and a match with a known DFT enables the audio signal to be identified.</p>
<p id="p-0032" num="0031">Preferably, the audio signal to be identified is stored in an audio buffer, and the audio buffer is separated into a plurality of equally-sized sample buffers. Then, an unknown DFT is generated for each sample buffer. Each unknown DFT is compared to each known DFT. When an audio signal is processed to produce a plurality of unknown DFTs, one or more of a plurality of DFTs generated from a known audio signal is selected to be used as the known DFT for that audio signal.</p>
<p id="p-0033" num="0032">Another aspect of the invention is directed to a method for using a computing device to interact with a VR system. In at least one embodiment, the VR system is an audio message service, and the interaction is managing a user's voicemail account, including retrieving audio messages from the remote audio message service. While not limited to use with VR systems that comprise an audio message service, when so employed, the method includes the steps of first establishing a logical connection between the computing device and the audio message service. Then a communication is received from the audio message service. In response, the computing device generates at least one unknown DFT based on the communication. The at least one unknown DFT is compared with at least one known DFT. Each known DFT corresponds to a command prompt that is likely to be received from the message service. If an acceptable level of correlation exists between the at least one unknown DFT and a known DFT, then the computing device provides the message service with the appropriate response to the command prompt identified by matching the at least one DFT to the known DFT. The steps of receiving a communication, generating unknown DFTs, matching unknown DFTs to known DFTs, and providing a correct response to the message service are repeated until the communication from the message service indicates that the next communication will be an audio message, rather than a command prompt. The messages stored by the message service are then retrieved.</p>
<p id="p-0034" num="0033">The logical connection is preferably a telephonic connection. Once the messages are retrieved, the computing device optionally provides the message service with the appropriate response required to instruct the message service to delete each message after it has been received by the computing device. In one related embodiment, instead of causing the message service to delete retrieved messages, the computing device generates a key for each message received from the message service, so that during a future message retrieval operation, the computing device can ignore already received messages that have not been deleted from the message service. Preferably, the keys are produced by generating a DFT of the message, and encoding the DFT to generate a unique key that is stored using relatively few bytes. Then, before retrieving a message, the computing device generates a key for an incoming message and checks the key for the incoming message against stored keys. If the key for the incoming message is the same as a stored key, the incoming message is ignored, since it was previously retrieved.</p>
<p id="p-0035" num="0034">Preferably, before the logical connection is established to retrieve messages stored by the message service, the computing device is taught how to recognize and respond to each command prompt likely to be received from the message service. To teach the computing device how to recognize and respond to each command prompt likely to be encountered, a logical connection is first established between the computing device and the message service. A command prompt is received from the message service, and at least one DFT based on the command prompt is generated. A user provides the correct response to the command prompt, and the computing device stores the correct response, as well as the DFT corresponding to the command prompt. Preferably, the correct response is stored as a program script that enables the computing device to duplicate the correct response for the DFT. The program script and DFT corresponding to that command prompt are stored in a memory accessible to the computing device. These steps are repeated for each command prompt likely to be encountered.</p>
<p id="p-0036" num="0035">To enhance the method of retrieving an audio message described above, preferably each communication received from the message service is stored in at least one audio buffer. Then, each audio buffer is separated into a plurality of window buffers. A DFT is generated for each window buffer. Each window buffer DFT is then compared with each known DFT.</p>
<p id="p-0037" num="0036">In one preferred embodiment two different, identically-sized audio buffers are used. Each audio buffer is sized to accommodate N samples, N having been selected to reflect a desired time resolution. Each audio buffer is sequentially filled with N samples of the communication, such that a first audio buffer is filled with older samples, and a second audio buffer is filled with newer samples. A plurality of window buffers are generated by segregating each audio buffer of size N into identically sized sample windows of size W, such that each sample window includes a whole number of samples, and such that N is both a whole number and a multiple of W. The next step involves iteratively generating window buffers of size N using the sample windows of size W, such that each window buffer includes multiple sample windows (totaling N samples), and each sequential window buffer includes one sample window (of size W) not present in the preceding window buffer.</p>
<p id="p-0038" num="0037">Preferably, any messages that are retrieved are stored in a digital format. Once in a digital format, the messages can be forwarded to a user's email address. It is also preferred to enable the user to access any stored message at a networked location. A preferred digital format is the MP3 file format, but other formats might alternatively be used.</p>
<p id="p-0039" num="0038">It is contemplated that the computing device will be programmed to establish a connection with a message facility according to a predefined schedule, so that messages are retrieved on a defined reoccurring basis.</p>
<p id="p-0040" num="0039">Still another aspect of the present invention is directed to a method of training a computing device to automatically interact with a VR system, where successful interaction requires providing a proper audio response to audio prompts issued by the VR system. While not limited to VR systems such as voicemail services, one preferred embodiment is directed to training a computing device to automatically manage a voicemail account, including retrieving, saving, and deleting messages. Steps of the method include launching a message retrieval application on the computing device, and then establishing a logical connection between the computing device and the remote message facility using either a telephonic connection or a network connection. Further steps include receiving a communication from the remote message facility, and then capturing a command prompt from the remote message facility in an audio buffer. A correct response to the audio command prompt (such as DTMF tone sequence or a audio message) is required to navigate a menu associated with the remote message facility to retrieve the desired messages. A user is enabled to provide the correct response, which is stored in a memory of the computing device. Additional steps include generating at least one DFT based on at least a portion of the audio buffer, the at least one DFT identifying the command prompt and thereby enabling the computing device to automatically recognize the command prompt during a subsequent automated message retrieval operation. A program script is generated for execution by the computing device, to duplicate the correct response. The final step requires storing the at least one DFT and the program script in a memory accessible by the computing device, such that the at least one DFT and program script enable the computing device to automatically recognize the command prompt and duplicate the correct response to the command prompt during a subsequent automated message retrieval operation.</p>
<p id="p-0041" num="0040">Preferably, the steps are repeated so that at least one DFT and a program script are generated for each different command prompt likely to be encountered when navigating a menu associated with the remote message facility. The computing device then automatically recognizes all command prompts likely to be issued by the remote message facility, and duplicates the correct response for each such command prompt during a subsequent automated message retrieval operation.</p>
<p id="p-0042" num="0041">It is further preferred that the contents of the audio buffer be separated into a plurality of equally sized sample buffers before generating the at least one DFT. The step of generating the at least one DFT preferably includes generating a plurality of sample DFTs, one for each sample buffer.</p>
<p id="p-0043" num="0042">Still another aspect of the present invention is directed to a method for enabling two computing devices to communicate using audio signals. Each computing device is provided a plurality of known DFTs that each corresponds to a specific audio signal. When a first of the two computing devices receives an input signal, the input signal is processed to perform one of the following functions. If the input signal is not an audio signal, then the input signal is converted into an audio signal, such that the audio signal thus generated corresponds to an audio signal whose DFT is stored in the memory of each computing device; the audio signal is then transmitted to the second of the two computing devices. If the input signal is already an audio signal but there is no known DFT corresponding to that input signal, then the input signal is separated into a plurality of audio signals such that each of the plurality of audio signals corresponds to an audio signal whose DFT is stored in the memory of each computing device, and each audio signal is transmitted to the second computing device. If the input signal is already an audio signal and there is a known DFT corresponding to that input signal, then that audio signal is transmitted to the second computing device. The second computing device processes each audio signal it receives by generating an unknown DFT based on an audio signal received, comparing the unknown DFT generated from the audio signal received with each known DFT, and identifying the audio signal received to reconstruct the input signal. The second computing device can then respond to the first computing device in the same manner.</p>
<p id="p-0044" num="0043">Still another aspect of the present invention is directed to a method for enabling a user to retrieve a digital copy of an audio message from a network location, when the audio message has been left at an audio message facility. The audio message facility provides audio command prompts to which appropriate responses must be made in order to successfully navigate through the audio message facility to retrieve any audio messages. The method involves the steps of establishing a logical connection between the user and the network location, and enabling the user to teach the network location how to recognize and respond to the audio command prompts issued by each audio message facility utilized by the user. The recognition is based on a comparison of a DFT of an audio command prompt with stored DFTs corresponding to each command prompt likely to be issued by each audio message facility utilized by the user. The method further involves enabling the user to instruct the network location to retrieve audio messages from at least one audio message facility utilized by the user. For each audio message facility utilized by the user from which the network location has been instructed to retrieve messages, the following steps are performed. A logical connection between the network location and the message facility is established to receive an audio signal from the message facility. An unknown DFT is generated based on the audio signal received. The unknown DFT generated from the audio signal received is compared with each known DFT to identify the command prompt being issued by the message facility, and the correct response to the command prompt is provided. These steps are repeated until access to messages stored by the message facility is granted. The messages are retrieved and converted into a digital format, so that the user is able to access the messages in the digital format.</p>
<p id="p-0045" num="0044">Other aspects of the present invention are directed to a system for executing steps generally consistent with the steps of the methods described above and to articles of manufacture intended to be used with computing devices, which include a memory medium storing machine instructions. The machine instructions define a computer program that when executed by a processor, cause the processor to perform functions generally consistent with the method steps described above.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWING FIGURES</heading>
<p id="p-0046" num="0045">The foregoing aspects and many of the attendant advantages of this invention will become more readily appreciated as the same becomes better understood by reference to the following detailed description, when taken in conjunction with the accompanying drawings, wherein:</p>
<p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. 1A</figref> is a schematic block diagram illustrating a computer that is using the present invention and is in communication with a VR system, such as a voicemail system, over an audio telephony connection;</p>
<p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. 1B</figref> is a schematic diagram showing an online service that employs the present invention;</p>
<p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. 2</figref> is a schematic block diagram illustrating two computers that are using the present invention to communicate with each other over an audio communications channel;</p>
<p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. 3</figref> is a schematic diagram of a computer connected to the Internet and using the present invention to communicate with a VR system located at a telephone company's central office, over the public telephone system;</p>
<p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. 4</figref> is a schematic block diagram illustrating the overall structure of a preferred embodiment of the present invention;</p>
<p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. 5</figref> is a schematic diagram illustrating the overall flow for the software employed in a preferred embodiment of the present invention;</p>
<p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. 6</figref> is a schematic block diagram showing the main recognition and action loop of the software implemented in a preferred embodiment of the present invention;</p>
<p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. 7</figref> is a flowchart illustrating the logic for the processing and display of newly arrived voicemail messages in a preferred embodiment of the present invention;</p>
<p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. 8</figref> is a schematic block diagram showing the manner in which message keys (generated for voicemail messages on arrival) are used to identify the same message if it is retrieved again;</p>
<p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. 9</figref> is a flowchart showing the steps used for configuring the software employed in the present invention to recognize a new audio phrase;</p>
<p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. 10</figref> is schematic diagram illustrating the process employed for generating a signature file from captured audio sequences in accord with a preferred embodiment of the present invention;</p>
<p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. 11</figref> is a schematic diagram that illustrates how an audio messenger application routes voice messages to an intended destination;</p>
<p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. 12</figref> is a screenshot of the portion of the graphical user interface (GUI) used in a preferred embodiment of the present invention, to allow the user to adjust new phrases during the creation of signature files;</p>
<p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. 13</figref> is a schematic flowchart of the interactions between two computers using the invention, wherein it is possible for the two computers using the invention to configure the recognition of audio messages generated by a third computer and learn the appropriate actions associated with them, with the first computer having no real-time access to a modem;</p>
<p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. 14</figref> is a flowchart showing the logic implemented by two computers using the present invention to communicate textual information when employing the human voice as an encoding medium;</p>
<p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. 15</figref> is a schematic diagram showing the manner in which incoming audio is compared to stored signatures during phrase recognition;</p>
<p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. 16</figref> is a block diagram of an exemplary computing device that can be used to implement the present invention;</p>
<p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. 17</figref> is a schematic diagram showing how overlapping audio buffers are used in determining the best signature block during signature creation;</p>
<p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. 18</figref> illustrates an exemplary GUI of an audio messenger application employed in a preferred embodiment of the present invention;</p>
<p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. 19</figref> is a flow diagram showing the logic for composing and sending a message with the audio messenger application; and</p>
<p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. 20</figref> is an exemplary embodiment of a Web page for a Voice-Messaging Web site (&#x201c;http://mygotvoice.com&#x201d;), used in conjunction with the audio messenger application, in accord with a preferred embodiment of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0006" level="1">DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading>
<p id="h-0007" num="0000">General Overview</p>
<p id="p-0068" num="0067">In <figref idref="DRAWINGS">FIG. 1A</figref>, a first computer system is a VR system <b>104</b>, which answers telephone calls, generates audio messages <b>106</b> and receives and acts upon a response <b>110</b> (DTMF or audio) from a caller. A voicemail system or a 411 information service are examples of VR system <b>104</b>. A second computer system <b>102</b> makes calls to VR system <b>104</b> and uses a signal processing technique to recognize the audio signals (i.e., phrases) that are issued by VR system <b>104</b>. Particularly when VR system <b>104</b> is a voicemail system, audio messages <b>106</b> are command prompts that require a specific response. System <b>102</b> sends response <b>110</b>, either as voice-band audio or as tones, in response to command prompts from VR system <b>104</b>, to establish control of the remote VR system. System <b>102</b> is controlled by a recognition program <b>108</b> specifically adapted to interact with VR system <b>104</b>. The recognition program can instruct system <b>102</b> to call, interrogate, download, and manage a voicemail account residing at VR system <b>104</b>, without human intervention. It should be understood that management of a voicemail account is not limited to merely retrieving messages, but encompasses normal voicemail management functionality, including message retrieval, message deletion, and message storage (e.g., storing messages as &#x201c;new&#x201d; messages).</p>
<p id="p-0069" num="0068"><figref idref="DRAWINGS">FIG. 1B</figref> illustrates an Internet-based online service that utilizes the present invention in providing online access to voicemail messages. A service center <b>141</b> houses computers that interface with the outside world both over Internet connections <b>121</b>, <b>124</b>, <b>127</b>, and <b>162</b>, and over public switched telephone network (PSTN) connections <b>132</b>, <b>133</b>, <b>134</b> and <b>135</b>. Note that logical connections <b>150</b>, <b>152</b>, <b>154</b>, <b>156</b>, <b>158</b> and <b>164</b> couple different elements of the service center <b>141</b> together. Typically such logical connections are implemented as network connections, coupling different computing devices together. Note that some functional elements of service center <b>141</b>, such as Web Interface <b>122</b> and inbound email gateway <b>125</b> could be implemented as a single computing device</p>
<p id="p-0070" num="0069">A spooling computer system <b>144</b> provides a bridge between the Internet and the PSTN, over which messages can flow in both directions, based on the method described in conjunction with <figref idref="DRAWINGS">FIG. 1B</figref>. The Service supports online access to the user's messages via a conventional Web browser application <b>120</b> (such as those executed on a PC, or a portable computing device), and/or a streaming media player <b>142</b>. Users may also receive messages using an email application <b>126</b> via an Internet connection <b>127</b> or via a dialup VR interface <b>140</b> using a PSTN connection <b>135</b> and a standard telephone handset <b>139</b>. In addition, new audio messages can be composed on a computer device equipped with a microphone <b>143</b> and an audio messenger application <b>123</b>. These messages are sent via email to an inbound email gateway <b>125</b> using internet connection <b>124</b>. From email gateway <b>125</b>, the messages can be directed to one or more of a Message Store <b>128</b> of an existing user, a VR system <b>137</b> (i.e., a VR based voicemail system) that of the user (using a PSTN connection <b>133</b>), or to a telephone <b>36</b> associated with the user (such as a cellular telephone, a mobile telephone, or a land line using a PSTN connection <b>132</b>).</p>
<p id="p-0071" num="0070"><figref idref="DRAWINGS">FIG. 2</figref> illustrates a second and related embodiment in which both computer systems <b>202</b> and <b>204</b> are capable of audio pattern recognition and audio response generation. In this case, these two computer systems can conduct an audio conversation with each other, in accord with their own individual recognition programs <b>210</b>A and <b>210</b>B. First computer system <b>202</b> sends audio messages <b>206</b>A and <b>206</b>B to computer system <b>204</b>, which recognizes them and sends its own audio responses <b>208</b>A and <b>208</b>B to computer system <b>202</b>. Both systems are controlled by respective programs <b>210</b>A and <b>210</b>B in accord with the present invention. The present invention, in its various embodiments, has applications in both civilian and military computer communications.</p>
<p id="h-0008" num="0000">Exemplary Computing Environment</p>
<p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. 16</figref>, and the following related discussion, are intended to provide a brief, general description of a suitable computing environment for practicing the present invention. In a preferred embodiment of the present invention, an audio recognition application is executed on a PC. Those skilled in the art will appreciate that the present invention may be practiced with other computing devices, including a laptop and other portable computers, multiprocessor systems, networked computers, mainframe computers, hand-held computers, personal data assistants (PDAs), and on devices that include a processor, a memory, and a display. An exemplary computing system <b>330</b> that is suitable for implementing the present invention includes a processing unit <b>332</b> that is functionally coupled to an input device <b>320</b>, and an output device <b>322</b>, e.g., a display. Processing unit <b>332</b> includes a central processing unit (CPU) <b>334</b> that executes machine instructions comprising an audio recognition application (that in at least some embodiments includes voicemail retrieval functionality) and the machine instructions for implementing the additional functions that are described herein. Those of ordinary skill in the art will recognize that CPUs suitable for this purpose are available from Intel Corporation, AMD Corporation, Motorola Corporation, and other sources.</p>
<p id="p-0073" num="0072">Also included in processing unit <b>332</b> are a random access memory (RAM) <b>336</b> and non-volatile memory <b>338</b>, which typically includes read only memory (ROM) and some form of memory storage, such as a hard drive, optical drive, etc. These memory devices are bi-directionally coupled to CPU <b>334</b>. Such storage devices are well known in the art. Machine instructions and data are temporarily loaded into RAM <b>336</b> from non-volatile memory <b>338</b>. As will be described in more detail below, included among the stored data are data sets corresponding to known audio signals, and program scripts that are to be executed upon the identification of a specific audio signal. Also stored in memory are operating system software and ancillary software. While not separately shown, it should be understood that a power supply is required to provide the electrical power needed to energize computing system <b>330</b>.</p>
<p id="p-0074" num="0073">Preferably, computing system <b>330</b> includes a modem <b>335</b> and speakers <b>337</b>. While these components are not strictly required in a functional computing system, their inclusion facilitates use of computing system <b>330</b> in connection with implementing many of the features of the present invention, and the present invention will generally require a modem (conventional, digital subscriber line (xDSL), or cable) or other form of interconnectivity to a network, such as the Internet. As shown, modem <b>335</b> and speakers <b>337</b> are components that are internal to processing unit <b>332</b>; however, such units can be, and often are, provided as external peripheral devices.</p>
<p id="p-0075" num="0074">Input device <b>320</b> can be any device or mechanism that enables input to the operating environment executed by the CPU. Such an input device(s) include, but are not limited to a mouse, keyboard, microphone, pointing device, or touchpad. Although, in a preferred embodiment, human interaction with input device <b>320</b> is necessary, it is contemplated that the present invention can be modified to receive input electronically, or in response to physical, molecular, or organic processes, or in response to interaction with an external system. Output device <b>322</b> generally includes any device that produces output information perceptible to a user, but will most typically comprise a monitor or computer display designed for human perception of output. However, it is contemplated that present invention can be modified so that the system's output is an electronic signal, or adapted to interact with mechanical, molecular, or organic processes, or external systems. Accordingly, the conventional computer keyboard and computer display of the preferred embodiments should be considered as exemplary, rather than as limiting in regard to the scope of the present invention.</p>
<p id="p-0076" num="0075">In <figref idref="DRAWINGS">FIG. 3</figref>, a telephone communications path exists between a PC <b>302</b> (such as a PC disposed in a user's home or work place, or spooling computer system <b>144</b> of <figref idref="DRAWINGS">FIG. 1B</figref>), and a voicemail server <b>304</b> (likely disposed at a telephone company's facility). A first portion of the communications path is an analog telephone line <b>308</b> carrying an analog audio signal, which couples voicemail server <b>304</b> to a modem <b>312</b>. A second portion of the communications path is a digital data cable <b>314</b> (such as a universal serial bus (USB) cable, a serial port cable, an IEEE 1394 data cable, a parallel port cable, or other suitable data cable) carrying a digital signal from modem <b>312</b> to PC <b>302</b>. Thus, at PC <b>302</b>, digitized incoming audio packets are available in real-time for use by applications running on PC <b>302</b>. Furthermore, applications running on PC <b>302</b> can output digital audio signal via digital data cable <b>314</b> to modem <b>312</b>, which then generates an analog audio signal to be transmitted over analog telephone line <b>308</b>. Note that a modem, which enables the passage of digitized audio between it and the host computer system, is commonly referred to as a &#x201c;voice modem.&#x201d;</p>
<p id="p-0077" num="0076">At the telephone company, the telephone line terminates at a line card installed in a telephone switch <b>306</b>. Digitized audio is then sent to and received from the line card and the voicemail server <b>304</b>. Any DTMF sequences generated by modem <b>312</b> or PC <b>302</b> are recognized by switch <b>306</b> and passed as digital messages over a computer network <b>310</b> to voicemail server <b>304</b>. In response to any commands encoded in the DTMF sequences, voicemail server <b>304</b> passes digitized audio messages to telephone switch <b>306</b>, where the digitized audio messages are turned back into analog audio for delivery over the telephone line, back to the caller.</p>
<p id="p-0078" num="0077">One preferred embodiment of the present invention is implemented in a software application that runs on PC <b>302</b>. Hereafter, this application will be referred to as the &#x201c;voice server.&#x201d; The voice server application makes calls over telephone voice circuits to voicemail server <b>304</b> to retrieve any voicemail for the user. Such a connection is made periodically (i.e., according to a predefined schedule), on demand, or both (as required or selectively initiated by a user). Once the connection is made, the audio (i.e., one or more spoken messages) output by voicemail server <b>304</b> is passed to the application running on PC <b>302</b>. The voice server application compares the incoming audio with a dictionary of phrases it holds in encoded form. If a phrase is recognized, the calling computer executes a script that can take certain predefined actions, such as sending a command to the voicemail system as a DTMF command, or hanging up. In the preferred embodiment the calling computer executes a script that downloads and captures the user's voicemail from a voicemail switch. Once downloaded, each voicemail message is available as a compressed digital audio file in the popular MP3 format. This file can be sent by email or be otherwise distributed electronically via a data connection <b>318</b> to a network <b>316</b> such as the Internet. Message files can also be carried with the user by being stored in the memory of a personal device such as a PDA or mobile telephone. Preferably, the voice server application has a GUI that allows the user to easily fetch, review, manage, and manipulate his voicemail messages, as if they were email messages.</p>
<p id="p-0079" num="0078">In addition to the voice server, a preferred implementation of the present invention includes two other elements; the &#x201c;service,&#x201d; which is an Internet service built around the voice server, and the &#x201c;audio messenger,&#x201d; which is an Internet client application.</p>
<p id="p-0080" num="0079">The service portion of the preferred embodiment is schematically illustrated in <figref idref="DRAWINGS">FIG. 1B</figref>. The service enables multiple users to share access to a small number of voice servers comprising a spooling computer system <b>144</b>. A service center <b>141</b> preferably includes a minimum of two computers. One computer, which in a preferred embodiment executes a Linux&#x2122; operating system, implements a message store <b>128</b>, a Web Interface <b>122</b> (by which users are enabled to gain access to their messages), and a backend telephone voicemail retrieval system <b>140</b>. In addition, the Linux&#x2122; operating system acts as an email gateway <b>125</b> for communicating with other applications, such as an email client <b>126</b>, or an audio messaging application <b>123</b> (residing on computer a computing device). In the following discussion, a preferred embodiment of audio messaging application <b>123</b> is referred to as the audio messenger. One or more additional computers are attached to the telephone system via voice modems and are connected to the computer running the Linux&#x2122; operating system over a LAN (see spooling computer system <b>144</b>). These computers implement the voice server functions of sending and retrieving voicemail messages over the telephone. Note that voice server <b>129</b> (sending function) and voice server <b>130</b> (retrieving function) can each be implemented on one or more individual computers, such that spooling computer system <b>144</b> includes one or more computers dedicated to the sending function, and one or more computers dedicated to the retrieving function. Of course, voice server <b>129</b> and voice server <b>130</b> can be implemented on a single computer, such that spooling computer system <b>144</b> is a single computer. Preferably, spooling computer system <b>144</b> executes a version of Microsoft Corporation Windows&#x2122; operating system. Those of ordinary skill in the art will recognize that the selection of a specific operating system is largely an element of preference, and that other operating systems, such as the Linux&#x2122; operating system, could be employed.</p>
<p id="p-0081" num="0080">The audio messenger portion in one preferred embodiment is shown in <figref idref="DRAWINGS">FIG. 1B</figref>, as audio messaging application <b>123</b> that is executed on the computing device. In an exemplary implementation of the present invention, the audio messenger is a small Windows&#x2122; application, which enables a user to record voice messages and send them directly into service <b>141</b> via email gateway <b>125</b>. An exemplary implementation of the GUI of the audio messenger is illustrated in <figref idref="DRAWINGS">FIG. 18</figref>. The audio messenger application may be replaced with a third party application, as long as such third party application is properly configured to communicate with email gateway <b>125</b>.</p>
<p id="p-0082" num="0081">An exemplary voice server application has been implemented as a software application running on a general purpose computer equipped with a voice modem connected to an analog telephone line. The exemplary voice server application is written in the popular C++ programming language and is designed to be portable. A beta version currently runs under both Microsoft Corporation's Windows&#x2122; and the Linux&#x2122; operating system.</p>
<p id="h-0009" num="0000">Structural Overview of a Preferred Embodiment of an Application</p>
<p id="p-0083" num="0082"><figref idref="DRAWINGS">FIG. 4</figref> shows the overall structure of the preferred voice server application. The software runs on the PC and interfaces with the outside world through a GUI <b>402</b>. A call control function <b>436</b> interfaces with a telephone service via a PSTN service interface <b>440</b>. The underlying implementation of this interface is normally provided by the modem manufacturer. The voice server application also makes use of other TCP/IP network services, such as domain name system (DNS) resolution, which are implemented by the underlying operating system.</p>
<p id="p-0084" num="0083">GUI <b>402</b> provides a user with functions to control and manage the application. <figref idref="DRAWINGS">FIG. 4</figref> shows the major functions supported by the GUI. These are: message management <b>410</b>; message playback, reply, and forwarding <b>412</b> (referred to hereafter simply as message playback <b>412</b>); local application configuration <b>414</b>; voicemail host configuration <b>416</b>; call scheduling <b>418</b>; and manual calling <b>420</b>. Commands to the application can be executed through the GUI <b>402</b> or they can arrive as email messages containing remote commands. These commands are processed by a remote commands processor <b>422</b>.</p>
<p id="p-0085" num="0084">Remote commands processor <b>422</b> communicates with the outside world via a job spooling directory <b>426</b>, into which command requests are placed by one or more other applications. In one preferred embodiment of the present invention, the service portion (described above in conjunction with <figref idref="DRAWINGS">FIG. 1B</figref>), uses spooling directory <b>426</b> and also accesses incoming messages, from within a message store <b>424</b>. The remote command processor enables the voice server application to be controlled and configured remotely.</p>
<p id="p-0086" num="0085">Other core functions within the voice server application, as shown in <figref idref="DRAWINGS">FIG. 4</figref>, include a scheduling engine <b>428</b>, and a host manager <b>430</b>. A voicemail retrieval function <b>432</b> uses call control function <b>436</b> to make, manage, and terminate telephone calls. Call control function <b>436</b> employs telephone PSTN service interface <b>440</b> to make telephone calls over the voice modem. The recognition of incoming audio is performed by a recognition engine <b>434</b>, which utilizes a host library <b>438</b>. The generation of the host library is described below. Messages may be heard utilizing a PC audio output, connected to a speaker <b>444</b>.</p>
<p id="h-0010" num="0000">Description of Main Software Loop</p>
<p id="p-0087" num="0086"><figref idref="DRAWINGS">FIG. 5</figref> shows a flow diagram for the main software loop of the voice server application. When the program starts at a block <b>518</b>, it first checks to see that a compatible voice modem is installed and operational in the host computer as indicated by a decision block <b>520</b>. If there is no modem, the voice server software disables all functions within the software that require a modem, as indicated in a block <b>522</b>. This step enables a subset of manual operations to be performed locally, and control passes directly to the main command loop at a block <b>528</b>.</p>
<p id="p-0088" num="0087">If a modem is present, the voice server software starts the call scheduler. This step involves loading a schedule in a block <b>524</b>, which is retrieved from a file location, as indicated by a block <b>525</b>. The voice server application starts a timer at a block <b>526</b>. The timer causes a schedule cycle to be executed when a predefined interval expires (the timer value determines the granularity of scheduling), at a block <b>532</b>. Typically the scheduler runs every few seconds, e.g., every 15 seconds.</p>
<p id="p-0089" num="0088">Following the initiation of the schedule cycle, the software application waits for the schedule cycle or interval to expire, as indicated by the timer. Commands can be initiated either from a user interface (when the scheduled cycle is not running), or as a result of the scheduler choosing a remote command or local schedule entry to be executed. Blocks <b>502</b>, <b>504</b>, <b>506</b>, <b>508</b>, <b>510</b> and <b>512</b> correspond to user selectable commands, which can be received from the UI, as indicated by a block <b>516</b>.</p>
<p id="p-0090" num="0089">When the schedule cycle is running and after the timer interval has expired, the voice server application determines if a call is in progress, in a decision block <b>534</b>. If it is, then the schedule cycle terminates, the timer is restarted, and control returns to the command loop, as indicated by block <b>528</b>. If there is no call in progress, then in a block <b>536</b>, the voice server application determines if there are any waiting jobs in the schedule cycle (i.e., any calls to start). If not, control again returns to the command at block <b>528</b>.</p>
<p id="p-0091" num="0090">If there is no call in progress and there are jobs in the schedule queue, a call is initiated. A first step in making a call is setting a call-in-progress indicator, as indicated in a block <b>540</b>. Before the call is made, the voice server software loads the data required to communicate with the chosen host in a block <b>542</b>. The host data includes a host script and a collection of signature files. Signature files each contain data used in the recognition of audio phrases by the remote VR system, and they are referenced by name from within the host script. For example, the signature defined in the file vwEnterPassword.sng is referenced in the host script as vwEnterPassword, the file extension being omitted. The host script contains a program script that instructs the voice server software what actions to take when a given signature phrase is recognized. The term host is used to refer to the combination of a host script, and associated signature files. Multiple hosts can share signature files, but they each have a unique host script file. Additional details relating to signature files, such as how they are generated and how the recognition of audio phrases using signature files is achieved, are provided below. Data corresponding to the host script are stored in a file location indicated by a block <b>546</b>, while data associated with signature files are stored in a file location indicated by a block <b>544</b>.</p>
<p id="p-0092" num="0091">In any case, once the host data (script and signatures) have been loaded in block <b>542</b>, the voice server application starts a telephone call using the modem, as indicated in block <b>550</b>. Then the host script routine is initiated in a block <b>548</b>. Once the connection is established, the voice server application waits for incoming audio to be received, as indicated by a block <b>552</b>. The incoming audio is being received from a voice modem identified as a block <b>592</b>. Once incoming audio signals are received, the voice server software enters a main recognition and action loop and begins processing incoming audio buffers as they arrive, as indicated in a block <b>554</b>. A predefined timeout (indicated by a block <b>594</b>) prevents the voice server software from being stuck in an infinite loop, which can occur in situations where the voice server software does not recognize any of the phrases in the audio signals that are received. Within the main recognition and action loop (i.e., in block <b>554</b>), the voice server software continually processes these incoming audio packets. By default, these audio packets are received in an uncompressed pulse code modulation (PCM) format with 8000, 16-bit samples per second. Each sample represents the amplitude of the audio signal expressed as a signed 16-bit integer. Each incoming audio buffer contains N samples, where N is chosen to reflect the desired time resolution of the recognizer. Typically N is 2000, representing 250 ms of real-time. Each time an audio buffer is received, it is processed to create a signature data structure, and this real-time signature is compared with the signatures of the expected phrases, as specified in the host script that was earlier loaded. When a host script is loaded, all of the referenced signature files are also loaded. If the current audio buffer does not match a signature phrase, the voice server application waits for the next audio buffer to be received from the modem, as indicated by block <b>592</b>. If the current audio buffer matches an expected phrase, the voice server program executes the actions that properly correspond to that phrase, in a block <b>556</b>, where the required action is specified in the host script that was earlier loaded. In a preferred embodiment, the following actions are available:
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0092">1. Send a DTMF (Touchtone&#x2122;) sequence over the telephone line to the voicemail host being called. These tones can either be generated via the modem or by the computer as audio played over the telephone line.</li>
        <li id="ul0002-0002" num="0093">2. Start audio capture, and when instructed, stop capture and save the captured audio into message files.</li>
        <li id="ul0002-0003" num="0094">3. Play audio files over voice-modem <b>292</b>.</li>
        <li id="ul0002-0004" num="0095">4. Record a progress or error message in the log file and/or on the computer console.</li>
        <li id="ul0002-0005" num="0096">5. Terminate the call.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0093" num="0097">Once these actions have been executed in block <b>556</b>, any timeouts are reset, and the voice server application determines if the call should be terminated in a block <b>558</b>. The termination can occur as the result of a hang-up action, as the result of user intervention, or because of a default timeout expiring. Timeouts need not cause a call to terminate; instead, they can have actions of their own, which can result in continued processing, as if a phrase had been recognized. Under normal circumstances the call is terminated when all relevant voicemail messages have been retrieved following a dialog between the software and the remote voicemail server.</p>
<p id="p-0094" num="0098">If a call is to be terminated, then control passes out of the main recognition loop, the telephone call is terminated in a block <b>560</b>, and the voice modem device is closed. The call-in-progress flag is cleared in a block <b>569</b>, and control returns to the main command loop, as indicated by block <b>528</b>. As provided by this block, in the main command loop, the voice server application is waiting for a next schedule cycle to initiate a call (see block <b>540</b>), or for a user input (see block <b>516</b>).</p>
<p id="p-0095" num="0099">Messages are captured and saved in message store <b>424</b> (shown in <figref idref="DRAWINGS">FIG. 4</figref>) during the execution of actions in block <b>556</b>. The message capture and storage elements of block <b>556</b> are described in greater detail below.</p>
<p id="p-0096" num="0100">Note that for each UI function indicated by blocks <b>502</b>, <b>504</b>, <b>506</b>, <b>508</b>, <b>510</b> and <b>512</b>, there is a corresponding function within the command loop, as indicated by blocks <b>530</b>, <b>580</b>, <b>582</b>, <b>584</b>, <b>586</b> and <b>588</b>.</p>
<p id="p-0097" num="0101">Note that manual calling is the function of initiating the call, under user control, from a menu, rather than having the call initiated by the scheduler. The user selects manual calling from a menu, enters the telephone number to call, and selects the script to be used (from a menu list of available scripts).</p>
<p id="h-0011" num="0000">Detailed Description of Main Recognition and Action Loop</p>
<p id="p-0098" num="0102"><figref idref="DRAWINGS">FIG. 6</figref> shows a schematic diagram of the main recognition and action loop of the program (more generally indicated by block <b>528</b> in <figref idref="DRAWINGS">FIG. 5</figref>). The voice server software calls a remote voicemail system <b>601</b> (i.e., a VR based voicemail system) over a PSTN line <b>603</b> using a voice modem <b>605</b>. Each incoming audio packet is processed as indicated by process block <b>607</b> and compared with a number of signatures, each representing a possible audio phrase to be recognized. The comparison is performed by a recognition engine <b>609</b>, using stored signatures <b>611</b>. Recognition engine <b>609</b> of <figref idref="DRAWINGS">FIG. 6</figref> is the same as recognition engine <b>434</b> in <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0099" num="0103">If a signature is recognized, then the actions associated with the recognized phrase in host script <b>615</b> are executed in a block <b>613</b>. These actions include sending a DTMF tone <b>617</b> over voice-modem <b>605</b> to the remote host <b>601</b>, and starting and stopping audio capture.</p>
<p id="p-0100" num="0104">In the case of audio capture commands, the actions control whether the incoming audio indicated by block <b>621</b> is to be routed to a message audio file <b>625</b>. The incoming audio is analyzed by process block <b>607</b>. Audio not part of a message is discarded.</p>
<p id="p-0101" num="0105">The phrases that are to be recognized are determined by the host script being executed. An example of part of a host script is shown in Table 1.</p>
<p id="p-0102" num="0106">
<tables id="TABLE-US-00001" num="00001">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="42pt" align="left"/>
<colspec colname="1" colwidth="175pt" align="left"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="1" rowsep="1">TABLE 1</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry>:getmessage 60</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2002;expect vwEndOfMessage</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;message End_Of_Message</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;save i</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;send 9</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2002;expect vwNextMessage</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;message Message_Saved</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;capture 1000</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2002;expect vwEndOfMessages</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;message End_Of_Messages</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;hangup</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0103" num="0107">In the above example, a label (:getmessage) is associated with three expect clauses, and a timeout value of 60 s (i.e., if nothing happens in 60 seconds, the voice server application terminates the connection). Each expect clause instructs the program to compare the signatures of incoming audio packets with the signature for an existing phrase (i.e., the signatures vwEndOfMessage, vwNextMessage, and vwEndOfMessages). There can be multiple parallel expect clauses, as shown in the above example. In this case, the incoming audio is compared with three identified possible phrases. If one of the phrases is recognized, the actions associated with the expect clauses are executed.</p>
<p id="p-0104" num="0108">In this example, if vwEndOfMessage is recognized by the voice server software then a status message &#x201c;End Of Message&#x201d; is output, the message is saved in the Inbox of the message store <b>424</b> (see <figref idref="DRAWINGS">FIG. 4</figref>), and a &#x201c;9&#x201d; DTMF code (or whatever DTMF code that particular VR system requires to save a message) is sent to the remote VR system to also save the message in its predefined storage.</p>
<p id="p-0105" num="0109">If vwNextMessage is recognized (signifying the start of a new message), the message &#x201c;Message Saved&#x201d; is output, and the capture of the new message begins. The parameter <b>1000</b> on the &#x201c;capture&#x201d; statement indicates that the first 1000 ms of audio should be trimmed from the message (for cosmetic reasons). If vwEndOfMessages is recognized (indicating the end of the last message), the voice server software terminates the call.</p>
<p id="p-0106" num="0110"><figref idref="DRAWINGS">FIG. 15</figref> provides details of how the recognition of incoming audio phrases proceed. Recognition does not begin until two audio buffers have been captured from the voice modem. Audio buffers <b>1500</b>A and <b>1500</b>B are each N samples in length. At each cycle of the recognition loop (indicated by block <b>554</b> of <figref idref="DRAWINGS">FIG. 5</figref>), the N samples comprising the last audio sample and the current (most recently arrived) audio sample are processed by iterating through a series of sample windows, of width N samples, starting at positions 0, W, 2W and 3W, where W is an exact fraction of N (in our example, it is assumed that W=N/4). At each iteration, the start of the sample buffer is advanced W samples.</p>
<p id="p-0107" num="0111">Use of this sliding window arrangement to derive successive input audio buffers is intended to compensate for the fact that the voice server application does not know where the real-time audio starts relative to the start of the recorded signature that is being compared with it. By ensuring that successive buffers overlap with each other, the discrimination of the recognition is improved, and the possibility for signatures to go unrecognized is reduced. This aspect of the invention is further discussed below, in the relation to signature creation.</p>
<p id="p-0108" num="0112">In the example of <figref idref="DRAWINGS">FIG. 15</figref>, there are four window sample buffers <b>1508</b>A-<b>1508</b>D. Note that buffers <b>1508</b>A-<b>1508</b>D include audio amplitude data corresponding to buffers <b>1500</b>A and <b>1500</b>B, which have been separated into buffer chunks A-H. Window sample buffer <b>1508</b>A includes buffer chunks A, B, C, and D; window sample buffer <b>1508</b>B includes buffer chunks B, C, D, and E; window sample buffer <b>1508</b>C includes buffer chunks C, D, E, and F; and window sample buffer <b>1508</b>D includes buffer chunks D, E, F, and G. Buffer chunk H forms the last buffer chunk of the first sample window when buffer <b>1500</b>B becomes the buffer corresponding to <b>1500</b>A, and buffer <b>1500</b>B is replaced with a new buffer (i.e., on the next cycle of the main recognition loop (block <b>554</b> in <figref idref="DRAWINGS">FIG. 5</figref>.)</p>
<p id="p-0109" num="0113">The audio amplitude data in each window sample buffer (i.e., buffers <b>1508</b>A-<b>1508</b>D) are processed to create a corresponding DFT of itself, thereby producing DFTs <b>1509</b>A-<b>1509</b>D. The generation of such DFTs is well-known to those of ordinary skill in this art. Each DFT represents the spectral characteristics of the audio data. Each data item in the DFT represents the normalized power present at a particular audio frequency. For an audio dataset of N samples, the DFT consists of N/2 values. For each of these values i, where i ranges from 1 to N/2, the value represents the power present at the frequency i. If the original N audio samples represent T seconds of real-time, then the real frequencies represented by the DFT are in the range of 1/T&#x3c;=f&#x3c;=N/2T. For example, if N is 2000 and T is &#xbc; second, then the range of the audio frequencies represented by the DFT is 4 Hz&#x3c;=f&#x3c;=4 KHz.</p>
<p id="p-0110" num="0114">For the four DFTs created (i.e., DFTs <b>1509</b>A-<b>1509</b>D), each is compared with pre-computed DFT buffers (DFTs <b>1510</b>A-<b>1510</b>C are three such DFT buffers), which are the signatures of the audio phrases to be recognized. A correlation function <b>1512</b> is applied to each pre-computed DFT (i.e., DFTs <b>1510</b>A-<b>1510</b>C) and each sample DFT (i.e., DFTs <b>1509</b>A-<b>1509</b>D) in turn, and if the correlation reaches a predetermined threshold, the phrase represented by one of the signatures <b>1510</b>A-<b>1510</b>C is deemed to have been recognized, and this recognition is output at a block <b>1514</b>. Correlation functions for comparing normalized data are well-known in the field of signal processing. The creation of signatures and the setting of correlation thresholds is a function of the learning process, which is described below.</p>
<p id="p-0111" num="0115">Preferably, buffers <b>1500</b>A and <b>1500</b>B (the recognition buffers) each include &#xbc; second of audio data. Thus, buffer chunks A-H each include 1/16 second of audio data. Four buffer chunks combined include &#xbc; second of audio data. As described in conjunction with <figref idref="DRAWINGS">FIG. 10</figref>, the best DFTs used for the signature (i.e., signature DFTs <b>1510</b>A-<b>1510</b>C) are preferably based on &#xbc; second of audio data. It should be understood that DFTs could be generated based on different lengths of audio data, as long as the DFTs in the signature file and the DFTs generated from incoming audio, as described in <figref idref="DRAWINGS">FIG. 15</figref>, are based on the samples of comparable size. Empirical data indicate that samples of &#xbc; second provide good results.</p>
<p id="p-0112" num="0116">As described above, once a phrase is recognized, the actions associated with its expect clause are executed, as defined in the current host script. The host script typically contains multiple labels, each associated with one or more expect clauses and actions. One of the results of recognition, therefore, can be the transfer of control from one label to another in the state table program. This transfer of control is performed via the &#x201c;goto&#x201d; statement. Table 2, which follows, shows examples of the &#x201c;goto&#x201d; statement in host scripts.</p>
<p id="p-0113" num="0117">In the example of Table 2 there are three labels: &#x201c;:start,&#x201d; &#x201c;:password,&#x201d; and &#x201c;:preamble.&#x201d; Control starts at the label &#x201c;:start,&#x201d; and the program waits for the remote voicemail system to say, &#x201c;Please enter your telephone number.&#x201d; This action triggers the expect clause for the signature &#x201c;nxEnterPhoneNumber,&#x201d; at which point, the script sends the telephone number (followed by an *) to the remote VR system as a sequence of DTMF tones &#x201c;send &#x26;n,*&#x201d;. A &#x201c;goto&#x201d; statement is then used to pass control to the label &#x201c;:password&#x201d;. The &#x201c;:password&#x201d; label expects to hear &#x201c;Please enter your password&#x201d; (nxEnterPassword) within 20 seconds. If it does not, the program executes the timeout clause and terminates the call with an error report &#x201c;E_Number_Rejected&#x201d;.</p>
<p id="p-0114" num="0118">If the password request arrives in time, the expect clause associated with &#x201c;nxEnterPassword&#x201d; is executed. The password is sent as a sequence of DTMF tones, and control passes via another &#x201c;goto&#x201d; statement, to the label &#x201c;:preamble,&#x201d; where message processing begins.</p>
<p id="p-0115" num="0119">
<tables id="TABLE-US-00002" num="00002">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="35pt" align="left"/>
<colspec colname="1" colwidth="182pt" align="left"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="1" rowsep="1">TABLE 2</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry>:start 30</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2002;expect nxEnterPhoneNumber</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;message Sending_Phone_Number</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;send &#x26;n,*</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;goto password</entry>
</row>
<row>
<entry/>
<entry>//</entry>
</row>
<row>
<entry/>
<entry>:password 20</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2002;expect nxEnterPassword</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;message Sending_Password</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;send &#x26;p</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;goto preamble</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2002;timeout</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;message E_Number_Rejected</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;hangup</entry>
</row>
<row>
<entry/>
<entry>//</entry>
</row>
<row>
<entry/>
<entry>:preamble 20</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2002;expect nxToPlayYourMessages</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;send 1</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;....</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
<br/>
Handling Multiple Simultaneous Signatures
</p>
<p id="p-0116" num="0120">The host scripts shown in Tables 1 and 2 are simple examples. In practice it is often necessary to have multiple expect clauses under the same label. Table 3 illustrates the use of multiple expect clauses.</p>
<p id="p-0117" num="0121">
<tables id="TABLE-US-00003" num="00003">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="35pt" align="left"/>
<colspec colname="1" colwidth="182pt" align="left"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="1" rowsep="1">TABLE 3</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry>:howmany</entry>
</row>
<row>
<entry/>
<entry>&#x2003;expect NoMessages</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;message You_Have_No_Messages</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;hangup</entry>
</row>
<row>
<entry/>
<entry>&#x2003;expect OneMessage</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;message You_Have_One_Message</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;hangup</entry>
</row>
<row>
<entry/>
<entry>&#x2003;expect MultipleMessages</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;message You_Have_Multiple_Messages</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;hangup</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0118" num="0122">In the example of Table 3, there are three expect clauses associated with the label &#x201c;:howmany.&#x201d; When the voice server is executing this script at the label &#x201c;:howmany,&#x201d; it compares the incoming audio with all three signatures. If the audio matches one of these signatures, then the corresponding expect clause is executed. The script in this example can therefore distinguish between no messages, one message, and multiple messages, and in response, displays the appropriate text &#x201c;You_Have_No_Messages,&#x201d; &#x201c;You_Have_One Message,&#x201d; etc. to the operator.</p>
<p id="h-0012" num="0000">The Complete Processing Cycle</p>
<p id="p-0119" num="0123"><figref idref="DRAWINGS">FIG. 7</figref> shows a flowchart detailing the processing of a call from the voice server application to a remote telephone voicemail system. Once the call has been started in a block <b>700</b> and audio processing has begun, the voice service software completes logging in to the remote voicemail system by identifying a security message in a block <b>702</b>, and responding with the proper password in a block <b>704</b>. In a block <b>706</b>, the voice server application processes and identifies the mail box status message, and in a decision block <b>708</b>, the voice server determines if the mail box is empty. If there are no messages to retrieve, then the call is terminated in a block <b>720</b>. Otherwise, message playback begins. Note that in some cases, a first message begins immediately following login, and in some cases, a DTMF tone sequence must be sent to begin message playback. Thus, in a decision block <b>710</b>, the voice server application determines if message playback is to begin immediately. If not, then in a block <b>712</b>, the correct DTMF tone sequence is sent to begin message playback. In any case, in a block <b>714</b>, the voice server application waits for any of: a timeout; a &#x201c;Start of Message&#x201d; indication; or an &#x201c;End of All Messages&#x201d; indication (indicating the last message has been captured).</p>
<p id="p-0120" num="0124">If a timeout occurs, then the call is terminated in block <b>720</b>, as indicated above. If receipt of a &#x201c;Start of Message&#x201d; indicator occurs, message capture begins in a block <b>716</b>, until the voice server application program identifies an end-of-messages indicator or a timeout, as indicated in block <b>718</b>. If a timeout occurs, the audio is captured for later review in a block <b>722</b>, and the call is terminated in block <b>720</b>. If an &#x201c;End of Message&#x201d; indicator is recognized, then the audio that has arrived since the capture was initiated is saved to a message file in a block <b>726</b>. At that point, the logic loops back to block <b>714</b> to await an additional message, a timeout, or an end of message indicator, as described above. Multiple messages are captured in this way, until an &#x201c;End of All Messages&#x201d; indicator or timeout is received, in which case the call is terminated in block <b>720</b>, as previously described. In a preferred embodiment, the captured audio messages are encoded in the popular MPEG-1, level 3 (MP3) format.</p>
<p id="h-0013" num="0000">Method of Creating Keys for Message Files</p>
<p id="p-0121" num="0125">One of the problems with voicemail retrieval is that it is often desirable to keep existing messages within the VR system for extended periods. If a message remains in the user's voicemail box, however, it will be repeatedly downloaded by the software and the user will be confused by multiple copies of the same message. The invention provides a method for recognizing messages that have already been seen. Duplicate messages can then be discarded, hidden from view, or otherwise disposed of.</p>
<p id="p-0122" num="0126">Each message file, as it is processed, has a key built for it. The key is a short sequence of numbers, saved in a key file associated with the message. This key is based on a compact encoding of the audio spectrum (DFT) of the message. This key can be compared with the keys of other messages using a correlation function. If the keys of the messages correlate, it is assumed that the two messages are identical. By choosing the length of the encoding window to be large with respect to the word length used in the messages (e.g., greater than two seconds), the correlation of messages with differing audio heads and tails (resulting from timing variations during calls to the VR system), but similar bodies, remains high. Because message keys are short (typically 100 bytes or less), the key for a new message can be correlated with a very large number of messages in a short time. A preferred key is the audio spectrum of the whole message, divided into 20 segments. The resulting 20 values, plus the message length and the message position (in the external voicemail box), are stored as American Standard Code for Information Interchange (ASCII) text in a key file.</p>
<p id="p-0123" num="0127"><figref idref="DRAWINGS">FIG. 8</figref> schematically illustrates how message keys are used to recognize similar messages and distinguish dissimilar messages. A new message indicated by an arrow <b>806</b>A is retrieved by the voice server application in a block <b>804</b>A. The voice server application processes the message to create a message key file <b>800</b>A and a message audio file <b>802</b>A. At some later time, the same message, as indicated by arrow <b>806</b>B, is retrieved again in a block <b>804</b>B. Once again, message key <b>800</b>B and message audio file <b>802</b>B are created. After message key <b>800</b>B is created, the voice server application compares message key <b>800</b>B with all other stored message keys. If a match is found, as is indicated by line <b>808</b> connecting message key <b>800</b>A and <b>800</b>B, the voice server application knows that message audio files <b>802</b>A and <b>802</b>B are for the same message. Message key <b>800</b>B and message audio file <b>802</b>B (or message key <b>800</b>A and message audio file <b>802</b>A) can be safely deleted, if desired. Now a third message (indicated by arrow <b>806</b>C), different from the other two, is retrieved at a block <b>804</b>C. A message key <b>800</b>C and message audio file <b>802</b>C are generated. Message key <b>800</b>C is compared with all previous messages (including <b>802</b>A and <b>802</b>B, if both have been saved). In this case, the keys do not match, as indicated by line <b>810</b>, and the message is considered distinct (i.e., not the same as any other message previously received).</p>
<p id="h-0014" num="0000">How New Phrases are Learned by the Voice Server Software</p>
<p id="p-0124" num="0128">In the above description of the voice server application implemented in one preferred embodiment, the recognition engine (corresponding to recognition engine <b>434</b> in <figref idref="DRAWINGS">FIG. 4</figref> and recognition engine <b>609</b> in <figref idref="DRAWINGS">FIG. 6</figref>) uses signatures <b>611</b> (shown in <figref idref="DRAWINGS">FIG. 6</figref>) to recognize phrases in incoming audio. <figref idref="DRAWINGS">FIG. 15</figref> schematically illustrates, and the above discussion explains, the method by which these signatures are compared with the incoming audio. Before a phrase can be recognized by the software, however, it is necessary for the software to be taught to recognize that phrase and to prepare a signature for it.</p>
<p id="p-0125" num="0129">Thus, before a signature (e.g., vwEnterPassword) can be used in a host script it must be learned by the voice server software. <figref idref="DRAWINGS">FIG. 9</figref> illustrates the steps involved in teaching the voice server software to recognize a new phrase. In the terminology used herein to describe the voice server application, a phrase represents the audio sequence to be turned into a signature. For example, the signature vwEnterPassword might be associated with a phrase containing the audio &#x201c;Please Enter your Password.&#x201d;</p>
<p id="p-0126" num="0130">The basic steps in creating a new signature file are as follows.
<ul id="ul0003" list-style="none">
    <li id="ul0003-0001" num="0000">
    <ul id="ul0004" list-style="none">
        <li id="ul0004-0001" num="0131">Make a call using a host script and capture the audio containing the new phrase to be learned.</li>
        <li id="ul0004-0002" num="0132">Use the signature creation tool (shown in <figref idref="DRAWINGS">FIG. 12</figref> and described in detail below) to examine the captured audio sequence offline, to choose the new phrase to be recognized and make a signature for it.</li>
        <li id="ul0004-0003" num="0133">Save the signature to a file. Preferably, by convention, signature files are named with a two letter prefix, signifying the host and a name spelling or identifying the corresponding phrase. Thus, the name &#x201c;vwEnterPassword,&#x201d; includes &#x201c;vw&#x201d; to identify the host (in this case Verizon Wireless&#x2122;) and &#x201c;EnterPassword&#x201d; to identify the phrase.</li>
        <li id="ul0004-0004" num="0134">Edit the host script to include the use of the new signature and make a test call using it.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0127" num="0135">Each of the high level steps used to create a new signature file are shown in <figref idref="DRAWINGS">FIG. 9</figref>. In this Figure, boxes <b>901</b> and <b>903</b> respectively separate the steps into online and offline groups. A block <b>900</b> indicates a start of the sequence of steps, while a block <b>918</b> indicates an end of the sequence of steps. The first step is to make a call to the remote VR system (i.e., to the host) whose phrase is to be learned, as indicated by a block <b>902</b>. This call is made with a partial script that enables the voice service application to navigate the remote VR system to the point where the host speaks the phrase to be recognized. At this point, the voice service application captures the audio signal, as indicated by a block <b>904</b>. If a capture command has been executed (as described above), but the call ends before a save command has been issued, the software saves all of the audio after the capture command in a message for diagnostic purposes. Therefore, scripts used for learning purposes usually contain a capture command, just before the new phrase is likely to be issued. Because the script generally cannot yet identify the new phrase, a timeout normally occurs after the capture of the new phrase, to end the call, as noted in a block <b>906</b>. The captured audio is saved as a normal voicemail message within the voice server message store.</p>
<p id="p-0128" num="0136">The GUI of the voice server software preferably enables any message to be selected as containing the audio for a new signature. In most error or unexpected phrase situations, the scripts will capture trailing audio automatically, and therefore, it is rarely necessary to make extra calls to capture new phrases to be recognized, except when building the basic scripts for a host for the first time.</p>
<p id="h-0015" num="0000">Phrase Selection and Signature Creation</p>
<p id="p-0129" num="0137">Once the audio containing the phrase to be learned has been captured in a message, a user selects the create signature tool from the GUI in a block <b>908</b>. In one preferred embodiment, when using the create signature tool, only one message (corresponding the next phrase to be recognized) is processed at a time. The message presented to the operator will be the last message captured by the voice server (see block <b>906</b> of <figref idref="DRAWINGS">FIG. 9</figref>). When the create signature tool is launched, the last message will be used as the audio source. The users utilize the create signature tool to select a signature reference phrase in a block <b>910</b>, as will be discussed in greater detail below. In a block <b>912</b>, the create signature tool generates the signature by applying a DFT to the audio. In a block <b>916</b>, the DFT is saved. Thus, each signature file contains the DFT of the phrase audio. Signature creation is described in greater detail below. As already described, this DFT is compared with incoming audio within the recognition engine of the voice server application. Once the DFT has been checked manually and any parameters adjusted (see below), it is saved to a signature file, and the new phrase may now be used in a host script.</p>
<p id="p-0130" num="0138">In creating a script from scratch, the process illustrated in <figref idref="DRAWINGS">FIG. 9</figref> is repeated until all the phrases used by a specific host have been learned, and the script for that host is completed. In most situations, only five or six phrases occur in the dialog with a particular host. Therefore, creating support for a completely new host is a relatively simple and quick process.</p>
<p id="h-0016" num="0000">Methodology of Phrase Selection</p>
<p id="p-0131" num="0139">In most cases, the selection of the phrase to be recognized is straightforward. As will be described in detail below, one preferred embodiment uses signatures that represent a &#xbc; second portion of the audio file. Therefore, each phrase is best recognized by that &#xbc; second portion of audio that is unique to that phrase (unique in the context of recognizing that phrase from other phrases). At any given time during a call, the &#x201c;recognition context&#x201d; is the set of all possible messages that may be heard. For example, in a typical situation during a mail box login, the context is very simple, likely consisting of a phrase similar to &#x201c;please enter your password,&#x201d; and a timeout error message such as &#x201c;press the star key for more options.&#x201d; In such a recognition context, the present invention requires the generation of a signature to enable the phrase &#x201c;please enter your password&#x201d; to be recognized. It is likely that this phrase will be repeated a plurality of times without interruption, before the error message is played. Because this recognition context is simple, any &#xbc; second portion of the phrase &#x201c;please enter your password&#x201d; will yield a signature that is readily distinguished over another signature, such as that produced by any &#xbc; second portion of the phrase &#x201c;press the star key for more options.&#x201d;</p>
<p id="p-0132" num="0140">Table 3 (above) provided a more complex example in which portions of three messages were very similar. Similar messages will likely be encountered when navigating through a menu of a voicemail system. The three messages include: &#x201c;You have no messages,&#x201d; &#x201c;you have one message,&#x201d; and &#x201c;you have &#x3c;N&#x3e; messages&#x201d; (where N is any number corresponding to the number of messages received). Because these messages have parts in common, the portion of the message to create a signature (i.e., the reference phrase) must be carefully selected. The phrases &#x201c;you have no messages&#x201d; and &#x201c;you have one message&#x201d; never vary, while the phrase &#x201c;you have &#x3c;N&#x3e; messages&#x201d; (where N is any number) includes the variable N. The following procedure can be used to select a portion of a message to enable that message to be distinguished from similar messages.
<ul id="ul0005" list-style="none">
    <li id="ul0005-0001" num="0000">
    <ul id="ul0006" list-style="none">
        <li id="ul0006-0001" num="0141">1. Recognize that the identical portions of similar messages (i.e., &#x201c;you have&#x201d;) cannot be selected for generating signatures that will distinguish similar messages. The selected portion must be based on the non-identical portions of the messages (in the instant example, the selected portion that can be used includes &#x201c;no messages,&#x201d; &#x201c;one message,&#x201d; and &#x201c;&#x3c;N&#x3e; messages.&#x201d;)</li>
        <li id="ul0006-0002" num="0142">2. When possible, select distinguishable and non varying portions of the phrases. In the instant example, the phrases &#x201c;you have no messages&#x201d; and &#x201c;you have one message&#x201d; can be distinguished by producing a signature based on the word &#x201c;no&#x201d; for the former phrase, and the word &#x201c;one&#x201d; for the latter phrase.</li>
        <li id="ul0006-0003" num="0143">3. For remaining messages or phrases, select a portion of the remaining phrase that is shared in common with similar phrases, such that the portion in common occurs later in other phrases than their signature portion. Note in the present example the words &#x201c;no&#x201d; and &#x201c;one&#x201d; occur before the word &#x201c;message.&#x201d; Thus the word &#x201c;messages&#x201d; can be used to generate a signature for the phrase &#x201c;you have &#x3c;N&#x3e; messages&#x201d;, because recognition of the phrase &#x201c;you have no messages&#x201d; occurs at &#x201c;no&#x201d;, and recognition of the phrase &#x201c;you have one message&#x201d; occurs at &#x201c;one&#x201d;.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0133" num="0144">The operation of the create signature tool (a function of the voice server that is used to select reference phrases and to create new signatures based on the reference phrases) is discussed in detail below.</p>
<p id="h-0017" num="0000">Details of Signature Creation</p>
<p id="p-0134" num="0145"><figref idref="DRAWINGS">FIG. 12</figref> shows an exemplary embodiment of the GUI of the create signature tool. It is a typical Windows&#x2122; dialog box. As indicated above, this tool is invoked at block <b>908</b> of <figref idref="DRAWINGS">FIG. 9</figref>, and the last audio file collected will be provided to the create signature tool. The name of the audio file being manipulated to produce a signature is displayed in a text field <b>1210</b>, while a name selected for the new signature is displayed in a text field <b>1206</b>. Once a signature has been created, it will be included in a &#x201c;Completed Signatures&#x201d; field <b>1208</b>. As will be described in detail below, multiple signatures can be derived from the same audio file. The hostname for which the signature is being prepared is optionally entered in a text field <b>1212</b>. By convention, the string entered in field <b>1212</b> is the name of the script for which the signature was first developed. Such data are for informational purposes only, and are not required by the voice server.</p>
<p id="p-0135" num="0146">The audio sequence (i.e., the audio file) for which a signature will be made can be many seconds long, and the audio sequence is displayed as an audio amplitude waveform in a panel <b>1220</b>. The create signature tool is coupled to the speaker output of the computer, and control buttons <b>1228</b>, <b>1232</b>, <b>1234</b>, and <b>1236</b> may be used to listen to the selected audio. Button <b>1236</b> is a stop button that terminates audio playback. Button <b>1234</b> is a play-all/pause button, and if this button is activated, the entire audio sequence is played, starting at the beginning. Button <b>1228</b> is a play phrase button that causes only a selected portion of the audio sequence to play. That selected portion corresponds to the portion residing between phrase cursors <b>1241</b>A and <b>1241</b>B. The phrase cursor indicates the reference phrase (i.e., the segment) of audio from which the new signature will be built. In a preferred embodiment, phrase cursor <b>1241</b>A is a green line, and phrase cursor <b>1241</b>B is a black line, but these colors are not important. Under a default setting in this embodiment, the reference phrase delineated by phrase cursor <b>1241</b>A and phrase cursor <b>1241</b>B is five seconds in length. The phrase cursors can be moved within the audio sequence using a cursor slider <b>1232</b>.</p>
<p id="p-0136" num="0147">The user chooses the best reference phrase (i.e., the best selected segment of the audio sequence displayed in panel <b>1220</b>) using cursor slider <b>1232</b>, and playloop button <b>1228</b>. The slider can be moved while the audio is playing, and this feature is of great utility in finding the right phrase (the slider is moved until the phrase is heard). Once the reference phrase has been chosen, and the chosen name for the signature has been entered in &#x201c;Select Token&#x201d; text field <b>1206</b>, the user presses a &#x201c;Make DFT&#x201d; button <b>1226</b>.</p>
<p id="p-0137" num="0148">The process performed by the create signature tool in response to the activation of &#x201c;Make DFT&#x201d; button <b>1226</b> is schematically illustrated in <figref idref="DRAWINGS">FIG. 10</figref>. The process involves five steps. Initially, the entire audio sequence is divided into three segments: a segment <b>1003</b> corresponding to audio under the reference cursor, a segment <b>1002</b> corresponding to the audio preceding the reference cursor, and a segment <b>1004</b> corresponding to the audio following the reference cursor. In a first step of the create signature process, the trailing audio (segment <b>1004</b>) is discarded. In a second step, the remaining audio (segments <b>1002</b> and <b>1003</b>) is divided into &#xbc; second segments, resulting in a plurality of buffers <b>1006</b> corresponding to segment <b>1002</b>, and a plurality of buffers <b>1008</b> corresponding to segment <b>1003</b>.</p>
<p id="p-0138" num="0149">Next, in a third step, a DFT operation is performed on the contents of each of audio buffers <b>1006</b> and <b>1008</b>, resulting in a plurality of DFT buffers <b>1010</b> and <b>1012</b>, each of which is the result of processing the corresponding audio buffers with the DFT function. Buffers <b>1010</b> and <b>1012</b> are thus referred to as DFT buffers. Note that DFT buffers <b>1010</b> correspond to segment <b>1002</b> and buffers <b>1006</b>, while DFT buffers <b>1012</b> correspond to segment <b>1003</b> and buffers <b>1008</b>. Thus, DFT buffer <b>1011</b> is based on a single &#xbc; second buffer from segment <b>1002</b>.</p>
<p id="p-0139" num="0150">In a fourth step, the create signature tool selects a single DFT buffer corresponding to the audio under the reference cursor (i.e., from the plurality of DFT buffers <b>1012</b>, each of which are based on segment <b>1003</b>). For convenience, the selected DFT buffer will be referred to as the selected DFT (or the best DFT). The selected DFT preferably is least like any of the DFTs derived from the preceding audio (i.e., DFT buffers <b>1010</b>). A function described in detail below is used to evaluate the differences among the DFTs, to facilitate the selection of the single DFT. As illustrated in <figref idref="DRAWINGS">FIG. 10</figref>, DFT buffer <b>1016</b> has been selected as the best DFT. In a fifth step, the selected DFT is saved in a signature file <b>1020</b>.</p>
<p id="p-0140" num="0151">While the method by which the best DFT to form the new signature is chosen is very simple, it is quite important. In fact, the selection of a best DFT is an important element in enabling successful functioning of the voice server application. It can be understood with reference to the following observations:
<ul id="ul0007" list-style="none">
    <li id="ul0007-0001" num="0000">
    <ul id="ul0008" list-style="none">
        <li id="ul0008-0001" num="0152">1. The preceding audio (i.e., segment <b>1002</b>) contains the audio between the start of the message and the reference phrase audio (i.e., segment <b>1003</b>). This segment of audio represents the ambient environment in which the phrase occurs and may include other &#x201c;phrases&#x201d; that are not used as a basis for recognition.</li>
        <li id="ul0008-0002" num="0153">2. It is very important that the best DFT correlates poorly with any of the preceding audio, so that the preceding audio is not incorrectly recognized as the reference phrase.</li>
        <li id="ul0008-0003" num="0154">3. It is very important that the best DFT correlates well with the reference phrase (i.e., segment <b>1003</b>), so that the recognition engine can be easily triggered.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0141" num="0155">In order to choose the best DFT, which meets the criteria defined by observation 2 and observation 3 (as described above), the processing proceeds as follows:
<ul id="ul0009" list-style="none">
    <li id="ul0009-0001" num="0000">
    <ul id="ul0010" list-style="none">
        <li id="ul0010-0001" num="0156">For each of the plurality of DFT buffers <b>1012</b> corresponding to the reference cursor audio portion (i.e., corresponding to segment <b>1003</b>), a correlation coefficient, c, is calculated between it and each DFT of the preceding audio region (i.e., for each of the plurality of DFT buffers <b>1010</b>). For each DFT in the reference cursor audio region, the maximum value of c, over all the DFT buffers <b>1010</b> corresponding to the preceding audio portion, is recorded as c<sub>MAX</sub>. While <figref idref="DRAWINGS">FIG. 10</figref> appears to indicate that DFT buffers <b>1012</b> include five individual DFT buffers, in a preferred embodiment, each DFT buffer is based on an audio sample &#xbc; second in length, and the reference cursor audio portion is 5 seconds in length. Thus, a reference cursor audio portion (i.e., segment <b>1003</b>) 5 seconds in length will include 20 discreet &#xbc; second samples (i.e., 5&#xf7;&#xbc;=20), from which 20 different DFT buffers <b>1012</b> can be generated. For each DFT in the reference cursor region (i.e. DFT buffers <b>1012</b>), a correlation coefficient, k, is calculated between itself and all the other DFT buffers <b>1012</b> in the reference cursor region, excluding itself For each DFT, the largest value of k is recorded as k<sub>MAX</sub>.</li>
        <li id="ul0010-0002" num="0157">For each DFT buffer <b>1012</b> in the reference cursor region, the value L<sub>i </sub>is calculated according the following formula.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>L</i><sub>i</sub>=sqrt((1<i>&#x2212;c</i><sub>MAX</sub>)<sup>2</sup><i>+k</i><sup>2</sup><sub>MAX</sub>)<?in-line-formulae description="In-line Formulae" end="tail"?>
        <ul id="ul0011" list-style="none">
            <li id="ul0011-0001" num="0158">The values of c and k lie between 0 and 1.</li>
        </ul>
        </li>
        <li id="ul0010-0003" num="0159">L<sub>i </sub>is the distance of the particular DFT from the origin the two-dimensional Euclidean space defined by (1&#x2212;c) and k. High values of L are therefore preferred, as they indicate low values of c (high values of 1&#x2212;c) along with high values of k.</li>
        <li id="ul0010-0004" num="0160">The DFT with the greatest value of L is chosen as the best DFT for use in the signature.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0142" num="0161">Referring once again to <figref idref="DRAWINGS">FIG. 12</figref>, the best DFT selected by the above function (and the associated data) is saved in the signature file using a save button <b>1218</b>. Preferably before the new signature is saved, the signature is inspected to determine if it is a good candidate. One such inspection process would be to test the selected best DFT against the audio file selected, to see if the selected best DFT properly identifies the audio file in question. This process is described in greater detail below. If it is determined that the best DFT selected based on a specific reference cursor audio portion does not provide the desired audio file recognition performance, slider <b>1232</b> can be used to move reference cursors <b>1241</b>A and <b>1241</b>B, so that a different reference cursor audio portion is selected. Then &#x201c;Make DFT&#x201d; button <b>1226</b> may be pressed again, so that the five step process described in conjunction with <figref idref="DRAWINGS">FIG. 10</figref> is executed once again. This can be repeated as often as desired before the signature is saved. The create signature tool is closed using a cancel button <b>1219</b>.</p>
<p id="p-0143" num="0162">The determination of whether a given DFT is a good candidate is ultimately a matter of judgment and experience. To aid in the choice, the create signature tool provides a number of aids to assist a user in determining if a selected best DFT will provide the desired audio file recognition performance. These aids, identified in <figref idref="DRAWINGS">FIG. 12</figref>, include:
<ul id="ul0012" list-style="none">
    <li id="ul0012-0001" num="0000">
    <ul id="ul0013" list-style="none">
        <li id="ul0013-0001" num="0163">The audio spectrum of the chosen reference signature (i.e., DFT <b>1016</b> from <figref idref="DRAWINGS">FIG. 10</figref>) is displayed in an upper panel <b>1242</b> of the create signature tool, whenever &#x201c;Make DFT&#x201d; button <b>1226</b> is pressed. The spectral display enables the experienced operator to distinguish between noise and speech, and therefore to adjust the reference point to correspond to a clean segment of speech. The DFT shown in panel <b>1242</b> of <figref idref="DRAWINGS">FIG. 12</figref> exhibits ordered spectral peaks, and thus likely corresponds to a clean speech segment of audio.</li>
        <li id="ul0013-0002" num="0164">The value of the c and k for the best DFT, correlated with each DFT in the preceding audio portion (i.e., DFTs <b>1010</b> corresponding to segment <b>1002</b> of <figref idref="DRAWINGS">FIG. 10</figref>) and the reference phrase (i.e., DFTs <b>1012</b> corresponding to segment <b>1003</b> of <figref idref="DRAWINGS">FIG. 10</figref>) is displayed in red as an overlay <b>1238</b> on the audio timeline. The y scale in this case covers the range 0 to 1. A green horizontal line <b>1240</b> indicates the maximum value of k.</li>
        <li id="ul0013-0003" num="0165">When a DFT is calculated, phrase cursor <b>1241</b>A (a vertical green line in this embodiment) moves to indicate the start of the chosen signature block.</li>
        <li id="ul0013-0004" num="0166">The value of k is displayed in a dialog box <b>1215</b>.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0144" num="0167">In order to determine if the chosen signature block is a good choice, a number of heuristics are applied, as follows:
<ul id="ul0014" list-style="none">
    <li id="ul0014-0001" num="0000">
    <ul id="ul0015" list-style="none">
        <li id="ul0015-0001" num="0168">If the audio segment corresponding to the best DFT does not look like speech (as indicated by observing the DFT displayed in panel <b>1242</b>), that best DFT should be rejected. This event is very unlikely, if the reference phrase corresponds to speech.</li>
        <li id="ul0015-0002" num="0169">If the value of k (as displayed in dialog box <b>1215</b>) is below 0.75, that best DFT should be rejected.</li>
        <li id="ul0015-0003" num="0170">If the peak values of c, as displayed in red overlay <b>1238</b> are above 0.4, then the DFT should be rejected, as values over that amount are likely to result in incorrect recognition.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0145" num="0171">The example in <figref idref="DRAWINGS">FIG. 12</figref> matches well with the above defined parameters, and is therefore an excellent candidate for use in creating the tsEnterPassword signature.</p>
<p id="p-0146" num="0172">In any event, if the user is dissatisfied with the best DFT selected, the user can mover slider <b>1232</b> to another portion of the audio file, as represented in panel <b>1220</b>, to select a different best DFT.</p>
<p id="h-0018" num="0000">Fine Control of Create Signature</p>
<p id="p-0147" num="0173">In addition to the controls described above, the user has access to a number of additional controls over signature parameters from within the create signature tool. A quantum control field <b>1230</b> can be used to improve the discrimination of the values of c. According to the value of this integer value (q&#x3e;=1), each reference DFT <b>1012</b> is compared to the preceding audio, as is schematically illustrated in <figref idref="DRAWINGS">FIG. 17</figref>.</p>
<p id="p-0148" num="0174"><figref idref="DRAWINGS">FIG. 17</figref> illustrates the case where q=4. In a preferred embodiment of the voice server application a value of 10 is used, hence the default value shown in field <b>1230</b>, but 4 is a good value for illustrative purposes. The method illustrated in <figref idref="DRAWINGS">FIG. 10</figref> implies a value for q of 1, again for illustrative purposes. Referring to <figref idref="DRAWINGS">FIG. 17</figref>, an audio buffer <b>1704</b> contains all the preceding audio with which a candidate reference DFT <b>1712</b> will be compared.</p>
<p id="p-0149" num="0175">DFT <b>1712</b> corresponds to the DFT of a specific &#xbc; second buffer of the reference phrase segment (segment <b>1003</b> from <figref idref="DRAWINGS">FIG. 10</figref>). It is DFT <b>1712</b> for which the values of c are being calculated (as indicated in <figref idref="DRAWINGS">FIG. 17</figref>, c is a result <b>1714</b> of the comparison of DFT <b>1712</b> with DFTs <b>1708</b>A-<b>1708</b>E). The &#xbc; second size of each buffer is a default value. The width of each preceding audio buffers <b>1706</b>A-<b>1706</b>E, from which the preceding audio DFTs <b>1708</b>A-<b>1708</b>E are calculated, must be the same width as the reference phrase segment. Thus, if the audio reference phrase segment is 5 seconds long, and each buffer is &#xbc; second, then the audio reference phrase segment includes 20 buffers, and each preceding audio buffer <b>1706</b>A-<b>1706</b>E includes 20 (&#xbc; second) buffers. In other words, the audio reference phrase segment and each preceding audio buffer <b>1706</b>A-<b>1706</b>E have a width of N audio samples.</p>
<p id="p-0150" num="0176">The value of q determines how far the starting point of the &#x201c;preceding audio&#x201d; buffer is advanced for each DFT calculation. N must be exactly divisible by q in the same manner as N/W must be an integer in the discussion of <figref idref="DRAWINGS">FIG. 15</figref>, above.</p>
<p id="p-0151" num="0177">If q=1, then the starting points S<sub>0</sub>-S<sub>4 </sub>(respectively labeled <b>1716</b>A-<b>1716</b>E) advance by exactly N between each successive portion, and the audio buffers used to calculate c values never overlap. If q is greater than 1, the buffers overlap. The overlap is important, because in the operational mode the starting point of any preceding audio portion cannot be predicted exactly, therefore this variability needs to be introduced into the calculations. If q is greater than 1, the time resolution of the calculations are effectively increased by a factor of q. The higher the value of q, the greater the processing burden, and while this is not a major issue during the operation of the create signature tool (which is not a real-time activity), it is a significant operational trade-off. It has been empirically shown that a value of 10, with a sample size of &#xbc; second, performs quite satisfactorily in a preferred embodiment of the present invention.</p>
<p id="p-0152" num="0178">The method schematically illustrated in <figref idref="DRAWINGS">FIG. 17</figref> is similar to the sliding window technique used by the recognition engine, described above and shown in <figref idref="DRAWINGS">FIG. 15</figref>. The use of an overlapping audio window in both the recognition engine and the create signature tool is an important factor in providing satisfactory performance in the present invention. Without overlapping windows, the performance of the preferred embodiment is marginally satisfactory. However, by using sliding windows (as described in conjunction with <figref idref="DRAWINGS">FIGS. 15 and 17</figref>), the performance of the present invention improves remarkably.</p>
<p id="p-0153" num="0179">A mean factor control <b>1224</b> is available in the create signature tool GUI of <figref idref="DRAWINGS">FIG. 12</figref> and is used to selectively control the DFT samples that are to be considered in the calculation of c values. Each DFT sample is examined and compared to a value, and only DFT samples that exceed that value will be used in the correlation calculations. The specific value employed is the mean of the preceding sample DFTs multiplied by a mean factor. The mean factor can be adjusted using mean factor control <b>1224</b>. For example, if mean factor control <b>1224</b> is set to 2, then only DFT values that exceed twice the mean value will be used in the correlation calculations. Proper adjustment of this control has the effect of removing noise (which has a low amplitude) from the comparisons. It has been empirically determined that selecting a mean factor of 2 usually provides good results.</p>
<p id="p-0154" num="0180">Referring once again to <figref idref="DRAWINGS">FIG. 12</figref>, a timeout field <b>1216</b> corresponds to a functionality that was used in testing and is now obsolete. The timeout value is specified in the host script (see the above description of <figref idref="DRAWINGS">FIG. 5</figref>).</p>
<p id="p-0155" num="0181">A threshold correlation coefficient displayed in field <b>1214</b> corresponds to a critical value. The threshold correlation coefficient determines the sensitivity of the recognition process. When the signature is created, the default value indicated here is defined as equal to one half the difference between k (displayed in field <b>1215</b>) and 0.5. Typically, for good signatures, the value calculated is greater than 0.62 (indicating a value for k of 0.84 or greater). The user can manually adjust this value (using the slide bar adjacent to field <b>1214</b>) if desired, before the signature is saved. Threshold correlation coefficient values below 0.6 are suspect, as are k values below 0.8. The threshold correlation value displayed in field <b>1214</b> is saved in the signature file and is used by the recognition engine. Note that field <b>1215</b> is not a user selectable field.</p>
<p id="p-0156" num="0182">Since signatures are files, they can readily be copied between voice servers, and signatures prepared on one voice server can be used by other voice servers. Typically, in a multi-server operation (see below), one computer running the create signature tool is employed to prepare signatures that will be used by multiple Voice servers. The create signature tool can therefore be implemented as a separate application built around the Voice server, but operated independently of the operational servers.</p>
<p id="h-0019" num="0000">Editing Signatures</p>
<p id="p-0157" num="0183">It may be desirable to recalculate an existing signature. The create signature tool can function as an editing tool for this purpose. When the voice server application is operating in manual mode, the create signature tool can be started at any time. In this case, all the installed signatures are displayed and may be chosen from a drop-down selection box <b>1204</b>. Since the system keeps the audio for all existing signatures, panels <b>1220</b> and <b>1242</b> instantly shows both the audio file and the DFT of the existing signature for the audio file. The phrase cursor is positioned over the existing reference phrase, and the name of the audio file associated with the signature is displayed in a dialog box <b>1210</b>.</p>
<p id="p-0158" num="0184">At this point, the user may recalculate the DFT after moving the cursor, delete the signature (using delete button <b>1222</b>), change the name of the signature (using text field <b>1206</b>), and/or modify the threshold correlation value in field <b>1214</b>. Once any such changes are complete, the existing signature can be overwritten using save button <b>1218</b>. If the name has changed, a new signature is created, so it is possible to derive new signatures from old signatures at any time. If the DFT has not been recalculated, only the changed, non-DFT values (e.g., the threshold correlation coefficient) are saved.</p>
<p id="h-0020" num="0000">How New Phrases May be Learned Remotely</p>
<p id="p-0159" num="0185">As described above and as discussed in greater detail below, the present invention enables the distribution of digital audio messages via email. Furthermore the service element of a preferred embodiment of the present invention enables one computer, attached to a voice modem, to act as a server for remote devices that lack a voice modem. In the simplest situation, the configuration of the voice server application to learn how to interrogate a new type of host (i.e., a new voicemail service, or VR system) is executed and controlled by a user using the computer that implements the voice server application.</p>
<p id="p-0160" num="0186">On the other hand, it is sometimes useful to enable a user to teach the Voice server application to handle a new voicemail host remotely (i.e., from a remote computer that lacks a voice modem). For example, the voice server application may be physically remote from the system administrator. The method of remotely configuring the voice server application to support a new VR host is illustrated in the flowchart of <figref idref="DRAWINGS">FIG. 13</figref>, which enables the voice server application to generate signatures that are to be used to recognize one or more phrases. The process begins at a start block <b>1300</b> (and subsequently ends at an end block <b>1336</b>). The remote computer, upon which the voice server application resides, prepares a host script in a block <b>1302</b>, and any signature files needed by another server to gain access to the VR host. Once the server computer has access to the VR host using this script, the script enables the server computer to obtain new phrases (i.e., audio prompts to which a specific response is required to navigate a menu in a VR host) from a VR host. That captured audio is returned to the remote computer, and the voice server application residing on the remote computer then generates new signatures that will enable the voice server application to recognize such phrases at a later time.</p>
<p id="p-0161" num="0187">In a block <b>1304</b>, the host script prepared in block <b>1302</b>, and any other configuration information required to enable the server computer to gain access to the VR host, are sent via email to the server computer. When the server computer retrieves this email, the host script and information supplied by the voice server application residing at the remote computer are used by the server computer (running the voice server software and using the scripts and signatures sent by the remote computer) to call the remote VR host (i.e., the remote voicemail system), as indicated by a block <b>1310</b>. The server computer uses its voice modem to connect to the VR host. Once the connection is established, the server computer executes the host script (emailed from the remote user) in a block <b>1312</b>. The script enables the server computer to navigate the VR host to the point where the phrase to be learned begins. In a block <b>1316</b>, the server computer captures the audio containing the new phrase to be learned, as described above with respect to <figref idref="DRAWINGS">FIG. 9</figref>. Since the server computer does not know precisely where the phrase being learned ends, the script captures all the trailing audio (in the manner described above). In a block <b>1318</b>, the server computer terminates the connection, and then in a block <b>1320</b>, the server computer returns the captured audio (via email) to the voice server application residing at the remote computer. Once the captured audio has been retrieved by the remote computer (via email, as indicated in a block <b>1324</b>), it is processed in a block <b>1328</b> using the create signature tool, as described in conjunction with <figref idref="DRAWINGS">FIG. 12</figref>, to create a signature for the new phrase. In a block <b>1330</b>, the new signature and supporting data are added to the host script for the VR host to which the server computer is connected. The process of configuring a new host is normally a multi step process. In a decision block <b>1332</b>, the voice server application determines if additional phrases need be learned. If so, the process returns to block <b>1302</b>, and additional script is prepared to once again enable the server computer to capture a new phrase from the VR host. If, in decision block <b>1332</b>, it is determined that no more phrases need to be learned, then the modified host script is saved in a final version in a block <b>1334</b>. The process then terminates in a block <b>1336</b>.</p>
<p id="h-0021" num="0000">The Service</p>
<p id="p-0162" num="0188">As discussed above, the preferred embodiment consists of three elements. The voice server application has been described above. The second element is the Service, which is built around the voice server application to enable multiple users to access and manipulate their voicemail and other audio messages over the Internet. Thus, in one embodiment, the voice server application resides on one or more server computers, enabling a plurality of clients to access the functionality of the voice server application using the service. The following discussion relates to <figref idref="DRAWINGS">FIG. 1B</figref>, which schematically illustrates the service.</p>
<p id="p-0163" num="0189">By maintaining scripts for multiple hosts, a single voice server can serve multiple VR systems and multiple users simultaneously. For users sharing the same VR system, no new signatures need be learned. Only the users' passwords and telephone numbers, etc. need be substituted into the host script for their particular type of VR system.</p>
<p id="p-0164" num="0190">The service functions as an Internet service, with the primary user interface operating over the World Wide Web (although versions of the service could also function on private networks). Users pay for a subscription to the service, and each user has a private Webpage where the user can review and manage the user's voicemail messages. A user can set up an account to retrieve voicemail from any of the Voicemail services supported by the host scripts installed on voice servers <b>129</b> and <b>130</b> (as described above, voice servers <b>129</b> and <b>130</b> can be implemented on one or more computers that collectively make up spooling computer system <b>144</b>). Although the voice server application works fine over long distance, or even International telephone circuits, in its normal configuration, the service supports scripts for all public voicemail services, and any private scripts for commercial customers, all of whom can be reached by a local call from service center <b>141</b>. With the exception of the voice servers, each of which in a preferred embodiment are implemented on their own separate computer using the Windows&#x2122; operating system, all other functionality can be provided by a single computer running a Linux&#x2122; operating system. The Web interface is provided through a familiar and standard Web site server software package (e.g., the Apache&#x2122; Web site server software), and the service uses off-the-shelf components to complete the application, including a relational database, a scripting language (personal home page or PHP scripting language), and the Linux&#x2122; email system. Messages are stored as files in Linux-based message store <b>128</b>, and such messages are accessible by both Linux&#x2122; programs and the voice servers using a standard network file system (the Samba&#x2122; software is employed in a preferred embodiment of the present invention).</p>
<p id="p-0165" num="0191">A number of scripts and C++ programs run on the computer running the Linux&#x2122; operating system to interface between the Web site and the system control and configuration functions. The primary control function is to place jobs in the schedules of voice servers <b>129</b> and <b>130</b>. In addition, a preferred embodiment includes a C++ application that runs on the computing device running the Linux&#x2122; operating system and routes incoming messages. Those of ordinary skill in the art will recognize that such functionalities are standard with respect to spooling systems and can be implemented using a variety of techniques. The specific techniques described in a preferred embodiment of the present invention are not intended to be limiting. In such a spooling system, a queue of commands (the jobs queue) is generated by one application, and the queue is read and its commands are executed asynchronously by a second application. One advantage of the spooling system is that the two applications may function independently from each other, enabling their functions to spread across multiple computers without the need for sophisticated synchronization.</p>
<p id="p-0166" num="0192">Referring once again to <figref idref="DRAWINGS">FIG. 1B</figref>, Web interface <b>122</b> is the primary user interface with the service. The user uses a Web browser application <b>120</b> to communicate with the service. Once the user has completed a login step (a preferred embodiment uses subscriber's telephone numbers and voicemail PINs as the password), the user reaches the Voicemail homepage of the user. An exemplary homepage <b>2000</b> is illustrated in <figref idref="DRAWINGS">FIG. 20</figref>. The voicemail messages are displayed, one to a line, in a main frame <b>2030</b> of the page. Each message is tagged with a telephone number <b>2020</b> from which the message was retrieved, a time and date <b>2010</b> of retrieval, and a length <b>2009</b> of the message in minutes and seconds. A space <b>2007</b> is provided for each message so that messages can be given a textual memo by the user, or by the system. The user can play a message by clicking on a speaker icon <b>2006</b> to the right of the message. This action causes the user's installed streaming media player <b>142</b> (<figref idref="DRAWINGS">FIG. 1B</figref>) for MP3 files to start and play a stream of audio delivered by the service.</p>
<p id="p-0167" num="0193">Users may select one or more messages using checkboxes <b>2011</b> at the left of each message, and they may then apply various actions to those messages using the buttons <b>2002</b>, <b>2003</b>, <b>2004</b>, and <b>2005</b>, which perform the labeled action on the selected message(s). Selecting add Memo button <b>2002</b> enables the user to change the text memo associated with the selected message(s). Email button <b>2003</b> enables the user to forward the selected messages as attachments by email. Delete button <b>2004</b> moves the selected message(s) to a trash folder. Put in Folder button <b>2005</b> is a pull-down menu list of the folders displayed at the left of the page, in a frame <b>2012</b>. These folders are created by the user to manage the messages received by the user more easily. The saved and trash folders are provided by the operating system. All deleted messages are kept in the trash folder until the user affirmatively deletes them. A user may move between folders and have the messages displayed on the mainframe by clicking on the chosen folder, in frame <b>2012</b>. The new folder in frame <b>2012</b> leads to a user interface for managing folders.</p>
<p id="p-0168" num="0194">The user can also control message retrieval by the voice servers from their Webpage. Note that a frame <b>2013</b> (labeled Voicemail Boxes) of homepage <b>2000</b> indicates that three telephone numbers are supported in this exemplary account. By clicking on a telephone icon <b>2022</b> that is disposed next to the appropriate number, a user can initiate voicemail retrieval for that number. By pressing on a trashcan icon <b>2024</b> next to a number, a user can delete the messages still saved on that telephone voicemail account, using the voice server. A &#x201c;Retrieve All Voicemail&#x201d; button <b>2026</b> is provided to retrieve messages from all their telephone voicemail accounts in one step. Activation of buttons <b>2022</b>, <b>2024</b>, and <b>2026</b> causes the system to create jobs in the jobs queue of voice server <b>130</b> (<figref idref="DRAWINGS">FIG. 1B</figref>). The progress of any retrieval calls is displayed on a call status bar <b>2008</b>. Various configuration, help, and account administration functions are provided through tabs <b>2001</b>, on Webpage <b>2000</b>.</p>
<p id="p-0169" num="0195">Referring once again to <figref idref="DRAWINGS">FIG. 1B</figref>, Messages and commands can be sent into the system via the email gateway <b>125</b>. Audio messaging application <b>123</b> (described in detail below) can be used to send a message, composed on an Internet computing device, to email gateway <b>125</b> via email. If this message is correctly addressed, the message can be deposited in the Inbox of one of the service's users in message store <b>128</b>, or forwarded by telephone to an external telephone number via a job being placed on the job queue of the &#x201c;send by telephone&#x201d; voice server <b>129</b>. The job command includes a copy of the message to be sent.</p>
<p id="p-0170" num="0196">Telephone text messaging services can be used to send commands directly from mobile telephones <b>166</b> to the service using PSTN line <b>164</b>, via email gateway <b>125</b>. Typically, such commands are used to initiate the fetching of voicemail before the user is at their computer. This ability for users of the service to initiate retrieval remotely, without Internet access, enables the service to avoid polling users' voicemail accounts except when the users want their voicemail, but at the same time, enables the users' messages to be ready before they reach their computer. For example, users can send text messages to the Service from within their cars before they reach home, and the service will retrieve their messages, such that the messages are ready for review by the time the users arrive at their homes.</p>
<p id="p-0171" num="0197">Outgoing Internet email interface <b>127</b> enables two functions of the service. A first function relates to the forwarding of copies of messages by email, either on user demand, or automatically, as part of the service. For example, automatic email forwarding will enable a user to automatically receive copies of all voicemails for the user on the user's PDA. The second function of email interface <b>127</b> is to allow a user to automatically receive voicemail within the user's email client <b>126</b>. In the latter application, each user is provided with an email address on the service (e.g., 8088767787@gotvoice.com). Whenever a user retrieves email at this address (by calling the service over email interface <b>127</b>), the user will initiate a call that will retrieve voicemail saved for the user's telephone number(s). The user will thus receive an email with the voicemail messages included as attachment(s).</p>
<p id="p-0172" num="0198">Since the service enables its users to consolidate voicemail from multiple telephone accounts in one place, it functions as a universal voicemail service. In order to capitalize on this feature, the service itself offers a standard Voicemail system interface <b>140</b> to its users. In a preferred embodiment, voicemail system interface <b>140</b> is a standard Linux&#x2122; software package (Vgetty&#x2122;) that interfaces with message store <b>128</b>. Users dial-in using telephone <b>139</b> and PSTN line <b>135</b> to reach the service's voicemail access number and then listen to their messages, just as done with conventional voicemail system. However, the present invention enables each user to access all the user's voicemail, for all of the user's telephone accounts, with one call. Interface <b>140</b> provides all the standard telephone voicemail message review and management features, controlled from the telephone keypad.</p>
<p id="h-0022" num="0000">Send-by-Phone</p>
<p id="p-0173" num="0199">One of the functions of the service is a Send-by-Phone function. This functionality uses the voice server application differently. Instead of capturing audio, the voice server application plays audio down the telephone connection. The voice server calls the recipient of the message directly, even if they are not a subscriber to the service. The host script used to send the message can discriminate between the telephone being answered by a human and one answered by a machine. When the telephone is first answered, the voice server plays a message such as &#x201c;press star for an important message from &#x3c;whomever&#x3e;.&#x201d; If a human answers and presses the * key on their telephone, the human will hear the message directly. If however, the incoming audio is interrupted by a beep, the voice server starts playback and leaves the message on the recipient's voicemail or answering service telephone. If no star key is pressed and there is no beep, the message is retained and the call is attempted again at a later time. The above sequence is very important, because it minimizes the annoyance to the recipient and ensures delivery of the voicemail. In order to make send-by-phone function in this manner, two additional recognition features of the preferred embodiment are used. The first allows the host script to distinguish between spoken voice and machine generated tones (i.e., beeps). By placing the statement &#x201c;expect Voice&#x201d; in the script, the associated actions will be executed whenever human speech is heard by the voice server. If the statement &#x201c;expect Tone&#x201d; is placed in the script, then the associated actions will be executed whenever a tone (of any frequency) is heard. Tables 1-3 provide examples of other expect statements, and the &#x201c;expect Voice&#x201d; and &#x201c;expect Tone&#x201d; statements are prepared in a similar manner. These functions are implemented in the voice server as built-in signatures that are triggered based on the number of frequency peaks in the incoming audio. If the number of frequency peaks in the DFT of the incoming audio falls below a threshold, then &#x2018;expect Tone&#x2019; is triggered. If the number of frequency peaks in the DFT exceeds a certain threshold, then &#x2018;expect Voice&#x2019; is triggered. In a preferred embodiment the value 6 (i.e., 6 peaks) is used as the threshold for Tone recognition and the value 20 (i.e., 20 peaks) is used as the threshold for Voice recognition, as speech normally includes more spectral peaks than does a machine generated tone or beep. The second feature which supports send by phone is the ability of the host script to be triggered by an incoming DTMF tone from the user (e.g., */star in the above example). In order to recognize a particular DTMF tone, the statement &#x2018;exdtmf &#x3c;tone&#x3e;&#x2019;, where &#x3c;tone&#x3e; is any single DTMF character (0123456789*#ABCD), is used. When the user enters the &#x201c;A&#x201d; DTMF tone, the actions associated with any corresponding exdtmf clause are executed.</p>
<p id="p-0174" num="0200">As discussed above, it is possible to compose messages using an Internet appliance (such as computing device executing Audio messaging application <b>123</b>) on the Internet, and then forward these messages to the service over Internet connection <b>124</b>, via email gateway <b>125</b>. Such messages can be routed to message store <b>128</b>, and either retained there until the recipient retrieves them, or the messages can be sent by telephone via voice server <b>129</b>, as described above. When coupled with mailing lists comprising multiple telephone numbers, the send by telephone service can be used to construct interesting vertical applications, for example, in the field of telemarketing.</p>
<p id="h-0023" num="0000">The Flow of Messages within the Service Center</p>
<p id="p-0175" num="0201">The messages arrive in service center <b>141</b> by two means: either as email (via email gateway <b>125</b>) or by telephone (via voice server <b>130</b>). If the messages arrive by email, they are distributed by a program running on the mail gateway's input, directly into message store <b>128</b> or placed into the outgoing message job queue of Send-by-Phone voice server <b>129</b>. If the messages arrive by telephone, they arrive in a directory (preferably named the &#x201c;arrival directory&#x201d;) owned by the voice server and accessible by the computer running the Linux&#x2122; operating system, over the network. A routine runs periodically (preferably every minute) on the Linux computer and checks for any new messages in the arrival directory. A time stamp of the last check is used to detect new files, and a lock file is used by the voice server to lock out the Linux program during file creation, when there is a danger of copying partial messages. Each message consists of a WAV file containing the message in uncompressed PCM audio format, and a meta-file containing the routing information for the message, its time of retrieval, its length, and other housekeeping data for the message. If a new message is found, the Linux program encodes the audio from the WAV file into another file in compressed MP3 format. This MP3 file is moved directly to the message store directory of the intended recipient. The newly arrived message can then be viewed with Web interface <b>122</b>. This method has two advantages: (1) the interface is simple and asynchronous, making the system simpler and more reliable; and, (2) keeping copies of the original messages in the arrival directory provides for redundancy and further improves the system's overall reliability.</p>
<p id="h-0024" num="0000">The Audio Messenger</p>
<p id="p-0176" num="0202">The third element of a preferred embodiment of the present invention is the audio messenger application (see <figref idref="DRAWINGS">FIG. 1B</figref>, audio messaging application <b>123</b>). Audio messenger application <b>123</b> is a simple popup application that runs on the user's Internet connected computing device. This device should be equipped with a microphone and audio playback capabilities, typically provided through headphones <b>143</b>.</p>
<p id="p-0177" num="0203">Using audio messaging application <b>123</b>, the user may record new audio voice-messages locally and then send them to the service via email gateway <b>125</b>. These messages are delivered as described above and can be routed to either message store <b>128</b>, or to the send-by-telephone job queue in voice server <b>129</b>. An exemplary Windows&#x2122; operating system version of a user interface <b>1800</b> for Audio messaging application <b>123</b> is shown in <figref idref="DRAWINGS">FIG. 18</figref>. A preferred embodiment of Audio messenger was written in the C++ programming language and has been designed to be ported to multiple computer platforms. The user interface includes the following elements:
<ul id="ul0016" list-style="none">
    <li id="ul0016-0001" num="0000">
    <ul id="ul0017" list-style="none">
        <li id="ul0017-0001" num="0204">A record button <b>1801</b> is used to start recording a message entered through the microphone. Each time record button <b>1801</b> is pressed, the old (previously recorded) message is overwritten.</li>
        <li id="ul0017-0002" num="0205">A play/stop button <b>1802</b> is used during playback to stop playback of the audio. If a message has already been recorded and the stop button pressed, then this button displays a play icon (&#x3e;), and pressing the button starts playback of the recorded audio. Thus, when audio is playing, this button functions as a stop control and when audio has been recorded but is not currently playing, it functions as a play button.</li>
        <li id="ul0017-0003" num="0206">An audio progress indicator <b>1803</b>. When audio is being recorded or played back, this indicator is animated to provide feedback to the user showing the extent of the message (or relative position within a recorded message that is being played).</li>
        <li id="ul0017-0004" num="0207">A Memo field <b>1813</b> is provided to enable a user to type a text memo to appear with the delivered message (if delivered directly into the message store).</li>
        <li id="ul0017-0005" num="0208">An Address pull-down <b>1812</b> contains a list of addresses entered in the address book by the user. Entries in the address book preferably include three elements: the address name (e.g. John Smith); the addressee's telephone number (e.g. 8088767766); and (optionally) the addressee's email address. Entries are added to the address book using a + button <b>1811</b>, which displays a dialog box that enables a new address entry to be added. Entries may be edited using a = button <b>1810</b>, which enables the currently selected address book entry to be edited and re-saved. A &#x2212;button <b>1809</b> is used to delete a selected address book entry.</li>
        <li id="ul0017-0006" num="0209">A send button <b>1805</b> dispatches a correctly recorded and addressed message to the service, via email.</li>
        <li id="ul0017-0007" num="0210">A setup button <b>1806</b> displays a dialog box for use in setting up the application. This setup process involves providing the application with personal preferences and login information for the different voice hosts.</li>
        <li id="ul0017-0008" num="0211">A by phone checkbox <b>1807</b>, if checked, directs the service to attempt to send the message over the telephone using the &#x201c;send-by-telephone&#x201d; service of voice server <b>129</b> (<figref idref="DRAWINGS">FIG. 1B</figref>). If this checkbox is unchecked, an attempt is make to deliver the message into message store <b>128</b> (<figref idref="DRAWINGS">FIG. 1B</figref>).</li>
        <li id="ul0017-0009" num="0212">A hifi checkbox <b>1808</b> enables the user to direct the system to encode the message at a higher fidelity than that used for telephone messages. If this checkbox is checked, then the message is encoded in the higher quality format, which enables messages containing, for example, music or a high quality speech recording, to be sent to the service without the loss of fidelity associated with passage over a telephone voice circuit. This option has no effect on the send-by-phone functionality. Normally, a preferred audio messenger application <b>123</b> encodes messages in a 16 kbps, monaural MP3 format. If the hifi checkbox is set, then they are encoded in a 64 kbps monaural format.
<br/>
Recording and Sending a Message
</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0178" num="0213">A flowchart of the process of recording and sending a voice-message with the Audio messenger is shown in <figref idref="DRAWINGS">FIG. 19</figref>. This process starts at a block <b>1900</b> when the audio messenger application is started. In a decision block <b>1902</b>, the audio messaging application <b>123</b> checks to see if there are any messages saved from the last (offline) session. If no messages are saved, the next step in the process is to wait for the user to record a message, as indicated in a block <b>1906</b>. If there are saved messages ready to send, the audio messaging application makes an attempt to send them via email to the appropriate gateway, at a block <b>1904</b>. Each branch from decision block <b>1902</b> leads to block <b>1906</b>. In order to record a message to be sent, the user uses record button <b>1801</b> (<figref idref="DRAWINGS">FIG. 18</figref>) to start recording, and &#x201c;stop&#x201d; button <b>1802</b> (<figref idref="DRAWINGS">FIG. 18</figref>) to stop the recording when finished. The manipulation of buttons <b>1801</b> and <b>1802</b> correspond to block <b>1906</b>.</p>
<p id="p-0179" num="0214">Once the message has been recorded, it can be reviewed in a block <b>1908</b>, using stop/play button <b>1802</b> (<figref idref="DRAWINGS">FIG. 18</figref>). In a decision block <b>1910</b> the user determines whether the message is satisfactory. If the message is not satisfactory, a new message can be recorded (over the old message), as noted above in a block <b>1906</b>. Of course, should a user wish to skip the evaluation of decision block <b>1910</b>, a user can proceed directly to the next step.</p>
<p id="p-0180" num="0215">If the message is satisfactory, the user can enter a short text memo in a block <b>1911</b>, which will be delivered to the service with the message. Such entry is optional. In a block <b>1912</b>, the message is addressed by selecting an entry from an address pull-down list box <b>1812</b> (<figref idref="DRAWINGS">FIG. 18</figref>). If necessary a new address is added to the address book first using + button <b>1811</b> (<figref idref="DRAWINGS">FIG. 18</figref>). Once the message has been addressed, the user selects any options, such as hifi or send-by-telephone in a block <b>1913</b>, to prepare the message for delivery. Once any options desired have been selected, an attempt to send the message is made in a block <b>1914</b>, using send button <b>1805</b> (<figref idref="DRAWINGS">FIG. 18</figref>).</p>
<p id="p-0181" num="0216">In a decision block <b>1915</b>, the audio messaging application determines if the gateway needed to send the message is accessible. If so, then in a block <b>1918</b>, the message is sent by email to service email gateway <b>125</b> (<figref idref="DRAWINGS">FIG. 1B</figref>). If the service email gateway is not accessible, then in a block <b>1916</b>, the message is saved locally for sending when the gateway is next available (see block <b>1904</b>).</p>
<p id="p-0182" num="0217">In a decision block <b>1920</b>, the logic determines if the user desires to send another message. If so, control passes back to block <b>1906</b> to wait for the user to record another message. If no more messages are to be sent, the user terminates the Audio messenger program, as indicated by a block <b>1922</b>.</p>
<p id="h-0025" num="0000">Addressing and Routing by the Audio Messenger</p>
<p id="p-0183" num="0218">In a preferred embodiment of the present invention, the service element is implemented using multiple service centers, similar to service center <b>141</b> of <figref idref="DRAWINGS">FIG. 1B</figref>. <figref idref="DRAWINGS">FIG. 11</figref> shows an implementation of the service element that includes three service centers <b>1100</b>, <b>1102</b>, and <b>1104</b>. Each service center serves a different area code. One service center per local calling area is required to enable messages to be retrieved and delivered by telephone at local calling rates. (For the sake of this example, it is assumed that each area-code corresponds to a local calling area for rate purposes).</p>
<p id="p-0184" num="0219">Each service center, also known as a point-of-presence, or POP, supports all the accounts for telephone numbers within its calling area and also serves as the retrieval and dispatch point for all voice-messaging within the calling area. Voice Messaging, as used herein, refers to the generalized function of sending voicemail messages or messages recorded using audio messenger <b>1106</b> by telephone or Internet. Audio messenger <b>1106</b> has the same functionality as audio messenger application <b>123</b> of <figref idref="DRAWINGS">FIG. 1B</figref>, and is intended to represent audio messenger applications residing on a plurality of Internet-connected user computer devices. Each POP contains at least one voice server performing those functions, and each POP also includes an email gateway function (see email gateway <b>125</b> of <figref idref="DRAWINGS">FIG. 1B</figref>) for its calling area.</p>
<p id="p-0185" num="0220">If a message is to be sent from audio messenger <b>1106</b>, then it must be directed at the right POP gateway (i.e., to the POP gateway for the recipient's local call area-code). There is no central email gateway, and the various service centers function independently of each other. Messages are routed according to their area-codes and the telephone number part of the address is therefore the critical element. Each POP is represented on the Internet by an Internet hostname corresponding to the area code (or codes) it supports. By convention these service centers are named &#x3c;area-code&#x3e;.&#x3c;service domain&#x3e;. Therefore, if the service domain is gotvoice.com, then the three POPs illustrated in <figref idref="DRAWINGS">FIG. 11</figref> have the hostnames as indicated (i.e., 206.gotvoice.com, 425.gotvoice.com, and 808.gotvoice.com). Each of these service centers has a special receiving email address to which messages are directed by audio messenger <b>1106</b>. Thus, messages for area code 206 telephone numbers (<b>1112</b>) are sent to receiving @206.gotvoice.com, messages for area code 425 telephone numbers (<b>1110</b>) are sent to receiving @425.gotvoice.com, and messages for area code 808 telephone numbers (<b>1108</b>) are sent to receiving @808.gotvoice.com.</p>
<p id="p-0186" num="0221">It is the function of Audio messenger <b>1106</b> to route messages directly. If the area code of the recipient is known, then the Audio messenger can correctly address the message and send it to the correct service center. The routing is implicit in the addressing scheme, and there is no need for any directory or routing infrastructure other that that provided by the Internet's base services (e.g., the DNS service).</p>
<p id="h-0026" num="0000">Enhancements to the Audio Messenger</p>
<p id="p-0187" num="0222">Although a preferred embodiment of the present invention that will be commercially employed does not yet include the following functions, they are expected to be added later, to provide enhanced desirable functionality for the present invention. These functions include:
<ul id="ul0018" list-style="none">
    <li id="ul0018-0001" num="0000">
    <ul id="ul0019" list-style="none">
        <li id="ul0019-0001" num="0223">Providing subscriber specific address books at service centers. Although the user's address book may be stored and maintained locally on the computer where the user runs the audio messenger, providing a centralized address book service, connected to the service, will enable the user access to their address book from any location (or from any device), in a similar fashion to the buddy lists of popular Instant Messenger applications. This facility is of great advantage to a user, since the user need not carry a device in which the address book is stored.</li>
        <li id="ul0019-0002" num="0224">Providing versions of the audio messenger application compatible with other operating systems will provide other options. By doing so, the voice-messages need not be limited to a personal computer or a laptop computer platform. For example, some PDAs and some smart telephones include dictation functionality. In order to make voice messaging as ubiquitous as possible, it is contemplated that versions of the audio messenger application will be provided for such platforms, including without limitation, computing devices running Microsoft Corporation's Pocket PC&#x2122; OS, those running the Palm OS&#x2122;, Linux&#x2122; or the Symbian&#x2122; OS.</li>
        <li id="ul0019-0003" num="0225">Providing import functionality to the audio messenger application, such that in addition to recording messages directly using a microphone, the user may also import WAV and MP3 files into the audio messenger, for delivery using the service. These formats have been chosen for their ubiquity, however those of ordinary skill in the art will recognize that many other formats could be used with minimal modifications to the preferred embodiment. Thus the identified formats are not intended to limit the invention.</li>
        <li id="ul0019-0004" num="0226">Providing multimedia functionality. For example, enabling video messages recorded with a WebCam to be sent to the service gateway. Incorporating video playback capability to the service Web site, and adding video messaging to the service represent straightforward extensions of the technology described above.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0188" num="0227">It should be understood that while in a preferred embodiment of the present invention, the service and the Audio messenger are configured to prepare and deliver audio files, the techniques and elements described above can also be employed to exchange files in virtually any digital format. It is also important to note that although the telephone interface will remain audio oriented, the Internet interfaces need not. In circumstances where a multimedia message is to be sent by telephone, the system can potentially deliver only the audio portion of the message, enabling a recipient to participate in the communication, albeit at a reduced capability. However, full enjoyment of the message will still be available to a user having access to a computer that provides other visual and/or audio capabilities for playing the message.</p>
<p id="h-0027" num="0000">Facilitating Communication Between Two Computers Using the Present Invention</p>
<p id="p-0189" num="0228">In a preferred embodiment described above, the method of the invention is used by a first computer to communicate with a second computer (such as a VR system), where the second computer does not implement the present invention. One additional embodiment of the present invention is directed to two computers that each implement the present invention. When both computers are configured to utilize the present invention, those two computers can be connected using an audio communication channel, such as a telephone line. This embodiment is illustrated in <figref idref="DRAWINGS">FIGS. 2 and 14</figref>. In <figref idref="DRAWINGS">FIG. 14</figref>, an operator/sender <b>1400</b> (human or mechanical) inputs the ASCII text &#x201c;HELLO&#x201d; into a capture text program in a block <b>1404</b>, which creates an audio stream encoding message <b>1402</b> (i.e., HELLO) as a sequence of audio clips or segments, as indicated by a block <b>1406</b>. The individual audio clips of the sequence are based on a library <b>1408</b> of stored audio clips, or &#x201c;words&#x201d;. In the example of <figref idref="DRAWINGS">FIG. 14</figref>, it is assumed that each letter in the Roman alphabet is represented by its audio equivalent from the international telephonetic alphabet (i.e., &#x201c;A&#x201d; is represented by the spoken word &#x201c;alpha,&#x201d; &#x201c;B&#x201d; by &#x201c;bravo,&#x201d; etc.). As will be described in more detail below, the specific audio signal employed to represent a particular text entry can be abstract, as long as the system corresponds a specific audio signal to a specific text entry.</p>
<p id="p-0190" num="0229">A call is made to the remote computer using the telephones <b>1410</b> and <b>1414</b>, and audio sequence <b>1412</b> (encoding &#x201c;HELLO&#x201d;) is played across the telephone connection linking telephones <b>1410</b> and <b>1414</b>. In this example, the sequence for HELLO comprises the words: &#x201c;hotel&#x201d; . . . &#x201c;echo&#x201d; . . . &#x201c;lima&#x201d; . . . &#x201c;lima&#x201d; . . . &#x201c;oscar&#x201d; . . . .</p>
<p id="p-0191" num="0230">Using the method of the present invention, the second computer recognizes the incoming words/phrases in a block <b>1416</b>, using a library <b>1418</b> of signatures/DFTs (corresponding to the words stored in the sender's library <b>1408</b>), and a script recognition program <b>1420</b> (based on the voice server application described above). When &#x201c;hotel&#x201d; is received by the second computer over the audio communication link, the process in block <b>1416</b> involves generating a DFT of the incoming audio, and then comparing that incoming DFT with each DFT stored in library <b>1418</b>, enabling the second computer to identify the text entry corresponding to the audio signal (in this case, an &#x201c;H&#x201d; text entry corresponds to the audio signal &#x201c;hotel&#x201d;). As the incoming audio signals are recognized, corresponding text is generated in a block <b>1422</b>, to be communicated to operator/receiver <b>1426</b>, for example, on a display or as an audible word <b>1424</b>.</p>
<p id="p-0192" num="0231">In the example, both the computers are operating in a full-duplex configuration. Each computer has available a library of audio signals that correspond to a specific text entry, and a library of DFTs corresponding to every audio signal that corresponds to a text entry. Thus each computer can convert a text entry into an audio signal, and use the DFT library to recognize an audio signal to recreate a text entry corresponding to that audio signal. Thus operator/receiver <b>1426</b> can not only receive messages, but can also send messages back to operator/sender <b>1400</b>, using the method described above. Thus operator/receiver <b>1426</b> can use the second computer to capture a word <b>1428</b> as text (as indicated in block <b>1430</b>), and employ a library <b>1434</b> to create an audio stream of sequences in a block <b>1432</b>. That sequence <b>1438</b> is then sent from telephone <b>1414</b> to telephone <b>1410</b>. To enable operator/sender <b>1400</b> to decode sequence <b>1438</b> in a block <b>1442</b>, the first computer (i.e., the computer being used by operator/sender <b>1400</b>) will need to include a library <b>1450</b> of signatures/DFTs, and a recognition program <b>1444</b>.</p>
<p id="p-0193" num="0232">In the above example, there was a clear correlation between the audio signal (i.e., &#x201c;hotel&#x201d;) and a text entry (i.e., &#x201c;H&#x201d;). It should be understood that the correlation could be entirely arbitrary, enabling coded messages to be sent and received. As long as each computer coupled by an audio link includes matching libraries that correspond audio signals to text, and DFTs to audio signals, communication over an audio link is facilitated. It should also be recognized that in a broadest sense an audio signal does not need to be linked to a single letter of text; rather each audio signal can be linked to a specific data token. Each data token could correspond to a word, a phrase, a sentence, etc.</p>
<p id="p-0194" num="0233">Although the present invention has been described in connection with the preferred form of practicing it and modifications thereto, those of ordinary skill in the art will understand that many other modifications can be made to the invention within the scope of the claims that follow. Accordingly, it is not intended that the scope of the invention in any way be limited by the above description, but instead be determined entirely by reference to the claims that follow.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>The invention in which an exclusive right is claimed is defined by the following:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method for interacting with a voice response (VR) system accessible via at least one connection, where the VR system provides audio command prompts to which appropriate responses must be made in order to successfully interact with the VR system, the method comprising the steps of:
<claim-text>(a) connecting a computing device including an interaction management application to the VR system using said at least one connection, wherein said at least one connection is selected from the group consisting of a telephonic connection and a network connection;</claim-text>
<claim-text>(b) receiving an audio communication from the VR system, comprising: storing the audio communication in at least one audio buffer having a size N; and separating each audio buffer into a plurality of window buffers, comprising:</claim-text>
<claim-text>dividing each audio buffer into X identically sized sample windows, where X is equal to N divided by W, each sample window being of size W, such that each sample window includes a whole number of samples, and X is a positive whole number; and</claim-text>
<claim-text>iteratively generating X window buffers using the sample windows, each window buffer being of the size N, such that each window buffer comprises X sample windows, and each sequential window buffer includes one sample window not present in a preceding window buffer;</claim-text>
<claim-text>(c) generating at least one discrete Fourier transform (DFT) for the audio communication that was received comprising generating a DFT for each window buffer;</claim-text>
<claim-text>(d) comparing the at least one DFT with known DFTs, comprising comparing each window buffer DFT with at least one of the known DFTs, each known DFT corresponding to a command prompt likely to be received from the VR system;</claim-text>
<claim-text>(e) providing the VR system any required response, if an acceptable level of correlation exists between said at least one DFT for the audio communication that was received and a known DFT; and</claim-text>
<claim-text>(f) repeating the steps defined in subparagraphs (b)-(e) until a desired interaction has been achieved between the computing device and the VR system.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising the step of teaching the computing device how to recognize and respond to each command prompt likely to be received from the VR system.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the step of teaching the computing device how to recognize and respond to each command prompt comprises the steps of:
<claim-text>(a) establishing a logical connection between the computing device and the VR system;</claim-text>
<claim-text>(b) receiving an audio communication comprising a command prompt from the VR system;</claim-text>
<claim-text>(c) generating at least one DFT based on the command prompt that was received;</claim-text>
<claim-text>(d) enabling a user to indicate the correct response to the command prompt;</claim-text>
<claim-text>(e) storing the DFT corresponding to the command prompt and a program script enabling the computing device to duplicate the correct response; and</claim-text>
<claim-text>(f) eating the steps defined in <claim-ref idref="CLM-00001">claim 1</claim-ref>, subparagraphs (b)-(e), until a DFT and program script have been stored for all command prompts likely to be received from the VR system.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step of storing the communication in at least one audio buffer comprises the steps of;
<claim-text>(a) providing two identically sized audio buffers, each sized to accommodate N samples, N being selected to achieve a desired time resolution; and</claim-text>
<claim-text>(b) sequentially filling each audio buffer with N samples of the audio communication, such that a first audio buffer is filled with relatively older samples, and a second audio buffer is filled with relatively newer samples, in time.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the VR system is an audio message service, and wherein the desired interaction comprises retrieving audio messages for a user from the audio message service.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, further comprising the step of generating a key for each message received from the message service, said key being stored in association with the message.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the step of generating a key for each message comprises the steps of:
<claim-text>(a) generating a DFT of the message; and</claim-text>
<claim-text>(b) as a function of the DFT, generating a unique key.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising the steps of checking the key for each message received against each key that was stored, and ignoring each message whose key matches a stored key, because such a match indicates that the message has previously been retrieved.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. A system for automatically interacting with a voice response (VR) system, to achieve a desired interaction with the VR system, comprising:
<claim-text>(a) a memory in which a plurality of machine instructions defining a retrieval application are stored, said memory also storing a plurality of known discrete Fourier transforms (DFTs), each DFT corresponding to a command prompt likely to be received from the VR system; and</claim-text>
<claim-text>(b) a processor that is coupled to the memory to access the machine instructions, said processor executing said machine instructions and thereby implementing a plurality of functions, including:
<claim-text>(i) establishing a logical connection with the VR system;</claim-text>
<claim-text>(ii) receiving an audio communication from the VR system, comprising:
<claim-text>storing the audio communication in at least one audio buffer having a size N; and</claim-text>
<claim-text>separating each audio buffer into a plurality of window buffers, comprising:
<claim-text>dividing each audio buffer into X identically sized sample windows, where X is equal to N divided by W, each sample window being of size W, such that each sample window includes a whole number of samples, and X is a positive whole number; and</claim-text>
<claim-text>iteratively generating X window buffers using the sample windows, each window buffer being of the size N, such that each window buffer comprises X sample windows, and each sequential window buffer includes one sample window not present in a preceding window buffer;</claim-text>
</claim-text>
</claim-text>
<claim-text>(iii) generating at least one DFT for the audio communication, comprising generating a DFT for each window buffer;</claim-text>
<claim-text>(iv) comparing the at least one each window buffer DFT with at least one known DFT, comprising comparing each window buffer DFT with at least one of the known DFTs, each known DFT corresponding to a different command prompt from a plurality of command prompts likely to be received from the VR system;</claim-text>
<claim-text>(v) if an acceptable level of correlation exists between at least one DFT one of the window buffer DFTs and a known DFT, then providing the VR system with any required response, said machine instructions comprising a program script required to generate any required response associated with each known DFT; and</claim-text>
<claim-text>(vi) repeating the steps defined in subparagraphs (ii)-(v) until the desired interaction is achieved.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step of connecting further comprises the steps of:
<claim-text>(a) loading data necessary to communicate with the VR system, wherein the data includes:
<claim-text>(i) a host script including at least one expect clause; and</claim-text>
<claim-text>(ii) a signature file; and</claim-text>
</claim-text>
<claim-text>(b) initiating a telephonic communication with the VR system.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising the step of carrying out at least one step selected from the group of steps consisting of:
<claim-text>(a) terminating the connection when the desired interaction has been achieved;</claim-text>
<claim-text>(b) starting a capture of the audio communication;</claim-text>
<claim-text>(c) stopping the capture of the audio communication;</claim-text>
<claim-text>(d) sending a DTMF code sequence over the telephonic connection to instruct the VR system to store a message; and</claim-text>
<claim-text>(e) saving a message to a message store.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the plurality of machine instructions cause the processor to display a graphical user interface that includes a plurality of controls, in order to enable a user to interact with the VR system.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the plurality of machine instructions further define an audio messenger application that is coupled to a microphone and a listening device, said machine instructions causing the processor to enable a user to record a message and send the message to a service that interacts with the VR system.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. A non-transitory computer readable storage medium on which machine readable instructions are stored, which when executed by a computing device cause the computing device to carry out a plurality of functions, such that interaction with a voice response (VR) system is accessible via at least one connection, where the VR system provides audio command prompts to which appropriate responses must be made in order to successfully interact with the VR system, said plurality of functions including:
<claim-text>(a) connecting a computing device including an interaction management application to the VR system using said at least one connection, wherein said at least one connection is selected from the group consisting of the a telephonic connection and the a network connection;</claim-text>
<claim-text>(b) receiving an audio communication from the VR system, comprising:
<claim-text>storing the audio communication in at least one audio buffer having a size N; and</claim-text>
<claim-text>separating each audio buffer into a plurality of window buffers, comprising:
<claim-text>dividing each audio buffer into X identically sized sample windows, where X is equal to N divided by W, each sample window being of size W, such that each sample window includes a whole number of samples, and X is a positive whole number; and</claim-text>
<claim-text>iteratively generating X window buffers using the sample windows, each window buffer being of the size N, such that each window buffer comprises X sample windows, and each sequential window buffer includes one sample window not present in a preceding window buffer;</claim-text>
</claim-text>
</claim-text>
<claim-text>(c) generating at least one discrete Fourier transform (DFT) for the audio communication that was received comprising generating a DFT for each window buffer;</claim-text>
<claim-text>(d) comparing the at least one DFT with known DFTs, comprising comparing each window buffer DFT with at least one of the known DFTs, each known DFT corresponding to a command prompt likely to be received from the VR system;</claim-text>
<claim-text>(e) providing the VR system any required response, if an acceptable level of correlation exists between said at least one DFT for the audio communication that was received and a known DFT; and</claim-text>
<claim-text>(f) repeating the steps defined in subparagraphs (b)-(e) until a desired interaction has been achieved between the computing device and the VR system.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
