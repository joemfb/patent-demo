<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626236-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626236</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12901209</doc-number>
<date>20101008</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<us-term-of-grant>
<us-term-extension>127</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>M</subclass>
<main-group>1</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>4555561</main-classification>
<further-classification>382217</further-classification>
<further-classification>382182</further-classification>
</classification-national>
<invention-title id="d2e53">System and method for displaying text in augmented reality</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>6092906</doc-number>
<kind>A</kind>
<name>Olmstead</name>
<date>20000700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6870529</doc-number>
<kind>B1</kind>
<name>Davis</name>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>7305435</doc-number>
<kind>B2</kind>
<name>Hamynen</name>
<date>20071200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>7672543</doc-number>
<kind>B2</kind>
<name>Hull et al.</name>
<date>20100300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>8401335</doc-number>
<kind>B2</kind>
<name>Huang</name>
<date>20130300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382182</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2006/0013444</doc-number>
<kind>A1</kind>
<name>Kurzweil et al.</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382114</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2008/0119236</doc-number>
<kind>A1</kind>
<name>Chen et al.</name>
<date>20080500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2008/0317346</doc-number>
<kind>A1</kind>
<name>Taub</name>
<date>20081200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382182</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2009/0085485</doc-number>
<kind>A1</kind>
<name>Young</name>
<date>20090400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2009/0135333</doc-number>
<kind>A1</kind>
<name>Tai et al.</name>
<date>20090500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2009/0289175</doc-number>
<kind>A1</kind>
<name>Mahowald et al.</name>
<date>20091100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2010/0172590</doc-number>
<kind>A1</kind>
<name>Foehr et al.</name>
<date>20100700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382217</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2010/0212478</doc-number>
<kind>A1</kind>
<name>Taub et al.</name>
<date>20100800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification> 84645</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2010/0329555</doc-number>
<kind>A1</kind>
<name>Chapman et al.</name>
<date>20101200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382167</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>EP</country>
<doc-number>2144189</doc-number>
<kind>A2</kind>
<date>20100100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>WO</country>
<doc-number>0246907</doc-number>
<kind>A2</kind>
<date>20020600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>WO</country>
<doc-number>03032237</doc-number>
<kind>A1</kind>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>WO</country>
<doc-number>2008114104</doc-number>
<kind>A1</kind>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00019">
<othercit>ABBYY Fine Reader OCR, document and PDF conversion application&#x2014;Full Feature List; http://www.abbyyusa.com/finereader/full<sub>&#x2014;</sub>feature<sub>&#x2014;</sub>list; retrieved from the internet Apr. 16, 2010.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00020">
<othercit>ABBY Product Overview; http://www.abbyy.com/mobileocr/overview/; retrieved from the internet Apr. 16, 2010.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00021">
<othercit>ABBYY OCR and Image Processing; http://www.abbyy.com/support/mobileocr/30/faq/doc<sub>&#x2014;</sub>process; accessed Apr. 16, 2010; retrieved from the internet Nov. 16, 2010.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00022">
<othercit>ABBYY Business Card Reader&#x2014;mobile application for recognition contact information; http://www.abbyy.com/bcr/; retrieved from the internet Apr. 16, 2010.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00023">
<othercit>ABBYY Business Card Reader , Screenshots &#x26; Demo; http://www.abbyy.com/bcr/screenshots/; retrieved from the internet Apr. 16, 2010.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00024">
<othercit>ABBYY Foto Translate; http://www.abbyy.com/fototranslate/; retrieved from the internet Apr. 16, 2010.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00025">
<othercit>Samsung announces first cellphone with night vision video; http://dvice.com/archives/2009/07/samsung-announc-1.php; accessed at least as early as Mar. 1, 2010.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00026">
<othercit>ATN: How Night Vision Works&#x2014;Night Vision Goggles, Night Vision Scopes, Binoculars, Rifelescopes . . . ; http://www.atncorp.com/hownightvisionworks; accessed at least as early as Mar. 1, 2010.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00027">
<othercit>Kuo, S.S., Ranganath, M.V.; &#x201c;Real time image enhancement for both text and color photo images&#x201d;; Proceedings of the 1995 International Conference on Image Processing (ICIP95), Washington D.C.; Oct. 23 to 26, 1995; pp. 159 to162; vol. 1; IEEE.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00028">
<othercit>Baumann, S. et al.; &#x201c;Message Extraction from Printed Documents: A Complete Solution&#x201d;; Proceedings of the 4th International Conference on Document Analysis and Recognition (ICDAR 97); 1997; pp. 1055 to 1059; IEEE.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00029">
<othercit>Wu, V. et al.: &#x201c;Automatic Text Detection and Recognition&#x201d;; Proceedings of Image Understanding Workshop (DARPA97); 1997; pp. 707 to 712.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00030">
<othercit>Papadimitrious, Spiros (spapadim); &#x201c;WordSnap OCR (Version 43)&#x201d;; Apr. 3, 2010; http://www.bitquill.net/trac/wiki/Android/OCR?version=43.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00031">
<othercit>Papadimitrious, Spiros (spapadim); &#x201c;WordSnap OCR&#x201d;; Aug. 22, 2009; http://www.youtube.com/watch?v=73jqb0EMA4.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00032">
<othercit>Papadimitrious, Spiros (spapadim); &#x201c;WordSnap OCR&#x2014;Live Capture&#x201d;; Aug. 28, 2009; http://www.youtube.com/watch?v=GhuOmn6s.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00033">
<othercit>Papadimitrious, Spiros (spadim); &#x201c;Mobile OCR input: &#x201c;Fully automatic&#x201d; and reality&#x201d;; Sep. 2009; www.bitquill.net/blog/?m=200909.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00034">
<othercit>Bissacco A. et al.; &#x201c;Translate the real world with Google Goggles&#x201d;; May 6, 2010 http://googlemobilt.blogspot.com/2010/05/translate-the-real-world-with-google.html.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00035">
<othercit>Neven, H.; &#x201c;Integrating translation into Google Goggles&#x201d;; Feb. 17, 2010; http://googlemobile.blogspot.com/2010/02/integrating-translation-into-google.html.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00036">
<othercit>GoogleVideos: &#x201c;Translation in Google Goggles Prototype&#x201d;; Feb. 17, 2010; http://www.youtube.com/watch?v=ae01yz5z99E.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00037">
<othercit>Linder, B.; &#x201c;Goggle Goggles Android app now translates text&#x201d;; May 6, 2010; http://www.youtube.com/watch?v=tXcaKPTxQHg.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00038">
<othercit>Kuo, S-S, et al.; &#x201c;Real Time Image Enhancement for Both Text and Color Photo Images&#x201d;; Proceedings of the International Conference on Image Processing (ICIP); 1995; Los Alamitos, U.S.A.; Oct. 23, 1995; pp. 159 to 162; vol. 1; IEEE Computer Society Press.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00039">
<othercit>Doermann, D. et al.; &#x201c;Progress in Camera-Based Document Image Analysis&#x201d;, Proceedings of the Seventh International Conference on Document Analysis and Recognition (ICDAR'03); 2003; Piscataway, NJ, U.S.A.; Aug. 3, 2003; pp. 606-616; IEEE Computer Society Press.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00040">
<othercit>Alecu, Teodor Iulian; Extended European Search Report dated May 18, 2011 from corresponding European Application No. 10187068.1.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00041">
<othercit>Papadimitrious, Spiros (spapadim); &#x201c;Changes between Version 42 and Version 434 of Android/OCR&#x201d;; Mar. 4, 2010; http://www.bitquill.net/trac/wiki/Andriod/OCR?action=diff&#x26;version=43.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00042">
<othercit>McCollum, A J et al: &#x201c;A Histogram Modification Unit for Real-Time Image Enhancement&#x201d;, Computer Vision Graphics and Image Processing, vol. 42, No. 1, Apr. 1, 1988, pp. 387-398.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00043">
<othercit>Tae-Chan, Kim et al: &#x201c;Real-Time Advanced Contrast Enhancement Algorithm&#x201d; In: &#x201c;Field Programmable Logic and Application&#x201d;, Jan. 1, 2003, vol. 2869, pp. 691-698.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00044">
<othercit>Christoph H. Lampert And Tim Braun And Adrian Ulges And Daniel Keysers And Thomas M. Breuel: &#x201c;Oblivious document capture and real-time retrieval&#x201d;, Int'l. Workshop on Camera-Based Document Analysis and Recognition, Jan. 1, 2005.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00045">
<othercit>Jie Zhao et al: &#x201c;Automatic Digital Image Enhancement for Dark Pictures&#x201d;, Acoustics, Speech and Signal Processing, 2006,Toulouse, France May 14, 2006, p. II.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00046">
<othercit>Kai Wang et al: &#x201c;Word Spotting in the Wild&#x201d;, European Conference on Computer Vision ECCV 2010, Sep. 5, 2010, pp. 591-604.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00047">
<othercit>Jonghyun Park et al: &#x201c;Automatic detection and recognition of Korean text in outdoor signboard images&#x201d;, Pattern Recognition Letters, vol. 31, No. 12, Sep. 1, 2010, pp. 1728-1739.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>22</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>4555561</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382182</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382217</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>12</number-of-drawing-sheets>
<number-of-figures>16</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120088543</doc-number>
<kind>A1</kind>
<date>20120412</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Lindner</last-name>
<first-name>Jeffery</first-name>
<address>
<city>Waterloo</city>
<country>CA</country>
</address>
</addressbook>
<residence>
<country>CA</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Hymel</last-name>
<first-name>James</first-name>
<address>
<city>Waterloo</city>
<country>CA</country>
</address>
</addressbook>
<residence>
<country>CA</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Lindner</last-name>
<first-name>Jeffery</first-name>
<address>
<city>Waterloo</city>
<country>CA</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Hymel</last-name>
<first-name>James</first-name>
<address>
<city>Waterloo</city>
<country>CA</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<last-name>So</last-name>
<first-name>Wilfred P.</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
<agent sequence="02" rep-type="attorney">
<addressbook>
<last-name>Slaney</last-name>
<first-name>Brett J.</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
<agent sequence="03" rep-type="attorney">
<addressbook>
<orgname>Blake, Cassels &#x26; Graydon LLP.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Blackberry Limited</orgname>
<role>03</role>
<address>
<city>Waterloo</city>
<country>CA</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Magloire</last-name>
<first-name>Vladimir</first-name>
<department>2645</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A system and a method are provided for displaying text in low-light environments. An original image of text is captured in a low-light environment using a camera on a mobile device, whereby the imaged text comprising images of characters. A brightness setting and a contrast setting of the original image are adjusted to increase the contrast of the imaged text relative to a background of the original image. Optical character recognition is applied to the adjusted image to generate computer readable text or characters corresponding to each of the imaged text. The original image of text is displayed on the mobile device. The computer readable text is also displayed, overlaid the original image, wherein the computer readable text is aligned with the corresponding imaged text.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="164.93mm" wi="203.88mm" file="US08626236-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="223.69mm" wi="177.55mm" orientation="landscape" file="US08626236-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="216.58mm" wi="175.60mm" orientation="landscape" file="US08626236-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="244.18mm" wi="126.24mm" orientation="landscape" file="US08626236-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="199.90mm" wi="162.14mm" orientation="landscape" file="US08626236-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="225.55mm" wi="175.60mm" orientation="landscape" file="US08626236-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="257.64mm" wi="179.41mm" orientation="landscape" file="US08626236-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="236.47mm" wi="199.31mm" orientation="landscape" file="US08626236-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="248.67mm" wi="199.90mm" orientation="landscape" file="US08626236-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="226.82mm" wi="180.09mm" orientation="landscape" file="US08626236-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="237.07mm" wi="178.14mm" orientation="landscape" file="US08626236-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="221.06mm" wi="171.11mm" orientation="landscape" file="US08626236-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="229.45mm" wi="167.22mm" orientation="landscape" file="US08626236-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">TECHNICAL FIELD</heading>
<p id="p-0002" num="0001">The following relates generally to viewing and displaying text data (e.g. letters, words, numbers, etc.) using a mobile device.</p>
<heading id="h-0002" level="1">DESCRIPTION OF THE RELATED ART</heading>
<p id="p-0003" num="0002">Text can be printed or displayed in many media forms such as, for example, books, magazines, newspapers, advertisements, flyers, etc. It is known that text can be scanned using devices, such as scanners. However, scanners are typically large and bulky and cannot be easily transported. Therefore, it is usually inconvenient to scan text at any moment. With developments in camera technology, it is also known that photographs can be scanned for text. However, these photographs are often captured with some delay and the text derived from the photographs usually does not reflect the text currently being viewed by a user.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0004" num="0003">Embodiments will now be described by way of example only with reference to the appended drawings wherein:</p>
<p id="p-0005" num="0004"><figref idref="DRAWINGS">FIG. 1</figref> a schematic diagram of a mobile device viewing text, displaying an image of the text, and displaying an indicator that low-lighting is detected.</p>
<p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. 2</figref> is a schematic diagram of the mobile device in <figref idref="DRAWINGS">FIG. 1</figref> augmenting the image with additional information.</p>
<p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. 3</figref> is a plan view of an example mobile device and a display screen therefor.</p>
<p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. 4</figref> is a plan view of another example mobile device and a display screen therefor.</p>
<p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. 5</figref> is a plan view of the back face of the mobile device shown in <figref idref="DRAWINGS">FIG. 3</figref>, and a camera device therefor.</p>
<p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. 6</figref> is a block diagram of an example embodiment of a mobile device.</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 7</figref> is a screen shot of a home screen displayed by the mobile device.</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 8</figref> is a block diagram illustrating exemplary ones of the other software applications and components shown in <figref idref="DRAWINGS">FIG. 6</figref>.</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 9</figref> is a block diagram of an example configuration of an augmented reality text viewer.</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 10</figref> is a block diagram of example user preferences shown in <figref idref="DRAWINGS">FIG. 9</figref>.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 11</figref> is a flow diagram of example computer executable instructions for displaying text in low-light environments.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 12</figref> is a flow diagram of further example computer executable instructions for displaying text in low-light environments.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 13</figref> is a schematic diagram of a mobile device displaying an augmentation of the imaged text in magnification mode.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 14</figref> is a schematic diagram of a mobile device displaying an augmentation of the imaged text including options to search, define, and translate selected words.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 15</figref> is a schematic diagram of a mobile device displaying an augmentation of the imaged text in an English to French translation mode, as well as options for other language translations.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 16</figref> is a flow diagram of example computer executable instructions for associating one or more functions with words extracted from the imaged text.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0021" num="0020">It will be appreciated that for simplicity and clarity of illustration, where considered appropriate, reference numerals may be repeated among the figures to indicate corresponding or analogous elements. In addition, numerous specific details are set forth in order to provide a thorough understanding of the embodiments described herein. However, it will be understood by those of ordinary skill in the art that the embodiments described herein may be practiced without these specific details. In other instances, well-known methods, procedures and components have not been described in detail so as not to obscure the embodiments described herein. Also, the description is not to be considered as limiting the scope of the embodiments described herein.</p>
<p id="p-0022" num="0021">In general, a system and a method are provided for displaying text in low-light environments. An original image of text is captured in a low-light environment using a camera on a mobile device, whereby the imaged text comprises images of characters. A brightness setting and a contrast setting of the original image are adjusted to increase the contrast of the imaged text relative to a background of the original image. Optical character recognition is applied to the adjusted image to generate computer readable text or characters corresponding to the imaged text. The original image of text is displayed on the mobile device. The computer readable text is also displayed, overlaid the original image.</p>
<p id="p-0023" num="0022">In another aspect, the computer readable text is aligned with the corresponding imaged text. In another aspect, upon capturing the original image of text, the low-light environment is detected from the original image. In another aspect, the computer readable text is displayed in at least one of: a larger font than the imaged text; a different font than the imaged text; a different color than the imaged text; a bold font; an underlined font; and a highlighted font. In another aspect, text recognition is applied to the computer readable text to extract at least words from the computer readable text. In another aspect, at least one function is associated with at least one of the words. In another aspect, the at least one function is initiated by receiving a user input in association with the at least one word. In another aspect, the at least one function associated with a given word comprises at least one of: retrieving a definition of the given word; retrieving a synonym or an antonym of the given word; searching for documents in a documents database using the given word as a search parameter; using the given word as a keyword parameter in a world wide web search engine; and providing an audio output pronouncing the given word. In another aspect, a language translator is applied to the extracted words to generate translated words. In another aspect, the translated words corresponding to the imaged text are displayed overlaid the original image.</p>
<p id="p-0024" num="0023">Turning to <figref idref="DRAWINGS">FIG. 1</figref>, an example of such an augmented reality display is provided. A mobile device <b>100</b> is shown viewing a medium <b>200</b> (i.e. a communication that includes text, such as e.g. an e-mail, web-site, book, magazine, newspaper, advertisement, another display screen, or other) in a low-light environment, or in a dark environment. The medium <b>200</b> includes text <b>202</b> and a graphic <b>204</b>, such as a photograph. The mobile device <b>100</b> uses a camera, such as a built-in camera, to view the medium <b>200</b> and display an image <b>224</b> on the mobile device's display screen <b>12</b>. Since the image <b>224</b> corresponds with the medium <b>200</b>, the image <b>224</b> includes the images of the graphic <b>206</b> and the text <b>208</b>. The image <b>224</b> is considered to be a streaming video image that corresponds with what is currently being viewed by the mobile device's camera. In other words, if the mobile device <b>100</b> is moved over a different part of the text <b>202</b>, then a different image of the different part of the text <b>202</b> will be displayed on the mobile device <b>100</b>.</p>
<p id="p-0025" num="0024">It can be appreciated that the low-light environment (such as e.g. at night time, in a dark room, on a bus at night, etc.) makes it difficult for a user to read the text <b>202</b> from the medium <b>200</b>. Low-light environments may vary depending on the sensitivity of a person's eye. However, low-light environments are typically measured in candela or lux, or both. For example, environments having lighting levels below approximately 0.1 cd/m<sup>2 </sup>or below approximately 1 lux are considered to be low-light environments. It can be understood that the original image <b>224</b> captured by the mobile device <b>100</b> does not clearly show the imaged text <b>208</b> due to the low-light conditions. Many camera devices are not able to properly image shapes, including text, in poor lighting conditions. In other words, camera images captured in low-lighting condition are considered to be of poor quality and features (e.g. shapes) in such images are difficult to recognize.</p>
<p id="p-0026" num="0025">Continuing with <figref idref="DRAWINGS">FIG. 1</figref>, the mobile device <b>100</b> detects low-lighting conditions based on the captured image <b>224</b>. The mobile device <b>100</b> displays a message <b>210</b> on the display <b>12</b> stating that low-light levels are detected, and poses a question as to whether or not text augmentation should be activated. Controls <b>212</b>, <b>214</b> are displayed to receive inputs as to whether or not the text augmentation is to be activated.</p>
<p id="p-0027" num="0026">Turning to <figref idref="DRAWINGS">FIG. 2</figref>, if the text augmentation is activated, a layer of computer readable text <b>216</b> is displayed on top of, or overlaid, the original image <b>224</b>. The computer readable text <b>216</b> comprises the same characters or words as the imaged text <b>208</b>, although the computer readable text <b>216</b> is displayed more clearly so that it can be easily viewed in low-lighting conditions. For example, the background of the computer readable text <b>216</b> can be brighter, and the computer readable text <b>216</b> itself can be darker. This provides contrast between the text and the background. In other examples, the computer readable text <b>216</b> can be displayed in larger font, in a brighter color, etc. The computer readable text <b>216</b> is also generally aligned with the imaged text <b>208</b>, from which it corresponds. For example, the position of the imaged text <b>208</b> &#x201c;Police are looking for a man . . . &#x201d; on the display <b>12</b> and the position of the computer readable text <b>216</b> &#x201c;Police are looking for a man . . . &#x201d; are generally the same. This allows the mobile device <b>100</b> to augment the reality of the text on the medium <b>200</b> with enriched data (e.g. computer readable text <b>216</b>). It can be appreciated that the imaged text <b>208</b> is an image and its meaning is not readily understood by a computing device or mobile device <b>100</b>. By contrast, the computer readable text <b>216</b> is a visual representation of character codes that are understood by a computing device or mobile device <b>100</b>, and can be more easily modified. Non-limiting examples of applicable character encoding and decoding schemes include ASCII code and Unicode. The words from the computer readable text can therefore be identified, as well as be underlined and associated with various functions. For example, the word &#x201c;syringe&#x201d; <b>220</b> in the computer readable text <b>216</b> is identified and underlined. The word or phrase &#x201c;Hotel Inn&#x201d; <b>222</b> is highlighted with a box. These words can be associated with other functions and turned into dynamic or active words, such as by making the words hyperlinks or controls. The display <b>12</b> also shows that the mobile device <b>100</b> is currently in augmented text mode <b>218</b>.</p>
<p id="p-0028" num="0027">As the mobile device <b>100</b> or the text <b>208</b> on the medium <b>200</b> move, the display <b>12</b> is automatically updated to show the text currently being viewed by the mobile device's camera. Accordingly, the computer readable text is also updated to correspond to the same currently imaged text.</p>
<p id="p-0029" num="0028">It can therefore be seen that text can be viewed in low-light conditions on a mobile device <b>100</b> by displaying corresponding computer readable text in real-time. This augments the reality.</p>
<p id="p-0030" num="0029">Examples of applicable electronic devices include pagers, cellular phones, cellular smart-phones, wireless organizers, personal digital assistants, computers, laptops, handheld wireless communication devices, wirelessly enabled notebook computers, camera devices and the like. Such devices will hereinafter be commonly referred to as &#x201c;mobile devices&#x201d; for the sake of clarity. It will however be appreciated that the principles described herein are also suitable to other devices, e.g. &#x201c;non-mobile&#x201d; devices.</p>
<p id="p-0031" num="0030">In an embodiment, the mobile device is a two-way communication device with advanced data communication capabilities including the capability to communicate with other mobile devices or computer systems through a network of transceiver stations. The mobile device may also have the capability to allow voice communication. Depending on the functionality provided by the mobile device, it may be referred to as a data messaging device, a two-way pager, a cellular telephone with data messaging capabilities, a wireless Internet appliance, or a data communication device (with or without telephony capabilities).</p>
<p id="p-0032" num="0031">Referring to <figref idref="DRAWINGS">FIGS. 3 and 4</figref>, one embodiment of a mobile device <b>100</b><i>a </i>is shown in <figref idref="DRAWINGS">FIG. 2</figref>, and another embodiment of a mobile device <b>100</b><i>b </i>is shown in <figref idref="DRAWINGS">FIG. 4</figref>. It will be appreciated that the numeral &#x201c;<b>100</b>&#x201d; will hereinafter refer to any mobile device <b>100</b>, including the embodiments <b>100</b><i>a </i>and <b>100</b><i>b</i>, those embodiments enumerated above or otherwise. It will also be appreciated that a similar numbering convention may be used for other general features common between all Figures such as a display <b>12</b>, a positioning device <b>14</b>, a cancel or escape button <b>16</b>, a camera button <b>17</b>, and a menu or option button <b>24</b>.</p>
<p id="p-0033" num="0032">The mobile device <b>100</b><i>a </i>shown in <figref idref="DRAWINGS">FIG. 3</figref> comprises a display <b>12</b><i>a </i>and the cursor or view positioning device <b>14</b> shown in this embodiment is a trackball <b>14</b><i>a</i>. Positioning device <b>14</b> may serve as another input member and is both rotational to provide selection inputs to the main processor <b>102</b> (see <figref idref="DRAWINGS">FIG. 6</figref>) and can also be pressed in a direction generally toward housing to provide another selection input to the processor <b>102</b>. Trackball <b>14</b><i>a </i>permits multi-directional positioning of the selection cursor <b>18</b> (see <figref idref="DRAWINGS">FIG. 7</figref>) such that the selection cursor <b>18</b> can be moved in an upward direction, in a downward direction and, if desired and/or permitted, in any diagonal direction. The trackball <b>14</b><i>a </i>is in this example situated on the front face of a housing for mobile device <b>100</b><i>a </i>as shown in <figref idref="DRAWINGS">FIG. 3</figref> to enable a user to manoeuvre the trackball <b>14</b><i>a </i>while holding the mobile device <b>100</b><i>a </i>in one hand. The trackball <b>14</b><i>a </i>may serve as another input member (in addition to a directional or positioning member) to provide selection inputs to the processor <b>102</b> and can preferably be pressed in a direction towards the housing of the mobile device <b>100</b><i>b </i>to provide such a selection input.</p>
<p id="p-0034" num="0033">The display <b>12</b> may include a selection cursor <b>18</b> that depicts generally where the next input or selection will be received. The selection cursor <b>18</b> may comprise a box, alteration of an icon or any combination of features that enable the user to identify the currently chosen icon or item. The mobile device <b>100</b><i>a </i>in <figref idref="DRAWINGS">FIG. 3</figref> also comprises a programmable convenience button <b>15</b> to activate a selected application such as, for example, a calendar or calculator. Further, mobile device <b>100</b><i>a </i>includes an escape or cancel button <b>16</b><i>a</i>, a camera button <b>17</b><i>a</i>, a menu or option button <b>24</b><i>a </i>and a keyboard <b>20</b>. The camera button <b>17</b> is able to activate photo and video capturing functions when pressed preferably in the direction towards the housing. The menu or option button <b>24</b> loads a menu or list of options on display <b>12</b><i>a </i>when pressed. In this example, the escape or cancel button <b>16</b><i>a</i>, the menu option button <b>24</b><i>a</i>, and keyboard <b>20</b> are disposed on the front face of the mobile device housing, while the convenience button <b>15</b> and camera button <b>17</b><i>a </i>are disposed at the side of the housing. This button placement enables a user to operate these buttons while holding the mobile device <b>100</b> in one hand. The keyboard <b>20</b> is, in this embodiment, a standard QWERTY keyboard.</p>
<p id="p-0035" num="0034">The mobile device <b>100</b><i>b </i>shown in <figref idref="DRAWINGS">FIG. 4</figref> comprises a display <b>12</b><i>b </i>and the positioning device <b>14</b> in this embodiment is a trackball <b>14</b><i>b</i>. The mobile device <b>100</b><i>b </i>also comprises a menu or option button <b>24</b><i>b</i>, a cancel or escape button <b>16</b><i>b</i>, and a camera button <b>17</b><i>b</i>. The mobile device <b>100</b><i>b </i>as illustrated in <figref idref="DRAWINGS">FIG. 4</figref>, comprises a reduced QWERTY keyboard <b>22</b>. In this embodiment, the keyboard <b>22</b>, positioning device <b>14</b><i>b</i>, escape button <b>16</b><i>b </i>and menu button <b>24</b><i>b </i>are disposed on a front face of a mobile device housing. The reduced QWERTY keyboard <b>22</b> comprises a plurality of multi-functional keys and corresponding indicia including keys associated with alphabetic characters corresponding to a QWERTY array of letters A to Z and an overlaid numeric phone key arrangement.</p>
<p id="p-0036" num="0035">It will be appreciated that for the mobile device <b>100</b>, a wide range of one or more positioning or cursor/view positioning mechanisms such as a touch pad, a positioning wheel, a joystick button, a mouse, a touchscreen, a set of arrow keys, a tablet, an accelerometer (for sensing orientation and/or movements of the mobile device <b>100</b> etc.), or other whether presently known or unknown may be employed. Similarly, any variation of keyboard <b>20</b>, <b>22</b> may be used. It will also be appreciated that the mobile devices <b>100</b> shown in <figref idref="DRAWINGS">FIGS. 3 and 4</figref> are for illustrative purposes only and various other mobile devices <b>100</b> are equally applicable to the following examples. For example, other mobile devices <b>100</b> may include the trackball <b>14</b><i>b</i>, escape button <b>16</b><i>b </i>and menu or option button <b>24</b> similar to that shown in <figref idref="DRAWINGS">FIG. 4</figref> only with a full or standard keyboard of any type. Other buttons may also be disposed on the mobile device housing such as colour coded &#x201c;Answer&#x201d; and &#x201c;Ignore&#x201d; buttons to be used in telephonic communications. In another example, the display <b>12</b> may itself be touch sensitive thus itself providing an input mechanism in addition to display capabilities.</p>
<p id="p-0037" num="0036">Referring to <figref idref="DRAWINGS">FIG. 5</figref>, in the rear portion of mobile device <b>100</b><i>a</i>, for example, there is a light source <b>30</b> which may be used to illuminate an object for taking capturing a video image or photo. Also situated on the mobile device's rear face is a camera lens <b>32</b> and a reflective surface <b>34</b>. The camera lens <b>32</b> allows the light that represents an image to enter into the camera device. The reflective surface <b>34</b> displays an image that is representative of the camera device's view and assists, for example, a user to take a self-portrait photo. The camera device may be activated by pressing a camera button <b>17</b>, such as the camera button <b>17</b><i>a </i>shown in <figref idref="DRAWINGS">FIG. 3</figref>.</p>
<p id="p-0038" num="0037">To aid the reader in understanding the structure of the mobile device <b>100</b>, reference will now be made to <figref idref="DRAWINGS">FIGS. 6 through 8</figref>.</p>
<p id="p-0039" num="0038">Referring first to <figref idref="DRAWINGS">FIG. 6</figref>, shown therein is a block diagram of an exemplary embodiment of a mobile device <b>100</b>. The mobile device <b>100</b> comprises a number of components such as a main processor <b>102</b> that controls the overall operation of the mobile device <b>100</b>. Communication functions, including data and voice communications, are performed through a communication subsystem <b>104</b>. The communication subsystem <b>104</b> receives messages from and sends messages to a wireless network <b>200</b>. In this exemplary embodiment of the mobile device <b>100</b>, the communication subsystem <b>104</b> is configured in accordance with the Global System for Mobile Communication (GSM) and General Packet Radio Services (GPRS) standards, which is used worldwide. Other communication configurations that are equally applicable are the 3G and 4G networks such as EDGE, UMTS and HSDPA, LTE, Wi-Max etc. New standards are still being defined, but it is believed that they will have similarities to the network behaviour described herein, and it will also be understood by persons skilled in the art that the embodiments described herein are intended to use any other suitable standards that are developed in the future. The wireless link connecting the communication subsystem <b>104</b> with the wireless network <b>200</b> represents one or more different Radio Frequency (RF) channels, operating according to defined protocols specified for GSM/GPRS communications.</p>
<p id="p-0040" num="0039">The main processor <b>102</b> also interacts with additional subsystems such as a Random Access Memory (RAM) <b>106</b>, a flash memory <b>108</b>, a display <b>110</b>, an auxiliary input/output (I/O) subsystem <b>112</b>, a data port <b>114</b>, a keyboard <b>116</b>, a speaker <b>118</b>, a microphone <b>120</b>, a GPS receiver <b>121</b>, short-range communications <b>122</b>, a camera <b>123</b>, a magnetometer <b>125</b>, and other device subsystems <b>124</b>. The display <b>110</b> can be a touch-screen display able to receive inputs through a user's touch.</p>
<p id="p-0041" num="0040">Some of the subsystems of the mobile device <b>100</b> perform communication-related functions, whereas other subsystems may provide &#x201c;resident&#x201d; or on-device functions. By way of example, the display <b>110</b> and the keyboard <b>116</b> may be used for both communication-related functions, such as entering a text message for transmission over the network <b>200</b>, and device-resident functions such as a calculator or task list.</p>
<p id="p-0042" num="0041">The mobile device <b>100</b> can send and receive communication signals over the wireless network <b>200</b> after required network registration or activation procedures have been completed. Network access is associated with a subscriber or user of the mobile device <b>100</b>. To identify a subscriber, the mobile device <b>100</b> may use a subscriber module component or &#x201c;smart card&#x201d; <b>126</b>, such as a Subscriber Identity Module (SIM), a Removable User Identity Module (RUIM) and a Universal Subscriber Identity Module (USIM). In the example shown, a SIM/RUIM/USIM <b>126</b> is to be inserted into a SIM/RUIM/USIM interface <b>128</b> in order to communicate with a network. Without the component <b>126</b>, the mobile device <b>100</b> is not fully operational for communication with the wireless network <b>200</b>. Once the SIM/RUIM/USIM <b>126</b> is inserted into the SIM/RUIM/USIM interface <b>128</b>, it is coupled to the main processor <b>102</b>.</p>
<p id="p-0043" num="0042">The mobile device <b>100</b> is a battery-powered device and includes a battery interface <b>132</b> for receiving one or more rechargeable batteries <b>130</b>. In at least some embodiments, the battery <b>130</b> can be a smart battery with an embedded microprocessor. The battery interface <b>132</b> is coupled to a regulator (not shown), which assists the battery <b>130</b> in providing power V+ to the mobile device <b>100</b>. Although current technology makes use of a battery, future technologies such as micro fuel cells may provide the power to the mobile device <b>100</b>.</p>
<p id="p-0044" num="0043">The mobile device <b>100</b> also includes an operating system <b>134</b> and software components <b>136</b> to <b>146</b> which are described in more detail below. The operating system <b>134</b> and the software components <b>136</b> to <b>146</b> that are executed by the main processor <b>102</b> are typically stored in a persistent store such as the flash memory <b>108</b>, which may alternatively be a read-only memory (ROM) or similar storage element (not shown). Those skilled in the art will appreciate that portions of the operating system <b>134</b> and the software components <b>136</b> to <b>146</b>, such as specific device applications, or parts thereof, may be temporarily loaded into a volatile store such as the RAM <b>106</b>. Other software components can also be included, as is well known to those skilled in the art.</p>
<p id="p-0045" num="0044">The subset of software applications <b>136</b> that control basic device operations, including data and voice communication applications, may be installed on the mobile device <b>100</b> during its manufacture. Software applications may include a message application <b>138</b>, a device state module <b>140</b>, a Personal Information Manager (PIM) <b>142</b>, a connect module <b>144</b> and an IT policy module <b>146</b>. A message application <b>138</b> can be any suitable software program that allows a user of the mobile device <b>100</b> to send and receive electronic messages, wherein messages are typically stored in the flash memory <b>108</b> of the mobile device <b>100</b>. A device state module <b>140</b> provides persistence, i.e. the device state module <b>140</b> ensures that important device data is stored in persistent memory, such as the flash memory <b>108</b>, so that the data is not lost when the mobile device <b>100</b> is turned off or loses power. A PIM <b>142</b> includes functionality for organizing and managing data items of interest to the user, such as, but not limited to, e-mail, contacts, calendar events, and voice mails, and may interact with the wireless network <b>200</b>. A connect module <b>144</b> implements the communication protocols that are required for the mobile device <b>100</b> to communicate with the wireless infrastructure and any host system, such as an enterprise system, that the mobile device <b>100</b> is authorized to interface with. An IT policy module <b>146</b> receives IT policy data that encodes the IT policy, and may be responsible for organizing and securing rules such as the &#x201c;Set Maximum Password Attempts&#x201d; IT policy.</p>
<p id="p-0046" num="0045">Other types of software applications or components <b>139</b> can also be installed on the mobile device <b>100</b>. These software applications <b>139</b> can be pre-installed applications (i.e. other than message application <b>138</b>) or third party applications, which are added after the manufacture of the mobile device <b>100</b>. Examples of third party applications include games, calculators, utilities, etc.</p>
<p id="p-0047" num="0046">The additional applications <b>139</b> can be loaded onto the mobile device <b>100</b> through at least one of the wireless network <b>200</b>, the auxiliary I/O subsystem <b>112</b>, the data port <b>114</b>, the short-range communications subsystem <b>122</b>, or any other suitable device subsystem <b>124</b>.</p>
<p id="p-0048" num="0047">The data port <b>114</b> can be any suitable port that enables data communication between the mobile device <b>100</b> and another computing device. The data port <b>114</b> can be a serial or a parallel port. In some instances, the data port <b>114</b> can be a USB port that includes data lines for data transfer and a supply line that can provide a charging current to charge the battery <b>130</b> of the mobile device <b>100</b>.</p>
<p id="p-0049" num="0048">For voice communications, received signals are output to the speaker <b>118</b>, and signals for transmission are generated by the microphone <b>120</b>. Although voice or audio signal output is accomplished primarily through the speaker <b>118</b>, the display <b>110</b> can also be used to provide additional information such as the identity of a calling party, duration of a voice call, or other voice call related information.</p>
<p id="p-0050" num="0049">Turning now to <figref idref="DRAWINGS">FIG. 7</figref>, the mobile device <b>100</b> may display a home screen <b>40</b>, which can be set as the active screen when the mobile device <b>100</b> is powered up and may constitute the main ribbon application. The home screen <b>40</b> generally comprises a status region <b>44</b> and a theme background <b>46</b>, which provides a graphical background for the display <b>12</b>. The theme background <b>46</b> displays a series of icons <b>42</b> in a predefined arrangement on a graphical background. In some themes, the home screen <b>40</b> may limit the number icons <b>42</b> shown on the home screen <b>40</b> so as to not detract from the theme background <b>46</b>, particularly where the background <b>46</b> is chosen for aesthetic reasons. The theme background <b>46</b> shown in <figref idref="DRAWINGS">FIG. 7</figref> provides a grid of icons. It will be appreciated that preferably several themes are available for the user to select and that any applicable arrangement may be used. An exemplary icon may be a camera icon <b>51</b> used to indicate an augmented reality camera-based application. One or more of the series of icons <b>42</b> is typically a folder <b>52</b> that itself is capable of organizing any number of applications therewithin.</p>
<p id="p-0051" num="0050">The status region <b>44</b> in this embodiment comprises a date/time display <b>48</b>. The theme background <b>46</b>, in addition to a graphical background and the series of icons <b>42</b>, also comprises a status bar <b>50</b>. The status bar <b>50</b> provides information to the user based on the location of the selection cursor <b>18</b>, e.g. by displaying a name for the icon <b>53</b> that is currently highlighted.</p>
<p id="p-0052" num="0051">An application, such as message application <b>138</b> may be initiated (opened or viewed) from display <b>12</b> by highlighting a corresponding icon <b>53</b> using the positioning device <b>14</b> and providing a suitable user input to the mobile device <b>100</b>. For example, message application <b>138</b> may be initiated by moving the positioning device <b>14</b> such that the icon <b>53</b> is highlighted by the selection box <b>18</b> as shown in <figref idref="DRAWINGS">FIG. 7</figref>, and providing a selection input, e.g. by pressing the trackball <b>14</b><i>b. </i></p>
<p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. 8</figref> shows an example of the other software applications and components <b>139</b> that may be stored and used on the mobile device <b>100</b>. Only examples are shown in <figref idref="DRAWINGS">FIG. 8</figref> and such examples are not to be considered exhaustive. In this example, an alarm application <b>54</b> may be used to activate an alarm at a time and date determined by the user. There is also an address book <b>62</b> that manages and displays contact information. A GPS application <b>56</b> may be used to determine the location of a mobile device <b>100</b>. A calendar application <b>58</b> that may be used to organize appointments. Another exemplary application is an augmented reality text viewer application <b>60</b>. This application <b>60</b> is able to augment an image by displaying another layer on top of the image, whereby the layer includes computer readable text corresponding to the text shown in the image.</p>
<p id="p-0054" num="0053">Other applications include an optical character recognition application <b>64</b>, a text recognition application <b>66</b>, and a language translator <b>68</b>. The optical character recognition application <b>64</b> and the text recognition application <b>66</b> may be a combined application or different application. It can also be appreciated that other applications or modules described herein can also be combined or operate separately. The optical character recognition application <b>64</b>, also referred to as OCR, is able to translate handwritten text, printed text, typewritten text, etc. into computer readable text, or machine encoded text. Known methods of translating an image of text into computer readable text, generally referred to as OCR methods, can be used herein. The OCR application <b>64</b> is also able to perform intelligent character recognition (ICR) to also recognize handwritten text. The text recognition application <b>66</b> recognizes the combinations of computer readable characters that form words, phrases, sentences, paragraphs, addresses, phone numbers, dates, etc. In other words, the meanings of the combinations of letters can be understood. Known text recognition software is applicable to the principles described herein. A language translator <b>68</b> translates the computer readable text from a given language to another language (e.g. English to French, French to German, Chinese to English, Spanish to German, etc.). Known language translators can be used.</p>
<p id="p-0055" num="0054">Turning to <figref idref="DRAWINGS">FIG. 9</figref>, an example configuration of the augmented reality text viewer <b>60</b> is provided. The augmented reality text viewer <b>60</b> receives an input from the camera <b>123</b>. In particular, the augmented reality text viewer <b>60</b>, or AR text viewer, receives camera or video images containing text. Using the images, the AR text viewer <b>60</b> displays the image on the display screen <b>12</b> as well as displays computer readable text above the image.</p>
<p id="p-0056" num="0055">Continuing with <figref idref="DRAWINGS">FIG. 9</figref>, the AR text viewer <b>60</b> includes a low-light level detector <b>230</b>, an image processing module <b>232</b>, computer readable text memory <b>234</b>, translated text memory <b>236</b>, a text and font display module <b>238</b>, user preferences memory <b>240</b>, an auxiliary text augmentation module <b>252</b>, and a graphical user interface (GUI) <b>250</b>. The AR text viewer <b>60</b> can also include or interact with the OCR application <b>64</b>, the text recognition application <b>66</b>, the language translator <b>68</b>, and a number of other functions <b>254</b> that are able to interact with the computer readable text.</p>
<p id="p-0057" num="0056">In particular, the camera <b>123</b> provides streaming images of text which are captured in low-light conditions. The low-light level detector <b>230</b> receives the images from the camera <b>123</b>. If the received images appear dark, or have low contrast between the objects (e.g. text) and the background, then the low-light level detector <b>230</b> can automatically activate the AR text viewer <b>60</b>. Alternatively, the low-light level detector <b>230</b> can display, through the GUI <b>250</b>, a message on the mobile device's display screen <b>12</b> stating that a low-light environment is detected, and provide controls allowing a user to activate the AR text viewer <b>60</b>. It can be appreciated that known techniques for detecting low-light using a charged-coupled device (CCD) or images captured by a camera can also be used. Upon activating the AR text viewer <b>60</b>, the original image viewed from the camera <b>123</b> undergoes image processing by the image processing module <b>232</b>. The brightness settings and contrast settings of the image are adjusted by module <b>232</b> to increase the definition of the imaged text. Alternatively, or additionally, the exposure settings of the camera <b>123</b> may be increased so that more light is absorbed by the camera. It is noted that without image processing, the text from the original image would not likely be properly recognized by an OCR application <b>64</b> due to the poor image quality from the low-light conditions. The adjusted image is then processed by the OCR application <b>64</b> to translate the more clearly defined imaged text into computer readable text. The computer readable text generated by the OCR application <b>64</b> is saved to memory <b>234</b>. The computer readable text also is processed by a text recognition application <b>66</b> to identify words, phrases, phone numbers, addresses, web addresses, etc., which are also saved in memory <b>234</b>. The computer readable text can then be displayed on the screen <b>12</b>, through the GUI <b>250</b>. The text and font display module <b>238</b> can control or manipulate the display of the computer readable text, such as by controlling the font style, font size, font color, highlighting, underlines, hyperlinks, etc. The computer readable text is displayed in way so that is easily read in low-light conditions. A user can provide preferences, stored in memory <b>240</b>, to the text and font display module <b>238</b>, which can be used to select display settings for the computer readable text. A language translator <b>68</b> can translate the computer readable text <b>234</b> into different languages and the translated text is saved in memory <b>236</b>. The display of the translated text can also be modified or controlled by the text and font display module <b>238</b>.</p>
<p id="p-0058" num="0057">The GUI <b>250</b> includes a text selector <b>244</b> allowing a user to select text, a functions interface <b>246</b> for facilitating interaction with various functions related to the text, and an augmented text display <b>248</b> for displaying computer readable text over the imaged text, among others. The augmented text display <b>248</b> also interacts with the image processing module <b>232</b> to determine the location (e.g. pixel location) of the imaged text in the original image and to generally align the location of the corresponding computer readable text over the imaged text.</p>
<p id="p-0059" num="0058">Other functions <b>254</b> can be performed to augment the display of text through the auxiliary text augmentation module <b>252</b>. Examples of such functions <b>254</b> include a dictionary application <b>256</b> for defining words, a thesaurus application <b>258</b> for providing synonyms and antonyms, a database search application <b>260</b> for searching databases (e.g. for documents, photos, files, etc.), a world wide web search engine application <b>262</b> (e.g. Google&#x2122; or Bing&#x2122;) for performing keyword based web searching, and a text-to-audio application <b>264</b> for translating text to speech.</p>
<p id="p-0060" num="0059">In an example embodiment, a user can select several words which can be translated to speech. In another example, a word can be selected and used as a search term in a database or on the world wide web. User preferences may also be associated with such functions.</p>
<p id="p-0061" num="0060">Turning to <figref idref="DRAWINGS">FIG. 10</figref>, examples of user preferences <b>240</b> for displaying computer readable text and associating functions with the text are provided. Preferences may include a list of words to highlight <b>270</b>, a list of words to bold <b>278</b>, and a list of words to underline <b>276</b>. For example, a user may be interested in looking for certain words in a document, and may use the AR text viewer <b>60</b> to bring these certain words to the user's attention by highlighting, underlining or bolding the computer readable text laid over the imaged text. Other preferences include calling on a list of words to automatically define <b>272</b>. Such a list <b>272</b>, for example, can be obtained from a dictionary of uncommon words, where the definitions are not likely to be known. There may also be a list of preferred fonts <b>274</b> for displaying the computer readable text in, for example, Arial font, Times New Roman font, Bank Gothic font, Freestyle script font, etc. There may also be a preferred font size <b>282</b> that is used when the AR text viewer is in a magnification mode. There may also be a list of words to automatically hyperlink to a world wide web search engine <b>280</b>. For example, the user may be interested in companies and every time a company name is detected in the computer readable text, an internet search for the company name is initiated.</p>
<p id="p-0062" num="0061">It will be appreciated that any module or component exemplified herein that executes instructions or operations may include or otherwise have access to computer readable media such as storage media, computer storage media, or data storage devices (removable and/or non-removable) such as, for example, magnetic disks, optical disks, or tape. Computer storage media may include volatile and non-volatile, removable and non-removable media implemented in any method or technology for storage of information, such as computer readable instructions, data structures, program modules, or other data, except transitory propagating signals per se. Examples of computer storage media include RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by an application, module, or both. Any such computer storage media may be part of the mobile device <b>100</b> or accessible or connectable thereto. Any application or module herein described may be implemented using computer readable/executable instructions or operations that may be stored or otherwise held by such computer readable media.</p>
<p id="p-0063" num="0062">Turning to <figref idref="DRAWINGS">FIG. 11</figref>, example computer executable instructions are provided for displaying text in low-light environments. At block <b>284</b>, the mobile device <b>100</b> captures an original image of text in a low-light environment using a camera <b>123</b> on the mobile device <b>100</b>. The imaged text includes images of characters (e.g. letters, punctuation, numbers, etc.). At block <b>286</b>, the brightness setting and contrast setting of the original image are adjusted to increase the contrast of the imaged text relative to a background of the original image. The background can be considered the area behind the imaged text. At block <b>288</b>, optical character recognition is applied to the adjusted image to generate computer readable characters or text corresponding to each of the images of the characters. At block <b>290</b>, the original image of the text is displayed on the mobile device <b>100</b>. At block <b>292</b>, the computer readable characters are displayed, overlaid the original image. The computer readable characters are generally aligned with the corresponding images of the characters. At block <b>294</b>, the mobile device <b>100</b> continuously captures images of the text and automatically updates the display when the position of the text changes, or if new text is detected. The process repeats by implementing the operations in blocks <b>286</b>, <b>288</b>, <b>290</b>, <b>292</b>, and <b>294</b>, which is represented by the dotted line <b>296</b>.</p>
<p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. 12</figref> provides further example computer executable instructions for augmenting the display of text in low-light conditions. At block <b>298</b>, an original image of text is captured in a low-light environment using a camera on a mobile device <b>100</b>. The imaged text can comprise images of characters. At block <b>300</b>, the low-light environment is detected based on the original image. At block <b>302</b>, a message is displayed to the user that the low-light environment is detected and controls are displayed to activate the augmentation of the imaged text. At block <b>304</b>, an input is received to augment the text. At block <b>306</b>, it is determined if there is a previously saved image processing setting (e.g. a brightness setting, a contrast setting, a sharpness setting, a brightness-to-contrast ratio setting, etc.). If a previous image processing setting is available, then at block <b>308</b> the brightness and contrast settings of the original image are adjusted to the previously saved settings. If further adjustment is required, then the image is adjusted at intervals until the imaged text is sufficiently recognizable to the OCR application <b>64</b>. For example, the imaged text is recognizable if it is sufficiently darker or lighter relative to the background. If the further adjustment is required, then the new image settings are saved for future use (block <b>312</b>). It is recognized that the lighting conditions of the text do not change significantly within most periods of use. Therefore, the previous image adjustment settings can be used. It can be appreciated that saving the image adjustment settings allows the mobile device <b>100</b> to process the images of text quickly, facilitating the real-time augmented reality performance. However, if there are no previously saved settings, then at block <b>310</b>, the brightness and contrast settings are adjusted. Usually, due to low-light environments, the brightness and contrast of the originally captured image will need to be increased. There may also be known optimal brightness-to-contrast settings based on the given low-light conditions. The settings can be increased at intervals until the imaged text is sufficiently lighter or darker than the background. The settings are then saved for future used (block <b>312</b>).</p>
<p id="p-0065" num="0064">At block <b>314</b>, optical character recognition is applied to the adjusted image to generate computer readable text or characters corresponding to the imaged text. As described earlier, methods and approaches for optical character recognition are known and can be applied to the principles herein. At block <b>316</b>, the computer readable text or characters are saved. At block <b>318</b>, the original image of text is displayed on the mobile device <b>100</b>. At block <b>320</b>, the pixel locations of the imaged text are determined. At block <b>322</b>, the computer readable text or characters are matched with the imaged text using the pixel locations. The locations of the matching computer readable text or characters are aligned with the locations of the imaged text on the display screen <b>12</b>. In this way, the computer readable text is overlaid the imaged text in an aligned manner. The computer readable text may not necessarily be aligned however. For example, a translucent layer can cover the imaged text portion to at least partially block out the imaged text, and the computer readable text can be displayed on top of the translucent layer. It can be appreciated that there are different ways of displaying the computer readable text.</p>
<p id="p-0066" num="0065">Continuing with <figref idref="DRAWINGS">FIG. 12</figref>, at block <b>324</b>, text recognition is applied to the computer readable text or characters to extract words, phone numbers, web addresses, dates, names, etc. At block <b>326</b>, the display of the computer readable characters, words, text, numbers, etc. is modified by adjusting the font style, font size, underlining, bolding, highlighting, etc.</p>
<p id="p-0067" num="0066"><figref idref="DRAWINGS">FIGS. 13</figref>, <b>14</b> and <b>15</b> show example screen shots of the AR text viewer. <figref idref="DRAWINGS">FIG. 13</figref> shows the AR text viewer in magnification mode. The augmented layer of text <b>330</b> includes computer readable text that is of a larger font size than the imaged text. Controls <b>332</b> and <b>334</b> allow the font size of the computer readable text to increase or decrease, respectively.</p>
<p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. 14</figref> shows a screen shot including an augmented layer of computer readable text <b>215</b> including words that have been associated with functions. For example, after translating the imaged text to computer readable text, and identifying the word &#x201c;syringe&#x201d; from the computer readable text, instances of the word &#x201c;syringe&#x201d; <b>220</b> have been underlined. Instances of the name &#x201c;Hotel Inn&#x201d; <b>220</b> have also been identified, and highlighted with a box. Controls <b>336</b>, <b>338</b>, <b>340</b> for initiating functions that have been associated with the word &#x201c;syringe&#x201d; are also displayed. Control <b>336</b> initiates searching for the word &#x201c;syringe&#x201d; using a world wide web search engine. Control <b>338</b> initiates obtaining the definition of the word &#x201c;syringe&#x201d;. Control <b>340</b> initiates translating the word &#x201c;syringe&#x201d; into French. It can be appreciated that a user can select another word, thereby associating functions with the selected word. Other controls for other functions can be displayed. It is also appreciated that the screen shots provided herein are just for example and other GUI configurations are applicable.</p>
<p id="p-0069" num="0068"><figref idref="DRAWINGS">FIG. 15</figref> shows a screen shot of the original image <b>224</b> containing images of English text and being augmented with translated French computer readable text <b>342</b>. The mobile device <b>100</b> shows that it is currently in an English-to-French translation mode. Control <b>346</b> allows the mobile device <b>100</b> to switch modes to English to Spanish, and control <b>348</b> allows the mobile device <b>100</b> to switch modes to English to German. It can be appreciated that the computer readable text can be translated into many different languages and displayed over the originally imaged text.</p>
<p id="p-0070" num="0069">Turning to <figref idref="DRAWINGS">FIG. 16</figref>, example computer executable instructions are provided for associating functions with words. At block <b>350</b>, the computer readable characters or text, such as words, phone numbers, web addresses, etc. are displayed on the mobile device <b>100</b>, overlaid the original image. One or more functions are associated with one or more of the words (block <b>352</b>), either automatically or manually. In other words, certain words all words can be associated with a function, for example, as determined by user preferences. Alternatively, words can be manually associated with a function. For example, a user can select a word in the computer readable text and associate it with an speech output. At block <b>354</b>, an input for initiating one or more functions associated with or more of the words, phone numbers, web addresses, etc. is received. The function or functions are then initiated.</p>
<p id="p-0071" num="0070">Continuing with <figref idref="DRAWINGS">FIG. 16</figref>, examples of initiating functions include: retrieving a definition of a given word and displaying the definition (block <b>356</b>); retrieving a synonym or antonym of a given words and displaying the same (block <b>358</b>); searching for documents in a documents database using the given words as a search parameter (block <b>360</b>); using the given words as a keyword parameters in a world wide web search engine and displaying the results (block <b>362</b>); providing an audio output pronouncing the given word (block <b>364</b>); and calling a phone number identified in the computer readable text (block <b>366</b>). The above list of functions is non-exhaustive, and other functions related to the computer readable text are also applicable. It can also be appreciated that functions may transmit requests for information from the mobile device <b>100</b> to other mobile devices or computing devices (not shown). For example, when searching for documents, the request for data may be sent to a document server that performs the search and returns a search result to the mobile device <b>100</b>. The mobile device <b>100</b> then displays the result from the document server.</p>
<p id="p-0072" num="0071">It can therefore be seen that reading text in low-light conditions can be improved by using the AR text viewer to capture images of the text and clearly display computer readable text over the captured images. Other benefits include using the AR text viewer to magnify text that is difficult to read because it is too small, and using the AR text viewer to translate text that is in another language.</p>
<p id="p-0073" num="0072">The schematics and block diagrams used herein are just for example. Different configurations and names of components can be used. For instance, components and modules can be added, deleted, modified, or arranged with differing connections without departing from the spirit of the invention or inventions.</p>
<p id="p-0074" num="0073">The steps or operations in the flow charts and diagrams described herein are just for example. There may be many variations to these steps or operations without departing from the spirit of the invention or inventions. For instance, the steps may be performed in a differing order, or steps may be added, deleted, or modified.</p>
<p id="p-0075" num="0074">It will be appreciated that the particular embodiments shown in the figures and described above are for illustrative purposes only and many other variations can be used according to the principles described. Although the above has been described with reference to certain specific embodiments, various modifications thereof will be apparent to those skilled in the art as outlined in the appended claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method of displaying text in low-light environments, the method comprising:
<claim-text>capturing a streaming video comprising an original image of text in a low-light environment using a camera on a mobile device;</claim-text>
<claim-text>adjusting a brightness setting and a contrast setting of the original image to increase the contrast of the imaged text relative to a background of the original image;</claim-text>
<claim-text>applying optical character recognition to the adjusted image to generate text corresponding to the imaged text;</claim-text>
<claim-text>displaying the original image of text on the mobile device;</claim-text>
<claim-text>displaying, overlaid the original image, the text with greater visibility in the low-light environment than the imaged text;</claim-text>
<claim-text>automatically updating the display of the text when the position of the corresponding imaged text changes location in a subsequent original image of the streaming video; and,</claim-text>
<claim-text>when new imaged text is detected in the subsequent original image, automatically updating the display with new text overlaid the corresponding new imaged text.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the text is aligned with the corresponding imaged text.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising, upon capturing the original image of text, detecting the low-light environment from the original image.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the text is displayed in at least one of: a larger font than the imaged text; a different font than the imaged text; a different color than the imaged text, a greater contrast to the background of the original image than the imaged text, a bold font; an underlined font; and a highlighted font.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising applying text recognition to the text to extract at least words from the text.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref> further comprising associating at least one function with at least one of the words.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref> wherein the at least one function is initiated by receiving a user input in association with the at least one word.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref> wherein the at least one function associated with a given word comprises at least one of: retrieving a definition of the given word; retrieving a synonym or an antonym of the given word; searching for documents in a documents database using the given word as a search parameter; using the given word as a keyword parameter in a world wide web search engine; and providing an audio output pronouncing the given word.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref> further comprising applying a language translator to the extracted words to generate translated words.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref> further comprising displaying, overlaid the original image, the translated words corresponding to the imaged text.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. A mobile device, comprising:
<claim-text>a display;</claim-text>
<claim-text>a camera configured to capture a streaming video comprising an original image of text in a low-light environment; and</claim-text>
<claim-text>a processor connected to the display and the camera, and configured to adjust a brightness setting and a contrast setting of the original image to increase the contrast of the imaged text relative to a background of the original image, apply optical character recognition to the adjusted image to generate text corresponding to the imaged text, display on the display the original image of text with the text overlaid the original image, the text being displayed with greater visibility in the low-light environment than the imaged text, and automatically update the display of the text when the position of the corresponding imaged text changes location in a subsequent original image of the streaming video, and, when new imaged text is detected in the subsequent original image, automatically update the display with new text overlaid the corresponding new imaged text.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The mobile device of <claim-ref idref="CLM-00011">claim 11</claim-ref> wherein the text is aligned with the corresponding imaged text.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The mobile device of <claim-ref idref="CLM-00011">claim 11</claim-ref> wherein upon capturing the original image of text, the processor is further configured to detect the low-light environment from the original image.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The mobile device of <claim-ref idref="CLM-00011">claim 11</claim-ref> wherein the text is displayed in at least one of: a larger font than the imaged text; a different font than the imaged text; a different color than the imaged text; a greater contrast to the background of the original image than the imaged text; a bold font; an underlined font; and a highlighted font.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The mobile device of <claim-ref idref="CLM-00011">claim 11</claim-ref> wherein the processor is further configured to apply text recognition to the text to extract at least words from the text.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The mobile device of <claim-ref idref="CLM-00015">claim 15</claim-ref> wherein the processor is further configured to associate at least one function with at least one of the words.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The mobile device of <claim-ref idref="CLM-00016">claim 16</claim-ref> wherein the at least one function is initiated by receiving a user input in association with the at least one word.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The mobile device of <claim-ref idref="CLM-00017">claim 17</claim-ref> wherein the at least one function associated with a given word comprises at least one of: retrieving a definition of the given word; retrieving a synonym or an antonym of the given word; searching for documents in a documents database using the given word as a search parameter; using the given word as a keyword parameter in a world wide web search engine; and providing an audio output pronouncing the given word.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The mobile device of <claim-ref idref="CLM-00015">claim 15</claim-ref> wherein the processor is further configured to apply a language translator to the extracted words to generate translated words.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The mobile device of <claim-ref idref="CLM-00019">claim 19</claim-ref> wherein the processor is further configured to display, overlaid the original image, the translated words corresponding to the imaged text.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the text is computer readable text.</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The mobile device of <claim-ref idref="CLM-00011">claim 11</claim-ref> wherein the text is computer readable text. </claim-text>
</claim>
</claims>
</us-patent-grant>
