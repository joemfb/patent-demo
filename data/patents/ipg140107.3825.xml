<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08624892-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08624892</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13677920</doc-number>
<date>20121115</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20110101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>T</subclass>
<main-group>15</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>345419</main-classification>
<further-classification>345422</further-classification>
<further-classification>345426</further-classification>
<further-classification>345522</further-classification>
</classification-national>
<invention-title id="d2e43">Integration of graphical application content into the graphical scene of another application</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5491813</doc-number>
<kind>A</kind>
<name>Bondy et al.</name>
<date>19960200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5682326</doc-number>
<kind>A</kind>
<name>Klingler et al.</name>
<date>19971000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5774720</doc-number>
<kind>A</kind>
<name>Borgendale et al.</name>
<date>19980600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5838326</doc-number>
<kind>A</kind>
<name>Card et al.</name>
<date>19981100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5889951</doc-number>
<kind>A</kind>
<name>Lombardi</name>
<date>19990300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6002403</doc-number>
<kind>A</kind>
<name>Sugiyama et al.</name>
<date>19991200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6088032</doc-number>
<kind>A</kind>
<name>Mackinlay</name>
<date>20000700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6229542</doc-number>
<kind>B1</kind>
<name>Miller</name>
<date>20010500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6417849</doc-number>
<kind>B2</kind>
<name>Lefebvre et al.</name>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6538660</doc-number>
<kind>B1</kind>
<name>Celi, Jr. et al.</name>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6597358</doc-number>
<kind>B2</kind>
<name>Miller</name>
<date>20030700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6721950</doc-number>
<kind>B1</kind>
<name>Lupu</name>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>6774919</doc-number>
<kind>B2</kind>
<name>Miller et al.</name>
<date>20040800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>6909443</doc-number>
<kind>B1</kind>
<name>Robertson et al.</name>
<date>20050600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>6919891</doc-number>
<kind>B2</kind>
<name>Schneider et al.</name>
<date>20050700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>7064766</doc-number>
<kind>B2</kind>
<name>Beda et al.</name>
<date>20060600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>7119819</doc-number>
<kind>B1</kind>
<name>Robertson et al.</name>
<date>20061000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>7170510</doc-number>
<kind>B2</kind>
<name>Kawahara et al.</name>
<date>20070100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>7170526</doc-number>
<kind>B1</kind>
<name>Johnson</name>
<date>20070100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>7215335</doc-number>
<kind>B2</kind>
<name>Matsumoto et al.</name>
<date>20070500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>7218319</doc-number>
<kind>B2</kind>
<name>Matsumoto et al.</name>
<date>20070500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>7245310</doc-number>
<kind>B2</kind>
<name>Kawahara et al.</name>
<date>20070700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>7277572</doc-number>
<kind>B2</kind>
<name>MacInnes et al.</name>
<date>20071000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382154</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>7290216</doc-number>
<kind>B1</kind>
<name>Kawahara et al.</name>
<date>20071000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>7400322</doc-number>
<kind>B1</kind>
<name>Urbach</name>
<date>20080700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>7432934</doc-number>
<kind>B2</kind>
<name>Salazar et al.</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>7443401</doc-number>
<kind>B2</kind>
<name>Blanco et al.</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>7480873</doc-number>
<kind>B2</kind>
<name>Kawahara</name>
<date>20090100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>7487463</doc-number>
<kind>B2</kind>
<name>Johnson</name>
<date>20090200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>7631277</doc-number>
<kind>B1</kind>
<name>Nie et al.</name>
<date>20091200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>US</country>
<doc-number>7685534</doc-number>
<kind>B2</kind>
<name>Kamen et al.</name>
<date>20100300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>US</country>
<doc-number>7773085</doc-number>
<kind>B2</kind>
<name>Hughes</name>
<date>20100800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00033">
<document-id>
<country>US</country>
<doc-number>7774430</doc-number>
<kind>B2</kind>
<name>Hughes</name>
<date>20100800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00034">
<document-id>
<country>US</country>
<doc-number>7868893</doc-number>
<kind>B2</kind>
<name>Feth et al.</name>
<date>20110100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00035">
<document-id>
<country>US</country>
<doc-number>8042094</doc-number>
<kind>B2</kind>
<name>Napoli et al.</name>
<date>20111000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00036">
<document-id>
<country>US</country>
<doc-number>2001/0040571</doc-number>
<kind>A1</kind>
<name>Miller</name>
<date>20011100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00037">
<document-id>
<country>US</country>
<doc-number>2002/0154214</doc-number>
<kind>A1</kind>
<name>Scallie et al.</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00038">
<document-id>
<country>US</country>
<doc-number>2004/0085310</doc-number>
<kind>A1</kind>
<name>Snuffer</name>
<date>20040500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345419</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00039">
<document-id>
<country>US</country>
<doc-number>2004/0135974</doc-number>
<kind>A1</kind>
<name>Favalora et al.</name>
<date>20040700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00040">
<document-id>
<country>US</country>
<doc-number>2004/0148221</doc-number>
<kind>A1</kind>
<name>Chu</name>
<date>20040700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00041">
<document-id>
<country>US</country>
<doc-number>2004/0174367</doc-number>
<kind>A1</kind>
<name>Liao</name>
<date>20040900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00042">
<document-id>
<country>US</country>
<doc-number>2004/0179262</doc-number>
<kind>A1</kind>
<name>Harman et al.</name>
<date>20040900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00043">
<document-id>
<country>US</country>
<doc-number>2004/0212589</doc-number>
<kind>A1</kind>
<name>Hall et al.</name>
<date>20041000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00044">
<document-id>
<country>US</country>
<doc-number>2005/0041736</doc-number>
<kind>A1</kind>
<name>Butler-Smith et al.</name>
<date>20050200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00045">
<document-id>
<country>US</country>
<doc-number>2005/0081161</doc-number>
<kind>A1</kind>
<name>MacInnes et al.</name>
<date>20050400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00046">
<document-id>
<country>US</country>
<doc-number>2005/0086612</doc-number>
<kind>A1</kind>
<name>Gettman et al.</name>
<date>20050400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00047">
<document-id>
<country>US</country>
<doc-number>2005/0149251</doc-number>
<kind>A1</kind>
<name>Donath et al.</name>
<date>20050700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00048">
<document-id>
<country>US</country>
<doc-number>2005/0179691</doc-number>
<kind>A1</kind>
<name>Johnson</name>
<date>20050800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00049">
<document-id>
<country>US</country>
<doc-number>2005/0179703</doc-number>
<kind>A1</kind>
<name>Johnson</name>
<date>20050800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00050">
<document-id>
<country>US</country>
<doc-number>2005/0182844</doc-number>
<kind>A1</kind>
<name>Johnson et al.</name>
<date>20050800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00051">
<document-id>
<country>US</country>
<doc-number>2005/0204306</doc-number>
<kind>A1</kind>
<name>Kawahara et al.</name>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00052">
<document-id>
<country>US</country>
<doc-number>2005/0253840</doc-number>
<kind>A1</kind>
<name>Kwon</name>
<date>20051100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00053">
<document-id>
<country>US</country>
<doc-number>2005/0281276</doc-number>
<kind>A1</kind>
<name>West et al.</name>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00054">
<document-id>
<country>US</country>
<doc-number>2006/0028479</doc-number>
<kind>A1</kind>
<name>Chun et al.</name>
<date>20060200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00055">
<document-id>
<country>US</country>
<doc-number>2006/0129634</doc-number>
<kind>A1</kind>
<name>Khouzam et al.</name>
<date>20060600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00056">
<document-id>
<country>US</country>
<doc-number>2007/0043550</doc-number>
<kind>A1</kind>
<name>Tzruya</name>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00057">
<document-id>
<country>US</country>
<doc-number>2007/0070066</doc-number>
<kind>A1</kind>
<name>Bakhash</name>
<date>20070300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00058">
<document-id>
<country>US</country>
<doc-number>2007/0124382</doc-number>
<kind>A1</kind>
<name>Hughes</name>
<date>20070500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00059">
<document-id>
<country>US</country>
<doc-number>2007/0171222</doc-number>
<kind>A1</kind>
<name>Kowalski</name>
<date>20070700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00060">
<document-id>
<country>US</country>
<doc-number>2011/0022677</doc-number>
<kind>A1</kind>
<name>Hughes</name>
<date>20110100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00061">
<othercit>Billinghurst, Mark, et al. &#x201c;Mixing realities in shared space: An augmented reality interface for collaborative computing.&#x201d; Multimedia and Expo, 2000. ICME 2000. 2000 IEEE International Conference on. vol. 3. IEEE, 2000.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00062">
<othercit>Mohr, Alex, and Michael Gleicher. &#x201c;HijackGL: reconstructing from streams for stylized rendering.&#x201d; Proceedings of the 2nd international symposium on Non-photorealistic animation and rendering. ACM, 2002.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00063">
<othercit>OpenGL Programming Guide, www.glprogramming.com/red/chapter03.html, Captured Nov. 19, 2005.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00064">
<othercit>Pan, Zhigeng, Xiaochao Wei, and Jian Yang. &#x201c;Geometric model reconstruction from streams of DirectX 3D game application.&#x201d; Proceedings of the 2005 ACM SIGCHI International Conference on Advances in computer entertainment technology. ACM, 2005.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00065">
<othercit>Reitmayr, Gerhard, Mark Billinghurst, and Dieter Schmalstieg. &#x201c;WireAR-legacy applications in augmented reality.&#x201d; Mixed and Augmented Reality, 2003. Proceedings. The Second IEEE and ACM International Symposium on. IEEE, 2003.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00066">
<othercit>International Preliminary Report on Patentability, mailed Sep. 18, 2008 and issued in corresponding International Patent Application No. PCT/US2007/005715, 5 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00067">
<othercit>International Preliminary Report on Patentability, mailed Sep. 18, 2008 and issued in corresponding International Patent Application No. PCT/US2007/005716, 4 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00068">
<othercit>International Preliminary Report on Patentability, mailed Sep. 18, 2008 and issued in corresponding International Patent Application No. PCT/US2007/005717, 5 pages</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00069">
<othercit>PCT International Search Report, mailed Mar. 5, 2008 and issued in related International Patent Application No. PCT/US2007/05715, 2 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00070">
<othercit>PCT International Search Report, mailed Mar. 6, 2008 and issued in related International Patent Application No. PCT/US2007/05717, 2 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00071">
<othercit>PCT International Search Report, mailed May 8, 2008 and issued in related International Patent Application No. PCT/US2007/05716, 2 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>19</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>None</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>15</number-of-drawing-sheets>
<number-of-figures>15</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>12987615</doc-number>
<date>20110110</date>
</document-id>
<parent-status>PENDING</parent-status>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13677920</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>11368451</doc-number>
<date>20060307</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>7868893</doc-number>
<date>20110111</date>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>12987615</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20130069963</doc-number>
<kind>A1</kind>
<date>20130321</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only" applicant-authority-category="assignee">
<addressbook>
<orgname>RPX Corporation</orgname>
<address>
<city>San Francisco</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Feth</last-name>
<first-name>William J.</first-name>
<address>
<city>San Jose</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Hughes</last-name>
<first-name>David William</first-name>
<address>
<city>Oxon</city>
<country>GB</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Boccara</last-name>
<first-name>Michael</first-name>
<address>
<city>Sfar Saba</city>
<country>IL</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Howison &#x26; Arnott, L.L.P.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>RPX Corporation</orgname>
<role>02</role>
<address>
<city>San Francisco</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Nguyen</last-name>
<first-name>Hau</first-name>
<department>2677</department>
</primary-examiner>
<assistant-examiner>
<last-name>Gray</last-name>
<first-name>Ryan M</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">This application describes a system that captures 3D geometry commands from a first 3D graphics process and stores them in a shared memory. A second 3D environment process creates a 3D display environment using a display and display hardware. A third process obtains the 3D commands and supplies them to the hardware to place 3D objects in the 3D environment. The result is a fused display environment where 3D objects are displayed along with other display elements. Input events in the environment are analyzed and mapped to the 3D graphics process or the environment where they affect corresponding processing.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="158.16mm" wi="198.29mm" file="US08624892-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="162.39mm" wi="168.74mm" file="US08624892-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="211.24mm" wi="168.74mm" orientation="landscape" file="US08624892-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="211.67mm" wi="175.77mm" orientation="landscape" file="US08624892-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="207.18mm" wi="175.26mm" orientation="landscape" file="US08624892-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="207.09mm" wi="173.91mm" orientation="landscape" file="US08624892-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="205.23mm" wi="182.88mm" orientation="landscape" file="US08624892-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="208.45mm" wi="171.37mm" file="US08624892-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="224.11mm" wi="150.20mm" file="US08624892-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="183.13mm" wi="160.19mm" file="US08624892-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="192.11mm" wi="135.81mm" orientation="landscape" file="US08624892-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="207.52mm" wi="169.42mm" orientation="landscape" file="US08624892-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="218.10mm" wi="176.19mm" file="US08624892-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="205.66mm" wi="169.08mm" orientation="landscape" file="US08624892-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="152.65mm" wi="163.24mm" orientation="landscape" file="US08624892-20140107-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="148.08mm" wi="158.58mm" orientation="landscape" file="US08624892-20140107-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0002" num="0001">1. Field of the Invention</p>
<p id="p-0003" num="0002">The present invention is directed to a system that integrates the graphical content from one application into the graphical scene of another application and particularly a system that extracts the 3D objects and materials that make up the images generated by a first application from its graphics data stream and fuses them into the second application.</p>
<p id="p-0004" num="0003">2. Description of the Related Art</p>
<p id="p-0005" num="0004">Customers often have many forms of related data ingested and presented by separate applications in separate windows, and even on separate computers in separate locations. For example, in the automotive industry, aerodynamics and crash analysis for a single car might be done using separate data sources and be analyzed in separate applications. If these analyses could be more integrated, it would speed up the decision cycle. In practice there may be many more than two data streams or two applications. This problem becomes even more difficult when the data streams represent 3D information.</p>
<p id="p-0006" num="0005">What is needed is a system that can integrate the view of these stovepipe applications and particularly when three-dimensional (3D) displays are involved.</p>
<heading id="h-0002" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0007" num="0006">It is an aspect of the embodiments discussed herein to provide a system that extracts from one application its 3D objects and materials, which may either comprised of pixel data or 3D geometry and other graphics library definition data, such as textures, colors, surface materials, animations, vertex programs, shading algorithms, etc., and fuses them into another application.</p>
<p id="p-0008" num="0007">It is also an aspect of the embodiments to receive user input device events from the fusion environment, modify them as needed to correspond to user input events expected by the graphics source application and supply them to the graphics source application.</p>
<p id="p-0009" num="0008">A further aspect of this invention is that an unmodified graphics application may serve as the source of the graphics data or as the target of the graphics data. Furthermore, an unmodified graphics application may serve as the target of user input events or as the source of user input events. That is, a given graphics application may act as the sender or receiver of graphics and input information without any modification to the code of the application, although the application does not need to be unmodified to perform in either capacity.</p>
<p id="p-0010" num="0009">The above aspects can be attained by a system that captures 3D graphics library commands including 3D geometry from a first application or the color and depth imagery produced by a first application and supplies them to a second application. In the second application the 3D objects are combined into a scene that may include display elements from other applications. The result is a fused display environment where 3D objects are displayed along with other display elements, such as flat windows, each 3D object or display element potentially coming from a different source application. Input events in the fused environment are analyzed and mapped to the first application where they affect the processing of the first application. In order to supply graphic information from an application to the other, the system may go through an intermediary stage if placing the graphic stream data in a memory that is shared between the two applications, using the operating system's shared memory or using a network protocol. This step actually allows more than two applications to access the graphic stream at the same time, allowing therefore collaboration between the users of the various applications.</p>
<p id="p-0011" num="0010">These together with other aspects and advantages which will be subsequently apparent, reside in the details of construction and operation as more fully hereinafter described and claimed, reference being had to the accompanying drawings forming a part hereof, wherein like numerals refer to like parts throughout.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 1</figref> illustrates a typical process by which a computer graphics program generates an image on a computer display.</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 2</figref> shows a process by which two computer graphics programs generate images on a computer display.</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 3</figref> the process of <figref idref="DRAWINGS">FIG. 2</figref> using two hardware graphics accelerators.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 4</figref> shows capturing 3D graphics commands and transferring them to another application.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 5</figref> shows the capturing of color and depth imagery and transferring them to another application.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 6</figref> shows another approach to capturing and processing function calls and environment inputs.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 7</figref> illustrates the graphics processing operations for the embodiment of <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 8</figref> illustrates the graphics processing operations for the alternate embodiment shown in <figref idref="DRAWINGS">FIG. 5</figref>.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 9</figref> illustrates input processing operations when they come from the fusion application and propagated to the source application.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 10</figref> illustrates the internal structure of the Graphics Device in <figref idref="DRAWINGS">FIG. 1</figref> and serves as an introduction background for the two next figures.</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 11</figref> is a more detailed view of <figref idref="DRAWINGS">FIG. 4</figref>, based on the internal structure of the Graphics Device as described in <figref idref="DRAWINGS">FIG. 10</figref>.</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 12</figref> shows the algorithm that drives the alteration of Graphic commands based on the example of real OpenGL calls and is a more detailed version of <figref idref="DRAWINGS">FIG. 7</figref>.</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 13</figref> shows some methods by which a user can transition from 2D to 3D representations of their application's graphics data.</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIGS. 14 and 15</figref> show dragging and drooping in the environment.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading>
<p id="p-0026" num="0025">Integration of graphical application content into the graphical scene of another application or media fusion can solve the general problem of &#x201c;stovepipe applications&#x201d;. Using various hardware and software sources of visual input, an integration system ingests, combines (&#x201c;fuses&#x201d;), and distributes various types of media streams (e.g. streams of pixels, polygons, user input events), which originate from various sources (e.g. 3D applications, remote desktops/PCs, video recordings, even other media fusion sessions). The system then &#x201c;fuses&#x201d; and displays the media streams side-by-side, superimposed, or combined in any number of other ways. Such a fusion session can also be recorded for later playback or visually served out for remote interaction and collaboration. Visual serving is the ability to stream in real time a view of a graphics application over a network with control passed back to the source application from the remote client.</p>
<p id="p-0027" num="0026">The current Integration of graphical application content into the graphical scene of another application, using video-input cards and Vizserver&#x2122; visual serving technology, brings disparate applications into a common environment. However, the output of these applications (models, drawings, statistics, etc.) is still contained within flat windows.</p>
<p id="p-0028" num="0027">The embodiments of the present invention allow full integration of the application's 3D data content into an integrated 3D landscape. This can be accomplished by intercepting an application's graphics data at any point in the graphics pipeline, which includes the creation and processing of graphical objects, conversion to a raster (pixel) form, and finally the creation of video image on a display surface. For example, near the end of the pipeline the system can extract depth values for every pixel of the application's video output and represent each pixel at a corresponding depth in the media fusion scene (instead of as a fiat 2D window in the media fusion scene). Alternatively, the system can extract the geometric primitives from the application at some point prior to its image generation (e.g. before they are sent to the graphics hardware), and insert the application's 3D objects directly into the 3D Media Fusion scene. These methods provide an improved way to comprehend and interact with applications' data. For example, instead of two 3D graphics applications displaying their visual output within two separate flat windows, possibly on separate computer systems and displays, the 3D data of the two applications is extracted and visually combined (&#x201c;fused&#x201d;) into a common 3D scene such that the data may mutually intersect or occlude each other. An extension of this is that the displayed data may be some derivative of multiple captured streams, for example the sum or difference of two streams of data.</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 1</figref> illustrates a normal process by which a computer graphics program <b>100</b> of a computer system <b>107</b> generates a live image on computer display <b>108</b> using modern computer graphics hardware <b>106</b>. Subsequent figures show a process by which the graphics commands of the graphics program may be captured for the purpose of storing them, modifying them, or transmitting them to other software programs. This is done without modifying the code of the originating graphics program. The originating graphics program is unaware that its graphics commands are being captured and manipulated.</p>
<p id="p-0030" num="0029">Normally, a computer graphics program <b>100</b> utilizes standard graphics software libraries <b>101</b>, such as an OpenGL library, to command computer graphics hardware <b>106</b> to form an image <b>110</b> in the program's window <b>109</b> on the computer display <b>108</b>. The logic of the graphics program executes as a computer process <b>102</b>. The process <b>102</b> invokes a sequence, or stream, of graphics commands a<b>1</b> that are interpreted by the computer graphics library <b>101</b>, namely the OpenGL library, and converted into hardware-specific graphics commands b<b>1</b>.</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 2</figref> shows the normal process by which two computer graphics programs <b>100</b> and <b>111</b> generate live images on computer display <b>108</b> using modern computer graphics hardware <b>106</b>. Normally, two computer graphics programs <b>100</b> and <b>111</b> (with process <b>112</b>) utilize standard graphics software libraries <b>101</b>, such as the OpenGL library, to command computer graphics hardware <b>106</b> to form images <b>110</b> and <b>113</b> in windows <b>109</b> and <b>114</b> respectively, on a computer display <b>108</b>. The graphics software library <b>101</b> accepts both command streams a<b>1</b> and a<b>2</b> and, while keeping the contents of the streams separate, it sends them both to the graphics pipe as two intact streams, b<b>1</b> and b<b>2</b>, destined for separate regions of the output video signal, e.</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 3</figref> illustrates the normal process by which two computer graphics programs <b>100</b> and <b>111</b> generate live images on a computer display using two hardware graphics accelerators <b>106</b><i>a </i>and <b>106</b><i>b</i>. This diagram shows the two graphics programs <b>100</b> and <b>111</b> utilizing the two graphics hardware accelerators <b>106</b><i>a </i>and <b>106</b><i>b </i>to draw their data onto two separate computer monitors <b>108</b><i>a </i>and <b>108</b><i>b</i>. In this case the OpenGL scene <b>113</b> drawn by the second graphics application program is a media fusion environment. Since each hardware graphics accelerator has its own graphics pipeline, each generates its own video signal, e<b>1</b> and e<b>2</b>.</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 4</figref> shows a process by which the graphics commands of one graphics program may be captured for the purpose of storing them, modifying them, or communicating them to other software programs. Furthermore, the diagram shows a method for inserting the captured graphics commands into the 3D scene (into the command stream) of another graphics program. This is done without modifying the code of the originating graphics program. The originating graphics program is unaware that its graphics commands are being captured and manipulated.</p>
<p id="p-0034" num="0033">This embodiment captures the computer graphics commands a<b>1</b> of a 3D graphics program <b>100</b> and later integrates these commands with the commands a<b>2</b> of another computer graphics program <b>111</b>, so that the visual output of both programs is combined, in reality or in appearance only, into a single 3D scene that looks and behaves as if only one graphics program had generated it. More generally, the graphics pipeline may be &#x201c;tapped into&#x201d; at any point between a<b>1</b> and e inclusive, not just at its end points (at command generation, pre-rasterization, or post-rasterization).</p>
<p id="p-0035" num="0034">First, intercept software <b>115</b>, typically in the form of a software library, is loaded into the computer process <b>102</b> of the first graphics program <b>100</b>. The intercept software <b>115</b> converts the graphics program's 3D graphics commands a<b>1</b> into a format f<b>1</b> that can be readily transmitted to other processes. This process is typically called serialization, or encoding, or packing. It is commonly performed on 3D graphics commands by software packages such as OpenGL Multipipe, as sold by Silicon Graphics, Inc., or Chromium, created by Stanford University. Preferably, this takes place application-transparently; that is, without modification of code in graphics program <b>100</b>. Graphics program <b>100</b> is unaware that a copy of its graphics commands is being made. In their readily transmittable format f<b>1</b> the graphics commands can be stored on some permanent storage device for later retrieval, transmitted over a network, or more preferably, placed in shared memory <b>112</b> that is shared between processes.</p>
<p id="p-0036" num="0035">In <figref idref="DRAWINGS">FIG. 4</figref>, graphics program <b>111</b> generates the 3D media fusion environment <b>113</b> into which we wish to insert the graphics output of the first graphics program <b>100</b>.</p>
<p id="p-0037" num="0036">Computer process <b>118</b> contains a program <b>117</b> that reads the commands f<b>1</b> out of shared memory <b>116</b>. Program <b>117</b> draws these graphics commands into the window of graphics program scene <b>113</b>. To do this, the two programs communicate about information in the graphics command stream f<b>1</b> that needs to be modified on the fly to be integrated into the scene <b>113</b>, of graphics program <b>111</b>. Such modifications include correlating the 3D coordinate systems and scene lighting of the two streams, and other visual effects that may require changes to stream f<b>1</b> to visually integrate the result <b>119</b> of graphics commands a<b>1</b>&#x2032; into the 3D scene <b>113</b>, that is produced by graphics commands a<b>2</b>. Notice, for example, the difference in orientation and illumination of 3D object <b>110</b> when drawn by first graphics program <b>100</b>, and after it is modified and drawn as an object <b>119</b> as part of the 3D scene <b>113</b> of the second graphics program <b>111</b>. Additional detailed are provided with respect to <figref idref="DRAWINGS">FIGS. 9</figref>, <b>10</b>, and <b>11</b> for the modifications to the OpenGL command stream to reconcile the 3D coordinate systems of the two applications.</p>
<p id="p-0038" num="0037">Depending on implementation, programs <b>111</b> and <b>117</b> may be combined into a single program (that is a single thread of execution), a single process with separate threads of execution, or as pictured, in two separate processes <b>112</b> and <b>118</b> each with its own thread of execution <b>111</b> and <b>117</b>. Depending on the implementation, therefore, programs <b>111</b> and <b>117</b> may produce a single combined graphics command stream, a<b>2</b>+a<b>1</b> or as depicted here, they may produce separate graphics streams that are later only visually merged into b<b>2</b>+b<b>1</b>&#x2032; by the graphics library <b>101</b>. Each of these implementation alternatives has its set of advantages and drawbacks that will be readily apparent to those skilled in the art.</p>
<p id="p-0039" num="0038">The difference between the combining of graphics command streams b<b>1</b> and b<b>2</b> of the two programs in <figref idref="DRAWINGS">FIG. 2</figref> and the combining of streams b<b>2</b> and b<b>1</b>&#x2032; in this <figref idref="DRAWINGS">FIG. 4</figref> are: A. In <figref idref="DRAWINGS">FIG. 4</figref>, programs <b>111</b> and <b>117</b> coordinate their graphics commands (communicating via g). B. In <figref idref="DRAWINGS">FIG. 2</figref>, graphics programs <b>100</b> and <b>111</b> draw their 3D scenes into two separate windows <b>109</b> and <b>114</b>, on the same monitor. In <figref idref="DRAWINGS">FIG. 4</figref>, both graphics programs <b>111</b> and <b>117</b>, draw their 3D scenes into the same window <b>114</b>, so the drawn outputs <b>113</b> and <b>119</b> appear as if drawn by a single program into a single window.</p>
<p id="p-0040" num="0039">Not only do graphics commands travel from the originating graphics program <b>100</b> to the window of the receiving graphics program <b>111</b> but some user input (keyboard and mouse) commands, received in the normal way by graphics program <b>111</b>, also need to be passed back to the originating graphics program <b>100</b>. To fully create the appearance of a single scene produced by a single graphics program, the user is allowed to manipulate and control the inserted 3D object or 3D scene <b>119</b>, just as he would any other object in the 3D scene of graphics program <b>111</b>: Control needs to be as seamless as if he was controlling the object in its original window <b>109</b>. Input events h are transformed from the 3D scene (&#x201c;world space&#x201d;) back into the 2D coordinate system of the original application's window. Depending on implementation, input event transformation may be handled in whole or in part by any of graphics programs <b>111</b> or <b>117</b>. A transformed event is decoded by input decoder process <b>120</b> and passed as a decoded event i from shared memory <b>116</b> back to application <b>100</b>, often via a regular window server (e.g. an X Window Server).</p>
<p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. 5</figref> shows a process by which depth and color values of the 3D output of one graphics program, rather than its graphics commands, may be captured for the purpose of storing them, modifying them, or communicating them to other software programs. Furthermore, the diagram shows a method for fusing the captured &#x201c;video+depth&#x201d; information into the 3D scene of another graphics program. This is done without modifying the code of the originating graphics program. The originating graphics program is unaware that its graphics output is being captured and manipulated.</p>
<p id="p-0042" num="0041">Unlike the embodiment of <figref idref="DRAWINGS">FIG. 4</figref>, which captures the computer graphics commands a<b>1</b> of a first 3D graphics program <b>100</b> before they are sent to graphics hardware <b>106</b>, the embodiment of <figref idref="DRAWINGS">FIG. 5</figref> captures the fully drawn video output produced by graphics hardware e<b>1</b> as well as a depth image of each video image to make e<b>1</b>&#x2032;: a stream of 2D graphics output+depth. The 2D graphics output+depth is later used to make graphics commands a<b>3</b>, which are merged with the graphics commands a<b>2</b> of a second computer graphics program <b>111</b>, so that the visual output of both programs is combined, in reality or in appearance only, into a single 3D scene that looks and behaves as if only one graphics program had generated it.</p>
<p id="p-0043" num="0042">In this embodiment, graphics intercept library <b>115</b> listens for certain &#x201c;trigger&#x201d; graphics commands, such as &#x201c;glXSwapBuffers&#x201d; commands, rather than capturing all graphics commands. This is so the intercept library can determine when it should retrieve a fully drawn image from a video buffer on the graphics hardware. Imagery may be retrieved from the graphics hardware using common graphics readback or video recording techniques such as those used in the Vizserver collaboration software as sold by Silicon Graphics, Inc. In addition to the normal &#x201c;color image&#x201d; that can be read back from the graphics hardware, this embodiment also retrieves a &#x201c;depth image&#x201d;. The pixels in a depth image indicate the 3D positions of their corresponding pixels in the color image. Intercept library <b>115</b> stores the combined color and depth imagery e<b>1</b>&#x2032; in shared memory <b>116</b>, possibly in an efficient, encoded format, f<b>1</b>.</p>
<p id="p-0044" num="0043">Graphics program <b>117</b> in this embodiment reads color and depth imagery f<b>1</b> out of shared memory <b>116</b>, then it constructs a 3D object from the depth image and applies the color image onto the surface of the object. The program may make automated or semi-automated modifications to f<b>1</b>, for example, to allow for parts of the imagery to be &#x201c;cut out&#x201d;, such as the background of the image: As before, when program <b>117</b> communicates with graphics program <b>111</b> to map 3D coordinate systems and other visual effects of the first graphics application to that of the second graphics application, it may require changes to stream f<b>1</b>. Then, it executes graphics commands a<b>3</b>, which produce a visually composited image <b>119</b> in the 3D scene <b>113</b> that is produced by graphics commands a<b>2</b>. Notice in this embodiment that the orientation and illumination of <b>119</b> is the same as in the original image <b>110</b> since it was derived from the imagery e<b>1</b>&#x2032; of the first application, rather than the 3D commands a<b>1</b> that were used to generate image <b>110</b>. While it is possible to alter image <b>119</b> so that it looks correct from various viewpoints, this embodiment provides less flexibility between the viewpoints of applications <b>100</b> and <b>111</b> than the embodiment of <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0045" num="0044">User inputs are handled in the same manner as in the previous embodiment.</p>
<p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. 6</figref> illustrates in detail the process of decoding from a shared memory the stream of OpenGL graphics commands from a remote application. The remote commands arrive in stream f<b>1</b> and are decoded by decoder <b>202</b>. These calls are passed to a modification step <b>300</b> where the OpenGL function calls may be modified here before they are sent to the graphics library/driver <b>101</b>. Process <b>302</b> provides input to the modification process for controlling orientation and pose of objects in f<b>1</b> and input events h from the media fusion environment <b>111</b>. Remote procedure call (RPC) facilities <b>301</b> are provided for communication between two drawing processes <b>111</b> and <b>117</b> for information, such as user input device events, graphics context switches, other points of synchronization. The input encoder <b>303</b> places filtered/application-window-relative input events h from the environment into shared memory <b>116</b>. An input decoder <b>304</b> decodes the input events obtained from shared memory <b>116</b>. The input event process <b>305</b> acts as a proxy for the user of the fusion environment. It sends artificial input events to an X Window System Server and/or to the graphics application <b>102</b> so that the application <b>102</b> receives the input events in the normal way&#x2014;if a normal user had supplied them.</p>
<p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. 7</figref> illustrates a flowchart of actions that occur in the embodiment of <figref idref="DRAWINGS">FIG. 4</figref> as a result of human interaction in the source graphics application program. The result of these actions being a visual update in the fusion environment. Objects C<b>1</b>, C<b>2</b>, and C<b>3</b> in the flowchart correspond to items <b>100</b>, <b>115</b>, and <b>117</b> in <figref idref="DRAWINGS">FIGS. 4 and 5</figref>.</p>
<p id="p-0048" num="0047">(o<b>1</b>) First the source application C<b>1</b> waits for and eventually receives an event from a user input device (e.g. a key is pressed on the keyboard). (o<b>2</b>) The application may update some internal information, such as the position of a 3D object or its color. (o<b>3</b>) This causes the application to issue new drawing commands to update the visual appearance of the 3D object. (o<b>4</b>) Now the graphics intercept library C<b>2</b> captures the application's drawing commands, and (o<b>5</b>) encodes or packs the commands into a transmittable format before (o<b>6</b>) placing them in shared memory. Now any process C<b>3</b> can decode and draw C<b>1</b>'s 3D commands that C<b>2</b> stored in shared memory, provided that it has (o<b>7</b>) established a connection to the fusion environment program, which draws the information on screen (in some embodiments, that may actually be process C<b>3</b>). After (o<b>8</b>) some further one-time setup procedures, a decoder process may begin to draw the graphics commands it reads from shared memory (o<b>9</b>) as the fusion environment program indicates that it is ready for the decoder to draw.</p>
<p id="p-0049" num="0048">(o<b>10</b>) 3D objects and drawing commands are then drawn iteratively. After the first frame, whose special treatment <figref idref="DRAWINGS">FIG. 11</figref> and its description cover more in depth, the decoder process makes a decision (t<b>1</b>) whether or not to alter the contents of the current drawing command based on its type. If the command must be modified, (o<b>12</b>) it retrieves information about the drawing state of the fusion environment and (o<b>13</b>) alters the command(s) accordingly before (o<b>11</b>) actually executing the drawing command. The decoding and drawing procedure is repeated until (t<b>2</b>) the last graphics command for the current animation frame has been read from the shared memory.</p>
<p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. 8</figref> illustrates a flowchart of actions that occur in the embodiment of <figref idref="DRAWINGS">FIG. 5</figref> as a result of human interaction in the source graphics application program. The result of these actions being a visual update in the fusion environment. Objects C<b>1</b>, C<b>2</b>, and C<b>3</b> in the flowchart correspond to items <b>100</b>, <b>115</b>, and <b>117</b> in <figref idref="DRAWINGS">FIGS. 4 and 5</figref>. The flowchart in <figref idref="DRAWINGS">FIG. 8</figref> is identical to <figref idref="DRAWINGS">FIG. 7</figref> with the following three exceptions: (o<b>33</b>) the decoder process reads color and depth images instead of graphics commands out of shared memory; the decoder process always retrieves position and orientation information from the fusion program; and (o<b>35</b>) the decoder process is responsible for drawing the color and depth imagery in an appropriate way and at the appropriate depth in the scene of the fusion program.</p>
<p id="p-0051" num="0050">The previous process describes the way that user input events on the source application are propagated implicitly to the fusion application via the changes in the 3D content carried by the 3D stream. <figref idref="DRAWINGS">FIG. 9</figref> describes another user input mechanism, this time when it comes from within the fusion application, and is propagated to the source application, so that in the end it influences the 3D data stream and content in both the source and fusion applications.</p>
<p id="p-0052" num="0051">The flowchart in <figref idref="DRAWINGS">FIG. 9</figref> diagrams the chain of events triggered by a user interaction in the fusion environment program that are eventually propagated explicitly back to the graphics data source application. We're supposing that the fusion application has a graphic user interface that allows determining that the target of received user events is actually the fused 3D content coming from the source application. In such case, since the 3D content of the source application is drawn in the context of the fusion program, input events (o<b>37</b>) must be reverse-mapped (o<b>38</b>, o<b>39</b>) back to the context of the source application. For example, the 3D content from the source application may be applied some transformation by the fusion program, like a rotation, in which case the mouse input events must be projected back to the original coordinate system of the source application, so to be sent back (o<b>40</b>) to the source application in the right context. Input events received (o<b>41</b>) within the fusion program, and mapped to the source application's context, may have applied farther processing (o<b>42</b>) before being sent (o<b>43</b>) to the input program, so that although they are fake events from the source application's standpoint, they will be handled as real ones as if they would have come from direct user interaction on the source application. Finally, the source application will handle (o<b>44</b>) those fake user events as real ones, which will cause changes in the 3D content, and these changes will be sent back (o<b>45</b>) to the fusion program, as described in <figref idref="DRAWINGS">FIGS. 7 and 8</figref>. From the fusion program user's viewpoint, he's directly interacting with the 3D content of the source application.</p>
<p id="p-0053" num="0052">The fusion environment is capable of presenting the data of a source application in one of three modes: 1.) 2D mode, 2.) Partial 3D mode, and 3.) Full 3D mode. Referring to <figref idref="DRAWINGS">FIGS. 4 and 5</figref>, in mode 1 only the captured image stream e<b>1</b> produced by the graphics hardware <b>106</b> is displayed in a 2D window suspended in the 3D fusion environment. In mode 2, the captured color and depth images of the source graphics application are combined and redrawn in the fusion environment to produce the same image as in mode 1, but with added 3D &#x201c;relief&#x201d; from the stream of depth imagery, capable of providing novel views of the object. In mode 3, the application's 3D graphics data itself is captured and displayed as a true 3D object in the fusion environment.</p>
<p id="p-0054" num="0053"><figref idref="DRAWINGS">FIGS. 10</figref>, <b>11</b>, and <b>12</b> explain the actions on the 3D commands that enable the fusion of source applications' 3D data into the destination application.</p>
<p id="p-0055" num="0054">The implementation of the 3D fusion depends on the internal architecture of the Graphics Device. Graphics devices can be classified into two categories: the scene graph based devices, and the buffer-based devices. The first category, scene-graph based, includes Raytracers and Radiosity processors, and is based on a an internal copy of the scene graph, i.e. a full tree-like database of all the geometry of the scene, where the 3D commands are actually explicit updates on the scene graph. The second category, buffer-based, includes most of the Accelerated Graphics hardware sold in the market (Nvidia, ATI, etc.), and is based on processing a flow of geometric primitives that are transformed into pixels and accumulated into buffers.</p>
<p id="p-0056" num="0055">When the Graphics device is based on a scene graph, the fusion is straightforward, as it just implies encoding the scene tree of the source application, and decoding it in the destination program before adding a branch in the target scene tree with the sub-scene tree of the source application.</p>
<p id="p-0057" num="0056">The buffer-based Graphics engine is less straightforward, as there is no global knowledge of the scene within the device. In the following sections, we're detailing the process of 3D fusion in this kind of Graphics Devices.</p>
<p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. 10</figref> provides a more detailed view of the Graphics Device's black box, in the buffer-based case. We should understand the graphics device has a pipeline of state machines acting as filters that transform geometric primitives into a video signal. The Geometry Engine <b>103</b> transforms and projects the flow b<b>1</b> of geometric primitives (vertices, triangles, normals, etc) into the coordinate system of the window, by &#x201c;filtering&#x201d; them via a transformation matrix stack. Then, each transformed, projected polygon in stream c<b>1</b> is rasterized into a pixilated representation called a fragment by a Raster Engine, or Fragment Processor <b>104</b>. The stream of pixel fragments d<b>1</b> is then accumulated in the video frame buffer of a Display Engine, or Display Generator <b>105</b>. At periodic intervals, typically every 60th of a second, the display generator <b>106</b> generates a video signal e from the pixels in its video frame buffer containing the transformed and rasterized representation <b>110</b> of the program's graphics commands a<b>1</b> placed in a window <b>109</b> on computer display <b>108</b> which is connected to computer <b>107</b>.</p>
<p id="p-0059" num="0058">More specifically, the stream of graphics commands a<b>1</b> from the application, and likewise the set of hardware-specific graphics commands b<b>1</b> from the graphics library, can be subdivided into 4 types of actions on the graphics pipeline, depending on what part of the Graphics Device they're acting on. The first set of commands, b<b>1</b>-G, contains the geometry and other graphics primitives (vertices, polygons, normals, texture mapping coordinates, etc.). These are pushed to the front of the pipeline. The second set, b<b>1</b>-M, operates on the Geometry Engine's state, typically on the internal transformation matrix stack applied to every geometric primitive. The third, b<b>1</b>-S, operates on the Fragment Processor's state (color, material, textures). And the last ones, b<b>1</b>-F, are direct operations on the video frame buffer, including clearing the buffer, drawing an image directly as pixels.</p>
<p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. 11</figref> illustrates the technique used to integrate the Graphic content of an application into another one. It is a more detailed version of <figref idref="DRAWINGS">FIG. 4</figref>, the detail being based on the internal structure of the Graphic Device as illustrated in <figref idref="DRAWINGS">FIG. 10</figref>. The Graphic Device used by the fusion program basically mixes two contexts of graphic command: one from the fusion scene, and the second from the Graphics commands received from the originating application, and decoded locally. The Graphic commands are redirected differently depending of which category, as detailed in <figref idref="DRAWINGS">FIG. 10</figref>, they belong to. Basically, from the original stream, only the matrix operations, and the direct actions on the video frame buffers, are &#x201c;altered&#x201d; before being sent to the Graphics Device. Others are sent unaltered. The alteration is mostly based on the current transformation matrix of the fusion program at the moment when it starts processing the Graphics commands stream.</p>
<p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. 12</figref> provides additional relative to <figref idref="DRAWINGS">FIG. 7</figref>, i.e. the alteration of commands. It is illustrated with the example of C functions of the OpenGL library, but it is easily generalizable to other APIs like Microsoft's DirectX. Whenever the fusion program's draw thread is ready (o<b>14</b>) to read a frame of the OpenGL commands stream from the source application, it first stores (o<b>15</b>) the full current state of the Graphics device. Then, depending on whether this is or not the first time (t<b>3</b>) the fusion program processes a frame of the 3D commands stream, it will respectively (o<b>16</b>) create or restore (o<b>17</b>) another instance of the Graphics device state, reserved for the source application's 3D commands stream. When processing each of the 3D commands (o<b>18</b>), the commands that will be altered (t<b>4</b>) are typically the non-incremental, non projective operations on the transformation matrix of the Geometry Engine (namely glLoadMatrix* and glLoadIdentity when applied to the OpenGL's model matrix, but (t<b>5</b>) not glFrustum nor glOrtho), and the raster operations that directly modify the target color and depth buffers (namely glClear, glScissor, glDrawPixels, glViewport, etc.). The matrix load operations are replaced (o<b>19</b>) with loading the destination's transformation matrix before the current frame of the shared 3D commands was started to be read, composed with the loaded matrix of the source application. This small alteration is actually doing the 3D fusion, by integration the 3D content of the source application within the 3D coordinate system of the destination 3D world. Projection operations should be ignored, as they are irrelevant in the fusion program, and they would just corrupt its projection algorithm on the target window. Regarding raster operations, coming from the 3D commands stream, they should be simply ignored, to avoid corrupting the color and depth buffer of the fusion program. Another option is to use those commands, together with projection commands, to draw (o<b>20</b>) a &#x201c;virtual frustum&#x201d; that represents the projection observed by the source application. The end of the commands for the current video frame is detected (t<b>6</b>) either by a break in the flow of commands from the shared stream, or by a command that explicitly terminates the drawing for this video frame (namely glXSwapBuffers in the case of a double color buffer in the graphic device). Once all the commands for the video frame have been processed, we should store (o<b>22</b>) the current state as the source application's state, as known as the graphic state of the 3D commands stream. Then, we restore (o<b>23</b>) the fusion program's graphic device state, so that it could continue processing the geometry of its local 3D virtual world.</p>
<p id="p-0062" num="0061">For more information regarding the process of manipulating OpenGL matrices for the purpose of correcting the orientation or appearance of 3D objects without modifying the code of an application, the reader may refer to U.S. Pat. No. 6,982,682, which utilizes a similar OpenGL matrix manipulation process for correcting images drawn onto curved surfaces.</p>
<p id="p-0063" num="0062">In addition to manipulating graphics commands related to geometry transformation, other graphics commands may be altered, added, or removed to improve the integration of a foreign graphics stream into the fusion environment. This includes, for example, commands that affect lighting, raster (2D) drawing, textures and other surface material properties, vertex programs, fragment shaders, and the recording and playback of command macros known as display lists.</p>
<p id="p-0064" num="0063">The diagrams in <figref idref="DRAWINGS">FIG. 13</figref> show that to transition between these three modes of operation, a user may utilize menus, buttons, or other user interface elements or techniques, including a method of dragging and dropping a 2D object or icon into a fusion environment to begin to display the stream of 3D data. For example, the user may drag D<b>1</b>-D<b>3</b> into the fusion environment an icon of an application or stream he wishes to initiate, or he may drag E<b>1</b>-E<b>3</b> the contents of a 2D window into the 3D scene of the fusion environment to switch from mode 1 (2D window) to modes 2 or 3 (partial or full 3D window).</p>
<p id="p-0065" num="0064">An implementation that supports such a 2D to full 3D transition (from mode 1 to mode 3) must include the ability to retrieve on demand or track and record the changes to important graphics state values in the source application such as transformation matrices, lighting modes, current color information, and many other graphics state parameters. Even if the application is only represented as a 2D picture, these states must be tracked from the moment the source application begins making drawing commands in case the user decides at a later time to transition from a mode 1 window to a mode 3 &#x201c;3D window&#x201d;. Otherwise the source application must temporarily suspend graphics operations and incur an expensive retrieval of graphics state information for the decoder process or fusion environment to begin drawing the remote application's graphics commands.</p>
<p id="p-0066" num="0065">An implementation that supports a 2D to partial 3D transition (from mode 1 to mode 2) does not need to perform any graphics state tracking or retrieval. It is therefore more convenient to create an implementation that converts from 2D to partial 3D in this way, but such an implementation places limits on the motion of the user's viewpoint without forcing the source application to redraw the image as the viewpoint changes.</p>
<p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. 14</figref> shows a method by which the user may utilize menus, buttons, or other user interface elements or techniques, including a drag and drop F<b>1</b>-F<b>3</b> methodology to initiate a mode where the 3D application stream fills the entire 3D space of the fusion environment, similar to the way a modern 2D computer application may have a &#x201c;full screen&#x201d; mode in which its 2D window covers other windows on the screen and takes over the entire computer screen. Some elements of the user's environment may remain visible while other interface elements become obscured when the application fills the fusion environment.</p>
<p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. 15</figref> shows a method by which the user may utilize menus, buttons, or other user interface elements or techniques, including a drag and drop methodology to initiate a mode where the content is dragged G<b>1</b>-G<b>3</b> into the 3D space of the fusion environment, similar to the way a modern 2D computer application may have a &#x201c;full screen&#x201d; mode in which its 2D window covers other windows on the screen and takes over the entire computer screen. Some elements of the user's environment may remain visible while other interface elements become obscured when the application fills the fusion environment.</p>
<p id="p-0069" num="0068">Other variations of the invention can be provided include the ability to replicate and distribute the captured streams via a form of broadcast function. Also, the ability to use captured streams as sources for some form of function, i.e. looking for interference, by doing a difference, construction via addition or even some algorithmic process applied to the input streams to create a derivative. Another aspect is the ability to use this to record states of development, for example, where this capture process can create a permanent record of different phases of a project by capturing and putting to storage. This may be used by capturing versions from a number of users at a particular point in time and keeping this as a snapshot for later review or audit. Another element to consider is the provision of a repository for common parts that can generated once and then shared with remote users. Another aspect is shared collaboration where data is captured via a fusion server and then made available to individuals or collaborators to work on jointly.</p>
<p id="p-0070" num="0069">Fused content from the fusion environment may be fed back into the original source application as mentioned previously. This is so all applications supplying 3D objects into the fusion environment will see in their 3D scenes, the 3D objects supplied by all the other applications. Essentially, every participating application can also become a fusion environment, This facilitates remote collaboration.</p>
<p id="p-0071" num="0070">With or without user assistance, meaningful &#x201c;slices&#x201d; of an OpenGL stream may be selectively extracted and drawn in the fusion environment, where a &#x201c;slice&#x201d; could be of time or of space. This capability could be manifested as a cutaway view, &#x201c;3D screen capture&#x201d;, or 3D movie recorder. Some of these 3D slicing capabilities may be found in the software HijackGL, created by The University of Wisconsin-Madison.</p>
<p id="p-0072" num="0071">The many features and advantages of the invention are apparent from the detailed specification and, thus, it is intended by the appended claims to cover all such features and advantages of the invention that fall within the true spirit and scope of the invention. Further, since numerous modifications and changes will readily occur to those skilled in the art, it is not desired to limit the invention to the exact construction and operation illustrated and described, and accordingly all suitable modifications and equivalents may be resorted to, falling within the scope of the invention.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method, comprising:
<claim-text>intercepting, by an intercept module, buffer-based architecture three-dimensional (3D) imagery produced by a first application, wherein the 3D imagery is not provided by the first application for consumption by the intercept module;</claim-text>
<claim-text>extracting a two-dimensional (2D) imagery, color imagery and depth imagery from the intercepted 3D imagery;</claim-text>
<claim-text>combining the 2D imagery, the color imagery and the depth imagery with a 3D graphics command stream of a second application to drive display hardware in order to integrate a 3D object based on the two-dimensional (2D) imagery, the color imagery and the depth imagery into a 3D scene, using a same 3D coordinate system of the 3D scene, based on the 3D graphics command stream of the second application, wherein the 3D scene is independent of the first application, wherein the combining further comprises modifying the 2D imagery, the color imagery and the depth imagery in order to visually integrate the 3D object into the 3D scene.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising producing an integrated 3D scene comprising the integrated 3D object and the 3D scene generated from the 2D imagery, the color imagery and the depth imagery in the display hardware.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the modifying comprises correlating 3D coordinate systems and scene lighting of the 3D object and the 3D scene.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the modifying comprises correlating visual effects of the 3D object and the 3D scene.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising driving a same graphics accelerator hardware with the 2D imagery, the color imagery and the depth imagery and the 3D graphics command stream.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising driving another display hardware with the 2D imagery, the color imagery and the depth imagery.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising storing the 2D imagery, the color imagery and the depth imagery in a memory that is shared among different processing threads, or distributed over the network between several computer hosts.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising transforming data of the 2D imagery, the color imagery and the depth imagery from a first application data type to a second application data type.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising creating a 3D media fusion environment at the second application.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the intercepting comprises copying function calls arriving at call entry points of the first application.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method as recited in <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the combining further comprises passing the copied function calls to entry points of the second application.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method as recited in <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the combining further comprises modifying the passed function responsive to a 3D fusion environment.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. A system, comprising:
<claim-text>graphics hardware configured to produce display signals;</claim-text>
<claim-text>a display configured to produce an image using the display signals;</claim-text>
<claim-text>a shared memory; and</claim-text>
<claim-text>a computer comprising:
<claim-text>a three-dimensional (3D) graphics process configured to generate buffer-based architecture 3D graphics imagery;</claim-text>
<claim-text>an intercept process configured to extract a two-dimensional (2D) graphics imagery, color imagery and depth imagery from the 3D graphics imagery produced by the 3D graphics process and, wherein the 3D graphics imagery is not provided by the 3D graphics process for consumption by the intercept process, and storing the 2D graphics imagery, the color imagery and the depth imagery in the shared memory;</claim-text>
<claim-text>a 3D environment process configured to generate a 3D graphics command stream for a 3D fusion environment and supply the 3D graphics command stream to the graphics hardware;</claim-text>
<claim-text>an integration process configured to integrate a 3D object based on the stored 2D graphics imagery, the color imagery and the depth imagery into the 3D fusion environment, using a same 3D coordinate system of the 3D fusion environment, based on the 3D graphics command stream of the second application, wherein the 3D fusion environment is independent of the 3D graphics process, wherein the integration process is further configured to modify the 2D imagery, the color imagery and the depth imagery in order to visually integrate the 3D object into the 3D fusion environment.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The system as recited in <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising:
<claim-text>second graphics hardware receiving the 2D imagery, the color imagery and the depth imagery; and</claim-text>
<claim-text>a second display producing a display for the 2D imagery, the color imagery and the depth imagery.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, the computer further comprising a mapping process configured to map inputs to the 3D fusion environment to the 3D graphics process and the 3D environment process.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. A display, comprising:
<claim-text>a three-dimensional (3D) fusion display environment displayed in a window, independent of a first process, created by a buffer-based architecture 3D graphics command stream generated by a second process;</claim-text>
<claim-text>a 3D object created by two-dimensional (2D) graphics imagery, color imagery and depth imagery generated from 3D graphics imagery generated by a first process, intercepted by an intercept module, and integrated into the 3D fusion display environment, using a same 3D coordinate system of the 3D fusion display environment, wherein the 2D graphics imagery, the color imagery and the depth imagery are not provided by the first process for consumption by the intercept module, wherein the combining further comprises modifying the 2D imagery, the color imagery and the depth imagery in order to visually integrate the 3D object into the 3D fusion display environment; and</claim-text>
<claim-text>display hardware configured to display the integrated 3D fusion display environment.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The display as recited in <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein data can be dragged and dropped into the window.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The display as recited in <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein transparent collaboration on the 3D object and the 3D fusion display environment displayed in the window can be performed without altering or recompiling source code of a source application.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. A non-transitory computer-readable medium having stored thereon computer-executable instructions that, if executed by a computing device, cause the computing device to perform a method comprising:
<claim-text>intercepting, by an intercept module, buffer-based architecture three-dimensional (3D) imagery produced by a first application, wherein the 3D imagery is not provided by the first application for consumption by the intercept module;</claim-text>
<claim-text>extracting a two-dimensional (2D) imagery, the color imagery and the depth imagery from the intercepted 3D imagery;</claim-text>
<claim-text>combining the 2D imagery, color imagery and depth imagery with a 3D graphics command stream of a second application to drive display hardware in order to integrate a 3D object based on the two-dimensional (2D) imagery, the color imagery and the depth imagery into a 3D scene, using a same 3D coordinate system of the 3D scene, based on the 3D graphics command stream of the second application, wherein the 3D scene is independent of the first application, wherein the combining further comprises modifying the 2D imagery, the color imagery and the depth imagery in order to visually integrate the 3D object into the 3D scene.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
