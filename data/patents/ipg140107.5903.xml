<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08627012-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08627012</doc-number>
<kind>B1</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13341345</doc-number>
<date>20111230</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>12</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>711135</main-classification>
<further-classification>710 57</further-classification>
<further-classification>707657</further-classification>
<further-classification>707662</further-classification>
</classification-national>
<invention-title id="d2e43">System and method for improving cache performance</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>6119209</doc-number>
<kind>A</kind>
<name>Bauman et al.</name>
<date>20000900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6157991</doc-number>
<kind>A</kind>
<name>Arnon</name>
<date>20001200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6173369</doc-number>
<kind>B1</kind>
<name>Nguyen et al.</name>
<date>20010100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6434681</doc-number>
<kind>B1</kind>
<name>Armangau</name>
<date>20020800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711162</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6496900</doc-number>
<kind>B1</kind>
<name>McDonald et al.</name>
<date>20021200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711112</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6587937</doc-number>
<kind>B1</kind>
<name>Jensen et al.</name>
<date>20030700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6604171</doc-number>
<kind>B1</kind>
<name>Sade</name>
<date>20030800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6725336</doc-number>
<kind>B2</kind>
<name>Cherabuddi</name>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6728836</doc-number>
<kind>B1</kind>
<name>Lambright et al.</name>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6757785</doc-number>
<kind>B2</kind>
<name>Brutman et al.</name>
<date>20040600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>7293196</doc-number>
<kind>B2</kind>
<name>Hicken et al.</name>
<date>20071100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>7421538</doc-number>
<kind>B2</kind>
<name>Bita et al.</name>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>7574538</doc-number>
<kind>B1</kind>
<name>Yochai</name>
<date>20090800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>7761680</doc-number>
<kind>B2</kind>
<name>Ash et al.</name>
<date>20100700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>7766418</doc-number>
<kind>B2</kind>
<name>Hemmersmeier</name>
<date>20100800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>7769951</doc-number>
<kind>B2</kind>
<name>Lu et al.</name>
<date>20100800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>7769952</doc-number>
<kind>B2</kind>
<name>Hashimoto et al.</name>
<date>20100800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>7814270</doc-number>
<kind>B2</kind>
<name>Shimada</name>
<date>20101000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>8365169</doc-number>
<kind>B1</kind>
<name>Watson et al.</name>
<date>20130100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>2003/0070043</doc-number>
<kind>A1</kind>
<name>Merkey</name>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>2003/0084252</doc-number>
<kind>A1</kind>
<name>Talagala</name>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711135</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>2004/0059869</doc-number>
<kind>A1</kind>
<name>Orsley</name>
<date>20040300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711114</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>2004/0117441</doc-number>
<kind>A1</kind>
<name>Liu et al.</name>
<date>20040600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709203</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>2004/0205295</doc-number>
<kind>A1</kind>
<name>O'Connor et al.</name>
<date>20041000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>2005/0071379</doc-number>
<kind>A1</kind>
<name>Kekre et al.</name>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>707200</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>2005/0216536</doc-number>
<kind>A1</kind>
<name>Stager et al.</name>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>707204</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>2006/0005189</doc-number>
<kind>A1</kind>
<name>Vega et al.</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>2006/0143383</doc-number>
<kind>A1</kind>
<name>Zohar et al.</name>
<date>20060600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>2006/0143390</doc-number>
<kind>A1</kind>
<name>Kottapalli</name>
<date>20060600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>2006/0168395</doc-number>
<kind>A1</kind>
<name>Deng et al.</name>
<date>20060700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711113</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>US</country>
<doc-number>2006/0179229</doc-number>
<kind>A1</kind>
<name>Clark et al.</name>
<date>20060800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>US</country>
<doc-number>2007/0050540</doc-number>
<kind>A1</kind>
<name>Klein</name>
<date>20070300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00033">
<document-id>
<country>US</country>
<doc-number>2007/0198612</doc-number>
<kind>A1</kind>
<name>Prahlad et al.</name>
<date>20070800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00034">
<document-id>
<country>US</country>
<doc-number>2007/0220207</doc-number>
<kind>A1</kind>
<name>Black et al.</name>
<date>20070900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00035">
<document-id>
<country>US</country>
<doc-number>2008/0215528</doc-number>
<kind>A1</kind>
<name>Sedlar</name>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00036">
<document-id>
<country>US</country>
<doc-number>2008/0320209</doc-number>
<kind>A1</kind>
<name>Lee et al.</name>
<date>20081200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00037">
<document-id>
<country>US</country>
<doc-number>2009/0113149</doc-number>
<kind>A1</kind>
<name>Kondo et al.</name>
<date>20090400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00038">
<document-id>
<country>US</country>
<doc-number>2009/0313435</doc-number>
<kind>A1</kind>
<name>Thantry et al.</name>
<date>20091200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00039">
<document-id>
<country>US</country>
<doc-number>2010/0100696</doc-number>
<kind>A1</kind>
<name>Suzuki</name>
<date>20100400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00040">
<document-id>
<country>US</country>
<doc-number>2010/0228919</doc-number>
<kind>A1</kind>
<name>Stabrawa et al.</name>
<date>20100900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00041">
<document-id>
<country>US</country>
<doc-number>2010/0235580</doc-number>
<kind>A1</kind>
<name>Bouvier</name>
<date>20100900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00042">
<document-id>
<country>US</country>
<doc-number>2010/0250833</doc-number>
<kind>A1</kind>
<name>Trika</name>
<date>20100900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00043">
<document-id>
<country>US</country>
<doc-number>2011/0258393</doc-number>
<kind>A1</kind>
<name>Flower et al.</name>
<date>20111000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00044">
<document-id>
<country>US</country>
<doc-number>2011/0264845</doc-number>
<kind>A1</kind>
<name>Choi</name>
<date>20111000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00045">
<document-id>
<country>US</country>
<doc-number>2012/0005468</doc-number>
<kind>A1</kind>
<name>Yu</name>
<date>20120100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00046">
<document-id>
<country>US</country>
<doc-number>2012/0173653</doc-number>
<kind>A1</kind>
<name>Bland et al.</name>
<date>20120700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00047">
<document-id>
<country>US</country>
<doc-number>2012/0221785</doc-number>
<kind>A1</kind>
<name>Chung et al.</name>
<date>20120800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00048">
<document-id>
<country>US</country>
<doc-number>2012/0239860</doc-number>
<kind>A1</kind>
<name>Atkisson et al.</name>
<date>20120900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00049">
<document-id>
<country>US</country>
<doc-number>2012/0324443</doc-number>
<kind>A1</kind>
<name>Low et al.</name>
<date>20121200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00050">
<document-id>
<country>US</country>
<doc-number>2013/0024920</doc-number>
<kind>A1</kind>
<name>Rodriguez</name>
<date>20130100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>23</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>711103</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>711135</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707657</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707662</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>710 57</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>15</number-of-drawing-sheets>
<number-of-figures>17</number-of-figures>
</figures>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Derbeko</last-name>
<first-name>Philip</first-name>
<address>
<city>Modiin</city>
<country>IL</country>
</address>
</addressbook>
<residence>
<country>IL</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Natanzon</last-name>
<first-name>Assaf</first-name>
<address>
<city>Tel Aviv</city>
<country>IL</country>
</address>
</addressbook>
<residence>
<country>IL</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Eyal</last-name>
<first-name>Anat</first-name>
<address>
<city>Tel Aviv</city>
<country>IL</country>
</address>
</addressbook>
<residence>
<country>IL</country>
</residence>
</us-applicant>
<us-applicant sequence="004" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Erel</last-name>
<first-name>David</first-name>
<address>
<city>Shoham</city>
<country>IL</country>
</address>
</addressbook>
<residence>
<country>IL</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Derbeko</last-name>
<first-name>Philip</first-name>
<address>
<city>Modiin</city>
<country>IL</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Natanzon</last-name>
<first-name>Assaf</first-name>
<address>
<city>Tel Aviv</city>
<country>IL</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Eyal</last-name>
<first-name>Anat</first-name>
<address>
<city>Tel Aviv</city>
<country>IL</country>
</address>
</addressbook>
</inventor>
<inventor sequence="004" designation="us-only">
<addressbook>
<last-name>Erel</last-name>
<first-name>David</first-name>
<address>
<city>Shoham</city>
<country>IL</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<last-name>Colandreo</last-name>
<first-name>Brian J.</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
<agent sequence="02" rep-type="attorney">
<addressbook>
<last-name>Whittenberger</last-name>
<first-name>Mark H.</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
<agent sequence="03" rep-type="attorney">
<addressbook>
<orgname>Holland &#x26; Knight LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>EMC Corporation</orgname>
<role>02</role>
<address>
<city>Hopkinton</city>
<state>MA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Parikh</last-name>
<first-name>Kalpit</first-name>
<department>2187</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A method, computer program product, and computing system for receiving, on a cache system, a plurality of data write requests, wherein each data write request identifies a data portion to be written to a data array associated with the cache system. The data portions associated with the data write requests are written to the cache system. The data portions associated with the data write requests are queued until the occurrence of a commit event. Upon the occurrence of the commit event, a consolidated write operation is performed to write the data portions associated with the data write requests to the data array.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="186.01mm" wi="214.63mm" file="US08627012-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="254.08mm" wi="202.61mm" orientation="landscape" file="US08627012-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="258.49mm" wi="215.22mm" file="US08627012-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="256.46mm" wi="211.24mm" orientation="landscape" file="US08627012-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="190.92mm" wi="215.90mm" orientation="landscape" file="US08627012-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="224.54mm" wi="215.90mm" orientation="landscape" file="US08627012-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="258.74mm" wi="214.55mm" file="US08627012-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="194.90mm" wi="215.90mm" orientation="landscape" file="US08627012-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="198.20mm" wi="215.90mm" orientation="landscape" file="US08627012-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="258.15mm" wi="214.88mm" file="US08627012-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="225.89mm" wi="215.90mm" orientation="landscape" file="US08627012-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="168.23mm" wi="209.97mm" orientation="landscape" file="US08627012-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="221.83mm" wi="209.21mm" orientation="landscape" file="US08627012-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="262.81mm" wi="215.22mm" orientation="landscape" file="US08627012-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="240.79mm" wi="212.85mm" orientation="landscape" file="US08627012-20140107-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="186.61mm" wi="215.90mm" orientation="landscape" file="US08627012-20140107-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">TECHNICAL FIELD</heading>
<p id="p-0002" num="0001">This disclosure relates to cache memory systems and, more particularly, to systems and methods for improving the performance of cache memory systems.</p>
<heading id="h-0002" level="1">BACKGROUND</heading>
<p id="p-0003" num="0002">Storing and safeguarding electronic content is of paramount importance in modern business. Accordingly, various systems may be employed to protect such electronic content.</p>
<p id="p-0004" num="0003">The use of solid-state storage devices is increasing in popularity. A solid state storage device is a content storage device that uses solid-state memory to store persistent content. A solid-state storage device may emulate (and therefore replace) a conventional hard disk drive. Additionally/alternatively, a solid state storage device may be used within a cache memory system. With no moving parts, a solid-state storage device largely eliminates (or greatly reduces) seek time, latency and other electromechanical delays and failures associated with a conventional hard disk drive.</p>
<heading id="h-0003" level="1">SUMMARY OF DISCLOSURE</heading>
<p id="p-0005" num="0004">In a first implementation, a computer-implemented method includes receiving, on a cache system, a plurality of data write requests, wherein each data write request identifies a data portion to be written to a data array associated with the cache system. The data portions associated with the data write requests are written to the cache system. The data portions associated with the data write requests are queued until the occurrence of a commit event. Upon the occurrence of the commit event, a consolidated write operation is performed to write the data portions associated with the data write requests to the data array.</p>
<p id="p-0006" num="0005">One or more of the following features may be included. The commit event may be chosen from the group consisting of: the expiry of a commit timer; and the receipt of a defined quantity of data portions. Performing the consolidated write operation may include writing the data portions associated with the data write requests to the data array. Performing the consolidated write operation further may include receiving a single write confirmation from the data array in response to writing the data portions associated with the data write requests to the data array. The cache system may include one or more flash memory storage devices. The data array may include one or more electro-mechanical storage devices. The cache system may be a content-aware cache system.</p>
<p id="p-0007" num="0006">In another implementation, a computer program product resides on a computer readable medium that has a plurality of instructions stored on it. When executed by a processor, the instructions cause the processor to perform operations including receiving, on a cache system, a plurality of data write requests, wherein each data write request identifies a data portion to be written to a data array associated with the cache system. The data portions associated with the data write requests are written to the cache system. The data portions associated with the data write requests are queued until the occurrence of a commit event. Upon the occurrence of the commit event, a consolidated write operation is performed to write the data portions associated with the data write requests to the data array.</p>
<p id="p-0008" num="0007">One or more of the following features may be included. The commit event may be chosen from the group consisting of: the expiry of a commit timer; and the receipt of a defined quantity of data portions. Performing the consolidated write operation may include writing the data portions associated with the data write requests to the data array. Performing the consolidated write operation further may include receiving a single write confirmation from the data array in response to writing the data portions associated with the data write requests to the data array. The cache system may include one or more flash memory storage devices. The data array may include one or more electro-mechanical storage devices. The cache system may be a content-aware cache system.</p>
<p id="p-0009" num="0008">In another implementation, a computing system includes at least one processor and at least one memory architecture coupled with the at least one processor, wherein the computing system is configured to perform operations including receiving, on a cache system, a plurality of data write requests, wherein each data write request identifies a data portion to be written to a data array associated with the cache system. The data portions associated with the data write requests are written to the cache system. The data portions associated with the data write requests are queued until the occurrence of a commit event. Upon the occurrence of the commit event, a consolidated write operation is performed to write the data portions associated with the data write requests to the data array.</p>
<p id="p-0010" num="0009">One or more of the following features may be included. The commit event may be chosen from the group consisting of: the expiry of a commit timer; and the receipt of a defined quantity of data portions. Performing the consolidated write operation may include writing the data portions associated with the data write requests to the data array. Performing the consolidated write operation further may include receiving a single write confirmation from the data array in response to writing the data portions associated with the data write requests to the data array. The cache system may include one or more flash memory storage devices. The data array may include one or more electro-mechanical storage devices. The cache system may be a content-aware cache system.</p>
<p id="p-0011" num="0010">In another implementation, a computer-implemented method includes receiving write requests on a server computer/controller. The content related to the write requests is written to a cache system. The content related to the write requests is queued until the occurrence of a commit event. Upon the occurrence of the commit event, a data array is notified of the upcoming commit event. A snapshot of the portion of the data array for which the above-described write requests concern is taken. The content related to the write requests is sent to the data array. A write operation is performed on the data array with respect to the content related to the write requests.</p>
<p id="p-0012" num="0011">One or more of the following features may be included. The data array may be notified that the commit event is complete. The snapshot may be deleted from the data array. In the event of a crash during the commit event, the portion of the data array may be restored based, at least in part, upon the snapshot. The cache system may include one or more flash memory storage devices. The data array may include one or more electro-mechanical storage devices.</p>
<p id="p-0013" num="0012">The details of one or more implementations are set forth in the accompanying drawings and the description below. Other features and advantages will become apparent from the description, the drawings, and the claims.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 1</figref> is a diagrammatic view of a storage system and a data caching process coupled to a distributed computing network;</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 2</figref> is a diagrammatic view of the storage system of <figref idref="DRAWINGS">FIG. 1</figref>;</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 3</figref> is a diagrammatic view of a data write request for use with the data caching process of <figref idref="DRAWINGS">FIG. 1</figref>;</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 4</figref> is a diagrammatic view of a data read request for use with the data caching process of <figref idref="DRAWINGS">FIG. 1</figref>;</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 5</figref> is a diagrammatic view of a content directory for use with the data caching process of <figref idref="DRAWINGS">FIG. 1</figref>;</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 6</figref> is a first flow chart of the data caching process of <figref idref="DRAWINGS">FIG. 1</figref>;</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 6A</figref> is a more-detailed version of the flow chart of <figref idref="DRAWINGS">FIG. 6</figref>;</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 7</figref> is a diagrammatic view of an alternative embodiment of the storage system of <figref idref="DRAWINGS">FIG. 1</figref>;</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 8</figref> is a second flow chart of the data caching process of <figref idref="DRAWINGS">FIG. 1</figref>;</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 9</figref> is a third flow chart of the data caching process of <figref idref="DRAWINGS">FIG. 1</figref>;</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 10</figref> is a diagrammatic view of an alternative embodiment of the storage system of <figref idref="DRAWINGS">FIG. 1</figref>;</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 11</figref> is a fourth flow chart of the data caching process of <figref idref="DRAWINGS">FIG. 1</figref>;</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 12</figref> is a fifth flow chart of the data caching process of <figref idref="DRAWINGS">FIG. 1</figref>;</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 12A</figref> is a more-detailed version of the flow chart of <figref idref="DRAWINGS">FIG. 13</figref>;</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 13A</figref> is a diagrammatic view of an alternative embodiment of the storage system of <figref idref="DRAWINGS">FIG. 1</figref>;</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 13B</figref> is a diagrammatic view of an alternative embodiment of the storage system of <figref idref="DRAWINGS">FIG. 1</figref>; and</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 14</figref> is a sixth flow chart of the data caching process of <figref idref="DRAWINGS">FIG. 1</figref>.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<p id="p-0031" num="0030">Like reference symbols in the various drawings indicate like elements.</p>
<heading id="h-0005" level="1">DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading>
<p id="h-0006" num="0000">General Information:</p>
<p id="p-0032" num="0031">As will be appreciated by one skilled in the art, the present disclosure may be embodied as a method, system, or computer program product. Accordingly, the present disclosure may take the form of an entirely hardware embodiment, an entirely software embodiment (including firmware, resident software, micro-code, etc.) or an embodiment combining software and hardware aspects that may all generally be referred to herein as a &#x201c;circuit,&#x201d; &#x201c;module&#x201d; or &#x201c;system.&#x201d; Furthermore, the present disclosure may take the form of a computer program product on a computer-usable storage medium having computer-usable program code embodied in the medium.</p>
<p id="p-0033" num="0032">Any suitable computer usable or computer readable medium may be utilized. The computer-usable or computer-readable medium may be, for example but not limited to, an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system, apparatus, device, or propagation medium. More specific examples (a non-exhaustive list) of the computer-readable medium would include the following: an electrical connection having one or more wires, a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory), an optical fiber, a portable compact disc read-only memory (CD-ROM), an optical storage device, a transmission media such as those supporting the Internet or an intranet, or a magnetic storage device. Note that the computer-usable or computer-readable medium could even be paper or another suitable medium upon which the program is printed, as the program can be electronically captured, via, for instance, optical scanning of the paper or other medium, then compiled, interpreted, or otherwise processed in a suitable manner, if necessary, and then stored in a computer memory. In the context of this document, a computer-usable or computer-readable medium may be any medium that can contain, store, communicate, propagate, or transport the program for use by or in connection with the instruction execution system, apparatus, or device. The computer-usable medium may include a propagated data signal with the computer-usable program code embodied therewith, either in baseband or as part of a carrier wave. The computer usable program code may be transmitted using any appropriate medium, including but not limited to the Internet, wireline, optical fiber cable, RF, etc.</p>
<p id="p-0034" num="0033">Computer program code for carrying out operations of the present disclosure may be written in an object oriented programming language such as Java, Smalltalk, C++ or the like. However, the computer program code for carrying out operations of the present disclosure may also be written in conventional procedural programming languages, such as the &#x201c;C&#x201d; programming language or similar programming languages. The program code may execute entirely on the user's computer, partly on the user's computer, as a stand-alone software package, partly on the user's computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario, the remote computer may be connected to the user's computer through a local area network (LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider).</p>
<p id="p-0035" num="0034">The present disclosure is described below with reference to flowchart illustrations and/or block diagrams of methods, apparatus (systems) and computer program products according to embodiments of the disclosure. It will be understood that each block of the flowchart illustrations and/or block diagrams, and combinations of blocks in the flowchart illustrations and/or block diagrams, can be implemented by computer program instructions. These computer program instructions may be provided to a processor of a general purpose computer, special purpose computer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute via the processor of the computer or other programmable data processing apparatus, create means for implementing the functions/acts specified in the flowchart and/or block diagram block or blocks.</p>
<p id="p-0036" num="0035">These computer program instructions may also be stored in a computer-readable memory that can direct a computer or other programmable data processing apparatus to function in a particular manner, such that the instructions stored in the computer-readable memory produce an article of manufacture including instruction means which implement the function/act specified in the flowchart and/or block diagram block or blocks.</p>
<p id="p-0037" num="0036">The computer program instructions may also be loaded onto a computer or other programmable data processing apparatus to cause a series of operational steps to be performed on the computer or other programmable apparatus to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide steps for implementing the functions/acts specified in the flowchart and/or block diagram block or blocks.</p>
<p id="h-0007" num="0000">System Overview:</p>
<p id="p-0038" num="0037">Referring to <figref idref="DRAWINGS">FIG. 1</figref>, there is shown data caching process <b>10</b> that may reside on and may be executed by storage system <b>12</b>, which may be connected to network <b>14</b> (e.g., the Internet or a local area network). Examples of storage system <b>12</b> may include, but are not limited to: a Network Attached Storage (NAS) system, a Storage Area Network (SAN), a personal computer with a memory system, a server computer with a memory system, and a cloud-based device with a memory system.</p>
<p id="p-0039" num="0038">As is known in the art, a SAN may include one or more of a personal computer, a server computer, a series of server computers, a mini computer, a mainframe computer, a RAID device and a NAS system. The various components of storage system <b>12</b> may execute one or more operating systems, examples of which may include but are not limited to: Microsoft Windows XP Server&#x2122;; Novell Netware&#x2122;; Redhat Linux&#x2122;, Unix, or a custom operating system, for example.</p>
<p id="p-0040" num="0039">The instruction sets and subroutines of data caching process <b>10</b>, which may be stored on storage device <b>16</b> included within storage system <b>12</b>, may be executed by one or more processors (not shown) and one or more memory architectures (not shown) included within storage system <b>12</b>. Storage device <b>16</b> may include but is not limited to: a hard disk drive; a tape drive; an optical drive; a RAID device; a random access memory (RAM); a read-only memory (ROM); and all forms of flash memory storage devices.</p>
<p id="p-0041" num="0040">Network <b>14</b> may be connected to one or more secondary networks (e.g., network <b>18</b>), examples of which may include but are not limited to: a local area network; a wide area network; or an intranet, for example.</p>
<p id="p-0042" num="0041">Various data requests (e.g. data request <b>20</b>) may be sent from client applications <b>22</b>, <b>24</b>, <b>26</b>, <b>28</b> to storage system <b>12</b>. Examples of data request <b>20</b> may include but are not limited to data write requests (i.e. a request that content be written to storage system <b>12</b>) and data read requests (i.e. a request that content be read from storage system <b>12</b>).</p>
<p id="p-0043" num="0042">The instruction sets and subroutines of client applications <b>22</b>, <b>24</b>, <b>26</b>, <b>28</b>, which may be stored on storage devices <b>30</b>, <b>32</b>, <b>34</b>, <b>36</b> (respectively) coupled to client electronic devices <b>38</b>, <b>40</b>, <b>42</b>, <b>44</b> (respectively), may be executed by one or more processors (not shown) and one or more memory architectures (not shown) incorporated into client electronic devices <b>38</b>, <b>40</b>, <b>42</b>, <b>44</b> (respectively). Storage devices <b>30</b>, <b>32</b>, <b>34</b>, <b>36</b> may include but are not limited to: hard disk drives; tape drives; optical drives; RAID devices; random access memories (RAM); read-only memories (ROM), and all forms of flash memory storage devices. Examples of client electronic devices <b>38</b>, <b>40</b>, <b>42</b>, <b>44</b> may include, but are not limited to, personal computer <b>38</b>, laptop computer <b>40</b>, personal digital assistant <b>42</b>, notebook computer <b>44</b>, a server (not shown), a data-enabled, cellular telephone (not shown), and a dedicated network device (not shown).</p>
<p id="p-0044" num="0043">Users <b>46</b>, <b>48</b>, <b>50</b>, <b>52</b> may access storage system <b>12</b> directly through network <b>14</b> or through secondary network <b>18</b>. Further, storage system <b>12</b> may be connected to network <b>14</b> through secondary network <b>18</b>, as illustrated with link line <b>54</b>.</p>
<p id="p-0045" num="0044">The various client electronic devices may be directly or indirectly coupled to network <b>14</b> (or network <b>18</b>). For example, personal computer <b>38</b> is shown directly coupled to network <b>14</b> via a hardwired network connection. Further, notebook computer <b>44</b> is shown directly coupled to network <b>18</b> via a hardwired network connection. Laptop computer <b>40</b> is shown wirelessly coupled to network <b>14</b> via wireless communication channel <b>56</b> established between laptop computer <b>40</b> and wireless access point (i.e., WAP) <b>58</b>, which is shown directly coupled to network <b>14</b>. WAP <b>58</b> may be, for example, an IEEE 802.11a, 802.11b, 802.11g, 802.11n, Wi-Fi, and/or Bluetooth device that is capable of establishing wireless communication channel <b>56</b> between laptop computer <b>40</b> and WAP <b>58</b>. Personal digital assistant <b>42</b> is shown wirelessly coupled to network <b>14</b> via wireless communication channel <b>60</b> established between personal digital assistant <b>42</b> and cellular network/bridge <b>62</b>, which is shown directly coupled to network <b>14</b>.</p>
<p id="p-0046" num="0045">As is known in the art, all of the IEEE 802.11x specifications may use Ethernet protocol and carrier sense multiple access with collision avoidance (i.e., CSMA/CA) for path sharing. The various 802.11x specifications may use phase-shift keying (i.e., PSK) modulation or complementary code keying (i.e., CCK) modulation, for example. As is known in the art, Bluetooth is a telecommunications industry specification that allows e.g., mobile phones, computers, and personal digital assistants to be interconnected using a short-range wireless connection.</p>
<p id="p-0047" num="0046">Client electronic devices <b>38</b>, <b>40</b>, <b>42</b>, <b>44</b> may each execute an operating system, examples of which may include but are not limited to Microsoft Windows&#x2122;, Microsoft Windows CE&#x2122;, Redhat Linux&#x2122;, or a custom operating system.</p>
<p id="h-0008" num="0000">The Data Caching Process:</p>
<p id="p-0048" num="0047">For the following discussion, client application <b>22</b> is going to be described for illustrative purposes. However, this is not intended to be a limitation of this disclosure, as other client applications (e.g., client applications <b>24</b>, <b>26</b>, <b>28</b>) may be equally utilized.</p>
<p id="p-0049" num="0048">For illustrative purposes, storage system <b>12</b> will be described as being a network-based storage system that includes a plurality of electro-mechanical backend storage devices. However, this is for illustrative purposes only and is not intended to be a limitation of this disclosure, as other configurations are possible and are considered to be within the scope of this disclosure. For example and as discussed above, storage system <b>12</b> may be a personal computer that includes a single electro-mechanical storage device.</p>
<p id="p-0050" num="0049">Referring also to <figref idref="DRAWINGS">FIG. 2</figref>, storage system <b>12</b> may include a server computer/controller (e.g. server computer/controller <b>100</b>), and a plurality of storage targets T (e.g. storage targets <b>102</b>, <b>104</b>, <b>106</b>, <b>108</b>). Storage targets <b>102</b>, <b>104</b>, <b>106</b>, <b>108</b> may be configured to provide various levels of performance and/or high availability. For example, one or more of storage targets <b>102</b>, <b>104</b>, <b>106</b>, <b>108</b> may be configured as a RAID 0 array, in which data is striped across storage targets. By striping data across a plurality of storage targets, improved performance may be realized. However, RAID 0 arrays do not provide a level of high availability. Accordingly, one or more of storage targets <b>102</b>, <b>104</b>, <b>106</b>, <b>108</b> may be configured as a RAID 1 array, in which data is mirrored between storage targets. By minoring data between storage targets, a level of high availability is achieved as multiple copies of the data are stored within storage system <b>12</b>.</p>
<p id="p-0051" num="0050">While storage targets <b>102</b>, <b>104</b>, <b>106</b>, <b>108</b> are discussed above as being configured in a RAID 0 or RAID 1 array, this is for illustrative purposes only and is not intended to be a limitation of this disclosure, as other configurations are possible. For example, storage targets <b>102</b>, <b>104</b>, <b>106</b>, <b>108</b> may be configured as a RAID 3, RAID 4, RAID 5 or RAID 6 array.</p>
<p id="p-0052" num="0051">While in this particular example, storage system <b>12</b> is shown to include four storage targets (e.g. storage targets <b>102</b>, <b>104</b>, <b>106</b>, <b>108</b>), this is for illustrative purposes only and is not intended to be a limitation of this disclosure. Specifically, the actual number of storage targets may be increased or decreased depending upon e.g. the level of redundancy/performance/capacity required.</p>
<p id="p-0053" num="0052">Storage system <b>12</b> may also include one or more coded targets <b>110</b>. As is known in the art, a coded target may be used to store coded data that may allow for the regeneration of data lost/corrupted on one or more of storage targets <b>102</b>, <b>104</b>, <b>106</b>, <b>108</b>. An example of such a coded target may include but is not limited to a hard disk drive that is used to store parity data within a RAID array.</p>
<p id="p-0054" num="0053">While in this particular example, storage system <b>12</b> is shown to include one coded target (e.g., coded target <b>110</b>), this is for illustrative purposes only and is not intended to be a limitation of this disclosure. Specifically, the actual number of coded targets may be increased or decreased depending upon e.g. the level of redundancy/performance/capacity required.</p>
<p id="p-0055" num="0054">Examples of storage targets <b>102</b>, <b>104</b>, <b>106</b>, <b>108</b> and coded target <b>110</b> may include one or more electro-mechanical hard disk drives, wherein a combination of storage targets <b>102</b>, <b>104</b>, <b>106</b>, <b>108</b> and coded target <b>110</b> may form non-volatile, electro-mechanical memory system <b>112</b>.</p>
<p id="p-0056" num="0055">The manner in which storage system <b>12</b> is implemented may vary depending upon e.g. the level of redundancy/performance/capacity required. For example, storage system <b>12</b> may be a RAID device in which server computer/controller <b>100</b> is a RAID controller card and storage targets <b>102</b>, <b>104</b>, <b>106</b>, <b>108</b> and/or coded target <b>110</b> are individual &#x201c;hot-swappable&#x201d; hard disk drives. An example of such a RAID device may include but is not limited to an NAS device. Alternatively, storage system <b>12</b> may be configured as a SAN, in which server computer/controller <b>100</b> may be e.g., a server computer and each of storage targets <b>102</b>, <b>104</b>, <b>106</b>, <b>108</b> and/or coded target <b>110</b> may be a RAID device and/or computer-based hard disk drive. Further still, one or more of storage targets <b>102</b>, <b>104</b>, <b>106</b>, <b>108</b> and/or coded target <b>110</b> may be a SAN.</p>
<p id="p-0057" num="0056">In the event that storage system <b>12</b> is configured as a SAN, the various components of storage system <b>12</b> (e.g. server computer/controller <b>100</b>, storage targets <b>102</b>, <b>104</b>, <b>106</b>, <b>108</b>, and coded target <b>110</b>) may be coupled using network infrastructure <b>114</b>, examples of which may include but are not limited to an Ethernet (e.g., Layer 2 or Layer 3) network, a fiber channel network, an InfiniBand network, or any other circuit switched/packet switched network.</p>
<p id="p-0058" num="0057">Storage system <b>12</b> may execute all or a portion of data caching process <b>10</b>. The instruction sets and subroutines of data caching process <b>10</b>, which may be stored on a storage device (e.g., storage device <b>16</b>) coupled to server computer/controller <b>100</b>, may be executed by one or more processors (not shown) and one or more memory architectures (not shown) included within server computer/controller <b>100</b>. Storage device <b>16</b> may include but is not limited to: a hard disk drive; a tape drive; an optical drive; a RAID device; a random access memory (RAM); a read-only memory (ROM); and all forms of flash memory storage devices.</p>
<p id="p-0059" num="0058">As discussed above, various data requests (e.g. data request <b>20</b>) may be generated. For example, these data requests may be sent from client applications <b>22</b>, <b>24</b>, <b>26</b>, <b>28</b> to storage system <b>12</b>. Additionally/alternatively and when server computer/controller <b>100</b> is configured as an application server, these data requests may be internally generated within server computer/controller <b>100</b>. Examples of data request <b>20</b> may include but are not limited to data write request <b>116</b> (i.e. a request that content <b>118</b> be written to storage system <b>12</b>) and data read request <b>120</b> (i.e. a request that content <b>118</b> be read from storage system <b>12</b>).</p>
<p id="p-0060" num="0059">Server computer/controller <b>100</b> may include input-output logic <b>122</b> (e.g., a network interface card or a Host Bus Adaptor (HBA)), processing logic <b>124</b>, and first cache system <b>126</b>. Examples of first cache system <b>126</b> may include but are not limited to a volatile, solid-state, cache memory system (e.g., a dynamic RAM cache memory system) and/or a non-volatile, solid-state, cache memory system (e.g., a flash-based, cache memory system).</p>
<p id="p-0061" num="0060">During operation of server computer/controller <b>100</b>, content <b>118</b> to be written to storage system <b>12</b> may be received by input-output logic <b>122</b> (e.g. from network <b>14</b> and/or network <b>18</b>) and processed by processing logic <b>124</b>. Additionally/alternatively and when server computer/controller <b>100</b> is configured as an application server, content <b>118</b> to be written to storage system <b>12</b> may be internally generated by server computer/controller <b>100</b>. As will be discussed below in greater detail, processing logic <b>124</b> may initially store content <b>118</b> within first cache system <b>126</b>.</p>
<p id="p-0062" num="0061">Depending on the manner in which first cache system <b>126</b> is configured, processing logic <b>124</b> may immediately write content <b>118</b> to second cache system <b>128</b>/non-volatile, electro-mechanical memory system <b>112</b> (if first cache system <b>126</b> is configured as a write-through cache) or may subsequently write content <b>118</b> to second cache system <b>128</b>/non-volatile, electro-mechanical memory system <b>112</b> (if first cache system <b>126</b> is configured as a write-back cache). Additionally and in certain configurations, processing logic <b>124</b> may calculate and store coded data on coded target <b>110</b> (included within non-volatile, electromechanical memory system <b>112</b>) that may allow for the regeneration of data lost/corrupted on one or more of storage targets <b>102</b>, <b>104</b>, <b>106</b>, <b>108</b>. For example, if processing logic <b>124</b> was included within a RAID controller card or a NAS/SAN controller, processing logic <b>124</b> may calculate and store coded data on coded target <b>110</b>. However, if processing logic <b>124</b> was included within e.g., an applications server, data array <b>130</b> may calculate and store coded data on coded target <b>110</b>.</p>
<p id="p-0063" num="0062">Examples of second cache system <b>128</b> may include but are not limited to a volatile, solid-state, cache memory system (e.g., a dynamic RAM cache memory system) and/or a non-volatile, solid-state, cache memory system (e.g., a flash-based, cache memory system).</p>
<p id="p-0064" num="0063">The combination of second cache system <b>128</b> and non-volatile, electromechanical memory system <b>112</b> may form data array <b>130</b>, wherein first cache system <b>126</b> may be sized so that the number of times that data array <b>130</b> is accessed may be reduced. Accordingly, by sizing first cache system <b>126</b> so that first cache system <b>126</b> retains a quantity of data sufficient to satisfy a significant quantity of data requests (e.g., data request <b>20</b>), the overall performance of storage system <b>12</b> may be enhanced. As will be described below in greater detail, first cache system <b>126</b> may be a content-aware cache system.</p>
<p id="p-0065" num="0064">Further, second cache system <b>128</b> within data array <b>130</b> may be sized so that the number of times that non-volatile, electromechanical memory system <b>112</b> is accessed may be reduced. Accordingly, by sizing second cache system <b>128</b> so that second cache system <b>128</b> retains a quantity of data sufficient to satisfy a significant quantity of data requests (e.g., data request <b>20</b>), the overall performance of storage system <b>12</b> may be enhanced. As will be described below in greater detail, second cache system <b>128</b> may be a content-aware cache system.</p>
<p id="p-0066" num="0065">As discussed above, the instruction sets and subroutines of data caching process <b>10</b>, which may be stored on storage device <b>16</b> included within storage system <b>12</b>, may be executed by one or more processors (not shown) and one or more memory architectures (not shown) included within storage system <b>12</b>. Accordingly, in addition to being executed on server computer/controller <b>100</b>, some or all of the instruction sets and subroutines of data caching process <b>10</b> may be executed by one or more processors (not shown) and one or more memory architectures (not shown) included within data array <b>130</b>.</p>
<p id="p-0067" num="0066">Referring also to <figref idref="DRAWINGS">FIGS. 3-4</figref>, data request <b>20</b> (e.g. data read request <b>116</b> and/or data write request <b>120</b>) may be processed by server computer/controller <b>100</b> to extract pertinent information concerning these data requests.</p>
<p id="p-0068" num="0067">When data request <b>20</b> is a data write request (e.g., write request <b>116</b>), write request <b>116</b> may include content <b>118</b> to be written to data array <b>130</b>. Additionally, write request <b>116</b> may include a storage address <b>200</b> that defines the intended storage location within storage array <b>130</b> at which content <b>118</b> is to be stored. For example, storage address <b>200</b> may define a particular logical unit within data array <b>130</b> (e.g., a LUN or Logical Unit Number) and a particular storage address within that specific logical unit (e.g., an LBA or Logical Block Address) for storing content <b>118</b>.</p>
<p id="p-0069" num="0068">Concerning read request <b>120</b>, these requests do not include any content to be written to data array <b>130</b>, as these are read requests and concern content to be read from data array <b>130</b>. Read request <b>120</b> may include a storage address <b>202</b> that defines the storage location within storage array <b>130</b> from which content is to be retrieved. For example, storage address <b>202</b> may define a particular logical unit within data array <b>130</b> (e.g., a LUN or Logical Unit Number) and a particular storage address within that specific logical unit (e.g., an LBA or Logical Block Address) for retrieving the content sought from data array <b>130</b>.</p>
<p id="p-0070" num="0069">As will be discussed below in greater detail and referring also to <figref idref="DRAWINGS">FIG. 5</figref>, data caching process <b>10</b> may maintain content directory <b>250</b>, which may be used to locate various pieces of content within first cache system <b>126</b>. In one particular embodiment of content directory <b>250</b>, content directory <b>250</b> may include plurality of entries <b>252</b>, wherein each of these entries may identify: data array storage address <b>200</b>/<b>202</b> (e.g. a logical storage unit and a storage address at which a specific piece of previously-written content is located within storage array <b>130</b>); first cache address <b>254</b> (e.g., the location within first cache system <b>126</b> at which the specific piece of previously-written content is also located), and content identifier <b>256</b> for the specific piece of previously-written content. Accordingly, content directory <b>250</b> may identify the location of specific pieces of content included within first cache system <b>126</b> and their corresponding pieces of data within data array <b>130</b>, as well as a content identifier that uniquely identifies the specific piece of content.</p>
<p id="p-0071" num="0070">Content identifier <b>256</b> may be used in a content-aware caching system and may, specifically, be a mathematical representation of the specific piece of previously-written content that may allow e.g. server computer/controller <b>100</b> to quickly determine whether two pieces of previously-written content are identical, as identical pieces of content would have identical content identifiers. In one particular embodiment, content identifier <b>256</b> may be a hash function (e.g., a cryptographic hash) of the previously-written content. Accordingly, through the use of a content-aware caching system, duplicate data entries within first cache system <b>126</b> and/or second cache system <b>128</b> may be quickly identified, avoided, and/or eliminated.</p>
<p id="p-0072" num="0071">As is known in the art, a hash function is an algorithm/subroutine that maps large data sets to smaller data sets. The values returned by a hash function are typically called hash values, hash codes, hash sums, checksums or simply hashes. Hash functions are mostly used to accelerate table lookup or data comparison tasks such as e.g., finding items in a database and detecting duplicated or similar records in a large file.</p>
<p id="h-0009" num="0000">General Read Request Processing:</p>
<p id="p-0073" num="0072">During operation of server computer/controller <b>100</b>, data caching process <b>10</b> may receive read request <b>120</b> on first cache system <b>126</b>, wherein read request <b>120</b> identifies previously-written content (as defined by storage address <b>202</b>) included within data array <b>130</b>.</p>
<p id="p-0074" num="0073">For example, assume that user <b>46</b> is using client application <b>22</b> to access data (i.e. content <b>132</b>) that is currently being stored on data array <b>130</b>. Accordingly, client application <b>22</b> may generate read request <b>120</b> which, as discussed above, may define a particular logical unit within data array <b>130</b> (e.g., a LUN or Logical Unit Number) and a particular storage address within that specific logical unit (e.g., an LBA or Logical Block Address) for retrieving content <b>132</b> sought from data array <b>130</b> by client application <b>22</b>.</p>
<p id="p-0075" num="0074">Assume that read request <b>120</b> defines LUN0/LBA5 as the location of content <b>132</b> within data array <b>130</b>. Upon receiving read request <b>120</b>, data caching process <b>10</b> may compare the location of content <b>132</b> within data array <b>130</b> (namely LUN0/LBA5) with each of the plurality of entries <b>252</b> defined within content directory <b>250</b> to determine if a copy of content <b>132</b> is locally available (i.e., cached) within first cache system <b>126</b>. If LUN0/LBA5 was defined within content directory <b>250</b> (meaning that a local cached copy of content <b>132</b> is present/available within first cache system <b>126</b>), that particular entry would also define a corresponding first cache address (e.g. first cache address <b>254</b>) within first cache system <b>126</b> at which content <b>132</b> would be locally-available and retrievable from the first cache system <b>126</b>. Conversely, in the event that LUN0/LBA5 is not defined within content directory <b>250</b> (meaning that a local cached copy of content <b>132</b> is not present/available within first cache system <b>126</b>), data caching process <b>10</b> may need to obtain content <b>132</b> identified in read request <b>120</b> from data array <b>130</b>.</p>
<p id="p-0076" num="0075">In this particular example, since LUN0/LBA5 is not defined within content directory <b>250</b>, a local cached copy of content <b>132</b> is not present/available within first cache system <b>126</b> and data caching process <b>10</b> will be need to obtain content <b>132</b> from data array <b>130</b>.</p>
<p id="p-0077" num="0076">Once content <b>132</b> is obtained by data caching process <b>10</b> from data array <b>130</b>, data caching process <b>10</b> may store content <b>132</b> within first cache system <b>126</b> and may provide content <b>132</b> to client application <b>22</b>, thus satisfying read request <b>120</b>. Additionally, content directory <b>250</b> may be amended by data caching process <b>10</b> to include an entry (e.g., entry <b>258</b>) that defines the data array storage address <b>200</b>/<b>202</b> (e.g. LUN0/LBA5); first cache address <b>254</b> (e.g., 111110), and content identifier <b>256</b> (e.g., ablccba) for content <b>132</b>.</p>
<p id="p-0078" num="0077">As discussed above, data array <b>130</b> may include second cache system <b>128</b>. Accordingly, data caching process <b>10</b> may execute the above-described functionality with respect to second cache system <b>128</b>.</p>
<p id="h-0010" num="0000">General Write Request Processing:</p>
<p id="p-0079" num="0078">During operation of server computer/controller <b>100</b>, data caching process <b>10</b> may receive write request <b>116</b> on first cache system <b>126</b>, wherein write request <b>116</b> identifies new content (e.g., content <b>118</b>) to be written to data array <b>130</b>.</p>
<p id="p-0080" num="0079">For example, assume that user <b>46</b> is using client application <b>22</b> to create content (i.e. content <b>118</b>) that is to be stored on data array <b>130</b>. Accordingly, client application <b>22</b> may generate write request <b>116</b> which, as discussed above, may define a particular logical unit within data array <b>130</b> (e.g., a LUN or Logical Unit Number) and a particular storage address within that specific logical unit (e.g., an LBA or Logical Block Address) for storing content <b>118</b> within data array <b>130</b>.</p>
<p id="p-0081" num="0080">As discussed above and depending on the manner in which first cache system <b>126</b> is configured, data caching process <b>10</b> may immediately write content <b>118</b> to data array <b>130</b> (if first cache system <b>126</b> is configured as a write-through cache) or may subsequently write content <b>118</b> to data array <b>130</b> (if first cache system <b>126</b> is configured as a write-back cache).</p>
<p id="p-0082" num="0081">Assuming that first cache system <b>126</b> in this example is configured as a write-through cache, data caching process <b>10</b> may immediately write content <b>118</b> to LUN0/LBA0 within data array <b>130</b> (as defined within write request <b>116</b>). Additionally, data caching process <b>10</b> may locally-store content <b>118</b> within first cache system <b>126</b> and may amend content directory <b>250</b> to include an entry (e.g., entry <b>260</b>) that defines the data array storage address <b>200</b>/<b>202</b> (e.g. LUN0/LBA0); first cache address <b>254</b> (e.g., 001011), and content identifier <b>256</b> (e.g., acdfcla) for content <b>118</b>.</p>
<p id="p-0083" num="0082">As discussed above, data array <b>130</b> may include second cache system <b>128</b>. Accordingly, data caching process <b>10</b> may execute the above described functionality with respect to second cache system <b>128</b>.</p>
<p id="h-0011" num="0000">Content Aware Caching</p>
<p id="p-0084" num="0083">As discussed above, content directory <b>250</b> may include a content identifier <b>256</b> that may be used in a content-aware caching system. A typical example of content identifier <b>256</b> may include but is not limited to a hash function of the content that content identifier <b>256</b> is associated with. Accordingly, through the use of content identifier <b>256</b> within a content-aware caching system, duplicate data entries within first cache system <b>126</b> and/or second cache system <b>128</b> may be quickly identified, avoided, and/or eliminated.</p>
<p id="p-0085" num="0084">For example, upon receiving write request <b>116</b> and content <b>118</b>, data caching process <b>10</b> may generate content identifier <b>256</b> for content <b>118</b>. As discussed above, content identifier <b>256</b> generated for the content (i.e., content <b>118</b>) identified within write request <b>116</b> may be a hash function (e.g., a cryptographic hash) of content <b>118</b>.</p>
<p id="p-0086" num="0085">Assume for illustrative purposes that write request <b>116</b> includes storage address <b>200</b> that defines the intended storage location for content <b>118</b> as LUN0/LBA0. Accordingly, upon receiving write request <b>116</b>, data caching process <b>10</b> may generate content identifier <b>256</b> for content <b>118</b>. Assume for illustrative purposes that data caching process <b>10</b> generates a hash of content <b>118</b>, resulting in the generation of content identifier <b>256</b> (namely hash value acdfcla).</p>
<p id="p-0087" num="0086">This newly-generated content identifier <b>256</b> (i.e. acdfcla) associated with content <b>118</b> may be compared to each of the other content identifiers (namely abalaby, alazchb, abalabz, alazcha) included within content directory <b>250</b> for first cache system <b>126</b> to determine if the newly-generated content identifier <b>256</b> (i.e. acdfcla) matches any of the other content identifiers (namely abalaby, alazchb, abalabz, alazcha) included within content directory <b>250</b>.</p>
<p id="p-0088" num="0087">As discussed above, each entry of the plurality of entries <b>252</b> included within content directory <b>250</b> is associated with a unique piece of content included within (in this example) first cache system <b>126</b>. Accordingly, each unique content identifier included within content directory <b>250</b> may be associated with a unique piece of content written to (in this example) first cache system <b>126</b>.</p>
<p id="p-0089" num="0088">If, when performing this comparison, data caching process <b>10</b> does not identify a content identifier (i.e., abalaby, alazchb, abalabz, alazcha) within content directory <b>250</b> that matches the above-described, newly-generated content identifier (i.e. acdfcla), data caching process <b>10</b> may write content <b>118</b> to (in this example) first cache system <b>126</b> and may provide a copy of content <b>118</b> to data array <b>130</b> for storage within data array <b>130</b>. Additionally, data caching process <b>10</b> may modify content directory <b>250</b> to include a new entry (i.e., entry <b>260</b>) that defines the newly-generated content identifier (i.e. acdfcla), the location of content <b>118</b> within (in this example) first cache system <b>126</b> (i.e., 001011), and the location of content <b>118</b> within data array <b>130</b> (i.e., LUN0/LBA0).</p>
<p id="p-0090" num="0089">If, when performing this comparison, data caching process <b>10</b> identified a content identifier within content directory <b>250</b> that matched the above-described, newly-generated content identifier (i.e. acdfcla), data caching process <b>10</b> would perform differently.</p>
<p id="p-0091" num="0090">To illustrate how data caching process <b>10</b> would react if it found a matching content identifier, further assume for illustrative purposes that a second write request (i.e., write request <b>116</b>&#x2032;) includes storage address <b>200</b>&#x2032; that defines the intended storage location for content <b>118</b>&#x2032; as LUN0/LBA2. Accordingly, upon receiving write request <b>116</b>&#x2032;, data caching process <b>10</b> may generate content identifier <b>256</b> for content <b>118</b>&#x2032;. Assume for illustrative purposes that data caching process <b>10</b> generates a hash of content <b>118</b>&#x2032;, resulting in the generation of content identifier <b>256</b> (namely hash value alazcha).</p>
<p id="p-0092" num="0091">This newly-generated content identifier <b>256</b> (i.e. alazcha) associated with content <b>118</b>&#x2032; may be compared to each of the other content identifiers (namely abalaby, alazchb, abalabz, alazcha) included within content directory <b>250</b> for (in this example) first cache system <b>126</b> to determine if the newly-generated content identifier <b>256</b> (i.e. alazcha) matches any of the other content identifiers (namely abalaby, alazchb, abalabz, alazcha) included within content directory <b>250</b>.</p>
<p id="p-0093" num="0092">If, when performing this comparison, data caching process <b>10</b> does identify a content identifier (namely alazcha) within content directory <b>250</b> that matches the above-described, newly-generated content identifier (i.e. alazcha), data caching process <b>10</b> may perform a couple of functions.</p>
<p id="p-0094" num="0093">For example, data caching process <b>10</b> may modify the entry (i.e., entry <b>262</b>) within content directory <b>250</b> that is associated with the matching content identifier (i.e., alazcha) to include storage address <b>200</b>&#x2032; that defines the intended storage location for content <b>118</b>&#x2032; (i.e., LUN0/LBA2 within data array <b>130</b>), thus generating modified entry <b>262</b>&#x2032;. Accordingly, modified entry <b>262</b>&#x2032; identifies that the pieces of content that are currently stored at LUN4/LBA7 and LUN0/LBA2 within data array <b>130</b> are identical. Accordingly, a single piece of cached content (located at first cache address 010111 within, in this example, first cache system <b>126</b>) may be used as a local cached copy for both pieces of content stored on data array <b>130</b>.</p>
<p id="p-0095" num="0094">While the system is described above as modifying entry <b>262</b> by adding a second LUN/LBA designation to generate modified entry <b>262</b>&#x2032;, this is for illustrative purposes only and is not intended to be a limitation of this disclosure, as other configurations are possible. For example, sub-tables/sub-entries may be utilized to show the manner in which multiple LUNs/LBAs are mapped to a single piece of content within, for example, first cache system <b>126</b>.</p>
<p id="p-0096" num="0095">As discussed above, data array <b>130</b> may include second cache system <b>128</b>. Accordingly, data caching process <b>10</b> may execute the above-described content aware functionality with respect to second cache system <b>128</b>.</p>
<p id="h-0012" num="0000">Cache Recovery</p>
<p id="p-0097" num="0096">Typically, content directory <b>250</b> is stored within some form of volatile memory system (e.g., volatile memory system <b>150</b>) within e.g. server computer/controller <b>100</b>. Since content directory <b>250</b> is essentially a &#x201c;roadmap&#x201d; to the content stored within e.g. first cache system <b>126</b>, in the event that e.g. server computer/controller <b>100</b> crashes or restarts, access to the content stored within e.g. first cache system <b>126</b> will be eliminated, as the location of the individual pieces of content within e.g. first cache system <b>126</b> may be unknown.</p>
<p id="p-0098" num="0097">Accordingly and referring also to <figref idref="DRAWINGS">FIG. 6</figref>, data caching process <b>10</b> may be configured to copy <b>300</b> content directory <b>250</b> associated with first cache system <b>126</b> from volatile memory system <b>150</b> (e.g., a random access memory system) to non-volatile memory system <b>152</b> (e.g., a flash-based memory system) included within server computer/controller <b>100</b>, thus establishing a backup copy of content directory <b>250</b>.</p>
<p id="p-0099" num="0098">As discussed above, data caching process <b>10</b> may receive <b>302</b>, on e.g., cache system <b>126</b>, a plurality of data requests (e.g., data write request <b>116</b> and data read request <b>120</b>) that concern a plurality of data actions (e.g., read actions and write actions) to be taken on an electro-mechanical storage device associated with first cache system <b>126</b>. For example and as discussed above, data caching process <b>10</b> may receive <b>302</b> read and write requests concerning data to be read from and written to the electro-mechanical storage devices include within data array <b>130</b>.</p>
<p id="p-0100" num="0099">During normal operation, data caching process <b>10</b> may update <b>304</b> content directory <b>250</b> residing on volatile memory system <b>150</b> based, at least in part, upon the plurality of data requests received <b>302</b> by data caching process <b>10</b>. For example, in the event that the data request received <b>302</b> is a data read request, data caching process <b>10</b> may update <b>304</b> content directory <b>250</b> to reflect any changes made to the content of e.g. first cache system <b>126</b> (as discussed above). Additionally, in the event that the data request received <b>302</b> is a data write request, data caching process <b>10</b> may update <b>304</b> content directory <b>250</b> to reflect the newly-added content within e.g. first cache system <b>126</b> (as discussed above). Additionally and for this example, assume that first cache system <b>126</b> is a write-through cache and, therefore, data caching process <b>10</b> immediately writes any content associated with data write requests to data array <b>130</b>.</p>
<p id="p-0101" num="0100">Further, data caching process <b>10</b> may store <b>306</b> a copy of the plurality of data requests received <b>302</b> in a tracking queue (e.g., tracking queue <b>154</b>) included within data array <b>130</b>. For example, as each data request is processed by data caching process <b>10</b>, data caching process <b>10</b> may store <b>306</b> the received data request within tracking queue <b>154</b>, which may be included within one or more of the electro-mechanical storage devices included within data array <b>130</b>.</p>
<p id="p-0102" num="0101">Data caching process <b>10</b> may continue to store <b>306</b> the data requests received <b>302</b> until e.g., tracking queue <b>154</b> is full (e.g., 1,000 data requests) or until a defined period of time has passed (e.g., 1,000 milliseconds). Once one of these events occurs, data caching process <b>10</b> may copy <b>300</b> this newly-updated content directory <b>250</b> associated with cache system <b>126</b> from volatile memory system <b>150</b> to non-volatile memory system <b>150</b>, thus generating an updated backup copy of content directory <b>250</b>. Further, data caching process <b>10</b> may flush <b>308</b> tracking queue <b>154</b>, thus restarting the tracking process. Accordingly, data caching process <b>10</b> may be configured so that tracking queue <b>154</b> only contains a copy of all of the data requests received <b>302</b> by data caching process <b>10</b> after the latest version of content directory <b>250</b> was copied <b>300</b> from volatile memory system <b>150</b> to non-volatile memory system <b>152</b>.</p>
<p id="p-0103" num="0102">Assume for illustrative purposes that server computer/controller <b>100</b> crashes (e.g. due to a power failure event or a software event) and is subsequently restarted. Further, assume for this example that first cache system <b>126</b> utilizes a flash-based (i.e. non-volatile) memory system. Accordingly, while the actual content included within first cache system <b>126</b> would survive the above-described crash, content directory <b>250</b> (which is stored in volatile memory <b>150</b>) would not, resulting in a cold cache, as all of the content within first cache system <b>126</b> would be non-accessible.</p>
<p id="p-0104" num="0103">Upon detecting the restart of server computer/controller <b>100</b>, data caching process <b>10</b> may update <b>310</b> the previously-copied version of content directory <b>250</b> (which was copied <b>300</b> by data caching process <b>10</b> from volatile memory <b>150</b> to non-volatile memory <b>152</b>) based, at least in part, upon the plurality of data requests stored within tracking queue <b>154</b> on data array <b>130</b>.</p>
<p id="p-0105" num="0104">As discussed above, tracking queue <b>154</b> contains a copy of each data request received <b>302</b> by data caching process <b>10</b> after the latest copy of content directory <b>250</b> was copied <b>300</b> from volatile memory system <b>150</b> to non-volatile memory system <b>152</b>. Accordingly, once server computer/controller <b>100</b> is restarted, data caching process <b>10</b> may sequentially process each data request included within tracking queue <b>154</b> so that the previously-copied version of content directory <b>250</b> may be made current.</p>
<p id="p-0106" num="0105">Unfortunately, simply because a data request is included within tracking queue <b>154</b> does not guarantee that the data request was correctly/completely processed. Accordingly and for example, a write request may have been written to tracking queue <b>154</b> and, unfortunately, server computer/controller <b>100</b> may have crashed prior to first cache system <b>126</b> being updated with the content associated with that data request. Accordingly, if the data requests within tracking queue <b>154</b> are to be processed to modify content directory <b>250</b>, data caching process <b>10</b> may need to verify that e.g., a piece of content written to data array <b>130</b> was indeed written to first cache system <b>126</b> prior to updating content directory <b>250</b> to indicate that it had been. This may be accomplished by comparing the actual piece of content stored within the data array <b>130</b> to the related piece of content stored within e.g., first cache system <b>126</b>.</p>
<p id="p-0107" num="0106">Alternatively, data caching process <b>10</b> may simply modify content directory <b>250</b> stored within non-volatile memory <b>152</b> to invalidate the entries within content directory <b>250</b> that are related to data requests included within tracking queue <b>154</b> (thus assuming that all data requests stored within tracking queue <b>154</b> were corrupt and not correctly processed).</p>
<p id="p-0108" num="0107">Once current, data caching process <b>10</b> may copy the now-updated version of content directory <b>250</b> from non-volatile memory <b>152</b> to volatile memory <b>150</b>, thus enabling use by data caching process <b>10</b>. Additionally, in the event that any of the data requests processed by data caching process <b>10</b> were data write requests, data caching process <b>10</b> may copy the associated content (which was previously stored on data array <b>130</b> due to first cache system <b>126</b> being a write-through cache) from data array <b>130</b> to first cache system <b>126</b>.</p>
<p id="p-0109" num="0108">Referring to <figref idref="DRAWINGS">FIG. 6A</figref>, there is shown a more detailed flowchart of data caching process <b>10</b> that differentiates between processes executed on server computer/controller <b>100</b> versus those executed on data array <b>130</b>.</p>
<p id="p-0110" num="0109">Specifically, data caching process <b>10</b> may update <b>320</b> content directory <b>250</b>, which is stored within volatile memory <b>150</b> on server computer/controller <b>100</b>. Further, data caching process <b>10</b> may store <b>322</b> data requests within tracking queue <b>154</b> (as described above) located within data array <b>130</b>. As discussed above, these processes may continue until e.g., tracking queue <b>154</b> fills up (e.g., 1,000 data requests) or until a defined period of time has passed (e.g., 1,000 milliseconds). At this point, data caching process <b>10</b> may notify <b>324</b> data array <b>130</b> (via notice <b>326</b>) that data caching process <b>10</b> is about to copy content directory <b>250</b> from volatile memory <b>150</b> to non-volatile memory <b>152</b>. Upon data array <b>130</b> receiving <b>328</b> notice <b>326</b> from server computer/controller <b>100</b>, data caching process <b>10</b> may mark <b>330</b> tracking queue <b>154</b> to indicates the latest data request processed by data caching process <b>10</b>. Upon data array <b>130</b> completing this marking process, data caching process <b>10</b> may notify <b>332</b> server computer/controller <b>100</b> (via notice <b>334</b>) that the marking process has been completed. Upon server computer/controller <b>100</b> receiving <b>336</b> notice <b>334</b>, data caching process <b>10</b> may copy <b>338</b> content directory <b>250</b> from volatile memory <b>150</b> to non-volatile memory <b>152</b>. Upon completing this copying process, data caching process <b>10</b> may notify <b>340</b> data array <b>130</b> (via notice <b>342</b>) that the copying process is complete. Upon data array <b>130</b> receiving notice <b>342</b>, data array <b>130</b> may delete the content of tracking queue <b>154</b> prior to the above-described mark.</p>
<p id="p-0111" num="0110">As discussed above, data array <b>130</b> may include second cache system <b>128</b>. Accordingly, data caching process <b>10</b> may execute the above-described cache recovery functionality with respect to second cache system <b>128</b>.</p>
<p id="h-0013" num="0000">Cache Content Sharing</p>
<p id="p-0112" num="0111">Referring also to <figref idref="DRAWINGS">FIGS. 7-8</figref>, assume for illustrative purposes that server computer/controller <b>100</b> is configured to execute several virtual machines (e.g., virtual machines <b>350</b>, <b>352</b>, <b>354</b>). As is known in the art, a virtual machine is a software implementation of a physical machine (e.g., a computer) that executes programs like a physical machine. Assume also for this implementation that first cache system <b>126</b> is configured as a communal cache system that includes several assigned cache portions (e.g., first assigned cache portion <b>356</b>, second assigned cache portion <b>358</b>, third assigned cache portion <b>360</b>), and an initial public cache portion <b>362</b>. Each of the assigned cache portions may be assigned to and only accessible by a specific virtual machine. Accordingly, first assigned cache portion <b>356</b> may be assigned to first virtual machine <b>350</b>; second assigned cache portion <b>358</b> may be assigned to second virtual machine <b>352</b>; and third assigned cache portion <b>360</b> may be assigned to third virtual machine <b>354</b>. Further, initial public cache portion <b>362</b> may be available for use by all virtual machines (e.g. virtual machines <b>350</b>, <b>352</b>, <b>354</b>). As would be expected, as the number virtual machines increases or decreases, data caching process <b>10</b> may increase or decrease the number of assigned cache portions.</p>
<p id="p-0113" num="0112">Accordingly and during normal operation, data caching process <b>10</b> may define <b>400</b> a first assigned cache portion (e.g., first assigned cache portion <b>356</b>) within the cache system (e.g., first cache system <b>126</b>), wherein first assigned cache portion <b>356</b> is associated with first virtual machine <b>350</b>.</p>
<p id="p-0114" num="0113">Further, assume that data caching process <b>10</b> defines <b>402</b> at least one additional assigned cache portion (e.g., second assigned cache portion <b>358</b> and third assigned cache portion <b>360</b>) within the cache system (e.g., first cache system <b>126</b>), wherein the additional assigned cache portions (e.g., second assigned cache portion <b>358</b> and third assigned cache portion <b>360</b>) are associated with virtual machines <b>352</b>, <b>354</b> (respectively).</p>
<p id="p-0115" num="0114">Assume that during the course of normal operation, data caching process <b>10</b> may write <b>404</b> content received by first virtual machine <b>350</b> to first assigned cache portion <b>356</b>. For example and as discussed above, one or more data requests (e.g., write request <b>116</b> and/or read request <b>120</b>) may be sent from client applications <b>22</b>, <b>24</b>, <b>26</b>, <b>28</b> or may be internally generated within server computer/controller <b>100</b>. Either way, the processing of these data requests may result in data caching process <b>10</b> writing <b>404</b> content (e.g., content <b>118</b>) to first assigned cache portion <b>356</b>. Additionally, data caching process <b>10</b> may maintain <b>406</b> content directory <b>250</b> by modifying the same in response to the processing of the above-described data requests.</p>
<p id="p-0116" num="0115">As discussed above, first cache system <b>126</b> and/or second cache system <b>128</b> may be content aware cache systems. Therefore, content directory <b>250</b> may include content identifier <b>256</b>. As discussed above, content identifier <b>256</b> may be used in a content-aware caching system and may, specifically, be a mathematical representation (e.g., a hash function) of the specific piece of previously-written content that may allow e.g. server computer/controller <b>100</b> to quickly determine whether two pieces of previously-written content are identical, as identical pieces of content would have identical content identifiers. Accordingly, through the use of content-aware caching, duplicate data entries within first cache system <b>126</b> and/or second cache system <b>128</b> may be quickly identified, avoided, and/or eliminated.</p>
<p id="p-0117" num="0116">Accordingly, as data caching process <b>10</b> processes data requests and writes <b>404</b> content received by first virtual machine <b>350</b> to first assigned cache portion <b>356</b>, data caching process <b>10</b> may modify content directory <b>250</b> in the manner described above. Further and assuming that (in this example) first cache system <b>126</b> is a content aware cache system, data caching process <b>10</b> may generate the above-described content identifiers, which may be included within content directory <b>250</b> and may uniquely identify the specific piece of content with which they are associated.</p>
<p id="p-0118" num="0117">After the occurrence of a reclassifying event (as described below), data caching process <b>10</b> may reclassify <b>408</b> first assigned cache portion <b>356</b> as a public cache portion within (in this example) first cache system <b>126</b>. For example, first assigned cache portion <b>356</b> may be added to initial public cache portion <b>362</b>. As discussed above, a public cache portion is associated with and accessible by all of the above-described virtual machines (e.g., virtual machines <b>350</b>, <b>352</b>, <b>354</b>). Accordingly, when data caching process <b>10</b> reclassifies <b>408</b> first assigned cache portion <b>356</b> as a public cache portion (i.e., equivalent to public cache portion <b>262</b>), what was only available to virtual machine <b>350</b> is now also available (in this example) to virtual machines <b>352</b>, <b>354</b>.</p>
<p id="p-0119" num="0118">Concerning the above-referenced reclassifying event, examples of such may include but are not limited to: the expiry of a reclassifying timer; the receipt of a defined quantity of content, and the filling of a tracking queue. For example, data caching process <b>10</b> may be configured to so that every e.g., 1,000 milliseconds, data caching process <b>10</b> reclassifies <b>408</b> first assigned cache portion <b>356</b> as a public cache portion. Alternatively, data caching process <b>10</b> may be configured to so that every time e.g., 1,000 write operations are performed on first assigned cache portion <b>356</b>, data caching process <b>10</b> reclassifies <b>408</b> first assigned cache portion <b>356</b> as a public cache portion. Further, data caching process <b>10</b> may be configured so that write operations are tracked within a tracking queue (e.g., tracking queue <b>154</b>) and every time that this tracking queue is filled, data caching process <b>10</b> reclassifies <b>408</b> first assigned cache portion <b>356</b> as a public cache portion</p>
<p id="p-0120" num="0119">When data caching process <b>10</b> reclassifies <b>408</b> first assigned cache portion <b>356</b> as a public cache portion, data caching process <b>10</b> may share <b>410</b> content directory <b>250</b> (which includes content identifiers <b>256</b>) with the other virtual machines (e.g., virtual machines <b>352</b>, <b>354</b>). Accordingly, the content stored within first assigned cache portion <b>356</b> that was reclassified <b>408</b> as a public cache portion is now available to all virtual machines.</p>
<p id="p-0121" num="0120">Accordingly and for example, when a data read request is received and processed by e.g., virtual machine <b>354</b> that concerns data stored at a specific LUN/LBA within data array <b>130</b>, data caching process <b>10</b> may review the content directory that was shared <b>410</b> by data caching process <b>10</b> and is associated with this newly-public cache portion to see if the appropriate LUN/LBA is defined within the content directory. If so, data caching process <b>10</b> may obtain the appropriate piece of content from the newly-public cache portion reclassified <b>408</b> by data caching process <b>10</b> (as opposed to obtaining the same from data array <b>130</b>).</p>
<p id="p-0122" num="0121">Further, when a data write request that includes data to be written to a specific LUN/LBA within data array <b>130</b> is received and processed by e.g., virtual machine <b>354</b>, data caching process <b>10</b> may generate a content identifier for the data included within the write request. Data caching process <b>10</b> may then compare this newly-created content identifier to the content identifiers included within the content directory that was shared <b>410</b> by data caching process <b>10</b> and is associated with this newly-public cache portion to see if a matching content identifier is defined within the content directory. If so, the data associated with the data write request already exists within the newly-public cache portion. Accordingly, data caching process <b>10</b> may simply modify the related entry within the content directory so that this entry is also related to the specific LUN/LBA defined within the data write request (as opposed to writing a second copy of the content to (in this example) first cache system <b>126</b>.</p>
<p id="p-0123" num="0122">Additionally and after the occurrence of the above-described reclassifying event, data caching process <b>10</b> may define <b>412</b> a new assigned cache portion within (in this example) first cache system <b>126</b>, wherein this new assigned cache portion is associated with first virtual machine <b>350</b>. Specifically, data caching process <b>10</b> may define <b>412</b> this new assigned cache portion to replace first assigned cache portion <b>356</b>, which was reclassified <b>408</b> by data caching process <b>10</b> as a public cache portion. When defining <b>412</b> a new assigned cache portion for virtual machine <b>350</b>, data caching process <b>10</b> may reclassify a portion of public cache portion <b>262</b> to define <b>412</b> the new assigned cache portion for virtual machine <b>350</b>.</p>
<p id="p-0124" num="0123">While the system is described above as being utilized with virtual machines, this is for illustrative purposes only and is not intended to be a limitation disclosure, as other configurations are possible and are considered to be within the scope of this disclosure. For example, the above-described system may be configured to work with physical machines, wherein a plurality of physical machines share a communal cache system. This communal cache system may include a plurality of individual assigned cache portions that are each assigned to individual physical machines, and a public cache portion is shared amongst the physical machines.</p>
<p id="p-0125" num="0124">As discussed above, data array <b>130</b> may include second cache system <b>128</b>. Accordingly, data caching process <b>10</b> may execute the above-described cache content sharing functionality with respect to second cache system <b>128</b>.</p>
<p id="h-0014" num="0000">Cache Warming</p>
<p id="p-0126" num="0125">When a cache portion is transitioned from an assigned cache portion to a public cache portion (i.e., after the occurrence of the above-described reclassifying event), data caching process <b>10</b> may utilize this newly-available information to warm the cache of another virtual machine.</p>
<p id="p-0127" num="0126">Assume for this example that second cache system <b>128</b> (in addition to first cache system <b>126</b>) is a content aware cache system. Accordingly, a content directory may be maintained for second cache system <b>128</b> and a content identifier may be generated for each piece of content included within second cache system <b>128</b>. Further, assume for this example that data array <b>130</b> is segmented based upon virtual machine. For example, storage target <b>102</b> (of data array <b>130</b>) may only be accessible by virtual machine <b>350</b>; storage target <b>104</b> (of data array <b>130</b>) may only be accessible by virtual machine <b>352</b>; and storage target <b>106</b> (of data array <b>130</b>) may only be accessible by virtual machine <b>354</b>. Additionally, second cache system <b>128</b> may also be segmented based upon virtual machine. For example, first cache portion <b>364</b> may be assigned to virtual machine <b>350</b>; second cache portion <b>366</b> may be assigned to virtual machine <b>352</b>; and third cache portion <b>368</b> may be assigned to virtual machine <b>354</b>.</p>
<p id="p-0128" num="0127">Referring also to <figref idref="DRAWINGS">FIG. 9</figref>, assume that the above-described reclassifying event occurs and data caching process <b>10</b> reclassifies <b>450</b> first assigned cache portion <b>356</b> associated with (in this example) first virtual machine <b>350</b> as a public cache portion that is associated with and accessible by all virtual machines (e.g., virtual machines <b>350</b>, <b>352</b>, <b>354</b>). As discussed above, this newly-public cache portion includes a plurality of pieces of content that were received by first virtual machine <b>350</b>. Further and as discussed above, when data caching process <b>10</b> reclassifies <b>450</b> first assigned cache portion <b>356</b> as a public cache portion, data caching process <b>10</b> may share content directory <b>250</b> (which includes content identifiers <b>256</b>) with the other virtual machines (e.g., virtual machines <b>352</b>, <b>354</b>). Additionally, data caching process <b>10</b> may share content directory <b>250</b> (and the content identifiers <b>256</b> included therein) with second cache system <b>128</b> (included within data array <b>130</b>). As discussed above, second cache system <b>128</b> is also a content aware cache system. Therefore, a content identifier has been generated for each piece of content included within second cache system <b>128</b>.</p>
<p id="p-0129" num="0128">Data caching process <b>10</b> may compare <b>452</b> the content identifier associated with each piece of content included within the newly-public cache portion (i.e., the cache portion that was reclassified <b>450</b> by data caching process <b>10</b>) with content identifiers for pieces of content included within the portions of data array <b>130</b> that are associated with other virtual machines to generate a list of matching data portions. For this example and as discussed above, storage target <b>102</b> (of data array <b>130</b>) is associated with virtual machine <b>350</b>; storage target <b>104</b> (of data array <b>130</b>) is associated with virtual machine <b>352</b>; and storage target <b>106</b> (of data array <b>130</b>) is associated with virtual machine <b>354</b>. Accordingly, data caching process <b>10</b> may compare the content identifiers included within the content directory of the newly-public cache portion (which was previous assigned to virtual machine <b>350</b> before being reclassified <b>450</b> by data caching process <b>10</b>) to the content identifiers included within the content directories associated with e.g., second cache portion <b>366</b> of second cache system <b>128</b> associated with storage target <b>104</b> of data array <b>130</b> (which is associated with virtual machine <b>352</b>) and third cache portion <b>368</b> of second cache system <b>128</b> associated with storage target <b>106</b> of data array <b>130</b> (which is associated with virtual machine <b>354</b>).</p>
<p id="p-0130" num="0129">Once data caching process <b>10</b> has compared <b>452</b> the above-described content identifiers to determine whether matching content identifiers exist (and, therefore, matching content exists within data array <b>130</b>), data caching process <b>10</b> may generate list <b>372</b> that defines such matching content. Examples of the information included within list <b>372</b> may include but are not limited to: a content identifier for the matching content, the location of the matching content within data array <b>130</b>, and the location of the matching content within the newly-public cache portion within first cache system <b>126</b>.</p>
<p id="p-0131" num="0130">Data caching process <b>10</b> may provide <b>454</b> list <b>372</b> to the assigned cache portion(s) (e.g., second cache portion <b>358</b> and/or third cache portion <b>360</b>) within first cache system <b>126</b> that are associated with e.g., virtual machines <b>352</b>, <b>354</b> (respectively). Data caching process <b>10</b> may then process list <b>372</b> to update <b>456</b> the content directory associated with each of the assigned cache portions (e.g., second cache portion <b>358</b> and/or third cache portion <b>360</b>) within first cache system <b>126</b> based, at least in part, upon list <b>372</b>.</p>
<p id="p-0132" num="0131">Assume for this example that data caching process <b>10</b> reclassifies <b>450</b> first assigned cache portion <b>356</b> associated with (in this example) first virtual machine <b>350</b> as a public cache portion that is associated with and accessible by all virtual machines (e.g., virtual machines <b>350</b>, <b>352</b>, <b>354</b>).</p>
<p id="p-0133" num="0132">As discussed above, data caching process <b>10</b> may compare <b>452</b> the content identifier associated with each piece of content included within the newly-public cache portion (i.e., the cache portion that was reclassified <b>450</b> by data caching process <b>10</b>) with content identifiers for pieces of content included within the portions of data array <b>130</b> that are associated with other virtual machines to identify matches and generate a list of matching data portions.</p>
<p id="p-0134" num="0133">Further, assume that a piece of content included within this newly-public cache portion (as defined by entry <b>258</b> within content directory <b>250</b>): is stored within data array <b>130</b> at LUN0/LBA5; is stored within this newly-public cache portion of first cache system <b>126</b> at address 111110, and has a content identifier <b>256</b> of ablccba. Further, assume that once data caching process <b>10</b> compares <b>452</b> the content identifier (i.e., ablccba) included within entry <b>258</b> to the content identifiers for the content included within the portions of data array <b>130</b> that are associated virtual machine <b>352</b> and virtual machine <b>354</b> (namely storage target <b>104</b> of data array <b>130</b> and storage target <b>106</b> of data array <b>130</b>; respectively), a piece of content located at LUN1/LBA9 (within storage target <b>104</b>) and a piece of content located at LUN2/LBA6 (within storage target <b>106</b>) have an identical content identifier (namely ablccba). Accordingly, the content at LUN0/LBA5, LUN1/LBA9, and LUN2/LBA6 is identical. And as discussed above, this content is locally cached within first cache system <b>126</b> at address 111110.</p>
<p id="p-0135" num="0134">Accordingly, data caching process <b>10</b> may update <b>456</b> the content directory utilized by first cache system <b>126</b> for virtual machine <b>352</b> to add an entry for LUN1/LBA9 that points to address 111110 within first cache system <b>126</b> and identifies a content identifier (namely ablccba) for this entry. Therefore, in the event that virtual machine <b>352</b> receives a read request for data located at LUN1/LBA9 within data array <b>130</b>, data caching process <b>10</b> may obtain the locally-cached copy of the content from address 111110 of first cache system <b>126</b> (as opposed to having to obtain it from data array <b>130</b>).</p>
<p id="p-0136" num="0135">Further, data caching process <b>10</b> may update <b>456</b> the content directory utilized by first cache system <b>126</b> for virtual machine <b>354</b> to add an entry for LUN2/LBA6 that points to address 111110 within first cache system <b>126</b> and identifies a content identifier (namely ablccba) for this entry. Therefore, in the event that virtual machine <b>354</b> receives a read request for data located at LUN2/LBA6 within data array <b>130</b>, data caching process <b>10</b> may obtain the locally-cached copy of the content from address 111110 of first cache system <b>126</b> (as opposed to having to obtain it from data array <b>130</b>).</p>
<p id="p-0137" num="0136">While the system is described above as being utilized with virtual machines, this is for illustrative purposes only and is not intended to be a limitation disclosure, as other configurations are possible and are considered to be within the scope of this disclosure. For example, the above-described system may be configured to work with physical machines, wherein a plurality of physical machines share a communal cache system. This communal cache system may include a plurality of individual assigned cache portions that are each assigned to individual physical machines, and a public cache portion is shared amongst the physical machines.</p>
<p id="h-0015" num="0000">Cache System Copy</p>
<p id="p-0138" num="0137">Referring also to <figref idref="DRAWINGS">FIGS. 10-11</figref>, assume for illustrative purposes that first server computer/controller <b>500</b> includes first cache system <b>502</b>. First cache system <b>502</b> may include first content directory <b>504</b> and first cache content <b>506</b>. As discussed above, first content directory <b>504</b> may be configured to associate first cache content <b>506</b> stored within first cache system <b>502</b> with content (e.g., content <b>132</b>) stored within data array <b>130</b>.</p>
<p id="p-0139" num="0138">Assume that for some reason, data caching process <b>10</b> receives <b>550</b> an indication, on first server computer/controller <b>500</b>, that first server computer/controller <b>500</b> is to be shut down or that a resource running on first server computer/controller <b>500</b> is about to be moved. For example, first server computer/controller <b>500</b> may be e.g., shutting down for a service upgrade or resources associated with first server computer/controller <b>500</b> (such as a LUN) may be moving/reassigned for load balancing purposes.</p>
<p id="p-0140" num="0139">For example, assume that first server computer/controller <b>500</b> and other machines/devices are in a cluster, which allows only one machine/device to access each resource (e.g. LUN) at a time. Accordingly, data caching process <b>10</b> may effectuate the below-described functionality prior to transferring a resource from one machine/device to another machine/device within the cluster.</p>
<p id="p-0141" num="0140">Upon receiving <b>550</b> such an indication, data caching process <b>10</b> may copy <b>552</b> first cache system <b>502</b> from first server computer/controller <b>500</b> to second server computer/controller <b>508</b>.</p>
<p id="p-0142" num="0141">Second server computer/controller <b>508</b> may include second cache system <b>510</b>, which may be configured as a duplicate of first cache system <b>502</b>. Specifically, as first cache system <b>502</b> is being copied <b>552</b> from first server computer/controller <b>500</b> to second server computer/controller <b>508</b>, data caching process <b>10</b> may configure second cache system <b>510</b> so that is a duplicate copy of first cache system <b>502</b> (at the point that the above-described copying procedure began, completed, or somewhere in between). Accordingly, second cache system <b>510</b> may include duplicate content directory <b>512</b> (which is a duplicate of first content directory <b>504</b>) and duplicate cache content <b>514</b> (which is a duplicate of first cache content <b>506</b>).</p>
<p id="p-0143" num="0142">As discussed above, first server computer/controller <b>500</b> and second server computer/controller <b>508</b> may be configured in a cluster, which may require the transferring of resources (e.g., a LUN) from one machine to another machine within the cluster before a device may control a resource. In order to effectuate such a transfer of resources, data caching process <b>10</b> may partition cache system <b>502</b> and/or cache system <b>508</b> so that each cache partition services only a single resource, thus allowing for simplified cache management/reassignment/transfer.</p>
<p id="p-0144" num="0143">During the above-described copying <b>552</b> procedure, data caching process <b>10</b> may receive <b>554</b>, on first server computer/controller <b>500</b>, one or more data requests (e.g. data write requests <b>116</b> (which may include content <b>118</b>) and data read request <b>120</b>) that are intended to be executed on first cache system <b>502</b>. Specifically, these data requests may concern a plurality of data actions to be taken on data array <b>130</b> that is associated with first server computer/controller <b>500</b>. As discussed above, data write request <b>116</b> may include a piece of content (content <b>118</b>) to be written a specific LUN/LBA within data array <b>130</b>. Further, data read request may concern a specific piece of content to be retrieved from a LUN/LBA within data array <b>130</b>.</p>
<p id="p-0145" num="0144">As these requests are received <b>554</b> and until the above-described copying <b>552</b> procedure is completed, data caching process <b>10</b> may store <b>556</b> the plurality of data requests on tracking queue <b>154</b> included within data array <b>130</b> associated with first server computer/controller <b>500</b> and may or may not suspend adding content to first cache content <b>506</b>. Data caching process <b>10</b> may continue the above-described storage <b>556</b> procedure until the above-describe copy <b>552</b> procedure is completed. Additionally, while the above-described requests may bypass first cache system <b>502</b> and are stored <b>556</b> within tracking queue <b>154</b>, these requests are still satisfied.</p>
<p id="p-0146" num="0145">For example, while the above-described copy <b>552</b> procedure is occurring, data caching process <b>10</b> may be configured so that content is not added to first cache content <b>506</b>. Specifically, when read requests arrive, data caching process <b>10</b> may satisfy the read request by providing a copy of content included within first cache content <b>506</b> (assuming that a cache hit occurs). In the event of a cache miss, data caching process <b>10</b> may obtain the requested content from data array <b>130</b> (but not write it to first cache content <b>506</b>). When a write request is processed by data caching process <b>10</b>, data caching process <b>10</b> may write the related content to data array <b>130</b> and invalidate the related cache entry within first content directory <b>504</b> (as the content was not written to first cache content <b>506</b>). Accordingly and in such a configuration, tracking queue <b>154</b> may only need to keep track of write requests so that the cache entries associated with those write requests can be invalidated.</p>
<p id="p-0147" num="0146">Once the above-described copy <b>552</b> procedure is completed, data caching process <b>10</b> may shut down <b>558</b> first server computer/controller <b>500</b> or may move the requested resource to/from the first server. Additionally and upon completion of the above-described copy <b>552</b> procedure, data caching process <b>10</b> may update <b>560</b> duplicate content directory <b>512</b> on second server computer/controller <b>508</b> based, at least in part, upon the one or more data requests (e.g. data write requests <b>116</b>, which may include content <b>118</b>) stored <b>556</b> on tracking queue <b>154</b>.</p>
<p id="p-0148" num="0147">Specifically, the above-described duplicate content directory <b>512</b> (which is a duplicate of first content directory <b>504</b>) and the above-described duplicate cache content <b>514</b> (which is a duplicate of first cache content <b>506</b>) are a snapshot of what first cache system <b>502</b> looked like at the point in time when the cache stopped being updated before copying <b>552</b> procedure. Further, the data requests currently stored within tracking queue <b>154</b> represent the changes that would have been made to first cache system <b>502</b> during the copying procedure. Accordingly, by processing the requests stored within tracking queue <b>154</b> (with respect to second cache system <b>510</b>), second cache system <b>510</b> may be appropriately modified by the data requests included within tracking queue <b>154</b>. For example, if the data request stored within tracking queue <b>154</b> was a data write request, the data may be invalidated from cache system <b>510</b>.</p>
<p id="p-0149" num="0148">In another example, during the above-described copy <b>552</b>, data caching process <b>10</b> may be configured so that content is added to first cache content <b>506</b>. Specifically, when read requests arrive, data caching process <b>10</b> may satisfy the read request by providing a copy of content included within first cache content <b>506</b> (assuming that a cache hit occurs). In the event of a cache miss, data caching process <b>10</b> may obtain the requested content from data array <b>130</b> and write it to first cache content <b>506</b>. When a write request is processed by data caching process <b>10</b>, data caching process <b>10</b> may write the related content to data array <b>130</b> and write the content to first cache content <b>506</b>. Accordingly and in such a configuration, tracking queue <b>154</b> may keep track of both write requests and read request.</p>
<p id="p-0150" num="0149">Once the above-described copy <b>552</b> procedure is completed, data caching process <b>10</b> may shut down <b>558</b> first server computer/controller <b>500</b> or may move the requested resource to/from the first server computer/controller <b>500</b>. Additionally and upon completion of the above-described copy <b>552</b> procedure, data caching process <b>10</b> may update <b>560</b> duplicate content directory <b>512</b> on second server computer/controller <b>508</b> based, at least in part, upon the one or more data requests (e.g. data write requests <b>116</b> (which may include content <b>118</b>) and data read requests <b>120</b>) stored <b>556</b> on tracking queue <b>154</b>.</p>
<p id="p-0151" num="0150">Specifically, the above-described duplicate content directory <b>512</b> (which is a duplicate of first content directory <b>504</b>) is a snapshot of the content directory at some point in time during copying <b>552</b> procedure and each entry at the above-described duplicate cache content <b>514</b> (which is a duplicate of first cache content <b>506</b>) are a snapshot of the same entry of first cache system <b>502</b> at the point in time during copying process <b>552</b>. Further, the data requests currently stored within tracking queue <b>154</b> represent the changes that would have been made to first cache system <b>502</b> during the copying procedure. Accordingly, by processing the requests stored within tracking queue <b>154</b> (with respect to second cache system <b>510</b>), second cache system <b>510</b> may be appropriately modified by the data requests included within tracking queue <b>154</b>. For example, if the request stored within tracking queue <b>154</b> was a data write request, the data that was already written to (and is available from) data array <b>130</b> may be copied to second cache system <b>510</b>.</p>
<p id="p-0152" num="0151">Unfortunately, simply because a data request is included within tracking queue <b>154</b> does not guarantee that the data request was correctly/completely processed. For example, if data caching process <b>10</b> is configured so that content is added to first cache content <b>506</b> while the above-described copying <b>552</b> procedure is occurring, the accuracy of second cache content <b>514</b> may be questionable. Therefore, if the data requests within tracking queue <b>154</b> are to be processed to update <b>560</b> duplicate content directory <b>512</b>, data caching process <b>10</b> may need to verify <b>562</b> one or more data requests included within tracking queue <b>154</b> to ensure that e.g., a piece of content written to data array <b>130</b> was indeed written to second cache system <b>510</b> (which is a duplicate of first cache system <b>502</b>) prior to updating duplicate content directory <b>512</b> to indicate that it had been. This may be accomplished by comparing the actual piece of content stored within data array <b>130</b> to the related piece of content stored within e.g., second cache system <b>510</b>.</p>
<p id="p-0153" num="0152">Alternatively, if data caching process <b>10</b> was configured so that content was not added to first cache content <b>506</b> while the above-described copying <b>552</b> procedure is occurring, when updating <b>560</b> duplicate content directory <b>512</b>, data caching process <b>10</b> may simply modify duplicate content directory <b>512</b> to invalidate <b>564</b> the entries within duplicate content directory <b>512</b> that are related to write requests included within tracking queue <b>154</b>.</p>
<p id="h-0016" num="0000">Consolidated Write Operation</p>
<p id="p-0154" num="0153">Data caching process <b>10</b> may be configured to effectuate one or more consolidated write operation is to increase the efficiency of the above-described cache systems. For example and referring also to <figref idref="DRAWINGS">FIG. 12</figref>, data caching process <b>10</b> may receive <b>650</b>, on e.g., cache system <b>502</b>, a plurality of data write requests (e.g., multiple versions of write request <b>116</b>, wherein each data write request identifies a data portion (e.g., content <b>118</b>) to be written to data array <b>130</b> associated with cache system <b>502</b>.</p>
<p id="p-0155" num="0154">Typically and with respect to any write-through cache, when data write request are received, they are immediately processed, in that content <b>118</b> is immediately written to data array <b>130</b> and immediately written to cache system <b>502</b>, a write confirmation is received from data array <b>130</b> by cache system <b>502</b>, and a write confirmation is provided to the requester.</p>
<p id="p-0156" num="0155">Alternatively and with respect to write-back caches, content <b>118</b> is immediately written to the cache system <b>502</b> and a confirmation is provided to the requesterr. However, a plurality of write requests may be queued (in e.g., write request queue) for subsequent processing with respect to data array <b>130</b>. Specifically, after a defined period of time, the write request queued within write request queue <b>524</b> may be processed and the respective pieces of content associated with each write request may be written to data array <b>130</b>. Further, a write confirmation may be provided by data array <b>130</b> for each of the pieces of content written to data array <b>130</b>. Accordingly, in the event that one hundred write requests were processed (resulting in one hundred pieces of data being written to data array <b>130</b>), one hundred write confirmations may be received from data array <b>130</b>.</p>
<p id="p-0157" num="0156">However and in this particular environment, data caching process <b>10</b> may be configured to more efficiently write content to data array <b>130</b>. Continuing with the above-stated example, data caching process <b>10</b> may write <b>652</b> the data portions associated with the data write requests received by cache system <b>502</b>. Accordingly, as data write requests are received by cache system <b>502</b>, these write requests are processed and any associated data portions are written <b>652</b> to cache system <b>502</b>. However, data caching process <b>10</b> may queue <b>654</b> (with respect to data array <b>130</b>) the data portions (e.g. content <b>118</b>) associated with the data write requests (e.g. write request <b>116</b>) until the occurrence of a commit event. Examples of such a commit event may include but are not limited to the expiry of a commit timer (1,000 milliseconds) and the receipt of a defined quantity of data portions (1,000 pieces of content). For example, data caching process <b>10</b> may be configured such that content is written to data array <b>130</b> every 1,000 milliseconds or whenever 1,000 pieces of content are present within e.g., write request queue <b>524</b>.</p>
<p id="p-0158" num="0157">Upon the occurrence of such a commit event, data caching process <b>10</b> may perform <b>656</b> a write operation to write the data portions (e.g. content <b>118</b>) associated with the data write requests (e.g. write request <b>116</b>) to data array <b>130</b>.</p>
<p id="p-0159" num="0158">Specifically, when performing <b>656</b> such a write operation, data caching process <b>10</b> may periodically write <b>658</b> the data portions (e.g. content <b>118</b>) associated with the data write requests (e.g. write request <b>116</b>) to data array <b>130</b> and may receive <b>660</b> a single write confirmation from data array <b>130</b> in response to writing <b>658</b> the data portions (e.g. content <b>118</b>) associated with the data write requests (e.g. write request <b>116</b>) to data array <b>130</b>.</p>
<p id="p-0160" num="0159">For example, data caching process <b>10</b> may periodically write <b>658</b> content to the backend storage (e.g., data array <b>130</b>) so that performance improvements may be achieved. For example, write folding may result in increased performance. Specifically, if several of the write requests included within write request queue <b>524</b> concern the same LUN/LBA within storage array <b>130</b>, data caching process <b>10</b> may only process the last write request for that LUN/LBA (as successive writes to the same LUN/LBA overwrite each other). Further, as data caching process <b>10</b> provides a large quantity of write requests (e.g., 16, 32, 64, 128, 256, 512, 1024 write requests) to data array <b>130</b> in an asynchronous parallel fashion (and the data array only sends a single write confirmation to server computer/controller <b>100</b>), performance is increased, as server computer/controller <b>100</b> does not need to wait until a write confirmation is received for a first write request prior to sending a second write request.</p>
<p id="p-0161" num="0160">Accordingly, in the event that 1.000 pieces of content are written <b>658</b> by data caching process <b>10</b> to data array <b>130</b>, data caching process <b>10</b> may only receive <b>660</b> a single write confirmation (confirming the writing on 1,000 pieces of content) from data caching process <b>10</b> (as opposed to 1,000 write confirmations, each confirming the writing on one piece of content).</p>
<p id="p-0162" num="0161">Referring to <figref idref="DRAWINGS">FIG. 12A</figref>, there is shown a more detailed flowchart of data caching process <b>10</b> that differentiates between processes executed on server computer/controller <b>100</b> versus those executed on data array <b>130</b>.</p>
<p id="p-0163" num="0162">Specifically, data caching process <b>10</b> may receive <b>662</b> write requests, which may be processed so the related content may be written <b>664</b> to e.g., cache system <b>502</b>. Additionally, data caching process <b>10</b> may queue <b>666</b> these write requests (for subsequent batch processing by data array <b>130</b>) until the occurrence of a commit event. Examples of such a commit event may include but are not limited to the expiry of a commit timer (1,000 milliseconds) and the receipt of a defined quantity of data portions (1,000 pieces of content). Once such a commit event occurs, data caching process <b>10</b> may notify <b>668</b> (via notice <b>670</b>) of the upcoming commit event. Upon notice <b>670</b> being received <b>672</b> by data array <b>130</b>, data caching process <b>10</b> may take <b>674</b> a &#x201c;snapshot&#x201d; of the portion of data array <b>132</b> for which the above-described write requests concern. For example, data caching process <b>10</b> may make a copy of a particular LUN within storage array <b>130</b>.</p>
<p id="p-0164" num="0163">Data caching process <b>10</b> may send <b>676</b> all of the content (e.g. content <b>678</b>) concerning the above-described write requests to data array <b>130</b>. Upon receiving content <b>678</b>, data caching process <b>10</b> may perform <b>680</b> the above-described write operation. Once all of the content associated with the above-described write requests is sent <b>676</b> to data array <b>130</b>, data caching process <b>10</b> may notify <b>682</b> (via notice <b>684</b>) data array <b>130</b> that the commit event is complete.</p>
<p id="p-0165" num="0164">Upon such notification being received <b>686</b> by data array <b>130</b>, data caching process <b>10</b> that may delete <b>688</b> the above-described &#x201c;snapshot&#x201d; and may send <b>690</b> a write confirmation (e.g. confirmations <b>692</b>) concerning the content written to data array <b>130</b>, which is received <b>694</b> by server computer/controller <b>100</b>.</p>
<p id="p-0166" num="0165">In the event of e.g., a crash of server computer/controller <b>100</b> prior to the completion of the above-described commit event, the data stored within data array <b>130</b> may be corrupt due to the asynchronous manner in which the content is written to data array <b>130</b>. Accordingly, data caching process <b>10</b> may use the &#x201c;snapshot&#x201d; to restore data array <b>130</b> to the condition that it was in prior to the commit event being initiated by data array <b>130</b>.</p>
<p id="h-0017" num="0000">Virtual Machine Copy</p>
<p id="p-0167" num="0166">Referring also to <figref idref="DRAWINGS">FIGS. 13A-13B</figref>, assume for the following example that first server computer/controller <b>700</b> is a physical device that is executing first virtual machine operating environment <b>702</b>. An example of such a virtual machine operating environment is a hypervisor, which is an instantiation of an operating system that allows for multiple virtual machines to operate within a single physical device (e.g., first server computer/controller <b>700</b>). Further, second server computer/controller <b>704</b> is a physical device that is executing second virtual machine operating environment <b>706</b> (e.g., a hypervisor).</p>
<p id="p-0168" num="0167">For this example, further assume that hypervisor <b>702</b> within first server computer/controller <b>700</b> is executing a virtual machine (e.g., virtual machine <b>708</b>). While hypervisor <b>702</b> is shown to be executing only a single virtual machine, this is for illustrative purposes only and is not intended to be a limitation of this disclosure, as the number of virtual machines executed within hypervisor <b>702</b> may increase/decrease depending upon need/loading.</p>
<p id="p-0169" num="0168">Assume further that first server computer/controller <b>700</b> and second server computer/controller <b>704</b> each includes a non-volatile storage system (e.g., flash memory devices <b>710</b>, <b>712</b>, respectively) that first server computer/controller <b>700</b> and second server computer/controller <b>704</b> may use for cache storage, Each of flash memory devices <b>710</b>, <b>712</b> may be compartmentalized (e.g., into &#x201c;n&#x201d; portions, wherein a unique portion of flash memory device <b>710</b> is assigned to each virtual machine running on hypervisor <b>702</b> and a unique portion of flash memory device <b>712</b> is assigned to each virtual machine running on hypervisor <b>706</b>.</p>
<p id="p-0170" num="0169">Virtual machine <b>708</b> may include cache system <b>714</b>, which may include content directory <b>716</b>. Content directory <b>716</b> may be configured to identify the location of specific pieces of content included within (in this example) flash memory device <b>710</b>) and their corresponding pieces of data within data array <b>130</b>.</p>
<p id="p-0171" num="0170">Referring also to <figref idref="DRAWINGS">FIG. 14</figref>, assume that for some reason (e.g., maintenance/load balancing), data caching process <b>10</b> receives <b>750</b> an indication, on virtual machine <b>708</b>, that virtual machine <b>708</b> is going to be migrated. Specifically, through the use of products like VMware, virtual machines may be migrated (from a first physical device to a second physical device) without being shut down, (which is commonly referred to as a live migration or vmotion). Importantly, virtual machines typically cannot be migrated when the virtual machine is using/coupled to a physical storage device. Accordingly and in such a situation, the physical storage device will first need to be detached from the virtual machine before the virtual machine can be migrated.</p>
<p id="p-0172" num="0171">Assume for illustrative purposes that the above-described indication concerns virtual machine <b>708</b> being migrated from hypervisor <b>702</b> running on first server computer/controller <b>700</b> to hypervisor <b>706</b> running on second server computer/controller <b>704</b>. Specifically, <figref idref="DRAWINGS">FIG. 13A</figref> is intended to illustrate the system prior to the migration of virtual machine <b>708</b> to hypervisor <b>706</b> and <figref idref="DRAWINGS">FIG. 13B</figref> is intended to illustrate the system subsequent to the migration of virtual machine <b>708</b> to hypervisor <b>706</b>. One or more of the above-described hypervisors (e.g., hypervisor <b>702</b> and/or hypervisor <b>706</b>) may use one or more plug-ins/applets within the management framework of the hypervisor to allow for processing of the above-described indication notification and to effectuate the above-described migration.</p>
<p id="p-0173" num="0172">Upon receiving <b>750</b> the above-described indication, data caching process <b>10</b> may downgraded <b>752</b> the mode of operation of cache system <b>714</b> within virtual machine <b>708</b>. The manner in which the mode of operation of cache system <b>714</b> is downgraded <b>752</b> by data caching process <b>10</b> may vary depending on the manner in which data caching process <b>10</b> is implemented. For example and for illustrative purposes, assume that data caching process <b>10</b> downgrades <b>752</b> the mode of operation of cache system <b>714</b> in a two-step process. For example, upon receiving the above-described indication, data caching process <b>10</b> may initially downgrade <b>754</b> the operation of cache system <b>714</b> so that, once initially downgraded <b>754</b>, cache system <b>714</b> may operate as follows: a) for read requests that result in cache hits, cache system <b>714</b> will obtain the requested content from the appropriate portion of flash memory device <b>710</b> that is associated with cache system <b>714</b>; for read requests that result in cache misses, cache system <b>714</b> may obtain the requested content from data array <b>130</b> (but the obtained content will not be copied into flash memory device <b>710</b>); and c) for write requests, the provided content may be written to data array <b>130</b> (but the provided content will not be copied into flash memory device <b>710</b>) and the corresponding cache entry in content directory <b>716</b> will be invalidated.</p>
<p id="p-0174" num="0173">Data caching process <b>10</b> may place flash memory device <b>710</b> into the above-described downgrade mode so that data caching process <b>10</b> may copy <b>756</b> content included within the appropriate portion of flash memory device <b>710</b> to the appropriate portion of flash memory device <b>712</b> without having new data written to flash memory device <b>710</b>. Accordingly, by placing flash memory device <b>710</b> into the above-described downgrade mode, data caching process <b>10</b> is ensuring that the appropriate content within flash memory device <b>710</b> will be accurately copied to the appropriate portion of flash memory device <b>712</b>. For example, assume that cache portion c<sub>1 </sub>of flash memory device <b>710</b> was assigned to virtual machine <b>708</b> and (once migrated) cache portion c<sub>1 </sub>of flash memory device <b>712</b> will be assigned to virtual machine <b>708</b>. Accordingly, by placing flash memory device <b>710</b> into the above-described downgrade mode, the accuracy of the above-described copy procedure <b>756</b> is insured by data caching process <b>10</b>, as no additional data will be written to cache portion c<sub>1 </sub>of flash memory device <b>710</b> once copy procedure <b>756</b> is initiated.</p>
<p id="p-0175" num="0174">Once copy procedure <b>756</b> is completed, data caching process <b>10</b> may place <b>758</b> flash memory device <b>710</b> into a subsequent downgrade mode, thus placing cache system <b>714</b> into a higher level of downgrade. When placed into this higher level of downgrade, cache system <b>714</b> may operate as follows: a) for read requests that result in cache hits, cache system <b>714</b> will obtain the requested content from data array <b>130</b>; for read requests that result in cache misses, cache system <b>714</b> may obtain the requested content from data array <b>130</b> (and the obtained content will not be copied into flash memory device <b>710</b>); and c) for write requests, the provided content may be written to data array <b>130</b> (but the provided content will not be copied into flash memory device <b>710</b>) and the corresponding cache entry in content directory <b>716</b> will be invalidated.</p>
<p id="p-0176" num="0175">Upon entering <b>758</b> subsequent downgrade mode, data caching process <b>10</b> may detach <b>760</b> flash memory device <b>710</b> from virtual machine <b>708</b> and may migrate <b>762</b> virtual machine <b>708</b> from hypervisor <b>702</b> to hypervisor <b>706</b>. As discussed above, through the use of products like VMware, virtual machine <b>708</b> may be migrated <b>762</b> from hypervisor <b>702</b> to hypervisor <b>706</b> without being shut down, which is commonly referred to as a live migration or vmotion.</p>
<p id="p-0177" num="0176">Once data caching process <b>10</b> migrates <b>762</b> virtual machine <b>708</b> from hypervisor <b>702</b> to hypervisor <b>706</b> (resulting in the system being configured in the manner shown in <figref idref="DRAWINGS">FIG. 13B</figref>), data caching process <b>10</b> may attach <b>764</b> flash memory <b>712</b> to the newly-migrated virtual machine <b>708</b> and resume <b>766</b> normal operation of virtual machine <b>708</b>.</p>
<p id="p-0178" num="0177">As will be appreciated by one skilled in the art, the present disclosure may be embodied as a method, system, or computer program product. Accordingly, the present disclosure may take the form of an entirely hardware embodiment, an entirely software embodiment (including firmware, resident software, micro-code, etc.) or an embodiment combining software and hardware aspects that may all generally be referred to herein as a &#x201c;circuit,&#x201d; &#x201c;module&#x201d; or &#x201c;system.&#x201d; Furthermore, the present disclosure may take the form of a computer program product on a computer-usable storage medium having computer-usable program code embodied in the medium.</p>
<p id="p-0179" num="0178">The flowchart and block diagrams in the Figures illustrate the architecture, functionality, and operation of possible implementations of systems, methods and computer program products according to various embodiments of the present disclosure. In this regard, each block in the flowchart or block diagrams may represent a module, segment, or portion of code, which comprises one or more executable instructions for implementing the specified logical function(s). It should also be noted that, in some alternative implementations, the functions noted in the block may occur out of the order noted in the figures. For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or the blocks may sometimes be executed in the reverse order, depending upon the functionality involved. It will also be noted that each block of the block diagrams and/or flowchart illustration, and combinations of blocks in the block diagrams and/or flowchart illustration, can be implemented by special purpose hardware-based systems that perform the specified functions or acts, or combinations of special purpose hardware and computer instructions.</p>
<p id="p-0180" num="0179">The terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of the disclosure. As used herein, the singular forms &#x201c;a&#x201d;, &#x201c;an&#x201d; and &#x201c;the&#x201d; are intended to include the plural forms as well, unless the context clearly indicates otherwise. It will be further understood that the terms &#x201c;comprises&#x201d; and/or &#x201c;comprising,&#x201d; when used in this specification, specify the presence of stated features, integers, steps, operations, elements, and/or components, but do not preclude the presence or addition of one or more other features, integers, steps, operations, elements, components, and/or groups thereof.</p>
<p id="p-0181" num="0180">The corresponding structures, materials, acts, and equivalents of all means or step plus function elements in the claims below are intended to include any structure, material, or act for performing the function in combination with other claimed elements as specifically claimed. The description of the present disclosure has been presented for purposes of illustration and description, but is not intended to be exhaustive or limited to the disclosure in the form disclosed. Many modifications and variations will be apparent to those of ordinary skill in the art without departing from the scope and spirit of the disclosure. The embodiment was chosen and described in order to best explain the principles of the disclosure and the practical application, and to enable others of ordinary skill in the art to understand the disclosure for various embodiments with various modifications as are suited to the particular use contemplated.</p>
<p id="p-0182" num="0181">A number of implementations have been described. Having thus described the disclosure of the present application in detail and by reference to embodiments thereof, it will be apparent that modifications and variations are possible without departing from the scope of the disclosure defined in the appended claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A computer-implemented method comprising:
<claim-text>receiving, on a cache system from a server computer/controller, a plurality of data write requests, wherein each data write request identifies a data portion to be written to a data array associated with the cache system;</claim-text>
<claim-text>writing the data portions associated with the data write requests to the cache system;</claim-text>
<claim-text>queuing the data portions associated with the data write requests until the occurrence of a commit event;</claim-text>
<claim-text>taking a snapshot of the data portion of the data array;</claim-text>
<claim-text>upon the occurrence of the commit event, performing a consolidated write operation to write the data portions associated with the data write requests to the data array;</claim-text>
<claim-text>deleting the snapshot in response to writing the data portions associated with the data write requests to the data array; and</claim-text>
<claim-text>receiving, on the cache system and the server computer/controller, a single write confirmation from the data array in response to deleting the snapshot and writing the data portions associated with the data write requests to the data array.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the commit event is chosen from the group consisting of:
<claim-text>the expiry of a commit timer; and</claim-text>
<claim-text>the receipt of a defined quantity of data portions.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein performing the consolidated write operation includes:
<claim-text>writing the data portions associated with the data write requests to the data array.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the cache system includes one or more flash memory storage devices.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the data array includes one or more electro-mechanical storage devices.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the cache system is a content-aware cache system.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. A computer program product residing on a non-transitory computer readable medium having a plurality of instructions stored thereon which, when executed by a processor, cause the processor to perform operations comprising:
<claim-text>receiving, on a cache system from a server computer/controller, a plurality of data write requests, wherein each data write request identifies a data portion to be written to a data array associated with the cache system;</claim-text>
<claim-text>writing the data portions associated with the data write requests to the cache system;</claim-text>
<claim-text>queuing the data portions associated with the data write requests until the occurrence of a commit event;</claim-text>
<claim-text>taking a snapshot of the data portion of the data array;</claim-text>
<claim-text>upon the occurrence of the commit event, performing a consolidated write operation to write the data portions associated with the data write requests to the data array;</claim-text>
<claim-text>deleting the snapshot in response to writing the data portions associated with the data write requests to the data array; and</claim-text>
<claim-text>receiving, on the cache system and the server computer/controller, a single write confirmation from the data array in response to deleting the snapshot and writing the data portions associated with the data write requests to the data array.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The computer program product of <claim-ref idref="CLM-00007">claim 7</claim-ref> wherein the commit event is chosen from the group consisting of:
<claim-text>the expiry of a commit timer; and</claim-text>
<claim-text>the receipt of a defined quantity of data portions.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The computer program product of <claim-ref idref="CLM-00007">claim 7</claim-ref> wherein the instructions for performing the consolidated write operation include instructions for:
<claim-text>writing the data portions associated with the data write requests to the data array.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The computer program product of <claim-ref idref="CLM-00007">claim 7</claim-ref> wherein the cache system includes one or more flash memory storage devices.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The computer program product of <claim-ref idref="CLM-00007">claim 7</claim-ref> wherein the data array includes one or more electro-mechanical storage devices.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The computer program product of <claim-ref idref="CLM-00007">claim 7</claim-ref> wherein the cache system is a content-aware cache system.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. A computing system including at least one processor and at least one memory architecture coupled with the at least one processor, wherein the computing system is configured to perform operations comprising:
<claim-text>receiving, on a cache system from a server computer/controller, a plurality of data write requests, wherein each data write request identifies a data portion to be written to a data array associated with the cache system;</claim-text>
<claim-text>writing the data portions associated with the data write requests to the cache system;</claim-text>
<claim-text>queuing the data portions associated with the data write requests until the occurrence of a commit event;</claim-text>
<claim-text>taking a snapshot of the data portion of the data array;</claim-text>
<claim-text>upon the occurrence of the commit event, performing a consolidated write operation to write the data portions associated with the data write requests to the data array;</claim-text>
<claim-text>deleting the snapshot in response to writing the data portions associated with the data write requests to the data array; and</claim-text>
<claim-text>receiving, on the cache system and the server computer/controller, a single write confirmation from the data array in response to deleting the snapshot and writing the data portions associated with the data write requests to the data array.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The computing system of <claim-ref idref="CLM-00013">claim 13</claim-ref> wherein the commit event is chosen from the group consisting of:
<claim-text>the expiry of a commit timer; and</claim-text>
<claim-text>the receipt of a defined quantity of data portions.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The computing system of <claim-ref idref="CLM-00013">claim 13</claim-ref> wherein performing the consolidated write operation includes:
<claim-text>writing the data portions associated with the data write requests to the data array.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The computing system of <claim-ref idref="CLM-00013">claim 13</claim-ref> wherein the cache system includes one or more flash memory storage devices.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The computing system of <claim-ref idref="CLM-00013">claim 13</claim-ref> wherein the data array includes one or more electro-mechanical storage devices.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The computing system of <claim-ref idref="CLM-00013">claim 13</claim-ref> wherein the cache system is a content-aware cache system.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. A computer-implemented method comprising:
<claim-text>receiving write requests on a server computer/controller;</claim-text>
<claim-text>writing the content related to the write requests to a cache system;</claim-text>
<claim-text>queuing the content related to the write requests until the occurrence of a commit event;</claim-text>
<claim-text>upon the occurrence of the commit event, notifying a data array of the upcoming commit event;</claim-text>
<claim-text>taking a snapshot of the portion of the data array;</claim-text>
<claim-text>sending the content related to the write requests to the data array;</claim-text>
<claim-text>performing a write operation on the data array with respect to the content related to the write requests;</claim-text>
<claim-text>deleting the snapshot in response to writing the content related to the write requests to the data array; and</claim-text>
<claim-text>receiving, on the server computer/controller and the cache system, a single write confirmation from the data array in response to deleting the snapshot and writing the content related to the write requests to the data array.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The computer-implemented method of <claim-ref idref="CLM-00019">claim 19</claim-ref> further comprising:
<claim-text>notifying the data array that the commit event is complete.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The computer-implemented method of <claim-ref idref="CLM-00020">claim 20</claim-ref> further comprising:
<claim-text>in the event of a crash during the commit event, restoring the portion of the data array based, at least in part, upon the snapshot.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The computer-implemented method of <claim-ref idref="CLM-00019">claim 19</claim-ref> wherein the cache system includes one or more flash memory storage devices.</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The computer-implemented method of <claim-ref idref="CLM-00019">claim 19</claim-ref> wherein the data array includes one or more electro-mechanical storage devices.</claim-text>
</claim>
</claims>
</us-patent-grant>
