<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08624836-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08624836</doc-number>
<kind>B1</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12258009</doc-number>
<date>20081024</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<us-term-of-grant>
<us-term-extension>1293</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20130101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>3</main-group>
<subgroup>033</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>345157</main-classification>
<further-classification>345158</further-classification>
<further-classification>345160</further-classification>
<further-classification>345162</further-classification>
<further-classification>345164</further-classification>
<further-classification>345167</further-classification>
</classification-national>
<invention-title id="d2e53">Gesture-based small device input</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>6144366</doc-number>
<kind>A</kind>
<name>Numazaki et al.</name>
<date>20001100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345156</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>7023426</doc-number>
<kind>B1</kind>
<name>Robinson</name>
<date>20060400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345169</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>7030840</doc-number>
<kind>B2</kind>
<name>Mametsuka</name>
<date>20060400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345 76</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>7038659</doc-number>
<kind>B2</kind>
<name>Rajkowski</name>
<date>20060500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345156</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>8011582</doc-number>
<kind>B2</kind>
<name>Ghafarzadeh</name>
<date>20110900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>235386</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>8036428</doc-number>
<kind>B2</kind>
<name>Lin et al.</name>
<date>20111000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382107</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2002/0033803</doc-number>
<kind>A1</kind>
<name>Holzrichter et al.</name>
<date>20020300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345158</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2003/0146887</doc-number>
<kind>A1</kind>
<name>Mametsuka</name>
<date>20030800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345 76</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2004/0021633</doc-number>
<kind>A1</kind>
<name>Rajkowski</name>
<date>20040200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345156</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2004/0155870</doc-number>
<kind>A1</kind>
<name>Middleton</name>
<date>20040800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345173</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2005/0004496</doc-number>
<kind>A1</kind>
<name>Pilu et al.</name>
<date>20050100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>600595</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2005/0099389</doc-number>
<kind>A1</kind>
<name>Ma et al.</name>
<date>20050500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345158</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2005/0099390</doc-number>
<kind>A1</kind>
<name>Ma et al.</name>
<date>20050500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345158</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2005/0156889</doc-number>
<kind>A1</kind>
<name>Ma et al.</name>
<date>20050700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345163</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2005/0253806</doc-number>
<kind>A1</kind>
<name>Liberty et al.</name>
<date>20051100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345156</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2006/0177227</doc-number>
<kind>A1</kind>
<name>Blasko et al.</name>
<date>20060800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>398128</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>2006/0195020</doc-number>
<kind>A1</kind>
<name>Martin et al.</name>
<date>20060800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>600301</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>2006/0244722</doc-number>
<kind>A1</kind>
<name>Gust</name>
<date>20061100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345156</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>2008/0084385</doc-number>
<kind>A1</kind>
<name>Ranta et al.</name>
<date>20080400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345157</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>2008/0097221</doc-number>
<kind>A1</kind>
<name>Florian</name>
<date>20080400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>600476</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>2008/0167535</doc-number>
<kind>A1</kind>
<name>Stivoric et al.</name>
<date>20080700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>600301</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>2009/0051649</doc-number>
<kind>A1</kind>
<name>Rondel</name>
<date>20090200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345156</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>2009/0059730</doc-number>
<kind>A1</kind>
<name>Lyons et al.</name>
<date>20090300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>368 69</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>2009/0131165</doc-number>
<kind>A1</kind>
<name>Buchner et al.</name>
<date>20090500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>463 30</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>2010/0016990</doc-number>
<kind>A1</kind>
<name>Kurtz</name>
<date>20100100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>623 24</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>2010/0321899</doc-number>
<kind>A1</kind>
<name>Vossoughi et al.</name>
<date>20101200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>361728</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>2011/0205156</doc-number>
<kind>A1</kind>
<name>Gomez et al.</name>
<date>20110800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345157</main-classification></classification-national>
</us-citation>
<us-citation>
<nplcit num="00028">
<othercit>Krueger et al. &#x201c;Videoplace&#x2014;An Artificial Reality.&#x201d; <i>CHI '85 Proceedings </i>, Apr. 1985, pp. 35-40.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00029">
<othercit>Arnaut, <i>Input Devices</i>, Academic Press, Inc., 1988, 5 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00030">
<othercit>Atkins, <i>Digital Waveguide Touch</i><sup>&#x2122;</sup>&#x2014;<i>A New Touch Screen Platform</i>, RPO, Inc., 2007, pp. 1-6.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00031">
<othercit>&#x201c;Bluetooth-The i.Tech Virtual Keyboard&#x201d; [online]. Golan Technology, [retrieved on Nov. 25, 2008]. Retrieved from the Internet: &#x3c;URL: http;//www.vkb-support.com/learn<sub>&#x2014;</sub>more.php&#x3e;. 4 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00032">
<othercit>Butler, &#x201c;SideSight: Multi-&#x201c;touch&#x201d; Interaction Around Small Devices,&#x201d; UIST 2008, pp. 201-204.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00033">
<othercit>Hinckley, &#x201c;Sensing Techniques for Mobile Interaction,&#x201d; <i>CHI Lettrs</i>, 2(2):91-100.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00034">
<othercit>Hollemans, &#x201c;Entertable: Multi-user multi-object concurrent input,&#x201d; UIST 2006 Adjunct Proceedings: Demonstrations, 2006, pp. 55-56.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00035">
<othercit>Howard and Howard, &#x201c;Ubiquitous Computing Enabled by Optical Reflectance Controller&#x201d; [online]. Lightglove, Inc, [retrieved on 25 Nov 2008]. Retrieved from the Internet: &#x3c;URL: http://www.lightglove.com/White%20Paper.hm&#x3e;. 8 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00036">
<othercit>Smith, &#x201c;Electric Field Sensing for Graphical Interfaces,&#x201d; IEEE Xplore, 1998, pp. 54-60.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00037">
<othercit>&#x201c;The i.Tech Virtual Keyboard&#x201d; [online]. Golan Technology, [retrieved on Nov 25, 2008]. Retrieved from the Internet: &#x3c;Url: http;//www.vkb-support.com/index.php&#x3e;. 1 page.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00038">
<othercit>Vogel, &#x201c;Shift: A Technique for Operating Pen-Based Interfaces Using Touch,&#x201d; CHI 2007 Proceedings, Mobile Interaction Techniques, 2007, pp. 657-666.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00039">
<othercit>Wigdor, &#x201c;LucidTouch: A See-Through Mobile Device,&#x201d; UIST 2007, 2007, pp. 269-278.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>32</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>345  8</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345102</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345156-179</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345204</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345213</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345418</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345581</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345589</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345629</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>340  561</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>340461</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>340462</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>340540</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>235386</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>4555661</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>398128</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>361728</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>600301</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>600476</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>600595</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>7</number-of-drawing-sheets>
<number-of-figures>7</number-of-figures>
</figures>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Miller</last-name>
<first-name>James B.</first-name>
<address>
<city>Sunnyvale</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Gossweiler, III</last-name>
<first-name>Richard C.</first-name>
<address>
<city>Sunnyvale</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Miller</last-name>
<first-name>James B.</first-name>
<address>
<city>Sunnyvale</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Gossweiler, III</last-name>
<first-name>Richard C.</first-name>
<address>
<city>Sunnyvale</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Fish &#x26; Richardson P.C.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Google Inc.</orgname>
<role>02</role>
<address>
<city>Mountain View</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Dharia</last-name>
<first-name>Prabodh M</first-name>
<department>2693</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A computer-implemented method is described that includes providing a watch defining a body and a watch face on a surface of the body, the body having one or more sensors arranged to sense user inputs in an area adjacent to the body. The method further comprises sensing a motion of an object in the area adjacent to, but not touching, the body using the one or more sensors and changing a display of a pointing element on a graphical user interface on the watch in coordination with the sensed motion.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="107.78mm" wi="154.01mm" file="US08624836-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="157.99mm" wi="111.34mm" orientation="landscape" file="US08624836-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="185.76mm" wi="115.74mm" orientation="landscape" file="US08624836-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="215.90mm" wi="166.79mm" orientation="landscape" file="US08624836-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="182.29mm" wi="124.38mm" file="US08624836-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="202.52mm" wi="115.82mm" file="US08624836-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="198.63mm" wi="111.34mm" file="US08624836-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="215.90mm" wi="169.50mm" orientation="landscape" file="US08624836-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">TECHNICAL FIELD</heading>
<p id="p-0002" num="0001">This instant specification relates to input devices for computers.</p>
<heading id="h-0002" level="1">BACKGROUND</heading>
<p id="p-0003" num="0002">As computer processors have decreased in size and expense, mobile computing devices have become increasingly widespread. Designed to be portable, many mobile computing devices are lightweight and small enough to be worn or carried in a pocket or handbag. However, the portability of modern mobile computing devices comes at a price: today's mobile computing devices often incorporate small input devices to reduce the size and weight of the device. For example, many current mobile devices include small QWERTY keyboards that many people (especially those with poor dexterity) find difficult to use.</p>
<heading id="h-0003" level="1">SUMMARY</heading>
<p id="p-0004" num="0003">In general, this document describes a gesture-based interface for small computing devices. For example, a wearable computing device (e.g., a watch) may include a plurality of sensor-emitter pairs that can create a virtual mouse pad on a user's forearm, wrist, or the back of the user's hand. When the user moves an object (e.g., a finger or stylus) in the virtual mouse pad area, a corresponding movement or action may take place in the device's graphical user interface (&#x201c;GUI&#x201d;). For instance, as the user moves his finger in the virtual mouse pad, the GUI's pointing device may track the movement of the user's finger. In some implementations, placing an object in the virtual mouse pad may result in the GUI performing an action associated with the object. For example, placing a key in the virtual mouse pad may unlock the device, allowing further device inputs.</p>
<p id="p-0005" num="0004">In one implementation, a computer-implemented method is described that includes providing a watch defining a body and a watch face on a surface of the body, the body having one or more sensors arranged to sense user inputs in an area adjacent to the body. The method further comprises sensing a motion of an object in the area adjacent to, but not touching, the body using the one or more sensors and changing a display of a pointing element on a graphical user interface on the watch in coordination with the sensed motion.</p>
<p id="p-0006" num="0005">In some aspects, sensing the motion of the object comprises sensing an intensity level of light reflected off the object. Also, one sensor can be vertically offset from another sensor and vertical motion can be used to generate a clicking event. In addition, the one or more sensors can include at least two light sources and two light sensors. Furthermore, an accelerometer input from an accelerometer can generate a clicking event. Moreover, the watch may include a CCD camera and a light emitting diode.</p>
<p id="p-0007" num="0006">In another implementation, a pointing system comprising a device having a body defining an upper face and an outer periphery is disclosed. The system may include one or more sensors arranged to sense user inputs in an area adjacent to, but not touching, the body. The system may also include a processor in the device, attached to the one or more sensors, to change a display of a pointing element on a graphical user interface on the device in coordination with sensed user input.</p>
<p id="p-0008" num="0007">In certain aspects, the sensors can be mounted in an outer periphery of the body and one sensor may be vertically offset from another sensor. Also, the one or more sensors can include at least two light sources and two light sensors. In addition, the processor can use accelerometer input from an accelerometer to generate a clicking event.</p>
<p id="p-0009" num="0008">The systems and techniques described here may provide one or more of the following advantages. First, the systems may allow a user to interact with a GUI without obscuring parts of the GUI (particularly when the GUI is on a very small screen). Second, the system may increase the gesture vocabulary of a device. Third, the system may provide gesture-based input without requiring a separate piece of equipment.</p>
<p id="p-0010" num="0009">The details of one or more embodiments of the gesture-based small device input feature are set forth in the accompanying drawings and the description below. Other features and advantages of the gesture-based small device input feature will be apparent from the description and drawings, and from the claims.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">DESCRIPTION OF DRAWINGS</heading>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 1A</figref> is a conceptual diagram of gesture-based small device input.</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 1B</figref> shows an illustrative placement of sensor-emitter pairs on a gesture-based small input device.</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 2</figref> shows a block diagram of an illustrative gesture-based small input device.</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 3</figref> shows a conceptual diagram illustrating the calculation of X-Y coordinates for a detected object.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 4</figref> is a flow chart of an example process for gesture-based device input.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 5</figref> is a flow chart of an example alternative process for gesture-based device input.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 6</figref> shows an example of a generic computer device and a generic mobile computer device, which may be used with the techniques described here.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<p id="p-0018" num="0017">Like reference symbols in the various drawings indicate like elements.</p>
<heading id="h-0005" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0019" num="0018">This document describes systems and techniques for providing gesture-based input to small devices. In general, a computer processor incorporated into a portable device (e.g, a wristwatch or cellular phone) can receive inputs from one or more sensor-emitters pairs integrated into the body of the device. The sensor-emitter pairs may detect inputs in an area adjacent to the device and provide these inputs to the processor. The processor may then translate the inputs into GUI commands. For example, a wristwatch strapped to a user's wrist may include various sensor-emitters that define a detection area on the user's wrist and forearm. Movements in the detection area can be identified by the sensor-emitter pairs and passed to the wristwatch's processor. The processor may convert the detected movements into GUI actions such as pointer drags, taps, etc.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 1A</figref> is a conceptual diagram <b>100</b> of gesture-based small device input. The diagram <b>100</b> shows a wearable computing device <b>102</b> strapped to a user's body by a band <b>104</b>. In an illustrative example, the device <b>102</b> may be a watch, but in other implementations the device may be a cellular telephone, personal digital assistant, portable media player, or other portable computing device. Also, functionality from more than one such type of apparatus may be incorporated into the device <b>102</b>.</p>
<p id="p-0021" num="0020">The illustrative computing device <b>102</b> may include a GUI produced by a computer operating system that permits a user to interact with the computing device <b>102</b>. In some implementations, the GUI may include one or more graphical components such as windows, icons, menus, or pointing devices that allow a user to interact with an operating system running on the device. The GUI may display a desktop environment where interaction with the environment can be controlled through use of an input device. For example, the computing device <b>102</b> may contain a plurality of sensor-emitter pairs <b>108</b> in the side of a device housing that can create a detection area <b>116</b> adjacent to the device <b>102</b>. In an illustrative example, a user may control the GUI by placing an object, such as the user's finger or a stylus, in the detection area <b>116</b>. As shown in the in the conceptual diagram <b>100</b>, as a user moves his finger in the detection area <b>116</b>, the GUI's pointing device may follow a parallel path in the GUI, as shown in an expanded view <b>118</b> of the screen <b>114</b>.</p>
<p id="p-0022" num="0021">The illustrative device <b>102</b> may also be able to identify objects placed in the detection area and perform a GUI action associated with the objects. For example, a small cube may have images of a calendar, envelope, and a globe on some of its sides. Placing the picture of the calendar in the detection area <b>116</b> may cause the device to load and execute a calendar application. Similarly, placing the envelope or globe side of the cube in the detection area <b>116</b> may result in the device running an email or map application. In some implementations, placing an object such as a business card or barcode in the detection area <b>116</b> may cause the device <b>102</b> to perform a web search for information related to the object.</p>
<p id="p-0023" num="0022">In some implementations, the GUI can process complex, multi-input interactions detected by the sensor-emitter pairs <b>108</b>. For example, the GUI may process actions such as pinching and rotating motions by a user's fingers that are detected by sensor-emitter pairs <b>108</b>. For example, users may select documents in the GUI by pinching a location in the detection area <b>116</b> corresponding to the documents and rotate the selected documents by rotating their fingers in the detection area <b>116</b>.</p>
<p id="p-0024" num="0023">In some implementations, the computing device <b>102</b> may use visible light to provide the user an indication of the dimensions of the detection area <b>116</b>. For example, an emitter in the sensor-emitter pairs <b>108</b> (e.g., an LED) may produce visible light. In other implementations, a separate visible light source may be placed adjacent to the sensor-emitter pairs <b>108</b>. The area illuminated by the light source may approximate the detection area of the sensor-emitter pairs. The illumination area may facilitate interacting with the device because the illumination area may provide the user with a rough estimate of the contours of the detection area.</p>
<p id="p-0025" num="0024">In other implementations, the computing device <b>102</b> may superimpose an image onto the detection area <b>116</b> to further facilitate user interaction with the GUI. For instance, in some implementations, the computing device <b>102</b> may superimpose the image of a QWERTY keyboard on the detection area. When a user presses a button on the virtual keyboard, a corresponding input can be registered by the GUI. For example, if the user selects the &#x201c;A&#x201d; button in the virtual keypad, an &#x201c;A&#x2019; can be entered in the GUI.</p>
<p id="p-0026" num="0025">The device <b>102</b> may also include a screen <b>114</b> that, in some implementations, can be a touch screen. The illustrative touch screen <b>114</b> can allow a user to interact with the graphical components by translating screen presses into corresponding GUI actions. For example, dragging an input device (e.g, a finger, stylus, etc.) across the touch screen can cause the pointing device to mimic the movement of the input device. Similarly, tapping the screen may cause a right-click or other action to occur at the location of the tap.</p>
<p id="p-0027" num="0026">The illustrative touch screen <b>114</b> may be a resistive panel comprised of several layers. Such a resistive screen may be relatively inexpensive and may be unaffected by outside elements such as dirt and dust. In other implementations, the screen <b>114</b> may be a capacitive screen as such screens may also be largely immune to outside elements and have high clarity. Other implementations may use surface acoustic wave technology or other touch screen technology.</p>
<p id="p-0028" num="0027">The illustrative touch screen <b>114</b> may be surrounded by a bezel <b>112</b>. In some implementations, the bezel may rotate 360 degrees in a clockwise or counterclockwise direction. Both the bezel <b>112</b> and the screen <b>114</b> may be supported by a frame <b>106</b>, which may be connected to the band <b>104</b>.</p>
<p id="p-0029" num="0028">In addition to allowing a user to strap the device <b>102</b> to his body, in some implementations the band <b>104</b> may include one or more sensors that can be used to provide input to the GUI. For example, in some implementations a user can move the GUI's pointing device by pressing his finger on the strap and dragging his finger along the strap in the desired direction. In another illustrative example, the user may right-click on an item in the GUI by dragging the pointing device to the desired application as described above and pressing into the strap for a predetermined amount of time. In another illustrative example, the computing device <b>102</b> may be a portable music player strapped to the user's upper arm. In the illustrative example, the user may press the strap to skip to the next track or slide his finger back and forth on the strap to fast forward or rewind the currently playing track.</p>
<p id="p-0030" num="0029">In such situations, the inputs may be sensed by a flexible area of the band <b>104</b> that includes a capacitive touch sensor or other similar sensor. Inputs of a user pushing in the middle of the sensor and the rolling their fingertip up, down, right, or left slight may change the capacitance in the sensor in measurable ways, so that such slight movements may be interpreted as gestures to move a pointer or cursor on a GUI.</p>
<p id="p-0031" num="0030">The illustrative device <b>102</b> may also include a crown <b>110</b> that may be attached to the frame <b>106</b>. The crown <b>110</b> may allow the user to adjust the display of the computing device <b>102</b>. For example, a user may want the computing device to display the time. In some implementations, by pressing the crown <b>110</b>, the user may toggle the computing device interface between a display comprising a dial having hour, minute, and second hands and the computing device's GUI.</p>
<p id="p-0032" num="0031">In some implementations, the crown <b>110</b> may extend out from the device <b>102</b> and swivel about its axis (e.g., when it has been pulled out from the device <b>102</b> body, much like pulling a stem out in order to switch from a winding mode to a time setting mode on a traditional analog mechanical watch). Sensors at the base of the crown <b>110</b> may be positioned to sense motion of the crown <b>110</b> in various directions, much like what occurs with a joystick input mechanism. Manipulating the crown <b>110</b> may allow the user to control the GUI. For example, a user may extend the crown <b>110</b> and twist it to move between GUI applications. In some implementations, the crown <b>110</b> may allow a user to control the GUI's pointing device. The crown may act as a mouse; e.g., a user may control the pointing device by moving the crown in the desired direction.</p>
<p id="p-0033" num="0032">In other implementations, manipulation of the crown <b>110</b> may control which sensor-emitter pairs are active. For example, in some implementations, sensor-emitter pairs <b>108</b> may be embedded in the frame on the side of the crown <b>110</b> and on the side of the frame opposite the crown, creating two detection areas: the detection area <b>116</b> on the user's forearm and another detection area (not shown) projected on the back of the user's hand. In the illustrative example, the user may use the crown <b>110</b> to determine which of the detection areas are active. For example, the user may press the crown once to activate the left detection area <b>116</b> or twice to activate a right detection area. In other implementations, twisting the crown <b>110</b> a specified number of full revolutions can similarly turn the detection areas on and off.</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 1B</figref> shows an illustrative placement of sensor-emitter pairs <b>108</b> on a gesture-based small input device <b>102</b>. As shown in <figref idref="DRAWINGS">FIG. 1B</figref>, in some implementations, the sensor-emitter pairs <b>108</b> may be vertically offset from one another, which may allow the sensor-emitter pairs to detect tapping motions in the detection area. For example, by determining when each sensor-emitter pair detects an object in the detection area <b>116</b>, the GUI may be able to determine what type of input was intended, e.g., tap, press, etc. In particular, a higher sensor or higher emitter may trigger before a lower sensor or emitter, thus indicating that a user has moved their finger downward. If the triggering then occurs quickly in the opposite order, the device may determine that the user has performed a tapping action.</p>
<p id="p-0035" num="0034">In some implementations, the sensor-emitter pairs <b>108</b> may be included in the rotatable bezel <b>112</b> located, in the illustrative example, on top of the frame <b>106</b>. This may allow a user to choose where the detection area created by the sensor-emitter pairs <b>108</b> is defined. For example, as noted above, in some implementations the bezel <b>112</b> may rotate 360 degrees in either direction. Because, in the illustrative example, the sensor-emitters <b>108</b> are included in the bezel <b>112</b>, the sensor-emitter pairs <b>108</b> are also rotated when the user rotates the bezel <b>112</b>, allowing the user to adjust the position of the detection area <b>116</b>. This may be useful, for example, when a user moves the computing device from one hand to the other (e.g., to match users who are right-handed or left-handed) because the user can adjust the bezel <b>112</b> so that the detection area <b>116</b> is placed on his forearm.</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 2</figref> shows a block diagram <b>200</b> of an illustrative gesture-based small input device. The device <b>202</b> may include multiple sensor-emitter pairs <b>204</b>; although two sensor-emitter pairs are illustrated, any appropriate number of sensor-emitter pairs may be used. In an illustrative implementation, the sensor-emitter pairs <b>204</b> may each include a LED and a photodiode. The LED may illuminate a detection area adjacent to the computing device <b>204</b>, while the photodiode may detect objects entering the detection area by measuring the amount of light reflected from such objects.</p>
<p id="p-0037" num="0036">In another example, the sensor-emitter pairs <b>204</b> can include a capacitive sensor. Because capacitive sensors can be relatively inexpensive and give high resolution measurements, capacitive sensors can be useful when cost is a factor and there is a small gap between the sensor and potential targets.</p>
<p id="p-0038" num="0037">In some implementations, the sensor-emitter pair <b>204</b> may include an inductive sensor. Because inductive sensors are generally insensitive to certain contaminants (e.g., water, oil, and dirt) and able to withstand shocks and vibrations, inductive sensors may be useful when the illustrative computing device is subject to less than ideal environmental conditions.</p>
<p id="p-0039" num="0038">In other implementations, the sensor-emitter pair <b>204</b> may include an infrared sensor. Infrared sensors may be relatively inexpensive, and infrared energy may be able to pass through some opaque materials; thus, infrared sensors may be advantageous when cost is a factor or when the sensor is covered by an opaque material, e.g., an opaque protective cover.</p>
<p id="p-0040" num="0039">In an alternative implementation, the sensor-emitter pair <b>204</b> may include a camera. For example, the camera can be a low-cost camera such as charge-coupled device (&#x201c;CCD&#x201d;) or a complementary metal-oxide (&#x201c;CMOS&#x201d;) camera. A camera may allow the computing device <b>202</b> to recognize more complex inputs. For example, the camera may capture images of multi-finger inputs that a processor may later convert into GUI actions.</p>
<p id="p-0041" num="0040">In some implementations, various sensing technologies may be used in combination with one another. For example, the sensor-emitter pair <b>204</b> may include an ultrasound device as well as a camera or photodiode. Combining sensing technologies in this manner may allow the device <b>202</b> to obtain better readings under non-ideal conditions. For example, in low-light situations, the ultrasound device may augment (or, if appropriate, replace) the reading produced by another device such as a camera.</p>
<p id="p-0042" num="0041">The device <b>202</b> may include a processor <b>206</b> that executes instructions stored in memory <b>218</b>. The processor <b>206</b> may comprise multiple processors responsible for coordinating interactions among other device components and communications over an I/O interface <b>222</b>. The processor <b>206</b> may be responsible for managing internal alerts generated by the device <b>202</b>. For example, an accelerometer <b>210</b> may alert the processor <b>206</b> when the accelerometer <b>210</b> detects movement. The processor <b>206</b> may determine which applications running on the device <b>202</b> are to be notified about such an activity.</p>
<p id="p-0043" num="0042">A display manager <b>208</b> is provided to supervise and coordinate information to be shown on a touch screen display <b>203</b>. The display manager <b>208</b>, for example, may be provided with data relating to information to be displayed and may coordinate data received from various different applications or modules. As one example, display manager <b>208</b> may receive data for overlapping windows on a windowed display and may determine which window is to be on top and where the lower window or windows is to be cut.</p>
<p id="p-0044" num="0043">An accelerometer <b>210</b> may be provided for use in a variety of manners. For example, the accelerometer may be used to determine the state the machine is currently experiencing, so as to provide a proper display to the user. For example, if the accelerometer <b>210</b> indicates that the device <b>202</b> has not been moved for a substantial period of time, the device may display the current time instead of the GUI or shut parts of the device down to conserve power. In some implementations, movements detected by the accelerometer <b>210</b> may used to generate right-clicks or taps. The accelerometer can also be used to recognize gestures and body movements (e.g., a virtual pedometer).</p>
<p id="p-0045" num="0044">The accelerometer <b>210</b> may cooperate with other input devices to enhance the device's <b>202</b> user interface. For example, input from the accelerometer <b>210</b> may be used in conjunction with gesture-based input to control some applications. For example, in a web browsing application a user may center a web page on a particular part of the page by pressing and holding a spot in the detection area. Then as the user moves the device <b>202</b>, the accelerometer <b>210</b> may provide movement data to the browsing application, allowing the browser to shift the current view of the page such that the selected point is still in view. This may be useful, for example, in a mapping application where a user wants to see the area surrounding a point in the map without navigating away from the point of interest.</p>
<p id="p-0046" num="0045">Device inputs such as presses on the touch screen <b>203</b> and inputs detected by the sensor-emitter pairs may be processed by an input manager <b>214</b>. For example, the input manager <b>214</b> may receive information regarding input provided by a user on touch screen <b>203</b>, and forward such information to various applications or modules. For example, the input manager <b>214</b> may cooperate with the display manager <b>208</b> so as to understand what onscreen elements a user is selecting when they press on the touch screen <b>203</b>.</p>
<p id="p-0047" num="0046">The input manager <b>214</b> may also process gesture-based inputs detected by the sensor-emitter pairs <b>204</b>. For example, the input manager <b>214</b> may determine where in the detection area an input occurred and translate the detected input into a GUI action. The input manager <b>214</b> may also help coordinate movements detected by the accelerometer <b>210</b> or inputs to a sensor incorporated into a strap or watch band <b>205</b>.</p>
<p id="p-0048" num="0047">The device may also include one or more power sources <b>212</b>. The illustrative power source may, in some implementations, be either a disposable or rechargeable battery. For example, the battery may be one of myriad types of disposable batteries: zinc-carbon, zinc-chloride, alkaline, silver-oxide, lithium-thionyl chloride, mercury, zinc-air, thermal, water-activated, nickel-oxyhydroxide, etc. In some implementations, the power source <b>212</b> may be a type of rechargeable battery such as nickel-cadmium, nickel-metal hydride, lithium ion, lithium ion polymer, rechargeable alkaline battery, or types of rechargeable batteries. In some cases, the device may contain one or more solar cells that can be used to recharge the battery. In some implementations, the one or more solar cells may be placed in the face of the device.</p>
<p id="p-0049" num="0048">In some implementations, the power source <b>212</b> may comprise an AC/DC power supply that can convert an external AC signal to a DC voltage signal that meets the operating requirements of the device components. In some implementations, the power source <b>212</b> may be charged through either contact or non-contact charging.</p>
<p id="p-0050" num="0049">The device also includes memory <b>218</b>. The memory <b>218</b> may comprise random access memory where computer instructions and data are stored in a volatile memory device for execution by the processor <b>206</b>. The memory <b>218</b> may also include read-only memory where invariant low-level systems code or data for basic system functions such as basic input and output, and startup instructions reside. In addition, the memory <b>218</b> may include other suitable types of memory such as programmable read-only memory, erasable programmable read-only memory, electrically erasable programmable read-only memory, hard disks, and removable memory such as microSD cards or Flash memory.</p>
<p id="p-0051" num="0050">Memory <b>218</b> may store commands or objects for a collection of actions that correspond to inputs detected by the sensor-emitter pairs <b>204</b>. For example, memory <b>218</b> may store a collection of object images and corresponding actions. In some implementations, when an image is captured by the sensor-emitter pairs <b>204</b>, objects in the image may be compared with the collection of object images stored in memory. If a match is found, the device <b>202</b> may perform the action (or actions) associated with the object.</p>
<p id="p-0052" num="0051">A variety of applications <b>220</b> may operate, generally on a common microprocessor <b>206</b>, on the device <b>202</b>. The applications <b>220</b> may take a variety of forms, such as mapping applications, e-mail and other messaging applications, web browser applications, and various applications running within a web browser or running extensions of a web browser.</p>
<p id="p-0053" num="0052">The device <b>202</b> may communicate with other devices through an input/output interface <b>222</b>. The interface <b>222</b> may manage communications with a wireless network. The interface <b>222</b> may provide for communication by the device <b>202</b> with messaging services such as text messaging, e-mail, and telephone voice mail messaging. In addition, the interface <b>222</b> may support downloads and uploads of content and computer code over a wireless network. The interface <b>222</b> may provide for voice communications in a wireless network in a familiar manner.</p>
<p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. 3</figref> shows a conceptual diagram <b>300</b> illustrating the calculation of X-Y coordinates for a detected object. Such calculation may be made in determining the position of an object next to a computing device, so as show corresponding motion of a pointer or other item in a GUI on the device.</p>
<p id="p-0055" num="0054">In the figure, a section of a periphery of a wearable computing device <b>302</b> is shown, which in the illustrative example, includes two sensor-emitter pairs <b>304</b>, <b>306</b>. In the illustrative diagram <b>300</b>, a user has placed an object <b>308</b> in a detection area <b>310</b> created by the sensor-emitter pairs <b>304</b>, <b>306</b>. The sensor-emitter pairs <b>304</b>, <b>306</b> may calculate their respective distances from the object <b>308</b> by measuring the amount of light reflected from the object. For example, in some implementations, the sensor-emitter pairs <b>304</b>, <b>306</b> may comprise an LED and photodiode. The LED may create a detection area (area <b>310</b> in the current example) by transmitting light into the area adjacent to the device <b>302</b>. This area may be monitored by the photodiode. When the photodiode notices a change above a predetermined threshold in the amount of light detected, the photodiode may transmit a signal to the device indicating that an object is in the detection area. The intensity of the reflected light may indicate the distance of the object from the sensor-emitter pairs <b>304</b>, <b>306</b>.</p>
<p id="p-0056" num="0055">Once the distance between each of the sensor-emitter pairs <b>304</b>, <b>306</b> and the object <b>308</b> has been determined, the X-Y coordinates of the object (X<sub>2</sub>, Y<sub>2</sub>) can be computed. In some implementations, the X-Y coordinates of the upper sensor <b>304</b> and the lower sensor <b>306</b> are known and may be stored as constants in the memory of the device <b>302</b> along with the distances between the object and the sensor <b>304</b>, <b>306</b>, Z<sub>0 </sub>and Z<sub>1</sub>, respectively. The coordinates of the sensors <b>304</b>, <b>306</b> and the distances between the sensors <b>304</b>, <b>306</b> and the object <b>308</b>, may be used to calculate the X-Y coordinates of the object <b>308</b>. For example, the X-Y coordinates of the upper sensor <b>304</b> may be (0,0), while the coordinates of the lower sensor <b>306</b> may be (0, Y<sub>1</sub>), where Y<sub>1 </sub>is a known constant. Applying the Pythagorean theorem to the triangle formed by the sensors <b>304</b>, <b>306</b> and the object <b>308</b> leads to following set of equations:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>Z</i><sub>0</sub><sup>2</sup><i>=X</i><sub>2</sub><sup>2</sup><i>+Y</i><sub>2</sub><sup>2</sup>&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>Z</i><sub>1</sub><sup>2</sup><i>=X</i><sub>2</sub><sup>2</sup>+(<i>Y</i><sub>2</sub><i>&#x2212;Y</i><sub>1</sub>)<sup>2</sup>.&#x2003;&#x2003;(2)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0057" num="0056">Values for the known constants (Z<sub>0</sub>, Z<sub>1</sub>, and Y<sub>1</sub>) may be substituted into these equations and the equations may be solved for the coordinates of the object <b>308</b>, X<sub>2 </sub>and Y<sub>2</sub>.</p>
<p id="p-0058" num="0057"><figref idref="DRAWINGS">FIGS. 4 and 5</figref> are flow charts of example processes <b>400</b> and <b>500</b> for gesture-based device input. The processes <b>400</b> and <b>500</b> may be performed, for example, by a system such as the system <b>200</b> and, for clarity of presentation, the description that follows uses the system <b>200</b> and the conceptual diagram <b>100</b> as the basis of an example for describing the processes. However, another system, or combination of systems, may be used to perform the processes <b>400</b> and <b>500</b>.</p>
<p id="p-0059" num="0058">Referring to <figref idref="DRAWINGS">FIG. 4</figref>, the illustrative gesture-based input process <b>400</b> begins at step <b>405</b> where the sensors are calibrated. A baseline intensity level of detected light may be obtained by measuring the amount of light observed by the sensor-emitter pairs <b>204</b> under ordinary operating conditions. For example, the device may be placed outdoors and then the device can measure the amount of light detected by the sensor-emitter pairs <b>204</b>. The amount of light measured can be used in future comparisons to determine whether the amount of light detected as changed, which may be an indication that an object has entered a detection area. In some implementations, the device <b>202</b> may periodically re-calibrate itself to adjust to changing operating conditions.</p>
<p id="p-0060" num="0059">The method may proceed to step <b>410</b> where the device <b>202</b> determines whether an object has been detected. For example, the sensor-emitter pairs <b>204</b> may measure the amount of light detected at regular intervals and compare the amount of detected light with the baseline light intensity level. If the amount of detected light has changed more than a specified amount from the baseline, the sensor-emitter pairs <b>204</b> may transmit a signal to the device's processor <b>206</b> indicating that an object has entered the detection area. If not, step <b>410</b> is repeated.</p>
<p id="p-0061" num="0060">Once an object has been detected, the distance from the sensor-emitter pairs <b>204</b> to the object may be calculated at step <b>415</b>. For example, the intensity of the reflected light may be used to estimate the distance between each sensor-emitter pair <b>204</b> and the object. Knowing these distances, the X-Y coordinates of the object may be computed at step <b>420</b>. As noted above in regards to <figref idref="DRAWINGS">FIG. 3</figref>, the distances determined at step <b>415</b>, along with the coordinates of the sensor-emitter pairs <b>204</b> may be used to solve equations (1) and (2) above for the X-Y coordinates of the object.</p>
<p id="p-0062" num="0061">The method may then proceed to step <b>425</b> where a GUI action is performed. For example, the pointing device of the GUI may be moved to the X-Y coordinates of the detected object. In some implementations, if the object has been detected at the same location for a predetermined amount of time, a right-click or similar action may be performed at the X-Y coordinates in the GUI that correspond to the location of the object.</p>
<p id="p-0063" num="0062">In implementations where the sensor-emitter pairs <b>204</b> are offset from one another the device may execute a tap command at the calculated X-Y coordinates. For example, if a lower sensor detects an object before an upper sensor does, the sensor-emitter pairs may transmit a signal to the device's processor <b>206</b> indicating that a tap has been performed. In response, the GUI may execute a tap command at the calculated coordinates.</p>
<p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. 5</figref> illustrates an alternative gesture-based input process <b>500</b>. Like the process <b>400</b> described above, process <b>500</b> begins by calibrating the sensor-emitter pairs <b>204</b> at step <b>505</b> and then enters an object-detection loop at step <b>510</b>. When an object is detected at step <b>510</b>, the process proceeds to step <b>515</b> where an image is captured. For example, in some implementations the sensor-emitter pairs <b>204</b> may include a CCD camera. When an object is detected at step <b>510</b>, a signal may be transmitted to the CCD camera and in response the camera may capture an image of the object.</p>
<p id="p-0065" num="0064">After the image is captured, the device <b>202</b> may process the image. For instance, at step <b>520</b> the background of the image may be removed. In some implementations, objects in the field of view of the camera that are relatively stationary may be removed. For example, the current image may be compared with a baseline image or an image taken shortly before the current image. Objects that are constant between the pictures may be removed, leaving the object of interest.</p>
<p id="p-0066" num="0065">Having removed the background, the process proceeds to step <b>525</b> where the resulting image is compared to known objects. For example, the image may be compared with images of objects stored in memory <b>218</b>. If a match is found, an action associated with the image may be performed at step <b>530</b>. For example, placing a coin in the detection area may result in the device <b>202</b> executing a calculator program. In some implementations, if the object does not have an associated image, an action may be dynamically associated with the object. For example, the user may be presented with a list of possible actions when an unidentified object is detected. Selecting an action may associate that action with the object.</p>
<p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. 6</figref> shows an example of a generic computer device <b>600</b> and a generic mobile computer device <b>650</b>, which may be used with the techniques described here. Computing device <b>600</b> is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. Computing device <b>650</b> is intended to represent various forms of mobile devices, such as personal digital assistants, cellular telephones, smartphones, and other similar computing devices. The components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document.</p>
<p id="p-0068" num="0067">Computing device <b>600</b> includes a processor <b>602</b>, memory <b>604</b>, a storage device <b>606</b>, a high-speed interface <b>608</b> connecting to memory <b>604</b> and high-speed expansion ports <b>610</b>, and a low speed interface <b>612</b> connecting to low speed bus <b>614</b> and storage device <b>606</b>. Each of the components <b>602</b>, <b>604</b>, <b>606</b>, <b>608</b>, <b>610</b>, and <b>612</b>, are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate. The processor <b>602</b> can process instructions for execution within the computing device <b>600</b>, including instructions stored in the memory <b>604</b> or on the storage device <b>606</b> to display graphical information for a GUI on an external input/output device, such as display <b>616</b> coupled to high speed interface <b>608</b>. In other implementations, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory. Also, multiple computing devices <b>600</b> may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system).</p>
<p id="p-0069" num="0068">The memory <b>604</b> stores information within the computing device <b>600</b>. In one implementation, the memory <b>604</b> is a volatile memory unit or units. In another implementation, the memory <b>604</b> is a non-volatile memory unit or units. The memory <b>604</b> may also be another form of computer-readable medium, such as a magnetic or optical disk.</p>
<p id="p-0070" num="0069">The storage device <b>606</b> is capable of providing mass storage for the computing device <b>600</b>. In one implementation, the storage device <b>606</b> may be or contain a computer-readable medium, such as a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. A computer program product can be tangibly embodied in an information carrier. The computer program product may also contain instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer- or machine-readable medium, such as the memory <b>604</b>, the storage device <b>606</b>, memory on processor <b>602</b>, or a propagated signal.</p>
<p id="p-0071" num="0070">The high speed controller <b>608</b> manages bandwidth-intensive operations for the computing device <b>600</b>, while the low speed controller <b>612</b> manages lower bandwidth-intensive operations. Such allocation of functions is exemplary only. In one implementation, the high-speed controller <b>608</b> is coupled to memory <b>604</b>, display <b>616</b> (e.g., through a graphics processor or accelerator), and to high-speed expansion ports <b>610</b>, which may accept various expansion cards (not shown). In the implementation, low-speed controller <b>612</b> is coupled to storage device <b>606</b> and low-speed expansion port <b>614</b>. The low-speed expansion port, which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet) may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.</p>
<p id="p-0072" num="0071">The computing device <b>600</b> may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a standard server <b>620</b>, or multiple times in a group of such servers. It may also be implemented as part of a rack server system <b>624</b>. In addition, it may be implemented in a personal computer such as a laptop computer <b>622</b>. Alternatively, components from computing device <b>600</b> may be combined with other components in a mobile device (not shown), such as device <b>650</b>. Each of such devices may contain one or more of computing device <b>600</b>, <b>650</b>, and an entire system may be made up of multiple computing devices <b>600</b>, <b>650</b> communicating with each other.</p>
<p id="p-0073" num="0072">Computing device <b>650</b> includes a processor <b>652</b>, memory <b>664</b>, an input/output device such as a display <b>654</b>, a communication interface <b>666</b>, and a transceiver <b>668</b>, among other components. The device <b>650</b> may also be provided with a storage device, such as a microdrive or other device, to provide additional storage. Each of the components <b>650</b>, <b>652</b>, <b>664</b>, <b>654</b>, <b>666</b>, and <b>668</b>, are interconnected using various buses, and several of the components may be mounted on a common motherboard or in other manners as appropriate.</p>
<p id="p-0074" num="0073">The processor <b>652</b> can execute instructions within the computing device <b>650</b>, including instructions stored in the memory <b>664</b>. The processor may be implemented as a chipset of chips that include separate and multiple analog and digital processors. The processor may provide, for example, for coordination of the other components of the device <b>650</b>, such as control of user interfaces, applications run by device <b>650</b>, and wireless communication by device <b>650</b>.</p>
<p id="p-0075" num="0074">Processor <b>652</b> may communicate with a user through control interface <b>658</b> and display interface <b>656</b> coupled to a display <b>654</b>. The display <b>654</b> may be, for example, a TFT LCD (Thin-Film-Transistor Liquid Crystal Display) or an OLED (Organic Light Emitting Diode) display, or other appropriate display technology. The display interface <b>656</b> may comprise appropriate circuitry for driving the display <b>654</b> to present graphical and other information to a user. The control interface <b>658</b> may receive commands from a user and convert them for submission to the processor <b>652</b>. In addition, an external interface <b>662</b> may be provide in communication with processor <b>652</b>, so as to enable near area communication of device <b>650</b> with other devices. External interface <b>662</b> may provide, for example, for wired communication in some implementations, or for wireless communication in other implementations, and multiple interfaces may also be used.</p>
<p id="p-0076" num="0075">The memory <b>664</b> stores information within the computing device <b>650</b>. The memory <b>664</b> can be implemented as one or more of a computer-readable medium or media, a volatile memory unit or units, or a non-volatile memory unit or units. Expansion memory <b>674</b> may also be provided and connected to device <b>650</b> through expansion interface <b>672</b>, which may include, for example, a SIMM (Single In Line Memory Module) card interface. Such expansion memory <b>674</b> may provide extra storage space for device <b>650</b>, or may also store applications or other information for device <b>650</b>. Specifically, expansion memory <b>674</b> may include instructions to carry out or supplement the processes described above, and may include secure information also. Thus, for example, expansion memory <b>674</b> may be provide as a security module for device <b>650</b>, and may be programmed with instructions that permit secure use of device <b>650</b>. In addition, secure applications may be provided via the SIMM cards, along with additional information, such as placing identifying information on the SIMM card in a non-hackable manner.</p>
<p id="p-0077" num="0076">The memory may include, for example, flash memory and/or NVRAM memory, as discussed below. In one implementation, a computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer- or machine-readable medium, such as the memory <b>664</b>, expansion memory <b>674</b>, memory on processor <b>652</b>, or a propagated signal that may be received, for example, over transceiver <b>668</b> or external interface <b>662</b>.</p>
<p id="p-0078" num="0077">Device <b>650</b> may communicate wirelessly through communication interface <b>666</b>, which may include digital signal processing circuitry where necessary. Communication interface <b>666</b> may provide for communications under various modes or protocols, such as GSM voice calls, SMS, EMS, or MMS messaging, CDMA, TDMA, PDC, WCDMA, CDMA2000, or GPRS, among others. Such communication may occur, for example, through radio-frequency transceiver <b>668</b>. In addition, short-range communication may occur, such as using a Bluetooth, WiFi, or other such transceiver (not shown). In addition, GPS (Global Positioning System) receiver module <b>670</b> may provide additional navigation- and location-related wireless data to device <b>650</b>, which may be used as appropriate by applications running on device <b>650</b>.</p>
<p id="p-0079" num="0078">Device <b>650</b> may also communicate audibly using audio codec <b>660</b>, which may receive spoken information from a user and convert it to usable digital information. Audio codec <b>660</b> may likewise generate audible sound for a user, such as through a speaker, e.g., in a handset of device <b>650</b>. Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.) and may also include sound generated by applications operating on device <b>650</b>.</p>
<p id="p-0080" num="0079">The computing device <b>650</b> may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a cellular telephone <b>680</b>. It may also be implemented as part of a smartphone <b>682</b>, personal digital assistant, watch <b>684</b>, or other similar mobile device.</p>
<p id="p-0081" num="0080">Various implementations of the systems and techniques described here can be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.</p>
<p id="p-0082" num="0081">These computer programs (also known as programs, software, software applications or code) include machine instructions for a programmable processor, and can be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms &#x201c;machine-readable medium&#x201d; &#x201c;computer-readable medium&#x201d; refers to any computer program product, apparatus and/or device (e.g., magnetic discs, optical disks, memory, Programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term &#x201c;machine-readable signal&#x201d; refers to any signal used to provide machine instructions and/or data to a programmable processor.</p>
<p id="p-0083" num="0082">To provide for interaction with a user, the systems and techniques described here can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback (e.g., visual feedback, auditory feedback, or tactile feedback); and input from the user can be received in any form, including acoustic, speech, or tactile input.</p>
<p id="p-0084" num="0083">The systems and techniques described here can be implemented in a computing system that includes a back end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front end component (e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the systems and techniques described here), or any combination of such back end, middleware, or front end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a local area network (&#x201c;LAN&#x201d;), a wide area network (&#x201c;WAN&#x201d;), and the Internet.</p>
<p id="p-0085" num="0084">The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.</p>
<p id="p-0086" num="0085">A number of embodiments have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the invention. For example, much of this document has been described with respect to a wristwatch form factor, but other forms of devices may be addressed.</p>
<p id="p-0087" num="0086">In addition, the logic flows depicted in the figures do not require the particular order shown, or sequential order, to achieve desirable results. In addition, other steps may be provided, or steps may be eliminated, from the described flows, and other components may be added to, or removed from, the described systems. Accordingly, other embodiments are within the scope of the following claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A computer-implemented method, comprising:
<claim-text>receiving, by a processor and from one or more sensors that are included in a body of a watch, an indication of detected input entered in a detection area that is:
<claim-text>(i) on a forearm, wrist, or back of hand of a user that is wearing the watch, and</claim-text>
<claim-text>(ii) in an area that is adjacent to the body of the watch;</claim-text>
</claim-text>
<claim-text>determining, by the processor and based on information obtained from the one or more sensors, that a finger or stylus has contacted the forearm, wrist, or back of hand of the user in the detection area and performed a gesture; and</claim-text>
<claim-text>changing, by the processor, a display of a pointing element in a graphical user interface of the watch to move in coordination with the gesture and the determined contact of the finger or stylus with the forearm, wrist, or back of hand of the user.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining that the finger or stylus has contacted the forearm, wrist, or back of hand of the user in the detection area includes sensing an intensity level of light reflected off the finger or stylus.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein one of the one or more sensors is vertically offset from another of the one or more sensors.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, further comprising using vertical motion to generate a clicking event.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more sensors include at least two light sources and two light sensors.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more sensors are mounted on an exterior portion of the body of the watch.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising generating a clicking event in the graphical user interface of the watch in response to detecting movement of the watch based on input from an accelerometer.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more sensors include a CCD camera.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the watch includes a light emitting diode.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. A computer-implemented method, comprising:
<claim-text>receiving, by a processor and from one or more sensors that are included on a side of a body-mounted accessory, an indication of detected input entered in a detection area that is:
<claim-text>(i) on a forearm, wrist, or back of hand of a user, and</claim-text>
<claim-text>(ii) adjacent to the body-mounted accessory;</claim-text>
</claim-text>
<claim-text>determining, by the processor and based on information obtained from the one or more sensors, that a finger or stylus has contacted the forearm, wrist, or back of hand of the user and performed a gesture in the detection area by sensing light reflected off of the finger or stylus; and</claim-text>
<claim-text>changing, by the processor, a graphical user interface of the body-mounted accessory so that a displayed element moves in coordination with the gesture and the determined contact of the finger or stylus with the forearm, wrist, or back of hand of the user.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the light detector includes a photodiode.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the body-mounted accessory has a light source, and the sensed light is from the light source.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the light source includes a light emitting diode.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the body-mounted accessory is strapped to an arm of the user.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the body-mounted accessory is a watch, a cellular telephone, or a portable media player.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, further comprising superimposing, by the body-mounted accessory, an image onto a portion of the forearm, wrist, or back of hand of the user, the image including a representation of a button;
<claim-text>wherein determining that the finger or stylus has contacted the forearm, wrist, or back of hand of the user includes identifying, by the processor, that the finger or stylus has contacted the forearm, wrist, or back of hand of the user at a location at which the representation of the button is superimposed onto the portion of the forearm, wrist, or back of hand of the user; and</claim-text>
<claim-text>wherein changing the graphical user interface includes indicating on the graphical user interface that the button has been selected.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein determining that the finger or stylus has contacted the forearm, wrist, or back of hand of the user and performed a gesture includes identifying a pinching motion by multiple fingers.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein determining that the finger or stylus has contacted the forearm, wrist, or back of hand of the user and performed a gesture includes calculating a distance from the light detector to the finger or stylus.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The method of <claim-ref idref="CLM-00018">claim 18</claim-ref>, further comprising computing X and Y coordinates of the finger or stylus using the calculated distance.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The method of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein changing the graphical user interface of the body-mounted accessory includes moving a pointing element of the graphical user interface to the X and Y coordinates.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, further comprising determining, by the processor and based on information obtained from the light detector, that the finger or stylus has contacted the forearm, wrist, or back of hand of the user at a same location for a predetermined amount of time, and, as a result, performing a clicking action.</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein determining that the finger or stylus has contacted the forearm, wrist, or back of hand of the user and performed a gesture includes sensing that the finger or stylus has moved with respect to the forearm, wrist, or back of hand of the user.</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the detection area comprises a virtual mouse pad on the forearm, wrist, or back of hand of the user.</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the graphical user interface is presented on a display device of the body-mounted accessory.</claim-text>
</claim>
<claim id="CLM-00025" num="00025">
<claim-text>25. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the detection area does not touch the body-mounted accessory.</claim-text>
</claim>
<claim id="CLM-00026" num="00026">
<claim-text>26. A pointing system, comprising:
<claim-text>a body-mounted device that includes a device body defining an upper face and an outer periphery;</claim-text>
<claim-text>one or more sensors coupled to the body-mounted device that are arranged to sense user inputs entered in a detection area that is (i) on a forearm, wrist, or back of hand of a user that is wearing the body-mounted device, and (ii) adjacent to, but not touching, the device body;</claim-text>
<claim-text>a processor that is: (i) in the body-mounted device, (ii) configured to receive input that is provided by the one or more sensors, and (iii) configured to determine that a finger or stylus has contacted the forearm, wrist, or back of hand of the user and performed a gesture; and</claim-text>
<claim-text>a display that includes a graphical user interface that changes to move a display of an element in response to the processor determining that the finger or stylus has contacted the forearm, wrist, or back of hand of the user and performance of the gesture.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00027" num="00027">
<claim-text>27. The system of <claim-ref idref="CLM-00026">claim 26</claim-ref>, wherein the sensors are mounted in the outer periphery of the device body.</claim-text>
</claim>
<claim id="CLM-00028" num="00028">
<claim-text>28. The system of <claim-ref idref="CLM-00026">claim 26</claim-ref>, wherein one of the one or more sensors is vertically offset from another of the one or more sensors.</claim-text>
</claim>
<claim id="CLM-00029" num="00029">
<claim-text>29. The system of <claim-ref idref="CLM-00028">claim 28</claim-ref>, wherein the processor generates a clicking event as a result of sensing vertical user input.</claim-text>
</claim>
<claim id="CLM-00030" num="00030">
<claim-text>30. The system of <claim-ref idref="CLM-00028">claim 28</claim-ref>, wherein the processor generates a clicking event based on information from an accelerometer.</claim-text>
</claim>
<claim id="CLM-00031" num="00031">
<claim-text>31. The system of <claim-ref idref="CLM-00028">claim 28</claim-ref>, wherein the one or more sensors include at least two light sources and two light sensors.</claim-text>
</claim>
<claim id="CLM-00032" num="00032">
<claim-text>32. A pointing system, comprising:
<claim-text>a watch that has one or more sensors and a watch body that defines an upper face and an outer periphery;</claim-text>
<claim-text>a detection area that is (i) on a forearm, wrist, or back of hand of a user that is wearing the watch, and (ii) adjacent to the watch body;</claim-text>
<claim-text>means for sensing, using the one or more sensors, a location of a finger or stylus that is within the detection area and that has contacted the forearm, wrist, or back of hand of the user; and</claim-text>
<claim-text>one or more computing devices in the watch that are configured to change a location of a pointing element in a graphical user interface of the watch in coordination with a gesture of the finger or stylus in the detection area, the gesture being identified from changes in sensed locations of the finger or stylus.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
