<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08624891-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08624891</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13007968</doc-number>
<date>20110117</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<us-term-of-grant>
<us-term-extension>417</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20110101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>T</subclass>
<main-group>15</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20110101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>T</subclass>
<main-group>15</main-group>
<subgroup>50</subgroup>
<symbol-position>L</symbol-position>
<classification-value>N</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>T</subclass>
<main-group>15</main-group>
<subgroup>60</subgroup>
<symbol-position>L</symbol-position>
<classification-value>N</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>345419</main-classification>
<further-classification>345426</further-classification>
</classification-national>
<invention-title id="d2e53">Iterative reprojection of images</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>6084908</doc-number>
<kind>A</kind>
<name>Chiang et al.</name>
<date>20000700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524003</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6636212</doc-number>
<kind>B1</kind>
<name>Zhu</name>
<date>20031000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345421</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>7254265</doc-number>
<kind>B2</kind>
<name>Naske et al.</name>
<date>20070800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382154</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>7551770</doc-number>
<kind>B2</kind>
<name>Harman</name>
<date>20090600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382154</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>7573475</doc-number>
<kind>B2</kind>
<name>Sullivan et al.</name>
<date>20090800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345427</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>8180145</doc-number>
<kind>B2</kind>
<name>Wu et al.</name>
<date>20120500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382154</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2011/0002532</doc-number>
<kind>A1</kind>
<name>Frakes et al.</name>
<date>20110100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382154</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2012/0176368</doc-number>
<kind>A1</kind>
<name>Genova</name>
<date>20120700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345419</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2012/0176473</doc-number>
<kind>A1</kind>
<name>Genova et al.</name>
<date>20120700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348 46</main-classification></classification-national>
</us-citation>
<us-citation>
<nplcit num="00010">
<othercit>Huw Bowles, &#x201c;Efficient Real-Time Stereoscopic3D Rendering&#x201d;, MS Thesis, Aug. 2010.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00011">
<othercit>Chen M., Lu W., Chen Q., Ruchala K. J., Oliverag. H.: A simple fixed-point approach to invert a deformation field. Medical Physics 35, 1 (2008), 81-88. 2, 6.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00012">
<othercit>Leonard McMillan Jr., &#x201c;An Image-Based Approach to Three-Dimensional Computer Graphics&#x201d;, dissertation of Ph.D, University of North Carolina at Chapel Hill.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00013">
<othercit>S.Zinger, L.Do, Y.Gao, P.H.N.de With,&#x201c; Conversion of free-viewpoint 3DTV signals for stereo displays&#x201d;, Multimedia and Expo (ICME), 2010 IEEE International Conference on Jul. 19-23, 2010.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00014">
<othercit>Chih-Cheng Wei, Chin-Hsing Chen, Jin-Yuan Wang, &#x201c;A Closed-Form Solution for Image Warping of Mesh in Quad-Tree Representation&#x201d;, 2007 International Conference on Advanced Information Technologies (AIT).</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00015">
<othercit>N. Herodotou, A. N. Venetsanopoulos, &#x201c;Temporal prediction of video sequences using an image warping technique based on color segmentation&#x201d;, Image Analysis and Processing Lecture Notes in Computer Science vol. 1310, 1997, pp. 494-501.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00016">
<othercit>Marcato R.: Optimizing an Inverse Warper. Masters thesis, Massachusetts Institute of Technology, 1998.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00017">
<othercit>Akenine-Moller, et al., &#x201c;Stochastic Rasterization Using Time-Continuous Triangles&#x201d;., Proceedings of the 22nd ACM SIGGRAPH/EUROGRAPHICS Symposium on Graphics Hardware (2007) pp. 7-16.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00018">
<othercit>Andreev, Dmitry &#x201c;Real-Time Frame Rate Up-Conversion for Video Games or how to get from 30 to 60 fps for free&#x201d;. ACM SIGGRAPH 2010 talks, Jul. 25-29, 2010 1 page.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00019">
<othercit>Chen, et al., &#x201c;View Interpolation for Image Synthesis&#x201d;. Proceedings of the 20th Annual Conference on Computer Graphics and Interactive Techniques, ACM 1993 New York, NY, pp. 279-288.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00020">
<othercit>Cook, et al., &#x201c;Distributed Ray Tracing&#x201d;. ACM SIGGRAPH Computer Graphics vol. 18, No. 3, Jul. 1984 pp. 137-145.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00021">
<othercit>Didyk, et al., &#x201c;Adaptive Image-Space Stereo View Synthesis&#x201d;. Vision, Modeling and Visualization Workshop, 2010 pp. 1-8.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00022">
<othercit>Fatahalian, et al., &#x201c;Reducing Shading on GPUs Using Quad-Fragment Merging&#x201d;. ACM Transactions on Graphics (TOG) 29, 4, Article 67 (Jul. 2010), 8 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00023">
<othercit>Marcato, Jr., Robert W., &#x201c;Optimizing an Inverse Warper&#x201d;. MIT, May 22, 1998 51 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00024">
<othercit>Mark, et al., &#x201c;Post-Rendering 3D Warping&#x201d;. Proceedings of the 1997 Symposium on Interactive 3D Graphics&#x2014;SI3D '97, ACM Press, New York, NY No., Figure 2, 7-ff, 10 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00025">
<othercit>McGuire, et al., &#x201c;Real-Tim Stochastic Rasterization on Conventional GPU Architectures&#x201d;., High Performance Graphics 2010, The Eurographics Association, 10 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00026">
<othercit>McMillan, Jr., Leonard, &#x201c;An Image-Based Approach to Three-Dimensional Computer Graphics&#x201d;., A Dissertation, Dept. of Computer Science, University of North Carolina (Chapel Hill), 1997, 209 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00027">
<othercit>Nehab, et al., &#x201c;Accelerating Real-Time Shading with Reverse Reporjection Caching&#x201d;. Proceedings of the 22nd ACM SIGGRAPH/EUROGRAPCICS Symosium on Graphics Hardware, 2007, 11 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00028">
<othercit>Reshetov, Alexander, &#x201c;Morphological Antialiasing&#x201d;., Proceedings of the 1st ACM Conference on High Performance Graphics&#x2014;HPG, '09, 8 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00029">
<othercit>Schobel, et al., &#x201c;No Title&#x201d;. AAA Stereo-3D in CryENGINE 3 in GDC10, 33 pages. http://www.crytek.com/cryengine/presentations/aaa-stereo-d3-in-cryengine-3.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00030">
<othercit>Torborg, et al., &#x201c;Talisman: Commodity Realtime 3D Graphics for the PC&#x201d;. ACM 1996, Annual Conference on Computer Graphics, 11 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00031">
<othercit>Walter, et al., &#x201c;Interactive Rendering Using the Render Cache&#x201d;. Proceedings of the 10th Eurographics Workshop on rendering, 10, 1999, pp. 235-246.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00032">
<othercit>Walter, et al., &#x201c;Enhancing and Optimizing the Render Cache&#x201d;. Proceedings of the 13th Eurographics Workshop on Rendering (2002) pp. 37-43.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00033">
<othercit>Zwicker, et al., &#x201c;A Survey and Classification of Real Time Rendering Methods&#x201d;. Mitsubishi Electric Research Laboratories, Cambridge Research Center. Mar. 2000, 40 pages. http://www.merl.com/papers/tr2000-09/(last accessed Apr. 2004).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>29</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>345419</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345426</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>14</number-of-drawing-sheets>
<number-of-figures>17</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120182299</doc-number>
<kind>A1</kind>
<date>20120719</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Bowles</last-name>
<first-name>Huw</first-name>
<address>
<city>Brighton</city>
<country>GB</country>
</address>
</addressbook>
<residence>
<country>GB</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Mitchell</last-name>
<first-name>Kenny</first-name>
<address>
<city>Addlestone</city>
<country>GB</country>
</address>
</addressbook>
<residence>
<country>GB</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Sumner</last-name>
<first-name>Robert</first-name>
<address>
<city>Zurich</city>
<country>CH</country>
</address>
</addressbook>
<residence>
<country>CH</country>
</residence>
</us-applicant>
<us-applicant sequence="004" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Jarosz</last-name>
<first-name>Wojciech</first-name>
<address>
<city>Zurich</city>
<country>CH</country>
</address>
</addressbook>
<residence>
<country>CH</country>
</residence>
</us-applicant>
<us-applicant sequence="005" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Gross</last-name>
<first-name>Markus</first-name>
<address>
<city>Uster</city>
<country>CH</country>
</address>
</addressbook>
<residence>
<country>CH</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Bowles</last-name>
<first-name>Huw</first-name>
<address>
<city>Brighton</city>
<country>GB</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Mitchell</last-name>
<first-name>Kenny</first-name>
<address>
<city>Addlestone</city>
<country>GB</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Sumner</last-name>
<first-name>Robert</first-name>
<address>
<city>Zurich</city>
<country>CH</country>
</address>
</addressbook>
</inventor>
<inventor sequence="004" designation="us-only">
<addressbook>
<last-name>Jarosz</last-name>
<first-name>Wojciech</first-name>
<address>
<city>Zurich</city>
<country>CH</country>
</address>
</addressbook>
</inventor>
<inventor sequence="005" designation="us-only">
<addressbook>
<last-name>Gross</last-name>
<first-name>Markus</first-name>
<address>
<city>Uster</city>
<country>CH</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Patterson &#x26; Sheridan, LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Disney Enterprises, Inc.</orgname>
<role>02</role>
<address>
<city>Burbank</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Yang</last-name>
<first-name>Ryan R</first-name>
<department>2679</department>
</primary-examiner>
<assistant-examiner>
<last-name>Ge</last-name>
<first-name>Jin</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Techniques are disclosed for performing image space reprojection iteratively. An insignificant parallax threshold depth is computed for a source image. Portions of the image having depth values greater than the insignificant parallax threshold depth may be shifted uniformly to produce corresponding portions of the reprojection (target) image. An iterative fixed-point reprojection algorithm is used to reproject the portions of the source image having depth values less than or equal to the insignificant parallax threshold depth. The fixed point reprojection algorithm quickly converges on the best pixel in the source image for each pixel in a target image representing an offset view of the source image. An additional rendering pass is employed to fill disoccluded regions of the target image, where the reprojection algorithm fails to converge.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="154.43mm" wi="215.90mm" file="US08624891-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="223.18mm" wi="156.89mm" file="US08624891-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="148.00mm" wi="130.98mm" file="US08624891-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="173.48mm" wi="163.75mm" file="US08624891-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="176.70mm" wi="162.98mm" file="US08624891-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="236.90mm" wi="148.93mm" orientation="landscape" file="US08624891-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="241.38mm" wi="135.47mm" file="US08624891-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="239.35mm" wi="156.89mm" file="US08624891-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="240.11mm" wi="154.43mm" file="US08624891-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="219.12mm" wi="145.12mm" file="US08624891-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="236.98mm" wi="155.28mm" file="US08624891-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="235.71mm" wi="180.34mm" file="US08624891-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="247.06mm" wi="140.72mm" file="US08624891-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="234.10mm" wi="165.35mm" file="US08624891-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="251.88mm" wi="163.75mm" file="US08624891-20140107-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND</heading>
<p id="p-0002" num="0001">1. Field of the Invention</p>
<p id="p-0003" num="0002">Embodiments of the present invention relate to the field of computer graphics and, in particular, to the reprojection of image data.</p>
<p id="p-0004" num="0003">2. Description of the Related Art</p>
<p id="p-0005" num="0004">In order to generate a second image based on a first image that is viewed from a different position, the first image may be reprojected in image space using an image offset. Image space reprojection may be used to produce stereoscopic image pairs, where a first image in each pair is viewed by the left eye and the second image in each pair is viewed by the right eye to produce images that appear to the viewer to be three-dimensional. An alternative to image space reprojection would be to render both images in the stereographic pair. However, rendering both images in real-time is not typically possible for graphics applications that require high-performance processing to render a single image in real-time.</p>
<p id="p-0006" num="0005">More recently, image space reprojection algorithms known as depth-image-based rendering (DIBR) have been developed. However, these image space reprojection computations are quite complex and time-consuming to execute and have not been successfully performed in real-time during the rendering of three-dimensional content. Additionally, the DIBR techniques are unable to properly handle disocclusion regions where a portion of a background in the rendered scene is revealed (unoccluded) in the reprojected image.</p>
<heading id="h-0002" level="1">SUMMARY</heading>
<p id="p-0007" num="0006">Embodiments of the invention provide techniques for performing image space reprojection iteratively. An insignificant parallax threshold depth is computed for an image. Portions of the image having depth values greater than the insignificant parallax threshold depth may be shifted uniformly to produce corresponding portions of the reprojection (target) image. An iterative fixed-point reprojection algorithm is used to reproject the portions of the source image having depth values less than or equal to the insignificant parallax threshold depth. The fixed point reprojection algorithm converges on the best pixel in the source image for each pixel in a target image representing an offset view of the source image. An additional rendering pass is employed to fill disoccluded regions of the target image where the reprojection algorithm fails to converge. In addition to generating stereoscopic pairs in real-time, the iterative fixed-point reprojection algorithm may also be applied to other near view-computation problems, such as depth of field, motion blur, and multiview point rendering (e.g., for auto-stereoscopic displays). The iterative fixed-point reprojection algorithm may be implemented in a single reprojection pass and for application in other areas of spatio-temporal rendering, such as 30-60 Hz conversion for split/second and lightfield rendering. Additionally, one or more reprojections may be combined to produce an image that is reprojected in time and/or image space.</p>
<p id="p-0008" num="0007">One embodiment of the invention includes a method for reprojecting a source image. This method may generally include receiving the source image and receiving difference information representing a difference between the source image and a target image that is a reprojection of the source image. For at least one location in the target image, a starting point in the source image determined based on the difference information. For the starting point, a processor iterates a function to generate target data for a respective location in the target image.</p>
<p id="p-0009" num="0008">One embodiment of the invention includes a method of reprojecting a source image using guidance data. This method may generally include receiving source image data and receiving difference information representing an offset between the source image and a target image that is a reprojection of the source image. The source image is mapped into a set of bounding areas that represent the guidance data. A processor computes a convergence term based on the difference information for at least one of the bounding areas in the set of bounding areas.</p>
<p id="p-0010" num="0009">Other embodiments include, without limitation, a computer-readable medium that includes instructions that enable a processing unit to implement one or more aspects of the disclosed methods as well as a system configured to implement one or more aspects of the disclosed methods.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0011" num="0010">So that the manner in which the above recited features of the invention can be understood in detail, a more particular description of the invention, briefly summarized above, may be had by reference to embodiments, some of which are illustrated in the appended drawings. It is to be noted, however, that the appended drawings illustrate only typical embodiments of this invention and are therefore not to be considered limiting of its scope, for the invention may admit to other equally effective embodiments.</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 1A</figref> is a block diagram of a system configured to implement one or more aspects of the present invention.</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 1B</figref> is a block diagram of the reprojection components of <figref idref="DRAWINGS">FIG. 1A</figref> that are configured to implement one or more aspects of the present invention.</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 2A</figref> illustrates an example of a source image including a source surface that is reprojected to generate a target surface, according to one embodiment of the invention.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 2B</figref> illustrates bounding boxes that are intersected by the target surface of <figref idref="DRAWINGS">FIG. 2A</figref>, according to one embodiment of the invention.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 2C</figref> illustrates a plot of distances after which approximation error is bounded by &#x3b5;, for different values of &#x3b5;, and z<sub>far</sub>, according to one embodiment of the invention.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 3A</figref> is a flowchart of a method steps describing reprojection of a source image to produce a target image, according to one embodiment of the invention.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 3B</figref> is a flowchart of a method step shown in <figref idref="DRAWINGS">FIG. 3A</figref>, according to one embodiment of the invention.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 3C</figref> is another flowchart of the method step shown in <figref idref="DRAWINGS">FIG. 3A</figref>, according to one embodiment of the invention.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 4A</figref> is a diagram illustrating a source image, according to one embodiment of the invention.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 4B</figref> is a diagram illustrating a target image that is a reprojection of the source image shown in <figref idref="DRAWINGS">FIG. 4A</figref>, according to one embodiment of the invention.</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 4C</figref> is a diagram illustrating the target image shown in <figref idref="DRAWINGS">FIG. 4B</figref> divided into a lowest level of bounding boxes, according to one embodiment of the invention.</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 4D</figref> is a diagram illustrating the target image shown in <figref idref="DRAWINGS">FIG. 4B</figref> divided into a level of bounding boxes, according to one embodiment of the invention.</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 4E</figref> is a diagram illustrating the target image shown in <figref idref="DRAWINGS">FIG. 4B</figref> divided into a first level of bounding boxes, according to one embodiment of the invention.</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 4F</figref> is a diagram illustrating a hierarchy of bounding box levels for the target image shown in <figref idref="DRAWINGS">FIG. 4B</figref>, according to one embodiment of the invention.</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 5A</figref> is a flowchart of a method steps describing reprojection of a source image using adaptive traversal of a bounding box hierarchy to produce a target image, according to one embodiment of the invention.</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 5B</figref> is a flowchart of a method step shown in <figref idref="DRAWINGS">FIG. 5A</figref>, according to one embodiment of the invention.</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 5C</figref> is a flowchart of a method steps describing reprojection of a source image using adaptive traversal of a bounding box hierarchy and a depth order optimization to produce a target image, according to one embodiment of the invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION OF EXEMPLARY EMBODIMENTS</heading>
<p id="p-0029" num="0028">Embodiments of the invention provide techniques for reprojecting a source image to generate a target image based on difference information. The difference information, such as motion paths or vectors, may be used to generate a second image of a stereoscopic pair, converting frame rates (30-60 Hz split/second), rendering lightfields, performing depth of field, motion blur, and multiview point rendering. The reprojection algorithm relies on a fixed point iteration to quickly converge on a pixel in the source image that corresponds to a pixel in the target image. In order to reproject images in real-time, an insignificant parallax threshold value is computed and is used to generate portions of the target image corresponding to pixels in the source image that have depth values greater than the insignificant parallax threshold value by applying a uniform shift to the corresponding pixels in the source image.</p>
<p id="p-0030" num="0029">The source image may be divided into bounding areas, where each bounding area is subdivided to produce levels of increasing detail. In one embodiment, a bounding area is a bounding box that is aligned to the x and y axes. However, other shapes and alignments may be used for the bounding areas. The resulting set of bounding boxes may be represented as a tree or hierarchical structure that provides guidance data. The structure may be adaptively traversed to optimize execution of the reprojection algorithm by iterating within bounding boxes where differences are present, according to the difference information. In order to determine target pixel values corresponding to pixels that are occluded in the source image and revealed (disoccluded) in the target image due to movement of a foreground object, a final rendering pass is performed to &#x201c;fill&#x201d; holes in the target image resulting from disocclusion. When combined with the adaptive traversal of the tree structure, a high quality reprojected target image may be generated in real-time.</p>
<p id="p-0031" num="0030">One embodiment of the invention provides a computer-implemented method for reprojecting a source image. The method includes receiving the source image and difference information representing a difference between the source image and a target image that is a reprojection of the source image. For at least one location in the target image, a starting point in the source image determined based on the difference information. For the starting point, a processor iterates a function to generate target data for a respective location in the target image.</p>
<p id="p-0032" num="0031">One embodiment of the invention provides a computer-implemented method for reprojecting a source image using guidance data. This method includes receiving source image data and difference information representing an offset between the source image and a target image that is a reprojection of the source image. The source image is mapped into a set of bounding areas that represent the guidance data. A processor computes a convergence term based on the difference information for at least one of the bounding areas in the set of bounding areas.</p>
<heading id="h-0005" level="1">System Overview</heading>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 1A</figref> is a diagram illustrating an example system <b>100</b> for animation generation and/or animation playback. The system <b>100</b> may be configured to generate animation sequences including reprojected images in real-time or for playback. The system <b>100</b> may also be configured to execute a game and to generate reprojected images during execution of the game. The system <b>100</b> is further configured to accept and process input from a user and to provide data for displaying the results of such user input.</p>
<p id="p-0034" num="0033">The user inputs commands using input devices <b>108</b>. The input devices <b>108</b> may be any device that allows the user to interact with the system <b>100</b>. For example, the input device <b>108</b> may comprise a keyboard, a joystick, a controller, a microphone, a camera, a keypad, or a series of buttons, among other devices and features. The system <b>100</b> outputs graphics and animations to a display device <b>110</b>, the display device <b>110</b> may be any device that receives data for display and presents it visually to the user. For example, the display device <b>110</b> may include a cathode ray tube, a plurality of light emitting diodes (LEDs), a liquid crystal display (LCD), a portable video game console, or a projector, among other devices and features.</p>
<p id="p-0035" num="0034">The system <b>100</b> includes a central processing unit (CPU) <b>102</b> that is in communication with the input devices <b>108</b> through an input/output (I/O) bridge <b>107</b>. The CPU <b>102</b> communicates with a graphics processing unit (GPU) <b>112</b> through a memory bridge <b>105</b> to generate images that are output to the display device <b>110</b>. In some embodiments, one or more of the GPU <b>112</b>, CPU <b>102</b>, I/O bridge <b>107</b>, and memory bridge <b>105</b> are integrated into a single device. The system <b>100</b> may further include a system memory <b>104</b> in communication with the CPU <b>102</b> through the memory bridge <b>105</b>. The CPU <b>102</b> is configured to retrieve and execute programming instructions stored in the system memory <b>104</b> and system disk <b>114</b>. Similarly, the CPU <b>102</b> is configured to store and retrieve application data residing in the system memory <b>104</b> and system disk <b>114</b>.</p>
<p id="p-0036" num="0035">The system memory <b>104</b> may comprise certain types of random access memory (RAM) such as dynamic random access memory (DRAM) or static random access memory (SRAM), or may comprise any other type of volatile memory. The volatile memory <b>104</b> may be used to store data and/or instructions during operation of the CPU <b>102</b>. In particular, the system memory <b>104</b> may store reprojection components <b>103</b> that are used to generate sequences of images that include reprojected images. In other embodiments the reprojection components <b>103</b> are stored in the system disk <b>114</b>. Those skilled in the art will recognize other types of memory and uses thereof.</p>
<p id="p-0037" num="0036">The system <b>100</b> may further include a non-volatile system disk <b>114</b> that is in communication with the CPU <b>102</b> through the I/O bridge <b>107</b> and memory bridge <b>105</b>. The system disk <b>114</b> may include flash memory, magnetic storage devices, hard disks, or read-only memory (ROM) such as erasable programmable read-only memory (EPROM), or any other type of non-volatile memory. The system disk <b>114</b> may be used to store games, instructions, character information, game status information, or any other information that is to be retained if power to the system <b>100</b> is removed. The system <b>100</b> may comprise an interface to install or temporarily locate additional non-volatile memory. Those skilled in the art will recognize other types of non-volatile memory and uses thereof.</p>
<p id="p-0038" num="0037">The GPU <b>112</b> is configured to render data supplied by the CPU <b>102</b> for display on the display device <b>110</b>. The GPU <b>112</b> may be configured to perform any number of functions related to providing data for display on the display device <b>110</b>. For example, the GPU <b>112</b> may be configured to render a plurality of polygons, apply shading or texture, create data representative of a three-dimensional environment, or convert between coordinate spaces, among other functions. In particular, the GPU <b>112</b> may be configured to generate additional images by reprojecting rendered image data based on difference data. Those skilled in the art will recognize other configurations and functionalities of the GPU <b>112</b>. An advantage of generating the additional images using reprojection is that the additional images are not rendered from the polygons used to compose each additional image.</p>
<p id="p-0039" num="0038">The system <b>100</b> may further include a disc drive <b>115</b> in communication with the CPU <b>102</b>. The CPU <b>102</b> may read data from a disc inserted into the disc drive <b>115</b>. In some embodiments, the system <b>100</b> is configured to record data on the disc using the disc drive <b>115</b>. In this way, data relating to animation may be transported to or from the system <b>100</b>. Alternatively, animation data may be transmitted to/from system <b>100</b> via a network.</p>
<p id="p-0040" num="0039">The system <b>100</b> is not limited to the devices, configurations, and functionalities described above. For example, although a single volatile memory <b>106</b>, non-volatile memory <b>108</b>, GPU <b>112</b>, disc drive <b>112</b>, input device <b>114</b>, and display device <b>116</b> are illustrated, a plurality of any of these devices may be implemented internal or external to the system <b>100</b>. In addition, the system <b>100</b> may comprise a power supply or a network access device. Those skilled in the art will recognize other such configurations of the system <b>100</b>. Other components (not explicitly shown), including USB or other port connections, CD drives, DVD drives, film recording devices, and the like, may also be connected to I/O bridge <b>107</b>. Communication paths interconnecting the various components in <figref idref="DRAWINGS">FIG. 1</figref> may be implemented using any suitable protocols, such as PCI (Peripheral Component Interconnect), PCI-Express, AGP (Accelerated Graphics Port), HyperTransport, or any other bus or point-to-point communication protocol(s), and connections between different devices may use different protocols as is known in the art.</p>
<p id="p-0041" num="0040">Graphics and animations for display by the system <b>100</b> can be created using any number of methods and devices. A variety of commercially available modeling software may be used to generate graphics and animations representing a three-dimensional environment. Using such software, an animator can create objects and simulations of objects that can be used by the reprojection components <b>103</b> of the system <b>100</b> to provide data for display on the display device <b>110</b>.</p>
<p id="p-0042" num="0041">The reprojection components <b>103</b> may be configured to perform reprojections using an image warp. Given an source image I, the image warp is a vector field V:R<sup>2</sup>&#x2192;R<sup>2 </sup>defined over I, where the vector field defines a displacement for every pixel P at source position x<sub>s </sub>in I to a respective warped position x<sub>w </sub>in a target image I*, such that x<sub>w</sub>=x<sub>s</sub>+V(x<sub>s</sub>). The warp field V encapsulates information about the required reprojection and may be a function of the pixel depth only (as is the case for reprojections that shift the camera orthogonally to the view direction. Alternatively, the warp field may be defined as the per-pixel velocities (as required for temporal reprojections) or formulated as some combination of pixel depth and velocity. The warp field may include motion or path vectors.</p>
<p id="p-0043" num="0042">The equation x<sub>w</sub>=x<sub>s</sub>+V(x<sub>s</sub>) corresponds to the forward mapping formulation. The reverse mapping formulation defines the warp in terms of the warped positions: x<sub>s</sub>=x<sub>w</sub>+V*(x<sub>w</sub>). Given V*, the target image may be directly generated using the reverse mapping formulation by inputting the coordinates x<sub>w </sub>of each pixel in the target view and evaluating the right hand side to yield the lookup position x<sub>s </sub>in the source view. However, in typical computer graphics situations a closed form expression for V* is not available. Typically, the available input is information from the source viewpoint (pixel colors, depths, motion data, and the like) from which the warp V may be defined for the source image pixels. Defining an inverse warp V* is more difficult since the target view is unknown. Therefore, the reprojection components <b>103</b> are configured to solve the forward mapping equation using iterative techniques. More specifically, the location of each pixel in the target image is known and fixed (x<sub>w</sub>), whereas the source image location (x<sub>s</sub>) is the unknown. Therefore, the forward mapping formulation is solved for x<sub>s</sub>.</p>
<p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. 1B</figref> is a block diagram of the reprojection components of <figref idref="DRAWINGS">FIG. 1A</figref> that are configured to implement one or more aspects of the present invention. The reprojection components <b>103</b> comprises a reprojection engine <b>128</b> including instructions or data that can be used to reproject images. In particular, the reprojection engine <b>128</b> may be configured to reproject images by applying fixed point iteration based on difference data. Inputs to the reprojection engine <b>128</b> may include source image data <b>120</b>, difference data <b>124</b>, and bounding box hierarchy <b>126</b>. When the reprojection engine <b>128</b> is executed by the CPU <b>102</b> and/or GPU <b>112</b>, target image data <b>122</b> is generated. The source image data <b>120</b> and target image data <b>122</b> may represent a sequence of frames and may be stored in the system memory <b>104</b> or dedicated graphics memory associated with the GPU <b>112</b>.</p>
<p id="p-0045" num="0044">The source image data <b>120</b> may comprise hand-drawn 2D images or images rendered from 3D graphics data. The difference data <b>124</b> may be difference information including optical flow data representing motion in space or time of surfaces in the source image relative to the same surfaces in the target image. Examples of optical flow data include a warp field, motion vector or motion path data approximating differences in screenspace positions of one or more surfaces between the source image and the target image, and motion vector or motion path data approximating the screenspace velocity and/or acceleration of a surface as the screenspace positions of the surface changes from the source image to the target image. Optical flow data may be generated along with color and other data during the rendering process. The difference data <b>124</b> may comprise parameters that are used to perform the reprojection, such as an offset value for generating stereoscopic image pairs, for converting frame rates, and light source positions for rendering lightfields.</p>
<p id="p-0046" num="0045">For temporal reprojections a time parameter may be defined to control how far into the past or future the source image is reprojected. Setting the time parameter to 0.5 will generate inbetween frames for a 30-60 Hz split/second frame rate conversion. For spatial reprojections camera translations may be specified as horizontal and vertical parameters in (u,v) coordinates. For a stereoscopic reprojections v may be set to zero and u may be a function of interoccular distance. For lightfield rendering u and v may be used to denote the horizontal and vertical positions of each camera. Spatial reprojections may also be used to perform motion blur and defocus blur effects. Finally, multiple spatio-temporal effects may be performed simultaneously by combining the image processing that generates the target image as a function of (u,v) and time (t).</p>
<p id="p-0047" num="0046">The bounding box hierarchy <b>126</b> comprises a tree structure that provides guidance data for the reprojection engine <b>128</b>. The bounding box hierarchy <b>126</b> may be adaptively traversed to optimize execution of the reprojection algorithm by iterating within bounding boxes, according to the difference data <b>124</b>. The target image data <b>122</b> is a 2D image that is generated by the reprojection engine <b>128</b>. In one embodiment, a first system is configured to transmit the source image data <b>120</b> and the difference data <b>124</b> to a second system that processes the source image data <b>120</b> and the difference data <b>124</b> to generate the target image data <b>122</b>. A sequence of frames may be generated in real-time that includes the source image data <b>120</b> and the target image data <b>122</b>.</p>
<p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. 2A</figref> illustrates an example of an image <b>208</b> including a source surface <b>200</b> at time t that is reprojected to generate a target surface <b>202</b> at time t+1 that is also shown in image <b>208</b>, according to one embodiment of the invention. Frames for time t and t+1 are overlaid in the image <b>208</b>. The goal of the reprojection is to reproject a source image including the source surface <b>200</b> to approximate a target image including the target surface <b>202</b>. The background objects in image <b>208</b> are stationary and appear the same in the source and target images.</p>
<p id="p-0049" num="0048">The reprojection may be computed by a pixel shader that is executed by a processor, GPU <b>112</b> or CPU <b>102</b>. A pixel in the source image, corresponding pixel in the source image <b>205</b>, that corresponds to a pixel in the target image <b>204</b> is located. The corresponding pixel in the source image <b>205</b> provides a depth and color value for the pixel in the target image <b>204</b>. The corresponding pixel in the source image <b>205</b> is located based on difference data, e.g., the motion vector <b>206</b>. The motion vector <b>206</b> indicates the difference in position of the source surface <b>200</b> for the frames at time t and time t+1, and approximates the velocity of the corresponding pixel in the source image <b>205</b>. In one embodiment a motion path is used instead of motion vector <b>206</b>, where the motion path may be non-linear. The difference data provides a starting point for the fixed point iteration, e.g., pixel in the target image <b>204</b>. When difference data is not used to determine a starting point, the position of the corresponding pixel in the source image <b>205</b> may be used as the starting point. However, the iteration may not converge or may converge to an object in the background instead of the source surface <b>200</b>. In order to produce a correct result, the iteration computation needs to converge to the frontmost (nearest) surface point.</p>
<p id="p-0050" num="0049">In order to determine starting points for the reprojection, the image may be divided into a set of regions. For each region, the coordinates of the bounding box that includes the endpoints of all of the motion paths or vectors in the respective region are stored. <figref idref="DRAWINGS">FIG. 2B</figref> illustrates bounding boxes that are intersected by the target surface <b>202</b> of <figref idref="DRAWINGS">FIG. 2A</figref>, according to one embodiment of the invention. The motion vector bounding box <b>212</b> includes the endpoints of all of the motion vectors associated with the source surface <b>200</b>. The bounding box <b>214</b> includes endpoints of some of the motion vectors associated with the source surface <b>200</b>. Either the bounding box <b>214</b> or the motion vector bounding box <b>212</b> may be used to reproject the portion of the source surface <b>200</b> that is within each respective bounding box. More specifically, to compute the color and depth values for the pixels within the motion vector bounding box <b>212</b>, the fixed point iteration uses the pixels within the bounding box where the motion vector <b>206</b> (and other motion vectors associated with the source surface <b>200</b>) originate.</p>
<heading id="h-0006" level="1">Fixed Point Iteration</heading>
<p id="p-0051" num="0050">In order to reproject images in real-time, an insignificant parallax threshold value is computed and used to generate portions of the target image corresponding to pixels in the source image that have depth values greater than the insignificant parallax threshold value by applying a uniform shift to the corresponding pixels in the source image. When moving from a source image to a target image, where the source and target images are a stereographic image pair, the size of the shift of each pixel is inversely proportional to the depth of the pixel. In other words, pixels at different depths shift in a non-uniform manner. This effect is known as motion parallax.</p>
<p id="p-0052" num="0051">At a certain depth in a scene, the maximum possible relative parallax between two neighboring pixels becomes insignificant; in other words, two neighboring pixels with sufficiently large depth values shift almost uniformly. This region of the view volume may be shifted uniformly, with a bounded error, using an efficient copy operation, e.g., texture copy. Only the near part of the view volume is reprojected using the fixed point iterative reprojection technique.</p>
<p id="p-0053" num="0052">The threshold depth at which the maximum possible relative parallax between two neighboring pixels becomes insignificant is the Insignificant Parallax Threshold, or IPT. An expression for the IPT may be derived and the depth value is z<sub>IPT</sub>. Although the IPT is described assuming that a single projection matrix is used for the entire scene, the derivation of the z<sub>IPT </sub>may be generalised to the case where multiple projection matrices are used by modifying the inputs to correspond to each individual projective zone in the scene.</p>
<p id="p-0054" num="0053">Consider a point P at coordinates (x<sub>w</sub>, y<sub>w</sub>, z<sub>w</sub>) in view space. When a stereoscopic image is generated using reprojection, only the x coordinate varies between two views of the scene (a left eye and right eye image pair) since they coordinate is constant. By projective geometry, the view space coordinate of point P on the focal plane is given by:</p>
<p id="p-0055" num="0054">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <msub>
          <mi>x</mi>
          <mi>p</mi>
        </msub>
        <mo>=</mo>
        <mrow>
          <mi>d</mi>
          <mo>&#xb7;</mo>
          <mfrac>
            <msub>
              <mi>x</mi>
              <mi>w</mi>
            </msub>
            <msub>
              <mi>z</mi>
              <mi>w</mi>
            </msub>
          </mfrac>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>equation</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>1</mn>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0056" num="0055">Let the signed value of the shift of the points be equal to &#x394;x<sub>w</sub>. After the view shift, the new projected coordinate on the focal plane is given by:</p>
<p id="p-0057" num="0056">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <msubsup>
          <mi>x</mi>
          <mi>p</mi>
          <mi>&#x2032;</mi>
        </msubsup>
        <mo>=</mo>
        <mrow>
          <mi>d</mi>
          <mo>&#xb7;</mo>
          <mfrac>
            <mrow>
              <msub>
                <mi>x</mi>
                <mi>w</mi>
              </msub>
              <mo>+</mo>
              <mrow>
                <mi>&#x394;</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <msub>
                  <mi>x</mi>
                  <mi>w</mi>
                </msub>
              </mrow>
            </mrow>
            <msub>
              <mi>z</mi>
              <mi>w</mi>
            </msub>
          </mfrac>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>equation</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>2</mn>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0058" num="0057">The shift in projective coordinates &#x394;x<sub>p </sub>is given by the difference of equations (1) and (2):</p>
<p id="p-0059" num="0058">
<maths id="MATH-US-00003" num="00003">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <msubsup>
          <mi>x</mi>
          <mi>p</mi>
          <mi>&#x2032;</mi>
        </msubsup>
        <mo>=</mo>
        <mrow>
          <mi>d</mi>
          <mo>&#xb7;</mo>
          <mfrac>
            <mrow>
              <msub>
                <mi>x</mi>
                <mi>w</mi>
              </msub>
              <mo>+</mo>
              <mrow>
                <mi>&#x394;</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <msub>
                  <mi>x</mi>
                  <mi>w</mi>
                </msub>
              </mrow>
            </mrow>
            <msub>
              <mi>z</mi>
              <mi>w</mi>
            </msub>
          </mfrac>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>equation</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>3</mn>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0060" num="0059">Now we consider two neighboring pixels, P<sub>1 </sub>and P<sub>2</sub>, with shifts given by &#x394;x<sub>p1 </sub>and &#x394;x<sub>p2 </sub>respectively. Let &#x3b4; denote the difference of the parallax shifts of the two pixels:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>&#x3b4;=&#x394;<i>x</i><sub>p2</sub><i>&#x2212;&#x394;x</i><sub>p1</sub>&#x2003;&#x2003;(equation 4)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0061" num="0060">A bound &#x3b5; may be established on the difference &#x3b4; in shift between P<sub>1 </sub>and P<sub>2</sub>:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>|&#x3b4;|&#x3c;&#x3b5;&#x2003;&#x2003;(equation 5)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0062" num="0061">It is useful to defines &#x3b5; in units of pixels (instead of view space units). The following equation may be used to perform the required conversion:</p>
<p id="p-0063" num="0062">
<maths id="MATH-US-00004" num="00004">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mo>&#xf603;</mo>
          <mi>&#x3b4;</mi>
          <mo>&#xf604;</mo>
        </mrow>
        <mo>&#x3c;</mo>
        <mrow>
          <mfrac>
            <mn>2</mn>
            <msub>
              <mi>S</mi>
              <mi>h</mi>
            </msub>
          </mfrac>
          <mo>&#xb7;</mo>
          <mi>&#x25b;</mi>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>equation</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>6</mn>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
Expanding &#x3b4;:
</p>
<p id="p-0064" num="0063">
<maths id="MATH-US-00005" num="00005">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mo>&#xf603;</mo>
          <mrow>
            <mrow>
              <mi>&#x394;</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <msub>
                <mi>x</mi>
                <mrow>
                  <mi>p</mi>
                  <mo>&#x2062;</mo>
                  <mstyle>
                    <mspace width="0.3em" height="0.3ex"/>
                  </mstyle>
                  <mo>&#x2062;</mo>
                  <mn>2</mn>
                </mrow>
              </msub>
            </mrow>
            <mo>-</mo>
            <mrow>
              <mi>&#x394;</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <msub>
                <mi>x</mi>
                <mrow>
                  <mi>p</mi>
                  <mo>&#x2062;</mo>
                  <mstyle>
                    <mspace width="0.3em" height="0.3ex"/>
                  </mstyle>
                  <mo>&#x2062;</mo>
                  <mn>1</mn>
                </mrow>
              </msub>
            </mrow>
          </mrow>
          <mo>&#xf604;</mo>
        </mrow>
        <mo>&#x3c;</mo>
        <mfrac>
          <mrow>
            <mn>2</mn>
            <mo>&#x2062;</mo>
            <mi>&#x25b;</mi>
          </mrow>
          <msub>
            <mi>S</mi>
            <mi>h</mi>
          </msub>
        </mfrac>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>equation</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>7</mn>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mo>&#xf603;</mo>
          <mrow>
            <mrow>
              <mi>d</mi>
              <mo>&#xb7;</mo>
              <mfrac>
                <mrow>
                  <mi>&#x394;</mi>
                  <mo>&#x2062;</mo>
                  <mstyle>
                    <mspace width="0.3em" height="0.3ex"/>
                  </mstyle>
                  <mo>&#x2062;</mo>
                  <msub>
                    <mi>x</mi>
                    <mi>w</mi>
                  </msub>
                </mrow>
                <msub>
                  <mi>z</mi>
                  <mrow>
                    <mi>w</mi>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.3em" height="0.3ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mn>2</mn>
                  </mrow>
                </msub>
              </mfrac>
            </mrow>
            <mo>-</mo>
            <mrow>
              <mi>d</mi>
              <mo>&#xb7;</mo>
              <mfrac>
                <mrow>
                  <mi>&#x394;</mi>
                  <mo>&#x2062;</mo>
                  <mstyle>
                    <mspace width="0.3em" height="0.3ex"/>
                  </mstyle>
                  <mo>&#x2062;</mo>
                  <msub>
                    <mi>x</mi>
                    <mi>w</mi>
                  </msub>
                </mrow>
                <msub>
                  <mi>z</mi>
                  <mrow>
                    <mi>w</mi>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.3em" height="0.3ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mn>1</mn>
                  </mrow>
                </msub>
              </mfrac>
            </mrow>
          </mrow>
          <mo>&#xf604;</mo>
        </mrow>
        <mo>&#x3c;</mo>
        <mfrac>
          <mrow>
            <mn>2</mn>
            <mo>&#x2062;</mo>
            <mi>&#x25b;</mi>
          </mrow>
          <msub>
            <mi>S</mi>
            <mi>h</mi>
          </msub>
        </mfrac>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>equation</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>8</mn>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
Rearranging Terms:
</p>
<p id="p-0065" num="0064">
<maths id="MATH-US-00006" num="00006">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mo>&#xf603;</mo>
          <mrow>
            <mfrac>
              <mn>1</mn>
              <msub>
                <mi>z</mi>
                <mrow>
                  <mi>w</mi>
                  <mo>&#x2062;</mo>
                  <mstyle>
                    <mspace width="0.3em" height="0.3ex"/>
                  </mstyle>
                  <mo>&#x2062;</mo>
                  <mn>2</mn>
                </mrow>
              </msub>
            </mfrac>
            <mo>-</mo>
            <mfrac>
              <mn>1</mn>
              <msub>
                <mi>z</mi>
                <mrow>
                  <mi>w</mi>
                  <mo>&#x2062;</mo>
                  <mstyle>
                    <mspace width="0.3em" height="0.3ex"/>
                  </mstyle>
                  <mo>&#x2062;</mo>
                  <mn>1</mn>
                </mrow>
              </msub>
            </mfrac>
          </mrow>
          <mo>&#xf604;</mo>
        </mrow>
        <mo>&#x3c;</mo>
        <mfrac>
          <mrow>
            <mn>2</mn>
            <mo>&#x2062;</mo>
            <mi>&#x25b;</mi>
          </mrow>
          <mrow>
            <msub>
              <mi>S</mi>
              <mi>h</mi>
            </msub>
            <mo>&#x2062;</mo>
            <mi>d</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>&#x394;</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <msub>
              <mi>x</mi>
              <mi>w</mi>
            </msub>
          </mrow>
        </mfrac>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>equation</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>9</mn>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0066" num="0065">Since no assertions have been made on the ordering of the two pixels, the following may be asserted without loss in generality:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>z</i><sub>w2</sub><i>&#x3c;z</i><sub>w1</sub>&#x2003;&#x2003;(equation 10)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
Further, matters may be simplified by assigning:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>z</i><sub>w1</sub><i>=z</i><sub>far</sub>&#x2003;&#x2003;(equation 11)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0067" num="0066">By making this assertion, one of the pixels out of the pair is at the very back of the scene, and therefore the maximum possible relative shift error will be evident. This is appropriate for generic scenes. On the other hand, if the maximum depth differential is known to be bounded based on the structure of the scene, a smaller depth differential may be used instead, which would in turn reduce the size of the error and allow more of the scene to be shifted uniformly.</p>
<p id="p-0068" num="0067">Equations (10) and (11) allow the equation to be simplified to the following:</p>
<p id="p-0069" num="0068">
<maths id="MATH-US-00007" num="00007">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mfrac>
            <mn>1</mn>
            <msub>
              <mi>z</mi>
              <mrow>
                <mi>w</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mn>2</mn>
              </mrow>
            </msub>
          </mfrac>
          <mo>-</mo>
          <mfrac>
            <mn>1</mn>
            <msub>
              <mi>z</mi>
              <mi>far</mi>
            </msub>
          </mfrac>
        </mrow>
        <mo>&#x3c;</mo>
        <mfrac>
          <mrow>
            <mn>2</mn>
            <mo>&#x2062;</mo>
            <mi>&#x25b;</mi>
          </mrow>
          <mrow>
            <msub>
              <mi>S</mi>
              <mi>h</mi>
            </msub>
            <mo>&#x2062;</mo>
            <mi>d</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>&#x394;</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <msub>
              <mi>x</mi>
              <mi>w</mi>
            </msub>
          </mrow>
        </mfrac>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>equation</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>12</mn>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
Rearranging Terms:
</p>
<p id="p-0070" num="0069">
<maths id="MATH-US-00008" num="00008">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mfrac>
          <mn>1</mn>
          <msub>
            <mi>z</mi>
            <mrow>
              <mi>w</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mn>2</mn>
            </mrow>
          </msub>
        </mfrac>
        <mo>&#x3c;</mo>
        <mrow>
          <mfrac>
            <mrow>
              <mn>2</mn>
              <mo>&#x2062;</mo>
              <mi>&#x25b;</mi>
            </mrow>
            <mrow>
              <msub>
                <mi>S</mi>
                <mi>h</mi>
              </msub>
              <mo>&#x2062;</mo>
              <mi>d</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mi>&#x394;</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <msub>
                <mi>x</mi>
                <mi>w</mi>
              </msub>
            </mrow>
          </mfrac>
          <mo>+</mo>
          <mfrac>
            <mn>1</mn>
            <msub>
              <mi>z</mi>
              <mi>far</mi>
            </msub>
          </mfrac>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>equation</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>13</mn>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <msub>
            <mi>z</mi>
            <mrow>
              <mi>w</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mn>2</mn>
            </mrow>
          </msub>
          <mo>&#x2265;</mo>
          <mfrac>
            <mrow>
              <msub>
                <mi>S</mi>
                <mi>h</mi>
              </msub>
              <mo>&#x2062;</mo>
              <mi>d</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mi>&#x394;</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <msub>
                <mi>x</mi>
                <mi>w</mi>
              </msub>
              <mo>&#x2062;</mo>
              <msub>
                <mi>z</mi>
                <mi>far</mi>
              </msub>
            </mrow>
            <mrow>
              <mrow>
                <mn>2</mn>
                <mo>&#x2062;</mo>
                <mi>&#x25b;</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <msub>
                  <mi>z</mi>
                  <mi>far</mi>
                </msub>
              </mrow>
              <mo>+</mo>
              <mrow>
                <msub>
                  <mi>S</mi>
                  <mi>h</mi>
                </msub>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mi>&#x394;</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <msub>
                  <mi>x</mi>
                  <mi>w</mi>
                </msub>
                <mo>&#x2062;</mo>
                <mi>d</mi>
              </mrow>
            </mrow>
          </mfrac>
        </mrow>
        <mo>=</mo>
        <msub>
          <mi>z</mi>
          <mi>IPT</mi>
        </msub>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>equation</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>14</mn>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0071" num="0070">This formulation (equation 14) is useful because z<sub>far </sub>can be provided, and specify the maximum error &#x3b5; that may be tolerated, and the formula will produce a threshold depth z<sub>IPT</sub>, beyond which all image points can be shifted uniformly.</p>
<p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. 2C</figref> illustrates a plot of distances after which approximation error is bounded by &#x3b5;, for different values of &#x3b5; and z<sub>far</sub>, according to one embodiment of the invention. This function used to compute z<sub>IPT </sub>has two desirable characteristics: (1) Large amounts of depth allowance may be given to the scene (i.e. large values for z<sub>far</sub>) with minimal increase in the threshold value z<sub>IPT</sub>. The difference in threshold values between z<sub>far</sub>=1,000 and z<sub>far</sub>=1,000,000 is very small, even for small error tolerances (&#x3b5;). (2) The threshold decreases quickly for increasing error tolerance &#x3b5;. If an error of up to 0.5 pixels may be accepted, full reprojection needs only be performed for the frontmost 30-40 m of the scene.</p>
<p id="p-0073" num="0072">It is interesting to observe that even for a very deep scene, where z<sub>far</sub>=1,000,000, all points after z=180 may be shifted uniformly. Therefore, 99.98% of the scene's depth range may be shifted uniformly. Depending on the scene and the reprojection technique used, this may significantly reduce the rendering cost for the target image.</p>
<p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. 3A</figref> is a flowchart of a method steps <b>300</b> describing reprojection of a source image to produce a target image, according to one embodiment of the invention. Persons skilled in the art would understand that, even though the method is described in conjunction with the systems of <figref idref="DRAWINGS">FIGS. 1A-2C</figref>, any system configured to perform the method steps, in any order, is within the scope of embodiments of the invention.</p>
<p id="p-0075" num="0074">The method <b>300</b> begins at step <b>305</b>, where a processing unit, such as the processing unit that executes the reprojection engine <b>128</b>, receives the source image data <b>120</b>. At step <b>310</b>, the processing unit receives the difference data <b>124</b>. At step <b>315</b>, a starting point in the source image is identified using the difference data. At step <b>320</b>, the reprojection engine <b>128</b> iterates through the source image to generate the destination pixel data, as further described in conjunction with <figref idref="DRAWINGS">FIG. 3B</figref>.</p>
<p id="p-0076" num="0075">At step <b>360</b>, the reprojection engine <b>128</b> determines whether data should be computed for another pixel in the target image, and, if so, steps <b>315</b> and <b>320</b> are repeated. Otherwise, at step <b>365</b>, the target data has been generated and the pixel shader performs a rendering pass performs disocclusion processing. At step <b>370</b>, the target image is complete and may be output by the pixel shader for display and/or stored.</p>
<p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. 3B</figref> is a flowchart of a method step shown in <figref idref="DRAWINGS">FIG. 3A</figref>, according to one embodiment of the invention. At step <b>325</b>, the reprojection engine <b>128</b> determines z<sub>IPT </sub>for the source image. At step <b>330</b>, the reprojection engine <b>128</b> compares the z value for the corresponding pixel in the source image, z<sub>source </sub>with z<sub>IPT</sub>. If the z<sub>source </sub>is not less than z<sub>IPT</sub>, then, at step <b>345</b>, the reprojection engine <b>128</b> performs a uniform shift operation on the corresponding pixel in the source image to produce data for the target pixel before proceeding to step <b>360</b>.</p>
<p id="p-0078" num="0077">If, at step <b>330</b>, the reprojection engine <b>128</b> determines that the z<sub>source </sub>is less than z<sub>IPT</sub>, then, at step <b>335</b>, the reprojection engine <b>128</b> computes the z value for the pixel in the target image, Z<sub>target </sub>by performing a first iteration of the fixed point iteration. At step <b>340</b>, the reprojection engine <b>128</b> determines whether the fixed point iteration converges, and, if so, the reprojection engine <b>128</b> proceeds to step <b>360</b>. Otherwise, at step <b>342</b>, the reprojection engine <b>128</b> determines whether the pixel in the target image corresponds to a pixel in the source image that is revealed (disoccluded) by movement of a surface. If, at step <b>342</b>, the reprojection engine <b>128</b> determines that the pixel in the target image corresponds to a pixel in the source image that is not disoccluded, then the method returns to step <b>335</b> and another iteration of the fixed point iteration is performed.</p>
<p id="p-0079" num="0078">If, at step <b>342</b>, the reprojection engine <b>128</b> determines that the pixel in the target image corresponds to a pixel in the source image that is disoccluded, then, at step <b>344</b>, the target pixel is tagged or otherwise marked as disoccluded by the reprojection engine <b>128</b> so that data for the pixel will be generated during step <b>370</b>. In the case of temporal reprojection, divergent regions of the target image are situated at the trailing edge of moving objects. The target data is undefined at points that are divergent unless one or more additional input images are available that may be used to fill the holes resulting from the disocclusion. In divergent regions, the iteration computations tend to flip-flop in value across the discontinuity. The regularity of this flip-flopping behavior also tends to repeat a surrounding texture. In some cases, setting the parity of the iteration count, so that the final iteration lands on the background texture, often produces a satisfactory approximation to the correct result.</p>
<p id="p-0080" num="0079">Referring back to the forward mapping equation, x<sub>w</sub>=x<sub>s</sub>+V(x<sub>s</sub>) is rewritten as x<sub>s</sub>=G(x<sub>s</sub>) in order to solve for the source image locations, x<sub>s</sub>, where the new function G is defined as:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>G</i>(<i>x</i><sub>s</sub>)=<i>x</i><sub>w</sub><i>&#x2212;V</i>(<i>x</i><sub>s</sub>)&#x2003;&#x2003;(equation 15)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
The value x<sub>s </sub>that satisfies x<sub>s</sub>=G(x<sub>s</sub>) corresponds to a fixed point of G; when x<sub>s </sub>is input to G the result is x<sub>s</sub>. Fixed point interation solves equations of this form by generating a set of iterates, x<sub>i</sub>, using the recurrence relation:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>x</i><sub>i+1</sub><i>=G</i>(<i>x</i><sub>i</sub>)&#x2003;&#x2003;(equation 16)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
When seeded with an initial value x<sub>0</sub>, successive iterates are computed through repeated applications of G.
</p>
<p id="p-0081" num="0080">TABLE 1 illustrates example pseudocode for performing the iteration, where x<sub>0 </sub>is the starting point for the iteration and x<sub>w </sub>is the location of the pixel in the target image that is being computed.</p>
<p id="p-0082" num="0081">
<tables id="TABLE-US-00001" num="00001">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="center"/>
<thead>
<row>
<entry namest="1" nameend="1" rowsep="1">TABLE 1</entry>
</row>
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
<row>
<entry>Fixed Point Iteration Pseudocode</entry>
</row>
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="28pt" align="left"/>
<colspec colname="1" colwidth="189pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>Procedure FixedPointIteration (Point x<sub>w</sub>, Point x<sub>0</sub>)</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="42pt" align="left"/>
<colspec colname="1" colwidth="175pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>begin</entry>
</row>
<row>
<entry/>
<entry>x &#x2190; x<sub>0</sub></entry>
</row>
<row>
<entry/>
<entry>count &#x2190; 0</entry>
</row>
<row>
<entry/>
<entry>residual &#x2190; &#x3b5;</entry>
</row>
<row>
<entry/>
<entry>while residual &#x2267; &#x3b5; and count &#x3c; MAX_ITERS do</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="56pt" align="left"/>
<colspec colname="1" colwidth="161pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>x<sub>prev </sub>&#x2190; x</entry>
</row>
<row>
<entry/>
<entry>x &#x2190; x<sub>w </sub>&#x2212; V(x)</entry>
</row>
<row>
<entry/>
<entry>residual &#x2190; &#x2225; x &#x2212; x<sub>prev </sub>&#x2225;</entry>
</row>
<row>
<entry/>
<entry>count &#x2190; count + 1</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="42pt" align="left"/>
<colspec colname="1" colwidth="175pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>end while</entry>
</row>
<row>
<entry/>
<entry>if count &#x3c; MAX_ITERS then</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="56pt" align="left"/>
<colspec colname="1" colwidth="161pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>return x</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="42pt" align="left"/>
<colspec colname="1" colwidth="175pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>else</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="56pt" align="left"/>
<colspec colname="1" colwidth="161pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>return 0</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="42pt" align="left"/>
<colspec colname="1" colwidth="175pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>end procedure</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0083" num="0082">Two mechanisms are employed to supervise the iteration. Convergence is determined by thresholding the progress of the iteration. When the difference across iterations of the iterate x is smaller than a threshold value &#x3b5;, the algorithm stops iterating. Secondly, to ensure that diverging or slowly converging cases terminate in a timely manner, a threshold MAX_ITERS may be set to limit the total iteration count.</p>
<p id="p-0084" num="0083"><figref idref="DRAWINGS">FIG. 3C</figref> is another flowchart of the method step <b>320</b> shown in <figref idref="DRAWINGS">FIG. 3A</figref>, according to one embodiment of the invention. Rather than determining convergence based on z<sub>IPT</sub>, convergence is determined based on the threshold value &#x3b5; and MAX_ITERS. At step <b>345</b>, the reprojection engine <b>128</b> initializes the count to 0 and the residual to &#x3b5;. At step <b>348</b>, the reprojection engine <b>128</b> computes x using fixed point iteration. The reprojection engine <b>128</b> also increments the count and updates the residual by comparing the previous x (x<sub>prev</sub>) to the computed x. At step <b>350</b>, the reprojection engine <b>128</b> determines whether the fixed point iteration converges, and, if so, the reprojection engine <b>128</b> proceeds to step <b>360</b>.</p>
<p id="p-0085" num="0084">Otherwise, at step <b>352</b>, the reprojection engine <b>128</b> determines if the count is greater than or equal to the MAX_ITERS, and, if not, then the reprojection engine <b>128</b> returns to step <b>348</b>. Otherwise, the reprojection engine <b>128</b> proceeds to step <b>354</b> and determines if the pixel in the target image corresponds to a pixel in the source image that is revealed (disoccluded) by movement of a surface. If, at step <b>354</b>, the reprojection engine <b>128</b> determines that the pixel in the target image corresponds to a pixel in the source image that is not disoccluded, then the reprojection engine may set the pixel value to 0 before proceeding to step <b>360</b>. If, at step <b>354</b>, the reprojection engine <b>128</b> determines that the pixel in the target image corresponds to a pixel in the source image that is disoccluded, then, at step <b>356</b>, the target pixel is tagged or otherwise marked as disoccluded by the reprojection engine <b>128</b> so that data for the pixel will be generated during step <b>370</b>.</p>
<heading id="h-0007" level="1">Guided Reprojection</heading>
<p id="p-0086" num="0085">While it is possible to generate the data for the target image by processing each pixel of the target image using the methods shown in <figref idref="DRAWINGS">FIGS. 3A</figref>, <b>3</b>B, and <b>3</b>C, some optimizations may be possible so that processing performance may be increased. <figref idref="DRAWINGS">FIG. 4A</figref> is a diagram illustrating a source image <b>400</b>, according to one embodiment of the invention. The source image includes a source surface <b>401</b> and a source surface <b>402</b>.</p>
<p id="p-0087" num="0086"><figref idref="DRAWINGS">FIG. 4B</figref> is a diagram illustrating a target image <b>405</b> that is a reprojection of the source image <b>400</b> shown in <figref idref="DRAWINGS">FIG. 4A</figref>, according to one embodiment of the invention. The target image <b>405</b> includes a target surface <b>403</b> that is reprojected source surface <b>401</b>. The target image <b>405</b> also includes a target surface <b>404</b> that is reprojected source surface <b>402</b>. Source surface <b>401</b> moves to the left and source surface <b>402</b> moves to the right to produce the target image <b>405</b>.</p>
<p id="p-0088" num="0087">In order to more easily identify the particular pixel in the source image <b>400</b> that reprojects to a pixel in the target image <b>405</b>, a hierarchy of bounding boxes may be constructed. The source image <b>400</b> may be divided into bounding boxes, where each box is subdivided to produce levels of increasing detail. The resulting hierarchy of bounding boxes is represented as a tree structure that provides guidance data. The tree structure may be adaptively traversed to optimize execution of the reprojection algorithm by iterating within bounding boxes where the difference data indicates that differences (or movement) are present. Each node in the tree structure is implicitly associated with the underlying pixels in the source image, and stores information about where the surfaces in the underlying source image will reproject to in the target image. More precisely, each node contains an image-space bounding box of the reprojected positions of all the underlying pixels. The tree represents a hierarchical map describing where surfaces in the source image will reproject to in the target image.</p>
<p id="p-0089" num="0088"><figref idref="DRAWINGS">FIG. 4C</figref> is a diagram illustrating the target image shown in <figref idref="DRAWINGS">FIG. 4B</figref> divided into a lowest level of bounding boxes, according to one embodiment of the invention. Each bounding box in the lowest divided level of the target image <b>408</b> may include one or more pixels of the target image <b>405</b> and bounds the pixels where underlying motion paths or vectors (or other difference data) associated with the source image <b>400</b> originates. Most of the lowest level bounding boxes are unperturbed, i.e., do not include difference data indicating that a source surface <b>401</b> or <b>402</b> moves or is offset. However, the lowest level bounding boxes that include portions of the target surface <b>403</b> and <b>404</b> are perturbed.</p>
<p id="p-0090" num="0089">At each higher level of the bounding box hierarchy, a bounding box represents the union of the underlying bounding boxes in the lower level. <figref idref="DRAWINGS">FIG. 4D</figref> is a diagram illustrating the target image <b>405</b> shown in <figref idref="DRAWINGS">FIG. 4B</figref> divided into a higher level of bounding boxes, according to one embodiment of the invention. Each bounding box <b>411</b>-<b>426</b> in the divided level of the target image <b>410</b> represents the union of the underlying bounding boxes. As shown, bounding boxes <b>417</b>, <b>418</b> and <b>423</b> are perturbed as indicated by the misaligned vertical lines in bounding boxes <b>417</b>, <b>418</b> and <b>423</b>.</p>
<p id="p-0091" num="0090"><figref idref="DRAWINGS">FIG. 4E</figref> is a diagram illustrating the target image shown in <figref idref="DRAWINGS">FIG. 4B</figref> divided into a first level of bounding boxes, according to one embodiment of the invention. Each bounding box <b>431</b>-<b>434</b> in the highest divided level of the target image <b>430</b> represents the union of the underlying bounding boxes of the divided level of the target image <b>410</b>. More specifically, bounding box <b>431</b> is the union of bounding boxes <b>411</b>, <b>412</b>, <b>415</b>, and <b>416</b>. Bounding box <b>432</b> is the union of bounding boxes <b>413</b>, <b>414</b>, <b>417</b>, and <b>418</b>. Bounding box <b>433</b> is the union of bounding boxes <b>419</b>, <b>420</b>, <b>423</b>, and <b>424</b>. Bounding box <b>434</b> is the union of bounding boxes <b>421</b>, <b>422</b>, <b>425</b>, and <b>426</b>.</p>
<p id="p-0092" num="0091">The tree structure is constructed using the difference information, e.g., vector (or warp) field V. Tree nodes are generated from the bottom level up. In a first pass, a bottom-level tree node is generated for each pixel, containing the reprojected posision of each pixel. In subsequent reductions, the number of tree nodes is reduced by a factor of four, i.e., each node is constructed from its four underlying children.</p>
<p id="p-0093" num="0092">A pixel shader may identify the region of the source image <b>400</b> that reprojects to a pixel in the target image <b>405</b> by performing bounding box intersections with each level of the bounding box hierarchy. <figref idref="DRAWINGS">FIG. 4F</figref> is a diagram illustrating a hierarchy of bounding box levels for the target image shown in <figref idref="DRAWINGS">FIG. 4B</figref>, according to one embodiment of the invention. Only the highest divided level of the target image <b>430</b> and the divided level of the target image <b>410</b> are shown in the tree structure. The tree structure includes additional levels (not shown) until the lowest divided level of the target image <b>408</b> is reached. The pixel shader may follow the tree hierarchy down to the source pixel corresponding to a target pixel to determine each starting source pixel that in order to generate the target image data. However, walking the entire tree hierarchy for every target pixel is not optimal and does not take advantage of the fast convergence of the fixed point iteration.</p>
<p id="p-0094" num="0093">Traversal of the tree structure at a tree node n for a particular target pixel x<sub>w </sub>consists of performing bounding box intersections with each child n<sub>c </sub>of n. If the bounding box corresponding to n<sub>c </sub>intersects with x<sub>w</sub>, the process is repeated recursively on n<sub>c</sub>. In this manner a processing thread drills down to each surface in the source region that reprojects to x<sub>w</sub>. If the region of the source image R represented by the current node n is entirely convergent, i.e., a convergence term is less than one at all points underneath the node, then traversal can be terminated and fixed point iteration may then be used to resolve the exact intersection, using any point R as the initial value x<sub>0</sub>. For this purpose, a max-reduction may be performed on the values of the convergence terms during tree construction.</p>
<p id="p-0095" num="0094">The fixed point iteration generally converges when the magnitude of the derivative of the iterated function is less than one in the vicinity of the fixed point, where the fixed point is a point in the source image. The derivative is the rate of change of the motion paths or vectors or difference data. When the motion paths or vectors vary smoothly, the fixed point iteration converges. A well defined convergence condition exists for the function G (see equation 15), when G is defined over some interval R. For a one-dimensional fixed point iteration, G is Lipschitz continuous with Lipschitz constant L if
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>|<i>G</i>&#x2032;(<i>x</i>)|&#x3c;<i>L,&#x2200;x&#x3b5;R</i>&#x2003;&#x2003;(equation 17)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0096" num="0095">In general, if there is a solution to equation 17 in R, and G is Lipschitz continuous in R with Lipschitz constant L&#x3c;1, fixed point iteration will converge to this solution, provided that it is seeded with an initial value in R. This is a result of the Bachach fixed point theorem or contraction mapping theorem, which states that if G is a contraction mapping over some convex region R, then G will admit precisely one fixed point x in R, and fixed point iteration will converge to x.</p>
<p id="p-0097" num="0096">A contraction mapping is a mapping from R to itself that satisfies the Lipschitz continuity condition with L&#x3c;1, as detailed above. In this context, the result is used in a slightly different manner. A solution in R is tested directly, and if a solution exists and the Lipschitz continuity condition is satisfied, then G must be a contraction mapping over R, and convergence of the iteration to the solution is guaranteed.</p>
<p id="p-0098" num="0097">In two or more dimensions, the analogous condition to that shown in equation 17 is:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>&#x3c1;(<i>G</i>&#x2032;(<i>x</i>))&#x3c;1,<i>&#x2200;x&#x3b5;R,</i>&#x2003;&#x2003;(equation 18)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where G&#x2032; is the Jacobian matrix of G and &#x3c1; denotes the spectral radius of G&#x2032;, e.g., the maximum modulus of the eigenvalues of the matrix. Although an analytic formula for computing the eigenvalues of G&#x2032; exists for the two-dimensional case, this computation can be avoided by noting that &#x3c1;(G&#x2032;)&#x2266;&#x2225;G&#x2032;&#x2225; for any matrix G&#x2032; and any induced matrix norm. The matrix norm induced by the vector p-norms for p=1 (the taxicab norm) and p=&#x221e; (the maximum norm) are particularly simple, corresponding to the maximum absolute column sum and maximum absolute row sum, respectively. When the latter is used:
</p>
<p id="p-0099" num="0098">
<maths id="MATH-US-00009" num="00009">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <msub>
          <mrow>
            <mo>&#xf605;</mo>
            <msup>
              <mi>G</mi>
              <mi>&#x2032;</mi>
            </msup>
            <mo>&#xf606;</mo>
          </mrow>
          <mi>&#x221e;</mi>
        </msub>
        <mo>=</mo>
        <mrow>
          <munder>
            <mi>max</mi>
            <mrow>
              <mn>1</mn>
              <mo>&#x2264;</mo>
              <mi>i</mi>
              <mo>&#x2264;</mo>
              <mi>n</mi>
            </mrow>
          </munder>
          <mo>&#x2062;</mo>
          <mrow>
            <munderover>
              <mo>&#x2211;</mo>
              <mrow>
                <mi>j</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mi>n</mi>
            </munderover>
            <mo>&#x2062;</mo>
            <mrow>
              <mo>&#xf603;</mo>
              <msubsup>
                <mi>G</mi>
                <mi>ij</mi>
                <mi>&#x2032;</mi>
              </msubsup>
              <mo>&#xf604;</mo>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>equation</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>19</mn>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where n=2 for the two dimensional case.
</p>
<p id="p-0100" num="0099">Using the definition provided by equation 19, the location of boundaries between convergent regions in the source image may be determined by computing the Jacobian G&#x2032; using discrete derivatives, and checking if the maximum norm is less than 1. Although in general the location of these boundaries depends on the difference data, for difference data corresponding to spatial and temporal reprojections, these boundaries typically lie along geometry edges, as pixels that share a geometric surface generally shift together in a smooth manner.</p>
<p id="p-0101" num="0100">Referring back to <figref idref="DRAWINGS">FIG. 4E</figref>, in bounding boxes <b>431</b> and <b>434</b>, the maximum convergence term is less than one. In other words, all of the underlying difference data, e.g., motion or displacement vectors, vary smoothly, and the fixed point iteration will converge. In bounding boxes <b>432</b> and <b>433</b> there are discontinuities or steep gradients in the underlying difference data, so more iterations may be needed to reach convergence or convergence may not be possible. Discontinuities may indicate disocclusion.</p>
<p id="p-0102" num="0101">When traversing the tree hierarchy, intersections between a position of the pixel in the source image that corresponds to a reprojected pixel in the target image and the bounding boxes are identified. When an intersection is found and the maximum convergence term is less than one, the fixed point iteration converges. When an intersection is found and the maximum convergence term is not less than one, the tree hierarchy is traversed to a lower level. For example, a position of a pixel may intersect with the bounding box <b>432</b>. Because the maximum convergence term in the bounding box <b>432</b> is not less than one, the bounding boxes underlying bounding box <b>432</b> are examined. As shown in <figref idref="DRAWINGS">FIG. 4D</figref>, bounding boxes <b>413</b>, <b>414</b>, <b>417</b>, and <b>418</b> underlie bounding box <b>432</b>. Assuming that the position of the pixel is within bounding box <b>418</b>, the maximum convergence term is less than one, so the iteration converges for the pixel.</p>
<p id="p-0103" num="0102">At the lowest divided level of the target image <b>408</b>, as shown in <figref idref="DRAWINGS">FIG. 4C</figref>, some bounding boxes lie within the target surface <b>404</b>. Bounding boxes within the center of the target surface <b>404</b> may converge since motion discontinuities do not exist within the center of the target surface <b>404</b> for gradual motion. However, at the edges of the target surface <b>404</b> there are motion discontinuities that may require additional iterations to resolve or may present an area of disocclusion.</p>
<p id="p-0104" num="0103">Pixels within bounding boxes that have a maximum convergence term that is less than one will converge to a fixed point, regardless of the initial starting point (initial iteration value). Pixels within bounding boxes that have a maximum convergence term that is not less than one may converge with fewer iterations if the initial starting point is in a smooth region (not an area of discontinuity). In one embodiment, the initial starting point of the iteration is identified within the source image that is offset from the corresponding pixel in the target image according to the difference data. The initial starting point may be determined as an offset of the motion vector bounding box.</p>
<p id="p-0105" num="0104"><figref idref="DRAWINGS">FIG. 5A</figref> is a flowchart of a method steps describing reprojection of a source image using adaptive traversal of a bounding box hierarchy to produce a target image, according to one embodiment of the invention. Persons skilled in the art would understand that, even though the method is described in conjunction with the systems of <figref idref="DRAWINGS">FIGS. 1A-2C</figref> and <b>4</b>A-<b>4</b>F, any system configured to perform the method steps, in any order, is within the scope of embodiments of the invention.</p>
<p id="p-0106" num="0105">The method <b>500</b> begins at step <b>505</b>, where a processing unit, such as the processing unit that executes the reprojection engine <b>128</b>, receives the source image data <b>120</b>. At step <b>510</b>, the processing unit receives the difference data <b>124</b>. At step <b>515</b>, the source image is partitioned into bounding boxes and a bounding box hierarchy is constructed. The reprojection engine <b>128</b> may also compute the maximum convergence term for each bounding box in the hierarchy at step <b>515</b>.</p>
<p id="p-0107" num="0106">At step <b>520</b>, a bounding box is selected and a particular starting point within the bounding box may be identified based on the difference data. At step <b>522</b>, the reprojection engine <b>128</b> determines whether the maximum convergence term for the bounding box is less than one, and, if so, then, at step <b>525</b>, the reprojection engine <b>128</b> iterates in the bounding box to generate the target pixel data. Otherwise, at step <b>530</b>, the reprojection engine <b>128</b> traverses the bounding box hierarchy to generate the target pixel data, as further described in conjunction with <figref idref="DRAWINGS">FIG. 5B</figref>.</p>
<p id="p-0108" num="0107">At step <b>570</b>, the reprojection engine <b>128</b> determines whether data should be computed for another bounding box in the source image, and, if so, the reprojection engine <b>128</b> returns to step <b>520</b>. Otherwise, at step <b>575</b>, the target data has been generated and the pixel shader performs a rendering pass for disocclusion processing, after which the target image is complete and may be output by the pixel shader for display and/or stored.</p>
<p id="p-0109" num="0108"><figref idref="DRAWINGS">FIG. 5B</figref> is a flowchart of the method step <b>530</b> shown in <figref idref="DRAWINGS">FIG. 5A</figref>, according to one embodiment of the invention. At step <b>530</b>, the reprojection engine <b>128</b> selects a lower level bounding box. At step <b>535</b>, the reprojection engine <b>128</b> determines whether the maximum convergence term for the bounding box is less than one, and, if so, then, at step <b>540</b>, the reprojection engine <b>128</b> iterates in the bounding box to generate the target pixel data. Otherwise, at step <b>545</b>, the reprojection engine <b>128</b> determines whether the lower level bounding box selected at step <b>530</b> is at the lowest level of the bounding box hierarchy. If the lowest level has not been reached, then, at step <b>550</b>, a lower level bounding box is selected by the reprojection engine <b>128</b>. At step <b>555</b>, the reprojection engine <b>128</b> iterates down the hierarchy for the bounding box to generate the target pixel data.</p>
<p id="p-0110" num="0109">If, at step <b>545</b>, the reprojection engine <b>128</b> determines that the lowest level of the bounding box hierarchy has been reached, then, at step <b>560</b>, the reprojection engine <b>128</b> iterates in the bounding box to generate the target pixel data. At step <b>565</b>, the reprojection engine <b>128</b> determines whether another bounding box exists at the level, and, if so, the bounding box is processed by the reprojection engine <b>128</b>. Otherwise, the reprojection engine <b>128</b> proceeds to step <b>570</b>.</p>
<p id="p-0111" num="0110">When a z (depth) value (z_converged) is computed for a reprojected pixel, a further optimization may be performed to produce high quality target images. In addition to storing the maximum convergence terms for each bounding box, the minimum depth values for bounding box may be stored in the bounding box hierarchy. The minimum depth value represents the minimum (nearest) depth of all surfaces represented by the bounding box, e.g., underlying surfaces. Lower level boxes are not traversed by the reprojection engine <b>128</b> when the minimum depth value is less than z_converged. Initially, z_converged equals z_far. After converging to a point on a surface at a particular depth, the (new) depth is compared with z_converged. When the (new) depth is less than (i.e., closer) than z_converged, the (new) depth replaces z_converged. Otherwise, the (new) depth is discarded.</p>
<p id="p-0112" num="0111">In sum, the reprojection engine <b>128</b> may adaptively traverse the bounding box hierarchy based on the minimum z values and the maximum convergence terms. When a depth value, z_converged is computed, the reprojection engine <b>128</b> then walks back up the bounding box hierarchy, checking the remaining bounding boxes for intersections. When an intersection is found, and the corresponding minimum z value is less than the z_converged, the corresponding bounding box is traversed in search of a closer surface.</p>
<p id="p-0113" num="0112"><figref idref="DRAWINGS">FIG. 5C</figref> is a flowchart of a method steps <b>501</b> describing reprojection of a source image using adaptive traversal of a bounding box hierarchy and a depth order optimization to produce a target image, according to one embodiment of the invention. Persons skilled in the art would understand that, even though the method is described in conjunction with the system of <figref idref="DRAWINGS">FIGS. 1A-2C</figref> and <b>4</b>A-<b>4</b>F, any system configured to perform the method steps, in any order, is within the scope of embodiments of the invention.</p>
<p id="p-0114" num="0113">In one embodiment, steps <b>505</b>, <b>510</b>, <b>515</b>, <b>520</b>, <b>522</b>, <b>525</b>, <b>530</b>, <b>570</b>, and <b>575</b> are performed as previously described in conjunction with <figref idref="DRAWINGS">FIG. 5A</figref>. After each z_converged has been computed at step <b>525</b> for a bounding box, at step <b>527</b>, the reprojection engine <b>128</b> records the minimum z depth for the bounding box. Similarly, following step <b>530</b>, at step <b>532</b> the reprojection engine <b>128</b> also records the minimum z depth for the bounding box.</p>
<p id="p-0115" num="0114">TABLE 2 illustrates pseudocode that may be used to perform image reprojection using a tree hierarchy (guidance tree).</p>
<p id="p-0116" num="0115">
<tables id="TABLE-US-00002" num="00002">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="center"/>
<thead>
<row>
<entry namest="1" nameend="1" rowsep="1">TABLE 2</entry>
</row>
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
<row>
<entry>Image Reprojection Pseudocode</entry>
</row>
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="14pt" align="left"/>
<colspec colname="1" colwidth="203pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>Procedure ReprojectImage (Point x<sub>w</sub>, GuidanceTree t)</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="42pt" align="left"/>
<colspec colname="1" colwidth="175pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>begin</entry>
</row>
<row>
<entry/>
<entry>// perform reprojection</entry>
</row>
<row>
<entry/>
<entry>return TraverseTree(t.root, x<sub>w</sub>, Z_FAR)</entry>
</row>
<row>
<entry/>
<entry>end procedure</entry>
</row>
<row>
<entry/>
<entry>procedure TraverseTree(Node n, Point x,</entry>
</row>
<row>
<entry/>
<entry>Float z<sub>converged</sub>)</entry>
</row>
<row>
<entry/>
<entry>begin</entry>
</row>
<row>
<entry/>
<entry>// check whether the source region reporjects</entry>
</row>
<row>
<entry/>
<entry>to the target position</entry>
</row>
<row>
<entry/>
<entry>// and that the region cannot be culled based</entry>
</row>
<row>
<entry/>
<entry>on the previously</entry>
</row>
<row>
<entry/>
<entry>// converged depth value</entry>
</row>
<row>
<entry/>
<entry>if InBB(x, n, BB) and n, z<sub>min </sub>&#x3c; z<sub>converged </sub>then</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="56pt" align="left"/>
<colspec colname="1" colwidth="161pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>if n.convergence_term &#x3c; 1 then</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="70pt" align="left"/>
<colspec colname="1" colwidth="147pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>// convergence condition satisfied,</entry>
</row>
<row>
<entry/>
<entry>switch to FP1</entry>
</row>
<row>
<entry/>
<entry>Point x<sub>0 </sub>&#x2190; n.source_region.center</entry>
</row>
<row>
<entry/>
<entry>x &#x2190; FixedPointIteration(x,x<sub>0</sub>)</entry>
</row>
<row>
<entry/>
<entry>if x &#x2260; 0 then</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="84pt" align="left"/>
<colspec colname="1" colwidth="133pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>depth &#x2190; Sample(SrcDepths, x)</entry>
</row>
<row>
<entry/>
<entry>if depth &#x3c; z<sub>converged </sub>then</entry>
</row>
<row>
<entry/>
<entry>return Sample(SrcImage, x)</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="70pt" align="left"/>
<colspec colname="1" colwidth="147pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>end if</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="56pt" align="left"/>
<colspec colname="1" colwidth="161pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>end if</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="42pt" align="left"/>
<colspec colname="1" colwidth="175pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>else</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="56pt" align="left"/>
<colspec colname="1" colwidth="161pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>// drill down into node's children in</entry>
</row>
<row>
<entry/>
<entry>depth-first fashion</entry>
</row>
<row>
<entry/>
<entry>Color child_result, final_result &#x2190; 0</entry>
</row>
<row>
<entry/>
<entry>For each child n<sub>c </sub>of n do</entry>
</row>
<row>
<entry/>
<entry>child_result &#x2190; TraverseTree(n<sub>c</sub>, x, z<sub>converged</sub>)</entry>
</row>
<row>
<entry/>
<entry>if child_result &#x2260; 0 then</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="70pt" align="left"/>
<colspec colname="1" colwidth="147pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>final_result &#x2190; child_result</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="56pt" align="left"/>
<colspec colname="1" colwidth="161pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>end for</entry>
</row>
<row>
<entry/>
<entry>return final_result</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="28pt" align="left"/>
<colspec colname="1" colwidth="189pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>end if</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="14pt" align="left"/>
<colspec colname="1" colwidth="203pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>end if</entry>
</row>
<row>
<entry/>
<entry>end procedure</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0117" num="0116">Advantageously, the technique of using fixed point iteration may be used to reproject a source image and generate a high-quality target image in real-time. The fixed point iteration technique may also be used for offline rendering. A constant offset may be defined as the difference data to perform stereo reprojection. The difference data may be defined to perform other spatial reprojections and/or temporal reprojections. Additionally, disocclusion artifacts may be resolved through a final rendering pass that computes data for pixels identified during the fixed point iteration process. Performance of the reprojection technique may be improved by constructing and relying on guidance data that is in the form of a bounding box hierarchy. Since the bounding boxes tend to move linearly across a sequence of images, positions of the bounding boxes may be determined as trajectories instead of absolute positions. Relying on trajectories means that the bounding box hierarchy does not need to be reconstructed for each source image in the sequence of images. Maximum convergent terms and minimum depth values may be stored for each bounding box that allow the reprojection engine to adaptively traverse the bounding box hierarchy in order to generate the target data.</p>
<p id="p-0118" num="0117">Various embodiments of the invention may be implemented as a program product for use with a computer system. The program(s) of the program product define functions of the embodiments (including the methods described herein) and can be contained on a variety of computer-readable storage media. Illustrative computer-readable storage media include, but are not limited to: (i) non-writable storage media (e.g., read-only memory devices within a computer such as CD-ROM disks readable by a CD-ROM drive, flash memory, ROM chips or any type of solid-state non-volatile semiconductor memory) on which information is permanently stored; and (ii) writable storage media (e.g., floppy disks within a diskette drive or hard-disk drive or any type of solid-state random-access semiconductor memory) on which alterable information is stored.</p>
<p id="p-0119" num="0118">The invention has been described above with reference to specific embodiments and numerous specific details are set forth to provide a more thorough understanding of the invention. Persons skilled in the art, however, will understand that various modifications and changes may be made thereto without departing from the broader spirit and scope of the invention. The foregoing description and drawings are, accordingly, to be regarded in an illustrative rather than a restrictive sense.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08624891-20140107-M00001.NB">
<img id="EMI-M00001" he="6.35mm" wi="76.20mm" file="US08624891-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US08624891-20140107-M00002.NB">
<img id="EMI-M00002" he="6.69mm" wi="76.20mm" file="US08624891-20140107-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00003" nb-file="US08624891-20140107-M00003.NB">
<img id="EMI-M00003" he="6.69mm" wi="76.20mm" file="US08624891-20140107-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00004" nb-file="US08624891-20140107-M00004.NB">
<img id="EMI-M00004" he="6.69mm" wi="76.20mm" file="US08624891-20140107-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00005" nb-file="US08624891-20140107-M00005.NB">
<img id="EMI-M00005" he="14.82mm" wi="76.20mm" file="US08624891-20140107-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00006" nb-file="US08624891-20140107-M00006.NB">
<img id="EMI-M00006" he="6.69mm" wi="76.20mm" file="US08624891-20140107-M00006.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00007" nb-file="US08624891-20140107-M00007.NB">
<img id="EMI-M00007" he="7.03mm" wi="76.20mm" file="US08624891-20140107-M00007.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00008" nb-file="US08624891-20140107-M00008.NB">
<img id="EMI-M00008" he="15.92mm" wi="76.20mm" file="US08624891-20140107-M00008.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00009" nb-file="US08624891-20140107-M00009.NB">
<img id="EMI-M00009" he="8.81mm" wi="76.20mm" file="US08624891-20140107-M00009.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A computer-implemented method for reprojecting a source image, comprising:
<claim-text>receiving difference information representing a difference between the source image and a target image that is a reprojection of the source image;</claim-text>
<claim-text>dividing the target image into a set of bounding areas that each include endpoints of vectors that originate in the source image and represent the difference information; and</claim-text>
<claim-text>by a processor and for a starting point in the source image determined for at least one location in the target image based on the difference information for at least a first of the set of bounding areas, iterating a predefined function in order to generate target data for a respective location in the target image, including traversing the set of bounding areas based on a comparison between: (i) a depth value computed to represent the nearest depth value of surfaces in the first bounding area and (ii) a converged depth value computed for the at least one location in the target image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising performing a rendering pass to generate target data for disoccluded pixels.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the predefined function is a fixed point function.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, further comprising tagging a pixel in the target image that corresponds to a first location as a disoccluded pixel when the fixed point function does not converge for the first location in the source image.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the difference information includes optical flow data indicating differences in positions of surfaces in the source image relative to positions of the surfaces in the target image.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the target image and the source image form a stereographic image pair and the difference information is an offset distance in screenspace.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the source image is a first frame in a sequence of frames including source images and target images, and the target image is a second frame in the sequence of frames.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein a frame rate of only the source images in the sequence of frames is a first frequency and a frame rate of the source images and the target images in the sequence of frames is a second frequency.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the difference information represents a spatial difference in a viewpoint position corresponding to the source image and the target image.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the difference information represents a temporal difference between the source image and the target image.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the iterating of the predefined function comprises computing a threshold depth at which a relative parallax between two neighboring locations in the source image that are reprojected to the target image is insignificant.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising generating data for a target location in the target image by shifting a corresponding source location in the source image when a reprojected depth value computed for the target location is not less than the threshold depth.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the source image and the target image correspond to lightfields associated with different camera views.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. A non-transitory computer-readable medium storing instructions that, when executed by a processor, cause a computer system to perform an operation for reprojecting a source image, the operation comprising:
<claim-text>receiving difference information representing a difference between the source image and a target image that is a reprojection of the source image;</claim-text>
<claim-text>dividing the target image into a set of bounding areas that each include endpoints of vectors that originate in the source image and represent the difference information; and</claim-text>
<claim-text>by a processor and for a starting point in the source image determined for at least one location in the target image based on the difference information for at least a first of the set of bounding areas, iterating a predefined function in order to generate target data for a respective location in the target image, including traversing the set of bounding areas based on a comparison between: (i) a depth value computed to represent the nearest depth value of surfaces in the first bounding area and (ii) a converged depth value computed for the at least one location in the target image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. A system, comprising:
<claim-text>a processor; and</claim-text>
<claim-text>a memory storing a reprojection engine that is configured to perform an operation for reprojecting a source image, the operation comprising:
<claim-text>receiving difference information representing a difference between the source image and a target image that is a reprojection of the source image;</claim-text>
<claim-text>dividing the target image into a set of bounding areas that each include endpoints of vectors that originate in the source image and represent the difference information; and</claim-text>
<claim-text>by a processor and for a starting point in the source image determined for at least one location in the target image based on the difference information for at least a first of the set of bounding areas, iterating a predefined function in order to generate target data for a respective location in the target image, including traversing the set of bounding areas based on a comparison between: (i) a depth value computed to represent the nearest depth value of surfaces in the first bounding area and (ii) a converged depth value computed for the at least one location in the target image.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. A computer-implemented method of reprojecting a source image using guidance data, comprising:
<claim-text>receiving source image data;</claim-text>
<claim-text>receiving difference information representing an offset between the source image and a target image that is a reprojection of the source image;</claim-text>
<claim-text>mapping the source image into a set of bounding areas that represent the guidance data;</claim-text>
<claim-text>computing a convergence term based on the difference information for at least one of the bounding areas in the set of bounding areas;</claim-text>
<claim-text>computing, for at least a first of the set of bounding areas, a depth value that represents the nearest depth value of surfaces in the first bounding area; and</claim-text>
<claim-text>computing a converged depth value for a location in the target image, wherein the set of bounding areas is traversed based on a comparison between the converged depth value and the converged depth value in order to generate the target image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the set of bounding areas is a hierarchy with an increasing number of bounding areas at successively lower levels of the hierarchy.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, further comprising, storing the set of bounding areas and a maximum convergence term in a memory.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the difference information includes optical flow data indicating differences in positions of surfaces in the source image relative to positions of the surfaces in the target image.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the target image and the source image form a stereographic image pair and the difference information is an offset distance in screenspace.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. A non-transitory computer-readable medium storing instructions that, when executed by a processor, cause a computer system to perform an operation for reprojecting a source image using guidance data, the operation comprising:
<claim-text>receiving source image data;</claim-text>
<claim-text>receiving difference information representing a difference between the source image and a target image that is a reprojection of the source image;</claim-text>
<claim-text>mapping the source image into a set of bounding areas that represent the guidance data;</claim-text>
<claim-text>computing a convergence term based on the difference information for at least one of the bounding areas in the set of bounding areas;</claim-text>
<claim-text>computing, for at least a first of the set of bounding areas, a depth value that represents the nearest depth value of surfaces in the first bounding area; and</claim-text>
<claim-text>computing a converged depth value for a location in the target image, wherein the set of bounding areas is traversed based on a comparison between the converged depth value and the converged depth value in order to generate the target image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The non-transitory computer-readable medium of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the set of bounding areas is a hierarchy with an increasing number of bounding areas at successively lower levels of the hierarchy.</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. A system, comprising:
<claim-text>a processor; and</claim-text>
<claim-text>a memory storing a reprojection engine that is configured to perform an operation for reprojecting a source image using guidance data, the operation comprising:
<claim-text>receiving source image data;</claim-text>
<claim-text>receiving difference information representing a distance between the source image and a target image that is a reprojection of the source image;</claim-text>
<claim-text>mapping the source image into a set of bounding areas that represent the guidance data;</claim-text>
<claim-text>computing a convergence term based on the difference information for at least one of the bounding areas in the set of bounding areas;</claim-text>
<claim-text>computing, for at least a first of the set of bounding areas, a depth value that represents the nearest depth value of surfaces in the first bounding area; and</claim-text>
<claim-text>computing a converged depth value for a location in the target image, wherein the set of bounding areas is traversed based on a comparison between the converged depth value and the converged depth value in order to generate the target image.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. The system of <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein the set of bounding areas is a hierarchy with an increasing number of bounding areas at successively lower levels of the hierarchy.</claim-text>
</claim>
<claim id="CLM-00025" num="00025">
<claim-text>25. A computer-implemented method of transmitting data to a receiving processor, comprising:
<claim-text>transmitting a source image to the receiving processor; and</claim-text>
<claim-text>transmitting difference information by a sending processor and representing a difference between the source image and a target image that is a reprojection of the source image to the receiving processor, wherein the receiving processor is configured to process the transmitted source image and difference information by:
<claim-text>dividing the target image into a set of bounding areas that each include endpoints of vectors that originate in the source image and represent the difference information; and</claim-text>
<claim-text>iterating, for a starting point in the source image determined for at least one location in the target image based on the difference information for at least a first of the set of bounding areas, a predefined function in order to generate target data for a respective location in the target image, including traversing the set of bounding areas based on a comparison between: (i) a depth value computed to represent the nearest depth value of surfaces in the first bounding area and (ii) a converged depth value computed for the at least one location in the target image.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00026" num="00026">
<claim-text>26. A non-transitory computer-readable medium storing instructions that, when executed, cause a computer system to perform an operation for transmitting data to a receiving processor, the operation comprising:
<claim-text>transmitting a source image to the receiving processor; and</claim-text>
<claim-text>transmitting, by a sending processor when executing the instructions, difference information representing a difference between the source image and a target image that is a reprojection of the source image to the receiving processor;</claim-text>
<claim-text>wherein the receiving processor is configured to process the transmitted source image and difference information by:
<claim-text>dividing the target image into a set of bounding areas that each include endpoints of vectors that originate in the source image and represent the difference information; and</claim-text>
<claim-text>iterating, for a starting point in the source image determined for at least one location in the target image based on the difference information for at least a first of the set of bounding areas, a predefined function in order to generate target data for a respective location in the target image, including traversing the set of bounding areas based on a comparison between: (i) a depth value computed to represent the nearest depth value of surfaces in the first bounding area and (ii) a converged depth value computed for the at least one location in the target image.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00027" num="00027">
<claim-text>27. The non-transitory computer-readable medium of <claim-ref idref="CLM-00026">claim 26</claim-ref>, wherein the receiving processor is further configured to perform a rendering pass to generate target data for disoccluded pixels.</claim-text>
</claim>
<claim id="CLM-00028" num="00028">
<claim-text>28. The non-transitory computer-readable medium of <claim-ref idref="CLM-00026">claim 26</claim-ref>, wherein the predefined function is a fixed point function.</claim-text>
</claim>
<claim id="CLM-00029" num="00029">
<claim-text>29. The non-transitory computer-readable medium of <claim-ref idref="CLM-00026">claim 26</claim-ref>, wherein the source image and the target image correspond to lightfields associated with different camera views. </claim-text>
</claim>
</claims>
</us-patent-grant>
