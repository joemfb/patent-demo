<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08627355-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08627355</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12673791</doc-number>
<date>20080820</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="regional">
<country>EP</country>
<doc-number>07114730</doc-number>
<date>20070822</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>239</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>7</main-group>
<subgroup>10</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>725 34</main-classification>
<further-classification>725 10</further-classification>
<further-classification>725 42</further-classification>
</classification-national>
<invention-title id="d2e71">System and method for displaying selected information to a person undertaking exercises</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>6663491</doc-number>
<kind>B2</kind>
<name>Watabe et al.</name>
<date>20031200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>463 36</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6778866</doc-number>
<kind>B1</kind>
<name>Bettwy</name>
<date>20040800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>700 56</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6904408</doc-number>
<kind>B1</kind>
<name>McCarthy et al.</name>
<date>20050600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6978244</doc-number>
<kind>B2</kind>
<name>Rovinelli et al.</name>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>705  2</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>2001/0051559</doc-number>
<kind>A1</kind>
<name>Cohen et al.</name>
<date>20011200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2003/0005067</doc-number>
<kind>A1</kind>
<name>Martin et al.</name>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2003/0208588</doc-number>
<kind>A1</kind>
<name>Segal</name>
<date>20031100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2006/0277075</doc-number>
<kind>A1</kind>
<name>Salwan</name>
<date>20061200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2007/0088603</doc-number>
<kind>A1</kind>
<name>Jouppi et al.</name>
<date>20070400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2008/0161733</doc-number>
<kind>A1</kind>
<name>Einav et al.</name>
<date>20080700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>601 34</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2008/0162352</doc-number>
<kind>A1</kind>
<name>Gizewski</name>
<date>20080700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>705 50</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>WO</country>
<doc-number>0219237</doc-number>
<kind>A1</kind>
<date>20020300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>WO</country>
<doc-number>2004012031</doc-number>
<kind>A2</kind>
<date>20040200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>WO</country>
<doc-number>2004051415</doc-number>
<kind>A2</kind>
<date>20040600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>WO</country>
<doc-number>2005089481</doc-number>
<kind>A2</kind>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>WO</country>
<doc-number>2006060626</doc-number>
<kind>A2</kind>
<date>20060600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>15</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>725 34</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>725 10</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>725 42</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>705  2</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>705  3</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>4</number-of-drawing-sheets>
<number-of-figures>4</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20110072457</doc-number>
<kind>A1</kind>
<date>20110324</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Lanfermann</last-name>
<first-name>Gerd</first-name>
<address>
<city>Aachen</city>
<country>DE</country>
</address>
</addressbook>
<residence>
<country>DE</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Weidenhaupt</last-name>
<first-name>Klaus</first-name>
<address>
<city>Wassenberg</city>
<country>DE</country>
</address>
</addressbook>
<residence>
<country>DE</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Willmann</last-name>
<first-name>Richard Daniel</first-name>
<address>
<city>Siegburg</city>
<country>DE</country>
</address>
</addressbook>
<residence>
<country>DE</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Lanfermann</last-name>
<first-name>Gerd</first-name>
<address>
<city>Aachen</city>
<country>DE</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Weidenhaupt</last-name>
<first-name>Klaus</first-name>
<address>
<city>Wassenberg</city>
<country>DE</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Willmann</last-name>
<first-name>Richard Daniel</first-name>
<address>
<city>Siegburg</city>
<country>DE</country>
</address>
</addressbook>
</inventor>
</inventors>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Koninklijke Philips N.V.</orgname>
<role>03</role>
<address>
<city>Eindhoven</city>
<country>NL</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Rabovianski</last-name>
<first-name>Jivka</first-name>
<department>2426</department>
</primary-examiner>
</examiners>
<pct-or-regional-filing-data>
<document-id>
<country>WO</country>
<doc-number>PCT/IB2008/053328</doc-number>
<kind>00</kind>
<date>20080820</date>
</document-id>
<us-371c124-date>
<date>20101118</date>
</us-371c124-date>
</pct-or-regional-filing-data>
<pct-or-regional-publishing-data>
<document-id>
<country>WO</country>
<doc-number>WO2009/024929</doc-number>
<kind>A </kind>
<date>20090226</date>
</document-id>
</pct-or-regional-publishing-data>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A system for displaying selected information to a person undertaking exercises includes a physical data assessment unit, a physical data gathering unit, and a motion template database. The physical data gathering unit and the motion template database are in communication with the physical data assessment unit. The system further includes an impairment profile generator, an information database including audiovisual information to be displayed to the person according to the impairment profile, and an audiovisual display unit. The information database and the audiovisual display unit are in communication with the impairment profile generator and the impairment profile generator is in communication with the physical data assessment unit. Information such as advertisements is displayed on display to the person where advertisement selection rules are based on Bayesian inference.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="118.36mm" wi="175.68mm" file="US08627355-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="172.30mm" wi="114.64mm" orientation="landscape" file="US08627355-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="172.30mm" wi="155.19mm" orientation="landscape" file="US08627355-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="174.92mm" wi="145.63mm" orientation="landscape" file="US08627355-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="201.34mm" wi="92.29mm" orientation="landscape" file="US08627355-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0002" num="0001">The present invention concerns a system and a method for displaying selected information to a person undertaking exercises.</p>
<p id="p-0003" num="0002">Personalized advertisement for television, radio or the internet is a key area of revenue for many content delivery businesses. The quality of personalized content is essential and a multitude of systems for generating personalized content are known.</p>
<p id="p-0004" num="0003">There is a very special situation in the life of a consumer: while he is a patient. In this situation the patient is often confronted with the necessity of adapting a different lifestyle or of making use of rehabilitation aids such canes, wheel chairs and the like. In such a circumstance the patient is likely to welcome personalized and targeted advertisements, since this content is addressing a real need for information.</p>
<p id="p-0005" num="0004">The condition of the patient may change over time. That means that the advertisement content must change as the condition improves or deteriorates. Otherwise the advertisement is no longer perceived to be personal.</p>
<p id="p-0006" num="0005">Personalized advertisements are often based on a profile of a user. An example is given in US 2007/00088603 A1, dealing with a method for targeted data delivery, said method comprising accessing a user profile associated with said user, wherein said user profile is used to target delivery of data to said user based on said user profile without requiring a release of any information in said user profile and weighting selected items in said user profile to determine a first score for said user profile, wherein said user is eligible to be presented with a first offer of data provided said first score satisfies a first threshold.</p>
<p id="p-0007" num="0006">However, the actual development of the user's condition over time is not used to generate a personalized advertisement content.</p>
<heading id="h-0002" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0008" num="0007">It will be apparent from the above that a need still exists in the art for generating personalized information directed to a patient which more closely addresses the patient's situation and which adapts to the changing condition of the patient. It is therefore an object of the invention to provide a system capable of this.</p>
<p id="p-0009" num="0008">To achieve this and other objects, the present invention is directed to a system for displaying selected information to a person undertaking exercises comprising:
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0009">an physical data assessment unit;</li>
        <li id="ul0002-0002" num="0010">a physical data gathering unit;</li>
        <li id="ul0002-0003" num="0011">a motion template database;
<br/>
the physical data gathering unit and the motion template database being in communication with the physical data assessment unit; the system further comprising:
</li>
        <li id="ul0002-0004" num="0012">an impairment profile generator;</li>
        <li id="ul0002-0005" num="0013">an information database comprising audiovisual information to be displayed to the person according to the impairment profile;</li>
        <li id="ul0002-0006" num="0014">an audiovisual display unit;
<br/>
the information database and the audiovisual display unit being in communication with the impairment profile generator and the impairment profile generator being in communication with the physical data assessment unit.
</li>
    </ul>
    </li>
</ul>
</p>
<heading id="h-0003" level="1">DETAILED DESCRIPTION OF THE INVENTION</heading>
<p id="p-0010" num="0015">Before the invention is described in detail, it is to be understood that this invention is not limited to the particular component parts of the devices described or process steps of the methods described as such devices and methods may vary. It is also to be understood that the terminology used herein is for purposes of describing particular embodiments only, and is not intended to be limiting. It must be noted that, as used in the specification and the appended claims, the singular forms &#x201c;a,&#x201d; &#x201c;an&#x201d; and &#x201c;the&#x201d; include singular and/or plural referents unless the context clearly dictates otherwise.</p>
<p id="p-0011" num="0016">Displaying selected information according to the present information is to be understood as conveying audiovisual information to a person. The information is selected for being interesting, useful or beneficial to the person. Information can mean audio files, video files or combined audio video files. A person undertaking exercises can be a patient in rehabilitation after suffering from a stroke or the like. The exercises are then those prescribed by a therapist to perform with or without supervision.</p>
<p id="p-0012" num="0017">The person is monitored by a physical data gathering unit. This can be undertaken either only while the person is conducting rehabilitation exercises or continuously. The physical data gathering unit translates signals from, for example, sensors on the person's body, into representations of the person's physical state, for example the person's posture, movement, cardiovascular fitness, and the like. The exercise to be conducted can be displayed on an exercise display unit such as a television or computer screen. The individual exercise is stored in a motion template database. The exercise display unit can display the required exercise in the form of an avatar. Additionally, a representation of the person's posture can also be displayed there in order to provide visual feedback whether the person is exercising correctly or not.</p>
<p id="p-0013" num="0018">With respect to the connection of the individual units, the physical data gathering unit and the physical data template database are in communication with the physical data assessment unit. When an exercise display unit is present, it is also in communication with the exercise assessment unit. The communication can either be achieved via integration of components into one system, via a wired connection, wirelessly or in a body area network using the electrical conductivity of the human body.</p>
<p id="p-0014" num="0019">A physical data assessment unit is used to compare the physical data from the person to the motion template of the exercise the person should be doing. The deviations are recorded.</p>
<p id="p-0015" num="0020">The system according to the present invention further comprises an impairment profile generator. This unit calculates a personalized impairment profile of the person undertaking the exercises. The profile can be based upon motor assessments, such as the deviation of the performed exercises from a given motion template. It can also take into account the cardiovascular fitness, as obtainable from blood pressure and pulse readings or results of a continuous monitoring of the person, such as how fast or to which extent a person can generally move a limb. The impairment profile may also be based on how well the person keeps his balance. Further input for the profile may come from cognitive performance results, for example when the person has undertaken memory, speech exercises or questionnaires about the amount of pain the person is perceiving.</p>
<p id="p-0016" num="0021">An information database comprising audiovisual information to be displayed to the person is also part of the system according to the invention. According to the impairment profile, audiovisual information is selected. The information can be in the form of audio clips, video clips or audiovisual clips. The information can relate to the person in the form of specialized advertisements. Selection rules can be deterministic rules such as selecting advertisements for a certain product if a mobility score is in a certain numerical range and for another product if a mobility score is in a different numerical range.</p>
<p id="p-0017" num="0022">A more advanced embodiment can have selection rules based on a probabilistic model of the usefulness of products for certain impairments. An example for such a model is a Bayesian network. In general, Bayesian networks are probabilistic graphical models that represent a set of variables and their probabilistic dependencies. Formally, Bayesian networks are directed acyclic graphs whose nodes represent variables, and whose arcs encode conditional dependencies between the variables.</p>
<p id="p-0018" num="0023">Within a Bayesian network, if there is an arc from a first node A to a second node B, A is called a parent of B and B is a child of A. The set of parent nodes of a node X<sub>i </sub>is denoted by parents(X<sub>i</sub>). The arcs from the child nodes to the parent nodes represent weak causal relationships and are modeled as local conditional probability distributions. If node X<sub>i </sub>has no parents, its local probability distribution is said to be unconditional; otherwise it is conditional.</p>
<p id="p-0019" num="0024">From the structure of a Bayesian network and the local probability tables attached to each node, any probability of the variables being in a certain combination of states can efficiently be calculated using the following formula, which is based on Bayes' theorem:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i>(<i><u style="single">X</u></i>)=<i>P</i>(<i>X</i><sub>1</sub><i>=x</i><sub>1</sub><i>, X</i><sub>2</sub><i>=x</i><sub>2</sub>, . . . ,X<sub>n</sub><i>=x</i><sub>n</sub><i>;&#x2200;x</i><sub>1 </sub><i>. . . x</i><sub>n</sub>)=&#x3a0;<sub>i=1 . . . n</sub><i>P</i>(<i>X</i><sub>i</sub>|parents(<i>X</i><sub>i</sub>))<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0020" num="0025">This formula forms the basis for the computational method called Bayesian inference or belief updating. It can be employed whenever observation nodes in a Bayesian network are instantiated.</p>
<p id="p-0021" num="0026">In a network according to the invention, information to be displayed and impairment characteristics are modelled as so-called target and observation nodes which are connected to each other via conditional probability tables. Using Bayesian inference the a-posteriori probability of certain information being useful for a current instantiation of observation nodes (in other words, the impairment profile) will be calculated. All the information with a usefulness probability higher than a certain threshold, for example higher than 80%, 85%, 90% or 95%, can then be selected for the personalized advertisement content. Whenever the impairment profile changes, the usefulness probabilities will be recalculated and the advertising content modified accordingly. For this computation, several exact as well as approximate algorithms are known, for example variable elimination, clique tree propagation, recursive conditioning and stochastic MCMC simulation. The audiovisual display unit is used to show or play back the content to the person.</p>
<p id="p-0022" num="0027">With respect to the interaction of these units, the information database and the audiovisual display unit are in communication with the impairment profile generator and the impairment profile generator is in communication with the physical data assessment unit. Therefore, the input from the physical data assessment unit serves to calculate an impairment profile.</p>
<p id="p-0023" num="0028">As already mentioned, the updating of the impairment profile according to the present invention and the selection of appropriate information allows the adaption of the information presented to the patient in changing conditions. For example, a patient may first need a wheelchair. After he gets better, a wheelchair advertisement would not be considered personalized any more and instead an advertisement for a walking stick would be presented to him.</p>
<p id="p-0024" num="0029">Within the scope of the present invention it is possible that the system further comprises an additional database comprising the person's data selected from the group comprising the medical history of the person, the medical state of the person and/or the viewing history of the audiovisual display unit. This database is in connection with the impairment profile generator so that a more detailed impairment profile can be calculated. The medical history and the medical state may comprise data relating to electromyograms (EMG), dietary needs, medication used, pulse rate, blood pressure, blood oxygen content, blood sugar content, severity of perspiration, respiratory rate and/or perceived severity of pain. For example, it is then easier to express how exhausted a person is after performing exercises.</p>
<p id="p-0025" num="0030">In a preferred embodiment of the present invention the physical data assessment unit (<b>1</b>) is an exercise assessment unit and the physical data gathering unit (<b>2</b>) is a posture assessment unit. With this, the system according to the invention focuses on physical exercises of the person as they are most beneficial during rehabilitation.</p>
<p id="p-0026" num="0031">In a further preferred embodiment of the present invention the system further comprises&#x2014;at least one motion sensor on the person undertaking exercises, the sensor being selected from the group comprising acceleration sensors, inertia sensors and/or gravity sensors; wherein
<ul id="ul0003" list-style="none">
    <li id="ul0003-0001" num="0000">
    <ul id="ul0004" list-style="none">
        <li id="ul0004-0001" num="0032">the at least one motion sensor transmits its signals to the physical data gathering unit; and</li>
        <li id="ul0004-0002" num="0033">the physical data gathering unit calculates a representation of the person's posture based on the signals of the at least one motion sensor.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0027" num="0034">The motion sensors can be worn on the body of the person on selected locations like upper arm, lower arm, upper leg, lower leg or torso. They can be commercially available highly integrated solid state sensors. The transmission of the sensor signals to the posture assessment unit can be undertaken via wire, wirelessly or in a body area network using the electrical conductivity of the human skin. After calculation of the posture the result can be displayed in the form of an avatar on the exercise display.</p>
<p id="p-0028" num="0035">In a further preferred embodiment of the present invention the physical data gathering unit comprises at least one optical mark on the person undertaking exercises, the physical data gathering unit comprises an optical tracking system for tracking the at least one optical mark and the physical data gathering unit calculates a representation of the person's posture based on the signals of the optical tracking system. The optical marks can be borne on the body of the person on selected locations like upper arm, lower arm, upper leg, lower leg or torso. The tracking of the marks can be effected with a single camera or a multitude of cameras. When a stereo camera is used, three-dimensional posture and movement data is generated. After image processing and calculation of the person's posture the result can be displayed in the form of an avatar on the exercise screen.</p>
<p id="p-0029" num="0036">It is also possible to combine several posture monitoring principles. For example, a combination of motion sensors and optical tracking may provide complementary data to better calculate the posture of the person.</p>
<p id="p-0030" num="0037">The present invention is also directed to a method for displaying selected information to a person undertaking exercises, comprising the steps of:
<ul id="ul0005" list-style="none">
    <li id="ul0005-0001" num="0038">a) gathering physical data from the person undertaking exercises;</li>
    <li id="ul0005-0002" num="0039">b) calculating the deviation of the physical data from a template stored in a motion template database;</li>
    <li id="ul0005-0003" num="0040">c) calculating an impairment profile of the person;</li>
    <li id="ul0005-0004" num="0041">d) selecting audiovisual information stored in an audiovisual information database by applying selection rules based on the calculated impairment profile;</li>
    <li id="ul0005-0005" num="0042">e) displaying the selected audiovisual information on a display unit.</li>
</ul>
</p>
<p id="p-0031" num="0043">The individual steps have been discussed above with reference to the system according to the present invention.</p>
<p id="p-0032" num="0044">In a preferred embodiment of the method according to the present invention the physical data from the person is selected from the group comprising motion data, posture data, electromyographic data, dietary needs, medication used, pulse rate, blood pressure, blood oxygen content, blood sugar content, severity of perspiration, respiratory rate and/or perceived severity of pain and the physical data is used to calculate the impairment profile. This has already been discussed with reference to the system according to the present invention.</p>
<p id="p-0033" num="0045">In a further preferred embodiment of the method according to the present invention a graphical representation of the person's posture and an exercise according to a motion template are displayed on an exercise display unit. This has already been discussed with reference to the system according to the present invention.</p>
<p id="p-0034" num="0046">In a further preferred embodiment of the method according to the present invention the audiovisual information in step d) is a target node in a Bayesian network, the impairment profile comprises one or more observation nodes in a Bayesian network and the audiovisual information is selected according to its probability in the Bayesian network. This has already been discussed with reference to the system according to the present invention.</p>
<p id="p-0035" num="0047">It is advantageous to perform the method according to the present invention using a system according to the present invention.</p>
<p id="p-0036" num="0048">The present invention is furthermore directed to the use of a system according to the present invention for displaying selected information to a person undertaking exercises.</p>
<p id="p-0037" num="0049">To provide a comprehensive disclosure without unduly lengthening the specification, the applicant hereby incorporates by reference each of the patents and patent applications referenced above.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0038" num="0050">The present invention will become more readily understood with reference to the following drawing, wherein</p>
<p id="p-0039" num="0051"><figref idref="DRAWINGS">FIG. 1</figref> shows a system according to the present invention</p>
<p id="p-0040" num="0052"><figref idref="DRAWINGS">FIG. 2</figref> shows a Bayesian network</p>
<p id="p-0041" num="0053"><figref idref="DRAWINGS">FIG. 3</figref> shows a further Bayesian network</p>
<p id="p-0042" num="0054"><figref idref="DRAWINGS">FIG. 4</figref> shows a screenshot of part of a conditional probability table</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0043" num="0055"><figref idref="DRAWINGS">FIG. 1</figref> shows a system according to the present invention for displaying selected information to a person. The person has motion sensors (<b>8</b>) situated on his thighs and his ankles Optical marks (<b>9</b>) are located on the wrists and the torso. The signals of the motion sensors (<b>8</b>) are transmitted wirelessly to the posture assessment unit (<b>2</b>). The posture assessment unit (<b>2</b>) further comprises an optical tracking system for identifying the position of the optical marks (<b>9</b>).</p>
<p id="p-0044" num="0056">According to an exercise stored in a motion template database (<b>4</b>) a first avatar, represented as drawn in dashed lines, is displayed on the exercise display unit (<b>3</b>). The person performs the movements as indicated by the avatar. A second avatar, represented as drawn in solid lines, shows the posture of the person. By comparing this to the first avatar, the person is able to correct his movements and to perform the exercise more correctly.</p>
<p id="p-0045" num="0057">The exercise assessment unit (<b>1</b>) receives data from the posture assessment unit (<b>2</b>) and the motion template database (<b>4</b>) and calculates how much the movements of the person deviate from the ideal movement of the motion template stored in database (<b>4</b>). This deviation information is passed on to the impairment profile generator (<b>5</b>). Using additional data such as medication used, pulse rate, blood pressure, blood oxygen content, blood sugar content, severity of perspiration and/or respiratory rate an impairment profile is calculated.</p>
<p id="p-0046" num="0058">Based on selection rules, information in the form of an advertisement audiovisual clip is selected from the corresponding information database (<b>6</b>). This advertisement is then displayed on the audiovisual display unit (<b>7</b>). In this case, it is an advertisement for a walking cane.</p>
<p id="p-0047" num="0059"><figref idref="DRAWINGS">FIG. 2</figref> shows a Bayesian network used to model the probabilities for either a wheelchair, a walker or a cane being useful to a person given a certain impairment profile of this person. These variables are modelled as target nodes and can adopt either the state of useful or not useful. An impairment profile is defined by the three variables blood pressure, mobility score and perceived pain and their respective states. These nodes are observation nodes, meaning that their actual states can be observed. There are arcs from every observation node to every target node.</p>
<p id="p-0048" num="0060">With respect to the blood pressure, it is categorized into three sections of high, normal and low blood pressure. By way of definition, a high blood pressure may be present at above 140/90 min Hg. A low blood pressure may be present at systolic pressure values of under 105 mm Hg. With respect to the mobility score, it is also categorized into high, medium and low mobility. The third variable is the perceived pain of the person in question. This information can be obtained via a questionnaire.</p>
<p id="p-0049" num="0061">Each of the states of the variables in the diagram of <figref idref="DRAWINGS">FIG. 2</figref> has been assigned a certain probability. In this diagram, nothing is known about the blood pressure, impairment profile and pain perception of the person. In other words, the corresponding observation nodes are uninstantiated. Therefore, a priori probabilities are assumed. As a result, the probability of the wheelchair being useful is highest with 77%, however the walker may be useful to the person with a probability of 16% and the cane may be useful with a probability of 17%.</p>
<p id="p-0050" num="0062"><figref idref="DRAWINGS">FIG. 3</figref> shows the same Bayesian network as <figref idref="DRAWINGS">FIG. 2</figref>, the difference being that the impairment profile of the person is now known and subsequently the probabilities being recalculated. It is now known that the person has a low blood pressure. Therefore, the probability of this observation node adopting the state of low blood pressure is 100%. Furthermore, the person has a low mobility score, corresponding to the probability of this observation node adopting the state of low mobility being 100%. Finally, the person's perceived pain is low, corresponding to the probability of this observation node adopting the state of low perceived pain being 100%. As the impairment profile of the person is known, the corresponding nodes are instantiated. This means that the a priori probabilities are overridden by the observed evidence. In turn this leads to a recalculation of the probability distribution of all non-instantiated nodes in the Bayesian network. As a result of the Bayesian inference, the probability of the wheelchair being useful is 95%, of the walker being useful is 5% and the cane is being useful is 0%. Therefore, a wheelchair will be presented to the person.</p>
<p id="p-0051" num="0063"><figref idref="DRAWINGS">FIG. 4</figref> is a screenshot of a computer application modelling the Bayesian network of <figref idref="DRAWINGS">FIGS. 2 and 3</figref>. The situation is after recalculation of the probability distributions, therefore displaying the status as in <figref idref="DRAWINGS">FIG. 3</figref>. The screenshot shows part of the conditional table that defines the causal relationship between the wheelchair node and the parent nodes of the impairment profile.</p>
<p id="p-0052" num="0064">The particular combinations of elements and features in the above detailed embodiments are exemplary only; the interchanging and substitution of these teachings with other teachings in this and the patents/applications incorporated by reference are also expressly contemplated. As those skilled in the art will recognize, variations, modifications, and other implementations of what is described herein can occur to those of ordinary skill in the art without departing from the spirit and the scope of the invention as claimed. Accordingly, the foregoing description is by way of example only and is not intended as limiting. The invention's scope is defined in the following claims and the equivalents thereto. Furthermore, reference signs used in the description and claims do not limit the scope of the invention as claimed.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A system for displaying selected information to a person undertaking exercises comprising:
<claim-text>a physical data assessment unit;</claim-text>
<claim-text>a physical data gathering unit;</claim-text>
<claim-text>a motion template database;</claim-text>
<claim-text>the physical data gathering unit and the motion template database being in communication with the physical data assessment unit;</claim-text>
<claim-text>the system further comprising:</claim-text>
<claim-text>an impairment profile generator;</claim-text>
<claim-text>an information database comprising audiovisual information to be displayed to the person according to an impairment profile generated by the impairment profile generator;</claim-text>
<claim-text>an audiovisual display unit;</claim-text>
<claim-text>the information database and the audiovisual display unit being in communication with the impairment profile generator and the impairment profile generator being in communication with the physical data assessment unit,</claim-text>
<claim-text>wherein the physical data gathering unit is configured to calculate an actual posture of the person undertaking exercises for display of a representation of the person undertaking exercises on the audiovisual display unit as a displayed representation,</claim-text>
<claim-text>wherein the physical data assessment unit is configured to form a determined deviation by determining a deviation between a movement of the displayed representation of the person undertaking exercises displayed on the audiovisual display unit and a template representation of a reference person displayed on the audiovisual display unit undergoing a predetermined movement in accordance with the template stored in the motion template database,</claim-text>
<claim-text>wherein the impairment profile generator is configured to update the impairment profile of the person to form an updated profile based on the determined deviation, and</claim-text>
<claim-text>wherein the physical data assessment unit is further configured to select information from the audiovisual information to form selected information for display on the audiovisual display unit based on the updated profile.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the physical data assessment unit is an exercise assessment unit and the physical data gathering unit is a posture assessment unit.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>at least one motion sensor on the person undertaking exercises, the sensor being selected from the group comprising acceleration sensors, inertia sensors and/or gravity sensors; wherein</claim-text>
<claim-text>the at least one motion sensor transmits its signals to the physical data gathering unit; and</claim-text>
<claim-text>the physical data gathering unit calculates the representation of the person's posture based on the signals of the at least one motion sensor.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein
<claim-text>the physical data gathering unit comprises at least one optical mark on the person undertaking exercises;</claim-text>
<claim-text>the physical data gathering unit comprises an optical tracking system for tracking the at least one optical mark; and</claim-text>
<claim-text>the physical data gathering unit calculates a representation of the person's posture based on the signals of the optical tracking system.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the selected information comprises advertisement related to a physical condition of the person undertaking exercises.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the representation is generated based on tracking of optical sensors on the person undertaking exercises.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the representation is generated based on signals of at least one motion sensor located on the person undertaking exercises.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the impairment profile generator updates the impairment profile of the person to include values of blood pressure, mobility and perceived pain of the person undertaking exercises detected by sensors, and wherein the values are used to calculate probabilities that the person needs a wheelchair, a walker and a cane.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. A method for displaying selected information to a person undertaking exercises, comprising the acts of:
<claim-text>displaying on a display unit a representation of the person undertaking exercises as a displayed representation and a template representation of a reference person undergoing a predetermined movement in accordance with a template stored in a motion template database;</claim-text>
<claim-text>gathering physical data from the person undertaking exercises including a movement of the displayed representation of the person undertaking exercises displayed on the display unit;</claim-text>
<claim-text>calculating deviation of the physical data from the template stored in the motion template database to form a calculated deviation;</claim-text>
<claim-text>calculating an impairment profile of the person based on the calculated deviation to form a calculated impairment profile;</claim-text>
<claim-text>selecting audiovisual information stored in an audiovisual information database to form selected audiovisual information by applying selection rules based on the calculated impairment profile;</claim-text>
<claim-text>displaying the selected audiovisual information on the display unit.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the physical data from the person further comprises at least one of posture data, electromyographic data, dietary needs, medication used, pulse rate, blood pressure, blood oxygen content, blood sugar content, severity of perspiration, respiratory rate and/or perceived severity of pain and wherein the physical data is used to calculate the impairment profile.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the selected audiovisual information is a target node in a Bayesian network, and wherein the impairment profile comprises one or more observation nodes in a Bayesian network and the audiovisual information is selected according to its probability in the Bayesian network.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the selected audiovisual information comprises advertisement related to a physical condition of the person undertaking exercises.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the representation is generated based on tracking of optical sensors on the person undertaking exercises.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the representation is generated based on signals of at least one motion sensor located on the person undertaking exercises.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the impairment profile of the person includes values of blood pressure, mobility and perceived pain of the person undertaking exercises detected by sensors, the method further comprising the act of using the values to calculate probabilities that the person needs a wheelchair, a walker and a cane. </claim-text>
</claim>
</claims>
</us-patent-grant>
