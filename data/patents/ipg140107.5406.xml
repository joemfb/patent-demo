<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626506-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626506</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>11336081</doc-number>
<date>20060120</date>
</document-id>
</application-reference>
<us-application-series-code>11</us-application-series-code>
<us-term-of-grant>
<us-term-extension>2643</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20130101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>10</class>
<subclass>L</subclass>
<main-group>15</main-group>
<subgroup>06</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>704243</main-classification>
<further-classification>704251</further-classification>
<further-classification>704254</further-classification>
<further-classification>704255</further-classification>
</classification-national>
<invention-title id="d2e53">Method and system for dynamic nametag scoring</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>4731811</doc-number>
<kind>A</kind>
<name>Dubus</name>
<date>19880300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>4776016</doc-number>
<kind>A</kind>
<name>Hansen</name>
<date>19881000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5476010</doc-number>
<kind>A</kind>
<name>Fleming et al.</name>
<date>19951200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5805672</doc-number>
<kind>A</kind>
<name>Barkat et al.</name>
<date>19980900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5832440</doc-number>
<kind>A</kind>
<name>Woodbridge</name>
<date>19981100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6073099</doc-number>
<kind>A</kind>
<name>Sabourin et al.</name>
<date>20000600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>7042566</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6112103</doc-number>
<kind>A</kind>
<name>Puthuff</name>
<date>20000800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6122612</doc-number>
<kind>A</kind>
<name>Goldberg</name>
<date>20000900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704231</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6223158</doc-number>
<kind>B1</kind>
<name>Goldberg</name>
<date>20010400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704252</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6256611</doc-number>
<kind>B1</kind>
<name>Ali-Yrkko</name>
<date>20010700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6289140</doc-number>
<kind>B1</kind>
<name>Oliver</name>
<date>20010900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6438520</doc-number>
<kind>B1</kind>
<name>Curt et al.</name>
<date>20020800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>6505780</doc-number>
<kind>B1</kind>
<name>Yassin et al.</name>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>6587824</doc-number>
<kind>B1</kind>
<name>Everhart et al.</name>
<date>20030700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>6684185</doc-number>
<kind>B1</kind>
<name>Junqua et al.</name>
<date>20040100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704243</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>6690772</doc-number>
<kind>B1</kind>
<name>Naik et al.</name>
<date>20040200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>6735632</doc-number>
<kind>B1</kind>
<name>Kiraly et al.</name>
<date>20040500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>6738738</doc-number>
<kind>B2</kind>
<name>Henton</name>
<date>20040500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>6804806</doc-number>
<kind>B1</kind>
<name>Bansal</name>
<date>20041000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>6836758</doc-number>
<kind>B2</kind>
<name>Bi et al.</name>
<date>20041200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>7110949</doc-number>
<kind>B2</kind>
<name>Bushey et al.</name>
<date>20060900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704251</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>7181398</doc-number>
<kind>B2</kind>
<name>Thong et al.</name>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704254</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>7292978</doc-number>
<kind>B2</kind>
<name>Endo et al.</name>
<date>20071100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704243</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>7299179</doc-number>
<kind>B2</kind>
<name>Block et al.</name>
<date>20071100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704254</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>7315818</doc-number>
<kind>B2</kind>
<name>Stevens et al.</name>
<date>20080100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704235</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>7467087</doc-number>
<kind>B1</kind>
<name>Gillick et al.</name>
<date>20081200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704260</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>2003/0083873</doc-number>
<kind>A1</kind>
<name>Ross et al.</name>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>2003/0120493</doc-number>
<kind>A1</kind>
<name>Gupta</name>
<date>20030600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>2005/0010412</doc-number>
<kind>A1</kind>
<name>Aronowitz</name>
<date>20050100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704254</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>2005/0182558</doc-number>
<kind>A1</kind>
<name>Maruta</name>
<date>20050800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704246</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>US</country>
<doc-number>2007/0073539</doc-number>
<kind>A1</kind>
<name>Chengalvarayan et al.</name>
<date>20070300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00032">
<othercit>Ng et al. &#x201c;Recent Advances in Cantonese Speech Recognition,&#x201d; Proceedings of International Symposium on Multi-Technology Information Processing, 1996, pp. 139-144.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00033">
<othercit>Ververidis et al. &#x201c;Automatic speech classification to five emotional states based on gender information&#x201d;. In Proc. 12th European Signal ProcessingConference, Sep. 2004, pp. 341-344.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00034">
<othercit>U.S. Appl. No. 10/444,270, filed May 23, 2003, Arun.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00035">
<othercit>U.S. Appl. No. 11/014,497, filed Dec. 16, 2004, Chengalvarayan et al.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00036">
<othercit>&#x201c;Robust HMM Training for Unified Dutch and German Speech Recognition&#x201d;, Rathi Chengalvarayan, Lucent Speech Solutions, Lucent Technologies Inc., pp. 509-512.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00037">
<othercit>R. Chengalvarayan and L. Deng, &#x201c;HMM-Based Speech Recognition Using State-Dependent, Discriminatively Derived Transforms on Mel-Warped DFT Features&#x201d;, IEEE Transactions on Speech and Audio Processing, vol. 5, No. 3, pp. 243-256, May 1997.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00038">
<othercit>P. Lucey, T. Martin and S. Sridharan, &#x201c;Confusability of Phonemes Grouped According to Their Viseme Classes in Noisy Environments&#x201d;, 10th Australian International Conference on Speech Science and Technology, pp. 265-270, Dec. 2004.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>13</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>704243</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704251-252</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704254-255</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>8</number-of-drawing-sheets>
<number-of-figures>10</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20070174055</doc-number>
<kind>A1</kind>
<date>20070726</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Chengalvarayan</last-name>
<first-name>Rathinavelu</first-name>
<address>
<city>Naperville</city>
<state>IL</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Correia</last-name>
<first-name>John J.</first-name>
<address>
<city>Livonia</city>
<state>MI</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Chengalvarayan</last-name>
<first-name>Rathinavelu</first-name>
<address>
<city>Naperville</city>
<state>IL</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Correia</last-name>
<first-name>John J.</first-name>
<address>
<city>Livonia</city>
<state>MI</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<last-name>Simon</last-name>
<first-name>Anthony Luke</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
<agent sequence="02" rep-type="attorney">
<addressbook>
<orgname>Reising Ethington P.C.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>General Motors LLC</orgname>
<role>02</role>
<address>
<city>Detroit</city>
<state>MI</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Chawan</last-name>
<first-name>Vijay B</first-name>
<department>2658</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A method for dynamic nametag scoring includes receiving at least one confusion table including at least one circumstantial condition wherein the confusion table is based on a plurality of phonetically balanced utterances, determining a plurality of templates for the nametag based on the received confusion tables, and determining a global nametag score for the nametag based on the determined templates. A computer usable medium with suitable computer program code is employed for dynamic nametag scoring.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="189.31mm" wi="139.36mm" file="US08626506-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="216.83mm" wi="136.31mm" file="US08626506-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="182.03mm" wi="164.76mm" file="US08626506-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="212.43mm" wi="133.35mm" file="US08626506-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="93.39mm" wi="121.67mm" file="US08626506-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="196.17mm" wi="143.17mm" file="US08626506-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="91.02mm" wi="127.42mm" file="US08626506-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="160.70mm" wi="127.34mm" file="US08626506-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="215.14mm" wi="149.94mm" file="US08626506-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">FIELD OF THE INVENTION</heading>
<p id="p-0002" num="0001">This invention relates generally to speech recognition systems. In particular the invention relates to a method and system for dynamic nametag scoring.</p>
<heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">Various schemes have been developed to improve the performance of speech recognition systems. Many factors interact to degrade the performance of speech recognition systems in mobile vehicles. Factors such as ambient noise conditions, cabin design variables, speaker gender, and speaker dialect interact to influence the acoustic signal received by the speech recognition signal. These factors cause decoding errors and false alarms thereby increasing user frustration with the system.</p>
<p id="p-0004" num="0003">Current speech recognition systems use a single template for each nametag. The template is used to match the nametag utterance received from the user with the proper nametag in the system. The template is created during system setup by receiving multiple utterances for each nametag and storing the correctly identified nametag in the template. The template will be based on the speaker, vehicle and environmental conditions that exist when it is created. User frustration occurs when multiple unsuccessful attempts to match the utterance with the nametag occur. These systems cannot adapt to new speakers or speaking scenarios without retraining of the system.</p>
<p id="p-0005" num="0004">A user can train the system under a variety of conditions and store a different template for each scenario. This requires additional user involvement and increases training time required by the system. Furthermore, the templates are not differentiated and probability of selecting a proper template is not increased.</p>
<p id="p-0006" num="0005">Multiple speech recognition engines run in parallel can be used to increase the likelihood of selecting the proper template as described in U.S. Pat. No. 6,836,758 to Bi, et al. This method produces more accurate nametag recognition but still does not account for variations in speaker, vehicle and environment nor does it adapt to changes in the acoustic signal without retraining. In addition, increased computational power and storage capacity is required to accommodate the additional speech recognition engines.</p>
<p id="p-0007" num="0006">A method for biasing paths in a Markov model is proposed in U.S. Pat. No. 4,741,036 to Bahl, et al. The individual phones that distinguish similar words are given more weight to emphasize the differences between the words. This method improves the distinction between similar words but does not account for the effects of ambient noise and speaker variables such as dialect or gender. Additionally the weighting vectors are static and determined at the time a system is trained. The weighting vectors are only updated to account for changes in speech input by retraining the system.</p>
<p id="p-0008" num="0007">It is therefore desirable to provide a method and system for dynamic nametag scoring that overcomes the limitations, challenges, and obstacles described above.</p>
<heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0009" num="0008">One aspect of the invention provides a method for dynamic nametag scoring. The method includes receiving at least one confusion table including at least one circumstantial condition wherein the confusion table is based on a plurality of phonetically balanced utterances, determining a plurality of templates for the nametag based on the received confusion tables, and determining a global nametag score for the nametag based on the determined templates.</p>
<p id="p-0010" num="0009">Another aspect of the invention provides a computer readable medium storing a computer program including computer program code for dynamic nametag scoring. The medium includes computer program code for receiving at least one confusion table including at least one circumstantial condition wherein the confusion table is based on a plurality of phonetically balanced utterances, computer program code for determining a plurality of templates for the nametag based on the received confusion tables, and computer program code for determining a global nametag score for the nametag based on the determined templates.</p>
<p id="p-0011" num="0010">A third aspect of the invention provides a system for dynamic nametag scoring. The system includes means for receiving at least one confusion table including at least one circumstantial condition wherein the confusion table is based on a plurality of phonetically balanced utterances, means for determining a plurality of templates for the nametag based on the received confusion table, and means for determining a global nametag score for the nametag based on the determined templates.</p>
<p id="p-0012" num="0011">A fourth aspect of the invention provides a method for determining a plurality of confusion tables including receiving a plurality of phonetically balanced utterances, generating a plurality of component sequences based on the test utterances and at least one circumstantial condition, determining a confusion matrix based on the component sequences and the test utterances, and generating a confusion table based on the confusion matrix.</p>
<p id="p-0013" num="0012">The aforementioned and other features and advantages of the invention will become further apparent from the following detailed description of the presently preferred embodiments, read in conjunction with the accompanying drawings. The detailed description and drawings are merely illustrative of the invention rather than limiting, the scope of the invention being defined by the appended claims and equivalents thereof.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 1</figref> illustrates a flowchart representative of one example of a method for dynamic nametag scoring, in accordance with the invention;</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 2</figref> illustrates a flowchart representative of one example of a method for determining the plurality of templates, in accordance with the invention;</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 3</figref> illustrates a schematic of one example of a system for generating a plurality of templates, in accordance with the invention;</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 4</figref> illustrates a flowchart representative of one example of a method for selecting templates, in accordance with the invention;</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 5</figref> illustrates a flowchart representative of one example of a method for determining the global nametag score, in accordance the invention;</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 6</figref> illustrates a flowchart representative of one example of a method for calculating the weight for a template, in accordance with the invention;</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 7</figref> illustrates a flowchart representative of one example of a method for dynamic learning of weights, in accordance with the invention;</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 8</figref> illustrates a flowchart representative of one example of a method for determining at least one confusion table, in accordance with the invention;</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 9</figref> illustrates a schematic of one example of a system for determining at least one confusion table, in accordance with the invention; and</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 10</figref> illustrates a schematic of one example of a system for dynamic nametag scoring, in accordance with the invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION OF THE PRESENTLY PREFERRED EXAMPLES</heading>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 1</figref> illustrates a flowchart representative of one example of a method for dynamic nametag scoring, in accordance with the invention at <b>10</b>. A nametag template is a series of feature vectors representing a command, proper name, or other word used by a speech recognition system. The template represents a string of components such as syllables or phonemes that constitute the nametag. In one example, the components are a series of states and state transitions. The method begins at <b>12</b>.</p>
<p id="p-0025" num="0024">A nametag is broken into its components, such as syllables or phonemes, and stored as a template. Multiple templates are possible when taking into account that some syllables or phonemes can be confused with other syllables or phonemes. In some speech recognition systems, a Hidden Markov Model (HMM) is used to inform the system's interpretation of the syllables or phonemes. For example, phoneme chains can branch as a result of phoneme confusion. The HMM evaluates the probability of various branches and biases the system's interpretation of the branching to increase the frequency of selecting the most likely branch.</p>
<p id="p-0026" num="0025">The speech recognition engine receives a confusion table based on a plurality of phonetically balanced utterances and circumstantial conditions (block <b>14</b>). A phonetically balanced utterance contains an equal number of phonemes uttered for a set of test sentences. For example, if a test set contains one hundred different sentences to utter, there will be an equal number of phonemes representing &#x2018;a&#x2019;, an equal number of phonemes representing &#x2018;b&#x2019;, and so on. Advantageously, phonetically balanced utterances avoid a statistical bias toward any particular phoneme. In one example, the speech recognition engine is operated within a telematics unit. The confusion table is generated, for example, from either a phoneme confusion matrix or a syllable confusion matrix. The confusion table reflects the fact that certain components of speech can be confused with a similar component by a speech recognition engine such as the confusion of the sounds &#x201c;m&#x201d; and &#x201c;n&#x201d;. A confusion matrix is constructed from a set of phonetically balanced utterances. The cells of the confusion matrix are given a numerical score. In one example, the numerical score represents a count of instances a phoneme is misrecognized for a recognition that is not expected. If the expected and recognized phonemes match, then those values are discarded. A confusion matrix is unsorted, that is, if &#x2018;m&#x2019; is misrecognized as &#x2018;n&#x2019;, and &#x2018;m&#x2019; is a positional distance from &#x2018;n&#x2019; in the confusion matrix, there may be numerical scores in the row cells adjacent to the cell representing &#x2018;m&#x2019; that are of greater of lesser numerical value. A confusion table is sorted, that is, the row cells are sorted in ascending or descending ranking order. For example, if &#x2018;m&#x2019; is most frequently misrecognized as &#x2018;n&#x2019;, then the phoneme for &#x2018;m&#x2019; will be placed in the first cell position of the confusion table. If &#x2018;r&#x2019; is the next most frequently misrecognized phoneme misrecognized as &#x2018;n&#x2019;, then the phoneme for &#x2018;r&#x2019; will be in the second cell position, and so on. Once the confusion table is constructed, the confusion matrix may be discarded. Circumstantial conditions are various factors that affect the incoming voice signal as detected by the system and affect the differentiation of various speech components by the system.</p>
<p id="p-0027" num="0026">Circumstantial conditions include noise introduced into the phonetically balanced utterances and speaker based variables such as the gender or the dialect of the speaker. The acoustic model, used when developing the confusion table from the test utterances, is trained for the application utilizing the nametag utterances as user input. One example of an application is a telematics system installed in a mobile vehicle. A confusion table designed for or best fitting a user is saved in a user based system such a vehicle command processor or at a remote server for later use.</p>
<p id="p-0028" num="0027">When the user trains the system, the user utters a nametag and the system stores the nametag utterance from the user. The user is required to utter the nametag only once. A plurality of templates is determined (block <b>20</b>), as shown in <figref idref="DRAWINGS">FIG. 2</figref>, from the nametag utterance using the confusion tables received at block <b>14</b>.</p>
<p id="p-0029" num="0028">The templates created for the nametag utterance are then selected based on at least one selection factor (block <b>30</b>). Selection allows the exclusion of templates that exhibit a low probability of providing a correct match to the nametag based on a comparison of the nametag to the templates. Template selection factors include a score, a syllable count, and a phoneme count. Templates are either saved or deleted based on the selection factor.</p>
<p id="p-0030" num="0029">If a predefined number of templates for a nametag are not determined (block <b>40</b>) additional templates are determined and then selected (block <b>20</b> and block <b>30</b>) until the predefined number of templates for that nametag is reached.</p>
<p id="p-0031" num="0030">A global nametag score is determined (block <b>50</b>) for each nametag stored in the system. One example of a method to calculate global nametag scores is detailed in <figref idref="DRAWINGS">FIG. 5</figref>. Each template associated with the nametag contributes to the global nametag score. The method terminates (block <b>95</b>).</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 2</figref> illustrates a flowchart representative of one method for determining the plurality of templates, in accordance with the invention at <b>200</b>. A nametag utterance is received at a speech recognition engine (block <b>210</b>). When the nametag utterance is received it is stored as a digitized audio string generated by an analog-to-digital converter. The speech recognition engine determines a best template for the received nametag utterance (block <b>220</b>). The audio string is matched, using a waveform analysis, to a stored model containing templates of known meaning. The template that most closely conforms to the audio string corresponding to the nametag utterance is chosen as the best template. The best template, therefore, contains the most probable or best pronunciation of the nametag.</p>
<p id="p-0033" num="0032">The system modifies the best template based on at least one confusion table (block <b>230</b>). The best template is modified by substituting, deleting, or inserting phonemes in the best template based on the confusion table. In one example, substitution rules are imposed on the phoneme substitutions. For example, a substitution rule specifies that the initial and final consonants of the nametag template can be substituted, deleted, or inserted. In another example, a rule specifies that vowel segments are left unmodified since vowel segments tend to carry the greatest energy in the speech signal. The circumstantial conditions associated with the different confusion tables allow the system to create multiple templates that resemble various speaking scenarios. In one example, multiple or alternate confusion tables are used to account for a variety of circumstantial conditions.</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 3</figref> illustrates a schematic of one example of a system for generating a plurality of templates, in accordance with the invention at <b>300</b>. The nametag utterance is received through microphone <b>310</b>. Speech recognition engine <b>320</b> generates an N-best template list <b>330</b>. N-best-1 template <b>332</b> is processed through confusion table <b>418</b> to generate multiple templates <b>350</b>. In one example, additional templates <b>360</b>, <b>370</b>, <b>380</b> are created by using additional N-best templates <b>334</b>, <b>336</b>, <b>338</b> of the nametag utterance. In another example, another type of speech recognition engine, such as a dynamic time warping based engine, provides a different N-best list used for the creation of additional templates. The templates generated represent the phoneme confusion accounting for the circumstantial conditions embedded in the confusion table.</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 4</figref> illustrates a flowchart representative of one example of a method for selecting the templates, in accordance with the invention at <b>400</b>.</p>
<p id="p-0036" num="0035">The system compares the stored nametag utterance from the user with all templates associated with that user using the acoustic model defined for the system (block <b>410</b>). Selection factors are determined from the comparison (block <b>420</b>). When a nametag utterance is compared to the complete template list, a word distance or score for each template is returned. The score represents the degree of match between the nametag utterance and the template and is used as a selection factor. In one example, a component count such as an average phoneme count of the template is an additional selection factor.</p>
<p id="p-0037" num="0036">The templates are ranked based on the selection factors to determine which templates to delete (block <b>430</b>). Low ranking templates have a small probability of providing a nametag match and are deleted (block <b>440</b>) to save storage space and reduce computational expense. If the phoneme count in the template is greater than or less than the phoneme count of the highest ranking template by a predetermined value, the template is invalid and is discarded.</p>
<p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. 5</figref> illustrates a flowchart representative of one example of a method for determining the global nametag score, in accordance with the invention at <b>500</b>. The global nametag score allows selection of a template based on the probability of successful retrieval rather than being based on identification of the template that provides the best match (score) to the nametag. Each template contributes proportionally to the global nametag score. The global nametag score changes as the system adapts to changing input. For example, the system adapts by adjusting the contribution of each template to the global nametag score for a nametag. The contribution of each template is dynamically varied by associating a weight with the template in determining the global nametag score.</p>
<p id="p-0039" num="0038">A template score is determined for each template (block <b>510</b>). The score for a template is determined as shown at block <b>420</b> of <figref idref="DRAWINGS">FIG. 4</figref>. The nametag utterance (i.e., the series of feature vectors) is compared against patterns known to the system, in the templates. A template score is obtained for each template based on an alignment of the nametag utterance with the templates. The alignment results in a score reflecting probability that a template correctly matches the nametag utterance. Template scores are calculated using any appropriate technique.</p>
<p id="p-0040" num="0039">A weight is determined for each template (block <b>520</b>). The weight is a probability factor and is based on the frequency the associated template is correctly selected for the nametag. One example of a method to calculate weights is detailed in <figref idref="DRAWINGS">FIG. 6</figref>.</p>
<p id="p-0041" num="0040">The score for each template is combined with the weight for that template resulting in a weighted template score (block <b>530</b>). The weighted template score is the product of the weight for that template and the template score. The global nametag score is found by calculating a linear combination of the weighted template scores for all saved templates associated with that nametag (block <b>540</b>). The global nametag score is continuously updated based on an updated weight determination (block <b>550</b>).</p>
<p id="p-0042" num="0041">In one example, the global score for a nametag is calculated as follows:</p>
<p id="p-0043" num="0042">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mrow>
  <msub>
    <mi>S</mi>
    <mi>j</mi>
  </msub>
  <mo>=</mo>
  <mrow>
    <munderover>
      <mo>&#x2211;</mo>
      <mrow>
        <mi>i</mi>
        <mo>=</mo>
        <mn>1</mn>
      </mrow>
      <msub>
        <mi>N</mi>
        <mi>j</mi>
      </msub>
    </munderover>
    <mo>&#x2062;</mo>
    <mrow>
      <msub>
        <mi>W</mi>
        <mi>ij</mi>
      </msub>
      <mo>&#x2062;</mo>
      <msub>
        <mi>S</mi>
        <mi>ij</mi>
      </msub>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
<br/>
such that
</p>
<p id="p-0044" num="0043">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mrow>
  <mrow>
    <munderover>
      <mo>&#x2211;</mo>
      <mrow>
        <mi>i</mi>
        <mo>=</mo>
        <mn>1</mn>
      </mrow>
      <msub>
        <mi>N</mi>
        <mi>j</mi>
      </msub>
    </munderover>
    <mo>&#x2062;</mo>
    <msub>
      <mi>W</mi>
      <mi>ij</mi>
    </msub>
  </mrow>
  <mo>=</mo>
  <mn>1</mn>
</mrow>
</math>
</maths>
</p>
<p id="p-0045" num="0044">Where:</p>
<p id="p-0046" num="0045">S<sub>j </sub>is the global score for the jth nametag;</p>
<p id="p-0047" num="0046">W<sub>ij </sub>is the weight for the i<sup>th </sup>template of the j<sup>th </sup>nametag;</p>
<p id="p-0048" num="0047">S<sub>ij </sub>is the score for the ith template of the j<sup>th </sup>nametag; and</p>
<p id="p-0049" num="0048">N<sub>j </sub>is the total number of templates available for the j<sup>th </sup>nametag.</p>
<p id="p-0050" num="0049">The method terminates (block <b>595</b>).</p>
<p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. 6</figref> illustrates a flowchart representative of one example of a method for calculating the weight for a template, in accordance with the invention at <b>600</b>. As used herein, the weight is a variable indicative of a selection frequency for a template. To determine the weight, a nametag selection count is determined (block <b>610</b>). The nametag selection count is the number of times a particular nametag is matched to any template.</p>
<p id="p-0052" num="0051">A threshold count representing a minimum value for the nametag selection count is pre-defined. Once the threshold count is realized, as determined by comparing the nametag selection count to the threshold (block <b>620</b>), a template selection count is determined (block <b>630</b>). The template selection count is a count of the number of instances a particular template is selected for a particular nametag. The weight is then determined by taking the quotient of the template selection count and the nametag selection count (block <b>640</b>). In one example the weight is calculated:</p>
<p id="p-0053" num="0052">
<maths id="MATH-US-00003" num="00003">
<math overflow="scroll">
<mrow>
  <msub>
    <mi>W</mi>
    <mi>i</mi>
  </msub>
  <mo>=</mo>
  <mrow>
    <mrow>
      <mfrac>
        <msub>
          <mi>C</mi>
          <mi>i</mi>
        </msub>
        <mrow>
          <munderover>
            <mo>&#x2211;</mo>
            <mrow>
              <mi>i</mi>
              <mo>=</mo>
              <mn>1</mn>
            </mrow>
            <mi>N</mi>
          </munderover>
          <mo>&#x2062;</mo>
          <msub>
            <mi>C</mi>
            <mi>j</mi>
          </msub>
        </mrow>
      </mfrac>
      <mo>&#x2062;</mo>
      <msub>
        <mi>C</mi>
        <mi>i</mi>
      </msub>
    </mrow>
    <mo>&#x2265;</mo>
    <mrow>
      <mn>0</mn>
      <mo>&#x2062;</mo>
      <mstyle>
        <mspace width="0.8em" height="0.8ex"/>
      </mstyle>
      <mo>&#x2062;</mo>
      <mi>and</mi>
      <mo>&#x2062;</mo>
      <mstyle>
        <mspace width="0.8em" height="0.8ex"/>
      </mstyle>
      <mo>&#x2062;</mo>
      <mn>0</mn>
    </mrow>
    <mo>&#x2264;</mo>
    <msub>
      <mi>W</mi>
      <mi>i</mi>
    </msub>
    <mo>&#x2264;</mo>
    <mn>1</mn>
  </mrow>
</mrow>
</math>
</maths>
<br/>
C<sub>i</sub>&#x2267;0 and 0&#x2266;W<sub>i</sub>&#x2266;1
</p>
<p id="p-0054" num="0053">Where:</p>
<p id="p-0055" num="0054">W<sub>i </sub>is the weight for the i<sup>th </sup>template;</p>
<p id="p-0056" num="0055">C<sub>i </sub>is the template selection count for the i<sup>th </sup>template;</p>
<p id="p-0057" num="0056">C<sub>j </sub>is the template selection count for the j<sup>th </sup>template; and</p>
<p id="p-0058" num="0057">N is the total number of templates defined for the selected nametag.</p>
<p id="p-0059" num="0058">The weight will change as the template selection counts change. The weight assigned to a template increases as the template selection count for that template increases.</p>
<p id="p-0060" num="0059">Statistically, as the sample size increases the weights should stabilize unless a change is introduced into the input to the system such as a by new user or by a noise condition not previously accounted for. A change in the input will cause a random oscillation of the weights. If unstable weight values for a nametag are detected, the system will revert back to the default method for that nametag. The default method does not weight the templates and instead relies on identifying a best match to determine the correct nametag. The system will start accumulating data for recalculating the weight to account for the change. In one example, the system can store user dependent weights in a user profile which allows multiple users to share the same system. The method terminates (block <b>695</b>).</p>
<p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. 7</figref> illustrates a flowchart representative of one example of a method for dynamic learning of weights, in accordance with the invention at <b>700</b>. The method starts at <b>701</b>. The user utters the nametag (block <b>710</b>) and the audio is received at a speech recognition engine. In one example, the audio is received at an in-vehicle microphone operatively coupled to a telematics unit in a mobile vehicle.</p>
<p id="p-0062" num="0061">A template is selected that best matches the audio of the received nametag utterance (block <b>720</b>). Prior to calculating a weight, the system accumulates an appropriate number of samples to support the statistical significance of the weight calculation. For example, the number of user utterances and resulting template selections reaches a pre-defined threshold, such as 100 samples before the weight is calculated. The system will advance a nametag sample count each time a nametag utterance is detected (block <b>730</b>).</p>
<p id="p-0063" num="0062">Sample collection will continue until the pre-defined threshold is reached (block <b>740</b>). Once that threshold is reached, the weight is calculated as shown in <figref idref="DRAWINGS">FIG. 6</figref> (block <b>750</b>). If the pre-defined threshold is not met (block <b>740</b>), then the method returns to block <b>710</b>.</p>
<p id="p-0064" num="0063">A weight is calculated for and applied to each template as shown in <figref idref="DRAWINGS">FIG. 5</figref> and <figref idref="DRAWINGS">FIG. 6</figref> (block <b>760</b>). The weight is based on template usage. The most frequently chosen templates are assigned the highest weight while less frequently chosen templates received a lower weight. For example, if a third template is chosen most often it will be assigned a higher weight thereby having a higher contribution to the global score. Without the weight, the system will continue to select the template based on determining the best match to the nametag. The number of nametag decoding errors is reduced by biasing template selection toward the most frequently selected template.</p>
<p id="p-0065" num="0064">A nametag retrieval is successful when the intended nametag is returned. A selection count is modified based on the nametag retrieval (block <b>770</b>). The selection count for the selected template is incremented for a correct nametag retrieval (block <b>772</b>). An increase in the selection count for a template will increase the weight for that template. An updated weight determination is made (block <b>774</b>) and the method ends (block <b>795</b>). If an incorrect nametag is returned, the selection count is decremented which in turn decreases the weight for that template (block <b>780</b>). The dynamic adjustment of weights reinforces correct template selections and penalizes incorrect template selections.</p>
<p id="p-0066" num="0065">If the weight for a particular template is consistently penalized its value will approach zero. A limit can be defined for the weight below which the associated template is determined as invalid (block <b>782</b>). If the weight is not below the pre-defined limit, the method terminates (block <b>795</b>).</p>
<p id="p-0067" num="0066">When the system determines that the weight for a template falls below the pre-defined limit, the template will be deleted (block <b>784</b>). The deleted template will be replaced, for example, by a new template generated based on the most recently spoken nametag utterance (block <b>786</b>). Once a new template is generated, the method returns to block <b>720</b> and statistics will be collected for the newly generated templates until the pre-defined threshold is reached. A weight will then be calculated for the new template and the selection process will continue. The method terminates (block <b>795</b>).</p>
<p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. 8</figref> illustrates a flowchart representative of one example of a method for determining at least one confusion table, in accordance with the invention at <b>800</b>.</p>
<p id="p-0069" num="0068">In one example, phonetically balanced utterances are received (block <b>810</b>) and passed through a recognizer such as a free phoneme recognizer. Component sequences, such as phoneme sequences are generated (block <b>820</b>) and saved to a log file. Several phoneme strings will be generated for each test utterance. The expected phoneme sequence for an utterance is then compared with the recognized phoneme string from the test utterance in determining the phoneme confusion matrix (block <b>830</b>).</p>
<p id="p-0070" num="0069">A confusion table that is a lookup table is generated from the confusion matrix (block <b>840</b>). Each phoneme of the nametag utterance is listed in the first column of the confusion table. Subsequent columns each contain a confusable phoneme for the phoneme listed in the first column. The columns are organized, for example, according to a confusability ranking from high to low.</p>
<p id="p-0071" num="0070">In one example, three error types are accounted for in the confusion table phoneme substitutions, phoneme deletions and phoneme insertions. A substitution involves replacing the phoneme in the first column with a phoneme from one of the subsequent columns, a deletion involves removing the phoneme in the first column from the phoneme sequence and an insertion involves placing an additional phoneme in the phoneme sequence. The confusion table specifies the phonemes that can be deleted and those that can be inserted.</p>
<p id="p-0072" num="0071">In one example the test utterances are separated based on ambient noise conditions and multiple confusion tables are available based on an ambient noise classification such as high, medium and low. Vehicle dependent test utterances can be simulated as described in U.S. patent application Ser. No. 11/235,961 to Chengalvarayan which is incorporated herein by reference in its entirety. By the same methods additional confusion tables based on factors such as gender, dialect, vehicle type, ambient noise conditions, and vehicle operating condition can be generated. The method terminates (block <b>895</b>).</p>
<p id="p-0073" num="0072"><figref idref="DRAWINGS">FIG. 9</figref> illustrates a schematic of one example of a system for determining at least one confusion table, in accordance with the invention at <b>900</b>. The template determining system includes speakers <b>902</b> providing a plurality of phonetically balanced utterances <b>904</b>. In one example, one or more speakers <b>902</b> providing the phonetically balanced utterances <b>904</b> are located in a remote facility. Circumstantial conditions <b>906</b>, such as gender, dialect, noise conditions, vehicle operating conditions are convolved with the phonetically balanced utterances <b>904</b> and passed through a free phoneme recognizer <b>910</b>. The free phoneme recognizer <b>910</b> contains one or more acoustic models. The recognized phoneme sequences <b>912</b> output by the recognizer <b>910</b> are aligned with the expected phoneme sequences <b>914</b> for an utterance creating a phoneme confusion matrix <b>916</b>. The phoneme confusion matrix <b>916</b> is converted to a confusion table <b>918</b>. Multiple confusion tables can result from segregating phonetically balanced utterances <b>904</b> based on circumstantial conditions <b>906</b>. In another example, circumstantial conditions <b>906</b> are simulated and combined with the phonetically balanced utterances <b>904</b>.</p>
<p id="p-0074" num="0073">The resulting confusion tables <b>918</b> are stored in a telematics unit <b>920</b> of a mobile vehicle <b>922</b> or a remote server <b>930</b>. In one example, the confusion tables based on a particular vehicle type and user type are downloaded to the telematics unit within the mobile vehicle when the mobile vehicle is supplied to the user.</p>
<p id="p-0075" num="0074"><figref idref="DRAWINGS">FIG. 10</figref> illustrates a schematic of one example of a system for dynamic nametag scoring, in accordance with the invention at <b>100</b>. The dynamic nametag scoring system includes a mobile vehicle communication unit (MVCU) <b>110</b>, a mobile vehicle communication network <b>112</b>, one or more embedded modules <b>139</b>, a communication device such as a telematics unit <b>120</b>, one or more wireless carrier systems <b>140</b>, one or more communication networks <b>142</b>, one or more land networks <b>144</b>, one or more client, personal, or user computers <b>150</b>, one or more web-hosting portals <b>160</b>, and one or more call centers <b>170</b>. In one example, MVCU <b>110</b> is implemented as a mobile vehicle equipped with suitable hardware and software for transmitting and receiving voice and data communications. In one example, a display <b>135</b> such as a dialed digital display in a radio unit or in an instrument panel is embedded in MVCU <b>110</b>. In other examples, automated recall notification system <b>100</b> includes additional components not relevant to the present discussion. Mobile vehicle communication systems and telematics units are known in the art.</p>
<p id="p-0076" num="0075">Embedded modules <b>139</b> are any electronic module configured to enable or assist in the operation of MVCU <b>110</b>, or any of its included systems. In one example, the embedded module <b>139</b> includes a speech recognition engine. For example, one embedded module performs odometer functions, while another embedded module controls HVAC operations within the mobile vehicle. In another example, an embedded module senses a mobile vehicle operation input, such as a key cycle, and sends a signal via vehicle communication network <b>112</b> that is received by telematics unit <b>120</b>. Any number of embedded modules <b>139</b> can be included.</p>
<p id="p-0077" num="0076">MVCU <b>110</b> is also referred to as a mobile vehicle in the discussion below. In operation, MVCU <b>110</b> may be implemented as a motorvehicle, a marine vehicle, or as an aircraft. MVCU <b>110</b> may include additional components not relevant to the present discussion.</p>
<p id="p-0078" num="0077">MVCU <b>110</b>, via a mobile vehicle communication network <b>112</b>, sends signals to various units of equipment and systems within mobile vehicle <b>110</b> to perform various functions such as monitoring the operational state of vehicle systems, collecting and storing data from the vehicle systems, providing instructions, data and programs to various vehicle systems, and calling from telematics unit <b>120</b>. In facilitating interactions among the various communication and electronic modules, mobile vehicle communication network <b>112</b> utilizes interfaces such as controller-area network (CAN), Media Oriented System Transport (MOST), Local Interconnect Network (LIN), Ethernet (10 base T, 100 base T), International Organization for Standardization (ISO) Standard 9141, ISO Standard 11898 for high-speed applications, ISO Standard 11519 for lower speed applications, and Society of Automotive Engineers (SAE) standard J1850 for higher and lower speed applications. In one example, mobile vehicle communication network <b>112</b> is a direct connection between connected devices.</p>
<p id="p-0079" num="0078">MVCU <b>110</b>, via telematics unit <b>120</b>, sends and receives radio transmissions from wireless carrier system <b>140</b>. Wireless carrier system <b>140</b> is implemented as any suitable system for transmitting a signal from MVCU <b>110</b> to communication network <b>142</b>.</p>
<p id="p-0080" num="0079">In one example, telematics unit <b>120</b> includes a processor <b>122</b> connected to a wireless modem <b>124</b>, a global positioning system (GPS) unit <b>126</b>, an in-vehicle memory <b>128</b>, a microphone <b>130</b>, one or more speakers <b>132</b>, and an embedded or in-vehicle mobile phone <b>134</b>. In other examples, telematics unit <b>120</b> is implemented without one or more of the above listed components such as, for example, GPS unit <b>126</b> or speakers <b>132</b> or includes additional components not relevant to the present discussion.</p>
<p id="p-0081" num="0080">In various examples, processor <b>122</b> is implemented as a digital signal processor (DSP), microcontroller, microprocessor, controller, host processor, or vehicle communications processor. In an example, processor <b>122</b> is implemented as an application-specific integrated circuit (ASIC). In another example, processor <b>122</b> is implemented as a processor working in conjunction with a central processing unit (CPU) performing the function of a general purpose processor. GPS unit <b>126</b> provides longitude and latitude coordinates of the mobile vehicle responsive to a GPS broadcast signal received from one or more GPS satellite broadcast systems (not shown). In-vehicle mobile phone <b>134</b> is a cellular-type phone such as, for example, an analog, digital, dual-mode, dual-band, multi-mode or multi-band cellular phone.</p>
<p id="p-0082" num="0081">Processor <b>122</b> executes various computer programs that control programming and operational modes of electronic and mechanical systems within MVCU <b>110</b>. Processor <b>122</b> controls communications (e.g., call signals) between telematics unit <b>120</b>, wireless carrier system <b>140</b>, and call center <b>170</b>.</p>
<p id="p-0083" num="0082">Communication network <b>142</b> includes services from one or more mobile telephone switching offices and wireless networks. Communication network <b>142</b> connects wireless carrier system <b>140</b> to land network <b>144</b>. Communication network <b>142</b> is implemented as any suitable system or collection of systems for connecting wireless carrier system <b>140</b> to MVCU <b>110</b> and land network <b>144</b>.</p>
<p id="p-0084" num="0083">Land network <b>144</b> connects communication network <b>142</b> to client computer <b>150</b>, web-hosting portal <b>160</b>, satellite uplink facility <b>165</b>, and call center <b>170</b>. In one example, land network <b>144</b> is a public-switched telephone network (PSTN). In another example, land network <b>144</b> is implemented as an Internet protocol (IP) network. In other examples, land network <b>144</b> is implemented as a wired network, an optical network, a fiber network, other wireless networks, or any combination thereof. Land network <b>144</b> is connected to one or more landline telephones. Communication network <b>142</b> and land network <b>144</b> connect wireless carrier system <b>140</b> to web-hosting portal <b>160</b>, satellite uplink facility <b>165</b>, and call center <b>170</b>.</p>
<p id="p-0085" num="0084">Client, personal, or user computer <b>150</b> includes a computer usable medium to execute Internet browser and Internet-access computer programs for sending and receiving data over land network <b>144</b> and, optionally, wired or wireless communication networks <b>142</b> to web-hosting portal <b>160</b>. Personal or client computer <b>150</b> sends user preferences to web-hosting portal through a web-page interface using communication standards such as hypertext transport protocol (HTTP), and transport-control protocol and Internet protocol (TCP/IP). In one example, the data includes directives to change certain programming and operational modes of electronic and mechanical systems within MVCU <b>110</b>. In operation, a client utilizes computer <b>150</b> to initiate setting or re-setting of user preferences for MVCU <b>110</b>. User-preference data from client-side software is transmitted to server-side software of web-hosting portal <b>160</b>. User-preference data is stored at web-hosting portal <b>160</b>.</p>
<p id="p-0086" num="0085">Web-hosting portal <b>160</b> includes one or more data modems <b>162</b>, one or more web servers <b>164</b>, one or more databases <b>166</b>, and a network system <b>168</b>. Web-hosting portal <b>160</b> is connected directly by wire to call center <b>170</b>, or connected by phone lines to land network <b>144</b>, which is connected to call center <b>170</b>. In an example, web-hosting portal <b>160</b> is connected to call center <b>170</b> utilizing an IP network. In this example, both components, web-hosting portal <b>160</b> and call center <b>170</b>, are connected to land network <b>144</b> utilizing the IP network. In another example, web-hosting portal <b>160</b> is connected to land network <b>144</b> by one or more data modems <b>162</b>. Land network <b>144</b> sends digital data to and receives digital data from modem <b>162</b>, data that is then transferred to web server <b>164</b>. In one example, modem <b>162</b> resides inside web server <b>164</b>. Land network <b>144</b> transmits data communications between web-hosting portal <b>160</b> and call center <b>170</b>.</p>
<p id="p-0087" num="0086">Web server <b>164</b> receives user-preference data from user computer <b>150</b> via land network <b>144</b>. In alternative examples, computer <b>150</b> includes a wireless modem to send data to web-hosting portal <b>160</b> through a wireless communication network <b>142</b> and a land network <b>144</b>. Data is received by land network <b>144</b> and sent to one or more web servers <b>164</b>. In one example, web server <b>164</b> is implemented as any suitable hardware and software capable of providing web services to help change and transmit personal preference settings from a client at computer <b>150</b> to telematics unit <b>120</b> in MVCU <b>110</b>. Web server <b>164</b> sends to or receives from one or more databases <b>166</b> data transmissions via network system <b>168</b>. In one example, web server <b>164</b> includes computer applications and files for managing and storing personalization settings supplied by the client and subscriber status supplied by telematics unit <b>120</b>. For each subscriber, the web server potentially stores hundreds of preferences for wireless vehicle communication, networking, maintenance and diagnostic services for a mobile vehicle.</p>
<p id="p-0088" num="0087">In one example, one or more web servers <b>164</b> are networked via network system <b>168</b> to distribute data among its network components such as database <b>166</b>. In an example, database <b>166</b> is a part of or a separate computer from web server <b>164</b>. Web server <b>164</b> sends data transmissions with status information to call center <b>170</b> through land network <b>144</b>.</p>
<p id="p-0089" num="0088">Call center <b>170</b> is a location where many calls are received and serviced at the same time, or where many calls are sent at the same time. In one example, the call center is a test center facilitating communications to mobile vehicle <b>110</b> for testing of embedded modules <b>139</b>. In another example, the call center is a telematics call center, facilitating communications to and from telematics unit <b>120</b> in MVCU <b>110</b>. In an example, the call center is a voice call center, providing verbal communications between an advisor in the call center and a subscriber in a mobile vehicle. In another example, the call center contains each of these functions. In other examples, call center <b>170</b> and web-hosting portal <b>160</b> are located in the same or in different facilities.</p>
<p id="p-0090" num="0089">In one example, call center <b>170</b> receives a recall report and creates a recall communication instruction based on the recall report. Call center <b>170</b> then sends recall communication instructions to telematics unit <b>120</b> of mobile vehicle <b>110</b> involved in a recall.</p>
<p id="p-0091" num="0090">Call center <b>170</b> contains one or more voice and data switches <b>172</b>, one or more communication services managers <b>174</b>, one or more communication services databases <b>176</b>, one or more communication services advisors <b>178</b>, and one or more network systems <b>180</b>.</p>
<p id="p-0092" num="0091">Switch <b>172</b> of call center <b>170</b> connects to land network <b>144</b>. Switch <b>172</b> transmits voice or data transmissions from call center <b>170</b>, and receives voice or data transmissions from telematics unit <b>120</b> in MVCU <b>110</b> through wireless carrier system <b>140</b>, communication network <b>142</b>, and land network <b>144</b>. Switch <b>172</b> receives data transmissions from and sends data transmissions to one or more web-hosting portals <b>160</b>. Switch <b>172</b> receives data transmissions from or sends data transmissions to one or more communication services managers <b>174</b> via one or more network systems <b>180</b>.</p>
<p id="p-0093" num="0092">Communication services manager <b>174</b> is any suitable hardware and software capable of providing requested communication services to telematics unit <b>120</b> in MVCU <b>110</b>. Communication services manager <b>174</b> sends to or receives from one or more communication services databases <b>176</b> data transmissions via network system <b>180</b>. Communication services manager <b>174</b> sends to or receives from one or more communication services advisors <b>178</b> data transmissions via network system <b>180</b>. Communication services database <b>176</b> sends to or receives from communication services advisor <b>178</b> data transmissions via network system <b>180</b>. Communication services advisor <b>178</b> receives from or sends to switch <b>172</b> voice or data transmissions.</p>
<p id="p-0094" num="0093">Communication services manager <b>174</b> provides one or more of a variety of services, including enrollment services, navigation assistance, directory assistance, roadside assistance, business or residential assistance, information services assistance, emergency assistance, and communications assistance. Communication services manager <b>174</b> receives requests for a variety of services from the client via computer <b>150</b>, web-hosting portal <b>160</b>, and land network <b>144</b> and awareness messages from telematics unit <b>120</b>. Communication services manager <b>174</b> transmits requests for subscriber status and other data to telematics unit <b>120</b> in MVCU <b>110</b> through wireless carrier system <b>140</b>, communication network <b>142</b>, land network <b>144</b>, voice and data switch <b>172</b>, and network system <b>180</b>. Communication services manager <b>174</b> stores or retrieves data and information from communication services database <b>176</b>. Communication services manager <b>174</b> can provide requested information to communication services advisor <b>178</b>.</p>
<p id="p-0095" num="0094">In one example, communication services advisor <b>178</b> is implemented as a real advisor. In an example, a real advisor is a human being in verbal communication with a user or subscriber (e.g., a client) in MVCU <b>110</b> via telematics unit <b>120</b>. In another example, communication services advisor <b>178</b> is implemented as a virtual advisor. In an example, a virtual advisor is implemented as a synthesized voice interface responding to requests from telematics unit <b>120</b> in MVCU <b>110</b>.</p>
<p id="p-0096" num="0095">Communication services advisor <b>178</b> provides services to telematics unit <b>120</b> in MVCU <b>110</b>. Services provided by communication services advisor <b>178</b> include enrollment services, recall notifications navigation assistance, real-time traffic advisories, directory assistance, roadside assistance, business or residential assistance, information services assistance, emergency assistance, and communications assistance. Communication services advisor <b>178</b> communicates with telematics unit <b>120</b> in MVCU <b>110</b> through wireless carrier system <b>140</b>, communication network <b>142</b>, land network <b>144</b> and web hosting portals <b>160</b> using voice or data transmissions. In an alternative example, communication services manager <b>174</b> communicates with telematics unit <b>120</b> in MVCU <b>110</b> through wireless carrier system <b>140</b>, communication network <b>142</b>, land network <b>144</b>, and web hosting portals <b>160</b> using voice or data transmissions. Switch <b>172</b> selects between voice transmissions and data transmissions.</p>
<p id="p-0097" num="0096">While the examples of the invention disclosed herein are presently considered to be preferred, various changes and modifications can be made without departing from the spirit and scope of the invention. The scope of the invention is indicated in the appended claims, and all changes that come within the meaning and range of equivalents are intended to be embraced therein.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08626506-20140107-M00001.NB">
<img id="EMI-M00001" he="9.57mm" wi="76.20mm" file="US08626506-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US08626506-20140107-M00002.NB">
<img id="EMI-M00002" he="9.57mm" wi="76.20mm" file="US08626506-20140107-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00003" nb-file="US08626506-20140107-M00003.NB">
<img id="EMI-M00003" he="10.24mm" wi="76.20mm" file="US08626506-20140107-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method for dynamic nametag scoring comprising:
<claim-text>receiving at a speech recognition engine at least one confusion table stored in at least one of a mobile vehicle telematics unit or a remote server and being sorted by phoneme misrecognition and including at least one circumstantial condition including at least one of a user gender, a user dialect, a vehicle type, or a vehicle operating condition, and wherein the confusion table is based on a plurality of phonetically balanced utterances;</claim-text>
<claim-text>determining a plurality of templates for a nametag based on the received at least one confusion table by using the speech recognition engine;</claim-text>
<claim-text>selecting templates from the determined plurality of templates; and</claim-text>
<claim-text>determining a global nametag score for the nametag based on a weighting indicative of a template selection frequency, wherein more frequently selected templates are assigned higher weights and less frequently selected templates are assigned lower weights.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein determining the plurality of templates for at least one nametag comprises:
<claim-text>receiving a nametag utterance at a speech recognition engine;</claim-text>
<claim-text>determining at least one best template for the received nametag utterance at the speech recognition engine based on a stored model; and</claim-text>
<claim-text>modifying the best template based on at least one confusion matrix to derive a plurality of templates corresponding to the best template.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein determining the global nametag score comprises:
<claim-text>determining at least one template score for each template;</claim-text>
<claim-text>determining at least one weight for each template;</claim-text>
<claim-text>combining the template score and the weight to yield a weighted template score; and</claim-text>
<claim-text>calculating a linear combination of the weighted template scores for all templates associated with a particular nametag.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref> wherein determining at least one weight for each template comprises:
<claim-text>determining a nametag selection count;</claim-text>
<claim-text>determining that the nametag selection count meets a threshold count; and thereafter:</claim-text>
<claim-text>determining a template selection count; and</claim-text>
<claim-text>calculating the quotient of the template selection count and the nametag selection count.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref> further comprising: modifying the template selection count based on a nametag retrieval.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising: continuously updating the global nametag score based on an updated weight determination.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the selecting step is based on at least one selection factor.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref> wherein selecting the templates based on at least one selection factor comprises:
<claim-text>comparing the plurality of templates to a nametag audio string based on an acoustic model;</claim-text>
<claim-text>determining at least one selection factor based on the comparison;</claim-text>
<claim-text>ranking the plurality of templates based on the selection factors; and</claim-text>
<claim-text>deleting at least one template based on the ranking.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. A method for determining a plurality of confusion tables comprising:
<claim-text>receiving a plurality of test utterances in a speech recognizer including acoustic models;</claim-text>
<claim-text>generating via the speech recognizer, a plurality of component sequences based on the test utterances and at least one circumstantial condition;</claim-text>
<claim-text>determining via the speech recognizer, a confusion matrix based on the component sequences and the test utterances; and</claim-text>
<claim-text>generating a confusion table based on the confusion matrix but sorted by phoneme misrecognition, and stored in at least one of a mobile vehicle telematics unit or a remote server.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. A method for dynamic nametag scoring comprising:
<claim-text>receiving at a speech recognition engine, an utterance for a nametag and at least one confusion table stored in at least one of a mobile vehicle telematics unit or a remote server and being sorted by phoneme misrecognition and including at least one circumstantial condition including at least one of a user gender, a user dialect, a vehicle type, or a vehicle operating condition; and</claim-text>
<claim-text>determining a plurality of templates for the nametag based on the received at least one confusion table by using the speech recognition engine; and</claim-text>
<claim-text>selecting templates from the determined plurality of templates.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref> wherein the selecting templates is based on at least one selection factor including at least one of a nametag score, a nametag syllable count, or a nametag phoneme count.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref> wherein the selecting templates comprises:
<claim-text>ranking the determined plurality of templates based on the at least one selection factor; and</claim-text>
<claim-text>deleting at least one template based on the ranking.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, further comprising determining a global nametag score for the nametag based on a weighting indicative of a template selection frequency, wherein more frequently selected templates are assigned higher weights and less frequently selected templates are assigned lower weights.</claim-text>
</claim>
</claims>
</us-patent-grant>
