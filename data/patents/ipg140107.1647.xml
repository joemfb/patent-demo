<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08622831-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08622831</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>11766483</doc-number>
<date>20070621</date>
</document-id>
</application-reference>
<us-application-series-code>11</us-application-series-code>
<us-term-of-grant>
<us-term-extension>1307</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>17</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>463 36</main-classification>
<further-classification>463  9</further-classification>
<further-classification>463 31</further-classification>
</classification-national>
<invention-title id="d2e53">Responsive cutscenes in video games</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>4305131</doc-number>
<kind>A</kind>
<name>Best</name>
<date>19811200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>364521</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5679075</doc-number>
<kind>A</kind>
<name>Forrest et al.</name>
<date>19971000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>463  9</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6364666</doc-number>
<kind>B1</kind>
<name>Jenkins et al.</name>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>434156</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6544040</doc-number>
<kind>B1</kind>
<name>Brelis et al.</name>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>434236</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>7155158</doc-number>
<kind>B1</kind>
<name>Iuppa et al.</name>
<date>20061200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>434350</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2003/0144045</doc-number>
<kind>A1</kind>
<name>Fujita</name>
<date>20030700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>463  1</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2004/0038739</doc-number>
<kind>A1</kind>
<name>Wanat</name>
<date>20040200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>463 36</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2004/0147324</doc-number>
<kind>A1</kind>
<name>Brown</name>
<date>20040700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>463 42</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2005/0278642</doc-number>
<kind>A1</kind>
<name>Chang et al.</name>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>715751</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2006/0040718</doc-number>
<kind>A1</kind>
<name>Davis</name>
<date>20060200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>463  9</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2006/0119598</doc-number>
<kind>A1</kind>
<name>Littlefield</name>
<date>20060600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>345419</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2006/0148545</doc-number>
<kind>A1</kind>
<name>Rhyene, IV et al.</name>
<date>20060700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>463  1</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2006/0252533</doc-number>
<kind>A1</kind>
<name>Sakaguchi et al.</name>
<date>20061100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>463 31</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>KR</country>
<doc-number>10-2005-0120389</doc-number>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>KR</country>
<doc-number>10-2007-0024918</doc-number>
<date>20070300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00016">
<othercit>Search Report and Written Opinion of the International Searching Authority in corresponding PCT application PCT/US2008066649. Filed Jun. 12, 2008.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00017">
<othercit>Davy McCarthy, Spider-Man 3: The Game, http://www.eurogamer.net/article.php?article<sub>&#x2014;</sub>id=75561, pp. 1-2, Apr. 20, 2007.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00018">
<othercit>Cutscene, http://tvtropes.org/pmwiki/pmwiki.php/Main.Cutscene, pp. 1-3, Apr. 27, 2007.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00019">
<othercit>Jerome Cukier, Cutscenes and dialogues, http://www.gamethink.net/Cutscenes-and-dialogues.html, pp. 1-3, Jun. 19, 2006.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00020">
<othercit>Call of Duty 3 Review (Xbox 360), http://www.gaminglife.com.au/, pp. 1-7, Dec. 19, 2006.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00021">
<othercit>Steve Wisdom, Miami Vice: The Game&#x2014;Article, http://www.deeko.com/psp/previewDetail.asp?id-613, pp. 1-2, Jun. 6, 2006.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00022">
<othercit>Office Action from corresponding Chinese application 200880021070.8, mailed Apr. 26, 2012.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00023">
<othercit>First Office Action from corresponding Chinese application 200880021070.8, mailed May 19, 2011.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00024">
<othercit>Office Action from corresponding Israeli application 201706.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>11</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>463  1</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>463  9</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>463 31</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>463 36</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>463 42</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>6</number-of-drawing-sheets>
<number-of-figures>12</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20080318676</doc-number>
<kind>A1</kind>
<date>20081225</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Ham</last-name>
<first-name>Richard Allen</first-name>
<address>
<city>Guildford</city>
<country>GB</country>
</address>
</addressbook>
<residence>
<country>GB</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Ham</last-name>
<first-name>Richard Allen</first-name>
<address>
<city>Guildford</city>
<country>GB</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Kelly, Holt &#x26; Christenson, PLLC</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<orgname>Microsoft Corporation</orgname>
<role>02</role>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Cuff</last-name>
<first-name>Michael</first-name>
<department>3716</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A determination is made that a player's avatar has performed an action while an audio signal representing a narrative of a non-player character is being produced. The action is mapped to an impression, which is mapped to a response. The audio signal is stopped before it is completed and the response is played by providing audio for the non-player character and/or animating the non-player character. After the response is played, steps ensure that critical information in the narrative has been provided to the player.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="140.04mm" wi="215.90mm" file="US08622831-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="223.27mm" wi="167.05mm" orientation="landscape" file="US08622831-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="264.84mm" wi="192.28mm" file="US08622831-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="275.00mm" wi="199.05mm" file="US08622831-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="272.63mm" wi="206.50mm" file="US08622831-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="114.81mm" wi="174.07mm" file="US08622831-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="201.51mm" wi="190.25mm" file="US08622831-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND</heading>
<p id="p-0002" num="0001">Video games typically include an avatar, which is a character or object in the game that is controlled by a player, and non-player characters, which are controlled by the game. In many games, the player's avatar is able to interact with non-player characters such that the non-player characters will respond to actions taken by the player's avatar. For example, if a player's avatar attacks a non-character player, the non-character player may counter attack or run away.</p>
<p id="p-0003" num="0002">Within video games, it is common for developers to include audio and video segments known as cutscenes that provide narrative information such as a story line for the game, contextual information for playing the game, or instructions for proceeding forward in the game. Traditionally, such cut scenes interrupted the game and took away the player's control of their avatar. Such cut scenes provide a movie-like experience where the player simply watches the action in the cut scene. Some video games have allowed the player to continue to control their avatar during the cut scene. However, actions taken by the avatar during such cut scenes are ignored by the non-player characters in the cut scene. Thus, the non-player characters do not interact with the player's avatar during the cut scene and seem to become robotic.</p>
<p id="p-0004" num="0003">The discussion above is merely provided for general background information and is not intended to be used as an aid in determining the scope of the claimed subject matter.</p>
<heading id="h-0002" level="1">SUMMARY</heading>
<p id="p-0005" num="0004">A determination is made that a player's avatar has performed an action while an audio signal representing a narrative of a non-player character is being produced. The action is mapped to an impression, which is mapped to a response. The audio signal is stopped before it is completed and the response is played by providing audio for the non-player character and/or animating the non-player character. After the response is played, steps ensure that critical information in the narrative has been provided to the player.</p>
<p id="p-0006" num="0005">This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used as an aid in determining the scope of the claimed subject matter. The claimed subject matter is not limited to implementations that solve any or all disadvantages noted in the background.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. 1</figref> is a perspective view of a gaming console.</p>
<p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram of components of a gaming console.</p>
<p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. 3</figref> is a block diagram of elements in a gaming console used for responsive cutscenes.</p>
<p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. 4</figref> is a flow diagram of a method of providing responsive cut scenes.</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 5</figref> is a top perspective view of a gaming world.</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 6</figref> is a top perspective view of a second embodiment of a gaming world.</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 7</figref> is a screen shot of a non-player character providing a narrative in a cutscene.</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 8</figref> is a screen shot showing a menu of expressions that the player's avatar may make.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 9</figref> is a screen shot showing a combination of a microreaction response and an animation for a cutscene.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 10</figref> is a screen shot of a non-player character showing an angry response.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 11</figref> is a screen shot of a non-player character showing a happy response.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 12</figref> is a screen shot of a non-player character showing a scared response.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 1</figref> shows an exemplary gaming and media system <b>100</b>. The following discussion of this Figure is intended to provide a brief, general description of a suitable environment in which certain methods may be implemented.</p>
<p id="p-0020" num="0019">As shown in <figref idref="DRAWINGS">FIG. 1</figref>, gaming and media system <b>100</b> includes a game and media console (hereinafter &#x201c;console&#x201d;) <b>102</b>. Console <b>102</b> is configured to accommodate one or more wireless controllers, as represented by controllers <b>104</b>(<b>1</b>) and <b>104</b>(<b>2</b>). A command button <b>135</b> on console <b>102</b> is used create a new wireless connection between on of the controllers and the console <b>102</b>. Console <b>102</b> is equipped with an internal hard disk drive (not shown) and a media drive <b>106</b> that supports various forms of portable storage media, as represented by optical storage disc <b>108</b>. Examples of suitable portable storage media include DVD, CD-ROM, game discs, and so forth. Console <b>102</b> also includes two memory unit card receptacles <b>125</b>(<b>1</b>) and <b>125</b>(<b>2</b>), for receiving removable flash-type memory units <b>140</b>.</p>
<p id="p-0021" num="0020">Console <b>102</b> also includes an optical port <b>130</b> for communicating wirelessly with one or more devices and two USB (Universal Serial Bus) ports <b>110</b>(<b>1</b>) and <b>110</b>(<b>2</b>) to support a wired connection for additional controllers, or other peripherals. In some implementations, the number and arrangement of additional ports may be modified. A power button <b>112</b> and an eject button <b>114</b> are also positioned on the front face of game console <b>102</b>. Power button <b>112</b> is selected to apply power to the game console, and can also provide access to other features and controls, and eject button <b>114</b> alternately opens and closes the tray of a portable media drive <b>106</b> to enable insertion and extraction of a storage disc <b>108</b>.</p>
<p id="p-0022" num="0021">Console <b>102</b> connects to a television or other display (not shown) via A/V interfacing cables <b>120</b>. In one implementation, console <b>102</b> is equipped with a dedicated A/V port (not shown) configured for content-secured digital communication using A/V cables <b>120</b> (e.g., A/V cables suitable for coupling to a High Definition Multimedia Interface &#x201c;HDMI&#x201d; port on a high definition monitor <b>150</b> or other display device). A power cable <b>122</b> provides power to the game console. Console <b>102</b> may be further configured with broadband capabilities, as represented by a cable or modem connector <b>124</b> to facilitate access to a network, such as the Internet.</p>
<p id="p-0023" num="0022">Each controller <b>104</b> is coupled to console <b>102</b> via a wired or wireless interface. In the illustrated implementation, the controllers are USB-compatible and are coupled to console <b>102</b> via a wireless or USB port <b>110</b>. Console <b>102</b> may be equipped with any of a wide variety of user interaction mechanisms. In an example illustrated in <figref idref="DRAWINGS">FIG. 1</figref>, each controller <b>104</b> is equipped with two thumbsticks <b>132</b>(<b>1</b>) and <b>132</b>(<b>2</b>), a D-pad <b>134</b>, buttons <b>136</b>, User Guide button <b>137</b> and two triggers <b>138</b>. By pressing and holding User Guide button <b>137</b>, a user is able to power-up or power-down console <b>102</b>. By pressing and releasing User Guide button <b>137</b>, a user is able to cause a User Guide Heads Up Display (HUD) user interface to appear over the current graphics displayed on monitor <b>150</b>. The controllers described above are merely representative, and other known gaming controllers may be substituted for, or added to, those shown in <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0024" num="0023">Controllers <b>104</b> each provide a socket for a plug of a headset <b>160</b>. Audio data is sent through the controller to a speaker <b>162</b> in headset <b>160</b> to allow sound to be played for a specific player wearing headset <b>160</b>. Headset <b>162</b> also includes a microphone <b>164</b> that detects speech from the player and conveys an electrical signal to the controller representative of the speech. Controller <b>104</b> then transmits a digital signal representative of the speech to console <b>102</b>. Audio signals may also be provided to a speaker in monitor <b>150</b> or to separate speakers connected to console <b>102</b>.</p>
<p id="p-0025" num="0024">In one implementation (not shown), a memory unit (MU) <b>140</b> may also be inserted into one of controllers <b>104</b>(<b>1</b>) and <b>104</b>(<b>2</b>) to provide additional and portable storage. Portable MUs enable users to store game parameters and entire games for use when playing on other consoles. In this implementation, each console is configured to accommodate two MUs <b>140</b>, although more or less than two MUs may also be employed.</p>
<p id="p-0026" num="0025">Gaming and media system <b>100</b> is generally configured for playing games stored on a memory medium, as well as for downloading and playing games, and reproducing pre-recorded music and videos, from both electronic and hard media sources. With the different storage offerings, titles can be played from the hard disk drive, from optical disk media (e.g., <b>108</b>), from an online source, from a peripheral storage device connected to USB connections <b>110</b> or from MU <b>140</b>.</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 2</figref> is a functional block diagram of gaming and media system <b>100</b> and shows functional components of gaming and media system <b>100</b> in more detail. Console <b>102</b> has a central processing unit (CPU) <b>200</b>, and a memory controller <b>202</b> that facilitates processor access to various types of memory, including a flash Read Only Memory (ROM) <b>204</b>, a Random Access Memory (RAM) <b>206</b>, a hard disk drive <b>208</b>, and media drive <b>106</b>. In one implementation, CPU <b>200</b> includes a level 1 cache <b>210</b>, and a level 2 cache <b>212</b> to temporarily store data and hence reduce the number of memory access cycles made to the hard drive, thereby improving processing speed and throughput.</p>
<p id="p-0028" num="0027">CPU <b>200</b>, memory controller <b>202</b>, and various memory devices are interconnected via one or more buses (not shown). The details of the bus that is used in this implementation are not particularly relevant to understanding the subject matter of interest being discussed herein. However, it will be understood that such a bus might include one or more of serial and parallel buses, a memory bus, a peripheral bus, and a processor or local bus, using any of a variety of bus architectures. By way of example, such architectures can include an Industry Standard Architecture (ISA) bus, a Micro Channel Architecture (MCA) bus, an Enhanced ISA (EISA) bus, a Video Electronics Standards Association (VESA) local bus, and a Peripheral Component Interconnects (PCI) bus also known as a Mezzanine bus.</p>
<p id="p-0029" num="0028">In one implementation, CPU <b>200</b>, memory controller <b>202</b>, ROM <b>204</b>, and RAM <b>206</b> are integrated onto a common module <b>214</b>. In this implementation, ROM <b>204</b> is configured as a flash ROM that is connected to memory controller <b>202</b> via a Peripheral Component Interconnect (PCI) bus and a ROM bus (neither of which are shown). RAM <b>206</b> is configured as multiple Double Data Rate Synchronous Dynamic RAM (DDR SDRAM) modules that are independently controlled by memory controller <b>202</b> via separate buses (not shown). Hard disk drive <b>208</b> and media drive <b>106</b> are shown connected to the memory controller via the PCI bus and an AT Attachment (ATA) bus <b>216</b>. However, in other implementations, dedicated data bus structures of different types can also be applied in the alternative.</p>
<p id="p-0030" num="0029">In some embodiments, ROM <b>204</b> contains an operating system kernel that controls the basic operations of the console and that exposes a collection of Application Programming Interfaces that can be called by games and other applications to perform certain functions and to obtain certain data.</p>
<p id="p-0031" num="0030">A three-dimensional graphics processing unit <b>220</b> and a video encoder <b>222</b> form a video processing pipeline for high speed and high resolution (e.g., High Definition) graphics processing. Data are carried from graphics processing unit <b>220</b> to video encoder <b>222</b> via a digital video bus (not shown). An audio processing unit <b>224</b> and an audio codec (coder/decoder) <b>226</b> form a corresponding audio processing pipeline for multi-channel audio processing of various digital audio formats. Audio data are carried between audio processing unit <b>224</b> and audio codec <b>226</b> via a communication link (not shown). The video and audio processing pipelines output data to an A/V (audio/video) port <b>228</b> for transmission to a television or other display containing one or more speakers. Some audio data formed by audio processing unit <b>224</b> and audio codec <b>226</b> is also directed to one or more headsets through controllers <b>104</b>. In the illustrated implementation, video and audio processing components <b>220</b>-<b>228</b> are mounted on module <b>214</b>.</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 2</figref> shows module <b>214</b> including a USB host controller <b>230</b> and a network interface <b>232</b>. USB host controller <b>230</b> is shown in communication with CPU <b>200</b> and memory controller <b>202</b> via a bus (e.g., PCI bus) and serves as host for peripheral controllers <b>104</b>(<b>1</b>)-<b>104</b>(<b>4</b>). Network interface <b>232</b> provides access to a network (e.g., Internet, home network, etc.) and may be any of a wide variety of various wire or wireless interface components including an Ethernet card, a modem, a Bluetooth module, a cable modem, and the like.</p>
<p id="p-0033" num="0032">In the implementation depicted in <figref idref="DRAWINGS">FIG. 2</figref>, console <b>102</b> includes a controller support subassembly <b>240</b>, for supporting up to four controllers <b>104</b>(<b>1</b>)-<b>104</b>(<b>4</b>). The controller support subassembly <b>240</b> includes any hardware and software components needed to support wired and wireless operation with an external control device, such as for example, a media and game controller. A front panel I/O subassembly <b>242</b> supports the multiple functionalities of power button <b>112</b>, the eject button <b>114</b>, as well as any LEDs (light emitting diodes) or other indicators exposed on the outer surface of console <b>102</b>. Subassemblies <b>240</b> and <b>242</b> are in communication with module <b>214</b> via one or more cable assemblies <b>244</b>. In other implementations, console <b>102</b> can include additional controller subassemblies. The illustrated implementation also shows an optical I/O interface <b>235</b> that is configured to send and receive signals that can be communicated to module <b>214</b>.</p>
<p id="p-0034" num="0033">MUs <b>140</b>(<b>1</b>) and <b>140</b>(<b>2</b>) are illustrated as being connectable to MU ports &#x201c;A&#x201d; <b>130</b>(<b>1</b>) and &#x201c;B&#x201d; <b>130</b>(<b>2</b>) respectively. Additional MUs (e.g., MUs <b>140</b>(<b>3</b>)-<b>140</b>(<b>4</b>)) are illustrated as being connectable to controller <b>104</b>(<b>1</b>), i.e., two MUs for each controller. Each MU <b>140</b> offers additional storage on which games, game parameters, and other data may be stored. In some implementations, the other data can include any of a digital game component, an executable gaming application, an instruction set for expanding a gaming application, and a media file. When inserted into console <b>102</b> or a controller, MU <b>140</b> can be accessed by memory controller <b>202</b>.</p>
<p id="p-0035" num="0034">Headset <b>160</b> is shown connected to controller <b>104</b>(<b>3</b>). Each controller <b>104</b> may be connected to a separate headset <b>160</b>.</p>
<p id="p-0036" num="0035">A system power supply module <b>250</b> provides power to the components of gaming system <b>100</b>. A fan <b>252</b> cools the circuitry within console <b>102</b>.</p>
<p id="p-0037" num="0036">Under some embodiments, an application <b>260</b> comprising machine instructions is stored on hard disk drive <b>208</b>. Application <b>260</b> provides a collection of user interfaces that are associated with console <b>102</b> instead of with an individual game. The user interfaces allow the user to select system settings for console <b>102</b>, access media attached to console <b>102</b>, view information about games, and utilize services provided by a server that is connected to console <b>102</b> through a network connection. When console <b>102</b> is powered on, various portions of application <b>260</b> are loaded into RAM <b>206</b>, and/or caches <b>210</b> and <b>212</b>, for execution on CPU <b>200</b>. Although application <b>260</b> is shown as being stored on hard disk drive <b>208</b>, in alternative embodiments, application <b>260</b> is stored in ROM <b>204</b> with the operating system kernel.</p>
<p id="p-0038" num="0037">Gaming system <b>100</b> may be operated as a standalone system by simply connecting the system to monitor, a television <b>150</b> (<figref idref="DRAWINGS">FIG. 1</figref>), a video projector, or other display device. In this standalone mode, gaming system <b>100</b> enables one or more players to play games, or enjoy digital media, e.g., by watching movies, or listening to music. However, with the integration of broadband connectivity made available through network interface <b>232</b>, gaming system <b>100</b> may further be operated as a participant in a larger network gaming community allowing, among other things, multi-player gaming.</p>
<p id="p-0039" num="0038">The console described in <figref idref="DRAWINGS">FIGS. 1 and 2</figref> is just one example of a gaming machine that can be used with various embodiments described herein. Other gaming machines such as personal computers may be used instead of the gaming console of <figref idref="DRAWINGS">FIGS. 1 and 2</figref>.</p>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. 3</figref> provides a block diagram of elements used in a method shown in <figref idref="DRAWINGS">FIG. 4</figref> for producing responsive cutscenes that respond to actions by a player's avatar while still conveying critical information of a narrative.</p>
<p id="p-0041" num="0040">At step <b>400</b> of <figref idref="DRAWINGS">FIG. 4</figref>, a player triggers the cutscene. As shown in the top perspective view of a gaming environment in <figref idref="DRAWINGS">FIG. 5</figref>, a player can trigger a cutscene under some embodiments by placing their avatar within a circumference <b>502</b> of a non-player character <b>504</b>. In other embodiments, the player can trigger the cutscene by placing the player's avatar <b>600</b> within a same room <b>602</b> as a non-player character <b>604</b> as shown in the top perspective view of a gaming environment in <figref idref="DRAWINGS">FIG. 6</figref>. Other techniques for triggering a cutscene include a player completing one or more tasks or selecting to initiate a cutscene using one or more control buttons.</p>
<p id="p-0042" num="0041">After the player triggers the cutscene, cutscene control <b>300</b> of <figref idref="DRAWINGS">FIG. 3</figref> is started and retrieves a first clip of the cutscene at step <b>402</b> of <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0043" num="0042">Under one embodiment, each cutscene is divided into a plurality of clips. Each clip includes an audio signal representing speech from a non-player character as well as animation descriptors that describe how the non-player character should be animated during the playing of the clip. Under one embodiment, each clip is a WAV file with a header that describes the animation for the non-player character.</p>
<p id="p-0044" num="0043">In <figref idref="DRAWINGS">FIG. 3</figref>, a plurality of cutscenes is shown including cutscene <b>302</b> and cutscene <b>304</b>. Each of the cutscenes includes a plurality of clips. For example, cutscene <b>302</b> includes clips <b>306</b>, <b>308</b> and <b>310</b> and cutscene <b>304</b> includes clips <b>312</b>, <b>314</b> and <b>316</b>. In addition, each cutscene includes a summary clip such as summary clip <b>318</b> of cutscene <b>302</b> and summary clip <b>320</b> of cutscene <b>304</b>. These summary clips are described further below.</p>
<p id="p-0045" num="0044">As noted below, dividing each cutscene into clips allows the cutscene to be broken into natural breakpoints where the cutscene can be restarted if a cutscene clip is interrupted by an action by the player's avatar. By restarting the cutscene at the beginning of the clip that was interrupted, a more natural restart of the cutscene is provided and helps to make the non-player character appear more realistic.</p>
<p id="p-0046" num="0045">At step <b>404</b> of <figref idref="DRAWINGS">FIG. 4</figref>, an audio signal and non-player character animation are produced based on the selected cutscene clip. Under one embodiment, to produce the animation, cut scene control <b>300</b> provides the animation information for the non-player character to a vertex data generation unit <b>323</b>. Vertex data generation unit <b>323</b> uses the animation information and a graphical model <b>322</b> of the non-player character to generate a set of vertices that describe polygons. The vertices are provided to 3D graphics processing unit <b>220</b>, which uses the vertices to render polygons representing the non-player character in the graphical three-dimensional gaming environment. The rendered polygons are transmitted through video encoder <b>222</b> and A/V port <b>228</b> of <figref idref="DRAWINGS">FIG. 2</figref>, to be displayed on an attached display screen. The audio signal for the non-player character is provided to audio processing unit <b>224</b>, which then generates an audio signal through audio code <b>226</b> and A/V port <b>228</b> of <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. 7</figref> provides a screen shot showing a non-player character <b>700</b> that is providing a cut scene narrative during step <b>404</b>.</p>
<p id="p-0048" num="0047">At step <b>406</b>, cutscene control <b>300</b> examines player state data <b>324</b> to determine if the player's avatar has performed an action. Examples of actions include attacking the non-player character, moving a threshold distance away from the non-player character, or performing other actions supported by the game. Under one embodiment, these other actions include things such as belching, performing a silly dance, flexing an arm, performing a rude hand gesture, and faking an attack on the non-player character. Such actions are referred to herein as expressions.</p>
<p id="p-0049" num="0048">Under one embodiment, a player may select an action from a list of actions listed in a menu. <figref idref="DRAWINGS">FIG. 8</figref> provides an example of a screen shot showing a possible menu <b>800</b> of actions that the player's avatar may perform. The player causes the menu to be displayed by either selecting an icon on the display or using one or more controls on the controller. Once the menu has been displayed, the player may select one of the actions from the menu using the controller. In other embodiments, actions may be mapped to one or more controls on the controller so that the player does not have to access the menu.</p>
<p id="p-0050" num="0049">Under some embodiments, the action may include the player's avatar moving more than a threshold distance away from the non-player character. For example, in <figref idref="DRAWINGS">FIG. 5</figref>, the player's avatar may move outside of circumference <b>506</b> and in <figref idref="DRAWINGS">FIG. 6</figref>, the player's avatar may move outside of room <b>602</b>. In both situations, such movement will be interpreted as an action by cut scene control <b>300</b>.</p>
<p id="p-0051" num="0050">If cut scene control determines that the player's avatar has not performed an action at step <b>406</b>, it determines if the end of the current cutscene clip has been reached at step <b>408</b>. If the end of the current cutscene clip has not been reached, cutscene control <b>300</b> continues producing the audio signal and non-player character animation by returning to step <b>404</b>. Steps <b>404</b>, <b>406</b> and <b>408</b> continue in a loop until an avatar action is received at step <b>406</b> or the end of a cutscene clip is received at step <b>408</b>. If the end of the cut scene clip is reached at step <b>408</b>, the process continues at step <b>410</b> where cutscene control <b>300</b> determines if there is another clip for the cutscene. If there is another clip for the cutscene, the next clip is retrieved at step <b>412</b>, and the audio signal and non-player character animation found in the clip is used to animate the non-player character and produce an audio signal for the non-player character.</p>
<p id="p-0052" num="0051">If cut scene control <b>300</b> determines that the player's avatar has performed an action at step <b>406</b>, it maps the action to an impression at step <b>414</b> using an action-to-impression mapping <b>326</b> in an action-to-response database <b>328</b>. An impression is the way that a non-player character will interpret the action. For example, a non-player character may interpret an action as being scary, insulting, impolite, funny, friendly, aggressive, inattentive, or impatient, each of which would be a possible impression. At step <b>416</b>, cutscene control <b>300</b> maps the impression to a response using impression-to-response mapping <b>330</b> of action-to-response database <b>328</b>. By performing two mapping functions, one from an action to an impression, and another from an impression to a response, embodiments described herein allow cutscene responses to be designed without needing to know all possible actions that may be performed. Instead, a limited number of impressions can be specified and cutscene responses can be produced for those impressions. This also allows actions to be added later without affecting the currently produced responses. Multiple actions may be mapped to a single impression in action-to-impression mapping <b>326</b> and multiple impressions may be mapped to a single response in impression-to-response mapping <b>330</b>.</p>
<p id="p-0053" num="0052">At step <b>418</b>, cutscene control <b>300</b> determines if a response has been identified through the impression-to-response mapping in step <b>416</b>. Under some embodiments, an impression may map to no response so that the non-player character will ignore the action taken by the player's avatar. If no response is to be provided at step <b>418</b>, the process returns to step <b>404</b> where the audio signal and non-player character animation continues for the cutscene clip. Note that although steps <b>406</b>, <b>414</b>, <b>416</b> and <b>418</b> appear to occur after step <b>404</b> in the flow diagram of <figref idref="DRAWINGS">FIG. 4</figref>, during steps <b>406</b>, <b>414</b>, <b>416</b> and <b>418</b>, the audio signal and animation of the current cutscene clip continues to be output by cutscene control <b>300</b>. Thus, there is no interruption in the cutscene while these steps are being performed.</p>
<p id="p-0054" num="0053">If the mapping of step <b>416</b> identifies a response, the response is retrieved from a set of stored responses <b>332</b>, which include cut scene responses <b>334</b>, <b>336</b>, and <b>338</b>, for example. The cut scene responses include animation information for movement of the non-player character and/or an audio signal containing dialog that represent the non-player characters response to the action of the player's avatar. In some embodiments, the cut scene responses also include &#x201c;scripting hooks&#x201d; that indicate directorial types of information such as directions to the non-player character to move to a particular location, movement of the camera, lighting effects, background music and sounds, and the like.</p>
<p id="p-0055" num="0054">At step <b>420</b>, the response is examined to determine if the response is a microreaction. Such information can be stored in a header of the response or can be stored in action-to-response database <b>328</b>. A microreaction is a small animation or small change in tone of the audio signal that does not interrupt the audio signal and non-player character animation of the cutscene clip, but instead slightly modifies it as it continues. If the response is a microreaction at step <b>420</b>, the microreaction is combined or integrated with the cut scene clip at step <b>422</b>. This can involve changing the tone of the audio signal of the cut scene by either raising or lowering the pitch or by adding additional animation features to the cutscene animation. If an animation is added, the audio signal of the cut scene continues without interruption as the microreaction animation is integrated with the cut scene animation.</p>
<p id="p-0056" num="0055">For example, in <figref idref="DRAWINGS">FIG. 9</figref>, the cutscene clip includes an animation in which the non-player character points to his left using his left arm <b>900</b>. Normally, during this animation, the non-player character's eyebrows would remain unchanged. However, based on a microreaction response to an avatar action, the right eyebrow of the non-player character is raised relative to the left eyebrow to convey that the non-player character has detected the action taken by the avatar and that the impression left with the non-player character is that the avatar is doing something slightly insulting.</p>
<p id="p-0057" num="0056">If the response found during mapping step <b>416</b> is more than a microreaction at step <b>420</b>, cutscene control <b>300</b> interrupts the cut scene clip and plays the cut scene response. Under one embodiment, the cut scene response is played by providing the animation information to vertex data generation unit <b>323</b>, which uses the animation information and NPC graphics model <b>322</b> to generate sets of vertices representing the movement of the non-player character. Each set of vertices is provided to 3D graphics processing unit <b>220</b>, which uses the vertices to render an animated image of the non-player character. The audio data associated with the response is provided to audio processing unit <b>224</b>.</p>
<p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. 10</figref> provides an example of a cutscene response in which the non-player character is animated to indicate that the impression of the avatar's action was highly insulting to the non-player character and made the non-player character angry. <figref idref="DRAWINGS">FIG. 11</figref> shows a cutscene response in which the non-player character smiles to indicate that the impression of the avatar's action is that it was funny to the non-player character and in <figref idref="DRAWINGS">FIG. 12</figref>, the cutscene response indicates that the impression of the non-player character is that the avatar's action was scary. Not all responses require both audio and animation. In some embodiments, the non-player character will be silent during the cutscene response and simply be animated to reflect the impression of the avatar's action. In other embodiments, the visual appearance of the non-player character will not change during the response other than to synchronize the non-player character's mouth to the audio response.</p>
<p id="p-0059" num="0058">Under some embodiments, a player is able to activate a summary clip of the cut scene by taking an action that conveys an impression of impatience. For example, the player may select an action in which their avatar requests &#x201c;just the facts&#x201d;, and this action will be mapped to an impatience impression. The impression-to-response mapping <b>330</b> will in turn map the impatience impression to a summary response. Under one embodiment, such summary clips are stored together with the other clips of the cut scene. In other embodiments, the summary clips may be stored with the cut scene responses <b>332</b>. The summary clip contains audio data and animation information that causes the non-player character to summarize the critical information that was to be conveyed by the cutscene. In general, cutscenes contain both critical information and stylistic information wherein the critical information is required for the player to advance through the game and the stylistic information is provided to convey an emotional or stylistic attribute to the game. Under one embodiment, the summary clip strips out most of the stylistic information to provide just the critical information.</p>
<p id="p-0060" num="0059">Since playing the summary clip ensures that the player has been given all of the critical information of the cut scene narrative, once the summary clip has been played, there is no need to continue with the cut scene.</p>
<p id="p-0061" num="0060">As such, at step <b>426</b>, cut scene control <b>300</b> determines if the response is a summary response and ends the cutscene procedure at step <b>432</b> if the response was a summary response.</p>
<p id="p-0062" num="0061">If the response was not a summary response, cut scene control <b>300</b> examines player state <b>324</b> to determine if the player is ready to continue with the cut scene clip at step <b>428</b>. For example, if the player's avatar has not returned to the non-player character after moving away from the non-player character, cut scene control <b>300</b> will determine that the player is not ready to continue with the cut scene clip. Under one embodiment, cut scene control <b>300</b> will set a timer if the player is not ready to continue with the cut scene. Cut scene control will then loop at step <b>428</b> until the player is ready to continue with the cut scene or until the timer expires. If the timer expires, cut scene control will unload the current cut scene such that the player will have to trigger the cut scene from the beginning again.</p>
<p id="p-0063" num="0062">When the avatar is ready to continue with the cut scene clip, for example by coming back to the non-player character, cut scene control <b>300</b> retrieves and plays an audio stitch from a collection of audio stitches <b>340</b> at step <b>430</b>. Audio stitches <b>340</b> include a collection of audio stitch files such as audio stitch files <b>342</b>, <b>344</b> and <b>346</b>. Each audio stitch file includes audio and animation data for the non-player character that provides an audio and visual segue between the response and restarting the cut scene clip that was interrupted at step <b>424</b>. Examples of audio stitches include &#x201c;as I was saying&#x201d;, &#x201c;if you are finished&#x201d;, and &#x201c;now then&#x201d;. Such audio stitches provide a smooth transition between a response and the resumption of the cut scene clip.</p>
<p id="p-0064" num="0063">At step <b>434</b>, the cut scene clip that was interrupted at step <b>424</b> is restarted from the beginning of the cut scene clip. By restarting the cut scene clip, cut scene control <b>300</b> ensures that the critical information of the cut scene narrative is provided to the player. In most cases, restarting the cut scene clip will involve reproducing the audio signal and animations that were played when the cut scene clip was initially started. The process then returns to step <b>404</b> to continue playing of the cutscene clip and to await further avatar actions.</p>
<p id="p-0065" num="0064">In other embodiments, instead of playing an audio stitch file and restarting the cut scene clip that was interrupted, cut scene control <b>300</b> will select an alternate cut scene clip to play instead of the interrupted cut scene clip. After playing the alternate cut scene clip, the process continues at step <b>412</b> by selecting a next cut scene clip of the cut scene to play. In such embodiments, the alternate cut scene clip and the next cut scene clip are selected to insure that the critical information of the cut scene is still provided to the player.</p>
<p id="p-0066" num="0065">The process of <figref idref="DRAWINGS">FIG. 4</figref> continues until a summary response is played, there are no more cutscene clips at step <b>410</b>, or a timeout occurs during step <b>428</b>.</p>
<p id="p-0067" num="0066">In the discussion above, the detection of an avatar action was shown as only occurring at step <b>406</b>. However, in other embodiments, cutscene control <b>300</b> is event driven such that at any point in the flow diagram of <figref idref="DRAWINGS">FIG. 4</figref>, cut scene control <b>300</b> may receive an indication from player state <b>324</b> that the avatar has taken an action. Based on that action, cutscene control <b>300</b> may map the action to an impression, map the impression to a cutscene response as shown at steps <b>414</b> and <b>416</b> and produce an animation and audio signal based on the new response. Thus, in the process of playing one response, cutscene control <b>300</b> may interrupt that response to play a different response based on a new avatar action.</p>
<p id="p-0068" num="0067">Although the subject matter has been described in language specific to structural features and/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather, the specific features and acts described above are disclosed as example forms of implementing the claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method comprising:
<claim-text>retrieving a cutscene clip file comprising dialog for a non-player character in a game, the dialog forming at least part of a cutscene that provides a complete narrative comprising critical information and stylistic information to be conveyed to a player;</claim-text>
<claim-text>producing an audio signal representing speech from the non-player character based on the dialog in the retrieved cutscene clip file;</claim-text>
<claim-text>determining that the player's avatar has performed an action while the audio signal is being produced;</claim-text>
<claim-text>mapping the action to an impression the non-player character has of the action, wherein the impression of the action is that the player's avatar is impatient;</claim-text>
<claim-text>mapping the impression that the player's avatar is impatient to a summary response;</claim-text>
<claim-text>stopping the audio signal for the retrieved cutscene clip file before all of the dialog in the retrieved file has been produced as an audio signal;</claim-text>
<claim-text>playing the summary response to produce an audio signal representing speech from the non-player character that is a summarized version of the complete narrative and that comprises all critical information found in the complete narrative with less stylistic information than is found in the complete narrative, thereby ensuring that the critical information of the narrative is provided to the player; and</claim-text>
<claim-text>after playing the summary response, ending the cutscene without completing the audio signal for the retrieved cutscene clip file.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein multiple actions are mapped to a single impression.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the retrieved file comprises one of a plurality of files that together form the complete narrative.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. A computer-readable medium having computer-executable instructions for performing steps comprising:
<claim-text>determining that a player of a game has triggered a cutscene containing critical information required for the player to advance through the game and stylistic information to convey a stylistic attribute to the game;</claim-text>
<claim-text>accessing a cutscene clip file containing data representing an audio signal and animated movements for a non-player character in a game, the audio signal and the animated movements corresponding to the non-player character speaking to a player's avatar;</claim-text>
<claim-text>playing part of the cutscene clip file;</claim-text>
<claim-text>determining that the player's avatar in the game has performed an action;</claim-text>
<claim-text>mapping the action to an impression;</claim-text>
<claim-text>mapping the impression to a summary response;</claim-text>
<claim-text>retrieving the summary response from a set of stored responses;</claim-text>
<claim-text>stopping the cutscene clip file before all of the dialog in the cutscene clip file has been produced as an audio signal;</claim-text>
<claim-text>playing the summary response to produce an audio signal representing speech from the non-player character, the summary response comprising all the critical information of the cutscene but less stylistic information than the cutscene, thereby ensuring that the critical information of the cutscene is provided to the player; and</claim-text>
<claim-text>after playing the summary response, immediately ending the cutscene at the end of the summary response.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The computer-readable medium of <claim-ref idref="CLM-00004">claim 4</claim-ref> wherein mapping an action to an impression comprises mapping the action to an impression that is not limited to being mapped to by only one action.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The computer-readable medium of <claim-ref idref="CLM-00004">claim 4</claim-ref> wherein the impression is the way a non-player interprets the action.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The computer-readable medium of <claim-ref idref="CLM-00006">claim 6</claim-ref> wherein the non-player character interprets the action as being impatient.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The computer-readable medium of <claim-ref idref="CLM-00004">claim 4</claim-ref> wherein the steps further comprise examining a header of the response to determine if it is a microreaction, wherein a microreaction combines animated movement in the cutscene clip file with animated movement of a response to form modified animations for the non-player character.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. A method comprising:
<claim-text>a player triggering a cutscene;</claim-text>
<claim-text>a cutscene control retrieving one of a plurality of cutscene clips that together constitute the cutscene, each cutscene clip having a start and an end;</claim-text>
<claim-text>producing an audio signal and an animation for a non-player character from the retrieved cutscene clip;</claim-text>
<claim-text>determining that a player's avatar has performed an action;</claim-text>
<claim-text>determining that the non-player character should respond to the action;</claim-text>
<claim-text>stopping the cutscene clip before reaching the end of the cutscene clip;</claim-text>
<claim-text>having the non-player character respond to the action by summarizing critical information that is in the plurality of cutscene clips while stripping out most stylistic information in the plurality of cutscene clips;</claim-text>
<claim-text>upon completing the response, immediately ending the cutscene.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref> wherein determining that a non-player character should respond to an action comprises mapping the action to an impression and mapping the impression to a response.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref> wherein the action comprises the player's avatar asking for just the facts. </claim-text>
</claim>
</claims>
</us-patent-grant>
