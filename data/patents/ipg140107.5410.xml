<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626510-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626510</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12559844</doc-number>
<date>20090915</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>JP</country>
<doc-number>2009-074849</doc-number>
<date>20090325</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>1149</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>10</class>
<subclass>L</subclass>
<main-group>13</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>704260</main-classification>
<further-classification>704258</further-classification>
</classification-national>
<invention-title id="d2e71">Speech synthesizing device, computer program product, and method</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5732395</doc-number>
<kind>A</kind>
<name>Silverman</name>
<date>19980300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704260</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>8015011</doc-number>
<kind>B2</kind>
<name>Nagano et al.</name>
<date>20110900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704260</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>2002/0120451</doc-number>
<kind>A1</kind>
<name>Kato et al.</name>
<date>20020800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704258</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>JP</country>
<doc-number>07-210194</doc-number>
<date>19950800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>JP</country>
<doc-number>07-253987</doc-number>
<date>19951000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>JP</country>
<doc-number>3060276</doc-number>
<date>20000400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>JP</country>
<doc-number>2007-212884</doc-number>
<date>20070800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>JP</country>
<doc-number>2008-225254</doc-number>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>JP</country>
<doc-number>2009-037214</doc-number>
<date>20090200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00010">
<othercit>Japanese Office Action for Japanese Application No. 2009-074849 mailed on Aug. 7, 2012.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>9</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>704258</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704260</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>10</number-of-drawing-sheets>
<number-of-figures>14</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20100250254</doc-number>
<kind>A1</kind>
<date>20100930</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Mizutani</last-name>
<first-name>Nobuaki</first-name>
<address>
<city>Kanagawa</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Mizutani</last-name>
<first-name>Nobuaki</first-name>
<address>
<city>Kanagawa</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Turocy &#x26; Watson, LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Kabushiki Kaisha Toshiba</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Harper</last-name>
<first-name>Vincent P</first-name>
<department>2657</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">An acquiring unit acquires pattern sentences, which are similar to one another and include fixed segments and non-fixed segments, and substitution words that are substituted for the non-fixed segments. A sentence generating unit generates target sentences by replacing the non-fixed segments with the substitution words for each of the pattern sentences. A first synthetic-sound generating unit generates a first synthetic sound, a synthetic sound of the fixed segment, and a second synthetic-sound generating unit generates a second synthetic sound, a synthetic sound of the substitution word, for each of the target sentences. A calculating unit calculates a discontinuity value of a boundary between the first synthetic sound and the second synthetic sound for each of the target sentences and a selecting unit selects the target sentence having the smallest discontinuity value. A connecting unit connects the first synthetic sound and the second synthetic sound of the target sentence selected.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="216.75mm" wi="130.56mm" file="US08626510-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="253.24mm" wi="164.93mm" orientation="landscape" file="US08626510-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="238.84mm" wi="170.94mm" file="US08626510-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="187.71mm" wi="163.91mm" file="US08626510-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="214.38mm" wi="188.47mm" file="US08626510-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="195.07mm" wi="163.24mm" file="US08626510-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="252.56mm" wi="161.80mm" orientation="landscape" file="US08626510-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="245.87mm" wi="165.69mm" file="US08626510-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="238.17mm" wi="163.24mm" file="US08626510-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="157.65mm" wi="157.65mm" file="US08626510-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="225.55mm" wi="150.96mm" file="US08626510-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application is based upon and claims the benefit of priority from the prior Japanese Patent Application No. 2009-074849, filed on Mar. 25, 2009; the entire contents of which are incorporated herein by reference.</p>
<heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">1. Field of the Invention</p>
<p id="p-0004" num="0003">The present invention relates to a device, a computer program product, and a method for speech synthesis.</p>
<p id="p-0005" num="0004">2. Description of the Related Art</p>
<p id="p-0006" num="0005">Speech synthesizing devices have been applied to voice services for traffic information and weather reports, bank transfer inquiry services, and interfaces of humanlike machines such as robots. The speech synthesizing devices therefore need to offer synthetic speeches that sound clear and natural.</p>
<p id="p-0007" num="0006">An example of such a technology conducts speech synthesis for a sentence containing fixed segments that are fixed information and non-fixed segments that are variable information (see JP-A H8-63187 (KOKAI), for example). Concerning the fixed segments, time-changing patterns of fundamental frequencies (hereinafter, referred to as &#x201c;F0 patterns&#x201d;) are extracted and stored from speeches of the sentences produced by a human. Concerning the non-fixed segments, F0 patterns corresponding to all combinations of the number of syllables of words or phrases and stresses of the words or phrases, which are expected to be input, are stored. A synthetic speech that sounds natural as a sentence is generated by selecting or generating F0 patterns for each of fixed segments and non-fixed segments and then connecting the F0 patterns.</p>
<p id="p-0008" num="0007">With conventional speech synthesizing devices, however, because a synthetic speech of only a single sentence is generated, unnaturalness accompanied by connecting synthetic sounds tends to be noticeable.</p>
<heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0009" num="0008">According to one aspect of the present invention, a speech synthesizing device includes an acquiring unit configured to acquire a plurality of pattern sentences, which are similar to one another and each include a fixed segment and a non-fixed segment, and a substitution word, the fixed segment is not to be replaced with any other word, the non-fixed segment is to be replaced with another word, the substitution word is substituted for the non-fixed segment; a sentence generating unit configured to generate a plurality of target sentences by replacing the non-fixed segment with the substitution word for each of the pattern sentences; a first synthetic-sound generating unit configured to generate a first synthetic sound, which is a synthetic sound of the fixed segment, for each of the target sentences; a second synthetic-sound generating unit configured to generate a second synthetic sound, which is a synthetic sound of the substitution word, for each of the target sentences; a calculating unit configured to calculate a discontinuity value of a boundary between the first synthetic sound and the second synthetic sound, for each of the target sentences; a selecting unit configured to select one of the target sentences having the smallest discontinuity value from the target sentences; and a connecting unit configured to connect the first synthetic sound and the second synthetic sound of the target sentence selected.</p>
<p id="p-0010" num="0009">According to another aspect of the present invention, a speech synthesizing device includes an acquiring unit configured to acquire a pattern sentence, which includes a fixed segment that is not to be replaced with any other word and a non-fixed segment that is to be replaced with another word, and a substitution word that is substituted for the non-fixed segment; a first sentence generating unit configured to generate a target sentence by replacing the non-fixed segment with the substitution word; a second sentence generating unit configured to generate an alternative target sentence that has a higher similarity to the target sentence than a threshold; a first synthetic-sound generating unit configured to generate a first synthetic sound, which is a synthetic sound of the fixed segment, for the target sentence and the alternative target sentence; a second synthetic-sound generating unit configured to generate a second synthetic sound, which is a synthetic sound of the substitution word, for the target sentence and the alternative target sentence; a calculating unit configured to calculate a discontinuity value of a boundary between the first synthetic sound and the second synthetic sound, for the target sentence and the alternative target sentence; a selecting unit configured to select the target sentence or the alternative target sentence, whichever has the smaller discontinuity value; and a connecting unit configured to connect the first synthetic sound and the second synthetic sound of the target sentence or the alternative target sentence that is selected.</p>
<p id="p-0011" num="0010">According to still another aspect of the present invention, a computer program product has a computer readable medium including programmed instructions for synthesizing a speech that, when executed by a computer, causes the computer to perform acquiring a plurality of pattern sentences, which are similar to one another and each include a fixed segment and a non-fixed segment, and a substitution word, the fixed segment is not to be replaced with any other word, the non-fixed segment is to be replaced with another word, the substitution word is substituted for the non-fixed segment; generating a plurality of target sentences by replacing the non-fixed segment with the substitution word for each of the pattern sentences; generating a first synthetic sound, which is a synthetic sound of the fixed segment, for each of the target sentences; generating a second synthetic sound, which is a synthetic sound of the substitution word, for each of the target sentences; calculating a discontinuity value of a boundary between the first synthetic sound and the second synthetic sound, for each of the target sentences; selecting one of the target sentences having the smallest discontinuity value from the target sentences; and connecting the first synthetic sound and the second synthetic sound of the target sentence selected.</p>
<p id="p-0012" num="0011">According to still another aspect of the present invention, a computer program product has a computer readable medium including programmed instructions for synthesizing a speech that, when executed by a computer, causes the computer to perform acquiring a pattern sentence, which includes a fixed segment that is not to be replaced with any other word and a non-fixed segment that is to be replaced with another word, and a substitution word that is to be substituted for the non-fixed segment; generating a target sentence by replacing the non-fixed segment with the substitution word; generating an alternative target sentence having a higher similarity to the target sentence than a threshold; generating a first synthetic sound, which is a synthetic sound of the fixed segment, for the target sentence and the alternative target sentence; generating a second synthetic sound, which is a synthetic sound of the substitution word, for the target sentence and the alternative target sentence; calculating a discontinuity value of a boundary between the first synthetic sound and the second synthetic sound, for the target sentence and the alternative target sentence; selecting the target sentence or the alternative target sentence, whichever has the smaller discontinuity value; and connecting the first synthetic sound and the second synthetic sound of the target sentence or the alternative target sentence that is selected.</p>
<p id="p-0013" num="0012">According to still another aspect of the present invention, a speech synthesizing method includes acquiring a plurality of pattern sentences, which are similar to one another and each include a fixed segment and a non-fixed segment, and a substitution word, the fixed segment is not to be replaced with any other word, the non-fixed segment is to be replaced with another word, the substitution word is substituted for the non-fixed segment; generating a plurality of target sentences by replacing the non-fixed segment with the substitution word for each of the pattern sentences; generating a first synthetic sound, which is a synthetic sound of the fixed segment, for each of the target sentences; generating a second synthetic sound, which is a synthetic sound of the substitution word, for each of the target sentences; calculating a discontinuity value of a boundary between the first synthetic sound and the second synthetic sound, for each of the target sentences; selecting one of the target sentences having the smallest discontinuity value from the target sentences; and connecting the first synthetic sound and the second synthetic sound of the target sentence selected.</p>
<p id="p-0014" num="0013">According to still another aspect of the present invention, a speech synthesizing method includes acquiring a pattern sentence, which includes a fixed segment that is not to be replaced with any other word and a non-fixed segment that is to be replaced with another word, and a substitution word that is to be substituted for the non-fixed segment; generating a target sentence by replacing the non-fixed segment with the substitution word; generating an alternative target sentence having a higher similarity to the target sentence than a threshold; generating a first synthetic sound, which is a synthetic sound of the fixed segment, for the target sentence and the alternative target sentence; generating a second synthetic sound, which is a synthetic sound of the substitution word, for the target sentence and the alternative target sentence; calculating a discontinuity value of a boundary between the first synthetic sound and the second synthetic sound, for the target sentence and the alternative target sentence; selecting the target sentence or the alternative target sentence, whichever has the smaller discontinuity value; and connecting the first synthetic sound and the second synthetic sound of the target sentence or the alternative target sentence that is selected.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram showing an example configuration of a speech synthesizing device according to a first embodiment;</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 2</figref> is a diagram showing examples of pattern sentences acquired by an acquiring unit according to the first embodiment;</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 3</figref> is a diagram showing examples of substitution words acquired by the acquiring unit according to the first embodiment;</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 4</figref> is a diagram showing examples of target sentences generated by a sentence generating unit according to the first embodiment;</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 5</figref> is a diagram explaining an example method of calculating a discontinuity value adopted by a calculating unit according to the first embodiment;</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 6</figref> is a diagram showing an example of a synthetic speech generated from synthetic sounds that are connected by a connecting unit according to the first embodiment;</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 7</figref> is a flowchart showing an example procedure of a speech synthesizing process performed by the speech synthesizing device according to the first embodiment;</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 8</figref> is a block diagram showing an example configuration of a speech synthesizing device according to a second embodiment;</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 9</figref> is a diagram explaining an example of an alternative target sentence generated by an alternative target-sentence generating unit according to the second embodiment by changing the word order;</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 10</figref> is a diagram explaining an example of an alternative target sentence generated by the alternative target-sentence generating unit according to the second embodiment by replacing a word with its synonym;</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 11</figref> is a diagram explaining an example of an alternative target sentence generated by the alternative target-sentence generating unit according to the second embodiment by replacing a phrase with an alternative phrase;</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 12</figref> is a diagram explaining another example of an alternative target sentence generated by the alternative target-sentence generating unit according to the second embodiment by replacing a phrase with an alternative phrase;</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 13</figref> is a diagram explaining another example of an alternative target sentence generated by the alternative target-sentence generating unit according to the second embodiment by replacing phrases with alternative phrases; and</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 14</figref> is a flowchart showing an example procedure of a speech synthesizing process performed by the speech synthesizing device according to the second embodiment.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION OF THE INVENTION</heading>
<p id="p-0029" num="0028">Exemplary embodiments of a speech synthesizing device, a computer program product, and a method according to the present invention are described in detail below with reference to the accompanying drawings.</p>
<p id="p-0030" num="0029">In a first embodiment, a plurality of target sentences are generated by replacing non-fixed segments of pattern sentences that are similar to one another with substitution words; one of the target sentences that has the smallest discontinuity value for the boundary between a fixed synthetic sound and rule-based synthetic sound is selected from the generated target sentences; and a synthetic speech is output by connecting the fixed synthetic sounds and the rule-based synthetic sounds of the selected target sentence. Pattern sentences are similar to one another and include fixed segments that are not to be replaced with any other word and non-fixed segments that are to be replaced with different words.</p>
<p id="p-0031" num="0030">First, the configuration of a speech synthesizing device according to the first embodiment is described.</p>
<p id="p-0032" num="0031">As illustrated in <figref idref="DRAWINGS">FIG. 1</figref>, the speech synthesizing device <b>1</b> includes an input unit <b>10</b>, an output unit <b>20</b>, a storage unit <b>30</b>, an acquiring unit <b>40</b>, a sentence generating unit <b>45</b>, a fixed synthetic-sound generating unit <b>50</b>, a rule-based synthetic-sound generating unit <b>55</b>, a calculating unit <b>60</b>, a selecting unit <b>65</b>, a connecting unit <b>70</b>, and an output controlling unit <b>75</b>.</p>
<p id="p-0033" num="0032">The input unit <b>10</b> is configured to input a sentence or word for speech synthesis. A conventional input device such as a keyboard, a mouse, and a touch panel may be used.</p>
<p id="p-0034" num="0033">The output unit <b>20</b> outputs speech synthesis results in response to an instruction from the later-described output controlling unit <b>75</b>. A conventional speech output device such as a speaker may be used.</p>
<p id="p-0035" num="0034">The storage unit <b>30</b> stores information that is used for various processes executed by the speech synthesizing device <b>1</b>. The storage unit <b>30</b> may be a conventional recording medium in which information is magnetically, electrically, or optically stored, such as a hard disk drive (HDD), a solid state drive (SSD), a memory card, an optical disk, and a random access memory (RAM). The storage unit <b>30</b> includes a speech storage unit <b>32</b> and a dictionary storage unit <b>34</b>. The speech storage unit <b>32</b> and the dictionary storage unit <b>34</b> are described in detail later.</p>
<p id="p-0036" num="0035">The acquiring unit <b>40</b> acquires a plurality of pattern sentences that are similar to one another and include fixed segments that are not to be replaced with any other word and non-fixed segments that are to be replaced with different words. The acquiring unit <b>40</b> also acquires substitution words with which the non-fixed segments are replaced. More specifically, the acquiring unit <b>40</b> acquires the similar pattern sentences and the substitution words that are input by the input unit <b>10</b>. If a non-fixed segment included in each of the pattern sentences is singular, a substitution word acquired by the acquiring unit <b>40</b> is also singular. The &#x201c;similar&#x201d; sentences mean that they are semantically equivalent to one another. The similar sentences may be determined to be similar by a user, or sentences that have degrees of similarity that exceed a threshold may be selected. A &#x201c;word&#x201d; can be a single character or a single word, or a combination thereof.</p>
<p id="p-0037" num="0036">As schematically illustrated in <figref idref="DRAWINGS">FIG. 2</figref>, pattern sentences may be Japanese sentences that are intended to report the evening weather and are semantically equivalent to one another. In each of the pattern sentences, it is assumed that the name of a specific area (e.g., Tokyo, Kanagawa, or Chiba) is inserted in A, while a specific condition of weather (e.g., fine, cloudy, or rainy) is inserted in B.</p>
<p id="p-0038" num="0037">In the first embodiment, portions sandwiched by bracket signs &#x2018;[&#x2019; and &#x2018;]&#x2019; in each of the pattern sentences are non-fixed segments, and other portions are fixed segments. For example, in a pattern sentence <b>101</b> in <figref idref="DRAWINGS">FIG. 2</figref>, words <b>102</b>, <b>103</b>, and <b>104</b> are fixed segments, and segments A and B are non-fixed segments.</p>
<p id="p-0039" num="0038">Substitution words <b>111</b> and <b>112</b> shown in <figref idref="DRAWINGS">FIG. 3</figref> substitute for the non-fixed segments A and B in the group of pattern sentences indicated in <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0040" num="0039">In <figref idref="DRAWINGS">FIG. 1</figref>, the sentence generating unit <b>45</b> replaces, in each of the pattern sentences acquired by the acquiring unit <b>40</b>, the non-fixed segments with the substitution words acquired by the acquiring unit <b>40</b> to generate a plurality of target sentences.</p>
<p id="p-0041" num="0040">The target sentences shown in <figref idref="DRAWINGS">FIG. 4</figref> are generated by substituting the substitution words <b>111</b> and <b>112</b> for the non-fixed segments A and B of each of the pattern sentences in <figref idref="DRAWINGS">FIG. 2</figref>. For example, a target sentence <b>121</b> in <figref idref="DRAWINGS">FIG. 4</figref> is generated by substituting the substitution words <b>111</b> and <b>112</b> for the non-fixed segments A and B, respectively, of the pattern sentence <b>101</b> in <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0042" num="0041">In <figref idref="DRAWINGS">FIG. 1</figref>, the speech storage unit <b>32</b> stores speech data that is to be used by the later-described fixed synthetic-sound generating unit <b>50</b> for speech synthesis. The &#x201c;speech data&#x201d; represents waveforms of prerecorded speeches, speech parameters obtained by converting such speeches, or the like. The &#x201c;speech parameters&#x201d; are speeches expressed numerically by use of speech generation models to compress the data volume. Examples of speech generation models include formants, PARCOR, LSP, LPC, and cepstrum. The speech parameters are stored for each phonogram, or in smaller units depending on environments such as preceding/following phonograms or the like.</p>
<p id="p-0043" num="0042">The fixed synthetic-sound generating unit <b>50</b> generates a fixed synthetic sound, which is a synthetic sound for a fixed segment, for each of the target sentences generated by the sentence generating unit <b>45</b>. More specifically, the fixed synthetic-sound generating unit <b>50</b> uses the speech data stored in the speech storage unit <b>32</b>, and then generates a fixed synthetic sound for each of the target sentences generated by the sentence generating unit <b>45</b>.</p>
<p id="p-0044" num="0043">When generating the fixed synthetic sound, a recording and editing method, in which a prerecorded speech is reproduced, or an analysis and synthesis method, in which a speech is synthesized from speech parameters that are obtained by converting a prerecorded speech, may be adopted. Examples of the analysis and synthesis method include formant synthesis, PARCOR synthesis, LSP synthesis, LPC synthesis, cepstrum synthesis, and waveform editing with which waveforms are directly edited. In the analysis and synthesis method, a speech parameter string of a fixed segment is generated from phonograms or the like, and a fixed synthetic sound is generated from the duration, F0 pattern, and speech parameter string of the fixed segment.</p>
<p id="p-0045" num="0044">The dictionary storage unit <b>34</b> stores dictionary data and speech parameter strings extracted from natural speeches, which are to be used for the speech synthesis by the later-described rule-based synthetic-sound generating unit <b>55</b>. The &#x201c;dictionary data&#x201d; includes data for linguistic analysis, such as morphological analysis and syntactic analysis of words, and data for accent and intonation processing. The dictionary storage unit <b>34</b> may also store model parameters that are obtained by approximating the speech parameter strings using models.</p>
<p id="p-0046" num="0045">The rule-based synthetic-sound generating unit <b>55</b> generates a rule-based synthetic sound, which is a synthetic sound of a substitution word, for each of the target sentences generated by the sentence generating unit <b>45</b>. More specifically, the rule-based synthetic-sound generating unit <b>55</b> generates the rule-based synthetic sound for each of the target sentences generated by the sentence generating unit <b>45</b> by referring to the dictionary data stored in the dictionary storage unit <b>34</b>.</p>
<p id="p-0047" num="0046">When generating the rule-based synthetic sound, a rule-based sound synthesis method may be adopted, with which a speech is generated from words by using rules such as dictionary data or the like. As a rule-based sound synthesis method, a method of reading speech parameter strings extracted from a natural speech, a method of converting model parameters to time-series speech parameter strings, or a method of generating model parameters regularly from the word analysis results and converting the model parameters to time-series speech parameter strings may be adopted.</p>
<p id="p-0048" num="0047">The calculating unit <b>60</b> calculates a discontinuity value of the boundary between a fixed synthetic sound generated by the fixed synthetic-sound generating unit <b>50</b> and a rule-based synthetic sound generated by the rule-based synthetic-sound generating unit <b>55</b> for each of the target sentences generated by the sentence generating unit <b>45</b>.</p>
<p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. 5</figref> shows speech waveforms that indicate synthetic sounds generated for some of the target sentences in <figref idref="DRAWINGS">FIG. 4</figref>. The calculating unit <b>60</b> calculates the discontinuity value of the connection boundary of the speech waveforms as a distortion value &#x3b5; for each target sentence.</p>
<p id="p-0050" num="0049">For example, speech waveforms <b>132</b>, <b>133</b>, and <b>134</b> in <figref idref="DRAWINGS">FIG. 5</figref> indicate the fixed synthetic sounds of the words <b>102</b>, <b>103</b>, and <b>104</b>, respectively, which are the fixed segments of the target sentence <b>121</b>. Speech waveforms <b>141</b> and <b>142</b> indicate the rule-based synthetic-sound of the substitution words <b>111</b> and <b>112</b>, respectively, of the target sentence <b>121</b>. In the example of <figref idref="DRAWINGS">FIG. 5</figref>, five synthetic sounds are generated for the target sentence <b>121</b>, which form four connection boundaries <b>151</b> to <b>154</b> in the target sentence <b>121</b>. Then, the calculating unit <b>60</b> calculates the discontinuity value of the connection boundaries <b>151</b> to <b>154</b> of the target sentence <b>121</b> as a distortion value &#x3b5;<b>81</b>.</p>
<p id="p-0051" num="0050">When a target sentence includes more than one connection boundaries as in the target sentence <b>121</b> in <figref idref="DRAWINGS">FIG. 5</figref>, a value having the highest degree of discontinuity among the discontinuity values of the connection boundaries may be determined as the distortion value &#x3b5;, or the sum or average of the discontinuity values of the connection boundaries may be determined as the distortion value &#x3b5;.</p>
<p id="p-0052" num="0051">In <figref idref="DRAWINGS">FIG. 1</figref>, the selecting unit <b>65</b> selects one of the target sentences having the smallest discontinuity value that is calculated by the calculating unit <b>60</b> from the target sentences generated by the sentence generating unit <b>45</b>. In particular, the selecting unit <b>65</b> identifies &#x3b5;_best, which is the smallest discontinuity value among the discontinuity values of the target sentences using expression (1) and selects the target sentence having this &#x3b5;_best.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>&#x3b5;_best=arg min &#x3b5;<sub>&#x2014;</sub><i>n</i>&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0053" num="0052">In the expression (1), &#x201c;&#x3b5;_n&#x201d; denotes the distortion value of each target sentence. In the example of <figref idref="DRAWINGS">FIG. 5</figref>, &#x3b5;_n={&#x3b5;81 . . . &#x3b5;90}. In other words, the expression (1) finds the smallest &#x3b5; from &#x3b5;_n.</p>
<p id="p-0054" num="0053">In the example of <figref idref="DRAWINGS">FIG. 5</figref>, it is assumed that &#x3b5;_best=&#x3b5;81, and that the selecting unit <b>65</b> selects the target sentence <b>121</b> from the target sentences in <figref idref="DRAWINGS">FIG. 5</figref>.</p>
<p id="p-0055" num="0054">The connecting unit <b>70</b> connects the fixed synthetic sounds and the rule-based synthetic sounds in the target sentence selected by the selecting unit <b>65</b>. The connecting unit <b>70</b> may execute post-processing such as smoothing so that the connection boundaries of the synthetic sounds can be smoothly connected.</p>
<p id="p-0056" num="0055">In the example of <figref idref="DRAWINGS">FIG. 5</figref>, the selecting unit <b>65</b> selects the target sentence <b>121</b>, and therefore the connecting unit <b>70</b> connects the speech waveforms <b>132</b>, <b>141</b>, <b>133</b>, <b>142</b>, and <b>134</b> to generate the synthetic speech of the target sentence <b>121</b>, as illustrated in <figref idref="DRAWINGS">FIG. 6</figref>.</p>
<p id="p-0057" num="0056">The output controlling unit <b>75</b> outputs the speech that is generated by the connecting operation of the connecting unit <b>70</b>, through the output unit <b>20</b>. More specifically, the output controlling unit <b>75</b> performs a digital-to-analog conversion on the synthetic speech generated by the connecting operation of the connecting unit <b>70</b> in order to obtain an analog signal and output the speech through the output unit <b>20</b>.</p>
<p id="p-0058" num="0057">The acquiring unit <b>40</b>, the sentence generating unit <b>45</b>, the fixed synthetic-sound generating unit <b>50</b>, the rule-based synthetic-sound generating unit <b>55</b>, the calculating unit <b>60</b>, the selecting unit <b>65</b>, the connecting unit <b>70</b>, and the output controlling unit <b>75</b> may be implemented by conventional controlling devices, which include components such as a central processing unit (CPU) and an application specific integrated circuit (ASIC).</p>
<p id="p-0059" num="0058">The operation of the speech synthesizing device according to the first embodiment is now explained.</p>
<p id="p-0060" num="0059">At Step S<b>10</b> shown in <figref idref="DRAWINGS">FIG. 7</figref>, the acquiring unit <b>40</b> acquires the pattern sentences and the substitution words that are input by the input unit <b>10</b>.</p>
<p id="p-0061" num="0060">At Step S<b>12</b>, the sentence generating unit <b>45</b> generates target sentences by substituting the substitution words acquired by the acquiring unit <b>40</b> for the non-fixed segments of the pattern sentences acquired by the acquiring unit <b>40</b>.</p>
<p id="p-0062" num="0061">At Step S<b>14</b>, the fixed synthetic-sound generating unit <b>50</b> generates fixed synthetic sounds for the target sentences generated by the sentence generating unit <b>45</b> by using the speech data stored in the speech storage unit <b>32</b>.</p>
<p id="p-0063" num="0062">At Step S<b>16</b>, the rule-based synthetic-sound generating unit <b>55</b> generates rule-based synthetic sounds for the target sentences generated by the sentence generating unit <b>45</b>, by referring the dictionary data stored in the dictionary storage unit <b>34</b>.</p>
<p id="p-0064" num="0063">At Step S<b>18</b>, the calculating unit <b>60</b> calculates a discontinuity value of the boundary between the fixed synthetic sounds generated by the fixed synthetic-sound generating unit <b>50</b> and the rule-based synthetic sounds generated by the rule-based synthetic-sound generating unit <b>55</b>, for the target sentences generated by the sentence generating unit <b>45</b>.</p>
<p id="p-0065" num="0064">At Step S<b>20</b>, the selecting unit <b>65</b> selects one of the target sentences having the smallest discontinuity value calculated by the calculating unit <b>60</b>, from the target sentences generated by the sentence generating unit <b>45</b>.</p>
<p id="p-0066" num="0065">At Step S<b>22</b>, the connecting unit <b>70</b> connects the fixed synthetic sounds and the rule-based synthetic sounds of the target sentence selected by the selecting unit <b>65</b>.</p>
<p id="p-0067" num="0066">At Step S<b>24</b>, the output controlling unit <b>75</b> outputs the synthetic speech connected by the connecting unit <b>70</b> through the output unit <b>20</b>.</p>
<p id="p-0068" num="0067">As described above, according to the first embodiment, a plurality of target sentences are generated by substituting substitution words for non-fixed segments of pattern sentences that are semantically equivalent to one another; one of the target sentences having the smallest discontinuity value for the connection boundary between the fixed synthetic sounds and the rule-based synthetic sounds is selected from the target sentences; and a synthetic speech is generated and output by connecting the fixed synthetic sounds and the rule-based synthetic sounds of the selected target sentence.</p>
<p id="p-0069" num="0068">According to the first embodiment, because the synthetic speech of the target sentence having the smallest discontinuity value is selected for output from a plurality of the target sentences that are semantically equal to one another, the synthetic speech can be generated with less unnaturalness, which is accompanied by connecting synthetic sounds.</p>
<p id="p-0070" num="0069">Next, a second embodiment will be described in which a target sentence and an alternative target sentence that is semantically equivalent to the target sentence are generated from a single pattern sentence; a sentence having a smaller discontinuity value for the connection boundary between the fixed synthetic sounds and the rule-based synthetic sounds is selected from the generated target sentence and the alternative target sentence; the fixed synthetic sounds and the rule-based synthetic sounds of the selected sentence are connected into a synthetic speech; and the synthetic speech is output.</p>
<p id="p-0071" num="0070">The following explanation mainly focuses on differences between the first and second embodiments. Components that have similar functions to those of the first embodiment are given the same names and numerals, and the explanation thereof is omitted.</p>
<p id="p-0072" num="0071">First, the configuration of a speech synthesizing device according to the second embodiment is described.</p>
<p id="p-0073" num="0072">The speech synthesizing device <b>1001</b> shown in <figref idref="DRAWINGS">FIG. 8</figref> is differentiated from the speech synthesizing device <b>1</b> according to the first embodiment in that an acquiring unit <b>1040</b> acquires a single pattern sentence.</p>
<p id="p-0074" num="0073">In addition, the speech synthesizing device <b>1001</b> is differentiated from the speech synthesizing device <b>1</b> in that a target-sentence generating unit <b>1045</b> and an alternative target-sentence generating unit <b>1046</b> are included in place of the sentence generating unit <b>45</b>.</p>
<p id="p-0075" num="0074">Furthermore, the speech synthesizing device <b>1001</b> is differentiated from the speech synthesizing device <b>1</b> in that a fixed synthetic-sound generating unit <b>1050</b> generates fixed synthetic sounds, a rule-based synthetic-sound generating unit <b>1055</b> generates rule-based synthetic sounds, and a calculating unit <b>1060</b> calculates discontinuity values, for each of the target sentence and the alternative target sentence.</p>
<p id="p-0076" num="0075">Still further, the speech synthesizing device <b>1001</b> is differentiated from the speech synthesizing device <b>1</b> in that a selecting unit <b>1065</b> selects the target sentence or the alternative target sentence whichever has the smallest discontinuity value, and a connecting unit <b>1070</b> connects the synthetic sounds of the target sentence or alternative target sentence whichever is selected.</p>
<p id="p-0077" num="0076">In the following description, the target-sentence generating unit <b>1045</b> and the alternative target-sentence generating unit <b>1046</b>, which are the main differences between the first and second embodiments, are explained.</p>
<p id="p-0078" num="0077">The target-sentence generating unit <b>1045</b> substitutes substitution words acquired by the acquiring unit <b>1040</b> for the non-fixed segments of a pattern sentence acquired by the acquiring unit <b>1040</b>, and generates a target sentence. The target-sentence generating unit <b>1045</b> generates a single target sentence. Other functions are the same as those of the sentence generating unit <b>45</b> according to the first embodiment, and therefore the detailed explanation is omitted.</p>
<p id="p-0079" num="0078">The alternative target-sentence generating unit <b>1046</b> generates an alternative target sentence that has a degree of similarity to the target sentence generated by the target-sentence generating unit <b>1045</b> higher than a threshold. More specifically, the alternative target-sentence generating unit <b>1046</b> generates the alternative target sentence by changing the word order of the pattern sentence, replacing some words of the pattern sentence with their synonyms, and/or replacing some phrases of the pattern sentence with other phrases, and also by substituting the substitution words for the non-fixed segments.</p>
<p id="p-0080" num="0079">The alternative target-sentence generating unit <b>1046</b> calculates the degree of similarity by using an edit distance that indicates how similar the alternative target sentence is to the target sentence, and generates the alternative target sentence of which the degree of similarity exceeds the threshold. More specifically, the alternative target-sentence generating unit <b>1046</b> calculates the degree of similarity between the target sentence and the alternative target sentence in accordance with expression (2).
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>&#x3c6;=1/(&#x3b3;+1)&#x2003;&#x2003;(2)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0081" num="0080">In the expression (2), the similarity &#x3c6; takes on values from 0 to 1, where it represents that the sentences have more similar (equivalent) meanings to each other as the value is closer to 1. The edit distance &#x3b3; represents how many times the following operations should be repeated to generate the alternative target sentence from the target sentence. The operations are (1) inserting a word into a specific position of the target sentence; (2) deleting a word from a specific position of the target sentence; and (3) changing the order of words at a specific position of the target sentence.</p>
<p id="p-0082" num="0081">The method of generating an alternative target sentence is discussed below with a specific example in which the threshold of the similarity is set to 0.3.</p>
<p id="p-0083" num="0082">In an example shown in <figref idref="DRAWINGS">FIG. 9</figref>, the alternative target-sentence generating unit <b>1046</b> performs natural language processing such as language analysis and syntactic analysis onto the pattern sentence <b>101</b>, and determines that words <b>102</b> and <b>1105</b> modify a word <b>1106</b> and that the words <b>102</b> and <b>1105</b> are interchangeable.</p>
<p id="p-0084" num="0083">Furthermore, the alternative target-sentence generating unit <b>1046</b> determines that a sentence <b>1121</b> can be generated from a target sentence that is generated by replacing the non-fixed segments A and B of the pattern sentence <b>101</b> with the substitution words <b>111</b> and <b>112</b>. Specifically, the alternative target-sentence generating unit <b>1046</b> determines that the sentence <b>1121</b> can be generated by changing the order of the words <b>102</b> and <b>1105</b> in the target sentence. The sentence <b>1121</b> is a Japanese sentence that means &#x201c;tonight's weather in the Tokyo area is fine&#x201d;.</p>
<p id="p-0085" num="0084">The edit distance &#x3b3;=1 and the similarity &#x3c6;=0.5 are established between the sentence <b>1121</b> and the target sentence generated from the pattern sentence <b>101</b>. Because the similarity exceeds the threshold, the alternative target-sentence generating unit <b>1046</b> generates the sentence <b>1121</b> as an alternative target sentence.</p>
<p id="p-0086" num="0085">In another example shown in <figref idref="DRAWINGS">FIG. 10</figref>, a pattern sentence <b>1201</b> is a Japanese sentence that indicates an approximate distance to a certain intersection and an instruction of turning left at the intersection. Substitution words <b>1211</b> and <b>1212</b> are to substitute for non-fixed segments C and D, respectively, of the pattern sentence <b>1201</b>. The substitution word <b>1211</b> is a numeral <b>100</b>, indicating the distance. The substitution word <b>1212</b> is Japanese words indicating the name of the intersection &#x201c;Kawasaki Station West Exit&#x201d;.</p>
<p id="p-0087" num="0086">In the example of <figref idref="DRAWINGS">FIG. 10</figref>, the alternative target-sentence generating unit <b>1046</b> refers to a synonym list (not shown) that defines synonyms, and then determines that a word <b>1202</b> of the pattern sentence <b>1201</b> can be replaced with a synonym <b>1203</b>. The word <b>1202</b> and the synonym <b>1203</b> are Japanese words both meaning &#x201c;approximately&#x201d;. The synonym list is stored in the storage unit <b>30</b> or the like in advance so that the alternative target-sentence generating unit <b>1046</b> can refer to the list.</p>
<p id="p-0088" num="0087">Furthermore, the alternative target-sentence generating unit <b>1046</b> determines that a sentence <b>1221</b> can be generated from a target sentence that is generated by replacing the non-fixed segments C and D of the pattern sentence <b>1201</b> with the substitution words <b>1211</b> and <b>1212</b>. Specifically, the alternative target-sentence generating unit <b>1046</b> determines that the sentence <b>1221</b> can be generated by replacing the word <b>1202</b> in the target sentence with the synonym <b>1203</b>. The sentence <b>1221</b> is a Japanese sentence that tells the user to turn left at the Kawasaki-Station-West-Exit intersection about 100 meters ahead.</p>
<p id="p-0089" num="0088">The edit distance &#x3b3;=1 and the similarity &#x3c6;=0.5 are established between the sentence <b>1221</b> and the target sentence generated from the pattern sentence <b>1201</b>. Because the similarity exceeds the threshold, the alternative target-sentence generating unit <b>1046</b> generates the sentence <b>1221</b> as an alternative target sentence.</p>
<p id="p-0090" num="0089">In still another example shown in <figref idref="DRAWINGS">FIG. 11</figref>, a pattern sentence <b>1301</b> is a Japanese sentence indicating that a possibility will be checked. A substitution word <b>1311</b> is a Japanese word for &#x201c;realization&#x201d;, which is to be substituted for a non-fixed segment E of the pattern sentence <b>1301</b>.</p>
<p id="p-0091" num="0090">In the example of <figref idref="DRAWINGS">FIG. 11</figref>, the alternative target-sentence generating unit <b>1046</b> determines that a phrase <b>1302</b> of the pattern sentence <b>1301</b> can be replaced with a phrase <b>1303</b>, by referring to a thesaurus or a phrasal thesaurus. The phrases <b>1302</b> and <b>1303</b> are Japanese phrases both indicating &#x201c;possibility&#x201d;. The thesaurus or the like is stored in advance in the storage unit <b>30</b> so that the alternative target-sentence generating unit <b>1046</b> can refer to the thesaurus.</p>
<p id="p-0092" num="0091">Furthermore, the alternative target-sentence generating unit <b>1046</b> determines that a sentence <b>1321</b> can be generated from a target sentence that is generated by replacing the non-fixed segment E of the pattern sentence <b>1301</b> with the substitution word <b>1311</b>. Specifically, the alternative target-sentence generating unit <b>1046</b> determines that the sentence <b>1321</b> can be generated by replacing the phrase <b>1302</b> in the target sentence with the phrase <b>1303</b>. The sentence <b>1321</b> is a Japanese sentence, meaning &#x201c;the realization possibility will be checked&#x201d;.</p>
<p id="p-0093" num="0092">The edit distance &#x3b3;=1, and the similarity &#x3c6;=0.5 are established between the sentence <b>1321</b> and the target sentence generated from the pattern sentence <b>1301</b>. Because the similarity exceeds the threshold, the alternative target-sentence generating unit <b>1046</b> generates the sentence <b>1321</b> as an alternative target sentence.</p>
<p id="p-0094" num="0093">In still another example shown in <figref idref="DRAWINGS">FIG. 12</figref>, a pattern sentence <b>1401</b> is a Japanese sentence indicating that something &#x201c;will be checked&#x201d;. A substitution word <b>1411</b> is a Japanese word for &#x201c;breakdown&#x201d;, and is to be substituted for a non-fixed segment F of the pattern sentence <b>1401</b>.</p>
<p id="p-0095" num="0094">In the example of <figref idref="DRAWINGS">FIG. 12</figref>, the alternative target-sentence generating unit <b>1046</b> determines that a phrase <b>1402</b> in the pattern sentence <b>1401</b> can be replaced with a phrase <b>1403</b>, by referring to the thesaurus or phrasal thesaurus. The phrase <b>1402</b> and the phrase <b>1403</b> are Japanese phrases both showing the object of the sentence, and both followed by a verb.</p>
<p id="p-0096" num="0095">Furthermore, the alternative target-sentence generating unit <b>1046</b> determines that a sentence <b>1421</b> can be generated from a target sentence that is generated by replacing the non-fixed segment F of the pattern sentence <b>1401</b> with the substitution word <b>1411</b>. Specifically, the alternative target-sentence generating unit <b>1046</b> determines that the sentence <b>1421</b> can be generated by replacing the phrase <b>1402</b> in the target sentence with the phrase <b>1403</b>. The sentence <b>1421</b> is a Japanese sentence meaning &#x201c;the breakdown will be checked&#x201d;.</p>
<p id="p-0097" num="0096">The edit distance &#x3b3;=1 and the similarity &#x3c6;=0.5 are established between the sentence <b>1421</b> and the target sentence generated from the pattern sentence <b>1401</b>. Because the similarity exceeds the threshold, the alternative target-sentence generating unit <b>1046</b> generates the sentence <b>1421</b> as an alternative target sentence.</p>
<p id="p-0098" num="0097">In still another example shown in <figref idref="DRAWINGS">FIG. 13</figref>, a pattern sentence <b>1501</b> is a Japanese sentence reporting the weather information of the evening in a certain region. Substitution words <b>1511</b> and <b>1512</b> are to be substituted for non-fixed segments G and H, respectively, of the pattern sentence <b>1501</b>. The substitution word <b>1511</b> is a Japanese word for a Japanese prefecture &#x201c;Chiba&#x201d;, while the substitution word <b>1512</b> is a Japanese word for &#x201c;cloudy&#x201d;.</p>
<p id="p-0099" num="0098">In the example of <figref idref="DRAWINGS">FIG. 13</figref>, the alternative target-sentence generating unit <b>1046</b> determines, by using the thesaurus or the phrasal thesaurus, that phrases <b>1502</b> and <b>1503</b> of the pattern sentence <b>1501</b> can be replaced with phrases <b>1504</b> and <b>1505</b>, respectively. The phrases <b>1502</b> and <b>1504</b> are Japanese phrases indicating &#x201c;tonight's weather&#x201d;, while the phrases <b>1503</b> and <b>1505</b> are Japanese phrases indicating the weather information of a certain region.</p>
<p id="p-0100" num="0099">Furthermore, the alternative target-sentence generating unit <b>1046</b> determines that a sentence <b>1521</b> can be generated from a target sentence that is generated by replacing the non-fixed segments G and H of the pattern sentence <b>1501</b> with the substitution words <b>1511</b> and <b>1512</b>, respectively. Specifically, the alternative target-sentence generating unit <b>1046</b> determines that the sentence <b>1521</b> can be generated by replacing the phrases in the target sentence with the phrases <b>1504</b> and <b>1505</b>. The sentence <b>1521</b> is a Japanese sentence meaning &#x201c;tonight's weather in the Chiba area is cloudy&#x201d;.</p>
<p id="p-0101" num="0100">The edit distance &#x3b3;=1 and the similarity &#x3c6;=0.5 are established between the sentence <b>1521</b> and the target sentence generated from the pattern sentence <b>1501</b>. Because the similarity exceeds the threshold, the alternative target-sentence generating unit <b>1046</b> generates the sentence <b>1521</b> as an alternative target sentence.</p>
<p id="p-0102" num="0101">In the second embodiment, the degree of similarity is calculated by use of the edit distance. Because words and phrases are hierarchically classified in a thesaurus and a phrasal thesaurus, the degree of similarity can be calculated based on this hierarchical structure. If this is the case, the alternative target-sentence generating unit <b>1046</b> calculates the degree of similarity between the target sentence and the alternative target sentence using expression (3).
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>&#x3be;=2<i>*Lc</i>/(<i>La+Lb</i>)&#x2003;&#x2003;(3)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0103" num="0102">In the expression (3), &#x201c;Lc&#x201d; represents the depth of a common upper level in the hierarchical structure, &#x201c;La&#x201d; represents a word in a target sentence, and &#x201c;Lb&#x201d; represents a word in an alternative target sentence that corresponds to the word of the target sentence. The level similarity takes on values between 0 and 1, where the value closer to 1 indicates that the relationship of the words is closer to the same linguistic information.</p>
<p id="p-0104" num="0103">In addition to the above method, other conventional methods may be adopted for the generation of an alternative target sentence, such as a method disclosed by Kentaro Inui and Atsushi Fujita, &#x201c;A Survey on Paraphrase Generation and Recognition&#x201d;, Journal of Natural Language Processing, Vol. 11, No. 5, pp. 151-198, 2004, 10.</p>
<p id="p-0105" num="0104">The processes performed by the fixed synthetic-sound generating unit <b>1050</b>, the rule-based synthetic-sound generating unit <b>1055</b>, and the calculating unit <b>1060</b> are the same as the processes performed by the fixed synthetic-sound generating unit <b>50</b>, the rule-based synthetic-sound generating unit <b>55</b>, and the calculating unit <b>60</b> according to the first embodiment, except that the processes are performed on each of the target sentence and the alternative target sentence. Thus, the detailed explanation thereof is omitted.</p>
<p id="p-0106" num="0105">Similarly, the processes performed by the selecting unit <b>1065</b> and the connecting unit <b>1070</b> are the same as the processes performed by the selecting unit <b>65</b> and the connecting unit <b>70</b> according to the first embodiment except that the processes are performed on each of the target sentence and the alternative target sentence, and therefore the detailed explanation thereof is omitted.</p>
<p id="p-0107" num="0106">The operation of the speech synthesizing device according to the second embodiment is described below.</p>
<p id="p-0108" num="0107">At Step S<b>100</b> shown in <figref idref="DRAWINGS">FIG. 14</figref>, the acquiring unit <b>1040</b> acquires a pattern sentence and substitution words input by the input unit <b>10</b>.</p>
<p id="p-0109" num="0108">At Step S<b>102</b>, the target-sentence generating unit <b>1045</b> replaces the non-fixed segments of the pattern sentence acquired by the acquiring unit <b>1040</b> with the substitution words acquired by the acquiring unit <b>1040</b> in order to generate a target sentence.</p>
<p id="p-0110" num="0109">At Step S<b>104</b>, the alternative target-sentence generating unit <b>1046</b> generates an alternative target sentence having a similarity higher than the threshold with regard to the target sentence generated by the target-sentence generating unit <b>1045</b>.</p>
<p id="p-0111" num="0110">At Step S<b>106</b>, the fixed synthetic-sound generating unit <b>1050</b> generates, by use of the speech data stored in the speech storage unit <b>32</b>, fixed synthetic sounds for the target sentence generated by the target-sentence generating unit <b>1045</b> and the alternative target sentence generated by the alternative target-sentence generating unit <b>1046</b>.</p>
<p id="p-0112" num="0111">At Step S<b>108</b>, the rule-based synthetic-sound generating unit <b>1055</b> generates rule-based synthetic sounds for the target sentence generated by the target-sentence generating unit <b>1045</b> and the alternative target sentence generated by the alternative target-sentence generating unit <b>1046</b>, by referring to the dictionary data stored in the dictionary storage unit <b>34</b>.</p>
<p id="p-0113" num="0112">At Step S<b>110</b>, the calculating unit <b>1060</b> calculates the discontinuity value of the boundary between the fixed synthetic sounds generated by the fixed synthetic-sound generating unit <b>1050</b> and the rule-based synthetic sounds generated by the rule-based synthetic-sound generating unit <b>1055</b>, for the target sentence generated by the target-sentence generating unit <b>1045</b> and the alternative target sentence generated by the alternative target-sentence generating unit <b>1046</b>.</p>
<p id="p-0114" num="0113">At Step S<b>112</b>, the selecting unit <b>1065</b> selects either the target sentence generated by the target-sentence generating unit <b>1045</b> or the alternative target sentence generated by the alternative target-sentence generating unit <b>1046</b>, whichever has the smaller discontinuity value calculated by the calculating unit <b>1060</b>.</p>
<p id="p-0115" num="0114">At Step S<b>114</b>, the connecting unit <b>1070</b> connects the fixed synthetic sounds and the rule-based synthetic sounds of the target sentence or the alternative target sentence, whichever is selected by the selecting unit <b>1065</b>.</p>
<p id="p-0116" num="0115">The process of Step S<b>116</b> is the same as that of Step S<b>24</b> in the flowchart of <figref idref="DRAWINGS">FIG. 7</figref>, and the explanation thereof is omitted.</p>
<p id="p-0117" num="0116">As described above, according to the second embodiment, a target sentence and an alternative target sentence that is semantically equivalent to the target sentence are generated from a single pattern sentence; the generated target sentence or alternative target sentence, whichever has the smaller discontinuity value for the connection boundary between the fixed synthetic sounds and the rule-based synthetic sounds, is selected; and a synthetic speech is output by connecting the fixed synthetic sounds and the rule-based synthetic sounds of the selected sentence.</p>
<p id="p-0118" num="0117">According to the second embodiment, the user does not have to prepare a plurality of pattern sentences that are semantically equal to one another in advance, and an alternative target sentence that is semantically equivalent to the target sentence can be automatically generated. Then, the target sentence or the alternative target sentence, whichever has the smaller discontinuity value, is selected to output a synthetic speech. Therefore, the synthetic speech with less unnaturalness, which is accompanied by connecting synthetic sounds, can be generated, while lightening the workload on the development.</p>
<p id="p-0119" num="0118">The above-described speech synthesizing devices <b>1</b> and <b>1001</b> according to the embodiments have a hardware structure utilizing an ordinary computer and include a controlling device such as a CPU, memory devices such as a read only memory (ROM) and a RAM, external memory devices such as an HDD, an SSD, and a removable drive device, a speech output device such as a speaker, and input devices such as a keyboard and a mouse.</p>
<p id="p-0120" num="0119">A speech synthesizing program executed by the speech synthesizing devices <b>1</b> and <b>1001</b> according to the embodiments is stored in a file of an installable or executable format, in a computer-readable memory medium such as a CD-ROM, a flexible disk (FD), a CD-R, and a digital versatile disk (DVD), and is provided as a computer program product.</p>
<p id="p-0121" num="0120">Furthermore, the speech synthesizing program executed by the speech synthesizing devices <b>1</b> and <b>1001</b> according to the embodiments may be stored in a ROM or the like to be provided.</p>
<p id="p-0122" num="0121">The speech synthesizing program executed by the speech synthesizing devices <b>1</b> and <b>1001</b> according to the embodiments has a module configuration containing the above-described units (the acquiring unit, the sentence generating unit, the fixed synthetic-sound generating unit, the rule-based synthetic-sound generating unit, the calculating unit, the selecting unit, the connecting unit, the output controlling unit, and the like). As the actual hardware configuration, the CPU (processor) reads and executes the speech synthesizing program from the memory medium so that the units are loaded onto the main storage device, where the acquiring unit, the sentence generating unit, the fixed synthetic-sound generating unit, the rule-based synthetic-sound generating unit, the calculating unit, the selecting unit, the connecting unit, the output controlling unit, and the like are implemented on the main storage device.</p>
<p id="p-0123" num="0122">The present invention is not limited to the above embodiments. In the implementation, the invention can be modified and embodied without departing the scope of the invention. Furthermore, the structural components disclosed in the embodiments can be suitably combined to offer various inventions. For example, some of the structural components may be eliminated from the structure indicated in any of the embodiments. The structural components of different embodiments may be suitably combined.</p>
<p id="p-0124" num="0123">The naturalness tends to be lost when the time change of the spectrum representing the acoustic characteristics is discontinuous at the connection boundary. For this reason, when calculating the discontinuity value, the calculating units <b>60</b> and <b>1060</b> according to the embodiments may take into account, as a spectrum distortion, the sum of the spectrum distances that represent the degrees of discontinuity for the spectrum parameters.</p>
<p id="p-0125" num="0124">In addition, the naturalness also tends to be lost when the time change of the fundamental frequencies representing intonations is discontinuous at the connection boundary. Thus, the calculating units <b>60</b> and <b>1060</b> according to the embodiments may take into account, as a fundamental frequency distortion, the sum of the fundamental frequency distances representing the discontinuity of the fundamental frequencies when calculating the discontinuity value.</p>
<p id="p-0126" num="0125">In a rule-based sound synthesizing method, less-frequently co-occurring phonemes that are generated in accordance with rules tend to sound less natural than more-frequently co-occurring phonemes. The calculating units <b>60</b> and <b>1060</b> according to the embodiments therefore may take into account the inverse of the phonological co-occurrence probability as a phonological co-occurrence distortion when calculating the discontinuity value.</p>
<p id="p-0127" num="0126">In addition, the naturalness tends to be lost when the same target sentence is repeatedly used. Thus, the calculating units <b>60</b> and <b>1060</b> according to the embodiments may assign weights to the calculated discontinuity values depending on the frequency of the target sentence selected by the selecting units <b>65</b> and <b>1065</b>, and calculate a new discontinuity value taking into account the calculated discontinuity values to which the weights are assigned. This would prevent any target sentence that is frequently used in the past from being repeatedly used. As an example of calculated and weight-assigned discontinuity value, the calculated discontinuity value of the target sentence multiplied by the frequency of selection of the target sentence may be adopted.</p>
<p id="p-0128" num="0127">With this arrangement, the synthetic speech of the same target sentence would not be repeatedly output, but different target sentences that are semantically equivalent are output. Hence, speech synthesis that is suitable for an interface of a human-like machine such as a robot can be realized.</p>
<p id="p-0129" num="0128">In the above explanation of the present embodiments, pattern sentences and substitution words that are to be acquired are input by the input unit <b>10</b>. Alternatively, the pattern sentences and substitution words may be pre-stored in the storage unit <b>30</b> so that the acquiring units <b>40</b> and <b>1040</b> can acquire the pattern sentences and the substitution words from the storage unit <b>30</b>.</p>
<p id="p-0130" num="0129">In the explanation of the second embodiment, a target sentence and an alternative target sentence are generated from a single pattern sentence. However, a plurality of target sentences and alternative target sentences may be generated from multiple pattern sentences in the second embodiment.</p>
<p id="p-0131" num="0130">In the explanation of the second embodiment, an alternative target sentence is generated by changing the word order of the pattern sentence and then replacing the non-fixed segments with the substitution words. An alternative target sentence may be generated by first generating a target sentence by replacing the non-fixed segments of the pattern sentence with the substitution words and then changing the word order of the target sentence.</p>
<p id="p-0132" num="0131">Additional advantages and modifications will readily occur to those skilled in the art. Therefore, the invention in its broader aspects is not limited to the specific details and representative embodiments shown and described herein. Accordingly, various modifications may be made without departing from the spirit or scope of the general inventive concept as defined by the appended claims and their equivalents.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A speech synthesizing device comprising:
<claim-text>an acquiring unit configured to acquire a plurality of pattern sentences, which are semantically equivalent to one another and each include a fixed segment and a non-fixed segment, and a substitution word, the fixed segment is not to be replaced with any other word, the non-fixed segment is to be replaced with another word, the substitution word is substituted for the non-fixed segment;</claim-text>
<claim-text>a sentence generating unit configured to generate a plurality of target sentences by replacing the non-fixed segment with the substitution word for each of the pattern sentences;</claim-text>
<claim-text>a first synthetic-sound generating unit configured to generate a first synthetic sound, which is a synthetic sound of the fixed segment, for each of the target sentences;</claim-text>
<claim-text>a second synthetic-sound generating unit configured to generate a second synthetic sound, which is a synthetic sound of the substitution word, for each of the target sentences;</claim-text>
<claim-text>a calculating unit configured to calculate a discontinuity value of a boundary between the first synthetic sound and the second synthetic sound, for each of the target sentences;</claim-text>
<claim-text>a selecting unit configured to select one of the target sentences having the smallest discontinuity value from the target sentences; and</claim-text>
<claim-text>a connecting unit configured to connect the first synthetic sound and the second synthetic sound of the target sentence selected.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the calculating unit calculates the discontinuity value taking into account at least one of a spectrum distortion, a fundamental frequency distortion, and a phonological co-occurrence distortion at the boundary between the first synthetic sound and the second synthetic sound.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the calculating unit calculates the discontinuity value taking into account a weight-assigned discontinuity value that is generated by assigning a weight to a calculated discontinuity value depending on a frequency with which the selecting unit selects the target sentence.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. A speech synthesizing device comprising:
<claim-text>an acquiring unit configured to acquire a pattern sentence, which includes a fixed segment that is not to be replaced with any other word and a non-fixed segment that is to be replaced with another word, and a substitution word that is substituted for the non-fixed segment;</claim-text>
<claim-text>a first sentence generating unit configured to generate a target sentence by replacing the non-fixed segment with the substitution word;</claim-text>
<claim-text>a second sentence generating unit configured to generate an alternative target sentence that has a similarity value to the target sentence that exceeds a threshold;</claim-text>
<claim-text>a first synthetic-sound generating unit configured to generate a first synthetic sound, which is a synthetic sound of the fixed segment, for the target sentence and the alternative target sentence;</claim-text>
<claim-text>a second synthetic-sound generating unit configured to generate a second synthetic sound, which is a synthetic sound of the substitution word, for the target sentence and the alternative target sentence;</claim-text>
<claim-text>a calculating unit configured to calculate a discontinuity value of a boundary between the first synthetic sound and the second synthetic sound, for the target sentence and the alternative target sentence;</claim-text>
<claim-text>a selecting unit configured to select the target sentence or the alternative target sentence, whichever has the smaller discontinuity value; and</claim-text>
<claim-text>a connecting unit configured to connect the first synthetic sound and the second synthetic sound of the target sentence or the alternative target sentence that is selected.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The device according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the second sentence generating unit generates the alternative target sentence by performing at least one of operations of changing a word order of the pattern sentence, replacing a word of the pattern sentence with a synonym, and replacing a phrase of the pattern sentence with a different phrase, in addition to replacing the non-fixed segment with the substitution word.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. A computer program product having a computer readable non-transitory medium including programmed instructions for synthesizing a speech that, when executed by a computer, causes the computer to perform:
<claim-text>acquiring a plurality of pattern sentences, which are semantically equivalent to one another and each include a fixed segment and a non-fixed segment, and a substitution word, the fixed segment is not to be replaced with any other word, the non-fixed segment is to be replaced with another word, the substitution word is substituted for the non-fixed segment; and</claim-text>
<claim-text>a substitution word that is substituted for the non-fixed segment;</claim-text>
<claim-text>generating a plurality of target sentences by replacing the non-fixed segment with the substitution word for each of the pattern sentences;</claim-text>
<claim-text>generating a first synthetic sound, which is a synthetic sound of the fixed segment, for each of the target sentences;</claim-text>
<claim-text>generating a second synthetic sound, which is a synthetic sound of the substitution word, for each of the target sentences;</claim-text>
<claim-text>calculating a discontinuity value of a boundary between the first synthetic sound and the second synthetic sound, for each of the target sentences;</claim-text>
<claim-text>selecting one of the target sentences having the smallest discontinuity value from the target sentences; and</claim-text>
<claim-text>connecting the first synthetic sound and the second synthetic sound of the target sentence selected.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. A computer program product having a computer readable non-transitory medium including programmed instructions for synthesizing a speech that, when executed by a computer, causes the computer to perform:
<claim-text>acquiring a pattern sentence, which includes a fixed segment that is not to be replaced with any other word and a non-fixed segment that is to be replaced with another word, and a substitution word that is to be substituted for the non-fixed segment;</claim-text>
<claim-text>acquiring a pattern sentence, which includes a fixed segment that is not to be replaced with any other word and a non-fixed segment that is to be replaced with another word, and a substitution word that is to be substituted for the non-fixed segment;</claim-text>
<claim-text>generating a target sentence by replacing the non-fixed segment with the substitution word;</claim-text>
<claim-text>generating an alternative target sentence having a higher similarity value to the target sentence that exceeds a threshold;</claim-text>
<claim-text>generating a first synthetic sound, which is a synthetic sound of the fixed segment, for the target sentence and the alternative target sentence;</claim-text>
<claim-text>generating a second synthetic sound, which is a synthetic sound of the substitution word, for the target sentence and the alternative target sentence;</claim-text>
<claim-text>calculating a discontinuity value of a boundary between the first synthetic sound and the second synthetic sound, for the target sentence and the alternative target sentence;</claim-text>
<claim-text>selecting the target sentence or the alternative target sentence, whichever has the smaller discontinuity value; and</claim-text>
<claim-text>connecting the first synthetic sound and the second synthetic sound of the target sentence or the alternative target sentence that is selected.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. A speech synthesizing method comprising:
<claim-text>acquiring a plurality of pattern sentences, which are semantically equivalent to one another and each include a fixed segment and a non-fixed segment, and a substitution word, the fixed segment is not to be replaced with any other word, the non-fixed segment is to be replaced with another word, the substitution word is substituted for the non-fixed segment;</claim-text>
<claim-text>generating a plurality of target sentences by replacing the non-fixed segment with the substitution word for each of the pattern sentences;</claim-text>
<claim-text>generating a first synthetic sound, which is a synthetic sound of the fixed segment, for each of the target sentences;</claim-text>
<claim-text>generating a second synthetic sound, which is a synthetic sound of the substitution word, for each of the target sentences;</claim-text>
<claim-text>calculating a discontinuity value of a boundary between the first synthetic sound and the second synthetic sound, for each of the target sentences;</claim-text>
<claim-text>selecting one of the target sentences having the smallest discontinuity value from the target sentences; and</claim-text>
<claim-text>connecting the first synthetic sound and the second synthetic sound of the target sentence selected.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. A speech synthesizing method comprising:
<claim-text>acquiring a pattern sentence, which includes a fixed segment that is not to be replaced with any other word and a non-fixed segment that is to be replaced with another word, and a substitution word that is to be substituted for the non-fixed segment;</claim-text>
<claim-text>generating a target sentence by replacing the non-fixed segment with the substitution word;</claim-text>
<claim-text>generating an alternative target sentence having a similarity value to the target sentence that exceeds a threshold;</claim-text>
<claim-text>generating a first synthetic sound, which is a synthetic sound of the fixed segment, for the target sentence and the alternative target sentence;</claim-text>
<claim-text>generating a second synthetic sound, which is a synthetic sound of the substitution word, for the target sentence and the alternative target sentence;</claim-text>
<claim-text>calculating a discontinuity value of a boundary between the first synthetic sound and the second synthetic sound, for the target sentence and the alternative target sentence;</claim-text>
<claim-text>selecting the target sentence or the alternative target sentence, whichever has the smaller discontinuity value; and</claim-text>
<claim-text>connecting the first synthetic sound and the second synthetic sound of the target sentence or the alternative target sentence that is selected.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
