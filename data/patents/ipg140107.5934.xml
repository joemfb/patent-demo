<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08627043-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08627043</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13430168</doc-number>
<date>20120326</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<us-term-of-grant>
<disclaimer>
<text>This patent is subject to a terminal disclaimer.</text>
</disclaimer>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>9</main-group>
<subgroup>30</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>712  3</main-classification>
</classification-national>
<invention-title id="d2e51">Data parallel function call for determining if called routine is data parallel</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5274818</doc-number>
<kind>A</kind>
<name>Vasilevsky et al.</name>
<date>19931200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5381536</doc-number>
<kind>A</kind>
<name>Phelps et al.</name>
<date>19950100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711168</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5381550</doc-number>
<kind>A</kind>
<name>Jourdenais et al.</name>
<date>19950100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6016397</doc-number>
<kind>A</kind>
<name>Ogasawara et al.</name>
<date>20000100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6106575</doc-number>
<kind>A</kind>
<name>Hardwick</name>
<date>20000800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>7334110</doc-number>
<kind>B1</kind>
<name>Faanes et al.</name>
<date>20080200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>712  3</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>7363472</doc-number>
<kind>B2</kind>
<name>Stuttard et al.</name>
<date>20080400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>712225</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>8019977</doc-number>
<kind>B2</kind>
<name>Gonion et al.</name>
<date>20110900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>712225</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2003/0037221</doc-number>
<kind>A1</kind>
<name>Gschwind et al.</name>
<date>20030200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2007/0233766</doc-number>
<kind>A1</kind>
<name>Gschwind</name>
<date>20071000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2008/0034357</doc-number>
<kind>A1</kind>
<name>Gschwind</name>
<date>20080200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2009/0307656</doc-number>
<kind>A1</kind>
<name>Eichenberger et al.</name>
<date>20091200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2010/0042789</doc-number>
<kind>A1</kind>
<name>Gonion et al.</name>
<date>20100200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2011/0161623</doc-number>
<kind>A1</kind>
<name>Eichenberger et al.</name>
<date>20110600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>GB</country>
<doc-number>2 409 064</doc-number>
<kind>A</kind>
<date>20050600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>WO</country>
<doc-number>WO 2006/044978</doc-number>
<kind>A2</kind>
<date>20060400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00017">
<othercit>International Search Report and Written Opinion dated May 30, 2011 for International Application No. PCT/EP2010/069502, 11 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00018">
<othercit>USPTO U.S. Appl. No. 12/649,751, 1 page.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00019">
<othercit>Andre, Francoise et al., &#x201c;The Pandore Data-Parallel Compiler and its Portable Runtime&#x201d;, International Conference on High-Performance Computering and Networking (HPCN '95), Milan:Italy (1995), 8 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00020">
<othercit>Gibbs, Ben et al., &#x201c;IBM eServer BladeCenter JS20 PowerPc 970 Programming Environment&#x201d;, Jan. 2005, www.redbooks.ibm.com/redpapers/pdfs/redp3890.pdf, 140 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00021">
<othercit>McCool, Michael D. , &#x201c;Data-Parallel Programming on the Cell BE and the GPU using the RapidMind Development Platform&#x201d;, Presented at the GSPx Multicore Applications Conference, Santa Clara, Oct. 31 to Nov. 2, 2006, pp. 1-9.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>9</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>712  3</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>6</number-of-drawing-sheets>
<number-of-figures>6</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>12649751</doc-number>
<date>20091230</date>
</document-id>
<parent-status>PENDING</parent-status>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13430168</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120180031</doc-number>
<kind>A1</kind>
<date>20120712</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Eichenberger</last-name>
<first-name>Alexandre E.</first-name>
<address>
<city>Chappaqua</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Flachs</last-name>
<first-name>Brian K.</first-name>
<address>
<city>Georgetown</city>
<state>TX</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Johns</last-name>
<first-name>Charles R.</first-name>
<address>
<city>Austin</city>
<state>TX</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="004" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Nutter</last-name>
<first-name>Mark R.</first-name>
<address>
<city>Austin</city>
<state>TX</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Eichenberger</last-name>
<first-name>Alexandre E.</first-name>
<address>
<city>Chappaqua</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Flachs</last-name>
<first-name>Brian K.</first-name>
<address>
<city>Georgetown</city>
<state>TX</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Johns</last-name>
<first-name>Charles R.</first-name>
<address>
<city>Austin</city>
<state>TX</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="004" designation="us-only">
<addressbook>
<last-name>Nutter</last-name>
<first-name>Mark R.</first-name>
<address>
<city>Austin</city>
<state>TX</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<last-name>Walder, Jr.</last-name>
<first-name>Stephen J.</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
<agent sequence="02" rep-type="attorney">
<addressbook>
<last-name>Talpis</last-name>
<first-name>Matthew B.</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>International Business Machines Corporation</orgname>
<role>02</role>
<address>
<city>Armonk</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Fennema</last-name>
<first-name>Robert</first-name>
<department>2183</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Mechanisms for performing data parallel function calls in code during runtime are provided. These mechanisms may operate to execute, in the processor, a portion of code having a data parallel function call to a target portion of code. The mechanisms may further operate to determine, at runtime by the processor, whether the target portion of code is a data parallel portion of code or a scalar portion of code and determine whether the calling code is data parallel code or scalar code. Moreover, the mechanisms may operate to execute the target portion of code based on the determination of whether the target portion of code is a data parallel portion of code or a scalar portion of code, and the determination of whether the calling code is data parallel code or scalar code.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="261.28mm" wi="194.06mm" file="US08627043-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="257.64mm" wi="203.71mm" file="US08627043-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="251.21mm" wi="186.52mm" orientation="landscape" file="US08627043-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="251.21mm" wi="191.01mm" orientation="landscape" file="US08627043-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="239.69mm" wi="170.43mm" orientation="landscape" file="US08627043-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="191.60mm" wi="101.85mm" file="US08627043-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="260.18mm" wi="194.14mm" file="US08627043-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<p id="p-0002" num="0001">This application is a continuation of application Ser. No. 12/649,751, filed Dec. 30, 2009, status pending.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND</heading>
<p id="p-0003" num="0002">The present application relates generally to an improved data processing apparatus and method and more specifically to mechanisms for providing a data parallel function call that determines if a called routine is data parallel or scalar.</p>
<p id="p-0004" num="0003">Multimedia extensions (MMEs) have become one of the most popular additions to general-purpose microprocessors. Existing multimedia extensions can be characterized as Single Instruction Multiple Datapath (SIMD) units that support packed fixed-length vectors. The traditional programming model for multimedia extensions has been explicit vector programming using either (in-line) assembly or intrinsic functions embedded in a high-level programming language. Explicit vector programming is time-consuming and error-prone. A promising alternative is to exploit vectorization technology to automatically generate SIMD codes from programs written in standard high-level languages.</p>
<p id="p-0005" num="0004">Although vectorization has been studied extensively for traditional vector processors decades ago, vectorization for SIMD architectures has raised new issues due to several fundamental differences between the two architectures. To distinguish between the two types of vectorization, the latter is referred to as SIMD vectorization, or SIMDization. One such fundamental difference comes from the memory unit. The memory unit of a typical SIMD processor bears more resemblance to that of a wide scalar processor than to that of a traditional vector processor. In the VMX instruction set found on certain PowerPC microprocessors (produced by International Business Machines Corporation of Armonk, N.Y.), for example, a load instruction loads 16-byte contiguous memory from 16-byte aligned memory, ignoring the last 4 bits of the memory address in the instruction. The same applies to store instructions.</p>
<p id="p-0006" num="0005">There has been a recent spike of interest in compiler techniques to automatically extract SIMD or data parallelism from programs. This upsurge has been driven by the increasing prevalence of SIMD architectures in multimedia processors and high-performance computing. These processors have multiple function units, e.g., floating point units, fixed point units, integer units, etc., which can execute more than one instruction in the same machine cycle to enhance the uni-processor performance. The function units in these processors are typically pipelined.</p>
<p id="p-0007" num="0006">Extracting data parallelism from an application is a difficult task for a compiler. In most cases, except for the most trivial loops in the application code, the extraction of parallelism is a task the application developer must perform. This typically requires a restructuring of the application to allow the compiler to extract the parallelism or explicitly coding the parallelism using multiple threads, a SIMD intrinsic, or vector data types available in new programming models, such as OpenCL.</p>
<p id="p-0008" num="0007">Before a compiler can determine if a portion of code can be parallelized and thereby perform data parallel compilation of the code, the compiler must prove that the portion of code is independent and no data dependencies between the portion of code and other code called by that code exist. Procedure calls are an inhibiting factor to data parallel compilation. That is, data parallel compilation is only possibly when the compiler can prove that the code will correctly execute when data parallel optimizations are performed. When the code calls a procedure, subroutine, or the like, from different portions of code, object modules, or the like, that are not visible to the compiler at the time of compilation, such data parallel compilation is not possible since the compiler cannot verify that the code will correctly execute when the data parallel optimizations are performed.</p>
<p id="p-0009" num="0008">Moreover, in a single instruction, multiple datapath (SIMD) architecture using a SIMD data parallel model, an important restriction implemented is that the architecture can only follow a single program flow at a time. This restriction makes data parallel compilation impossible in an object oriented model where the object inheritance might provide different methods for different portions of code subject to data parallelism optimizations.</p>
<heading id="h-0002" level="1">SUMMARY</heading>
<p id="p-0010" num="0009">In one illustrative embodiment, a method, in a data processing system having a processor, for performing data parallel function calls in code during runtime. The method may comprise executing, in the processor, a portion of code having a data parallel function call to a target portion of code. The portion of code having the data parallel function call is calling code. The method may further comprise determining, at runtime by the processor, whether the target portion of code is a data parallel portion of code or a scalar portion of code and determining whether the calling code is data parallel code or scalar code. Moreover, the method may comprise executing the target portion of code based on the determination of whether the target portion of code is a data parallel portion of code or a scalar portion of code, and the determination of whether the calling code is data parallel code or scalar code.</p>
<p id="p-0011" num="0010">In other illustrative embodiments, a computer program product comprising a computer useable or readable medium having a computer readable program is provided. The computer readable program, when executed on a computing device, causes the computing device to perform various ones, and combinations of, the operations outlined above with regard to the method illustrative embodiment.</p>
<p id="p-0012" num="0011">In yet another illustrative embodiment, a system/apparatus is provided. The system/apparatus may comprise one or more processors and a memory coupled to the one or more processors. The memory may comprise instructions which, when executed by the one or more processors, cause the one or more processors to perform various ones, and combinations of, the operations outlined above with regard to the method illustrative embodiment.</p>
<p id="p-0013" num="0012">These and other features and advantages of the present invention will be described in, or will become apparent to those of ordinary skill in the art in view of, the following detailed description of the example embodiments of the present invention.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE SEVERAL VIEWS OF THE DRAWINGS</heading>
<p id="p-0014" num="0013">The invention, as well as a preferred mode of use and further objectives and advantages thereof, will best be understood by reference to the following detailed description of illustrative embodiments when read in conjunction with the accompanying drawings, wherein:</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 1</figref> is an exemplary block diagram of a data processing system in which aspects of the present invention may be implemented;</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram of a processor architecture shown for purposes of discussion of the improvements made by the illustrative embodiments;</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 3</figref> is an exemplary diagram showing the alternative processor architecture in accordance with some illustrative embodiment;</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 4</figref> is an example block diagram illustrating a SIMD unit having logic for performing the various hardware logic operations in accordance with one illustrative embodiment;</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 5</figref> is a flowchart outlining an example operation for compiling and linking code to generate executable code; and</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 6</figref> is a flowchart outlining an operation for executing a data parallel call in accordance with one illustrative embodiment.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0021" num="0020">The illustrative embodiments provide a mechanism for performing a data parallel call when calling a target procedure, routine, object model, etc. The data parallel call allows a compiler to assume that calls to target procedures, routines, object models, etc. that are not visible to the compiler at the time of compilation, are in fact data parallel, even if they are not. That is, the data parallel call allows the compiler to assume data parallelism unless the compiler can prove that the target procedure, routine, object model, etc. is not data parallel and must be executed in a scalar, i.e. sequential, manner. This also applies to a class of code where the compiler may not know, at compile/link time, which specific function will be called for each of the runtime objects, but where the compiler knows the set of functions that may be called.</p>
<p id="p-0022" num="0021">With the data parallel call of the illustrative embodiments, at compilation time the compiler may replace calls in the source code with a data parallel call for calls to target procedures that are not visible by the compiler. In this way, the compiler assumes data parallelism of the target procedure of the call, but uses the data parallel call to determine, at runtime, whether the target procedure of the call is in fact data parallel or not. When the data parallel call is performed from a data parallel region of calling code (i.e. a region of code that has been determined by a compiler to be data parallel under the assumptions of the illustrative embodiments that code is data parallel unless proven to be not data parallel) to a procedure whose implementation is unknown, i.e. it is not clear whether the target called procedure is data parallel or not, the data parallel call performs analysis on the target procedure to determine if it is data parallel or not. Each vector slot of a single instruction multiple data (SIMD) vector register of a SIMD unit is investigated to generate a parallel execution group. That is, the target address of the target procedure, routine, etc., stored in the SIMD vector slot is identified and compared to the target addresses stored in the other SIMD vector slots to identify matches. All of the SIMD vector slots that access the same target address are placed in the same parallel execution group. A first target address in the SIMD vector slots that differs from a first SIMD vector slot's target address starts a next parallel execution group. Thus, a parallel execution group may be comprised of one or more instructions of one or more SIMD vector slots. For a particular parallel execution group, based on the comparison of target addresses, a mask value may be set to identify which SIMD vector slots have instructions targeting the address of the particular parallel execution group and may be executed in parallel.</p>
<p id="p-0023" num="0022">Once a parallel execution group is generated by comparing target addresses of the SIMD vector slots, the target procedure, routine, etc. specified by the target address is analyzed to determine if it is a data parallel procedure, routine, etc. (hereafter simply referred to as a routine). To perform this analysis, a sentinel value may be retrieved and analyzed to determine if the sentinel value identifies the target routine as being a data parallel routine or a scalar routine. The sentinel value may be provided by a linker when generating the executable code based on characteristic information from metadata associated with the target routine. That is, a compiler may store characteristic information in metadata associated with the code of the target routine at compilation time indicating whether the compiler was able to apply data parallelism optimizations or not. This information may be used by the linker to generate a sentinel value that is stored in a known table data structure, at a known address, or the like. For example, in one illustrative embodiment, the sentinel value is stored at the target address-1 location in memory. The data parallel call of the illustrative embodiments, in such an implementation, is established such that it will always look at the target address-1 to get the sentinel value when determining if the target routine is data parallel or not.</p>
<p id="p-0024" num="0023">If the sentinel value indicates that the target routine is a data parallel routine, the parallel execution group is executed in parallel using the mask value generated so that only those instructions associated with the SIMD vector slots identified as targeting the same target address are executed in parallel. The remaining SIMD vector slots are then executed in a subsequent iteration.</p>
<p id="p-0025" num="0024">If the sentinel value indicates that the target routine is not data parallel, i.e. it is a scalar routine, then the arguments of the call to the target routine, for a first SIMD vector slot in the parallel execution group, are converted to scalar arguments using a scalar application binary interface (ABI), which is a low level scalar interface between the application code and the operating system. The target routine is then called in a scalar manner and result values are returned. The results that are returned are then converted back into data parallel return values.</p>
<p id="p-0026" num="0025">In the case where the compiler is not certain of which specific functions in a set of functions are going to be called at runtime, the above process may also be applied to determine data parallelism at runtime based on the sentinel values. It should be noted, however, that when the compiler knows the set of functions being called, and when the compiler knows that each of the functions in this set have been parallelized, then the compiler may omit actual runtime testing of sentinel values, as it can guarantee its value at compile time.</p>
<p id="p-0027" num="0026">It should further be noted that the linker may invoke, in turn, the compiler to re-generate code as more global information may be available during the linker invocation. For example, a traditional compiler typically processes one file at a time, whereas a linker typically processes all the statically-linked object files at a same time. Thus, re-invoking the compiler on each of the object files may allow the compiler to use more information that it can deduce from seeing all of the statically-linked object files at the same time.</p>
<p id="p-0028" num="0027">The above process may be repeated for each SIMD vector slot that was not executed in the previous execution. Thus, if the sentinel value indicates a data parallel target routine, then the next SIMD vector slot that was not in the parallel execution group is the first SMID vector slot for the next iteration. The above process may be repeated such that this next SIMD vector slot's address may be compared with any remaining SIMD vector slots whose instructions have not completed to determine if an additional parallel execution group can be generated. If the sentinel value indicates that the target routine is scalar, then the next SIMD vector slot may be a SIMD vector slot that was in the parallel execution group but whose instruction was not actually completed due to the scalar execution, or may be a SIMD vector slot that was not in the original parallel execution group, depending upon the situation. The process is repeated until all of the instructions in the SIMD vector slots are completed at which time the operation repeats for a next set of values in the SIMD vector.</p>
<p id="p-0029" num="0028">If the code that is calling the target routine is scalar code, i.e. not data parallel code, again the data parallel call may be used, however because the calling code is scalar, the target routine must be called in a scalar manner. In such a case, the data parallel call again causes the illustrative embodiments to investigate the sentinel value for the target routine and determine if the sentinel value indicates that the target routine is data parallel or not. If the target routine is scalar, then the target routine is called in a normal scalar manner by the scalar calling code. However, if the target routine is data parallel, then the arguments of the call are converted to a scalar form by using a mask that identifies a preferred SIMD vector slot. The target routine is then called with the specified mask such that the data parallel target routine is executed in a data parallel manner using the preferred SIMD vector slot. Return values are then converted back into a scalar form from the vector form.</p>
<p id="p-0030" num="0029">In this way, the compiler when compiling the original source code can assume that all routines called by code being compiled are data parallel unless the compiler is able to analyze the target code and prove that the target code is not data parallel. This assumption may then be checked at runtime by investigating a sentinel value provided by the linker that identifies the target routine as being data parallel or not. Conversion of arguments based on whether or not a target routine is data parallel or not may then be made at runtime as a result of the analysis of the sentinel value.</p>
<p id="p-0031" num="0030">The mechanisms of the illustrative embodiments are preferably implemented in conjunction with a compiler that transforms source code into code for execution on one or more processors capable of performing vectorized instructions, e.g., single instruction, multiple data (SIMD) instructions. One example of a data processing system in which SIMD capable processors are provided is the Cell Broadband Engine (CBE) available from International Business Machines Corporation of Armonk, N.Y. While the following description will assume a CBE architecture is used to implement the mechanisms of the illustrative embodiments, it should be appreciated that the present invention is not limited to use with the CBE architecture. To the contrary, the mechanisms of the illustrative embodiments may be used with any architecture in which array reference safety analysis may be used with transformations performed by a compiler. The CBE architecture is provided hereafter as only one example of one type of data processing system in which the mechanisms of the illustrative embodiments may be utilized and is not intended to state or imply any limitation with regard to the mechanisms of the illustrative embodiments.</p>
<p id="p-0032" num="0031">As will be appreciated by one skilled in the art, the present invention may be embodied as a system, method, or computer program product. Accordingly, aspects of the present invention may take the form of an entirely hardware embodiment, an entirely software embodiment (including firmware, resident software, micro-code, etc.) or an embodiment combining software and hardware aspects that may all generally be referred to herein as a &#x201c;circuit,&#x201d; &#x201c;module&#x201d; or &#x201c;system.&#x201d; Furthermore, aspects of the present invention may take the form of a computer program product embodied in any one or more computer readable medium(s) having computer usable program code embodied thereon.</p>
<p id="p-0033" num="0032">Any combination of one or more computer readable medium(s) may be utilized. The computer readable medium may be a computer readable signal medium or a computer readable storage medium. A computer readable storage medium may be, for example, but not limited to, an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system, apparatus, device, or any suitable combination of the foregoing. More specific examples (a non-exhaustive list) of the computer readable medium would include the following: an electrical connection having one or more wires, a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory), an optical fiber, a portable compact disc read-only memory (CDROM), an optical storage device, a magnetic storage device, or any suitable combination of the foregoing. In the context of this document, a computer readable storage medium may be any tangible medium that can contain or store a program for use by or in connection with an instruction execution system, apparatus, or device.</p>
<p id="p-0034" num="0033">A computer readable signal medium may include a propagated data signal with computer readable program code embodied therein, for example, in a baseband or as part of a carrier wave. Such a propagated signal may take any of a variety of forms, including, but not limited to, electro-magnetic, optical, or any suitable combination thereof. A computer readable signal medium may be any computer readable medium that is not a computer readable storage medium and that can communicate, propagate, or transport a program for use by or in connection with an instruction execution system, apparatus, or device.</p>
<p id="p-0035" num="0034">Computer code embodied on a computer readable medium may be transmitted using any appropriate medium, including but not limited to wireless, wireline, optical fiber cable, radio frequency (RF), etc., or any suitable combination thereof.</p>
<p id="p-0036" num="0035">Computer program code for carrying out operations for aspects of the present invention may be written in any combination of one or more programming languages, including an object oriented programming language such as Java&#x2122;, Smalltalk&#x2122;, C++, or the like, and conventional procedural programming languages, such as the &#x201c;C&#x201d; programming language or similar programming languages. The program code may execute entirely on the user's computer, partly on the user's computer, as a stand-alone software package, partly on the user's computer and partly on a remote computer, or entirely on the remote computer or server. In the latter scenario, the remote computer may be connected to the user's computer through any type of network, including a local area network (LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider).</p>
<p id="p-0037" num="0036">Aspects of the present invention are described below with reference to flowchart illustrations and/or block diagrams of methods, apparatus (systems) and computer program products according to the illustrative embodiments of the invention. It will be understood that each block of the flowchart illustrations and/or block diagrams, and combinations of blocks in the flowchart illustrations and/or block diagrams, can be implemented by computer program instructions. These computer program instructions may be provided to a processor of a general purpose computer, special purpose computer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute via the processor of the computer or other programmable data processing apparatus, create means for implementing the functions/acts specified in the flowchart and/or block diagram block or blocks.</p>
<p id="p-0038" num="0037">These computer program instructions may also be stored in a computer readable medium that can direct a computer, other programmable data processing apparatus, or other devices to function in a particular manner, such that the instructions stored in the computer readable medium produce an article of manufacture including instructions that implement the function/act specified in the flowchart and/or block diagram block or blocks.</p>
<p id="p-0039" num="0038">The computer program instructions may also be loaded onto a computer, other programmable data processing apparatus, or other devices to cause a series of operational steps to be performed on the computer, other programmable apparatus, or other devices to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide processes for implementing the functions/acts specified in the flowchart and/or block diagram block or blocks.</p>
<p id="p-0040" num="0039">The flowchart and block diagrams in the figures illustrate the architecture, functionality, and operation of possible implementations of systems, methods and computer program products according to various embodiments of the present invention. In this regard, each block in the flowchart or block diagrams may represent a module, segment, or portion of code, which comprises one or more executable instructions for implementing the specified logical function(s). It should also be noted that, in some alternative implementations, the functions noted in the block may occur out of the order noted in the figures. For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or the blocks may sometimes be executed in the reverse order, depending upon the functionality involved. It will also be noted that each block of the block diagrams and/or flowchart illustration, and combinations of blocks in the block diagrams and/or flowchart illustration, can be implemented by special purpose hardware-based systems that perform the specified functions or acts, or combinations of special purpose hardware and computer instructions.</p>
<p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. 1</figref> is an exemplary block diagram of a data processing system in which aspects of the present invention may be implemented. The exemplary data processing system shown in <figref idref="DRAWINGS">FIG. 1</figref> is an example of the Cell Broadband Engine (CBE) data processing system. While the CBE will be used in the description of the preferred embodiments of the present invention, the present invention is not limited to such, as will be readily apparent to those of ordinary skill in the art upon reading the following description.</p>
<p id="p-0042" num="0041">As shown in <figref idref="DRAWINGS">FIG. 1</figref>, the CBE <b>100</b> includes a power processor element (PPE) <b>110</b> having a processor (PPU) <b>116</b> and its L1 and L2 caches <b>112</b> and <b>114</b>, and multiple synergistic processor elements (SPEs) <b>120</b>-<b>134</b> that each has its own synergistic processor unit (SPU) <b>140</b>-<b>154</b>, memory flow control <b>155</b>-<b>162</b>, local memory or store (LS) <b>163</b>-<b>170</b>, and bus interface unit (BIU unit) <b>180</b>-<b>194</b> which may be, for example, a combination direct memory access (DMA), memory management unit (MMU), and bus interface unit. A high bandwidth internal element interconnect bus (EIB) <b>196</b>, a bus interface controller (BIC) <b>197</b>, and a memory interface controller (MIC) <b>198</b> are also provided.</p>
<p id="p-0043" num="0042">The local memory or local store (LS) <b>163</b>-<b>170</b> is a non-coherent addressable portion of a large memory map which, physically, may be provided as small memories coupled to the SPUs <b>140</b>-<b>154</b>. The local stores <b>163</b>-<b>170</b> may be mapped to different address spaces. These address regions are continuous in a non-aliased configuration. A local store <b>163</b>-<b>170</b> is associated with its corresponding SPU <b>140</b>-<b>154</b> and SPE <b>120</b>-<b>134</b> by its address location, such as via the SPU Identification Register, described in greater detail hereafter. Any resource in the system has the ability to read/write from/to the local store <b>163</b>-<b>170</b> as long as the local store is not placed in a secure mode of operation, in which case only its associated SPU may access the local store <b>163</b>-<b>170</b> or a designated secured portion of the local store <b>163</b>-<b>170</b>.</p>
<p id="p-0044" num="0043">The CBE <b>100</b> may be a system-on-a-chip such that each of the elements depicted in <figref idref="DRAWINGS">FIG. 1</figref> may be provided on a single microprocessor chip. Moreover, the CBE <b>100</b> is a heterogeneous processing environment in which each of the SPUs may receive different instructions from each of the other SPUs in the system. Moreover, the instruction set for the SPUs is different from that of the PPU, e.g., the PPU may execute Reduced Instruction Set Computer (RISC) based instructions while the SPU executes vector instructions. In another aspect of the CBE architecture, the PPU supports the Power Instruction Set Architecture (ISA) data-parallel SIMD extensions,</p>
<p id="p-0045" num="0044">The SPEs <b>120</b>-<b>134</b> are coupled to each other and to the L2 cache <b>114</b> via the EIB <b>196</b>. In addition, the SPEs <b>120</b>-<b>134</b> are coupled to MIC <b>198</b> and BIC <b>197</b> via the EIB <b>196</b>. The MIC <b>198</b> provides a communication interface to shared memory <b>199</b>. The BIC <b>197</b> provides a communication interface between the CBE <b>100</b> and other external buses and devices.</p>
<p id="p-0046" num="0045">The PPE <b>110</b> is a dual threaded PPE <b>110</b>. The combination of this dual threaded PPE <b>110</b> and the eight SPEs <b>120</b>-<b>134</b> makes the CBE <b>100</b> capable of handling 10 simultaneous threads and over 128 outstanding memory requests. The PPE <b>110</b> acts as a controller for the other eight SPEs <b>120</b>-<b>134</b> which handle most of the computational workload. The PPE <b>110</b> may be used to run conventional operating systems while the SPEs <b>120</b>-<b>134</b> perform vectorized floating point code execution, for example.</p>
<p id="p-0047" num="0046">The SPEs <b>120</b>-<b>134</b> comprise a synergistic processing unit (SPU) <b>140</b>-<b>154</b>, memory flow control units <b>155</b>-<b>162</b>, local memory or store <b>163</b>-<b>170</b>, and an interface unit <b>180</b>-<b>194</b>. The local memory or store <b>163</b>-<b>170</b>, in one exemplary embodiment, comprises a 256 KB instruction and data memory which is visible to the PPE <b>110</b> and can be addressed directly by software.</p>
<p id="p-0048" num="0047">The PPE <b>110</b> may load the SPEs <b>120</b>-<b>134</b> with small programs or threads, chaining the SPEs together to handle each step in a complex operation. For example, a set-top box incorporating the CBE <b>100</b> may load programs for reading a DVD, video and audio decoding, and display, and the data would be passed off from SPE to SPE until it finally ended up on the output display. At 4 GHz, each SPE <b>120</b>-<b>134</b> gives a theoretical 32 GFLOPS of performance with the PPE <b>110</b> having a similar level of performance.</p>
<p id="p-0049" num="0048">The memory flow control units (MFCs) <b>155</b>-<b>162</b> serve as an interface for an SPU to the rest of the system and other elements. The MFCs <b>155</b>-<b>162</b> provide the primary mechanism for data transfer, protection, and synchronization between main storage and the local storages <b>163</b>-<b>170</b>. There is logically an MFC for each SPU in a processor. Some implementations can share resources of a single MFC between multiple SPUs. In such a case, all the facilities and commands defined for the MFC must appear independent to software for each SPU. The effects of sharing an MFC are limited to implementation-dependent facilities and commands.</p>
<p id="p-0050" num="0049">With the data processing system <b>100</b> of <figref idref="DRAWINGS">FIG. 1</figref>, the processor <b>106</b> may have facilities for processing both integer (scalar) and floating point (vector) instructions and operating on both types of data. The scalar facilities may be used for integer processing and inter alia loop control and memory access control, for example. The vector facilities may be used for data parallel operations to take advantage of efficiencies in operating on data in a parallel manner.</p>
<p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram of a processor architecture shown for purposes of discussion of the improvements made by the illustrative embodiments. The particular processor architecture shown in <figref idref="DRAWINGS">FIG. 2</figref> is for the PowerPC&#x2122; <b>970</b> microprocessors available from International Business Machines Corporation of Armonk, N.Y. and described in the Redbook by Gibbs et al. entitled &#x201c;IBM eServer BladeCenter JS20 PowerPC 970 Programming Environment,&#x201d; January 2005 (available at www.redbooks.ibm.com/redpapers/pdfs/redp3890.pdf).</p>
<p id="p-0052" num="0051">As shown in <figref idref="DRAWINGS">FIG. 2</figref>, the processor architecture includes an instruction cache <b>202</b>, an instruction fetch unit <b>204</b>, an instruction decode unit <b>206</b>, and a dispatch buffer <b>208</b>. Instructions are fetched by the instruction fetch unit <b>204</b> from the instruction cache <b>202</b> and provided to the instruction decode unit <b>206</b>. The instruction decode unit <b>206</b> decodes the instruction and provides the decoded instruction to the dispatch buffer <b>208</b>. The output of the decode unit <b>206</b> is provided to both the register maps <b>210</b> and the global completion table <b>212</b>. The register maps <b>210</b> map to one or more of the general purpose registers (GPRs), floating point registers (FPRs), vector register files (VRF), and the like. The instructions are then provided to an appropriate one of the issues queues <b>220</b>-<b>232</b> depending upon the instruction type as determined through the decoding and mapping of the instruction decode unit <b>206</b> and register maps <b>210</b>. The issue queues <b>220</b>-<b>232</b> provide inputs to various ones of execution units <b>240</b>-<b>258</b>. The outputs of the execution units <b>240</b>-<b>258</b> go to various ones of the register files <b>260</b>-<b>272</b>. Data for use with the instructions may be obtained via the data cache <b>280</b>.</p>
<p id="p-0053" num="0052">Of particular note, it can be seen in the depicted architecture that there are separate issue queues and execution units for floating point, vector, and fixed point, or integer, instructions in the processor. As shown, there is a single floating point unit (FPU) issue queue <b>224</b> that has two output ports to two floating point execution units <b>244</b>-<b>246</b> which in turn have output ports to a floating point register file <b>264</b>. A single vector permute issue queue <b>226</b> has a single output port to a vector permute execution unit <b>248</b> which in turn has a port for accessing a vector register file (VRF) <b>266</b>. The vector arithmetic logic unit (ALU) issue queue <b>228</b> has one issue port for issuing instructions to the vector ALU <b>250</b> which has a port for accessing the vector register file <b>268</b>. It should be appreciated that these issue queues, execution units, and register files all take up resources, area, and power.</p>
<p id="p-0054" num="0053">With some illustrative embodiments, these issue units <b>224</b>-<b>228</b>, the execution units <b>244</b>-<b>250</b>, and register files <b>264</b>-<b>268</b> are replaced with a single issue queue, execution unit, and register file. <figref idref="DRAWINGS">FIG. 3</figref> is an exemplary diagram showing the alternative processor architecture in accordance with some illustrative embodiment. The processor architecture shown in <figref idref="DRAWINGS">FIG. 3</figref> is of a modified form of the PowerPC&#x2122; <b>970</b> architecture shown in <figref idref="DRAWINGS">FIG. 2</figref> and thus, similar elements to that of <figref idref="DRAWINGS">FIG. 2</figref> are shown with similar reference numbers. It should be appreciated that the example modified architecture is only an example and similar modifications can be made to other processor architectures to reduce the number of issue units, execution units, and register files implemented in these other architectures. Thus, the mechanisms of the illustrative embodiments are not limited to implementation in a modified form of the PowerPC&#x2122; <b>970</b> architecture.</p>
<p id="p-0055" num="0054">As shown in <figref idref="DRAWINGS">FIG. 3</figref>, the modified architecture shown in <figref idref="DRAWINGS">FIG. 3</figref> replaces the issue units <b>224</b>-<b>228</b> with a single quad-processing execution unit (QPU) issue unit <b>310</b>. Moreover, the execution units <b>244</b>-<b>250</b> are replaced with the single quad-processing execution unit (QPU) <b>320</b>. Furthermore, the register files <b>264</b>-<b>268</b> are replaced with a single quad-vector register file (QRF) <b>330</b>. Because the quad-processing unit (QPU) can execute up to 4 data elements concurrently with a single instruction, this modified architecture not only reduces the resource usage, area usage, and power usage, while simplifying the design of the processor, but the modified architecture also increases performance of the processor.</p>
<p id="p-0056" num="0055">In one illustrative embodiment, the mechanisms of the illustrative embodiment for providing a data parallel function call are provided primarily as logic elements in the QPU <b>320</b>. Additional logic may be provided in one or more of the memory units LS<b>1</b> and LS<b>2</b> as appropriate. In other illustrative embodiments, the mechanisms of the illustrative embodiments may be implemented as logic in other elements of the modified architecture shown in <figref idref="DRAWINGS">FIG. 3</figref>, such as distributed amongst a plurality of the elements shown in <figref idref="DRAWINGS">FIG. 3</figref>, or in one or more dedicated logic elements coupled to one or more elements shown in <figref idref="DRAWINGS">FIG. 3</figref>. In order to provide one example of the implementation of the illustrative embodiments, it will be assumed for purposes of this description that the mechanisms of the illustrative embodiments are implemented as logic in the QPU <b>320</b> unless otherwise indicated.</p>
<p id="p-0057" num="0056">Referring again to <figref idref="DRAWINGS">FIG. 1</figref>, the SPEs <b>120</b>-<b>134</b> and/or PPE <b>110</b> of the CBE <b>100</b> may make use of a SIMD architecture as shown in <figref idref="DRAWINGS">FIG. 3</figref>, for example, and may use vector instructions, e.g., SIMD instructions, and vector based data storage. Alternatively, other SIMD architectures may be used in which the processors utilize vector instructions having other types of vector elements. Thus, source code may be optimized by a compiler for execution on these SPEs <b>120</b>-<b>134</b> or PPE <b>110</b> with Power ISA or SIMD ISA extensions, by assuming data parallelism in the source code and verifying that assumption at runtime utilizing runtime data parallel mechanisms in accordance with the illustrative embodiments of the present invention. In some illustrative embodiments, it is preferably that the SIMD architecture of the data processing system that implements the illustrative embodiments support a preferred scalar slot in the SIMD vector register. That is, scalar operations are performed in the architecture using SIMD vectors where the preferred scalar slot contains valid instructions/data and all other slots (or elements) of the SIMD vector register are populated with &#x201c;don't care&#x201d; values, such as through a load-and-splat operation or other replication operation, a padding operation, or the like. One example of a processor architecture that supports such a preferred slot in a SIMD vector register is described in commonly assigned and co-pending U.S. patent application Ser. No. 12/134,495, entitled &#x201c;Optimized Scalar Promotion with Load and Splat SIMD Instructions,&#x201d; the description of which is incorporated herein by reference.</p>
<p id="p-0058" num="0057">With the mechanisms of the illustrative embodiments, when a compiler compiles a portion of code, the compiler assumes that any calls made by the compiler to other portions of code, such as in another object model, which are not visible to the compiler, e.g., the compiler is not made aware of the name of the target object, routine, procedure, etc., or the target portion of code is not accessible by the compiler at compile time, are in fact able to be executed in a data parallel manner. By &#x201c;data parallel&#x201d; what is meant is that there are not dependencies between iterations of execution of the portion of code, e.g., the object model, routine, procedure, etc., and there are no dependencies within the portion of code, e.g., in loops or such, that would prevent multiple iterations of the portion of code to be executed in parallel at substantially the same time. Similarly, such assumptions of data parallelism may be made in situations where the application code may call a set of known functions, but it is not known to the compiler at compile time exactly which functions will be called by the application code at runtime.</p>
<p id="p-0059" num="0058">Typically, as noted above, a compiler cannot assume that code is data parallel code and instead must take a more conservative approach and consider all code to be scalar, i.e. not data parallel but rather must be executed sequentially, unless the code can be proven to be data parallel. The illustrative embodiments take an opposite approach and assume code is data parallel unless proven during runtime to be scalar, at which point mechanisms are provided for handling the scalar execution of the portion of code.</p>
<p id="p-0060" num="0059">The compiler, for calls to other portions of code that are not visible to the compiler, or calls to portions of code that the compiler does not know at compile time will be called by the code at runtime, replaces the calls with a data parallel call in accordance with the illustrative embodiments. The data parallel call verifies the assumption of data parallelism of the target portion of code (hereafter referred to as the target routine), and converts arguments as necessary based on whether the target routine is determined at runtime to be actually data parallel or not, and also based on the nature of the portion of code calling the target routine, e.g., whether the calling code is data parallel or scalar code. A linker, which acts on the optimized and compiled code, links the code to other portions of code called by the code. As part of the linking process, the linker looks at the metadata associated with the portions of code called by the compiled code to determine if these other portions of code are data parallel or scalar. For example, the compiler may store in the metadata of the code an identifier of whether a data parallel optimization was applied to the compiled code or not and this identifier may be used by the linker to determine whether the code is data parallel or scalar code. Based on this determination, the linker may store a sentinel value at an address of the portion of code minus 1, in a table data structure, or at another known storage location, that may be used to determine if the called portion of code is data parallel or scalar. In general, any type of mapping may be used for storing this sentinel value, such as a hash set or any other data structure that allows two types of information to be linked together.</p>
<p id="p-0061" num="0060">The resulting compiled and linked code, i.e. the executable code, may then be executed on a processor utilizing a vector architecture, such as a single instruction multiple data (SIMD) architecture, or the like. Preferably, the SIMD architecture utilizes a preferred scalar slot for scalar operations. Moreover, the architecture may include a SIMD unit, which may be a function unit, similar to a floating point unit, fixed point unit, or the like, in the processor architecture, which comprises hardware logic for verifying data parallelism of code, as described in greater detail hereafter. This hardware logic may operate at runtime based on the data parallel function calls in the executable code to verify the data parallel nature of the called target routine and to execute the called target routine accordingly.</p>
<p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. 4</figref> is an example diagram of a SIMD unit in which hardware logic may be provided for performing, or at least assisting with the performance, of a data parallel call in accordance with one illustrative embodiment. For example, the SIMD unit may be, or may be part of, the QPU <b>320</b> in <figref idref="DRAWINGS">FIG. 3</figref>. As a further example, the SIMD unit may be, or may be part of, the VPERM <b>248</b> and/or the VALU <b>250</b> in <figref idref="DRAWINGS">FIG. 2</figref>. The example SIMD unit shown in <figref idref="DRAWINGS">FIG. 4</figref> is a 4-wide SIMD unit in which there are 4 SIMD vector slots per vector. It should be appreciated that the illustrative embodiments are not limited to use with a SIMD unit or with a 4-wide SIMD unit. To the contrary, the mechanisms of the illustrative embodiments may be implemented in other architectures, such as multithreaded architectures, or the like, that may or may not use SIMD units. Furthermore, other widths of SIMD units may be utilized without departing from the spirit and scope of the illustrative embodiments. For example, a 2-wide, 8-wide, 16-wide, or the like, SIMD unit may be utilized.</p>
<p id="p-0063" num="0062">Furthermore, as mentioned above, the mechanisms of the illustrative embodiments may be implemented entirely in software or in a combination of hardware and software without departing from the spirit and scope of the illustrative embodiments. For example, software may implement the data parallel checking logic <b>450</b> of <figref idref="DRAWINGS">FIG. 4</figref> while the other mechanisms in <figref idref="DRAWINGS">FIG. 4</figref> may be implemented in hardware which supports the operation of the data parallel checking logic <b>450</b>. For purposes of the following description, however, it will be assumed that the elements shown in <figref idref="DRAWINGS">FIG. 4</figref> are implemented as hardware logic within a processor of a data processing system.</p>
<p id="p-0064" num="0063">As shown in <figref idref="DRAWINGS">FIG. 4</figref>, the SIMD unit <b>400</b> includes SIMD vector slot registers <b>410</b>-<b>416</b>. The data in each SIMD vector slot register <b>410</b>-<b>416</b> may correspond to a separate iteration of a loop within a given parallel execution group/section. This data may comprise an address of an instruction/data that is the target of the operation being performed by the parallel execution associated with that SIMD vector slot register <b>410</b>-<b>416</b>. Thus, for example, SIMD slot <b>0</b> <b>410</b> may store address information for a first instruction that is to be executed in parallel, SIMD slot <b>1</b> <b>412</b> may store address information for a second instruction that is to be executed in parallel, and the like. The address information in each SIMD slot <b>0</b>-<b>3</b> <b>410</b>-<b>416</b> may be the same or different. For example, if multiple executions of a same instruction are being performed in parallel, they may all reference the same address. This may occur, for example, if multiple iterations of a loop are being performed in parallel.</p>
<p id="p-0065" num="0064">The SIMD vector slot registers <b>410</b>-<b>416</b> are coupled to masking logic <b>420</b>. The masking logic allows software or hardware to prevent a corresponding SIMD vector slot register <b>410</b>-<b>416</b> from contributing to the parallel execution. Initially, the masking logic <b>420</b> allows all of the SIMD vector slots <b>410</b>-<b>416</b> to contribute to the parallel execution results. However, in the event that determination is made that a particular SIMD vector slot <b>410</b>-<b>416</b> should not be allowed to complete its execution, e.g., a particular SIMD vector slot <b>410</b>-<b>416</b> is not part of a parallel execution group or only a preferred SIMD vector slot is to be utilized, the SIMD vector slot <b>410</b>-<b>416</b> corresponding to a parallel execution that is not to be completed is blocked by the setting of a mask value <b>430</b> in the masking logic <b>420</b> that identifies which SIMD vector slots <b>410</b>-<b>416</b> may contribute to a result of the parallel execution. For example, initially, the mask value may be set to &#x201c;1 1 1 1&#x201d; with bits in the mask value <b>430</b> being set to &#x201c;0&#x201d; when a determination is made that a corresponding SIMD vector slot <b>410</b>-<b>416</b> is not part of the parallel execution group or only a preferred SIMD vector slot, e.g., slot <b>410</b>, for scalar operations is to be used. It should be noted that the values 0 and 1 in the present description are only exemplary of values that can be used to indicate the logical values of, respectively, false and true. In other illustrative embodiments, other representations may be utilized, such as the values 0 and &#x2212;1, two disjoint set of integer values, two disjoint set of floating point values, or the like.</p>
<p id="p-0066" num="0065">The data parallel checking logic <b>450</b> operates on the address information of the target routines identified by the addresses in the SIMD vector slots <b>410</b>-<b>416</b>, and determines how to set the bits in the mask value <b>430</b> of the masking logic <b>420</b> so that appropriate operations associated with select SIMD vector slots <b>410</b>-<b>416</b> may be allowed to complete execution while others are blocked. For the calling code, instructions are included in the code, such as by the compiler, to move execution from a scalar mode (in which the execution begins) to a data parallel processing mode and generate an initial mask for the data parallel processing mode. When the data parallel region of code is done executing, instructions are included in the code to move from the data parallel processing mode back into the scalar mode.</p>
<p id="p-0067" num="0066">In some embodiments, all code may be assumed to be parallelized (whether it is actually or not), in which case each call may be tested in this manner, whether it is part of a data parallel portion/region of code or not. In other illustrative embodiments, the compiler may statically generate up to three versions of each portion of code, e.g., functions, one that is only sequential code, one that is data parallel code, and one that is data parallel code with mask values associated with it. The compiler may call directly the sequential version when the compiler knows that it calls a function in a static context. The compiler may directly call the data parallel version when the compiler knows that this data parallel function is applicable in a given context. The compiler may call the data parallel version with masks, i.e. the mode described above, as a default if the other two situations are not discernable to the compiler.</p>
<p id="p-0068" num="0067">The data parallel checking logic <b>450</b> operates in response to a data parallel call being performed from one of a scalar region or a data parallel region of calling code. The data parallel call is either an instruction, or includes an instruction, that informs the data processing system that a data parallel call is in progress. A data parallel region is region of code that has been determined by a compiler to be data parallel, i.e. there are no data dependencies that prevent parallel execution of more than one iteration or thread at substantially a same time, under the assumptions of the illustrative embodiments that code is data parallel unless proven to be not data parallel. The data parallel call is a call of a portion of code, e.g., a routine, procedure, object, or the like, whose implementation is unknown, i.e. it is not clear whether the target called routine is data parallel or not.</p>
<p id="p-0069" num="0068">In response to a data parallel call, the data parallel checking logic <b>450</b> compares the target address of target portion of code being called by the data parallel call, to the target addresses stored in the other SIMD vector slots <b>410</b>-<b>416</b>. The addresses stored in the SIMD vector slot <b>412</b>-<b>416</b>, for example, are compared to the first SIMD vector slot <b>410</b> in sequence and a determination is made as to whether they have a matching address. Each SIMD vector slot <b>412</b>-<b>416</b> having a matching address is added to a same parallel execution group until a first non-matching address in a SIMD vector slot <b>412</b>-<b>416</b> is encountered. Only those having matching target addresses stored in the SIMD vector slots <b>410</b>-<b>416</b> are included in the same parallel execution group while other SIMD vector slots <b>410</b>-<b>416</b> not having a matching address are excluded from the parallel execution group.</p>
<p id="p-0070" num="0069">A pointer <b>440</b> is set based on the identification of matching addresses in SIMD vector slots <b>410</b>-<b>416</b>. The pointer <b>440</b> points to the first SIMD vector slot <b>412</b>-<b>416</b> that does not have a matching target address as the first SIMD vector slot <b>410</b>. This pointer <b>440</b> thus, points to the first SIMD vector slot <b>412</b>-<b>416</b> for a next parallel execution group for a subsequent pass, as discussed hereafter.</p>
<p id="p-0071" num="0070">Once a parallel execution group is generated in this manner, in accordance with one illustrative embodiment, a determination is made by the logic <b>450</b> as to whether the target code corresponding to the target address of the parallel execution group is data parallel code or scalar code. This determination involves looking at a sentinel value associated with the portion of code which identifies whether the code was optimized using data parallel optimizations by the compiler. The sentinel value is stored at a known location, such as at the address-1 location for the address of the calling code (or target portion of code as discussed hereafter), in a table data structure <b>460</b>, or the like.</p>
<p id="p-0072" num="0071">The calling code is either data parallel or scalar code as may be determined by the compiler at compilation time. The compiler may insert instructions into the code defining sections or regions of the code as data parallel code or scalar code. Unless data parallel code instructions are inserted into the code, either by the compiler or by the code writer, the code is considered to be scalar. Based on the setting of the sentinel value for the target code and the nature of the calling code, the data parallel checking logic <b>450</b> determines how to perform the data parallel call. For example, if the sentinel value corresponding to the target address indicates that the target portion of code (routine) is data parallel code, and the calling code is determined to be data parallel code, the data parallel call for the corresponding SIMD vector slot <b>410</b>-<b>416</b> is executed in parallel for each of the SIMD vector slots <b>410</b>-<b>416</b> that are part of the parallel execution group while other SIMD vector slots <b>410</b>-<b>416</b> are blocked from contributing to the execution results during this pass. This blocking may be performed by the logic <b>450</b> setting a mask bit in the mask value <b>430</b> of the masking logic <b>420</b> to block certain SIMD vector slots <b>410</b>-<b>416</b> that are not part of the current parallel execution group. The calls/instructions associated with the remaining SIMD vector slots <b>410</b>-<b>416</b> are then executed in a subsequent iteration.</p>
<p id="p-0073" num="0072">If the sentinel value indicates that the target routine is not data parallel, i.e. it is a scalar routine, then the arguments of the call to the target routine, for a first SIMD vector slot, e.g., SIMD vector slot <b>410</b>, in the parallel execution group, are converted to scalar arguments using a scalar application binary interface (ABI) <b>470</b>, which is a low level scalar interface between the application code and the operating system. The logic <b>450</b> then causes the target portion of code (routine) to be called in a scalar manner and result values are returned. The results that are returned are then converted back into data parallel return values by the logic <b>450</b>. For example, the scalar ABI <b>470</b> may provide a vector to a handler, such as the calling instruction, that converts the vector arguments to scalar arguments, such as by using a preferred vector slot for scalar arguments.</p>
<p id="p-0074" num="0073">As an example, assume that there is a vector of 4 addresses, vector slots <b>0</b>, <b>1</b>, and <b>3</b> have been determined to have the same address to a scalar routine or function, and there is a loop iterating over all of the vector slots. For each slot S, in turn, a determination is made as to whether the corresponding mask is on/off. When the mask is on, each of the parameters required by the function are moved to where they are expected by the scalar function (i.e. in the scalar register or preferred scalar vector slot). The function can then be called and the return value (if any) is place dint eh slot S of the SIMD vector register holding the return value.</p>
<p id="p-0075" num="0074">The above process is then repeated for each SIMD vector slot that was not executed in the previous execution. Thus, if the sentinel value indicates a data parallel target routine, then the next SIMD vector slot that was not in the parallel execution group, as pointed to by the pointer <b>440</b>, is the first SMID vector slot for the next pass or iteration. The above process may be repeated such that this next SIMD vector slot's address may be compared with any remaining SIMD vector slots whose instructions have not completed, to determine if an additional parallel execution group can be generated. If the sentinel value indicates that the target routine is scalar, then the next SIMD vector slot may be a SIMD vector slot that was in the parallel execution group but whose instruction was not actually completed due to the scalar execution, or may be a SIMD vector slot that was not in the original parallel execution group, depending upon the situation. The process is repeated until all of the instructions in the SIMD vector slots <b>410</b>-<b>416</b> are completed, at which time the operation repeats for a next set of values in the SIMD vector register.</p>
<p id="p-0076" num="0075">If the calling code that is calling the target routine is scalar code, i.e. not data parallel code, the data parallel call may again be used. However, because the calling code is scalar, the target portion of code is called in a scalar manner. In such a case, the data parallel call again causes the logic <b>450</b> to investigate the sentinel value for the target portion of code (routine) and determine if the sentinel value indicates that the target routine is data parallel or not. If the target routine is scalar, then the target routine is called in a normal scalar manner by the scalar calling code. However, if the target routine is data parallel, then the arguments of the call are converted to a scalar form by using the mask value <b>430</b> to mask all but a preferred SIMD vector slot, e.g., SIMD vector slot <b>410</b> using mask value (1 0 0 0). The target routine is then called with the specified mask value <b>430</b> such that the data parallel target routine is executed in a data parallel manner using the preferred SIMD vector slot <b>410</b>.</p>
<p id="p-0077" num="0076">Return values are then converted back into a scalar form from the vector form, such as providing a vector for the return processing. Return values are typically found in one register, dictated by the convention that holds for the machine/operating system/compiler, or any combination thereof. For example, a scalar function may be expected to return its return value in register <b>3</b>; and a SIMD parallel function may be expected to return its value in SIMD vector register <b>3</b> (same number, but different register files). Then, when returning from scalar code, the value in scalar register r<b>3</b> is moved to the Sth slot of the SIMD vector register (where S corresponds to the slot being processed at this time by the scalar function). Technically, values can be moved from one register file to another register file using special move instructions (when available) or via memory (by first storing the value from one register file into memory, and then reading the value from memory into the second register file).</p>
<p id="p-0078" num="0077"><figref idref="DRAWINGS">FIG. 5</figref> is a flowchart outlining an example operation for compiling and linking code to generate executable code. As shown in <figref idref="DRAWINGS">FIG. 5</figref>, the operation starts with receiving a portion of source code (step <b>510</b>). A determination is made by the compiler as to whether data parallel optimizations may be applied to the portion of source code (step <b>520</b>). If so, the data parallel optimizations, such as data parallel if conversion or the like, are applied and metadata is created for the portion of code indicating that the data parallel optimization has been applied (step <b>530</b>). Thereafter, the portion of code is provided to a linker (step <b>540</b>). The linker links the portion of code (calling code) with other portions of code (target code) called by the calling code (step <b>550</b>). The linker sets sentinel values for each portion of code based on the metadata associated with the portions of code to thereby indicate whether the portions of code are data parallel or scalar code (step <b>560</b>). The resulting executable code is then output for execution by a processor (step <b>570</b>). The operation then terminates.</p>
<p id="p-0079" num="0078"><figref idref="DRAWINGS">FIG. 6</figref> is a flowchart outlining an operation for executing a data parallel call in accordance with one illustrative embodiment. <figref idref="DRAWINGS">FIG. 6</figref> outlines an operation that can be performed entirely in software executed by one or more processors of a data processing system, entirely in hardware of one or more processors of a data processing system, or in any combination of hardware and software. In an entirely software approach, it should be appreciated that the software may be stored in a memory of other storage device and may be executed from that memory by one or more processors to perform the various operations and functions described in the flowchart. Similarly, for a software and hardware embodiment, some of the operations and functions described may be performed in software while others are performed in hardware.</p>
<p id="p-0080" num="0079">It should be noted that the separation of work between the compiler and linker in <figref idref="DRAWINGS">FIG. 6</figref>, and in the present description, is only an example and is not intended to state or imply any limitations on the particular implementations of the present invention envisioned or encompassed by the present invention. To the contrary, all of the work may be done by the linker (possibly calling in turn the compiler), by the compiler, or any combination of the linker and compiler, such as the distribution of work described herein.</p>
<p id="p-0081" num="0080">As shown in <figref idref="DRAWINGS">FIG. 6</figref>, the operation starts by receiving a data parallel call (step <b>610</b>). A determination is made as to whether the data parallel call is from calling code that is data parallel or not (step <b>615</b>). As mentioned above, this may be done by looking at the sentinel value for the calling code and determining if the sentinel value indicates that the calling code is data parallel or not, for example.</p>
<p id="p-0082" num="0081">If the calling code is data parallel code, the target address in each SIMD vector slot of the SIMD unit is investigated to generate a parallel execution group (step <b>620</b>). As mentioned above, this may involve, for example, the target address of the target portion of code (target code) stored in the SIMD vector slot being identified and compared to the target addresses stored in the other SIMD vector slots to identify matches. All of the SIMD vector slots that access the same target address are placed in the same parallel execution group. A first target address in the SIMD vector slots that differs from a first SIMD vector slot's target address starts a next parallel execution group. Thus, a parallel execution group may be comprised of one or more instructions of one or more SIMD vector slots. For a particular parallel execution group, based on the comparison of target addresses, a mask value may be set (step <b>625</b>) to identify which SIMD vector slots have instructions targeting the address of the particular parallel execution group and may be executed in parallel.</p>
<p id="p-0083" num="0082">A pointer to a first SIMD vector slot of a next parallel execution group is set based on the generation of the parallel execution group (step <b>630</b>). Once a parallel execution group is generated by comparing target addresses of the SIMD vector slots, the target code specified by the target address is analyzed (step <b>635</b>) to determine if it is data parallel code (step <b>640</b>). To perform this analysis, for example, a sentinel value associated with the target address may be retrieved and analyzed to determine if the sentinel value identifies the target code as being a data parallel routine or a scalar routine. If the sentinel value indicates that the target code is data parallel code, the parallel execution group is executed in parallel using a mask value generated so that only those instructions associated with the SIMD vector slots identified as targeting the same target address are executed in parallel (step <b>645</b>). A determination is then made as to whether all of the operations associated with the SIMD vector slots have completed (step <b>650</b>). If so, the operation terminates. Otherwise, a next parallel execution group is then generated by resetting the mask values, thereby generating a new mask, and starting the operation again using the SIMD vector slot pointed to by the pointer set in step <b>630</b> (step <b>655</b>). The remaining SIMD vector slots are then executed in a subsequent iteration.</p>
<p id="p-0084" num="0083">If the sentinel value indicates that the target code is not data parallel (step <b>640</b>), i.e. it is scalar code, then the arguments of the call to the target code, for a first SIMD vector slot in the parallel execution group, are converted to scalar arguments using a scalar ABI (step <b>660</b>). The target routine is then called in a scalar manner and result values are returned (step <b>665</b>). The results that are returned are then converted back into data parallel return values (step <b>670</b>) and the operation goes to step <b>650</b>.</p>
<p id="p-0085" num="0084">The above process is then repeated for each SIMD vector slot that was not executed in the previous execution. Thus, if the sentinel value indicates a data parallel target routine, then the next SIMD vector slot that was not in the parallel execution group is the first SMID vector slot for the next iteration. The above process may be repeated such that this next SIMD vector slot's address may be compared with any remaining SIMD vector slots whose instructions have not completed to determine if an additional parallel execution group can be generated. If the sentinel value indicates that the target routine is scalar, then the next SIMD vector slot may be a SIMD vector slot that was in the parallel execution group but whose instruction was not actually completed due to the scalar execution, or may be a SIMD vector slot that was not in the original parallel execution group, depending upon the situation. The process is repeated until all of the instructions in the SIMD vector slots are completed at which time the operation repeats for a next set of values in the SIMD vector.</p>
<p id="p-0086" num="0085">If the calling code that is calling the target routine is scalar code (step <b>615</b>), i.e. not data parallel code, again the data parallel call may be used, however because the calling code is scalar, the target routine must be called in a scalar manner. In such a case, the data parallel call again causes the illustrative embodiments to investigate the sentinel value for the target routine (step <b>675</b>) and determine if the sentinel value indicates that the target routine is data parallel or not (step <b>680</b>). If the target routine is scalar, then the target routine is called in a normal scalar manner by the scalar calling code (step <b>685</b>). However, if the target routine is data parallel, then the arguments of the call are converted to a scalar form by using a mask that identifies a preferred SIMD vector slot (step <b>690</b>). The target routine is then called with the specified mask such that the data parallel target routine is executed in a data parallel manner using the preferred SIMD vector slot (step <b>695</b>). Return values are then converted back into a scalar form from the vector form (step <b>700</b>) and the operation branches to step <b>650</b>.</p>
<p id="p-0087" num="0086">In this way, the compiler when compiling the original source code can assume that all routines called by code being compiled are data parallel unless the compiler is able to analyze the target code and prove that the target code is not data parallel. This assumption may then be checked at runtime by investigating a sentinel value provided by the linker that identifies the target routine as being data parallel or not. Conversion of arguments based on whether or not a target routine is data parallel or not may then be made at runtime as a result of the analysis of the sentinel value.</p>
<p id="p-0088" num="0087">As noted above, it should be appreciated that the illustrative embodiments may take the form of an entirely hardware embodiment, an entirely software embodiment or an embodiment containing both hardware and software elements. In one example embodiment, the mechanisms of the illustrative embodiments are implemented in software or program code, which includes but is not limited to firmware, resident software, microcode, etc.</p>
<p id="p-0089" num="0088">A data processing system suitable for storing and/or executing program code will include at least one processor coupled directly or indirectly to memory elements through a system bus. The memory elements can include local memory employed during actual execution of the program code, bulk storage, and cache memories which provide temporary storage of at least some program code in order to reduce the number of times code must be retrieved from bulk storage during execution.</p>
<p id="p-0090" num="0089">Input/output or I/O devices (including but not limited to keyboards, displays, pointing devices, etc.) can be coupled to the system either directly or through intervening I/O controllers. Network adapters may also be coupled to the system to enable the data processing system to become coupled to other data processing systems or remote printers or storage devices through intervening private or public networks. Modems, cable modems and Ethernet cards are just a few of the currently available types of network adapters.</p>
<p id="p-0091" num="0090">The description of the present invention has been presented for purposes of illustration and description, and is not intended to be exhaustive or limited to the invention in the form disclosed. Many modifications and variations will be apparent to those of ordinary skill in the art. The embodiment was chosen and described in order to best explain the principles of the invention, the practical application, and to enable others of ordinary skill in the art to understand the invention for various embodiments with various modifications as are suited to the particular use contemplated.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method, in a data processing system having a processor, for performing data parallel function calls in code during runtime, comprising:
<claim-text>executing, in the processor, a portion of code having a data parallel function call to a target portion of code, wherein the portion of code having the data parallel function call is calling code;</claim-text>
<claim-text>determining, at runtime by the processor, whether the target portion of code is a data parallel portion of code or a scalar portion of code;</claim-text>
<claim-text>determining, at runtime by the processor, whether the calling code is data parallel code or scalar code; and</claim-text>
<claim-text>executing the target portion of code, where the manner of execution of the target portion of code is based on the determination of whether the target portion of code is a data parallel portion of code or a scalar portion of code, and the determination of whether the calling code is data parallel code or scalar code;</claim-text>
<claim-text>wherein if the calling code is scalar code, and the target portion of code is a data parallel portion of code, the processor is configured to execute the target portion of code by calling the target portion of code with a mask to identify a preferred slot of a vector register.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein if the calling code is data parallel code, and the target portion of code is a data parallel portion of code, executing the target portion of code comprises:
<claim-text>executing, by the processor, the target portion of code in a data parallel manner using one or more parallel execution groups.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein if the calling code is scalar code, and the target portion of code is a data parallel portion of code, executing the target portion of code comprises:
<claim-text>converting results of the target portion of code to a scalar format.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining, at runtime by the processor, whether the target portion of code is a data parallel portion of code or a scalar portion of code comprises retrieving a sentinel value associated with the target portion of code and analyzing the sentinel value to determine if the sentinel value indicates the target portion of code to be data parallel or scalar.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the sentinel value is stored in a known location relative to an address of the target portion of code.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the sentinel value is generated by one of a compiler or a linker based on whether or not the compiler successfully applied a data parallel optimization to the target portion of code.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the portion of code is executed as part of a parallel execution group comprising a plurality of iterations of the portion of code, each iteration in the plurality of iterations being associated with a separate vector register slot or separate thread of execution in the processor.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising:
<claim-text>generating the parallel execution group by comparing an address stored in each separate vector register slot or targeted by each separate thread;</claim-text>
<claim-text>identifying separate vector register slots or threads targeting a same address;</claim-text>
<claim-text>combining the separate vector register slots or threads targeting the same address into the parallel execution group; and</claim-text>
<claim-text>setting a pointer to point to a first separate vector register slot or thread that targets an address different than the address of the target portion of code.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein, in a subsequent execution of the iterations associated with the separate vector register slots or threads, a subsequent parallel execution group is generated based on the setting of the pointer. </claim-text>
</claim>
</claims>
</us-patent-grant>
