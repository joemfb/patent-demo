<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625844-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625844</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12344271</doc-number>
<date>20081225</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>TW</country>
<doc-number>96151587 A</doc-number>
<date>20071231</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>1082</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382103</main-classification>
<further-classification>463 36</further-classification>
</classification-national>
<invention-title id="d2e71">Method and device for adjusting output frame</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5574836</doc-number>
<kind>A</kind>
<name>Broemmelsiek</name>
<date>19961100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345427</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6009210</doc-number>
<kind>A</kind>
<name>Kang</name>
<date>19991200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6011581</doc-number>
<kind>A</kind>
<name>Swift et al.</name>
<date>20000100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348 58</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6864912</doc-number>
<kind>B1</kind>
<name>Mahaffey et al.</name>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348 61</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>7203911</doc-number>
<kind>B2</kind>
<name>Williams</name>
<date>20070400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715864</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>7874917</doc-number>
<kind>B2</kind>
<name>Marks et al.</name>
<date>20110100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>463 36</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>7883415</doc-number>
<kind>B2</kind>
<name>Larsen et al.</name>
<date>20110200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>463 36</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>TW</country>
<doc-number>455769</doc-number>
<date>20010900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>TW</country>
<doc-number>200634665</doc-number>
<date>20061000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>TW</country>
<doc-number>200643806</doc-number>
<date>20061200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>TW</country>
<doc-number>200725567</doc-number>
<date>20070700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>TW</country>
<doc-number>200727101</doc-number>
<date>20070700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>WO</country>
<doc-number>2007004377</doc-number>
<kind>A1</kind>
<date>20070100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00014">
<othercit>&#x201c;Office Action of Taiwan Counterpart Application&#x201d;, issued on Apr. 14, 2011, p. 1-p. 11.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00015">
<othercit>&#x201c;2nd Office Action of European Counterpart Application&#x201d;, issued on Jul. 20, 2010, p. 1-p. 5.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00016">
<othercit>Nicholas J. Ward et al., &#x201c;Head-Up Displays and Their Automotive Application: An Overview of Human Factors Issues Affecting Safety&#x201d;, Accident Analysis &#x26; Prevention, vol. 26, No. 6, Dec. 1994, pp. 703-717.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00017">
<othercit>&#x201c;Search Report of European counterpart application&#x201d;, issued on May 8, 2009, p. 1-p. 8.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00018">
<othercit>&#x201c;1st Office Action of China Counterpart Application&#x201d;, issued on Jun. 4, 2010, p. 1-p. 4.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>20</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382103</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382107</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382154</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382276</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348 47- 60</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345  7</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345  8</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345  9</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345 55</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345156</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345157</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345158</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345419</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345427</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>702152</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>702153</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>463 36</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>463 37</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>5</number-of-drawing-sheets>
<number-of-figures>10</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20090169058</doc-number>
<kind>A1</kind>
<date>20090702</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Chen</last-name>
<first-name>Ming-Yu</first-name>
<address>
<city>Taoyuan County</city>
<country>TW</country>
</address>
</addressbook>
<residence>
<country>TW</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Chen</last-name>
<first-name>Ming-Yu</first-name>
<address>
<city>Taoyuan County</city>
<country>TW</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Jianq Chyun IP Office</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>HTC Corporation</orgname>
<role>03</role>
<address>
<city>Taoyuan County</city>
<country>TW</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Le</last-name>
<first-name>Brian</first-name>
<department>2666</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A method and a device for adjusting an output frame are provided. The present disclosure is suitable for an electronic device with a display. In the present disclosure, a relative position between a user and the display is obtained first. Then, an output frame which will be outputted by the display later is adjusted according to the relative position. As a result, the output frame is adjusted in advance to fit the present position of the user, so that the user views the output frame more easily and enjoys the best display result.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="71.37mm" wi="102.95mm" file="US08625844-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="205.74mm" wi="160.53mm" file="US08625844-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="215.90mm" wi="146.22mm" file="US08625844-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="215.90mm" wi="148.00mm" file="US08625844-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="231.90mm" wi="140.80mm" file="US08625844-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="215.90mm" wi="131.66mm" file="US08625844-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading>
<p id="p-0002" num="0001">This application claims the priority benefit of Taiwan application serial no. 96151587, filed on Dec. 31, 2007. The entirety of the above-mentioned patent application is hereby incorporated by reference herein and made a part of this specification.</p>
<heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">1. Field of the Invention</p>
<p id="p-0004" num="0003">The present invention is related to a method and a device for outputting a frame, and particularly to a method and a device capable of automatically adjusting the output frame according to a user's position.</p>
<p id="p-0005" num="0004">2. Description of Related Art</p>
<p id="p-0006" num="0005">As displaying technology advances rapidly, types of displays also evolve from cathode ray tube (CRT) displays in the early days, to flat panel displays capable of reducing more occupied space, and further to touch displays which have been extensively applied to all kinds of electronic products nowadays.</p>
<p id="p-0007" num="0006">From a user's viewpoint, display result of a display most directly influences the impression the user has after using an electronic product. The user can adjust parameters of the display, such as resolution, contrast and luminance, to obtain a better output frame according to his/her own habits. However, when outputting frames through the display, current electronic products usually cannot automatically adjust the frame dynamically according to the user's current position. For example, generally, the size of the words displayed on the frame is fixed, and therefore when the user keeps a farther distance from the display, the user very possibly cannot clearly see the contents of the frame because the words on the frame are too small.</p>
<p id="p-0008" num="0007">When the user views a frame displayed by a liquid crystal display (LCD), the frame is also affected and limited by gray scale inversion of the LCD such that when viewed at certain angles the user would see frames like those in black and white inversion. Moreover, as for most current naked-eye 3D displays, in the actual implementation of the 3D displays, barriers or micro-prisms are disposed therein to limit the left eye of the user to view pixels on the left and the right eye of the user to view pixels on the right so as to achieve the three-dimensional display effect. Therefore, when the user views a 3D display, the viewing is limited by view angles. When the user is located at a dead view angle, the left eye of the user can only see the pixels on the right, and the right eye of the user can only see the pixels on the left. At this moment, the frame seen by the user is broken.</p>
<p id="p-0009" num="0008">As such, if the display cannot adjust an output frame correspondingly according to the user's current position, the display function is very easily limited and will display many display results that are unacceptable to viewers. Since the user cannot see the displayed frames clearly, more negative impressions of the electronic products are likely to be formed.</p>
<heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0010" num="0009">In view of the foregoing, the present disclosure provides a method for adjusting an output frame, which adjusts the output frame correspondingly according to a position where a user views a display so as to generate a better output result.</p>
<p id="p-0011" num="0010">The present disclosure provides a device for adjusting an output frame. After detecting a relative position between a user and a display, the device automatically adjusts the output frame according to the relative position so as to render it more convenient for the user to view the output frame.</p>
<p id="p-0012" num="0011">The present disclosure provides a method for adjusting an output frame, which is suitable for an electronic device with a display. In the method, a relative position between the user and the display is obtained first. Next, the output frame of the display is correspondingly adjusted according to the relative position.</p>
<p id="p-0013" num="0012">According to an embodiment of the present disclosure, the step of obtaining the relative position between the user and the display includes capturing an input image including the user, obtaining a facial feature of the user from the input image, and then determining the relative position between the user and the display according to the facial feature. The facial feature can be eye positions, a nose position, a mouth position or a facial outline of the user.</p>
<p id="p-0014" num="0013">According to an embodiment of the present disclosure, the step of determining the relative position according to the facial feature includes determining a rotating angle of the user relative to the display according to an angle formed by a connected line of the eye positions and a rotating reference line preset on the input image.</p>
<p id="p-0015" num="0014">According to an embodiment of the present disclosure, the step of determining the relative position according to the facial feature includes determining a shift of the user relative to the display according to a shift reference point on the input image and a position of the facial feature in the input image.</p>
<p id="p-0016" num="0015">According to an embodiment of the present disclosure, the step of determining the relative position according to the facial feature includes obtaining a size and proportion of the facial feature relative to the input image and comparing the size and proportion with a proportion reference value so as to determine a distance between the user and the display.</p>
<p id="p-0017" num="0016">According to an embodiment of the present disclosure, the step of correspondingly adjusting the output frame according to the relative position includes scaling the output frame, rotating the output frame, shifting the output frame, adjusting colors of the output frame, changing contents of the output frame and adjusting pixels of the output frame to optimize display result of the output frame.</p>
<p id="p-0018" num="0017">From another viewpoint, the present disclosure provides a device for adjusting an output frame. The device for adjusting the output frame includes a display, a user detecting module, a relative position obtaining module and an image processing module. The display displays the output frame. The user detecting module obtains an input information related to the user. The relative position obtaining module is connected to the user detecting module and determines the relative position between the user and the display with the input information. The image processing module is connected to the display and the relative position obtaining module and adjusts the output frame correspondingly according to the relative position between the user and the display so as to control the display to display the adjusted output frame.</p>
<p id="p-0019" num="0018">According to an embodiment of the present disclosure, the user detecting module is an image capturing device and captures an input image including the user as the input information. Herein, the relative position obtaining module obtains a facial feature of the user from the input image captured by the user detecting module and determines the relative position between the user and the display according to the facial feature.</p>
<p id="p-0020" num="0019">According to an embodiment of the present disclosure, the user detecting module can be an image capturing device. The facial feature can be eye positions, a nose position, a mouth position or a facial outline of the user.</p>
<p id="p-0021" num="0020">According to an embodiment of the present disclosure, the relative position obtaining module determines a rotating angle of the user relative to the display according to an angle formed by a connected line of the eye positions and a rotating reference line on the input image.</p>
<p id="p-0022" num="0021">According to an embodiment of the present disclosure, the relative position obtaining module determines a shift of the user relative to the display according to a shift reference point on the input image and a position of the facial feature in the input image.</p>
<p id="p-0023" num="0022">According to an embodiment of the present disclosure, the relative position obtaining module obtains a size and proportion of the facial feature relative to the input image and compares a proportion reference value and the size and proportion to determine a distance between the user and the display.</p>
<p id="p-0024" num="0023">According to an embodiment of the present disclosure, the image processing module scales the output frame, rotates the output frame, shifts the output frame, adjusts colors of the output frame, changes contents of the output frame or adjusts pixels of the output frame according to the relative position so as to optimize display result of the output frame.</p>
<p id="p-0025" num="0024">In the present disclosure, the output frame is adjusted correspondingly according to the relative position between the user and the display when the user is viewing the display. In other words, the display of the present disclosure outputs the frame most suitable for the user to view in the current position such that the user can view the contents of the output frame with a more relaxing manner and enjoy the best display result at the same time.</p>
<p id="p-0026" num="0025">In order to make the aforementioned and other objects, features and advantages of the present disclosure more comprehensible, several embodiments accompanied with figures are described in detail below.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0027" num="0026">The accompanying drawings are included to provide a further understanding of the disclosure, and are incorporated in and constitute a part of this specification. The drawings illustrate embodiments of the disclosure and, together with the description, serve to explain the principles of the disclosure.</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram of a device for adjusting an output frame according to an embodiment of the present disclosure.</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 2</figref> is a flowchart showing a method for adjusting an output frame according to an embodiment of the present disclosure.</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 3</figref> is a flowchart of obtaining a relative position according to an embodiment of the disclosure.</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIGS. 4</figref>, <b>5</b>, <b>6</b>, <b>7</b>, <b>8</b>, <b>9</b>, and <b>10</b> are schematic views of an image including a user according to an embodiment of the present disclosure.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DESCRIPTION OF EMBODIMENTS</heading>
<p id="p-0032" num="0031">When outputting a frame through a display, if an electronic device can timely adjust the size, angle or even the contents of the output frame according to the current state of a user, the user certainly can experience better viewing result. The present disclosure is a method and a device for adjusting an output frame which are developed from said viewpoint. In order to render the present disclosure more comprehensible, embodiments are described below as the examples to prove that the disclosure can actually be realized.</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram of a device for adjusting an output frame according to an embodiment of the present disclosure. Referring to <figref idref="DRAWINGS">FIG. 1</figref>, a device <b>100</b> for adjusting an output frame includes a display <b>110</b>, a user detecting module <b>120</b>, a relative position obtaining module <b>130</b> and an image processing module <b>140</b>. The display <b>110</b> displays the output frame. The user detecting module <b>120</b> receives an input information related to the user. The relative position obtaining module <b>130</b> is connected to the user detecting module <b>120</b> and determines a relative position between the user and the display <b>110</b> with the input information received by the user detecting module <b>120</b>. The image processing module <b>140</b> is simultaneously connected to the display <b>110</b> and the relative position obtaining module <b>130</b> to correspondingly adjust the output frame according to the relative position between the user and the display <b>110</b> before the display <b>110</b> displays the output frame.</p>
<p id="p-0034" num="0033">According to an embodiment of the present disclosure, the user detecting module <b>120</b> can be an image capturing device such as a photograph lens, and the input information at the moment can be an input image captured by the image capturing device. The user detecting module <b>120</b> also receives the input image and determines the relative position between the user and the display <b>110</b> according to the input image. When the user is using the device <b>100</b> for adjusting the output frame, the image capturing device and the display <b>110</b> both face the user.</p>
<p id="p-0035" num="0034">The device <b>100</b> for adjusting the output frame of the present disclosure can be applied in any electronic device with a display, e.g., mobile phones, personal digital assistants (PDAs), smartphones, personal computers or notebook computers. Herein, the present disclosure does not limit a range of the electronic device. Through operation of the device <b>100</b> for adjusting the output frame, the output frame is dynamically adjusted so that the electronic device maintains its optimal display result. In order to further illustrate the operation of the device <b>100</b> for adjusting the output frame, another embodiment is enumerated for more detailed description below.</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 2</figref> is a flowchart showing a method for adjusting an output frame according to an embodiment of the present disclosure. Referring to both <figref idref="DRAWINGS">FIGS. 1 and 2</figref>, as shown by a step <b>210</b>, the user detecting module <b>120</b> first provides the input information for the relative position obtaining module <b>130</b> to determine a current relative position between the user and the display <b>110</b>. According to the present embodiment, the user detecting module <b>120</b> can be an image capturing device, and the input information provided by the user detecting module <b>120</b> is an input image. As such, steps for obtaining the relative position are shown by <figref idref="DRAWINGS">FIG. 3</figref>.</p>
<p id="p-0037" num="0036">In a step <b>310</b> of <figref idref="DRAWINGS">FIG. 3</figref>, first, the user detecting module <b>120</b> captures an input image including a photograph of the user. Afterwards, as shown by a step <b>320</b>, the relative position obtaining module <b>130</b> obtains a facial feature of the user according to the input image. Herein, the facial feature can be eye positions, a nose position, a mouth position or a facial outline. The relative position obtaining module <b>130</b> can apply any facial detecting technology to obtain the facial feature, and the present disclosure is not limited in this aspect. Nevertheless, to facilitate illustration and considering that the eyes of the user are the most direct subject when viewing the output frame, it is presumed in all the following embodiments that the facial feature to be obtained by the relative position obtaining module <b>130</b> is the eye positions of the user.</p>
<p id="p-0038" num="0037">After the eye positions of the user are obtained, subsequently in a step <b>330</b>, the relative position obtaining module <b>130</b> determines a relative position (e.g., a distance, an angle, or a shift) between the user and the display <b>110</b> according to the eye positions. Several embodiments are enumerated as follows to illustrate steps of determining the relative position by the relative position obtaining module <b>130</b>.</p>
<p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. 4</figref> is a schematic view of an input image including a user according to an embodiment of the present disclosure. Referring to <figref idref="DRAWINGS">FIG. 4</figref>, in the present embodiment, the relative position obtaining module <b>130</b> first obtains display positions of the user's eyes in an input image <b>400</b>. Then, after a shift reference point X set on the input image <b>400</b> and the eye positions are compared, the eye positions are found to be higher than the shift reference point X. Hence, it is determined that the user is currently viewing the display <b>110</b> from an angle higher than the display <b>110</b>. When the input image captured by the user detecting module <b>120</b> is an input image <b>500</b> as shown in <figref idref="DRAWINGS">FIG. 5</figref>, since the user's eyes in the input image <b>500</b> are positioned on the left of a central point X, it is determined that the user views the display <b>110</b> from an angle on the left of the display <b>110</b>. In view of the foregoing, by comparing the eye positions with the central point, a shift of the user in an upper, a lower, the left or the right direction relative to the display <b>110</b> is determined. According to another embodiment, after the shift is obtained, a current view angle of the user can be further calculated from the shift. According to the present embodiment, the shift reference point X can be set in the central points of the input images <b>400</b> and <b>500</b>, but the disposition of the shift reference point X actually is not limited to this.</p>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIGS. 6 and 7</figref> are schematic views of an input image including a user according to another embodiment of the present disclosure. In an input image <b>600</b>, the relative position obtaining module <b>130</b> first determines the user's eye positions by the facial detecting technology, thereby obtaining the user's eye outlines. Thereafter, a size and proportion between the eye outlines and the input image <b>600</b> is calculated. Finally, the size and proportion and a preset proportion reference value are compared to determine a distance between the user and the display <b>110</b>. For example, when the user is closer to the display <b>110</b>, the size and proportion of the eyes in the input image becomes larger (as shown by the input image <b>600</b> in <figref idref="DRAWINGS">FIG. 6</figref>). When the user is farther away from the display <b>110</b>, the proportion of the eyes in the input image becomes smaller (as shown by an input image <b>700</b> in <figref idref="DRAWINGS">FIG. 7</figref>). Therefore, a current distance between the user and the display <b>110</b> is determined by the size and proportion of the facial feature in the input image.</p>
<p id="p-0041" num="0040">According to another embodiment, the user may have to rotate the display <b>110</b> for viewing because the frame is displayed in a different direction. Suppose before the user rotates the display <b>110</b>, an input image <b>800</b> as shown in <figref idref="DRAWINGS">FIG. 8</figref> is captured by the user detecting module <b>120</b>. As shown by the input image <b>800</b>, under circumstances where the display <b>110</b> is not rotated, an angle &#x3b8; formed by a connected line <b>810</b> of the user's eye positions and a rotating reference line <b>820</b> set on the input image <b>800</b> is 90 degrees. When the user rotates the display <b>110</b> to the left by 90 degrees, an input image captured by the user detecting module <b>120</b> is an input image <b>900</b> as shown by <figref idref="DRAWINGS">FIG. 9</figref>, for example. In such a state, a connected line <b>910</b> of the user's eye positions is parallel to a rotating reference line <b>920</b> set on the input image <b>900</b>. In other words, a rotating angle of the user relative to the display <b>110</b> can be determined by an angle formed by the connected line of the user's eye positions and the rotating reference line set on the input image. According to the present embodiment, the rotating reference lines <b>820</b> and <b>920</b> are a horizontal line or a vertical line preset on the input images <b>800</b> and <b>900</b> respectively, but the disposition of the rotating reference lines in fact is not limited thereto.</p>
<p id="p-0042" num="0041">As described above, the relative position obtaining module <b>130</b> not only determines that the user is currently viewing the display <b>110</b> in a position slanting upwards or downwards, or slanting towards the left or the right according to the input image captured by the user detecting module <b>120</b>. Meanwhile, the relative position obtaining module <b>130</b> also determines information as what the distance is between the user and the display <b>110</b> and whether or not the user rotates the display <b>110</b>. However, it should be emphasized that the relative position obtaining module <b>130</b> can obtain the facial feature by any facial identification or detecting technology and calculate the relative position between the user and the display <b>110</b>. In the present disclosure, the relative position is not limited to the shift, rotating angle and distance between the user and the display. Likewise, an algorithm for obtaining the relative position is not limited, either.</p>
<p id="p-0043" num="0042">Next, reverting to a step <b>220</b> in <figref idref="DRAWINGS">FIG. 2</figref>, after obtaining the current relative position between the user and the display <b>110</b>, the image processing module <b>140</b> adjusts the output frame of the display <b>110</b> correspondingly according to the relative position. According to an embodiment, the image processing module <b>140</b> timely scales the contents of the output frame according the distance between the user and the display <b>110</b>. For example, when the distance between the user and the display <b>110</b> is farther, words or graphics in the output frame are automatically enlarged. When the user is very close to the display <b>110</b>, the contents in the output frame such as words and graphics are displayed in a reduced size or a standard size.</p>
<p id="p-0044" num="0043">According to another embodiment, when a shift occurs between the user and the display <b>110</b>, the image processing module <b>140</b> can, for example, shift the output frame correspondingly in a direction of the shift so as to achieve an effect similar to rolling frames. Or, after a current view angle and position of the user is calculated according to the shift, when the user is determined as being in a dead angle of gray scale inversion of an LCD, the image processing module <b>140</b> first adjusts colors of the output frame and then controls the display <b>110</b> to output the frames so that the user sees the frames in the correct colors.</p>
<p id="p-0045" num="0044">Furthermore, when a rotating angle exists between the user and the display <b>110</b>, the image processing module <b>140</b> rotates the output frame correspondingly according to the rotating angle. For example, when the user rotates the display <b>110</b> by 90 degrees, the image processing module <b>140</b> changes a display direction of the output frame from the original vertical direction to a horizontal direction. It should be noted that besides rotating the output frame by 90 degrees, if the user detecting module <b>120</b> captures an input image <b>1000</b> as shown by <figref idref="DRAWINGS">FIG. 10</figref>, the image processing module <b>140</b> also rotates the output frame correspondingly according to an angle &#x3b8; formed by a connected line <b>1010</b> of the user's eye positions and a rotating reference line <b>1020</b> set on the input image <b>1000</b> (i.e., the output frame being rotated by &#x3b8; degrees and then outputted by the display <b>110</b>). Thus, the user can view the words or graphics in the output frame more conveniently.</p>
<p id="p-0046" num="0045">In addition to changing the size and the display direction of the output frame, according to an embodiment, the image processing module <b>140</b> can further change the contents of the output frame according to the relative position between the user and the display <b>110</b>. For example, when the user is close to the display <b>110</b>, the image processing module <b>140</b> controls the display <b>110</b> to output a frame with an analog clock. When the user keeps a farther distance from the display <b>110</b>, the output frame is changed into a digital clock to enhance readability. According to still another embodiment, the image processing module <b>140</b> can further display different output frames according to the angle at which the user rotates the display <b>110</b> so as to achieve an effect similar to displaying a laser animation sticker or flipping through pages.</p>
<p id="p-0047" num="0046">According to another embodiment, suppose the display <b>110</b> is a 3D display. When the relative position between the user and the display <b>110</b> indicates that the user is located exactly in a dead view angle, the image processing module <b>140</b> optimizes the output frame by adjusting pixels of the output frame, for example, to present the best 3D display result so that the view angle issue of the 3D display is resolved.</p>
<p id="p-0048" num="0047">In summary, the method and the device for adjusting the output frame in the present disclosure correspondingly adjusts the output frame according to the relative position between the user and the display. Thereby, the output frame is displayed not only in the most suitable size, the most correct colors and the best angle, flexibility of the output frame is also enhanced simultaneously so that the user enjoys the optimal display result in any view position, and the impression formed from use of the electronic device improves as well.</p>
<p id="p-0049" num="0048">It will be apparent to those skilled in the art that various modifications and variations can be made to the structure of the present disclosure without departing from the scope or spirit of the disclosure. In view of the foregoing, it is intended that the present disclosure cover modifications and variations of this disclosure provided they fall within the scope of the following claims and their equivalents.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method for adjusting an output frame, suitable for an electronic device with a display, the method comprising:
<claim-text>obtaining a relative position between a user and the display by a relative position obtaining module; and</claim-text>
<claim-text>adjusting an output frame of the display according to the relative position by an image processing module,</claim-text>
<claim-text>wherein adjusting the output frame comprises scaling the output frame based on a distance between the user and the display such that when the distance is farther, a content in the output frame is enlarged accordingly,</claim-text>
<claim-text>wherein when the distance is shorter than a set distance, the content in the output frame is displayed in a reduced size or an original size accordingly,</claim-text>
<claim-text>wherein a current view angle based on the relative position of the user is calculated with respect to a shift, when the user is determined as being in a dead angle of gray scale inversion of the display, the image processing module first adjusts colors of the output frame and then controls the display to display the output frame so that the user sees the output frame in correct colors.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method for adjusting the output frame as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein obtaining the relative position between the user and the display comprises:
<claim-text>capturing an input image;</claim-text>
<claim-text>obtaining a facial feature of a user from the input image; and</claim-text>
<claim-text>determining the relative position between the user and the display according to the facial feature.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method for adjusting the output frame as claimed in <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the facial feature comprises at least one of the following: eye positions, a nose position, a mouth position and a facial outline.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method for adjusting the output frame as claimed in <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein determining the relative position according to the facial feature comprises:
<claim-text>determining a rotating angle of the user relative to the display according to an angle formed by a connected line of the eye positions and a rotating reference line set on the input image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method for adjusting the output frame as claimed in <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein determining the relative position according to the facial feature comprises:
<claim-text>determining a shift of the user relative to the display according to a shift reference point set on the input image and a position of the facial feature in the input image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method for adjusting the output frame as claimed in <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein determining the relative position according to the facial feature comprises:
<claim-text>obtaining a size and proportion of the facial feature relative to the input image; and</claim-text>
<claim-text>comparing a proportion reference value and the size and proportion to determine a distance between the user and the display.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method for adjusting the output frame as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the output frame is adjusted by at least one of scaling the output frame, rotating the output frame and shifting the output frame.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method for adjusting the output frame as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the output frame is adjusted by adjusting colors of the output frame.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method for adjusting the output frame as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the output frame is adjusted by changing contents of the output frame.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method for adjusting the output frame as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the output frame is adjusted by adjusting pixels of the output frame.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. A device for adjusting an output frame, comprising:
<claim-text>a display, for displaying the output frame;</claim-text>
<claim-text>a user detecting module, for obtaining an input information related to a user, wherein a distance between the user and the display is also detected;</claim-text>
<claim-text>a relative position obtaining module, connected to the user detecting module and used for determining a relative position between the user and the display with the input information; and</claim-text>
<claim-text>an image processing module, connected to the display and the relative position obtaining module to adjust the output frame according to the relative position and controlling the display to display the output frame,</claim-text>
<claim-text>wherein adjusting the output frame comprises scaling the output frame based on the distance between the user and the display such that when the distance is farther, a content in the output frame is enlarged accordingly,</claim-text>
<claim-text>wherein when the distance is shorter, the content in the output frame is displayed in a reduced size or an original size accordingly,</claim-text>
<claim-text>wherein a current view angle based on the relative position of the user is calculated with respect to a shift, when the user is determined as being in a dead angle of gray scale inversion of the display, the image processing module first adjusts colors of the output frame and then controls the display to display the output frame so that the user sees the output frame in correct colors.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The device for adjusting the output frame as claimed in <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the user detecting module is an image capturing device, the input information is an input image comprising the user, and the relative position obtaining module obtains a facial feature of the user from the input image and determines the relative position between the user and the display according to the facial feature.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The device for adjusting the output frame as claimed in <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the facial feature comprises at least one of the following: eye positions, a nose position, a mouth position and a facial outline.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The device for adjusting the output frame as claimed in <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the relative position obtaining module determines a rotating angle of the user relative to the display according to an angle formed by a connected line of the eye positions and a rotating reference line set on the input image.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The device for adjusting the output frame as claimed in <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the relative position obtaining module determines a shift of the user relative to the display according to a shift reference point preset on the input image and a position of the facial feature in the input image.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The device for adjusting the output frame as claimed in <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the relative position obtaining module obtains a size and proportion of the facial feature relative to the input image and compares a proportion reference value and the size and proportion to determine a distance between the user and the display.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The device for adjusting the output frame as claimed in <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the image processing module carries out at least one of scaling the output frame according to the relative position, rotating the output frame according to the relative position and shifting the output frame according to the relative position.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The device for adjusting the output frame as claimed in <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the image processing module adjusts colors of the output frame according to the relative position.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The device for adjusting the output frame as claimed in <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the image processing module changes contents of the output frame according to the relative position.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The device for adjusting the output frame as claimed in <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the image processing module adjusts pixels of the output frame according to the relative position. </claim-text>
</claim>
</claims>
</us-patent-grant>
