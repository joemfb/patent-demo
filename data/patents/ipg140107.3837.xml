<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08624904-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08624904</doc-number>
<kind>B1</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13530291</doc-number>
<date>20120622</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20110101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>T</subclass>
<main-group>15</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>345473</main-classification>
<further-classification>345474</further-classification>
<further-classification>345475</further-classification>
<further-classification>345629</further-classification>
</classification-national>
<invention-title id="d2e43">Controlling animated character expressions</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5111409</doc-number>
<kind>A</kind>
<name>Gasper et al.</name>
<date>19920500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5995119</doc-number>
<kind>A</kind>
<name>Cosatto et al.</name>
<date>19991100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345473</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6061072</doc-number>
<kind>A</kind>
<name>Rouet et al.</name>
<date>20000500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345473</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6307576</doc-number>
<kind>B1</kind>
<name>Rosenfeld</name>
<date>20011000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715700</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6731302</doc-number>
<kind>B1</kind>
<name>Cote</name>
<date>20040500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345619</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6876364</doc-number>
<kind>B2</kind>
<name>Buddemeier et al.</name>
<date>20050400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>7027054</doc-number>
<kind>B1</kind>
<name>Cheiky et al.</name>
<date>20060400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345473</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>7209577</doc-number>
<kind>B2</kind>
<name>McAlpine et al.</name>
<date>20070400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>7483553</doc-number>
<kind>B2</kind>
<name>Xu et al.</name>
<date>20090100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382118</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>8207971</doc-number>
<kind>B1</kind>
<name>Koperwas et al.</name>
<date>20120600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345473</main-classification></classification-national>
</us-citation>
<us-citation>
<nplcit num="00011">
<othercit>Seo et al, Synthesizing Animatable Body Models with Parameterized Shape Modifications, pp. 1-7, 2003.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00012">
<othercit>Allen et al, The space of human body shapes:reconstruction and parameterization from range scans, pp. 1-8, 2003.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00013">
<othercit>Thalmann et al, Automatic Modeling of Virtual Humans and Body Clothing, pp. 575-584, 2004.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00014">
<othercit>Parke, Parameterize Model for facial Animations, pp. 61-68, Nov. 1982.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00015">
<othercit>J.P. Lewis, et al. &#x201c;Reducing Blendshape Interference by Selected Motion Attenuation.&#x201d;, Proceedings of the 2005 Symposium on Interactive 3D Graphics and Games (I3D '05), 2005, 5 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>39</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>345473</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345474</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345475</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345629</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>9</number-of-drawing-sheets>
<number-of-figures>9</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>12388806</doc-number>
<date>20090219</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>8207971</doc-number>
<date>20120626</date>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13530291</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>61141778</doc-number>
<date>20081231</date>
</document-id>
</us-provisional-application>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Koperwas</last-name>
<first-name>Michael</first-name>
<address>
<city>San Francisco</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Pighin</last-name>
<first-name>Frederic P.</first-name>
<address>
<city>Sausalito</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Phillips</last-name>
<first-name>Cary</first-name>
<address>
<city>Moss Beach</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="004" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Sullivan</last-name>
<first-name>Steve</first-name>
<address>
<city>San Francisco</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="005" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Hueso</last-name>
<first-name>Eduardo</first-name>
<address>
<city>Plantation</city>
<state>FL</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Koperwas</last-name>
<first-name>Michael</first-name>
<address>
<city>San Francisco</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Pighin</last-name>
<first-name>Frederic P.</first-name>
<address>
<city>Sausalito</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Phillips</last-name>
<first-name>Cary</first-name>
<address>
<city>Moss Beach</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="004" designation="us-only">
<addressbook>
<last-name>Sullivan</last-name>
<first-name>Steve</first-name>
<address>
<city>San Francisco</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="005" designation="us-only">
<addressbook>
<last-name>Hueso</last-name>
<first-name>Eduardo</first-name>
<address>
<city>Plantation</city>
<state>FL</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Kilpatrick Townsend &#x26; Stockton LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Lucasfilm Entertainment Company Ltd.</orgname>
<role>02</role>
<address>
<city>San Francisco</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Nguyen</last-name>
<first-name>Phu K</first-name>
<department>2677</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A system includes a computer system capable of representing one or more animated characters. The computer system includes a blendshape manager that combines multiple blendshapes to produce the animated character. The computer system also includes an expression manager to respectively adjust one or more control parameters associated with each of the plurality of blendshapes for adjusting an expression of the animated character. The computer system also includes a corrective element manager that applies one or more corrective elements to the combined blendshapes based upon at least one of the control parameters. The one or more applied corrective elements are adjustable based upon one or more of the control parameters absent the introduction of one or more additional control parameters.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="160.78mm" wi="191.60mm" file="US08624904-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="214.29mm" wi="188.55mm" orientation="landscape" file="US08624904-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="231.65mm" wi="181.61mm" orientation="landscape" file="US08624904-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="183.22mm" wi="176.28mm" orientation="landscape" file="US08624904-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="193.29mm" wi="213.11mm" orientation="landscape" file="US08624904-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="191.69mm" wi="194.56mm" orientation="landscape" file="US08624904-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="238.51mm" wi="200.49mm" orientation="landscape" file="US08624904-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="226.23mm" wi="179.41mm" orientation="landscape" file="US08624904-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="187.28mm" wi="205.57mm" orientation="landscape" file="US08624904-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="262.38mm" wi="187.62mm" orientation="landscape" file="US08624904-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CLAIM OF PRIORITY</heading>
<p id="p-0002" num="0001">This application is a continuation application and claims priority under 35 U.S.C. &#xa7;120 to U.S. patent application Ser. No. 12/388,806 filed on Feb. 19, 2009 (U.S. Pat. No. 8,207,971 to be issued on Jun. 26, 2012), which claims priority under 35 USC &#xa7;119(e) to U.S. Patent Application Ser. No. 61/141,778, filed on Dec. 31, 2008, the entire contents of which are hereby incorporated by reference.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">TECHNICAL FIELD</heading>
<p id="p-0003" num="0002">This document relates to controlling and adjusting expressions of animated characters.</p>
<heading id="h-0003" level="1">BACKGROUND</heading>
<p id="p-0004" num="0003">Parametric models may be generated by a computer for producing animated characters with user-adjustable facial expressions. To simulate the appearance of muscle movement in a character's face, one or more types of models may be implemented. For one model, a character's face may be represented with a collection of contour lines and vertices. Dependent upon the desired level of resolution, an extremely large number of vertices (e.g., tens of thousands, millions, etc.) may be included in the model. By adjusting the position of the vertices, various facial expressions may be represented on the character face. For some facial expressions, relatively few vertex position adjustments are needed while significant adjustments may needed to for representing other facial expressions.</p>
<p id="p-0005" num="0004">In some implementations, each facial expression is attained from a linear combination of a selected set of facial expressions (referred to as blendshapes). By adjusting one or more parameters associated with the linear combination, a range of facial expressions can be created while utilizing relatively small amounts of computational resources. For each blendshape, a deformable surface that represents the animated character's face may be divided into distinct shapes with non-intersecting boundaries. As such, adjacent shapes that can represent muscular movements tend not to interfere since the shapes do not overlap, however, the range of producible facial expressions that may be limited.</p>
<heading id="h-0004" level="1">SUMMARY</heading>
<p id="p-0006" num="0005">The systems and techniques described here relate to using corrective shapes to control facial expressions of animated characters.</p>
<p id="p-0007" num="0006">In one aspect, a computer-implemented method includes combining two or more blendshapes of an animated character, in which each blendshape is capable of being respectively adjusted by a control parameter. The method also includes applying one or more corrective elements to the combined blendshapes based upon at least one the control parameters.</p>
<p id="p-0008" num="0007">Implementations may include any or all of the following features. Applying one or more corrective elements may include producing a corrective element to adjust the combined blendshapes. For example, the applied corrective element may adjust the geometry of the animated character. Applying a corrective element may include adjusting a non-geometrical feature of the animated character. Applying a corrective element may also returning the animated character to a predefined facial expression. Corrective elements may be applied to independent features of the combined blendshapes. Multiple corrective elements may be applied to the combined blendshapes, based upon one or more of the control parameters of the blendshapes. Multiple corrective elements may also return the animated character to a predefined facial expression. The values of one control parameter may be representative of a range of facial expressions of the animated character, a range of muscular movements of the animated character, a range of simulated movements of a surface of the animated character, a range of movements of one or more joints of the animated character, and the like. Application of the corrective element to the combined blendshapes may also be based on user input.</p>
<p id="p-0009" num="0008">In another aspect, a system includes an expression manager for respectively adjusting control parameters of blendshapes to adjust an expression of an animated character. The system also includes a blendshape manager for combining two or more blendshapes of the animated character. The system further includes a corrective element manager for applying at least one corrective element to the combined blendshapes based upon at least one of the control parameters.</p>
<p id="p-0010" num="0009">In another aspect, a computer program product tangibly embodied in an information carrier and comprising instructions that when executed by a processor perform a method that include combining two or more blendshapes of an animated character. At least one control parameter is capable of adjusting each blendshape. The method also includes applying one or more corrective elements to the combined blendshapes based upon at least one of the control parameters.</p>
<p id="p-0011" num="0010">In another aspect, a system includes a computer system capable of representing one or more animated characters. The computer system includes a blendshape manager that combines multiple blendshapes to produce the animated character. The computer system also includes an expression manager to respectively adjust one or more control parameters associated with each of the blendshapes for adjusting an expression of the animated character. The computer system also includes a corrective element manager that applies one or more corrective elements to the combined blendshapes based upon at least one of the control parameters. The one or more applied corrective elements is adjustable based upon one or more of the control parameters absent the introduction of one or more additional control parameters.</p>
<p id="p-0012" num="0011">Details of one or more implementations are set forth in the accompanying drawings and the description below. Other features, aspects and advantages will be apparent from the description and drawings, and from the claims.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">DESCRIPTION OF DRAWINGS</heading>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 1</figref> illustrates exemplary ranges of animated character expressions.</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 2</figref> illustrates interfering shapes.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 3</figref> illustrates use of a corrective element.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 4</figref> is a diagram of a character development system.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIGS. 5-7</figref> illustrate applying corrective elements.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 8</figref> illustrates operations of an expression manager, a blendshape manager and a corrective element manager.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 9</figref> is a flowchart of operations of an expression manager, a blendshape manager and a corrective element manager.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<p id="p-0020" num="0019">Like reference symbols in the various drawings indicate like elements.</p>
<heading id="h-0006" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0021" num="0020">Referring to <figref idref="DRAWINGS">FIG. 1</figref>, facial expressions may be represented by models produced from a linear combination of a selected set of facial expressions (referred to as blendshapes). By adjusting one or more parameters associated with the linear combination, a range of facial expressions can be created while utilizing relatively small amounts of computational resources. For each blendshape, a deformable surface that represents the animated character's face may be divided into distinct geometries with non-intersecting boundaries. For example, two blendshapes are illustrated that are each capable of providing two ranges of facial expressions for a relatively simplistic animated character. Individual expressions may be produced based upon one or more parameters that control the deformable geometries of the blendshapes. In this particular example, one blendshape <b>100</b> represents a range of facial expressions that are bounded by a neutral facial expression <b>102</b> and an expression <b>104</b> of happiness (e.g., a smiling face). Similarly, a second blendshape <b>106</b> is capable of producing a range of expressions that are also bounded by a neutral expression <b>108</b> (similar to neutral expression <b>102</b>) and an expression <b>110</b> representing an emotion of surprise. In this example, the blendshapes <b>100</b>, <b>106</b> are respectively bounded by a neutral expression and an expression of happiness or surprise, however, in some arrangements other emotions (e.g., sadness, anger, etc.) and bounding expressions may be represented in the blendshapes.</p>
<p id="p-0022" num="0021">In general, the animated character's face is represented by a collection of vertices (e.g., a mesh) with adjustable positions. For example, the positions of the vertices may be adjusted to illustrate facial expression changes from a neutral position (e.g., facial expression <b>102</b>) to another facial expression (e.g., facial expression <b>104</b>). As such, the blendshapes may include information that represents the positional difference of the vertices between one facial expression (e.g., a neutral expression) and another expression (e.g., an expression of an extreme emotion such as happiness). By controlling the vertices positions, the blendshape may be adjusted to produce each of the bounding expressions (e.g., neutral expression <b>102</b>, expression of happiness <b>104</b>) and expressions that represent linear interpolations of the bounding expressions. In some arrangements, the effect of a geometrical adjustment on a single vertex is a three-dimensional displacement that is referred to as a delta. As such, a blendshape may be considered as a collection of three-dimensionally displaced vertices or deltas.</p>
<p id="p-0023" num="0022">One or more techniques may be implemented for producing distinct expressions from the ranges of expressions provided by the blendshapes <b>100</b>, <b>106</b>. For example, a control parameter (e.g., a weight) that ranges in value from 0 to 1 may be associated with each blendshape. A weight value of 0 may be assigned to one expression (e.g., the neutral expression <b>102</b>) located at one limit of the expression range and a weight value of 1 may be assigned to the other limit of the range (e.g., the expression of happiness <b>104</b>). To illustrate the use of such control parameter values, respective sliders <b>112</b>, <b>114</b>, <b>116</b> and <b>118</b> represent the values assigned to each respective expression. For facial expressions between the two boundary expressions, a weight value between 0 and 1 (e.g., 0.8) may be assigned and set by a corresponding slider. As such, control parameter values may be considered as driving a blendshape to a particular expression. For example, by adjusting a slider (e.g., slider <b>114</b>) to a particular weight value, a particular expression (e.g., the expression of happiness <b>104</b>) may be &#x201c;dialed-up&#x201d; by a user and rendered on the animated character.</p>
<p id="p-0024" num="0023">While each of the blendshapes <b>100</b> and <b>106</b> provides a range of facial expressions, by combining blendshapes, additional expressions may be represented on an animated face of a character. For example, by combining the blendshape <b>100</b> (that represents a range expression happiness) with the blendshape <b>106</b> (that represents a range of emotions of surprise), additional expressions may be represented. Although, by combining such expressions, the deltas included in one blendshape may not properly combine with corresponding deltas of the other blendshapes, thereby causing unrealistic facial expressions to be produced.</p>
<p id="p-0025" num="0024">Referring to <figref idref="DRAWINGS">FIG. 2</figref>, the blendshape <b>100</b> (represented by the facial expression <b>104</b>) is illustrated as being combined with the blendshape <b>106</b> (represented by the facial expression <b>110</b>) to produce a blendshape that includes an expression <b>200</b>. However, by combining the blendshapes, individual geometries included in the two blendshapes may improperly combine. Facial muscle movements generally correlate across a variety of facial expressions and combining expressions may produce highly irregular muscle representations and unrealistic facial expressions. For example, the facial expression <b>104</b> (dialed up with slider <b>114</b>) is illustrated as being combined with the facial expression <b>110</b> (dialed up with slider <b>118</b>) to produce the expression <b>200</b>, which represents a maximum level of the happiness being combined a maximum level of surprise (as illustrated with sliders <b>202</b> and <b>204</b>). Due to the correlated geometries included in the two facial expressions <b>104</b> and <b>110</b>, features of the expression <b>200</b> appear improperly proportioned. For example, a feature <b>206</b> that represents the mouth of the character appears abnormally large with respect to the size of the character's face. As such, one or more facial features of the combined blendshapes may need to be corrected so that a realistic and recognizable expression is produced. For such corrections, one or more corrective elements may be applied to the blendshapes for geometrical and non-geometrical adjustments. For example, one or more geometries (referred to as corrective shapes) may be added to the combined blendshapes to achieve a desired expression. Non-geometrical adjustments, which may be provided by applied corrective elements, may include texture adjustments (e.g., skin texture adjustments), color adjustments (e.g., adjusting skin pigmentation), or other similar types of adjustments. Various types of corrective elements may be provided for geometric and non-geometric adjustments, for example, blendshapes, textures, normal maps, displacement maps, etc. may be implemented.</p>
<p id="p-0026" num="0025">Referring to <figref idref="DRAWINGS">FIG. 3</figref>, one type of corrective element, a corrective shape <b>300</b>, is applied to the combined blendshape that includes the facial expression <b>200</b> to produce a realistic facial expression <b>302</b> that properly represents a combined expression of happiness and surprise. For example, application of the corrective shape <b>300</b> adjusts the expression <b>200</b> to reshape one or more facial features (e.g., the character's mouth). One or more techniques may used to define and produce the corrective shape <b>300</b>. For example, the corrective shape <b>300</b> may represent the positional difference of vertices included in the irregular facial expression <b>200</b> and vertices of a desired expression. As such, by applying the corrective shape <b>300</b> to the expression <b>200</b>, interference of correlated geometrical movement is substantially removed and, for this example, a corrected facial feature <b>304</b> is provided.</p>
<p id="p-0027" num="0026">Typically, one or more corrective shapes is used to address geometry conflicts for a particular feature or a portion of a feature of a character's face. For example, corrective shapes may be produced for adjusting the corner of a character's mouth, an eyebrow, or other type of facial feature. As such, a large number of corrective shapes may be applied to correct interfering geometries caused by combining blendshapes. For example, to animate a relatively complex character face in which subtle facial variations are needed for a particular performance, an extremely large number of corrective shapes may be needed. Furthermore, applying one corrective shape to correct one facial feature (e.g., the corner of a character's mouth) may interfere with another facial feature (e.g., the upper lip of the character) or even another corrective shape, thereby causing the need for even more corrective shapes.</p>
<p id="p-0028" num="0027">Similar to dialing up expressions included in a blendshape, a control parameter (e.g., a weight) may be used to adjust the contribution of a corrective shape. As such, as more and more corrective shapes (e.g., hundreds, thousands) are applied to an animated character, the number of control parameters proportionally increase. Correspondingly, a large number of control parameter adjustments may be needed to adjust each corrective shape for each facial expression of a blendshape. Along with possibly requiring a significant amount of modeler time to create and apply the corrective shapes, considerable amount of animator time may be needed for adjusting the corrective shapes to produce the desired character facial expressions for a performance (e.g., a movie, television program, etc.).</p>
<p id="p-0029" num="0028">Referring to <figref idref="DRAWINGS">FIG. 4</figref>, a character development system <b>400</b> includes a computer system <b>402</b> (or other type of computing device) capable of generating animated characters that use a reduced number of control parameters for adjusting facial expressions of the characters. Along with creating and applying corrective elements (e.g., corrective shapes) to reduce geometry interference, the character development system <b>400</b> also drives control parameters of blendshapes and corrective elements in such a manner that less controls parameters need to be adjusted by a user.</p>
<p id="p-0030" num="0029">Along with components (e.g., interface cards, etc.) for receiving user input (e.g., from a modeler, animator, etc.) and data (e.g., character models) from various sources (e.g., a library of character models, the Internet, other computer systems, etc.), the computer system <b>402</b> also includes memory (not shown) and one or more processors (also not shown) to execute processing operations. A storage device <b>404</b> (e.g., a hard drive, a CD-ROM, a Redundant Array of Independent Disks (RAID) drive, etc.) is in communication with the computer system <b>402</b> and is capable of storing data and providing stored data associated with character generation and character performance production. For example, an illustrative set of blendshapes <b>406</b> and corrective shapes <b>408</b> are represented as being stored on the storage device <b>404</b> and retrievable by the computer system <b>402</b> for creating characters and animating performances of the characters. Additional types of geometries and information may also be stored in the storage device <b>404</b>.</p>
<p id="p-0031" num="0030">In this arrangement, to incorporate individual blendshapes or combinations of blendshapes into animated characters, a blendshape manager <b>410</b> is executed by the computer system <b>402</b>. In some arrangements, in addition to storing and retrieving blendshapes to and from the storage device <b>404</b>, the blendshape manager <b>410</b> may execute other operations for character development. For example, a modeler may use the blendshape manager <b>410</b> for revising previously created characters (e.g., character facial expressions) or create (e.g., electronically sculpt) new character expressions that may be stored in the storage device <b>404</b>. Automated software packages (e.g., drawing and painting packages, CAD packages, photograph editing packages, etc.) may be used in concert with the blendshape manager <b>410</b> to produce such new and revised facial expressions. The blendshape manager <b>410</b> also associates one or more control parameters with each blendshape. For example, one or more weights may be assigned to allow dialing up of different expressions that may be provided by a blendshape.</p>
<p id="p-0032" num="0031">To reduce geometry interference (e.g., caused by combining two or more blendshapes), a corrective shape manager <b>412</b> is executed by the computer system <b>402</b>. Similarly, for non-geometrical adjustments (or geometrical and non-geometrical adjustments), a corrective element manager may be executed. Along with generating corrective shapes to counteract conflicting geometries, the corrective shape manager <b>412</b> also associates one or more control parameters with corresponding corrective shapes. For example, one or more variable weights may be assigned to a corrective shape for adjusting its contribution.</p>
<p id="p-0033" num="0032">For controlling the blendshapes and corrective shapes, and to reduce the number of control parameters that need user interaction, an expression manager <b>414</b> is also executed by the computer system <b>402</b>. By reducing user interaction, less modeler time is needed for producing character models and less animator time is needed for adjusting geometries for a character performance. In general, the expression manager <b>414</b> allows a large number of geometries to be controlled by a relatively small number of control parameters. In one arrangement, the expression manager <b>414</b> provides one or more high level adjustable control parameters that drive the control parameters of associated blendshapes and corrective shapes. For example, high level adjustable control parameters may be provided for particular facial features or portions of a character's face. As such, a high level control for a character's mouth, eyebrows, forehead, etc. may be provided for user adjustments. Individual muscles and muscle groups may also be assigned one or more high level control parameters for adjusting muscle geometries to produce a variety of expressions. In some arrangements, control parameters may be assigned for controlling the movement of one or multiple surfaces such as character surfaces (e.g., skin, flesh, etc.), articles associated with a character (e.g., clothing) or other similar movable surfaces. In still other arrangements, the control parameters may be assigned to controlling the movement of structures associated with the character. For example, the movements of joints (e.g., shoulder, wrist, elbow, knee, etc.) may be assigned to one or more control parameters. Character emotions are still another basis for a high level control parameter that may be used to drive individual blendshapes and corrective shapes associated with individual expressions or combinations of expressions. For example, one high level control parameter may be associated with different levels of the emotion happiness and used to drive control parameters of various geometries to provide a range of expressions of happiness.</p>
<p id="p-0034" num="0033">Along with applying corrective shapes to the geometry of a character being developed, information associated with the corrective shapes may be stored for later retrieval and reapplication. For example, a modeler may apply a corrective shape to the nose of a character that has been dialed up (with a high level control) to express an emotion of extreme happiness. Upon being applied, the corrective shape may be stored in the storage unit <b>404</b> along with the blendshapes to which the corrective shape has been applied. Information is also stored that records this association between the corrective shape and the blendshapes. For example, data may be stored in a shape database <b>416</b> that represents the application of the corrective shape to the blendshapes for providing an appropriate geometry of a character nose for an expression of extreme happiness. As such, the corrective shape may be retrieved and applied to the blendshapes for each instance of the character being dialed up to present an emotion of happiness. Thus, application of a corrective shape is initiated by use of a high level control to dial a character to an expression. Furthermore, along with initiating the application of a corrective shape, the high level control may trigger adjustments to the corrective shape. Continuing with the example, the corrective shape may be retrieved and applied based on the high level control being dialed to a value associated with an emotion of extreme happiness. Correspondingly, as the high level control is used to reduce the happiness level being expressed, the geometry of the corrective shape may be adjusted such that the character's nose is adjusted for this expression of reduced happiness.</p>
<p id="p-0035" num="0034">Referring to <figref idref="DRAWINGS">FIG. 5</figref>, information may be also be stored for applying one or more corrective shapes to a combination of blendshapes. As illustrated in <figref idref="DRAWINGS">FIG. 3</figref>, the corrective shape <b>300</b> is applied to a combination of blendshapes to produce the facial expression <b>302</b> to appropriately represent combined emotions of happiness and surprise. By storing the corrective shape <b>300</b> (e.g., in the storage device <b>404</b>), the expression manager <b>414</b> can initiate retrieving and applying the corrective shape to the combined blendshape for each instance of reproducing the expression. For example, whenever the sliders <b>202</b> and <b>204</b> are adjusted to values of one, the corrective shape is retrieved and applied to the combination of blendshapes to produce the facial expression <b>302</b>.</p>
<p id="p-0036" num="0035">In a similar manner, additional corrective shapes may be applied to a combination of blendshapes for other expressions (and stored for latter retrieval and application). For example, upon the slider <b>204</b> being adjusted such that the happiness control parameter has a value of 0.5, the combined blendshape provides a facial expression <b>500</b>. Due to the application of the corrective shape <b>300</b>, in this instance, the expression <b>500</b> may be unrealistic. In this particular example, a facial feature <b>502</b> that represents the mouth of the character is abnormally large and elongated. To adjust the size and geometry of the facial feature <b>502</b>, another corrective shape <b>504</b> is applied to produce a more realistic facial expression <b>506</b>. With the introduction of the corrective shape <b>504</b>, the corrective shape <b>300</b> compensates (e.g., adjusts deltas) to account for the second corrective shape. Such compensations allow the facial expression <b>302</b> to be returned when the happiness control parameter is dialed up (via the slider <b>204</b>) to a value of one. Thus, during such control parameter adjustments (for dialing up facial expressions), the corrective shapes correspondingly adjust. For example, a parameter (e.g., a numerical value) that represents the geometry of the second corrective shape <b>504</b> may change correspondingly with the control parameters (being adjusted via the sliders <b>202</b>, <b>204</b>, etc.). Alternatively, the parameter associated with the second corrective shape <b>504</b> may be affected by control parameter adjustments. For example, as the happiness control parameter is dialed down (via the slider <b>204</b>) from the value of 0.5 to zero, the parameter associated with second corrective shape <b>504</b> may adjust from a value of one to zero. However, as the control parameter is dial up (via the slider <b>204</b>) from the value of 0.5 to one, the parameter associated with the second corrective shape <b>503</b> may retain a value of one.</p>
<p id="p-0037" num="0036">One or more techniques may be implemented to produce the corrective shape <b>504</b>, for example, a modeler may electronically sculpt the facial expression <b>506</b> from the abnormal facial expression <b>500</b> to produce the corrective shape <b>504</b>. The character development system <b>400</b> may also detect the differences between the facial expressions <b>500</b> and <b>110</b> and produce the corrective shape in an automatic manner. For example, a facial expression that defines a boundary of a blendshape may be identified by the system <b>400</b>. Upon an expression of the blendshape being reproduced due to a particular control parameter setting being detected (e.g., the surprise control parameter set to one and the happiness control parameter being set to zero), one or more corrective shapes are applied.</p>
<p id="p-0038" num="0037">Along with applying the corrective shape <b>504</b>, the corrective shape manager <b>412</b> also initiates information being stored (e.g., in the storage unit <b>404</b>) that represents the corrective shape and the association of the corrective shape with the blendshapes (and possibly other corrective shapes) being adjusted. For example, information representative of the control parameter settings may be stored in the shape database <b>416</b> such that upon the settings being detected at another time (from the sliders <b>202</b>, <b>204</b>), the corrective shape <b>504</b> is retrieved and applied to the combination of blendshapes. Furthermore, the applied geometry of the corrective shape <b>504</b> may be adjusted as the control parameter settings are adjusted. For example, if the happiness control parameter is slightly increased, the corrective shape <b>504</b> is adjusted such that a proportional level of happiness appears in the facial expression <b>506</b>. As such, one or more corrective shapes may be applied and adjusted based upon control parameters associated with two emotions (e.g., surprise and happiness).</p>
<p id="p-0039" num="0038">Referring to <figref idref="DRAWINGS">FIG. 6</figref>, other methodologies and techniques may be applied to reduce geometry interference along with the amount of user interaction needed to set control parameters for adjusting character expressions. For example, one or more facial expressions may be constrained to appear substantially constant for each selected instance. As such, upon returning to a facial expression selected to be invariant, face geometries are returned to reproduce the invariant expression. For example, as shown in step A, the sliders <b>202</b> and <b>204</b> may be fully dialed (to values of one) to produce the expression <b>302</b> of combined emotions of happiness and surprise. As shown in <figref idref="DRAWINGS">FIG. 5</figref>, and shown here in step B, the slider <b>204</b> (associated with happiness) is set to a zero value, thereby returning the animated face to the recognizable expression <b>506</b> (e.g., by applying the corrective shape <b>504</b>), as shown in step B.</p>
<p id="p-0040" num="0039">As mentioned, one or more facial features may be adjusted to alter the expression being represented. For example, features (e.g., the mouth) may be electronically sculpted to redefine the expression of surprise. As illustrated in an expression <b>600</b>, the shape of a feature <b>602</b> is adjusted such that the represented mouth is opened wider for expressing surprise, as shown in step C.</p>
<p id="p-0041" num="0040">In this example, the combined expression of happiness and surprise <b>302</b> (i.e., in which sliders <b>202</b> and <b>204</b> are set to values of one) is selected as an invariant expression. As such, upon returning the sliders to control parameter values of one, the invariant expression is reproduced. For example, in step D, the slider <b>204</b> associated with the happiness control parameter is dialed back to a value of one to return to the invariant expression. However, due to the re-sculpting to produce the surprise facial expression <b>600</b>, additional geometry interferences may appear as the control parameter (associated with slider <b>204</b>) is adjusted back to a value of one. For example, the re-sculpturing may result in an expression <b>604</b> that is not equivalent to the invariant expression <b>302</b>. Due to the geometry interference, an abnormally large and disproportionate mouth <b>606</b> is present in the expression <b>604</b>.</p>
<p id="p-0042" num="0041">To return the expression <b>604</b> to an expression equivalent to the invariant expression <b>302</b>, one more techniques may be implemented. For example, once the expression manager <b>414</b> detects that the expression <b>604</b> differs from the invariant expression <b>302</b>, another corrective shape <b>608</b> (or multiple corrective shapes) may be applied to the expression <b>604</b> to reproduce the invariant expression <b>302</b>. Once produced by the corrective shape manager <b>412</b>, the corrective shape <b>608</b> is applied to the expression <b>604</b> to produce an expression <b>610</b> that is substantially equivalent to the expression <b>302</b> (shown in step E). Typically production and application of the corrective shape (e.g., corrective shape <b>604</b>) or corrective shapes is executed independent of input from a modeler or animator. As such, the production and application of the corrective shape <b>608</b> is controlled by adjusting the expression control parameters associated with the sliders <b>202</b> and <b>204</b> without additional user input and without increasing the amount of user controls needed to adjust the corrective shapes and the facial expressions. However, in some implementations, additional user input is used for corrective element (e.g., corrective shape) production and application.</p>
<p id="p-0043" num="0042">Along with adding one or more corrective shapes to assure returning to an invariant expression, other techniques may be implemented. For example, operations may be initiated for modifying and/or deleting existing corrective shapes individually or in combination. Further, multiple expressions may be selected as invariant expressions. For example, the expression <b>506</b>, which corresponds to the happiness control parameter being set to zero (via slider <b>204</b>) and the surprise control parameter being set to one (via slider <b>202</b>), may be defined as another invariant expression. As such, upon dialing up the control parameters (e.g., happiness control parameter set to zero and surprise control parameter set to one) on the sliders <b>202</b>, <b>204</b>, previously produced corrective shapes (e.g., corrective shape <b>504</b>) or newly created corrective shapes may be used for returning to the invariant expression <b>506</b>. An expression that includes expressions of multiple blendshapes may also be defined as an invariant expression. For example, dialing a value of 0.2 on slider <b>204</b> (i.e., 20% happiness control parameter) and 0.5 on slider <b>202</b> (i.e., 50% surprise control parameter) may be selected as an invariant expression. As such, upon these values being set with the sliders <b>202</b>, <b>204</b>, one or more corrective shapes (if needed) may be applied to ensure the invariant expression is produced.</p>
<p id="p-0044" num="0043">Invariant expressions may also be defined from uniquely created expressions (e.g., electronically sculpted). For example, a modeler may produce one or a series of facial expressions of a character and define one or more control parameters for adjusting the expressions. Upon the control parameters being adjusted to settings that correspond to one of the sculpted expressions, one or more corrective shapes are applied to return the character to the invariant expression.</p>
<p id="p-0045" num="0044">For situations in which multiple invariant expressions are defined, one or more techniques may be implemented for producing expressions that reside between invariant expressions. For example, one or more estimation techniques (e.g., interpolation, a least-squares estimation procedure) may be implemented for adjusting geometries (e.g., vertex positions) and corrective shapes for expressions between two invariant expressions. In some arrangements, selected invariant expressions may represent extreme emotions. For example, one invariant expression may represent a state of extreme happiness of the character and another invariant may be an extreme expression of surprise or other type of emotion. Upon adjusting control parameters, one or more interpolation techniques may be used to calculate a corresponding expression that lies between the two extreme expressions.</p>
<p id="p-0046" num="0045">Referring to <figref idref="DRAWINGS">FIG. 7</figref>, features (or portions) of a facial expression may be selected for independent geometry adjustments. As such, the character development system <b>400</b> allows a feature geometry (e.g., an eyebrow) to be adjusted without causing the geometries of other features to change. In this example, a character expression <b>700</b> includes a right eyebrow <b>702</b> (presented in the figure to the view's left) that may be selected by a user (e.g., a modeler). The facial expression <b>700</b> also includes a left eyebrow <b>704</b> (presented in the figured to the view's right). Based upon the user, the geometry and position of the eyebrow <b>702</b> may be adjusted for providing different facial expressions. For example, as represented in step A, both the right eyebrow <b>702</b> and the left eyebrow <b>704</b> are oriented substantially horizontal and located relatively near the respective left and right eyes. By the user selecting the right eyebrow <b>702</b>, the geometry and position of the eyebrow may be adjusted for creating various expressions. To control movement, a control parameter may be assigned to the right eyebrow (e.g., a slider) for raising and lowering the eyebrow. However, raising and lowering the eyebrow <b>702</b> may effect other portions and features of the expression <b>700</b>. For example, the orientation, geometry and position of the left eyebrow <b>704</b> may be effected by the independent movements of the right eyebrow <b>702</b>. To reduce such interference, corrective shapes may be applied to features and portions of the character face such as the left eyebrow <b>704</b>. As such, a portion of the character face <b>700</b> may be considered invariant and one or more corrective shapes may be implemented by the corrective shape manager <b>412</b> for geometry retention.</p>
<p id="p-0047" num="0046">In this scenario, the right eyebrow <b>702</b> is selected for being moved independently with a parameter control, however, one or more features or portions of the character may be joined to the selected feature (e.g., the eyebrow <b>702</b>). For example, the left eyebrow <b>704</b> may be joined to the right eyebrow <b>702</b>, and together the two features may be assigned to a control parameter for adjusting movement of both features.</p>
<p id="p-0048" num="0047">As shown in step B, by setting the control parameter, the right eyebrow <b>702</b> is illustrated as being raised while remaining substantially horizontal in orientation. Based upon the movement of the right eyebrow <b>702</b>, the left eyebrow <b>704</b> is moved and changes orientation from horizontal to slanted. The movement of the left eyebrow <b>704</b> is detected by the expression manager <b>414</b> and one or more corrective shapes (represented as a corrective shape <b>706</b>) are applied (in step C) by the corrective shape manager <b>412</b> for adjusting the left eyebrow <b>704</b> back to a horizontal orientation and position as shown in step A (to counteract the effects of the right eyebrow <b>704</b> movement).</p>
<p id="p-0049" num="0048">Based upon the application of the corrective shape <b>706</b>, upon moving the right eyebrow <b>702</b>, portions of the expression <b>700</b> may be effected. For example, as illustrated in step D, upon returning the right eyebrow <b>702</b> to the original position (shown in step A), the left eyebrow <b>704</b> is again moved from the horizontal orientation to a slanted orientation. However, by defining the expression <b>700</b> shown in step A as invariant, the expression manager <b>414</b> detects the movement of eyebrow <b>704</b> and returns the eyebrow <b>704</b> to an orientation to reproduce the invariant expression. To provide this adjustment, another corrective shape <b>708</b> is applied to the character face <b>700</b> such that the expression shown in step E is equivalent to the expression <b>700</b> of step A. However, in some situations, other corrective shape adjustments may be executed, for example, with or without adding another corrective shape, one or more corrective shapes may be deleted or adjusted individually or in combination. As such, individual features and portions of a character face may be moved independently or jointly (e.g., by assigning one or more control parameters) to reduce the amount of user input needed to expression adjustments. Furthermore, by defining one or more invariant expressions or invariant features of a character face, adjustments may be implemented (e.g., adding one or more corrective shapes) to appropriately return the character to the proper invariant expression.</p>
<p id="p-0050" num="0049">Referring to <figref idref="DRAWINGS">FIG. 8</figref>, exemplary interactions among the expression manager <b>414</b>, the corrective shape manager <b>412</b> and the blendshape manager <b>410</b> demonstrate the reduced user interaction needed for adjusting facial expressions of an animated character. In this example, two blendshapes are combined, in which each blendshape represents ranges of different emotions. One blendshape provides a range of expressions associated with the emotion of happiness and the other blendshape provides expressions associated with the emotion of surprise. To select among the expressions, sliders <b>800</b>, <b>802</b> are respectively assigned to each blendshape by the expression manager <b>414</b>. Similar to the sliders <b>202</b> and <b>204</b> (shown in <figref idref="DRAWINGS">FIG. 6</figref>), a user may adjust the sliders <b>800</b>, <b>802</b> individually or in combination for dialing up expressions of interest. As such, a user (e.g., a modeler, an animator, etc.) can select expressions without being aware of the creation and adjustments of corrective shapes to adjust expressions. Further, by one or more expressions being identified as invariant, corrective shapes may be created and adjusted (without user awareness) such that the invariant expressions are consistently recreated upon being selected (e.g., by dialing the appropriate slider or sliders). In this arrangement data from the shape database <b>416</b> identifies one or more invariant expressions (or invariant facial features) to the expression manager <b>414</b>.</p>
<p id="p-0051" num="0050">Upon the sliders <b>800</b>, <b>802</b> being set for selecting an expression from the range of emotions, data representative of the control parameters is sent to the blendshape manager <b>410</b> for retrieving the appropriate blendshape or blendshapes and adjusting corresponding geometries to present the emotional expression of interest. In this example, due to the selected control parameter values (e.g., a values of one for both the happiness control parameter and the surprise control parameter), the blendshapes <b>100</b> and <b>106</b> are retrieved from the storage device <b>404</b> and respectively adjusted to produce the expressions <b>104</b> and <b>110</b>. Additionally, the blendshape manager <b>414</b> combines the expressions <b>104</b>, <b>110</b> to produce the expression of interest. Similar to the example shown in <figref idref="DRAWINGS">FIG. 6</figref>, this particular combination for blendshapes <b>104</b>, <b>110</b> has been defined as an invariant expression and is thereby actively adjusted to return to the original expression.</p>
<p id="p-0052" num="0051">Upon detecting that the selected values of the control parameters (as provided by the sliders <b>800</b>, <b>802</b>), the expression manager <b>414</b> provides data to the corrective shape manager <b>412</b> to initiate the creation of one or more appropriate corrective shapes for being applied to the combination of blendshape expressions <b>104</b> and <b>110</b>. In this particular example the corrective shape <b>300</b> is produced by the corrective shape manager <b>412</b> (e.g., created, retrieved from the storage device <b>404</b>, etc.) and applied to the combination of blendshape expressions to produce the invariant expression <b>302</b>. As such, by operating just two control parameters (via the sliders <b>800</b>, <b>802</b>), blendshape expressions are selected and combined. Additionally, a corrective shape is produced and applied to the combined expressions to provide an appropriate and realistic expression.</p>
<p id="p-0053" num="0052">Referring to <figref idref="DRAWINGS">FIG. 9</figref>, a flowchart <b>900</b> represents some of the operations of the expression manager <b>414</b>, the blendshape manager <b>410</b> and a corrective element manager (such as the corrective shape manager <b>412</b>). The operations may be executed by a single computer system (e.g., computer system <b>402</b>) or multiple computing devices. Along with being executed at a single site (e.g., at one computer system), operation execution may be distributed among two or more sites.</p>
<p id="p-0054" num="0053">Operations include receiving <b>902</b> one or more values representative of one or more corresponding control parameters. For example, interface devices (e.g., sliders <b>800</b>, <b>802</b>) may be used for selecting values that represent facial expressions within a range of expressions representing an emotion. Rather than emotions, the control parameters values may represent the position, orientation and movement of muscles or muscle groups, facial features (e.g., character mouth, nose, etc.) or other portions of an animated character (e.g., surfaces such as skin, flesh, clothing, etc.). Operations also include receiving <b>904</b> blendshapes (e.g., from the storage device <b>404</b>) and combining <b>906</b> the blendshapes based upon the values of the control parameters. In some implementations, blendshape retrieval and combining is provided by the blendshape manager <b>410</b>, however, such operations may be executed by the expression manager <b>414</b> or the corrective shape manager <b>412</b> individually or in any combination with the blendshape manager <b>410</b>.</p>
<p id="p-0055" num="0054">Operations also include determining <b>908</b> if the selected expression (as provided by the control parameter values) is associated with a previously defined invariant expression such as illustrated with expression <b>302</b> in <figref idref="DRAWINGS">FIG. 6</figref>. If identified as invariant, operations include adjusting <b>910</b> elements such as geometries such as blendshapes and facial features to return the character facial expression to the invariant expression. Interpolation techniques may also be implemented for geometry adjustments. For example, based upon the selected control parameter values, an expression may be interpolated from one or more invariant expressions.</p>
<p id="p-0056" num="0055">Upon adjusting geometries to account for one or more invariant expressions, or if not needing to account for an invariant expression, operations include determining <b>912</b> if application of one or more corrective elements (e.g., corrective shapes) is needed. For example, based upon the control parameter values, one or more instances of blendshape interference may occur and call for at least one corrective shape being applied. Operations also include determining <b>914</b> if one or more corrective elements (e.g., corrective shapes) need to be created to reduce blendshape interference or whether the current corrective element or elements may be used and accordingly adjusted to reduce interference. If needed, operations include producing <b>916</b> one or more corrective elements (e.g., corrective shapes) to substantially reduce interference. Upon producing additional corrective elements or determining that additional corrective elements are not needed, operations include applying <b>918</b> the corrective elements (e.g., corrective shapes). Generally, the corrective shape manager <b>412</b> executes operations for producing and applying corrective shapes, however, in some implementations such operations may be executed individually or in combination with the expression manager <b>414</b>, the blendshape manager <b>410</b> or other type of process.</p>
<p id="p-0057" num="0056">To perform the operations described in flow chart <b>900</b>, the expression manager <b>414</b>, the blendshape manager <b>410</b> and the corrective shape manager <b>412</b>, individually or in combination, may perform any of the computer-implement methods described previously, according to one implementation. For example, a computer system such as computer system <b>402</b> (shown in <figref idref="DRAWINGS">FIG. 4</figref>) may execute the expression manager <b>414</b>. The computer system may include a processor (not shown), a memory (not shown), a storage device (e.g., storage device <b>404</b>), and an input/output device (not shown). Each of the components may be interconnected using a system bus or other similar structure. The processor is capable of processing instructions for execution within the computer system. In one implementation, the processor is a single-threaded processor. In another implementation, the processor is a multi-threaded processor. The processor is capable of processing instructions stored in the memory or on the storage device to display graphical information for a user interface on the input/output device.</p>
<p id="p-0058" num="0057">The memory stores information within the computer system. In one implementation, the memory is a computer-readable medium. In one implementation, the memory is a volatile memory unit. In another implementation, the memory is a non-volatile memory unit.</p>
<p id="p-0059" num="0058">The storage device is capable of providing mass storage for the computer system. In one implementation, the storage device is a computer-readable medium. In various different implementations, the storage device may be a floppy disk device, a hard disk device, an optical disk device, or a tape device.</p>
<p id="p-0060" num="0059">The input/output device provides input/output operations for the computer system. In one implementation, the input/output device includes a keyboard and/or pointing device. In another implementation, the input/output device includes a display unit for displaying graphical user interfaces.</p>
<p id="p-0061" num="0060">The features described can be implemented in digital electronic circuitry, or in computer hardware, firmware, software, or in combinations of them. The apparatus can be implemented in a computer program product tangibly embodied in an information carrier, e.g., in a machine-readable storage device or in a propagated signal, for execution by a programmable processor; and method steps can be performed by a programmable processor executing a program of instructions to perform functions of the described implementations by operating on input data and generating output. The described features can be implemented advantageously in one or more computer programs that are executable on a programmable system including at least one programmable processor coupled to receive data and instructions from, and to transmit data and instructions to, a data storage system, at least one input device, and at least one output device. A computer program is a set of instructions that can be used, directly or indirectly, in a computer to perform a certain activity or bring about a certain result. A computer program can be written in any form of programming language, including compiled or interpreted languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.</p>
<p id="p-0062" num="0061">Suitable processors for the execution of a program of instructions include, by way of example, both general and special purpose microprocessors, and the sole processor or one of multiple processors of any kind of computer. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for executing instructions and one or more memories for storing instructions and data. Generally, a computer will also include, or be operatively coupled to communicate with, one or more mass storage devices for storing data files; such devices include magnetic disks, such as internal hard disks and removable disks; magneto-optical disks; and optical disks. Storage devices suitable for tangibly embodying computer program instructions and data include all forms of non-volatile memory, including by way of example semiconductor memory devices, such as EPROM, EEPROM, and flash memory devices; magnetic disks such as internal hard disks and removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, ASICs (application-specific integrated circuits).</p>
<p id="p-0063" num="0062">To provide for interaction with a user, the features can be implemented on a computer having a display device such as a CRT (cathode ray tube) or LCD (liquid crystal display) monitor for displaying information to the user and a keyboard and a pointing device such as a mouse or a trackball by which the user can provide input to the computer.</p>
<p id="p-0064" num="0063">The features can be implemented in a computer system that includes a back-end component, such as a data server, or that includes a middleware component, such as an application server or an Internet server, or that includes a front-end component, such as a client computer having a graphical user interface or an Internet browser, or any combination of them. The components of the system can be connected by any form or medium of digital data communication such as a communication network. Examples of communication networks include, e.g., a LAN, a WAN, and the computers and networks forming the Internet.</p>
<p id="p-0065" num="0064">The computer system can include clients and servers. A client and server are generally remote from each other and typically interact through a network, such as the described one. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.</p>
<p id="p-0066" num="0065">A number of implementations have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the following claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A computer-implemented method comprising:
<claim-text>combining a first deformable geometry of a portion of an animated character and a second deformable geometry of another portion of the animated character to form a combined geometry of the animated character, wherein a first control parameter is capable of adjusting the first deformable geometry and a second control parameter is capable of adjusting the second deformable geometry; and</claim-text>
<claim-text>applying at least a first corrective geometry, different than the first deformable geometry and second deformable geometry, to the combined geometry to correct for correlated geometries in the first and second deformable geometries.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein applying the first corrective geometry includes producing the first corrective geometry to adjust the combined first deformable geometry and second deformable geometry.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein applying the first corrective geometry includes adjusting the geometry of the animated character.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising adjusting a non-geometrical feature of the animated character.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein applying the first corrective geometry includes returning the animated character to a predefined facial expression.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein applying the first corrective geometry includes applying the first corrective shape to a first feature of the combined first and second deformable geometry independent of a second feature of the combined first and second deformable geometry.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>applying at least a second corrective geometry to the combined first deformable geometry and second deformable geometry based upon at least one of the first control parameter and the second control parameter.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The computer-implemented method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein applying the at least second corrective geometry includes returning the animated character to a predefined facial expression.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein values of the first control parameter are representative of a range of facial expressions of the animated character.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein values of the first control parameter are representative of a range of muscular movements of the animated character.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein values of the first control parameter are representative of a range of simulated movements of a surface of the animated character.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein values of the first control parameter are representative of a range of movements of at least one joint of the animated character.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein applying the first corrective geometry to the combined first deformable geometry and second deformable geometry is based upon user input.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. A system comprising:
<claim-text>a computing device comprising:</claim-text>
<claim-text>a memory configured to store instructions; and</claim-text>
<claim-text>a processor to execute the instructions to perform operations comprising:
<claim-text>combining a first deformable geometry of a portion of an animated character and a second deformable geometry of another portion of the animated character to form a combined geometry of the animated character, wherein a first control parameter is capable of adjusting the first deformable geometry and a second control parameter is capable of adjusting the second deformable geometry; and</claim-text>
<claim-text>applying at least a first corrective geometry, different than the first deformable geometry and second deformable geometry to correct for correlated geometries in the first and second deformable geometries.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the corrective element manager is configured to produce at least one corrective geometry to adjust the combined first deformable geometry and second deformable geometry.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the first corrective geometry is configured to adjust the geometry of the animated character.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the first corrective geometry is configured to adjust a non-geometrical feature of the animated character.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the first corrective geometry returns the animated character to a predefined facial expression.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the corrective element manager is configured to apply the first corrective geometry to a first feature of the combined first and second deformable geometry independent of a second feature of the combined first and second deformable geometry.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the corrective element manager is configured to apply at least a second corrective geometry to the combined first deformable geometry and second deformable geometry based upon at least one of the first control parameter and the second control parameter.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The system of <claim-ref idref="CLM-00020">claim 20</claim-ref>, wherein application of the at least second corrective geometry returns the animated character to a predefined facial expression.</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein values of the first control parameter are representative of a range of facial expressions of the animated character.</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein values of the first control parameter are representative of a range of muscular movements of the animated character.</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein values of the first control parameter are representative of a range of simulated movements of a surface of the animated character.</claim-text>
</claim>
<claim id="CLM-00025" num="00025">
<claim-text>25. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein values of the first control parameter are representative of a range of movements of at least one joint of the animated character.</claim-text>
</claim>
<claim id="CLM-00026" num="00026">
<claim-text>26. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the corrective element manager is configured to receive user input for applying the first corrective element.</claim-text>
</claim>
<claim id="CLM-00027" num="00027">
<claim-text>27. A computer program product embodied in a non-transitory computer-readable medium and comprising instructions that when executed by a processor perform operations comprising:
<claim-text>combining a first deformable geometry of a portion of an animated character and a second deformable geometry of another portion of the animated character to form a combined geometry of the animated character, wherein a first control parameter is capable of adjusting the first deformable geometry and a second control parameter is capable of adjusting the second deformable geometry; and</claim-text>
<claim-text>applying at least a first corrective geometry, different than the combined first deformable geometry and second deformable geometry, to correct for correlated geometries in the first and second deformable geometries.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00028" num="00028">
<claim-text>28. The computer program product of <claim-ref idref="CLM-00027">claim 27</claim-ref>, wherein applying the first corrective geometry includes producing the first corrective geometry to adjust the combined first deformable geometry and second deformable geometry.</claim-text>
</claim>
<claim id="CLM-00029" num="00029">
<claim-text>29. The computer program product of <claim-ref idref="CLM-00027">claim 27</claim-ref>, wherein applying the first corrective geometry includes adjusting the geometry of the animated character.</claim-text>
</claim>
<claim id="CLM-00030" num="00030">
<claim-text>30. The computer program product of <claim-ref idref="CLM-00027">claim 27</claim-ref>, wherein applying the first corrective geometry includes adjusting a non-geometrical feature of the animated character.</claim-text>
</claim>
<claim id="CLM-00031" num="00031">
<claim-text>31. The computer program product of <claim-ref idref="CLM-00027">claim 27</claim-ref>, wherein applying the first corrective geometry includes returning the animated character to a predefined facial expression.</claim-text>
</claim>
<claim id="CLM-00032" num="00032">
<claim-text>32. The computer program product of <claim-ref idref="CLM-00027">claim 27</claim-ref>, wherein applying the first corrective geometry includes applying the first corrective shape to a first feature of the combined first and second deformable geometry independent of a second feature of the combined first and second deformable geometry.</claim-text>
</claim>
<claim id="CLM-00033" num="00033">
<claim-text>33. The computer program product of <claim-ref idref="CLM-00027">claim 27</claim-ref>, wherein the method further comprises:
<claim-text>applying at least a second corrective geometry to the combined first deformable geometry and second deformable geometry based upon at least one of the first control parameter and the second control parameter.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00034" num="00034">
<claim-text>34. The computer program product of <claim-ref idref="CLM-00033">claim 33</claim-ref>, wherein applying the at least second corrective geometry includes returning the animated character to a predefined facial expression.</claim-text>
</claim>
<claim id="CLM-00035" num="00035">
<claim-text>35. The computer program product of <claim-ref idref="CLM-00027">claim 27</claim-ref>, wherein values of the first control parameter are representative of a range of facial expressions of the animated character.</claim-text>
</claim>
<claim id="CLM-00036" num="00036">
<claim-text>36. The computer program product of <claim-ref idref="CLM-00027">claim 27</claim-ref>, wherein values of the first control parameter are representative of a range of muscular movements of the animated character.</claim-text>
</claim>
<claim id="CLM-00037" num="00037">
<claim-text>37. The computer program product of <claim-ref idref="CLM-00027">claim 27</claim-ref>, wherein values of the first control parameter are representative of a range of simulated movements of a surface of the animated character.</claim-text>
</claim>
<claim id="CLM-00038" num="00038">
<claim-text>38. The computer program product of <claim-ref idref="CLM-00027">claim 27</claim-ref>, wherein values of the first control parameter are representative of a range of movements of at least one joint of the animated character.</claim-text>
</claim>
<claim id="CLM-00039" num="00039">
<claim-text>39. The computer program product of <claim-ref idref="CLM-00027">claim 27</claim-ref>, wherein applying the first corrective geometry to the combined first deformable geometry and second deformable geometry is based upon user input.</claim-text>
</claim>
</claims>
</us-patent-grant>
