<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625850-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625850</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13472788</doc-number>
<date>20120516</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>JP</country>
<doc-number>2011-112005</doc-number>
<date>20110519</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>55</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382103</main-classification>
<further-classification>382104</further-classification>
<further-classification>382106</further-classification>
<further-classification>356  3</further-classification>
<further-classification>356 11</further-classification>
<further-classification>348118</further-classification>
<further-classification>348135</further-classification>
<further-classification>348148</further-classification>
<further-classification>348169</further-classification>
</classification-national>
<invention-title id="d2e71">Environment recognition device and environment recognition method</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5530420</doc-number>
<kind>A</kind>
<name>Tsuchiya et al.</name>
<date>19960600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>340435</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6088469</doc-number>
<kind>A</kind>
<name>Fukumura et al.</name>
<date>20000700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6138062</doc-number>
<kind>A</kind>
<name>Usami</name>
<date>20001000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>701 23</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6204754</doc-number>
<kind>B1</kind>
<name>Berstis</name>
<date>20010300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>340435</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6370261</doc-number>
<kind>B1</kind>
<name>Hanawa</name>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382104</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6453055</doc-number>
<kind>B1</kind>
<name>Fukumura et al.</name>
<date>20020900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6888953</doc-number>
<kind>B2</kind>
<name>Hanawa</name>
<date>20050500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382104</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>7266220</doc-number>
<kind>B2</kind>
<name>Sato et al.</name>
<date>20070900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382104</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>7640107</doc-number>
<kind>B2</kind>
<name>Shimizu et al.</name>
<date>20091200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>701523</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2003/0099377</doc-number>
<kind>A1</kind>
<name>Hanawa</name>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382104</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2003/0210807</doc-number>
<kind>A1</kind>
<name>Sato et al.</name>
<date>20031100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382104</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2005/0099821</doc-number>
<kind>A1</kind>
<name>Potter et al.</name>
<date>20050500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>362548</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2008/0266396</doc-number>
<kind>A1</kind>
<name>Stein</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348148</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2012/0106786</doc-number>
<kind>A1</kind>
<name>Shiraishi et al.</name>
<date>20120500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2012/0294485</doc-number>
<kind>A1</kind>
<name>Kasaoki</name>
<date>20121100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>JP</country>
<doc-number>2008-59323</doc-number>
<kind>A</kind>
<date>20080300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>11</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382103</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382104</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382106</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>356  3</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>356 11</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348118-120</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348135-142</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348148</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348169</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>14</number-of-drawing-sheets>
<number-of-figures>29</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120294485</doc-number>
<kind>A1</kind>
<date>20121122</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Kasaoki</last-name>
<first-name>Seisuke</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Kasaoki</last-name>
<first-name>Seisuke</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>McDermott Will &#x26; Emery LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Fuji Jukogyo Kabushiki Kaisha</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Carter</last-name>
<first-name>Aaron W</first-name>
<department>2665</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">There are provided an environment recognition device and an environment recognition method. The device obtains position information of a target portion in a detection area, including a relative distance from a subject vehicle; groups continuous target portions into a target object of which position differences in a width direction vertical to an advancing direction of the vehicle and in a depth direction parallel to the advancing direction fall within a first distance; determines that the target object is a candidate of a wall, when the target portions forming the target object forms a tilt surface tilting at a predetermined angle or more with respect to a plane vertical to the advancing direction; and determines that the continuous wall candidates of which position differences in the width and depth directions among the wall candidates fall within a second predetermined distance longer than the first predetermined distance are a wall.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="270.17mm" wi="212.51mm" file="US08625850-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="180.68mm" wi="207.43mm" file="US08625850-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="177.21mm" wi="167.81mm" file="US08625850-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="195.50mm" wi="210.90mm" file="US08625850-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="136.06mm" wi="199.56mm" file="US08625850-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="266.19mm" wi="177.21mm" file="US08625850-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="166.54mm" wi="207.09mm" file="US08625850-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="270.59mm" wi="213.36mm" file="US08625850-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="265.26mm" wi="209.89mm" file="US08625850-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="256.46mm" wi="210.23mm" file="US08625850-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="206.16mm" wi="194.82mm" file="US08625850-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="165.61mm" wi="153.08mm" file="US08625850-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="196.77mm" wi="186.69mm" file="US08625850-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="268.99mm" wi="212.77mm" file="US08625850-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="262.38mm" wi="180.42mm" file="US08625850-20140107-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">The present application claims priority from Japanese Patent Application No. 2011-112005 filed on May 19, 2011, the entire contents of which are hereby incorporated by reference.</p>
<heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">1. Field of the Invention</p>
<p id="p-0004" num="0003">The present invention relates to an environment recognition device and an environment recognition method for recognizing a target object based on luminances of the target object in a detection area.</p>
<p id="p-0005" num="0004">2. Description of Related Art</p>
<p id="p-0006" num="0005">Conventionally, a technique has been known that detects a target object such as an obstacle including a vehicle and a traffic light located in front of a subject vehicle for performing control to avoid collision with the detected target object and to maintain a safe distance between the subject vehicle and the preceding vehicle.</p>
<p id="p-0007" num="0006">In this technique, stereo matching processing is performed an image of an environment ahead of a vehicle captured by two cameras, for example, in order to derive parallax for each block including a predetermined number of pixels. Based on this parallax, a position of the vehicle in thereat world is derived.</p>
<p id="p-0008" num="0007">There is also a technique that performs more advanced control. Specifically, it not only specifies a target object uniformly as a solid object, but further determines whether the detected target object is a preceding vehicle that is running at the same speed as the subject vehicle or a fixed object that does not move. For example, there has been proposed a technique in which a point detected as being tilted at a predetermined angle with respect to the advancing direction of the subject vehicle is determined as a wall such as a guard rail, while a point detected along the horizontal direction of a plane vertical to the advancing direction of the subject vehicle is determined as a back surface of a preceding vehicle (for example, Japanese Patent. Application Laid-Open (JP-A) No. 2008-59323).</p>
<p id="p-0009" num="0008">However, for example, when light of a tail lamp is diffused at night or in rainy weather, a block that does not indicate the tail lamp might erroneously be matched with a block indicating the tail lamp in the stereo matching process. In this case, the position of the tail lamp, which is derived from the mismatching, in the real world is recognized as being tilted at a predetermined angle with respect to the advancing direction of the subject vehicle. By virtue of this, the portion that are supposed be determined to be the back surface of the preceding vehicle might be determined to be a wall in the technique described in JP-A No. 2008-59323. When an object determined to be a wall is detected in the advancing direction, automatic control for performing an avoiding operation, which is originally unnecessary, might be executed.</p>
<heading id="h-0003" level="1">BRIEF SUMMARY OF THE INVENTION</heading>
<p id="p-0010" num="0009">The present invention is accomplished in view of the circumstance described above, and aims to provide an environment recognition device and an environment recognition method that are capable of preventing an object that is not a wall from being erroneously determined to be a wall in an environment where light is easily diffused, such as at night or in rainy weather.</p>
<p id="p-0011" num="0010">In order to solve the above problems, an aspect of the present invention provides an environment recognition device that includes: a position information obtaining unit that obtains position information of a target portion in a detection area of a luminance image, including a relative distance from a subject vehicle; a grouping unit that groups target portions, of which positions differences in a vertical direction and a parallel direction with respect to a base line which corresponds to advancing direction of the subject vehicle fall within a first distance, into a target object; a candidate determining unit that determines that the target object is a candidate of a wall, when the target portions included in the target object form a tilt surface tilting at a predetermined angle or more against a vertical plane with respect to a base plane which corresponds to the advancing direction of the subject vehicle; and a wall determining unit that determines that the wall candidates, of which positions differences in the vertical direction and the parallel direction with respect to a base line which corresponds to the advancing direction of the subject vehicle fall within a second predetermined distance longer than the first predetermined distance, are a wall.</p>
<p id="p-0012" num="0011">The grouping unit may make a determination for the grouping based on not only the position differences in the vertical direction and the parallel direction with respect to a base line which corresponds to the advancing direction of the subject vehicle but also a position difference in the height direction from a road surface, and the wall determining unit may make a determination of a wall based on not only the position differences in the vertical direction and in the parallel direction with respect to a base line which corresponds to the is advancing direction of the vehicle but also the position difference in the height direction from the road surface.</p>
<p id="p-0013" num="0012">The environment recognition device may further include an environment determining unit that determines whether or not the detection area is in an environment where light is diffused. The wall determining unit may determine all of the wall candidates as a wall when the environment determining unit determines that the detection area is in the environment where light is not diffused.</p>
<p id="p-0014" num="0013">The environment recognition device may further include a distance deriving unit that derives each average of in-line relative distances, which are assigned to target portions included in a target obi act and which are aligned in a horizontal direction of the image corresponding to the width direction in the real world; and a group dividing unit that divides the target object at a position where differences of the averages in the vertical direction of the image corresponding to the height direction in the real world exceeds the predetermined threshold value, and defines the position comprised of the horizontally aligned target portions as a boundary of the divided target objects.</p>
<p id="p-0015" num="0014">The group dividing unit may determine the predetermined threshold value based on the average of the relative distances of all target portions included in the target object.</p>
<p id="p-0016" num="0015">In order to solve the above problems, another aspect of the present invention provides an environment recognition method that includes: obtaining position information of a target portion in a detection area of a luminance image including a relative distance from a subject vehicle;</p>
<p id="p-0017" num="0016">grouping target portions, of which positions differences in a vertical direction and a parallel direction with respect to a base line which corresponds to advancing direction of the subject vehicle fail within a first predetermined distance, into a target object; determining that the target object is a candidate of a wall, when the target portions included in the target object form a tilt surface tilting at a predetermined angle or more against a vertical plane with respect to a base plane which corresponds to the advancing direction of the subject vehicle; determining that determines that the wall candidates, of which positions differences in the vertical direction and the parallel direction with respect to a base line which corresponds to the advancing direction of the subject vehicle fall within a second predetermined distance longer than the first predetermined distance, are a wall.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE SEVERAL VIEWS OF THE DRAWINGS</heading>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram illustrating a connection relationship in an environment recognition system;</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIGS. 2A</figref>, and <b>2</b>B are explanatory diagrams for explaining a luminance image and a distance image;</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 3</figref> is a functional block diagram schematically illustrating functions of an environment recognition device;</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 4</figref> is an explanatory diagram for explaining conversion into three-dimensional position information is performed by a position information obtaining unit;</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIGS. 5A to 5D</figref> are explanatory diagrams for explaining processing of a grouping unit;</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 6</figref> is an explanatory diagram for explaining one example of a luminance image at night;</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIGS. 7A to 7D</figref> are explanatory diagrams for explaining wall determining processing;</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIGS. 8A to 8D</figref> are explanatory diagrams for explaining group dividing processing;</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIGS. 9A to 9C</figref> are explanatory diagrams for explaining the group dividing processing;</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIGS. 10A and 10B</figref> are explanatory diagrams for explaining the group dividing processing;</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 11</figref> is a flowchart illustrating an overall flow of an environment recognition method;</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 12</figref> is a flowchart illustrating a flow of grouping processing;</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 13</figref> is a flowchart illustrating a flow of wall determining processing; and</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 14</figref> is a flowchart illustrating a flow of group dividing processing.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION OF THE INVENTION</heading>
<p id="p-0032" num="0031">A preferred embodiment of the present invention will be hereinafter explained in detail with reference to attached drawings. The size, materials, and other specific numerical values shown in the embodiment are merely exemplification for the sake of easy understanding of the invention, and unless otherwise specified, they do not limit the present invention. In the specification and the drawings, elements having substantially same functions and configurations are denoted with same reference numerals, and repeated explanation thereabout is omitted. Elements not directly related, to the present invention are omitted in the drawings.</p>
<p id="p-0033" num="0032">(Environment Recognition System <b>100</b>)</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 1</figref> is, a block diagram illustrating connection relationship in an environment recognition system <b>100</b>. The environment recognition system <b>100</b> includes a plurality of image capturing devices <b>110</b> (two image capturing devices <b>110</b> in the present embodiment), an image processing device <b>120</b>, an environment recognition device <b>130</b>, and a vehicle control device <b>140</b> that are provided in a subject vehicle <b>1</b>.</p>
<p id="p-0035" num="0034">The image capturing devices <b>110</b> include an imaging element such as a CCD (Charge-Coupled Device) and a CMOS (Complementary Metal-Oxide Semiconductor), and can obtain a color image, that is, luminances of three color phases (red, green, blue) per pixel. In the present embodiment, color and luminance are dealt in the same way; if both wordings are included in one sentence, both can be read as luminance configuring color, or color having luminances. In this case, a color image captured by the image capturing devices <b>110</b> is referred to as luminance image and is distinguished from a distance image to be explained later. The image capturing devices <b>110</b> are disposed to be spaced apart from each other in a substantially horizontal direction so that optical axes of the two image capturing devices <b>110</b> are substantially parallel in a proceeding direction of the vehicle <b>1</b>. The image capturing devices <b>110</b> continuously generates image data obtained by capturing an image of a target object existing in a detection area in front of the vehicle <b>1</b> at every 1/60 seconds (60 fps) for example. In this case, the target object may be not only an independent three-dimensional object such as a vehicle, a traffic light, a road, and a guardrail, but also an illuminating portion such as a tail lamp, a turn signal, a traffic light that can be specified as a portion of a three-dimensional object. Each later-described functional unit in the embodiment performs processing in response to the update of such image data.</p>
<p id="p-0036" num="0035">The image processing device <b>120</b> obtains image data from each of the two image capturing devices <b>110</b>, and derives, based on the two pieces of image data, parallax information including a parallax of any block (a set of a predetermined number of pixels) in the image and a position representing a position of the any block in the image. Specifically, the image processing device <b>120</b> derives a parallax using so-called pattern matching that searches a block in one of the image data corresponding to the block optionally extracted from the other image data. The block is, for example, an array including four pixels in the horizontal direction and four pixels in the vertical direction. In this embodiment, the horizontal direction means a horizontal direction for the captured image, and corresponds to the width direction in the real world. On the other hand, the vertical direction means a vertical direction for the captured image, and corresponds to the height direction in the real world.</p>
<p id="p-0037" num="0036">One way of performing the pattern matching is to compare luminance values (Y color difference signals) between two image data by the block indicating any image position. Examples include an SAD (Sum of Absolute Difference) obtaining a difference of luminance values, an SSD (Sum of Squared intensity Difference) squaring a difference, and an NCC (Normalized Cross Correlation) adopting the degree of similarity of dispersion values obtained by subtracting a mean luminance value from a luminance value of each pixel. The image processing device <b>120</b> performs such parallax deriving processing on all the blocks appearing in the detection area (for example, 600 pixels&#xd7;200 pixels). In this case, the block is assumed to include 4 pixels&#xd7;4 pixels, but the number of pixels in the block may be set at any value.</p>
<p id="p-0038" num="0037">Although the image processing device <b>120</b> can derive a parallax for each block serving as a detection resolution unit, it is impossible to recognize what kind of target object the block belongs to. Therefore, the parallax information is not derived by the target object, but is independently derived by the resolution (for example, by the block) in the detection area In this embodiment, an image obtained by associating the parallax information thus derived (corresponding to a later-described relative distance) with image data is referred to as a distance image.</p>
<p id="p-0039" num="0038"><figref idref="DRAWINGS">FIGS. 2A and 2B</figref> are explanatory diagrams for explaining a luminance image <b>124</b> and a distance image <b>126</b>. For example, Assume that the luminance image (image data) <b>124</b> as shown in <figref idref="DRAWINGS">FIG. 2A</figref> is generated regard to a detection area <b>122</b> by the two image capturing devices <b>110</b>. Here, for the sake of easy understanding, only one of the two luminance images <b>124</b> is schematically shown. The image processing device <b>120</b> obtains a parallax for each block from such luminance image <b>124</b>, and forms the distance image <b>126</b> as shown in <figref idref="DRAWINGS">FIG. 2B</figref>. Each block of the distance image <b>126</b> is associated with a parallax of the block. In the drawing, for the sake of explanation, a block from which a parallax is derived is indicated by a black dot.</p>
<p id="p-0040" num="0039">The parallax can be easily specified at the edge portion (portion where there is contrast between adjacent pixels) of objects, and therefore, the block from which parallax is derived, which is denoted with black dots in the distance image <b>126</b>, is likely to also be an edge in the luminance image <b>124</b>. Therefore, the luminance image <b>124</b> as shown in <figref idref="DRAWINGS">FIG. 2A</figref> and the distance image <b>126</b> as shown in <figref idref="DRAWINGS">FIG. 2B</figref> are similar in terms of outline of each target object.</p>
<p id="p-0041" num="0040">The environment recognition device <b>130</b> uses a so-called stereo method to convert the parallax information for each block in the detection area <b>122</b> (distance image <b>126</b>) derived by the image processing device <b>120</b> into three-dimensional position information including a relative distance, thereby deriving heights. The stereo method is a method using a triangulation method to derive a relative distance of a target object with respect to the image capturing device <b>110</b> from the parallax of the target object. The environment recognition device <b>130</b> will be explained later in detail.</p>
<p id="p-0042" num="0041">The vehicle control device <b>140</b> avoids a collision with the target object specified by the environment recognition device <b>130</b> and performs control so as to maintain a safe distance from the preceding vehicle. More specifically, the vehicle control device <b>140</b> obtains a current cruising state of the subject vehicle <b>1</b> based on, for example, a steering angle sensor <b>142</b> for detecting an angle of the steering and a vehicle speed sensor <b>144</b> for detecting a speed of the subject vehicle <b>1</b>, thereby controlling an actuator <b>146</b> to maintain a safe distance from the preceding vehicle. The actuator <b>146</b> is an actuator for vehicle control used to control a brake, a throttle valve, a steering angle and the like. When collision with a target object is expected, the vehicle control device <b>140</b> displays a warning (notification) of the expected collision on a display <b>148</b> provided in front of a driver, and controls the actuator <b>146</b> to automatically decelerate the subject vehicle <b>1</b>. The vehicle control device <b>140</b> can also be integrally implemented with the environment recognition device <b>130</b>.</p>
<p id="p-0043" num="0042">(Environment Recognition Device <b>130</b>)</p>
<p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. 3</figref> is a functional block diagram schematically illustrating functions of an environment recognition device <b>130</b>. As shown in <figref idref="DRAWINGS">FIG. 3</figref>, the environment recognition device <b>130</b> includes an I/F unit <b>150</b>, a data retaining unit <b>152</b>, an environment detecting unit <b>154</b>, and a central control unit <b>156</b>.</p>
<p id="p-0045" num="0044">The I/F unit <b>150</b> is an interface for interactive information exchange with she image processing device <b>120</b> and the vehicle control device <b>140</b>. The data retaining unit <b>152</b> is constituted by a RAM, a flash memory, an HDD and the like, and retains various kinds of information required for processing performed by each functional unit explained below. In addition, the data retaining unit <b>152</b> temporarily retains the luminance image <b>124</b> and the distance image <b>126</b> received from the image processing device <b>120</b>.</p>
<p id="p-0046" num="0045">The environment detecting unit <b>154</b> includes, for example, a night detecting unit that detects nighttime and a rain detecting unit that detects rainfall. When detecting nighttime or rainfall, the environment detecting unit <b>154</b> outputs to an environment determining unit <b>168</b> described below detection information indicating the detection of nighttime or rainfall. The environment detecting unit <b>154</b> may detect not only nighttime and rainfall but also an environment where light from a light source in the captured image is diffused, for example, an environment where the image capturing device <b>110</b> captures a backlit image. The night detecting unit may detect nighttime when detecting a lighting of a night lamp of the subject vehicle <b>1</b>, for example. The night detecting unit and the rain detecting unit can be realized by any existing techniques. Therefore, the detailed description such as its configuration will be omitted here.</p>
<p id="p-0047" num="0046">The central control unit <b>156</b> is comprised of a semiconductor integrated circuit including, for example, a central processing unit (CPU), a ROM storing a program and the like, and a RAM serving as a work area, and controls the I/F unit <b>150</b>, the data retaining unit <b>152</b> and the environment detecting unit <b>154</b> through a system bus <b>158</b>. In the present embodiment, the central control unit <b>156</b> also functions as a luminance obtaining unit <b>160</b>, a position information obtaining unit <b>162</b>, a candidate determining unit <b>164</b>, a wall determining unit <b>166</b>, an environment determining unit <b>168</b>, a distance deriving unit <b>170</b>, a group dividing unit <b>172</b> and a pattern matching unit <b>174</b>.</p>
<p id="p-0048" num="0047">The position information obtaining unit <b>160</b> uses the stereo method to convert parallax information, derived by the image processing apparatus <b>120</b>, for each block in the detection area <b>122</b> of the distance image <b>126</b> into three-dimensional position information including the width direction x, the height direction y, and the depth direction z. Here, the target portion is supposed to composed of a pixel or a block formed by collecting pixels. In the present embodiment, the target portion has a size equal to the size of the block used in the image processing device <b>120</b>.</p>
<p id="p-0049" num="0048">The parallax information derived by the image processing device <b>120</b> represents a parallax of each target portion in the distance image <b>126</b>, whereas the three-dimensional position information represents information about the relative distance of each target portion in the real world. Accordingly, a term such as the relative distance and the height refers to a distance in the real world, whereas a term such as a detected distance refers to a distance in the distance image <b>126</b>.</p>
<p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. 4</figref> is an explanatory diagram for explaining conversion into three-dimensional position information by the position information obtaining unit <b>160</b>. First, the position information obtaining unit <b>160</b> treats the distance image <b>126</b> as a coordinate system in a pixel unit as shown in <figref idref="DRAWINGS">FIG. 4</figref>. In <figref idref="DRAWINGS">FIG. 4</figref>, the lower left corner is adopted as an origin (0, 0). The horizontal direction is adopted as an i coordinate axis, and the vertical direction is adopted as a j coordinate axis. Therefore, a pixel having a parallax dp can be represented as (i, j, dp) using a pixel position i, j and the parallax dp.</p>
<p id="p-0051" num="0050">The three-dimensional coordinate system in the real world according to the present embodiment will be considered using a relative coordinate system in which the vehicle <b>1</b> is located in the center. The right side of the direction in which the subject vehicle <b>1</b> moves is denoted as a positive direction of X axis, the upper side of the subject vehicle <b>1</b> is denoted as a positive direction of Y axis, the direction in which the subject vehicle <b>1</b> moves (front side) is denoted as a positive direction of Z axis, and the crossing point between the road surface and a vertical line passing through the center of two image capturing devices <b>110</b> is denoted as an origin (0, 0, 0). When the road is assumed to be a flat plane, the road surface matches the X-Z plane (y=0). The position information obtaining unit <b>162</b> uses (formula 1) to (formula 3) shown below to transform the coordinate of the pixel (i, j, dp) in the distance image <b>126</b> into a three-dimensional point (x, y, z) in the real world.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>x=CD/</i>2<i>+z&#xb7;PW</i>&#xb7;(<i>i&#x2212;IV</i>).&#x2003;&#x2003;(formula 1)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>y=CH+z&#xb7;PW</i>&#xb7;(<i>j&#x2212;JV</i>)&#x2003;&#x2003;(formula 2)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>z=KS/dp</i>&#x2003;&#x2003;(formula 3)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
Here, CD denotes an interval (baseline length) between the image capturing devices <b>110</b>, PW denotes a corresponding distance in the real world to a distance between adjacent pixels in the image, so-called like an angle of view per pixel, CH denotes an disposed height of the image capturing device <b>120</b> from the road surface, Iv and JV denote coordinates (pixels) in the image at an infinity point in front of the subject vehicle <b>1</b>, and KS denotes a distance coefficient (KS=CD/PW).
</p>
<p id="p-0052" num="0051">The grouping unit <b>162</b> groups continuous target portions of which position differences in a width direction x vertical, to the advancing direction of the subject vehicle <b>1</b> and in a depth direction z parallel to the advancing direction of the subject vehicle <b>1</b> fall within a first predetermined distance, thereby forming a target group. Specifically, the grouping unit <b>162</b> derives the position in the real world of any given target portion and adopts it as a base point. Then the grouping unit <b>162</b> groups target portions of which position differences in the width direction x and in the depth direction z from the target portion serving as the base point fall within the first predetermined distance into a target object. The first predetermined distance is represented by a distance in the real world, and can be set as any given value (for example, 1.0 m). The continuous target portions are target portions located on a same curve that is one-order or multiple-order curve.</p>
<p id="p-0053" num="0052">The grouping unit <b>162</b> also adopts the target portion newly added through the grouping processing as a base point and groups the relevant target portions which are provisionally determined to correspond to a same specific object and of which position differences in the width direction x and in the height direction y are within the first predetermined distance. Consequently, as long as the distances between the target portions provisionally determined to be the same specific object is within the first predetermined distance, all of such target portions are grouped.</p>
<p id="p-0054" num="0053"><figref idref="DRAWINGS">FIGS. 5A and 5B</figref> are explanatory diagrams for explaining grouping of target portions <b>202</b> of a preceding vehicle <b>200</b>. It is assumed that the luminance image <b>124</b> illustrated in <figref idref="DRAWINGS">FIG. 5A</figref> is generated. <figref idref="DRAWINGS">FIGS. 5B</figref>, <b>5</b>C, and <b>5</b>D are plan views when the preceding vehicle <b>200</b> on a driving lane <b>204</b> in the luminance image <b>124</b> is viewed from above in the vertical direction.</p>
<p id="p-0055" num="0054">When the target portions <b>202</b> at the back of the preceding vehicle <b>200</b> in <figref idref="DRAWINGS">FIG. 5B</figref> are detected as illustrated in <figref idref="DRAWINGS">FIG. 5C</figref>, the grouping unit <b>162</b> groups the target portions <b>202</b> so as to form a target object <b>206</b> as illustrated in <figref idref="DRAWINGS">FIG. 5D</figref>.</p>
<p id="p-0056" num="0055">The target portions <b>202</b> at the back of the preceding vehicle <b>200</b> are detected such that they are almost parallel to the X-axis direction. On the other hand, target portions of a wall such as a guard rail forms a tilt surface that is tilted at a predetermined angle (for example, 45 degrees) or more with respect to the X-axis direction. Therefore, one way of detecting a wall is to search a target object composed of target portion that form a tilt surface tilting at the predetermined angle or more with respect to the X-axis direction. However, this might cause an erroneous detection of a wall at night or in rainy weather.</p>
<p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. 6</figref> is an explanatory diagram for explaining one example of the luminance image <b>124</b> at night. As illustrated in <figref idref="DRAWINGS">FIG. 6</figref>, a tail lamp <b>208</b> is lighted at the back of the preceding vehicle <b>200</b>. However, the light from the tail lamp <b>208</b> is diffused and dimmed due to the nighttime. In this case, an error might be caused on the parallax information derived by the image processing device <b>120</b>.</p>
<p id="p-0058" num="0057"><figref idref="DRAWINGS">FIGS. 7A to 7F</figref> are explanatory diagrams for explaining wall determining processing. <figref idref="DRAWINGS">FIG. 7A</figref> illustrates the case where the target portions <b>202</b> at the back of the preceding vehicle <b>200</b> are detected as in <figref idref="DRAWINGS">FIG. 5C</figref>, but an error is caused on the parallax information due to nighttime.</p>
<p id="p-0059" num="0058">When the error is caused on the parallax information as illustrated in <figref idref="DRAWINGS">FIG. 7A</figref> due to the nighttime, the target portions <b>202</b> that are supposed to be detected as parallel to the X-axis direction might be detected such that they form a tilt surface tilting at the predetermined angle or more with respect to the direction. Therefore, a target portion <b>206</b><i>a </i>formed by grouping the target portions <b>202</b> might erroneously be detected as a wall.</p>
<p id="p-0060" num="0059">In the present embodiment, the candidate determining unit <b>164</b>, the wall determining unit <b>166</b>, and the environment determining unit <b>168</b> function for avoiding an erroneous detection of a wall. The wall determining processing by the candidate determining unit <b>164</b>, the wall determining unit <b>166</b>, and the environment determining unit <b>168</b> will be described below in detail.</p>
<p id="p-0061" num="0060">(Wall Determining Processing)</p>
<p id="p-0062" num="0061">The candidate determining unit <b>164</b> determines whether or not the target portions <b>202</b> which are grouped as a target object <b>206</b> at a position in the height direction y of 0 or more (located above a road surface) form a tilt surface tilting at the predetermined angle or more with respect to a plane vertical to the advancing direction of the subject vehicle <b>1</b>. Specifically, the candidate determining unit <b>164</b> determines whether or not the target portions <b>202</b> form a tilt surface tilting at the predetermined angle or more with respect to the X-axis direction. In the present embodiment, the candidate determining unit <b>164</b> makes a determination for a target object that is formed by at least three or more target portions <b>202</b> in order to enhance the accuracy of detecting a wall.</p>
<p id="p-0063" num="0062">When the target portions are located on a tilt surface, the candidate determining unit <b>164</b> does not determine that the target object formed by grouping the target portions is a wall only based on this condition, but tentatively determines that they are wall candidates that are candidates for a wall. In <figref idref="DRAWINGS">FIG. 7A</figref>, the target object <b>206</b><i>a </i>is determined as a wall candidate.</p>
<p id="p-0064" num="0063">The wall determining unit <b>166</b> determines whether or not there are other wall candidates within a range of a second predetermined distance in the X-axis direction (for example, within AB range illustrated in <figref idref="DRAWINGS">FIG. 7B</figref>, within 5.0 m) and within a second predetermined distance in the Z-axis direction around the central position of the target object <b>206</b><i>a</i>. The second predetermined distance is longer than the above-mentioned first predetermined distance.</p>
<p id="p-0065" num="0064">In <figref idref="DRAWINGS">FIG. 7B</figref>, the target objects <b>206</b><i>a </i>and <b>206</b><i>b </i>are detected from the back of the preceding vehicle <b>200</b>. Therefore, there is no other wall candidate near the target object <b>206</b><i>a</i>. When no other wall candidate is detected, the wall determining unit <b>166</b> determines that the wall candidate is not a wall.</p>
<p id="p-0066" num="0065">In <figref idref="DRAWINGS">FIG. 7B</figref>, if the target object <b>206</b><i>b </i>is also determined to be a wall candidate according to its placement, the target object <b>206</b><i>a </i>and <b>206</b><i>b </i>might be determined as a wall.</p>
<p id="p-0067" num="0066">A wall such as a guard rail has a tilt surface tilting at a predetermined angle or more with respect to the X-axis direction. Therefore, the target portions of the wall candidates detected from a wall tend to gather on the same tilt surface. On the other hand, when wall candidates are erroneously determined from the hack surface of the preceding vehicle <b>200</b>, target portions thereof are not likely to be gather on the same tilt surface.</p>
<p id="p-0068" num="0067">Therefore, if there are other wall candidates located within a predetermined distance from an extended line linking the target portions which grouped into a target object, the wall determining unit <b>166</b> determines that these other candidates is a wall. The wall determining unit <b>166</b> may employ an approximate curve derived with the positions of the target portions being defined as samples, for example, instead of the line linking the target portions. With this configuration, the wall determining unit <b>166</b> can precisely make a determination of a wall.</p>
<p id="p-0069" num="0068">In this case, the wall determining unit <b>166</b> defines the wall candidate (target object <b>206</b><i>a</i>) that is not determined to be a wall and the target object <b>206</b><i>b</i>, which is not the wall candidate and is located within a third predetermined distance (for example, within the range of CD illustrated in <figref idref="DRAWINGS">FIG. 7C</figref>, 3.0 m) from the wall candidate in the X-axis direction and in the Z-axis direction for example, all together as one target object <b>206</b><i>c </i>as illustrated in <figref idref="DRAWINGS">FIG. 7D</figref>.</p>
<p id="p-0070" num="0069">Since a wall candidate that is not determined as a wall is thus grouped with another target object, the later-described pattern matching unit <b>174</b> can highly accurately determine that the target object <b>206</b><i>c </i>formed by the grouping is the back surface of the vehicle ahead <b>200</b>, for example.</p>
<p id="p-0071" num="0070"><figref idref="DRAWINGS">FIG. 7E</figref> illustrates target objects <b>206</b><i>d</i>, <b>206</b><i>e</i>, <b>206</b><i>f</i>, <b>206</b><i>g</i>, and <b>206</b><i>h </i>detected from a guard rail on a curve. In this case, the wall determining unit <b>166</b> detects the target object <b>206</b><i>e </i>which is close to the target object <b>206</b><i>d </i>within the second predetermined distance in the X-axis direction and in the Z-axis direction around the center of the subject <b>206</b><i>d</i>. Similarly, the wall determining unit <b>166</b> sequentially detects the target object <b>206</b><i>f </i>close to the target object <b>206</b><i>e</i>, and the target object <b>206</b><i>g </i>close to the target object <b>206</b><i>f</i>. In this manner, the target objects <b>206</b><i>d </i>to <b>206</b><i>h </i>are detected.</p>
<p id="p-0072" num="0071">As illustrated in <figref idref="DRAWINGS">FIG. 7F</figref>, the wall determining unit <b>166</b> determines that these wall candidates which are closely continuous within the second predetermined distance is a wall <b>210</b>.</p>
<p id="p-0073" num="0072">A size of a light source such as a tail lamp is smaller than a size of a wall in the direction of the X-Z plane. The wall determining unit <b>166</b> in the present embodiment determines continuous wall candidates as a wall, and does not determine a wall candidate that is solely detected as a wall. Therefore, even if the tail lamp is erroneously detected such that it forms a tilt surface like a wall due to the light diffusion, the wall determining unit <b>166</b> does not determine the tail lamp as a wall, thereby preventing the erroneous detection of a wall.</p>
<p id="p-0074" num="0073">The environment determining unit <b>168</b> determines whether or not the detection area <b>122</b> is in an environment where light is diffused. The environment where light is diffused includes nighttime and an environment where water droplets are deposited onto a front window due to rainfall, or streak is caused by cleaning the water droplets. The environment, determining unit <b>168</b> determines that the detection area <b>122</b> is in the environment where light is diffused, when the detection information output from the environment detecting unit <b>154</b> indicates nighttime or rainfall.</p>
<p id="p-0075" num="0074">When the environment determining unit <b>168</b> determines that the detection area <b>122</b> is not in the environment where light is diffused, the wall determining unit <b>166</b> determines all wail candidates are a wall. This is because, if the detection area is not in the environment where light is diffused as illustrated in <figref idref="DRAWINGS">FIG. 6</figref>, the possibility of an error caused in the parallax information is low. With this configuration, the wall determining unit <b>166</b> can reduce a processing load for the wail determining processing, when the detection area is not in the environment where light is diffused.</p>
<p id="p-0076" num="0075">(Group Dividing Processing)</p>
<p id="p-0077" num="0076">The group dividing processing will be described in detail below, in which target objects that are not determined to be a wall and that are erroneously grouped as one target object are divided into separate target objects.</p>
<p id="p-0078" num="0077"><figref idref="DRAWINGS">FIGS. 8A to 9C</figref> are explanatory diagrams for explaining the group dividing processing. <figref idref="DRAWINGS">FIG. 8A</figref> is an example of the actual luminance image <b>124</b>, and <figref idref="DRAWINGS">FIG. 8B</figref> is the distance image <b>126</b> corresponding to the luminance image <b>124</b>. The components unnecessary for the description of the group dividing processing are not illustrated in the distance image <b>126</b>. As illustrated in <figref idref="DRAWINGS">FIGS. 8A and 8B</figref>, it is assumed that a bus <b>222</b> and a vehicle <b>224</b> run on a driving lane <b>220</b>, on which the subject vehicle <b>1</b> runs, in line in the longitudinal direction to the advancing direction such that they are very close to each other in the depth direction z. In this case, the bus <b>222</b> and the vehicle <b>224</b> in line might erroneously be detected as one target object <b>226</b> as illustrated in <figref idref="DRAWINGS">FIG. 8C</figref>.</p>
<p id="p-0079" num="0078">Accordingly, the distance deriving unit <b>170</b> derives an average indicated by arrows in <figref idref="DRAWINGS">FIG. 8D</figref> of the relative distances in the depth direction z of target portions of the target object <b>226</b> in the distance image <b>126</b>, the target portions being in the horizontal, direction.</p>
<p id="p-0080" num="0079">It is assumed that the bus <b>222</b> and the vehicle <b>224</b> are in line as illustrated in <figref idref="DRAWINGS">FIG. 9A</figref>, when they are viewed from the X-axis direction. The back surface of the vehicle <b>224</b> nearer to the vehicle <b>1</b> is entirely displayed, but a part of the back surface of the bus <b>222</b> is not displayed onto the luminance image <b>124</b>, because it is behind the vehicle <b>224</b>. Therefore, a trajectory of the average of the relative distances in the depth direction z of target portions horizontally in line is plotted on the coordinate system of the j and Z axes on the range image <b>126</b> is as illustrated in <figref idref="DRAWINGS">FIG. 9B</figref>, for example.</p>
<p id="p-0081" num="0080"><figref idref="DRAWINGS">FIG. 9C</figref> is a view obtained by rotating the coordinate axes in <figref idref="DRAWINGS">FIG. 9B</figref> by 90 degrees, and inverting the resultant like a mirror image. In <figref idref="DRAWINGS">FIG. 9C</figref>, the relative distance in the depth direction z significantly changes compared to the change in the j coordinate value in a portion indicated by a broken line <b>228</b>. This portion (hereinafter referred to as vertical position c) corresponds to a boundary portion between the bus <b>222</b> and the vehicle <b>224</b> on the luminance image <b>124</b>.</p>
<p id="p-0082" num="0081">The group dividing unit <b>172</b> derives a derivative value of the trajectory of a data column of the average of the relative distances in the depth direction z, disposed based on the j coordinate value, and compares the derived derivative value to a predetermined threshold value, thereby specifying the vertical position c of a group of the target portions exceeding the predetermined threshold value.</p>
<p id="p-0083" num="0082"><figref idref="DRAWINGS">FIGS. 10A and 10B</figref> are explanatory diagrams for explaining the group dividing processing. When specifying the vertical position c, the group dividing unit <b>172</b> divides the target object <b>226</b> astride the vertical position c illustrated in <figref idref="DRAWINGS">FIG. 10A</figref> using the vertical position c as a boundary, whereby the target object <b>226</b> is divided into two new target objects <b>226</b><i>a </i>and <b>226</b><i>b </i>as illustrated in <figref idref="DRAWINGS">FIG. 10B</figref>.</p>
<p id="p-0084" num="0083">In this manner, the group dividing unit <b>172</b> correctly divides the target object <b>226</b> including target portions that are supposed to be grouped as a different group but are undesirably grouped all together into different target objects <b>226</b><i>a </i>and <b>226</b><i>b</i>. Accordingly, the environment recognition apparatus <b>130</b> can enhance the accuracy of subsequent pattern matching processing.</p>
<p id="p-0085" num="0084">The group dividing unit <b>172</b> determines the predetermined threshold value based on the average of the relative distances z of all target regions forming the target object <b>226</b>.</p>
<p id="p-0086" num="0085">As the relative distance in the depth direction z is larger, the distance resolution of the position derived by the position acquiring unit <b>160</b> is low. Therefore, the group dividing unit <b>172</b> adjusts the predetermined threshold value according to the relative distance in the depth direction z such that a value not larger than the distance resolution is neglected. With this configuration, the group dividing unit <b>172</b> can avoid the is situation in which the target object <b>226</b> is erroneously divided due to the influence of an error not larger than the distance resolution.</p>
<p id="p-0087" num="0086">The pattern matching unit <b>174</b> performs pattern matching to model, data of a three-dimensional object preliminarily retained in the data retaining unit <b>152</b>. The pattern matching unit <b>174</b> determines whether or not a target object corresponds to any three-dimensional object in this way, the environment recognition apparatus <b>130</b> can recognize not only a wall but also a three-dimensional object other than a wall.</p>
<p id="p-0088" num="0087">(Environment Recognition Method)</p>
<p id="p-0089" num="0088">Hereinafter, the particular processings performed by the environment recognition device <b>130</b> will be explained based on the flowchart shown in <figref idref="DRAWINGS">FIGS. 11 to 14</figref>. <figref idref="DRAWINGS">FIG. 11</figref> illustrates an overall flow of interrupt processing when the image processing device <b>120</b> transmits the distance image (parallax information) <b>126</b>. <figref idref="DRAWINGS">FIGS. 12 to 14</figref> illustrate subroutines therein.</p>
<p id="p-0090" num="0089">As illustrated in <figref idref="DRAWINGS">FIG. 11</figref>, when an interrupt occurs according to the environment recognition method in response to reception of the distance image <b>126</b>, the position information indicating the position of each target portion in the real world is derived based on the parallax information for each block in the detection area <b>122</b> derived by the image processing device <b>120</b> (S<b>300</b>). The grouping processing is then performed on the target portions (S<b>302</b>).</p>
<p id="p-0091" num="0090">Next, the wall determining processing is performed on a target object obtained by the grouping processing (S<b>304</b>), and the group dividing processing is performed on a target object that is not determined as a wall (S<b>306</b>). Then, the pattern matching unit <b>174</b> performs a pattern matching between the target object and a three-dimensional object for the target object (S<b>308</b>). Hereinafter, the above processings will be explained more specifically.</p>
<p id="p-0092" num="0091">(Grouping Processing S<b>302</b>)</p>
<p id="p-0093" num="0092">As shown in <figref idref="DRAWINGS">FIG. 12</figref>, the grouping unit <b>162</b> selects one target portion in the luminance image <b>124</b> that is not yet selected (S<b>350</b>). The grouping unit <b>162</b> determines whether or not there is a target portion of which position differences from the selected target portion in the width direction x and in the depth direction z fall within the first predetermined distance (S<b>352</b>). When there is a target portion within the first predetermined distance (YES in S<b>352</b>), the grouping unit <b>162</b> groups all target portions within the first predetermined distance into a target object (S<b>354</b>).</p>
<p id="p-0094" num="0093">When all target portions within the first predetermined distance are already grouped into a target object, the target object is also used for grouping to form one target object. In this manner, the grouping unit <b>162</b> groups a plurality of closely continuous target portions into a target object.</p>
<p id="p-0095" num="0094">When there is no target portion within the first predetermined distance (NO in S<b>352</b>), and after target object setting processing in S<b>354</b>, the grouping unit <b>162</b> determines whether or not there is a target portion that is not yet selected in the luminance image <b>124</b> (S<b>356</b>). When there is such target portion (YES in S<b>356</b>), the grouping unit <b>162</b> returns to the target object selecting processing in S<b>350</b>. When there is no target portion that not yet selected (NO in S<b>356</b>), the grouping processing S<b>302</b> is terminated.</p>
<p id="p-0096" num="0095">(Wall Determining Processing S<b>304</b>)</p>
<p id="p-0097" num="0096">As shown in <figref idref="DRAWINGS">FIG. 13</figref>, the candidate determining unit <b>164</b> selects one target object that is not yet selected (S<b>400</b>). The candidate determining unit <b>164</b> determines whether or not the position in the height direction y of the center of the target object is 0 or more (is located above a road surface) (S<b>402</b>). When the position in the height direction y is less than 0 (NO in S<b>402</b>), the candidate determining unit <b>164</b> moves to step S<b>408</b> that determines whether or not there is a target object that is not yet selected.</p>
<p id="p-0098" num="0097">When the position in the height direction y is 0 or more (YES in S<b>402</b>), the candidate determining unit <b>164</b> determines whether or not the target portions grouped as a target object form a tilt surface tilting at a predetermined angle or more with respect to a plane vertical to the advancing direction of the subject vehicle <b>1</b> (S<b>404</b>). When they do not form the tilt surface (NO in S<b>404</b>), the candidate determining unit <b>164</b> moves to step S<b>408</b> for determining whether or not there is a target object that is not yet selected.</p>
<p id="p-0099" num="0098">When the target portions form a tilt surface (YES in S<b>404</b>), the candidate determining unit <b>164</b> determines that the target object formed by grouping the target portions is a wall candidate that is a candidate of a wall (S<b>406</b>). Then, the is candidate determining unit <b>164</b> moves to step S<b>408</b> for determining whether or not there is a target object that is not yet selected.</p>
<p id="p-0100" num="0099">Next, the candidate determining unit <b>164</b> determines whether or not there is a target object that is not yet selected (S<b>408</b>) When there is the target object that is not yet selected (YES in S<b>408</b>), the candidate determining unit <b>164</b> returns to the subject selecting processing in S<b>400</b>.</p>
<p id="p-0101" num="0100">After the wall, candidate determining process is performed on all target objects (NO in S<b>408</b>), the environment determining unit <b>168</b> determines whether or not the detection area is in the environment where light is diffused, such as whether or not it is at night or in rainy weather (S<b>410</b>). When the detection region is not in the environment where light is diffused (NO in S<b>410</b>), the wall determining unit <b>166</b> determines all wall candidates are a wall (S<b>412</b>). When the detection area is in the environment where light is diffused (YES in S<b>410</b>), the wall determining unit <b>166</b> determines whether or not there are any wall candidates that are not yet selected (S<b>414</b>). When there is a wall candidate (YES in S<b>414</b>), the wall determining unit <b>166</b> selects one wall candidate that is not yet selected (S<b>416</b>) The wall determining unit <b>166</b> then determines whether or not there is another wall candidate within a range of a second predetermined distance in the X-axis direction and within a range of a second predetermined distance in the Z-axis direction around the central position of the selected wall candidate (S<b>418</b>).</p>
<p id="p-0102" num="0101">The wall determining unit <b>166</b> also determines for the selected target object that is a wall candidate whether or not there is any other wall candidates located within a predetermined distance from an extended line linking the target portions grouped into a target object.</p>
<p id="p-0103" num="0102">When there are other wall candidates located within a predetermined distance from the extended line linking the target portions (YES in S<b>418</b>), the wall determining unit <b>166</b> determines all of the wall candidates are a wall (S<b>420</b>). Then, the wall determining unit <b>166</b> returns to step S<b>414</b> for determining whether or not there is a wall candidate that is not yet selected. Thus, the wall determining unit <b>166</b> determines these wall candidates which are closely continuous within the second predetermined distance as a wall.</p>
<p id="p-0104" num="0103">When there is no other wall candidate (NO in S<b>418</b>), the wall determining unit <b>166</b> determines that the wall candidate is not a wall (S<b>422</b>). Then, the wall determining unit <b>166</b> determines whether or not there is another target object that is not the wall candidate within a third predetermined distance in the X-axis direction and in the Z-axis direction from the wall candidate which is determined not to be a wall (S<b>424</b>), When there is no target object other than the wall candidate (NO in S<b>424</b>), the wall determining unit <b>166</b> returns to step S<b>414</b> for determining whether or not there is a wall candidate that is not yet selected.</p>
<p id="p-0105" num="0104">When there is another target object other than the wall candidate (YES in S<b>424</b>), the wall determining unit <b>166</b> defines the wall candidate which is determined not to be a wall and the target object which is not the wail candidate all together as one target object (S<b>426</b>). Then, the wall determining unit <b>166</b> returns to step S<b>414</b> for determining whether or not there is a wall candidate that is not yet selected.</p>
<p id="p-0106" num="0105">When there is no wall candidate that is not yet selected in step S<b>414</b> for determining whether or not there is a wall candidate that is not yet selected (NO in S<b>414</b>), the wall determining processing S<b>304</b> is terminated.</p>
<p id="p-0107" num="0106">(Group Dividing Processing S<b>306</b>)</p>
<p id="p-0108" num="0107">As shown <figref idref="DRAWINGS">FIG. 14</figref>, the distance deriving unit <b>170</b> selects one target object that is not been selected in the group dividing processing in S<b>306</b> (S<b>450</b>). Then, the distance deriving unit <b>170</b> derives an average of the relative distances in the depth direction z of target portions of the selected subject which are horizontally in line in the luminance image <b>124</b> (S<b>452</b>). The distance deriving unit <b>170</b> then generates a data column in which derived averages of the relative distances in the depth direction z are disposed based on the position in the vertical direction in the luminance image <b>124</b> (S<b>454</b>).</p>
<p id="p-0109" num="0108">Next, the group dividing unit <b>172</b> derives the average of the relative distances in the depth direction z of all target portions forming the selected target object, and determines the predetermined threshold value based on the derived average (S<b>456</b>) The group dividing unit <b>172</b> derives a derivative value of the data column of generated averages of the relative distances in the depth direction z (S<b>458</b>), and determines whether or not the derived derivative value exceeds the predetermined threshold value (S<b>460</b>). When it does not exceed the predetermined threshold value (NO in S<b>460</b>), the flow moves to step S<b>466</b> for determining whether or not there is another target object that is not yet selected.</p>
<p id="p-0110" num="0109">When the derivative value exceeds the predetermined threshold value (YES in S<b>460</b>), the group dividing unit <b>172</b> specifies the vertical position c of target portions corresponding to the derivative value exceeding the predetermined threshold value (S<b>462</b>). The group dividing unit <b>172</b> divides the target object astride the vertical position c, using the vertical position c as a boundary (S<b>464</b>). Then, the flow moves to step S<b>466</b> for determining whether or not there is another target object that is not yet selected.</p>
<p id="p-0111" num="0110">Next, the distance deriving unit <b>170</b> determines whether or not there is a target object that is not yet selected in the group dividing processing in S<b>306</b> (S<b>466</b>). When there is a target object that is not yet selected (YES in S<b>466</b>), the flow returns to the target object selecting processing in s<b>450</b>. When there is no target object that is not yet selected (NO in S<b>466</b>) the group dividing processing S<b>306</b> is terminated.</p>
<p id="p-0112" num="0111">As described above, the environment recognition device <b>130</b> can prevent an object that is not a wall from being erroneously determined to be a wall in an environment where light is easily e diffused, such as at night or in rainy weather.</p>
<p id="p-0113" num="0112">A, program for allowing a computer to function as the environment recognition device <b>130</b> is also provided as well as a storage medium such as a computer-readable flexible disk, a magneto-optical disk, a ROM, a CD, a DVD, a BD storing the program. Here, the program means a data processing function described in any language or description method.</p>
<p id="p-0114" num="0113">While a preferred embodiment of the present invention has been described hereinabove with reference so the appended drawings, it is to be understood that the present invention is not limited to such embodiment. It will be apparent to those skilled in the art that various, changes may be made without departing from the scope of the invention.</p>
<p id="p-0115" num="0114">In the above-mention embodiment, the grouping unit <b>162</b> and the wall determining unit <b>166</b> make a determination using the distance in the with direction x and the relative distance in the depth direction z in the real world, but a determination may be made using the detection distances in the luminance image <b>124</b> and the distance image <b>126</b>. In this case, the first predetermined distance and the second predetermined distance are changed according to the relative distance of the target portion. As shown in <figref idref="DRAWINGS">FIG. 2</figref> and the like, distant objects and close objects are represented in the flat plane in the Luminance image <b>124</b> and the distance image <b>126</b>, and therefore, an object located at a distant position is represented in a small (short) size and an object located at a close position is represented in a large (long) size. Therefore, for example, the threshold values of the first predetermined distance and the second predetermined distance in the luminance image <b>124</b> and the distance image <b>126</b> are set as a small value for a distant target portion, and set at a large value for a close target portion. Therefore, even when the detection distances are different between a distant position and a close position, the grouping processing can be stably performed.</p>
<p id="p-0116" num="0115">In addition to the difference in the width direction x and the difference in the depth direction z explained above, the grouping unit <b>166</b> may group target portions of which position difference in the height direction y is within the first predetermined distance. In this case, when any one of the difference of positions in the width direction x, the difference of positions in the height direction y, and the difference of positions (relative distances) in the depth direction z is greatly different, the group of the target portions may be deemed as an independent target object. In so doing, it is possible to perform highly accurate grouping processing.</p>
<p id="p-0117" num="0116">Similarly, the wall determining unit <b>166</b> may make a determination of a wall based on not only the difference in the width direction x and the difference in the depth direction z but also the difference in the height direction y from the road surface. Considering the difference in the height direction y upon a determination of a wall can enhance the accuracy of detecting a wall by the wall determining unit <b>166</b>.</p>
<p id="p-0118" num="0117">In the above description, the grouping unit <b>162</b> independently determines each of the difference in the width direction x, the difference in the height direction y and the difference in the depth direction z is independently determined, and groups target portions into a same group only when all of them are included within the first predetermined distance, the target portions are grouped into a the same group. However, grouping processing may be performed using another calculation. For example, when a square mean value &#x221a; or the difference in the width direction x, the difference in the height direction y, and the difference in the depth direction z ((difference in the width distance x)2+(difference in the height direction v)2+(difference in the depth direction z)2) is included within the first predetermined distance, target portions may be grouped into a same group. With such calculation, distances between target portions in the real world can be derived accurately, and therefore, grouping accuracy can be enhanced.</p>
<p id="p-0119" num="0118">Similarly, the wall determining unit <b>166</b> independently determines each of the difference in the width direction x, the difference in the height direction y and the difference in the depth direction z is independently determined, and groups target portions into a same group only when all of them are included within the second predetermined distance, the target portions are grouped into a the same group. However, grouping processing may be performed using another calculation. For example, when a square mean value &#x221a; of the difference in the width direction x, the difference in the height direction y, and the difference in the depth direction z ((difference in the width distance x)2+(difference in the height direction y) 2+(difference in the depth direction z) 2) is included within the second predetermined distance, target portions may be is grouped into a same group. With such calculation, distances between target portions in the real world can be derived accurately, and therefore, grouping accuracy can be enhanced.</p>
<p id="p-0120" num="0119">In the above embodiment, the three-dimensional position of the target object is derived based on the parallax between image data using the plurality of image capturing devices <b>110</b>. However, the present invention is not limited to such case. Alternatively, for example, a variety of known distance measuring devices such as a laser radar distance measuring device may be used in this case, the laser radar distance measuring device emits laser beam to the detection area <b>122</b>, receives light reflected when the laser beam is irradiated the object, and measures the distance to the object based on the time required for this event.</p>
<p id="p-0121" num="0120">The above embodiment describes an example in which the position information obtaining unit <b>160</b> receives the distance image (parallax information) <b>126</b> from the image processing device <b>120</b>, and generates the three-dimensional position information. However, the present invention is not limited to such case. The image processing device <b>120</b> may generate the three-dimensional position information in advance, and the position information obtaining unit <b>160</b> may obtain the generated three-dimensional position information. Such a functional distribution can reduce the processing load of the environment recognition device <b>130</b>.</p>
<p id="p-0122" num="0121">In the above embodiment, the position information acquiring unit <b>160</b>, the grouping unit <b>162</b>, the candidate determining unit <b>164</b>, the wall determining unit <b>166</b>, the environment determining unit <b>168</b>, the distance deriving unit <b>170</b>, the group dividing unit <b>172</b>, and the pattern matching unit <b>174</b> are configured to be operated by the central control unit <b>154</b> with software. However, the functional units may be configured with hardware.</p>
<p id="p-0123" num="0122">The steps of the environment recognition method in this specification do not necessarily need to be processed chronologically according to the order described in the flowchart. The steps may be processed in parallel, or may include processings using subroutines.</p>
<p id="p-0124" num="0123">The present invention can be used for an environment recognition device and an environment recognition method for recognizing a target object based on the luminances of the target object in a detection area.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. An environment recognition device comprising:
<claim-text>a position information obtaining unit that obtains position information of a target portion in a detection area of a luminance image, including a relative distance from a subject vehicle;</claim-text>
<claim-text>a grouping unit that groups target portions, of which positions differences in a vertical direction and a parallel direction with respect to a base line which corresponds to advancing direction of the subject vehicle fall within a first distance, into a target object;</claim-text>
<claim-text>a candidate determining unit that determines that the target object is a candidate of a wall, when the target portions included in the target object form a tilt surface tilting at a predetermined angle or more against a vertical plane with respect to a base plane which corresponds to the advancing direction of the subject vehicle; and</claim-text>
<claim-text>a wall determining unit that determines that the wall candidates, of which positions differences in the vertical direction and the parallel direction with respect to a base line which corresponds to the advancing direction of the subject vehicle fall within a second predetermined distance longer than the first predetermined distance, are a wall.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The environment recognition device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the grouping unit makes a determination for the grouping based on not only the position differences in the vertical direction and the parallel direction with respect to a base line which corresponds to the advancing direction of the subject vehicle but also a position difference in the height direction from a road surface, and the wall determining unit may make a determination of a wall based on not only the position differences in the vertical direction and in the parallel direction with respect to a base line which corresponds to the advancing direction of the vehicle but also the position difference in the height direction from the road surface.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The environment recognition device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>an environment determining unit that determines whether or not the detection area is in an environment where light is diffused, wherein</claim-text>
<claim-text>the wall determining unit determines all of the wall candidates as a wall when the environment determining unit determines that the detection area is in the environment where light is not diffused.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The environment recognition device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>a distance deriving unit that derives each average of in-line relative distances, which are assigned to target portions included in a target object and which are aligned in a horizontal direction of the image corresponding to the width direction in the real world; and</claim-text>
<claim-text>a group dividing unit that divides the target object at a position where differences of the averages in the vertical direction of the image corresponding to the height direction in the real world exceeds the predetermined threshold value, and defines the position comprised of the horizontally aligned target portions as a boundary of the divided target objects.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The environment recognition device according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the group dividing unit determines the predetermined threshold value based on the average of the relative distances of all target portions included in the target object.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. An environment recognition method comprising:
<claim-text>obtaining position information of a target portion in a detection area of a luminance image including a relative distance from a subject vehicle;</claim-text>
<claim-text>grouping target portions, of which positions differences in a vertical direction and a parallel direction with respect to a base line which corresponds to advancing direction of the subject vehicle fall within a first predetermined distance, into a target object;</claim-text>
<claim-text>determining that the target object is a candidate of a wall, when the target portions included in the target object form a tilt surface tilting at a predetermined angle or more against a vertical plane with respect to a base plane which corresponds to the advancing direction of the subject vehicle; and</claim-text>
<claim-text>determining that determines that the wall candidates, of which positions differences in the vertical direction and the parallel direction with respect to a base line which corresponds to the advancing direction of the subject vehicle fall within a second predetermined distance longer than the first predetermined distance, are a wall.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The environment recognition device according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising:
<claim-text>an environment determining unit that determines whether or not the detection area is in an environment where light is diffused, wherein</claim-text>
<claim-text>the wall determining unit determines all of the wall candidates as a wall when the environment determining unit determines that the detection area is in the environment where light is not diffused.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The environment recognition device according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising:
<claim-text>a distance deriving unit that derives each average of in-line relative distances, which are assigned to target portions included in a target object and which are aligned in a horizontal direction of the image corresponding to the width direction in the real world; and</claim-text>
<claim-text>a group dividing unit that divides the target object at a position where differences of the averages in the vertical direction of the image corresponding to the height direction in the real world exceeds the predetermined threshold value, and defines the position comprised of the horizontally aligned target portions as a boundary of the divided target objects.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The environment recognition device according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, further comprising:
<claim-text>a distance deriving unit that derives each average of in-line relative distances, which are assigned to target portions included in a target object and which are aligned in a horizontal direction of the image corresponding to the width direction in the real world; and</claim-text>
<claim-text>a group dividing unit that divides the target object at a position where differences of the averages in the vertical direction of the image corresponding to the height direction in the real world exceeds the predetermined threshold value, and defines the position comprised of the horizontally aligned target portions as a boundary of the divided target objects.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The environment recognition device according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the group dividing unit determines the predetermined threshold value based on the average of the relative distances of all target portions included in the target object.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The environment recognition device according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the group dividing unit determines the predetermined threshold value based on the average of the relative distances of all target portions included in the target object.</claim-text>
</claim>
</claims>
</us-patent-grant>
