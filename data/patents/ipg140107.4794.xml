<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625887-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625887</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13182076</doc-number>
<date>20110713</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<us-term-of-grant>
<us-term-extension>249</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>62</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382159</main-classification>
<further-classification>382209</further-classification>
</classification-national>
<invention-title id="d2e53">Systems and methods for matching visual object components</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>7239929</doc-number>
<kind>B2</kind>
<name>Ulrich et al.</name>
<date>20070700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>7725484</doc-number>
<kind>B2</kind>
<name>Nister et al.</name>
<date>20100500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>2009/0125487</doc-number>
<kind>A1</kind>
<name>Rossi et al.</name>
<date>20090500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>2009/0220166</doc-number>
<kind>A1</kind>
<name>Choi et al.</name>
<date>20090900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>2009/0289942</doc-number>
<kind>A1</kind>
<name>Bailloeul et al.</name>
<date>20091100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2010/0211602</doc-number>
<kind>A1</kind>
<name>Menon et al.</name>
<date>20100800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2010/0329544</doc-number>
<kind>A1</kind>
<name>Sabe et al.</name>
<date>20101200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2011/0103699</doc-number>
<kind>A1</kind>
<name>Ke et al.</name>
<date>20110500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2011/0106798</doc-number>
<kind>A1</kind>
<name>Li et al.</name>
<date>20110500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>KR</country>
<doc-number>20100082683</doc-number>
<kind>A</kind>
<date>20100700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00011">
<othercit>Bryan C. Russell, Antonio Torralba, Kevin P. Murphy, and William T. Freeman. &#x201c;LabelMe: A Database and Web-Based Tool for Image Annotation&#x201d;. International Journal of Computer Vision. vol. 77, Issue 1-3, pp. 157-173. May 1, 2008.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00012">
<othercit>Widyantoro, D. H., Ioerger, T. R. and Yen, J. (2001), Learning user interest dynamics with a three-descriptor representation. J. Am. Soc. Inf. Sci., 52: 212-225.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00013">
<othercit>Hare, Jonathon S., and Paul H. Lewis. &#x201c;On image retrieval using salient regions with vector-spaces and latent semantics.&#x201d; Image and Video Retrieval. Springer Berlin Heidelberg, 2005. 540-549.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00014">
<othercit>Jianjiang Lu, Tianzhong Zhao, Yafei Zhang, Feature selection based-on genetic algorithm for image annotation, Knowledge-Based Systems, vol. 21, Issue 8, Dec. 2008, pp. 887-891.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00015">
<othercit>Deng, Jia, et al. &#x201c;Imagenet: A large-scale hierarchical image database.&#x201d; Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. IEEE, 2009.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00016">
<othercit>Carneiro, Gustavo, et al. &#x201c;Supervised learning of semantic classes for image annotation and retrieval.&#x201d; Pattern Analysis and Machine Intelligence, IEEE Transactions on 29.3 (2007): 394-410.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00017">
<othercit>Srihari, Rohini K. &#x201c;Automatic indexing and content-based retrieval of captioned images.&#x201d; Computer 28.9 (1995): 49-56.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00018">
<othercit>Zhu, Qiang, Mei-Chen Yeh, and Kwang-Ting Cheng. &#x201c;Multimodal fusion using learned text concepts for image categorization.&#x201d; Proceedings of the 14th annual ACM international conference on Multimedia. ACM, 2006.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00019">
<othercit>Rabinovich, Andrew, et al. &#x201c;Objects in context.&#x201d; Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on. IEEE, 2007.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00020">
<othercit>Chang, Shih-Fu, Wei-Ying Ma, and Arnold Smeulders. &#x201c;Recent advances and challenges of semantic image/video search.&#x201d; Acoustics, Speech and Signal Processing, 2007. ICASSP 2007. IEEE International Conference on. vol. 4. IEEE, 2007.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00021">
<othercit>Liu, Ying, et al. &#x201c;A survey of content-based image retrieval with high-level semantics.&#x201d; Pattern Recognition 40.1 (2007): 262-282.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00022">
<othercit>Lew, Michael S., et al. &#x201c;Content-based multimedia information retrieval: State of the art and challenges.&#x201d; ACM Transactions on Multimedia Computing, Communications, and Applications (TOMCCAP) 2.1 (2006): 1-19.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00023">
<othercit>International Search Report for corresponding International Application No. PCT/US12/42095, Mar. 27, 2013.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00024">
<othercit>Written Opinion for corresponding International Application No. PCT/US12/42095, Mar. 27, 2013.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>20</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>None</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>9</number-of-drawing-sheets>
<number-of-figures>12</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20130016899</doc-number>
<kind>A1</kind>
<date>20130117</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Li</last-name>
<first-name>Yuan</first-name>
<address>
<city>Los Angeles</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Adam</last-name>
<first-name>Hartwig</first-name>
<address>
<city>Marina del Rey</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Li</last-name>
<first-name>Yuan</first-name>
<address>
<city>Los Angeles</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Adam</last-name>
<first-name>Hartwig</first-name>
<address>
<city>Marina del Rey</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Fish &#x26; Richardson P.C.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Google Inc.</orgname>
<role>02</role>
<address>
<city>Mountain View</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Koziol</last-name>
<first-name>Stephen R</first-name>
<department>2665</department>
</primary-examiner>
<assistant-examiner>
<last-name>Conner</last-name>
<first-name>Sean</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Systems and methods for modeling the occurrence of common image components (e.g., sub-regions) in order to improve visual object recognition are disclosed. In one example, a query image may be matched to a training image of an object. A matched region within the training image to which the query image matches may be determined and a determination may be made whether the matched region is located within an annotated image component of the training image. When the matched region matches only to the image component, an annotation associated with the component may be identified. In another example, sub-regions within a plurality of training image corpora may be annotated as common image components including associated information (e.g., metadata). Matching sub-regions appearing in many training images of objects may be down-weighted in the matching process to reduce possible false matches to query images including common image components.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="186.44mm" wi="162.05mm" file="US08625887-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="175.34mm" wi="120.57mm" file="US08625887-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="207.86mm" wi="161.12mm" file="US08625887-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="195.66mm" wi="174.41mm" file="US08625887-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="120.99mm" wi="117.77mm" file="US08625887-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="227.50mm" wi="162.98mm" file="US08625887-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="132.00mm" wi="111.84mm" file="US08625887-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="235.37mm" wi="171.79mm" file="US08625887-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="233.85mm" wi="154.69mm" orientation="landscape" file="US08625887-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="210.14mm" wi="172.55mm" file="US08625887-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">FIELD</heading>
<p id="p-0002" num="0001">This disclosure relates to object recognition, and in examples, to recognition of object components.</p>
<heading id="h-0002" level="1">BACKGROUND</heading>
<p id="p-0003" num="0002">The ability to recognize objects can be used in many applications. While humans may easily identify a given object within an image or video, the same task may be more complex for computer vision systems. Numerous approaches and techniques have been developed to attempt to recognize objects using computer vision systems. One example includes training a recognition engine, or matcher, using a set of training, or reference, images.</p>
<p id="p-0004" num="0003">A matcher may have knowledge of a set of training images used for recognizing objects. For example, training corpora may contain images of a set of objects, with one or more images of visual appearances of the object, as well as metadata identifying the objects. Given an image of an object to be recognized, the matcher may retrieve training images similar to the object and determine a match. Often times corresponding metadata of the matched training images may be returned, such as a type, name, title, etc. of the recognized object. Additionally, a matched region of the training image, to which the image of the object matches, may also be identified.</p>
<heading id="h-0003" level="1">SUMMARY</heading>
<p id="p-0005" num="0004">This disclosure may disclose, inter alia, systems and methods for learning and matching visual object components.</p>
<p id="p-0006" num="0005">In one example, a method for identifying an annotated image component matching a query image is provided. The method includes, but is not limited to, receiving a query image. The method further includes matching the query image to an object using a visual object recognition module. The visual object recognition module may be configured to utilize information from a plurality of training image corpora to match the query image to a training image of the object, for example. The method also includes determining a matched region within the training image to which the query image matches using the visual object recognition module. Additionally, the method includes determining whether the matched region is located within an annotated image component of the training image. In one example, the annotated image component may be a sub-region of the training image including an associated annotation describing the sub-region. The method further includes identifying an annotation associated with the annotated image component when the matched region is located within the annotated image component of the training image.</p>
<p id="p-0007" num="0006">In another example, a non-transitory computer-readable medium with instructions stored thereon is provided. The instructions may be executable by a computing device. The instructions may be executable for receiving a query image and matching the query image to an object using a visual object recognition module. The visual object recognition module may be configured to utilize information from a plurality of training image corpora, for example, to match the query image to a training image of the object. The instructions may be further executable for determining a matched region within the training image to which the query image matches using the visual object recognition module. The instructions may also be executable for determining whether the matched region is located within an annotated image component of the training image. The annotated image component may be a sub-region of the training image, for example, and may include an associated annotation describing the sub-region. According to the executable instructions, an annotation associated with the annotated image component may be identified when the matched region is located within an annotated image component of the training image.</p>
<p id="p-0008" num="0007">In another example, a system is provided. The system involves a memory and a processor coupled to the memory. The system further includes instructions, executable by the processor, stored in the memory. The instructions may be executable for receiving a query image and matching the query image to an object using a visual object recognition module. The visual object recognition module may be configured to utilize information from a plurality of training image corpora, for example, to match the query image to a training image of the object. The instructions may be further executable for determining a matched region within the training image to which the query image matches using the visual object recognition module. The instructions may also be executable for determining whether the matched region is located within an annotated image component of the training image. The annotated image component may be a sub-region of the training image, for example, and may include an associated annotation describing the sub-region. According to the executable instructions, an annotation associated with the annotated image component may be identified when the matched region is located within an annotated image component of the training image.</p>
<p id="p-0009" num="0008">The foregoing summary is illustrative only and is not intended to be in any way limiting. In addition to the illustrative aspects, embodiments, and features described above, further aspects, embodiments, and features will become apparent by reference to the figures and the following detailed description.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE FIGURES</heading>
<p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. 1</figref> illustrates an example system.</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 2</figref> is an example block diagram of a method to match a query image to an annotated image component, in accordance with at least some embodiments described herein.</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 3A</figref> illustrates an example of a query image and a matched training image.</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 3B</figref> illustrates another example of a query image and a matched training image.</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 4</figref> is an example block diagram of a method to discover and annotate object components, in accordance with at least some embodiments described herein.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 5A</figref> is an example directed acyclic graph of possible object component relationships.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 5B</figref> illustrates an example of annotating an object component.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 6</figref> is an example block diagram of a method to assign a weighting factor to common image components, in accordance with at least some embodiments described herein.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 7A</figref> illustrates an example of common image components.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 7B</figref> illustrates another example of common image components.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 8</figref> is a functional block diagram illustrating an example computing device used in a computing system that is arranged in accordance with at least some embodiments described herein.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 9</figref> is a schematic illustrating a conceptual partial view of an example computer program product that includes a computer program for executing a computer process on a computing device, arranged according to at least some embodiments presented herein.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0022" num="0021">In the following detailed description, reference is made to the accompanying figures, which form a part hereof. In the figures, similar symbols typically identify similar components, unless context dictates otherwise. The illustrative embodiments described in the detailed description, figures, and claims are not meant to be limiting. Other embodiments may be utilized, and other changes may be made, without departing from the scope of the subject matter presented herein. It will be readily understood that the aspects of the present disclosure, as generally described herein, and illustrated in the figures, can be arranged, substituted, combined, separated, and designed in a wide variety of different configurations, all of which are explicitly contemplated herein.</p>
<p id="p-0023" num="0022">This disclosure may disclose, inter alia, systems and methods for automatically learning and matching visual object components. The systems and methods may be directed to modeling the occurrence of common image components (e.g., sub-regions) in order to improve visual object recognition.</p>
<p id="p-0024" num="0023">In one example, a query image may be received and matched to an object using a visual object recognition module. The visual object recognition module may be configured to utilize information from a plurality of training image corpora, for example, to match the query image to one or more training images of the object. A matched region within the training image to which the query image matches may also be determined by the visual object recognition module. Moreover, a determination may be made whether the matched region is located within an annotated image component of the training image. As an example, an annotated image component may be a sub-region of a training image including an annotation describing the sub-region.</p>
<p id="p-0025" num="0024">In one example, when the matched region matches only to the image component (or a portion of an object of the image) rather than the whole object of the image, an annotation associated with the component may be identified and returned to a user. For example, when the matched region is located within the annotated image component of the training image, an annotation associated with the annotated image component may be identified. In addition, when a percentage of the matched region located within the annotated image component is less than a percentage threshold, an annotation associated with an entirety of the training image may be identified.</p>
<p id="p-0026" num="0025">In another example, a plurality of training image corpora may be received. Cross-corpus image region matching may be performed among the plurality of training image corpora to discover common image components. Matched regions within the plurality of training image corpora may be annotated as sub-regions within the training images including associated information (e.g., metadata).</p>
<p id="p-0027" num="0026">In yet another example, weighting factors influencing a matching process of the visual object recognition module may be adjusted. For example, common image components that frequently appear in many training images of objects may be down-weighted in the matching process to reduce possible false matches to query images including common image components.</p>
<p id="p-0028" num="0027">Referring now to the figures, <figref idref="DRAWINGS">FIG. 1</figref> illustrates an example system <b>100</b>. One or more parts of the system <b>100</b> may be used to perform methods for learning and matching visual object components. The system <b>100</b> may include a processor <b>102</b> coupled to a memory <b>104</b>. Additionally the system <b>100</b> may include a plurality of training image corpora <b>106</b>, a query image <b>108</b>, and a visual object recognition module <b>110</b>, all of which may be coupled to the processor <b>102</b> and the memory <b>104</b>.</p>
<p id="p-0029" num="0028">The processor <b>102</b> may be any type of processor, such as a microprocessor, digital signal processor (DSP), multicore processor, etc., coupled to the memory <b>104</b>. The memory <b>104</b> may be any type of memory, such as volatile memory like random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), or non-volatile memory like read-only memory (ROM), flash memory, magnetic or optical disks, or compact-disc read-only memory (CD-ROM), among other devices used to store data or programs on a temporary or permanent basis. In one example, the memory <b>104</b> may include non-transitory computer-readable media.</p>
<p id="p-0030" num="0029">In one embodiment, the plurality of training image corpora <b>106</b> may include or describe a set of objects. For each object, there may be one or more training images containing visual appearances of the object, as well as metadata associated with the object (e.g., type, name, etc.).</p>
<p id="p-0031" num="0030">In some examples, information within the plurality of training image corpora <b>106</b> may be processed for use by the visual object recognition module <b>110</b>. For example, raw image data may be computed for comparing image similarity within a matching routine. Alternatively, matching may be performed based on image templates as an alternative to (or in addition to) matching raw image data. Matching image templates may be a more effective method than matching raw image data in terms of determining image similarity, for example.</p>
<p id="p-0032" num="0031">In one example, for each image (e.g., training images of the plurality of training image corpora <b>106</b> and the query image <b>108</b>), descriptors for image interest points may be extracted and an image template may be built. A descriptor may include information extracted from local pixels around an image interest point. An image interest point may be a point in the image that can have a clear definition (e.g., may be mathematically well-founded), can have a well-defined position in image space, can have a local image structure that is rich in terms of local information contents, and is stable under local and global perturbations in the image domain. For example, image interest points may be corners, text within an image, blobs, etc., and may be located using optical character recognition (OCR), edge detection techniques, analyzing predetermined positions, among other possibilities. In some examples, a template may include the extracted information of the image and a set of descriptors of all interest points in the image.</p>
<p id="p-0033" num="0032">The query image <b>108</b> may be or include one or more images submitted by one or more users. The query image <b>108</b> may be of any size, shape, and/or form. Similarly, the query image <b>108</b> may be two-dimensional (e.g., a photograph, a figures, a picture) or three-dimensional (e.g. a hologram). The query image <b>108</b> may be captured by optical devices (cameras, mirrors, lenses, etc.) or otherwise input by a system or user (e.g., visual sketch drawn or otherwise rendered on an input surface). In one example, the query image <b>108</b> may be a frame or multiple frames captured from a moving image or video.</p>
<p id="p-0034" num="0033">In some examples, the query image <b>108</b> may include an image provided in order to perform content-based image retrieval. Content from within the query image <b>108</b> may be analyzed and one or more training images matching the content of the query image <b>108</b> may be returned. For example, content of the query image <b>108</b> may refer to colors, shapes, textures, or other information derived from an image. In some examples, the query image <b>108</b> may include an image submitted by a user containing metadata such as keywords, tags, or other descriptions associated with the image.</p>
<p id="p-0035" num="0034">The visual object recognition module <b>110</b> may include or be configured to operate according to one or more programming instructions, for example, computer executable and/or logic implemented instructions to perform functions or steps. Additionally, the visual object recognition module <b>110</b> may be circuitry wired to perform one or more programming instructions.</p>
<p id="p-0036" num="0035">In one example, the visual object recognition module <b>110</b> may have access to and interact with a set of training images of the plurality of training image corpora <b>106</b>. Given a query image <b>108</b>, the visual object recognition module <b>110</b> may retrieve and output training images that are similar to the query image <b>108</b>. Additionally, for each similar training image, a match score may be provided. For example, the match score may be computed based on a combination of one or more of a number of matched descriptors between the query image <b>108</b> and a training image, a computed similarity between raw image data of the query image <b>108</b> and a training image, or other potential image comparison algorithms or methods. Moreover, the visual object recognition module <b>110</b> may output corresponding metadata of the matched training images.</p>
<p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. 2</figref> is an example block diagram of a method <b>200</b> to match a query image to an annotated image component, in accordance with at least some embodiments described herein. The method <b>200</b> shown in <figref idref="DRAWINGS">FIG. 2</figref> presents an embodiment of a method that may, for example, be used by the system <b>100</b> of <figref idref="DRAWINGS">FIG. 1</figref>. Method <b>200</b> may include one or more operations, functions, or actions as illustrated by one or more of blocks <b>201</b>-<b>213</b>. Although the blocks are illustrated in a sequential order, these blocks may also be performed in parallel, and/or in a different order than those described herein. Also, the various blocks may be combined into fewer blocks, divided into additional blocks, and/or removed from the method, based upon the desired implementation of the method.</p>
<p id="p-0038" num="0037">In addition, for the method <b>200</b> and other processes and methods disclosed herein, the flowchart shows functionality and operation of one possible implementation of present embodiments. In this regard, each block may represent a module, a segment, or a portion of program code, which includes one or more instructions executable by a processor for implementing specific logical functions or steps in the process. The program code may be stored on any type of computer readable medium, for example, such as a storage device including a disk or hard drive. The computer readable medium may include non-transitory computer readable medium, for example, such as computer-readable media that stores data for short periods of time like register memory, processor cache and random access memory (RAM). The computer readable medium may also include non-transitory media, such as secondary or persistent long term storage, like read only memory (ROM), optical or magnetic disks, compact-disc read only memory (CD-ROM), for example. The computer readable media may also be any other volatile or non-volatile storage systems. The computer readable medium may be considered a computer readable storage medium, for example, or a tangible storage device.</p>
<p id="p-0039" num="0038">In addition, for the method <b>200</b> and other processes and methods disclosed herein, each block in <figref idref="DRAWINGS">FIG. 2</figref> may represent circuitry that is wired to perform the specific logical functions in the process.</p>
<p id="p-0040" num="0039">Initially, at block <b>201</b>, the method <b>200</b> includes receiving a query image. The query image may be an image collected and stored by a system which receives query images from users. In one example, the query image may be a user submitted image for which information is desired. A user may submit a query image in order to determine images matching the query image, or to determine information about the query image, for example. Images matching the query image may include metadata associated with and/or describing the matched images that may be output to the user. Additionally, the query image may be a query comprising multiple images.</p>
<p id="p-0041" num="0040">The query image may be of one or more image file formats. For example, the query image may be an image file composed of either pixel or vector data that is rasterized to pixels when displayed in a vector graphic display. Example image file formats include JPEG, JPEG 2000, TIFF, RAW, PNG, GIF, BMP, among other possible image file formats. Similarly, the query image may optionally be compressed using a variety of image compression techniques. In some examples, the method <b>200</b> includes decompressing the query image for use in conjunction with the method <b>200</b> and/or other systems and methods disclosed.</p>
<p id="p-0042" num="0041">At block <b>203</b>, the method <b>200</b> includes matching the query image to an object using a visual object recognition module. Given a query image, the visual object recognition module may retrieve and output one or more training images matching the query image. The visual object recognition module may utilize information from a plurality of training image corpora. For example, the plurality of training image corpora may include one or more training images associated with a plurality of objects.</p>
<p id="p-0043" num="0042">In one example, a query image may be an image of an object differing from training images of the object within the training corpus. The query image may, for example, be matched to a training image from the plurality of objects of the training corpus with a variation in illumination of the object. The query image may have been captured under different lighting settings compared with the illumination of the object within the training images of the object. Similarly, in another example, the query image may be an image of an object with a variation in pose or shape of the object as compared to the pose or shape of the object in a given training image.</p>
<p id="p-0044" num="0043">In another example, the visual object recognition module may match a region or multiple regions of a query image to one or more training images. For example, a query image may be processed to remove background information from the query image or split into multiple regions of separate objects. In one example, a number of similar image descriptors between a training image and the query image may be above a threshold. Accordingly, the training image may be determined to be a match to the query image. Other example methods and techniques used for image matching include template matching, normalized cross-correlation, or multi-variate cross-correlation.</p>
<p id="p-0045" num="0044">In yet another example, match scores may be associated with matches between the query image and one or more training images (or one or more matched regions of the one or more training images) of the plurality of objects. The match score may indicate a level of similarity between a query image and a matching training image (or matched region of a training image), for example. Matches may be prioritized or ranked based on the match scores. In one example, a training image associated with a maximum match score may be selected as a match the query image. In another example, the similarity may be computed based on a number of matched descriptors between a query image and a training image. However, other methods of determining similarity are also contemplated. In one example, an image distance measure may compare the similarity of two images in various dimensions such as color, texture, shape, and others. For example, color histograms of images may be compared, where a color histogram is computed by quantizing the colors within an image and counting the number of pixels of each color. In another example, an intensity-based method may be used to compare patterns in a query image and a training image using correlation metrics.</p>
<p id="p-0046" num="0045">At block <b>205</b>, the method <b>200</b> includes determining a matched region within a training image to which the query image matches using the visual object recognition module. For example, the visual object recognition module may also output a matched region and/or matched descriptors associated with a query image and one or more matching training images. In one example, the matched region may be determined by forming a boundary (e.g., a rectangle, circle, or free-form shape) around matched descriptors with a level of similarity or quality of match above a threshold. The various methods described above with reference to determining similarity between images (e.g., image distance measure, color methods, intensity-based methods, etc.) may also be applied at the image descriptor level. In another example, the query image may only match a portion of the training image. Similarly, a matched region within a training image may be determined by forming a boundary around matched descriptors. In a further example, multiple matched regions may be determined.</p>
<p id="p-0047" num="0046">At block <b>207</b>, the method <b>200</b> includes determining whether the matched region is located within an annotated image component of the training image. The annotated image component may be, for example, a sub-region of the training image including an annotation describing the sub-region. Additionally, a training image may include multiple annotated image components, and a determination may be made whether the matched region is located within one or more annotated image components.</p>
<p id="p-0048" num="0047">In one example, a tolerance may be used for determining whether the matched region is located within an annotated image component. The matched region may be determined to be located within an annotated image component if the matched region lies largely within the annotated image component. For example, the determination may be made based on a percentage of the matched region located within an annotated image component. Similarly, the determination may be made based on a number of matched descriptors of the matched region located within an annotated image component.</p>
<p id="p-0049" num="0048">At block <b>209</b>, a decision may be made based on the determination at block <b>207</b>. If the matched region is determined to be located within an annotated image component of the training image, block <b>211</b> of the method <b>200</b> may be executed. Alternatively, if the matched region is determined to not be located within an annotated image component, block <b>213</b> of the method <b>200</b> may be executed. For example, the matched region may match to an entirety of the training image or multiple regions of the training image.</p>
<p id="p-0050" num="0049">At block <b>211</b>, the method <b>200</b> includes identifying an annotation associated with the annotated image component. Accordingly, the annotation may be returned responsive to the query image being received. The query image may be determined to be a match to the image component, and metadata of the component may be identified and output instead of metadata of the object which the matching training image depicts.</p>
<p id="p-0051" num="0050">In one example, the annotation associated with the annotated image component may be identified when a percentage of the matched region located within the annotated image component is greater than a percentage threshold. For example, the percentage threshold may be 50%, 75%, 90%, or above 75%. Likewise, the annotation associated with the annotated image component may be identified when a number of descriptors of the matched region within the annotated image component is above a number threshold.</p>
<p id="p-0052" num="0051">At block <b>213</b>, the method <b>200</b> includes identifying an annotation associated with an entirety of the training image. Accordingly, the annotation may be returned responsive to the query image being received. The query image may be determined to be a match to the training image as a whole, and metadata of the object which the training image depicts may be identified and output.</p>
<p id="p-0053" num="0052">In one example, the annotation associated with the entirety of the training image may be identified when a percentage of the matched region located within the annotated image component is less than a percentage threshold. Likewise, the annotation associated with entirety of the training image may be identified when a number of descriptors of the matched region within the annotated image component is above a number threshold.</p>
<p id="p-0054" num="0053">According to an example of the method <b>200</b>, the query image may be received from a client device. The identified annotation associated with the image component of the training image may be sent to the client device when a determination may be made that, for example, the matched region is located within an annotated image component of the training image.</p>
<p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. 3A</figref> illustrates an example of a query image <b>301</b><i>a </i>and a matched training image <b>303</b>. The query image <b>301</b><i>a </i>may be matched to the training image <b>303</b>, for example, by a visual recognition module. The visual recognition module may also determine a matched region <b>305</b><i>a </i>within the training image <b>303</b> to which the query image <b>301</b><i>a </i>(or a region of the query image <b>301</b><i>a</i>) matches.</p>
<p id="p-0056" num="0055">In one example, a determination may be made whether the matched region <b>305</b><i>a </i>is located within an annotated image component <b>307</b> of the training image <b>303</b>. The annotated image component <b>307</b> may be a sub-region of the training image <b>303</b> and may include an associated component annotation <b>309</b> describing the sub-region. For example, the annotated image component <b>307</b> may be an image of a can of soda on the cover of a book about soda. The component annotation <b>309</b> of the annotated image component <b>307</b> may be metadata describing a logo on the can of soda.</p>
<p id="p-0057" num="0056">Although the matched region <b>305</b><i>a </i>in <figref idref="DRAWINGS">FIG. 3A</figref> may include the annotated image component <b>307</b>, the matched region <b>305</b><i>a</i>, as illustrated, does not lie within (or substantially within) the annotated image component <b>307</b>. Accordingly, a system, such as the system <b>100</b> of <figref idref="DRAWINGS">FIG. 1</figref>, responsive to receiving the query image <b>301</b><i>a</i>, may return an object annotation <b>311</b> associated with an object (e.g., a book titled &#x201c;A book about soda&#x201d;) described by the entirety of the training image <b>303</b>. In this example, the object annotation <b>311</b> may be identified rather than the component annotation <b>309</b>.</p>
<p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. 3B</figref> illustrates another example of a query image <b>301</b><i>b </i>and a matched training image <b>303</b>. The query image <b>301</b><i>b </i>may be matched to the training image <b>303</b>, for example, by a visual object recognition module. The visual object recognition module may also determine a matched region <b>305</b><i>b </i>within the training image <b>303</b> to which the query image <b>301</b><i>b </i>(or a region of the query image <b>301</b><i>b</i>) matches.</p>
<p id="p-0059" num="0058">In one example, a determination may be made whether the matched region <b>305</b><i>b </i>is located within annotated image component <b>307</b> of the training image <b>303</b>. The matched region <b>305</b><i>b </i>in <figref idref="DRAWINGS">FIG. 3B</figref> may lie within (or substantially within) the annotated image component <b>307</b>. Accordingly, a system, such as the system <b>100</b> of <figref idref="DRAWINGS">FIG. 1</figref>, responsive to receiving the query image <b>301</b><i>b</i>, may return the component annotation <b>309</b> associated with the annotated object component <b>307</b>. The component annotation <b>309</b> may be returned rather than the object annotation <b>311</b>. Thus, the query image <b>301</b><i>b </i>may be matched to an object component (or information associated with the object component, e.g., soda), rather than incorrectly matched to an object (or information associated with the object, e.g., book) with a sub-region depicting the query image <b>301</b><i>b. </i></p>
<p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. 4</figref> is an example block diagram of a method <b>400</b> to discover and annotate object components, in accordance with at least some embodiments described herein. The method <b>400</b> shown in <figref idref="DRAWINGS">FIG. 4</figref> presents an embodiment of a method that may, for example, be used by the system <b>100</b> of <figref idref="DRAWINGS">FIG. 1</figref>. Method <b>400</b> may include one or more operations, functions, or actions as illustrated by one or more of blocks <b>401</b>-<b>405</b>. Although the blocks are illustrated in a sequential order, these blocks may also be performed in parallel, and/or in a different order than those described herein. Also, the various blocks may be combined into fewer blocks, divided into additional blocks, and/or removed from the method, based upon the desired implementation of the method. Each block may represent a module, a segment, or a portion of program code, which includes one or more instructions executable by a processor for implementing specific logical functions or steps in the process. In addition, each block in <figref idref="DRAWINGS">FIG. 4</figref> may represent circuitry that is wired to perform the specific logical functions in the process.</p>
<p id="p-0061" num="0060">Initially, at block <b>401</b>, the method <b>400</b> includes receiving a plurality of training image corpora. The plurality of training image corpora may describe a set of objects. For each object, there may be one or more training images containing visual appearances of the object, as well as metadata associated with the object (e.g., type, name, etc.). The plurality of training image corpora may be utilized by the visual object recognition module to match images. For example, the visual object recognition module may be trained based on images within the plurality of training image corpora.</p>
<p id="p-0062" num="0061">At block <b>403</b>, the method <b>400</b> includes performing cross-corpus image region matching among the plurality of training image corpora. Common object components appearing in more than one image of the plurality of training image corpora may be discovered and identified. Common object components may include, for example, logos, famous artwork, letters in a specific font, etc., appearing on different objects represented within the plurality of training image corpora. The plurality of training image corpora may be compared to determine similarities, and object components with a level of similarity above a threshold may be determined to be common object components.</p>
<p id="p-0063" num="0062">In one example, a matcher or visual object recognition module may be built from all the training images of the plurality of training image corpora. Each training image may be matched using the matcher to identify common sub-structure that appears in more than one training image. For example, matched regions within matching training images of the plurality of training image corpora may be determined based on a similarity between image descriptors of the matching training images. In another example, optical character recognition (OCR) may be used to detect text within training images. Recognized characters associated with training images may be compared and common image components may be determined based on a comparison of recognized characters between training images.</p>
<p id="p-0064" num="0063">At block <b>405</b>, the method <b>400</b> includes, for matched regions within the plurality of training image corpora, annotating the matched regions as sub-regions within the training images comprising associated information. For example, the matcher may identify a matched region in a first training image matching a second training image. An annotation associated with the second training image may be associated with the matched region of the first training image. Similarly, the matcher may identify a matched region in a first training image matching a sub-region of second training image for which a component annotation is associated. The component annotation of the second training image may be associated with the matched region of the first training image.</p>
<p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. 5A</figref> is an example directed acyclic graph <b>500</b> of possible object component relationships. In <figref idref="DRAWINGS">FIG. 5A</figref>, a collection of vertices and directed edges are illustrated. However, the vertices and directed edges are not meant to be limiting, and are provided as examples modeling a structure. Structures associated with the systems and methods described herein may depart from the example illustrated in <figref idref="DRAWINGS">FIG. 5A</figref>, and may include more or less vertices and/or directed edges than illustrated.</p>
<p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. 5A</figref> includes vertices or nodes representing training corpora <b>501</b><i>a</i>-<i>d</i>. The training corpora <b>501</b><i>a</i>-<i>d </i>may comprise training images of a logo (<b>501</b><i>a</i>), a piece of artwork (<b>501</b><i>b</i>), a landmark (<b>501</b><i>c</i>), and a book (<b>501</b><i>d</i>) respectively.</p>
<p id="p-0067" num="0066">Additionally, the training corpora <b>501</b><i>a</i>-<i>d </i>may be connected by directed edges <b>503</b><i>a</i>-<i>e</i>. A directed edge <b>503</b><i>a</i>-<i>e </i>from a first training corpus to a second corpus may represent the presence of an image (or portion of an image) from the first training corpus within an image from the second corpus. For example, directed edge <b>503</b><i>a </i>may present the presence of an image from training corpus <b>501</b><i>a </i>(e.g., a logo) within an image from training corpus <b>501</b><i>c </i>(e.g., a landmark). Similarly, an image of a book cover from training corpus <b>501</b><i>d </i>may include a logo, a piece of artwork, or a landmark image.</p>
<p id="p-0068" num="0067">In one example, a visual object recognition module may match training images of first training corpus against training images of a second training corpus for each directed edge <b>503</b><i>a</i>-<i>e</i>. If a match is found (as in the example of <figref idref="DRAWINGS">FIG. 5B</figref>), the matching region of a training image from the second training corpus may be annotated with the metadata of the training image from the first training corpus.</p>
<p id="p-0069" num="0068"><figref idref="DRAWINGS">FIG. 5B</figref> illustrates an example of annotating an object component. Cross-corpus image region matching may be performed among the training corpora <b>501</b><i>a</i>-<i>d</i>. In one example, training images (or sub-regions of training images) from a first training corpus may be matched to training images of a second training corpus. For example, a first training image <b>507</b><i>d </i>from training corpus <b>501</b><i>d </i>may match to a second training image <b>507</b><i>a </i>from training corpus <b>501</b><i>a</i>. The first training image <b>507</b><i>d </i>and the second training image <b>507</b><i>a </i>may each include associated information. The first training image <b>507</b><i>d </i>may include associated metadata <b>509</b><i>d</i>, while the second training image <b>507</b><i>a </i>may include associated metadata <b>509</b><i>a. </i></p>
<p id="p-0070" num="0069">A matched region <b>511</b> within the first training image <b>507</b><i>d </i>may be determined for the match. As an example, the first training image <b>507</b><i>d </i>may be an image of a book cover and the matched region <b>511</b> may be a component of the book cover including an image of a logo. In one example, the matched region <b>511</b> may be annotated as a sub-region within the first training image <b>507</b><i>d </i>comprising associated information. For example, the matched region <b>511</b> may be annotated with metadata <b>509</b><i>a. </i></p>
<p id="p-0071" num="0070">In another example, multiple matched regions with the first training image <b>507</b><i>d </i>may be determined for the match. The multiple matched regions may each be annotated as sub-regions within the first training image <b>507</b><i>d </i>comprising associated information.</p>
<p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. 6</figref> is an example block diagram of a method <b>600</b> to assign a weighting factor to common image components, in accordance with at least some embodiments described herein. The method <b>600</b> shown in <figref idref="DRAWINGS">FIG. 6</figref> presents an embodiment of a method that may, for example, be used by the system <b>100</b> of <figref idref="DRAWINGS">FIG. 1</figref>. Method <b>600</b> may include one or more operations, functions, or actions as illustrated by one or more of blocks <b>601</b>-<b>607</b>. Although the blocks are illustrated in a sequential order, these blocks may also be performed in parallel, and/or in a different order than those described herein. Also, the various blocks may be combined into fewer blocks, divided into additional blocks, and/or removed from the method, based upon the desired implementation of the method. Each block may represent a module, a segment, or a portion of program code, which includes one or more instructions executable by a processor for implementing specific logical functions or steps in the process. In addition, each block in <figref idref="DRAWINGS">FIG. 6</figref> may represent circuitry that is wired to perform the specific logical functions in the process.</p>
<p id="p-0073" num="0072">Initially, at block <b>601</b>, the method <b>600</b> includes performing cross-corpus matching among the plurality of training image corpora to identify common image components appearing in more than one training image. Common object components appearing in more than one training image of the plurality of training image corpora may be discovered and identified.</p>
<p id="p-0074" num="0073">Common object components may include, for example, logos, famous artwork, letters in a specific font, etc., appearing on different objects represented within the plurality of training image corpora. Text within training images may be a common image component. Optionally, the text within multiple training images may be in similar fonts. Other sources of common object components appearing in multiple training images include: stock images reused in images of different objects; common patterns such as grids, stars, circles, etc.; and common parts such as a similar frame used in images of two different pieces of artwork.</p>
<p id="p-0075" num="0074">In one example, a matcher or visual object recognition module may be built from all the training images of the plurality of training image corpora. Each training image may be matched using the matcher to identify common sub-structure that appears in more than one training image.</p>
<p id="p-0076" num="0075">In another example, optical character recognition (OCR) may be used to detect text within training images. Recognized characters associated with training images may be compared and common image components may be determined based on a comparison of recognized characters between training images.</p>
<p id="p-0077" num="0076">In a further example, a matcher or visual object recognition module may be built from a database of stock images. The matcher may be used to detect reused image sub-structure in the training images as common image components.</p>
<p id="p-0078" num="0077">At block <b>603</b>, the method <b>600</b> includes identifying image descriptors appearing in more than one training image. Similar image descriptors within common image components may be determined. For example, an image descriptor within a common image component appearing in more than one training image may be annotated with additional information indicating that the image descriptor may be part of a common image component. In another example, an image descriptor within a common image component may be annotated with additional information indicating the number of training images in which the image descriptor appears.</p>
<p id="p-0079" num="0078">In a further example, image descriptors may be determined by examining an image, identifying image interest points (e.g., corners, text, edges, etc.) that include distinctive regions in the image, and assigning an identifier to the image interest point. In one instance, an image descriptor includes a numerical representation of pixels that fall within an image interest point (or image sub-region) of the image. In one example, image descriptors include descriptions of visual features of contents in the image, such as characteristics including a shape, a color, or a texture among others. The image descriptors may be compressed, and compressed image descriptors may be compared to identify similar image descriptors.</p>
<p id="p-0080" num="0079">For example, a technique for compressing image descriptors includes product quantization. Image descriptors may comprise a number of floating point numbers (e.g., referred to as the dimensionality of the descriptor). Product quantization may be performed to divide the dimensions into groups (e.g., such as evenly divided groups) and quantize or represent each group of dimensions using one of K exemplars. An exemplar may be chosen to minimize a sum of squared distances between the descriptor dimension and a nearest neighbor exemplar. Exemplars may be learned from unsupervised clustering algorithms, such as k-means, for example. The k-means clustering algorithm may include a method of cluster analysis that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean. The k-means clustering algorithm may attempt to find a center of natural clusters in the data.</p>
<p id="p-0081" num="0080">In one example, in which a descriptor has 100 dimensions (and thus may comprise 100 floating point values) product quantization may include dividing the 100 dimensions into groups, and each group may include four to five dimensions. Thus, the 100 dimensions may be divided into 20 groups with 5 dimensions per group (e.g., group 1 includes dimensions #1-5, group 2 includes dimensions #6-10, and so on). Product quantization represents each group separately using exemplars.</p>
<p id="p-0082" num="0081">In one example, a list of exemplars may be provided within a codebook. A closest exemplar to the group of dimensions is identified, and selected to represent the group. In the example above, group 1 including dimensions #1-5 (e.g., five floating point values) may be represented by a single exemplar. Thus, instead of representing or storing five floating point values, a single exemplar may be used to represent and store the group of dimensions. Using this method, the descriptor including 100 dimensions, which were divided into 20 groups can be represented by a data representation using 20 exemplars. Thus, the descriptor may be compressed from a data representation of 100 data point to 20 data points. The 20 data points may be compared across image descriptors of two or more potentially matching training images. In one example, if an image descriptor from a first training image matches an image descriptor from a second training image, the image descriptor may be annotated with an additional data point indicating the image descriptor may match to another training image. Similarly, the additional data point may be increased if the image descriptor may be found to be similar to image descriptors of more training images.</p>
<p id="p-0083" num="0082">At block <b>605</b>, the method <b>600</b> includes assigning a weighting factor to image descriptors associated with the common image components. Instead of treating all descriptors in an image equally, a weighting factor may be assigned to each of the image descriptors within the image. The weighting factor may affect a match score or measure of similarity between one or more image descriptors of training images. In one example, lowering the weighting factor of image descriptors within a common image component may have the effect of lowering match scores which (fully or partially) come from the common image component. Thus, matches may be suppressed in varying degrees depending on the portion of a match that comes from common image components.</p>
<p id="p-0084" num="0083">In one example, the weighting factor associated with all image descriptors of an image may be 1. Additionally, the weighting factor associated with descriptors determined to be associated with a common image component may be assigned a weighting factor of alpha, where alpha is less than 1. The value of the weighting factor alpha may be a fixed or predetermined value. In other examples, the value of the weighting factor alpha may be a variable, dependent on one or more factors of a system or method.</p>
<p id="p-0085" num="0084">In one example, the value of the weighting factor alpha may be determined experimentally. A plurality of matching experiments may be performed on a validation set using a series of weighting factor values for alpha. The validation set may be a set of query images including one or more common image components within the plurality of training image corpora, for example. Matching accuracies associated with results of the plurality of matching experiments may be determined. A matching accuracy may be determined for each value of alpha from the series of weight factor values of alpha. The matching accuracy may be determined by analyzing the results received for the set of query images. A value of alpha corresponding to the weighting factor yielding the maximum matching accuracy may be selected and used for image descriptors of common image components.</p>
<p id="p-0086" num="0085">In another example, weighting factors for image descriptors of a training image within a plurality of training image corpora may be determined using a term frequency-inverse document frequency (TF-IDF) formula. For example, the weighting factors for image descriptors of a training image may be determined using the following formula:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>(1/<i>x</i>)*log(<i>D/d</i>),<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
wherein x is a total number of image descriptors in the training image, D is a total number of training images in the plurality of training image corpora, and d is a total number of training images in which the image descriptor appears.
</p>
<p id="p-0087" num="0086">At block <b>607</b>, the method <b>600</b> includes determining adjusted match scores based on the weighting factor. Multiple approaches exist on how the weight of descriptors may be applied to obtain an adjust match score. One possible example includes determining an adjust match score given an original match score based on the following formula:</p>
<p id="p-0088" num="0087">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mrow>
  <mrow>
    <msup>
      <mi>S</mi>
      <mi>&#x2032;</mi>
    </msup>
    <mo>=</mo>
    <mrow>
      <mi>S</mi>
      <mo>*</mo>
      <mfrac>
        <msub>
          <mi>dw</mi>
          <mi>matched</mi>
        </msub>
        <msub>
          <mi>dw</mi>
          <mi>image</mi>
        </msub>
      </mfrac>
    </mrow>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
</maths>
<br/>
where S&#x2032; is an adjusted match score, S is a match score computed without assigning weighting factors to image descriptors, dw<sub>matched </sub>is a sum of weighting factors of matched image descriptors within a training image, and dw<sub>image </sub>is a sum of weighting factors of image descriptors within the training image. Thus, the match score may be adjusted according to descriptor weights within a training image. A match consisting of mainly low-weight descriptors, for example, may have a lower adjusted match score than original match score computed without assigning weighting factors.
</p>
<p id="p-0089" num="0088"><figref idref="DRAWINGS">FIG. 7A</figref> illustrates an example of common image components <b>701</b><i>a</i>, <b>701</b><i>b</i>. In the example, similar fonts may be used for the headings of two newspapers and may cause incorrect or irrelevant matches between images of the objects. The common image components <b>701</b><i>a</i>, <b>701</b><i>b </i>may appear within training images <b>703</b><i>a</i>, <b>703</b><i>b </i>respectively of the two newspapers. The common image components <b>701</b><i>a</i>, <b>701</b><i>b</i>, as illustrated may be characters of a common word (e.g., the characters &#x201c;mes&#x201d; of the word &#x201c;Times&#x201d;) appearing within training images <b>703</b><i>a</i>, <b>703</b><i>b. </i></p>
<p id="p-0090" num="0089"><figref idref="DRAWINGS">FIG. 7B</figref> illustrates another example of common image components <b>701</b><i>c</i>, <b>701</b><i>d</i>. A query image <b>705</b> may be matched to two similar training images <b>703</b><i>c</i>, <b>703</b><i>d </i>using a matcher or visual object recognition module. Matched regions within the training images <b>703</b><i>c</i>, <b>703</b><i>d </i>may be identified. However, the matched regions may be identified as common image components <b>701</b><i>c</i>, <b>701</b><i>d </i>which may not be specific to the query image <b>705</b>. In one example, the similar training images <b>703</b><i>c</i>, <b>703</b><i>d </i>may be determined not to be matches to the query image <b>705</b>. In another example, the ranking of a match score between the query image <b>705</b> and the similar training images <b>703</b><i>c</i>, <b>703</b><i>d </i>may be decreased or down-weighted based on the identification of common image components <b>701</b><i>c</i>, <b>701</b><i>d. </i></p>
<p id="p-0091" num="0090"><figref idref="DRAWINGS">FIG. 8</figref> is a functional block diagram illustrating an example computing device <b>800</b> used in a computing system that is arranged in accordance with at least some embodiments described herein. The computing device may be a personal computer, mobile device, cellular phone, touch-sensitive wristwatch, tablet computer, video game system, or global positioning system, and may be implemented to improve visual object recognition as described in <figref idref="DRAWINGS">FIGS. 1-7</figref>. In a basic configuration <b>802</b>, computing device <b>800</b> may typically include one or more processors <b>810</b> and system memory <b>820</b>. A memory bus <b>830</b> can be used for communicating between the processor <b>810</b> and the system memory <b>820</b>. Depending on the desired configuration, processor <b>810</b> can be of any type including but not limited to a microprocessor (&#x3bc;P), a microcontroller (&#x3bc;C), a digital signal processor (DSP), or any combination thereof. A memory controller <b>815</b> can also be used with the processor <b>810</b>, or in some implementations, the memory controller <b>815</b> can be an internal part of the processor <b>810</b>.</p>
<p id="p-0092" num="0091">Depending on the desired configuration, the system memory <b>820</b> can be of any type including but not limited to volatile memory (such as RAM), non-volatile memory (such as ROM, flash memory, etc.) or any combination thereof. System memory <b>820</b> may include one or more applications <b>822</b>, and program data <b>824</b>. Application <b>822</b> may include a component matching algorithm <b>823</b> that is arranged to provide inputs to the electronic circuits, in accordance with the present disclosure. Program data <b>824</b> may include content information <b>825</b> that could be directed to any number of types of data. In some example embodiments, application <b>822</b> can be arranged to operate with program data <b>824</b> on an operating system.</p>
<p id="p-0093" num="0092">Computing device <b>800</b> can have additional features or functionality, and additional interfaces to facilitate communications between the basic configuration <b>802</b> and any devices and interfaces. For example, data storage devices <b>840</b> can be provided including removable storage devices <b>842</b>, non-removable storage devices <b>844</b>, or a combination thereof. Examples of removable storage and non-removable storage devices include magnetic disk devices such as flexible disk drives and hard-disk drives (HDD), optical disk drives such as compact disk (CD) drives or digital versatile disk (DVD) drives, solid state drives (SSD), and tape drives to name a few. Computer storage media can include volatile and nonvolatile, non-transitory, removable and non-removable media implemented in any method or technology for storage of information, such as computer readable instructions, data structures, program modules, or other data.</p>
<p id="p-0094" num="0093">System memory <b>820</b> and storage devices <b>840</b> are examples of computer storage media. Computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by computing device <b>800</b>. Any such computer storage media can be part of device <b>800</b>.</p>
<p id="p-0095" num="0094">Computing device <b>800</b> can also include output interfaces <b>850</b> that may include a graphics processing unit <b>852</b>, which can be configured to communicate to various external devices such as display devices <b>860</b> or speakers via one or more A/V ports or a communication interface <b>880</b>. The communication interface <b>870</b> may include a network controller <b>872</b>, which can be arranged to facilitate communications with one or more other computing devices <b>780</b> over a network communication via one or more communication ports <b>874</b>. The communication connection is one example of a communication media. Communication media may be embodied by computer readable instructions, data structures, program modules, or other data in a modulated data signal, such as a carrier wave or other transport mechanism, and includes any information delivery media. A modulated data signal can be a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example, and not limitation, communication media can include wired media such as a wired network or direct-wired connection, and wireless media such as acoustic, radio frequency (RF), infrared (IR) and other wireless media.</p>
<p id="p-0096" num="0095">Computing device <b>800</b> can be implemented as a portion of a small-form factor portable (or mobile) electronic device such as a cell phone, a personal data assistant (PDA), a personal media player device, a wireless web-watch device, a personal headset device, an application specific device, or a hybrid device that include any of the above functions. Computing device <b>800</b> can also be implemented as a personal computer including both laptop computer and non-laptop computer configurations.</p>
<p id="p-0097" num="0096">In some embodiments, the disclosed methods may be implemented as computer program instructions encoded on a non-transitory computer-readable storage media in a machine-readable format, or on other non-transitory media or articles of manufacture. <figref idref="DRAWINGS">FIG. 9</figref> is a schematic illustrating a conceptual partial view of an example computer program product <b>900</b> that includes a computer program for executing a computer process on a computing device, arranged according to at least some embodiments presented herein. In one embodiment, the example computer program product <b>900</b> is provided using a signal bearing medium <b>901</b>. The signal bearing medium <b>901</b> may include one or more programming instructions <b>902</b> that, when executed by one or more processors may provide functionality or portions of the functionality described above with respect to <figref idref="DRAWINGS">FIGS. 1-8</figref>. Thus, for example, referring to the embodiments shown in <figref idref="DRAWINGS">FIG. 2</figref>, <figref idref="DRAWINGS">FIG. 4</figref>, and <figref idref="DRAWINGS">FIG. 6</figref> one or more features of blocks <b>201</b>-<b>213</b>, blocks <b>401</b>-<b>405</b>, and blocks <b>601</b>-<b>607</b> may be undertaken by one or more instructions associated with the signal bearing medium <b>901</b>.</p>
<p id="p-0098" num="0097">In some examples, the signal bearing medium <b>901</b> may encompass a computer-readable medium <b>903</b>, such as, but not limited to, a hard disk drive, a Compact Disc (CD), a Digital Video Disk (DVD), a digital tape, memory, etc. In some implementations, the signal bearing medium <b>901</b> may encompass a computer recordable medium <b>904</b>, such as, but not limited to, memory, read/write (R/W) CDs, R/W DVDs, etc. In some implementations, the signal bearing medium <b>901</b> may encompass a communications medium <b>905</b>, such as, but not limited to, a digital and/or an analog communication medium (e.g., a fiber optic cable, a waveguide, a wired communications link, a wireless communication link, etc.). Thus, for example, the signal bearing medium <b>901</b> may be conveyed by a wireless form of the communications medium <b>905</b> (e.g., a wireless communications medium conforming with the IEEE 902.11 standard or other transmission protocol).</p>
<p id="p-0099" num="0098">The one or more programming instructions <b>902</b> may be, for example, computer executable and/or logic implemented instructions. In some examples, a computing device such as the computing device <b>800</b> of <figref idref="DRAWINGS">FIG. 8</figref> may be configured to provide various operations, functions, or actions in response to the programming instructions <b>902</b> conveyed to the computing device <b>800</b> by one or more of the computer readable medium <b>903</b>, the computer recordable medium <b>904</b>, and/or the communications medium <b>905</b>.</p>
<p id="p-0100" num="0099">It should be understood that arrangements described herein are for purposes of example only. As such, those skilled in the art will appreciate that other arrangements and other elements (e.g. machines, interfaces, functions, orders, and groupings of functions, etc.) can be used instead, and some elements may be omitted altogether according to the desired results. Further, many of the elements that are described are functional entities that may be implemented as discrete or distributed components or in conjunction with other components, in any suitable combination and location.</p>
<p id="p-0101" num="0100">While various aspects and embodiments have been disclosed herein, other aspects and embodiments will be apparent to those skilled in the art. The various aspects and embodiments disclosed herein are for purposes of illustration and are not intended to be limiting, with the true scope being indicated by the following claims, along with the full scope of equivalents to which such claims are entitled. It is also to be understood that the terminology used herein is for the purpose of describing particular embodiments only, and is not intended to be limiting.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08625887-20140107-M00001.NB">
<img id="EMI-M00001" he="6.69mm" wi="76.20mm" file="US08625887-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US08625887-20140107-M00002.NB">
<img id="EMI-M00002" he="6.69mm" wi="76.20mm" file="US08625887-20140107-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method comprising:
<claim-text>receiving a query image;</claim-text>
<claim-text>matching the query image to an object using a visual object recognition module, wherein the visual object recognition module is configured to utilize information from a plurality of training image corpora to match the query image to a training image of the object;</claim-text>
<claim-text>determining a matched region within the training image to which the query image matches using the visual object recognition module; and</claim-text>
<claim-text>responsive to determining the matched region:
<claim-text>determining whether the matched region is located within an annotated image component of the training image, wherein the annotated image component is a sub-region of the training image and has an associated annotation describing the sub-region, and</claim-text>
<claim-text>when the matched region is located within the annotated image component of the training image, identifying an annotation associated with the annotated image component.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising identifying the annotation associated with the annotated image component when a percentage of the matched region located within the annotated image component is greater than a percentage threshold.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising identifying an annotation associated with an entirety of the training image when a percentage of the matched region located within the annotated image component is less than a percentage threshold.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>receiving the query image from a client device; and</claim-text>
<claim-text>sending the identified annotation associated with the annotated image component to the client device.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>receiving the plurality of training image corpora, wherein the plurality of training image corpora comprise a plurality of training images of a plurality of objects, wherein one or more training images comprise metadata related to the plurality of objects;</claim-text>
<claim-text>performing cross-corpus image region matching among the plurality of training image corpora; and</claim-text>
<claim-text>for matched regions within the plurality of training image corpora, annotating the matched regions as sub-regions within the training images comprising associated information.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising
<claim-text>matching the query image to one or more training images of the plurality of objects;</claim-text>
<claim-text>determining matched regions within the one or more training images to which the query image matches;</claim-text>
<claim-text>determining match scores indicating a level of similarity between the query image and the matched regions; and</claim-text>
<claim-text>selecting the training image associated with a maximum match score as a match to the query image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising:
<claim-text>performing cross-corpus matching among the plurality of training image corpora to identify common image components appearing in more than one training image;</claim-text>
<claim-text>identifying image descriptors appearing in more than one training image, wherein image descriptors comprise information associated with image interest points within a training image;</claim-text>
<claim-text>assigning a weighting factor to image descriptors associated with the common image components, wherein the weighting factor influences the match scores between the query image and the matched regions; and</claim-text>
<claim-text>determining adjusted match scores based on the weighting factor.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the common image components comprise one or more of the following identified within multiple training images:
<claim-text>similar text, similar fonts, similar patterns, and stock images.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising:
<claim-text>performing a plurality of matching experiments on a validation set using a series of weighting factors, wherein the validation set is a set of query images comprising one or more of the common image components;</claim-text>
<claim-text>determining matching accuracies associated with results of the plurality of matching experiments performed using the series of weighting factors; and</claim-text>
<claim-text>selecting the weighting factor yielding a maximum matching accuracy.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising:
<claim-text>assigning a weighting factor to image descriptors of the training images, wherein image descriptors comprise information associated with image interest points within a training image; and</claim-text>
<claim-text>determining adjusted match scores from the visual object recognition module based on the weighting factor.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the weighting factors are computed using the following formula:
<claim-text>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>(1/<i>x</i>)*log(<i>D/d</i>),<?in-line-formulae description="In-line Formulae" end="tail"?>
</claim-text>
<claim-text>wherein x is a total number of image descriptors in the training image, D is a total number of training images in the plurality of training image corpora, and d is a total number of training images in which the image descriptor appears.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, further comprising the visual object recognition module computing the adjusted match scores using the following formula:</claim-text>
<claim-text>
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mrow>
  <mrow>
    <msup>
      <mi>S</mi>
      <mi>&#x2032;</mi>
    </msup>
    <mo>=</mo>
    <mrow>
      <mi>S</mi>
      <mo>*</mo>
      <mfrac>
        <msub>
          <mi>dw</mi>
          <mi>matched</mi>
        </msub>
        <msub>
          <mi>dw</mi>
          <mi>image</mi>
        </msub>
      </mfrac>
    </mrow>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
</maths>
<claim-text>wherein S&#x2032; is an adjusted match score, S is a match score computed without assigning weighting factors to image descriptors, dw<sub>matched </sub>is a sum of weighting factors of matched image descriptors within a training image, and dw<sub>image </sub>is a sum of weighting factors of image descriptors within the training image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein performing cross-corpus matching further comprises:
<claim-text>using optical character recognition (OCR) to recognize characters in the plurality of training image corpora; and</claim-text>
<claim-text>comparing the recognized characters to identify common image components appearing in more than one training image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. A non-transitory computer readable medium having stored therein instructions executable by a computing device to cause the computing device to perform functions of:
<claim-text>receiving a query image;</claim-text>
<claim-text>matching the query image to an object using a visual object recognition module, wherein the visual object recognition module is configured to utilize information from a plurality of training image corpora to match the query image to a training image of the object;</claim-text>
<claim-text>determining a matched region within the training image to which the query image matches using the visual object recognition module; and</claim-text>
<claim-text>responsive to determining the matched region:
<claim-text>determining whether the matched region is located within an annotated image component of the training image, wherein the annotated image component is a sub-region of the training image and has an associated annotation describing the sub-region, and</claim-text>
<claim-text>when the matched region is located within the annotated image component of the training image, identifying an annotation associated with the annotated image component.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The computer readable medium of <claim-ref idref="CLM-00014">claim 14</claim-ref>, further comprising instructions to perform functions of:
<claim-text>receiving a plurality of training image corpora, wherein the plurality of training image corpora comprise a plurality of training images of a plurality of objects, wherein one or more training images comprise metadata related to the plurality of objects;</claim-text>
<claim-text>performing cross-corpus image region matching among the plurality of training image corpora; and</claim-text>
<claim-text>for matched regions within the plurality of training image corpora, annotating the matched regions as sub-regions within the training images comprising associated information.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The computer readable medium of <claim-ref idref="CLM-00014">claim 14</claim-ref>, further comprising instructions to perform functions of:
<claim-text>matching the query image to one or more training images of the plurality of objects;</claim-text>
<claim-text>determining matched regions within the one or more training images to which the query image matches;</claim-text>
<claim-text>performing cross-corpus matching among the plurality of training image corpora to identify common image components appearing in more than one training image;</claim-text>
<claim-text>identifying image descriptors appearing in more than one training image, wherein image descriptors comprise information associated with image interest points within a training image;</claim-text>
<claim-text>assigning a weighting factor to image descriptors associated with the common image components, wherein the weighting factor influences match scores between the query image and the matched regions;</claim-text>
<claim-text>determining match scores based on the weighting factor; and</claim-text>
<claim-text>selecting the training image associated with a maximum match score as a match to the query image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. A system comprising:
<claim-text>a memory;</claim-text>
<claim-text>a processor coupled to the memory; and</claim-text>
<claim-text>instructions stored in the memory and executable by the processor to perform functions of:
<claim-text>receiving a query image;</claim-text>
<claim-text>matching the query image to an object using a visual object recognition module, wherein the visual object recognition module is configured to utilize information from the plurality of training image corpora to match the query image to a training image of the object;</claim-text>
<claim-text>determining a matched region within the training image to which the query image matches using the visual object recognition module; and</claim-text>
<claim-text>responsive to determining the matched region:
<claim-text>determining whether the matched region is located within an annotated image component of the training image, wherein the annotated image component is a sub-region of the training image and has an associated annotation describing the sub-region, and</claim-text>
<claim-text>when the matched region is located within the annotated image component of the training image, identifying an annotation associated with the annotated image component.</claim-text>
</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, further comprising instructions for:
<claim-text>receiving the plurality of training image corpora, wherein the plurality of training image corpora comprise a plurality of training images of a plurality of objects, wherein one or more training images comprise metadata related to the plurality of objects;</claim-text>
<claim-text>performing cross-corpus image region matching among the plurality of training image corpora; and</claim-text>
<claim-text>for matched regions within the plurality of training image corpora, annotating the matched regions as sub-regions within the training images comprising associated information.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, further comprising instructions for:
<claim-text>matching the query image to one or more training images of the plurality of objects;</claim-text>
<claim-text>determining matched regions within the one or more training images to which the query image matches;</claim-text>
<claim-text>performing cross-corpus matching among the plurality of training image corpora to identify common image components appearing in more than one training image;</claim-text>
<claim-text>identifying image descriptors appearing in more than one training image, wherein image descriptors comprise information associated with image interest points within a training image;</claim-text>
<claim-text>assigning a weighting factor to image descriptors associated with the common image components, wherein the weighting factor influences match scores between the query image and the matched regions;</claim-text>
<claim-text>determining match scores based on the weighting factor; and</claim-text>
<claim-text>selecting the training image associated with a maximum match score as a match to the query image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the query image only matches a portion of the training image. </claim-text>
</claim>
</claims>
</us-patent-grant>
