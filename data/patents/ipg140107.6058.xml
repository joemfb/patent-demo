<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08627181-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08627181</doc-number>
<kind>B1</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13826205</doc-number>
<date>20130314</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>11</class>
<subclass>C</subclass>
<main-group>29</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>714773</main-classification>
</classification-national>
<invention-title id="d2e43">Storage apparatus, storage controller, and method for managing locations of error correcting code blocks in array</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5548711</doc-number>
<kind>A</kind>
<name>Brant et al.</name>
<date>19960800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>714  511</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5911779</doc-number>
<kind>A</kind>
<name>Stallmo et al.</name>
<date>19990600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>714  612</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>2010/0005228</doc-number>
<kind>A1</kind>
<name>Fukutomi et al.</name>
<date>20100100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>2010/0057988</doc-number>
<kind>A1</kind>
<name>Okamoto et al.</name>
<date>20100300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>2012/0005402</doc-number>
<kind>A1</kind>
<name>Yamamoto et al.</name>
<date>20120100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>JP</country>
<doc-number>2010-15516</doc-number>
<date>20100100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>JP</country>
<doc-number>2010/55247</doc-number>
<date>20100300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>JP</country>
<doc-number>2012-14415</doc-number>
<date>20120100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>WO</country>
<doc-number>WO 2011/010344</doc-number>
<kind>A1</kind>
<date>20110100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>20</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>714773</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>13</number-of-drawing-sheets>
<number-of-figures>14</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>PCT/JP2012/073339</doc-number>
<date>20120912</date>
</document-id>
<parent-status>PENDING</parent-status>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13826205</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only" applicant-authority-category="assignee">
<addressbook>
<orgname>Kabushiki Kaisha Toshiba</orgname>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only" applicant-authority-category="assignee">
<addressbook>
<orgname>Toshiba Solutions Corporation</orgname>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Ito</last-name>
<first-name>Tatsuya</first-name>
<address>
<city>Tama</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Finnegan, Henderson, Farabow, Garrett &#x26; Dunner, LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Kabushiki Kaisha Toshiba</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
<assignee>
<addressbook>
<orgname>Toshiba Solutions Corporation</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Decady</last-name>
<first-name>Albert</first-name>
<department>2112</department>
</primary-examiner>
<assistant-examiner>
<last-name>Ahmed</last-name>
<first-name>Enam</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">According to one embodiment, a storage controller of a storage apparatus includes an array management unit managing an array. The array includes a plurality of stripe groups. A set of an error correcting code block and a plurality of data blocks is arranged in each of the plurality of stripe groups. Each of the plurality of stripe groups includes a set of stripe blocks of which physical positions correspond to each other in the plurality of solid state drives. The array management unit regularly arranges the error correcting code blocks and the data blocks in the plurality of stripe groups such that the numbers of the arranged error correcting code blocks are non-uniform among the plurality of solid state drives.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="213.19mm" wi="153.75mm" file="US08627181-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="207.60mm" wi="138.35mm" file="US08627181-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="224.03mm" wi="152.57mm" file="US08627181-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="228.01mm" wi="166.37mm" file="US08627181-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="233.26mm" wi="168.57mm" file="US08627181-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="210.23mm" wi="138.09mm" file="US08627181-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="136.40mm" wi="129.12mm" file="US08627181-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="160.44mm" wi="139.36mm" file="US08627181-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="233.93mm" wi="152.91mm" file="US08627181-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="237.57mm" wi="160.78mm" file="US08627181-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="228.01mm" wi="86.95mm" orientation="landscape" file="US08627181-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="240.88mm" wi="168.06mm" file="US08627181-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="241.47mm" wi="171.37mm" file="US08627181-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="240.54mm" wi="165.10mm" file="US08627181-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application is a Continuation Application of PCT Application No. PCT/JP2012/073339, filed Sep. 12, 2012, the entire contents of which are incorporated herein by reference.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">FIELD</heading>
<p id="p-0003" num="0002">Embodiments described herein relate generally to a storage apparatus, a storage controller, and a method for managing locations of error correcting code blocks in an array.</p>
<heading id="h-0003" level="1">BACKGROUND</heading>
<p id="p-0004" num="0003">Conventionally, storage devices each including a disk array are known. Generally, the disk array is constructed by RAID (redundant arrays of independent disks or redundant arrays of inexpensive disks) using a plurality of hard disk drives (HDD). Such a disk array (hereinafter, referred to as a first array) has redundancy by applying error correcting code data such as parity data.</p>
<p id="p-0005" num="0004">In addition, in recent years, storage devices each including an array (hereinafter, referred to as a second array) that comprises a plurality of solid state drives (SSD) appear. The SSD comprises non-volatile rewritable memories such as NAND-type flash memories. Accordingly, the access performance of the SSD is higher than that of the HDD accompanied by a mechanical operation in reading/writing data. However, the life of an SSD, for example, depends on the number of data write operations to the SSD.</p>
<p id="p-0006" num="0005">In the first array, for load distribution, generally, error correcting code blocks (for example, parity blocks) are arranged by being uniformly distributed on the plurality of HDDs in units of areas called stripe groups. Thus, also in the second array, the error correcting code blocks are assumed to be arranged by being uniformly distributed on the plurality of SSDs in units of stripe groups. In such a case, the numbers of data write operations to the plurality of SSDs are uniformized, and there is a high possibility that the plurality of SSDs fail at the same time. Accordingly, in the second array, it is required that the numbers of data write operations to the plurality of SSDs are non-uniform.</p>
<p id="p-0007" num="0006">Thus, according to a conventional technology, a controller (storage controller) in the storage device monitors the numbers of data write operations to the plurality of SSDs. As a result of the monitoring, in a case where the numbers of writing needs to be non-uniform, the controller dynamically changes the locations of the error correcting code blocks and the locations of the data blocks in units of stripe groups. For the change of the locations, the controller manages the locations of the error correcting code blocks and the locations of the data blocks for each stripe group, using a map table.</p>
<p id="p-0008" num="0007">However, according to the conventional technology, the memory capacity that is necessary for storing the map table tends to increase in accordance with a recent increase in the capacity of the array. In addition, in a case where the changes in the locations of the error correcting code blocks and the locations of the data blocks are excessively performed, the number of data write operations to each SSD increases in accordance with the changes.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram showing an exemplary hardware configuration of a storage system according to an embodiment;</p>
<p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram showing the functional configuration of a storage controller shown in <figref idref="DRAWINGS">FIG. 1</figref>;</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 3</figref> is a diagram showing an example of the arrangement of a parity block and data blocks for each stripe group in a solid state drive (SSD) array of RAID level 5;</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 4</figref> is a diagram showing an example of the data structure of array management tables applied to the embodiment;</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 5</figref> is a diagram showing, in association with the array management tables, an example of the arrangement of parity blocks and data blocks in an SSD array applied to the embodiment;</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 6</figref> is a flowchart illustrating an exemplary procedure of a parity block location calculation process applied to the embodiment;</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 7</figref> is a flowchart illustrating an exemplary procedure of a data block location calculation process applied to the embodiment;</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 8A</figref> is a diagram showing a part of a flowchart illustrating an exemplary procedure of a block specifying process applied to the embodiment;</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 8B</figref> is a diagram showing the remaining part of the flowchart illustrating the exemplary procedure of the block specifying process;</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 9</figref> is a diagram showing, in association with the array management tables, an example of the state after completion of the rearrangement of parity blocks and data blocks in block rearrangement applied to the embodiment;</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 10</figref> is a diagram showing an example of parameter value switching according to the embodiment in a table form;</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 11</figref> is a flowchart illustrating an exemplary procedure of a rearrangement completion/rearrangement incompletion determination process applied to the embodiment;</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 12</figref> is a diagram showing, in association with the array management tables, an example of the state in the middle of the rearrangement of parity blocks and data blocks in the block rearrangement applied to the embodiment; and</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 13</figref> is a diagram showing, in association with the array management tables, an example of the state after completion of the rearrangement of parity blocks and data blocks in the block rearrangement applied to a modification of the embodiment.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0023" num="0022">Various embodiments will be described hereinafter with reference to the accompanying drawings.</p>
<p id="p-0024" num="0023">In general, according to one embodiment, a storage apparatus comprises a group of solid state drives and a storage controller. The group of solid state drives includes a plurality of solid state drives. Each of the plurality of solid state drives comprises a first storage area divided into a plurality of stripe blocks. The storage controller is configured to control the group of the solid state drives. The storage controller comprises an array management unit configured to manage an array. The array comprises a second storage area divided into a plurality of stripe groups. A set of an error correcting code block used for storing an error correcting code and a plurality of data blocks used for storing data is arranged in each of the plurality of stripe groups. Each of the plurality of stripe groups comprises a set of the stripe blocks of which physical positions correspond to each other in the plurality of solid state drives. The array management unit is configured to regularly arrange the error correcting code blocks and the data blocks in the plurality of stripe groups based on a predetermined arrangement rule such that the numbers of the arranged error correcting code blocks are non-uniform among the plurality of solid state drives. The array management unit is configured to manage locations of the error correcting code blocks based on the arrangement rule.</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram showing an exemplary hardware configuration of a storage system according to an embodiment. The storage system comprises a storage device <b>1</b>, a-host computer (hereinafter, referred to as a host) <b>2</b>, and a network <b>3</b>. The storage device <b>1</b> is connected to the host <b>2</b> via the network <b>3</b>. The host <b>2</b> uses the storage device <b>1</b> as an external storage device. The network <b>3</b>, for example, is a storage area network (SAN), the Internet, or an intranet. The Internet or the intranet, for example, is configured using Ethernet (registered trademark).</p>
<p id="p-0026" num="0025">The storage device <b>1</b> includes a plurality of solid state drives (SSDs), for example, seven SSDs <b>11</b>-<b>0</b> to <b>11</b>-<b>6</b> (SSD-#<b>0</b> to SSD-#<b>6</b>), a storage controller <b>12</b>, and a drive interface bus <b>13</b>. Each of the SSDs <b>11</b>-<b>0</b> to <b>11</b>-<b>6</b> is configured by a set of rewritable non-volatile memories (for example, flash memories). The SSDs <b>11</b>-<b>0</b> to <b>11</b>-<b>6</b> are connected to the drive interface bus <b>13</b> via drive slots #<b>0</b> to #<b>6</b>. Specific drive slot numbers <b>0</b> to <b>6</b> are allocated to the drive slots #<b>0</b> to #<b>6</b>. The drive slot numbers <b>0</b> to <b>6</b> are serial numbers that are fixedly allocated to the drive slots #<b>0</b> to #<b>6</b>. In the description presented below, the drive slot numbers <b>0</b> to <b>6</b> are denoted by SSD-#<b>0</b> to SSD-#<b>6</b> for convenience sake. SSD-#<b>0</b> to SSD-#<b>6</b> also indicate that the attributes of the SSDs <b>11</b>-<b>0</b> to <b>11</b>-<b>6</b> connected to the drive slots #<b>0</b> to #<b>6</b> represent SSDs (that is, the elements of an SSD array <b>110</b>-*) used in the SSD array <b>110</b>-* to be described later. In addition, in the description presented below, the SSDs <b>11</b>-<b>0</b> to <b>11</b>-<b>6</b> connected to the drive slots #<b>0</b> to #<b>6</b> may be denoted by SSD-#<b>0</b> to SSD-#<b>6</b>.</p>
<p id="p-0027" num="0026">The storage controller <b>12</b> is connected to the SSDs <b>11</b>-<b>0</b> to <b>11</b>-<b>6</b> (SSD-#<b>0</b> to SSD-#<b>6</b>) via the drive interface bus <b>13</b>. The interface type of the drive interface bus <b>13</b>, for example, is a small computer system interface (SCSI), a fibre channel (FC), a serial attached SCSI (SAS), or a serial AT attachment (SATA).</p>
<p id="p-0028" num="0027">The storage controller <b>12</b> controls the SSDs <b>11</b>-<b>0</b> to <b>11</b>-<b>6</b>. The storage controller <b>12</b> configures and manages each of arrays (hereinafter, referred to as SSD arrays), which have redundancy, using three or more of the SSDs <b>11</b>-<b>0</b> to <b>11</b>-<b>6</b>. In the example shown in <figref idref="DRAWINGS">FIG. 1</figref>, three SSD arrays <b>110</b>-<b>0</b> to <b>110</b>-<b>2</b> are shown as the arrays having redundancy. The SSD arrays <b>110</b>-<b>0</b> to <b>110</b>-<b>2</b>, for example, are arrays (that is, RAID arrays) having the RAID configuration. The RAID arrays are managed as logical drives. In the description presented below, when the SSD arrays <b>110</b>-<b>0</b> to <b>110</b>-<b>2</b> do not need to be particularly discriminated from one another, each of the SSD arrays <b>110</b>-<b>0</b> to <b>110</b>-<b>2</b> will be denoted by an SSD array <b>110</b>-*. Similarly, when the SSDs <b>11</b>-<b>0</b> to <b>11</b>-<b>6</b> do not need to be particularly discriminated from one another, each of the SSDs <b>11</b>-<b>0</b> to <b>11</b>-<b>6</b> will be denoted by an SSD <b>11</b>-*.</p>
<p id="p-0029" num="0028">The storage controller <b>12</b> includes a host interface (host I/F) <b>121</b>, a drive interface (drive I/F) <b>122</b>, a cache memory <b>123</b>, a cache controller <b>124</b>, a flash ROM (FROM) <b>125</b>, a local memory <b>126</b>, a CPU <b>127</b>, a chipset <b>128</b>, and an internal bus <b>129</b>. The storage controller <b>12</b> is connected to the host <b>2</b> by the host I/F <b>121</b> via the network <b>3</b>. The interface type of the host I/F <b>121</b>, for example, is an FC or an Internet SCSI (iSCSI).</p>
<p id="p-0030" num="0029">The host I/F <b>121</b> controls data transmission (data transmission protocol) to or from the host <b>2</b>. The host I/F <b>121</b> receives a data access request (a read request or a write request) for a logical volume, which is supplied from the host <b>2</b>, and replies with a response to the data access request. The logical volume is logically implemented using a part of the data storage area in the SSD array <b>110</b>-* as an entity. When the data access request is received from the host <b>2</b>, the host I/F <b>121</b> transfers the request to the CPU <b>127</b> via the internal bus <b>129</b> and the chipset <b>128</b>. The CPU <b>127</b> that has received the data access request processes the data access request based on a storage control program to be described later.</p>
<p id="p-0031" num="0030">When the data access request is a write request, the CPU <b>127</b> specifies a data area of the SSD array <b>110</b>-* that corresponds to an access area (an access area in the logical volume) designated by the write request and controls data writing. More specifically, the CPU <b>127</b> controls first data writing or second data writing. The first data writing is an operation of storing write data in the cache memory <b>123</b> once and then writing the data to the specified area of the SSD array <b>110</b>-*. The second data writing is an operation of directly writing write data to the specified area. In the embodiment, it is assumed that the first data writing is performed.</p>
<p id="p-0032" num="0031">On the other hand, when the data access request is a read request, the CPU <b>127</b> specifies a data area of the SSD array <b>110</b>-* that corresponds to an access area designated by the read request and controls data reading. More specifically, the CPU <b>127</b> controls first data reading or second data reading. The first data reading is performed in a case where data of the specified data area is stored in the cache memory <b>123</b>. That is, the first data reading is an operation of reading the data of the specified physical area from the cache memory <b>123</b> and replying with the read data to the host I/F <b>121</b>, thereby causing the host I/F <b>121</b> to reply with the read data to the host <b>20</b>. The second data reading is performed in a case where the data of the specified physical area is not stored in the cache memory <b>123</b>. That is, the second data reading is an operation of reading the data from the specified physical area of the disk array <b>110</b>-* and immediately replying with the read data to the host I/F <b>121</b>, thereby causing the host I/F <b>121</b> to reply with the read data to the host <b>20</b>.</p>
<p id="p-0033" num="0032">The drive I/F <b>122</b> transmits a write request or a read request for an SSD <b>11</b>-* of the SSD array <b>110</b>-* in accordance with a data access request (a write request or a read request for a logical volume) from the host <b>2</b>, which has been received by the CPU <b>127</b> (storage control program), and receives a reply thereto. When a data access request from the host <b>2</b> is received by the host I/F <b>121</b>, the cache memory <b>123</b> is used as a buffer for speeding up a reply of the completion to the data access request (a write request or a read request).</p>
<p id="p-0034" num="0033">When the data access request is a write request, the CPU <b>127</b> completes the write process by storing write data in the cache memory <b>123</b> once using the cache controller <b>124</b> and replies with a response to the host <b>2</b>. Thereafter, the CPU <b>127</b> writes the write data to the SSD array <b>110</b>-* at an arbitrary timing. Then, the CPU <b>127</b> frees the storage area of the cache memory <b>123</b> in which the write data is stored, using the cache controller <b>124</b>. On the other hand, when the data access request is a read request, in a case where the requested data (that is, data to be read) is stored in the cache memory <b>123</b>, the CPU <b>127</b> operates as follows. The CPU <b>127</b> acquires the requested data from the cache memory <b>123</b>, using the cache controller <b>124</b> and replies with a response to the host <b>2</b> (first data reading).</p>
<p id="p-0035" num="0034">The cache controller <b>124</b> reads data from the cache memory <b>123</b> in accordance with a command supplied from the CPU <b>127</b> (storage control program). In addition, the cache controller <b>124</b> writes data to the cache memory <b>123</b> in accordance with a command supplied from the CPU <b>127</b>.</p>
<p id="p-0036" num="0035">The FROM <b>125</b> is a rewritable non-volatile memory. The FROM <b>125</b> is used for storing a storage control program executed by the CPU <b>127</b>. As a first process performed when the storage controller <b>12</b> is started up, the CPU <b>127</b> copies a storage control program stored in the FROM <b>125</b> to the local memory <b>126</b>. Here, a non-volatile memory dedicated for reading data, for example, a ROM may be used instead of the FROM <b>125</b>.</p>
<p id="p-0037" num="0036">The local memory <b>126</b> is a volatile memory the data of which can be rewritten, such as a DRAM. A part of the area of the local memory <b>126</b> is used for storing the storage control program copied from the FROM <b>125</b>. On the other hand, the other part of the area of the local memory <b>126</b> is used as a work area for the CPU <b>127</b>. The CPU <b>127</b> controls the entire storage device <b>1</b> (especially, each unit of the storage controller <b>12</b>) in accordance with program codes of the storage control program stored in the local memory <b>126</b>. That is, the CPU <b>127</b> executes the storage control program stored in the local memory <b>126</b> via the chipset <b>128</b>, thereby controlling the entire storage device <b>1</b>.</p>
<p id="p-0038" num="0037">The chipset <b>128</b> is a bridge circuit that connects the CPU <b>127</b> and peripheral circuits thereof to the internal bus <b>129</b>. The internal bus <b>129</b> is a universal bus and, for example, is a peripheral component interconnect (PCI) express bus. The host I/F <b>121</b>, the drive I/F <b>122</b>, and the chipset <b>128</b> are interconnected via the internal bus <b>129</b>. In addition, the cache controller <b>124</b>, the FROM <b>125</b>, the local memory <b>126</b>, and the CPU <b>127</b> are connected to the internal bus <b>129</b> via the chipset <b>128</b>.</p>
<p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram showing the functional configuration of the storage controller <b>12</b> shown in <figref idref="DRAWINGS">FIG. 1</figref>. This functional configuration is implemented by the CPU <b>127</b> of the storage controller <b>12</b> shown in <figref idref="DRAWINGS">FIG. 1</figref> executing the storage control program. The storage controller <b>12</b> includes an array management unit <b>21</b>, a parity block arrangement calculation unit <b>22</b>, a data block arrangement calculation unit <b>23</b>, a block specifying unit <b>24</b>, a drive replacement controller <b>25</b>, a block rearrangement determination unit <b>26</b>, the array access controller <b>27</b>, an array management table <b>28</b> (#<b>0</b>), and an array management table <b>29</b> (#<b>1</b>). The array management unit <b>21</b> includes an array generation unit <b>210</b>. The drive replacement controller <b>25</b> includes a block rearrangement unit <b>250</b>. Such elements will be described later.</p>
<p id="p-0040" num="0039">The SSD array <b>110</b>-*, as described above, comprises three or more SSDs <b>11</b>-* and is managed as one logical drive called a RAID array using the RAID technology. The SSD <b>11</b>-* has advantages that data stored in the SSD <b>11</b>-* (a flash memory included in the SSD <b>11</b>-*) is non-volatile, and the data transmission speed is much higher than that of the HDD. On the other hand, for the SSD <b>11</b>-*, there is an upper limit of the number of data write operations to the SSD <b>11</b>-* (more specifically, the number of times data write operations to the SSD <b>11</b>-* can be performed). That is, the life of the SSD <b>11</b>-* depends on the number of data write operations, and the SSD <b>11</b>-* is inferior to the HDD in this viewpoint.</p>
<p id="p-0041" num="0040">Here, a RAID level applied to the SSD array <b>110</b>-* will be described. As a RAID level that is generally employed in an RAID array constructed using a plurality of HDDs (more specifically, three or more HDDs), RAID level 5 is known. In the RAID level 5, for example, parity data is arranged by being uniformly distributed into a plurality of HDDs in units of areas called stripe groups.</p>
<p id="p-0042" num="0041">In the RAID array of the RAID level 5, even when one HDD included in the RAID array fails, the data of the HDD that fails can be restored based on data of the other HDDs. In addition, in the RAID array of the RAID level 5, every time data is written to an area arranged in a stripe group, the rewriting of parity data included in the stripe group occurs. However, in the RAID array of the RAID level 5, by arranging the parity data to be distributed uniformly in units of stripe groups, the concentration of accesses (load) to a specific HDD can be prevented. That is, degradation of the response performance according to the RAID control for write accesses to the RAID array can be suppressed to a minimum level.</p>
<p id="p-0043" num="0042">Thus, it is assumed that the SSD array <b>110</b>-* is constructed using the RAID level 5 technology. In such a case, by arranging parity data to be uniformly distributed in units of stripe groups, the number of data write operations for each SSD <b>11</b>-* included in the SSD array <b>110</b>-* tends to be uniformized. As a result thereof, in a plurality of SSDs <b>11</b>-* included in the SSD array <b>110</b>-*, there is a high probability that the numbers of data write operations to the plurality of SSDs <b>11</b>-* reach the upper limit at the same time. If so, a situation equivalent to a case in which a plurality of SSDs fail at the same time in the SSD array <b>110</b>-* occurs, and accordingly, the redundancy of the SSD array <b>110</b>-* cannot be maintained.</p>
<p id="p-0044" num="0043">The storage control program executed by the CPU <b>127</b> includes first to fifth functions described below. The first function is a function of counting the number of data write operations to each SSD <b>11</b>-*. The second function is a function of monitoring the number of data write operations to the SSD <b>11</b>-* that is indicated by writing number information supplied by each SSD <b>11</b>-*.</p>
<p id="p-0045" num="0044">The third function is a function using a monitoring notification function called self monitoring and reporting technology (SMART) included in each SSD <b>11</b>-*. This monitoring notification function includes a function of notifying that the number of data write operations to the SSD <b>11</b>-* is close to the upper limit as an indication of disorder using the SSD <b>11</b>-*.</p>
<p id="p-0046" num="0045">The fourth function is a function of detecting an SSD <b>11</b>-* to which the number of data write operations is close to the upper limit as a first SSD having a highest probability of the occurrence of fail, using one of the first to third functions. The fifth function is a drive preventive maintenance for replacing the first SSD with another second SSD while the SSD array <b>110</b>-* is maintained in the state of being accessible. The fifth function corresponds to the function of the drive replacement controller <b>25</b> to be described later and includes the following sixth to ninth functions.</p>
<p id="p-0047" num="0046">The sixth function is a function of saving the data stored in the first SSD to a hot spare SSD that is in a standby state in advance. The seventh function is a function of allowing the first SSD to be in the state of being separated from the SSD array <b>110</b>-* by causing the hot spare SSD to temporarily serve as a replacement SSD of the first SSD after the save. The eighth function is a function of rewriting the data stored in the hot spare SSD to the second SSD after the first SSD separated from the SSD array <b>110</b>-* is replaced with the second SSD. The ninth function is a function of releasing the replacement SSD state of the hot spare SSD after the completion of the rewriting. According to such functions, the first SSD to which the number of data write operations is close to the upper limit can be replaced with the second SSD while the SSD array <b>110</b>-* is maintained in the state of being accessible.</p>
<p id="p-0048" num="0047">Here, it is assumed that the numbers of data write operations to a plurality of SSDs <b>11</b>-* are close to the upper limit at the same time in the SSD array <b>110</b>-*. In such a situation, preventive maintenance of the plurality of SSDs <b>11</b>-* to which the number of data write operations is close to the upper limit is performed for one of the SSDs at a time by employing the drive preventive maintenance function. In such a case, there is a possibility that the number of data write operations to another SSD <b>11</b>-* in the wait state of the preventive maintenance execution arrives at the upper limit in accordance with the repetition of data writing occurring with an elapse of time.</p>
<p id="p-0049" num="0048">Thus, a configuration may be considered in which the preventive maintenance of the plurality of SSDs <b>11</b>-* to which the number of data write operations is close to the upper limit is performed at the same time by employing the drive preventive maintenance function. However, in order to perform the preventive maintenance of the plurality of SSDs <b>11</b>-* at the same time, it is necessary to prepare hot spare SSDs corresponding to the number of SSDs <b>11</b>-* used in the SSD array <b>110</b>-*. Therefore, considering that a unit price of an SSD per constant capacity is higher than that of an HDD, the simultaneous preventive maintenance of a plurality of SSDs <b>11</b>-* is not practical.</p>
<p id="p-0050" num="0049">Considering this point, the storage controller <b>12</b> of the storage device <b>1</b> is required to suppress the numbers of data write operations to a plurality of SSDs <b>11</b>-* in the SSD array <b>110</b>-* to arrive at the upper limit at the same time. Accordingly, the storage controller <b>12</b> manages the locations of parity blocks and the locations of data blocks in the SSD array <b>110</b>-* such that the numbers of data write operations to a plurality of SSDs <b>11</b>-* in the SSD array are non-uniform, as will be described later.</p>
<p id="p-0051" num="0050">Hereinafter, the locations of parity blocks and data blocks in an SSD array that has been known will be described. Here, the SSD array is assumed to be a RAID array of the RAID level 5. In such an SSD array, the storage area (first storage area) of each of a plurality of SSDs configuring the SSD array is divided into a plurality of stripe blocks of equal capacity. For each set of stripe blocks having the same physical position in the plurality of SSDs, a stripe group is defined. Consecutive stripe group numbers used for identifying the plurality of stripe groups are assigned to the stripe groups. Here, it is assumed that the number of SSDs configuring the SSD array is M.</p>
<p id="p-0052" num="0051">In the SSD array of the RAID level 5, parities (parity data) are stored in M SSDs by being uniformly distributed in units of (M) stripe groups corresponding to the number M of SSDs configuring the SSD. That is, parities are stored in stripe blocks of mutually-different SSDs in M stripe groups by being uniformly distributed in accordance with a predetermined arrangement rule in units of M stripe groups. From this, when one SSD fails in the SSD array, the data of the failed SSD can be restored based on data and a parity of the remaining SSDs for each stripe group. In the description presented below, a stripe block in which a parity are stored will be referred to as a parity block, and a stripe block in which data is stored will be referred to as a data block.</p>
<p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. 3</figref> shows an example of the locations of a parity block and data blocks for each stripe group in an SSD array of the RAID level 5 as described above. The example shown in <figref idref="DRAWINGS">FIG. 3</figref> is premised on a case (M=7) where the SSD array comprises seven SSDs corresponding to SSDs <b>11</b>-<b>0</b> to <b>11</b>-<b>6</b> shown in <figref idref="DRAWINGS">FIG. 1</figref>. Here, the seven SSDs are denoted by SSD-#<b>0</b> to SSD-#<b>6</b> for convenience sake.</p>
<p id="p-0054" num="0053">In the example shown in <figref idref="DRAWINGS">FIG. 3</figref>, the same number of parity blocks are arranged in SSD-#<b>0</b> to SSD-#<b>6</b> in the SSD array, and the same number of parity blocks are arranged in SSD-#<b>0</b> to SSD-#<b>6</b> while being uniformly distributed. Here, each stripe group of the SSD array comprises one parity block P and six data blocks D<b>0</b> to D<b>5</b>. According to the SSD array shown in <figref idref="DRAWINGS">FIG. 3</figref>, write accesses are uniformly distributed to SSD-#<b>0</b> to SSD-#<b>6</b>, and the concentration of the load on a specific SSD can be prevented. When data writing to a data block Dj (j=0, 1, . . . , 5) in a stripe group occurs, the parity stored in the parity block P in the stripe group needs to be rewritten. Accordingly, the parity block P becomes a hot spot at which the number of write operations drastically increases out of all the stripe blocks in the stripe group.</p>
<p id="p-0055" num="0054">Here, it is assumed that data is respectively written to all the data blocks D<b>0</b> to D<b>5</b> in one stripe group once. In such a situation, every time data writing occurs, the parity stored in the parity block P is rewritten. In such a case, the number of data write operations to the parity block P is six, which is the same as the number of the data blocks D<b>0</b> to D<b>5</b> in the stripe group.</p>
<p id="p-0056" num="0055">Generally, in the array (RAID array) of the RAID level 5, parity blocks that become hot spots in data writing as described above are arranged such that the same number of the parity blocks are distributed in all the drives included in the array. According to the parity arrangement that is specific to the RAID level 5, write accesses are distributed to all the drives. Accordingly, the concentration of write accesses in a specific drive can be prevented. That is, degradation of the write access performance accompanying RAID control can be reduced.</p>
<p id="p-0057" num="0056">Such an effect is high in the case of HDDs having a response speed of the drive configuring the array much lower than that of SSDs and accompanied by a mechanical operation. However, in a case where drives configuring the array are SSDs, the SSDs operate at a sufficiently high speed and are not accompanied by a mechanical operation. In such a case, the uniformly-distributed arrangement of parity blocks as in the array of the RAID level 5 mostly does not contribute to the write access performance. In addition, write accesses tend to be performed for all the drives in an almost uniform pattern. Accordingly, the possibility that the numbers of data write operations to a plurality of SSDs in the array (SSD array) are close to the upper limit is high, and a probability causing the stoppage of the array (so-called RAID stoppage) is high. Therefore, it is not necessarily preferable to apply the RAID level 5 to the array configured by a plurality of SSDs.</p>
<p id="p-0058" num="0057">Thus, in the RAID array (SSD array) that comprises a plurality of SSDs and has redundancy according to parities, the possibility that a plurality of SSDs to which the number of data write operations arrives at the upper limit are present at the same time is requested to be excluded. Accordingly, in a conventional technology, parity blocks are arranged so as to be non-uniformly distributed to a plurality of SSDs such that the numbers of data write operations to the plurality of SSDs are not uniform. More specifically, in a case where the numbers of data write operations to the plurality of SSDs need to be non-uniform, the locations of the parity block and data blocks are dynamically changed in units of stripe groups. In order to perform the location change (that is, rearrangement), the locations of the parity block and data blocks are managed by a map table (arrangement location management table) for each stripe group.</p>
<p id="p-0059" num="0058">However, as a result of an increase in the capacity of the SSD array, the memory capacity that is necessary for storing the map table increases. Accordingly, a structure is requested to be realized in which the locations of the parity block and data blocks can be simply managed without using a map table while a configuration is employed in which the parity blocks are arranged so as to be non-uniformly distributed to the plurality of SSDs configuring the SSD array. In addition, when changes in the locations of the parity block and data blocks are excessively made, as a result of the changes, the number of write operations to each SSD increases. Accordingly, it is also requested to prevent excessive changes in the locations of the parity block and data blocks.</p>
<p id="p-0060" num="0059">Therefore, in the embodiment, the storage controller <b>12</b> arranges the parity blocks P to be distributed in units of stripe groups such that the numbers of parity blocks P arranged in SSD-#<b>0</b> to SSD-#<b>6</b> (SSDs <b>11</b>-<b>0</b> to <b>11</b>-<b>6</b>) configuring the SSD array <b>110</b>-* are totally different among SSD-#<b>0</b> to SSD-#<b>6</b>. Particularly, the storage controller <b>12</b> arranges N parity blocks P to be distributed regularly and non-uniformly to SSD-#<b>0</b> to SSD-#<b>6</b> in units of N stripe groups, which are determined in advance, having consecutive stripe group numbers. In the embodiment, when the number of SSDs configuring the SSD array <b>110</b>-* is M, N is represented as M(M&#x2212;1)/2. That is, when M is 7, N=21.</p>
<p id="p-0061" num="0060">According to the arrangement of the parity blocks P that is specific to the embodiment, the numbers of accesses to SSD-#<b>0</b> to SSD-#<b>6</b> (particularly, write accesses) can be non-uniform. In addition, in the embodiment, the storage controller <b>12</b> applies a regular arrangement (that is, a patterned arrangement) in units of N (N=M(M&#x2212;1)/2) stripe groups instead of a random arrangement to the non-uniform arrangement of the parity blocks P to SSD-#<b>0</b> to SSD-#<b>6</b>.</p>
<p id="p-0062" num="0061">Accordingly, the storage controller <b>12</b> can manage the locations of the parity blocks P and the locations of the data blocks D<b>0</b> to D<b>5</b> based on a predetermined arrangement rule (that is, an arrangement pattern). That is, in the embodiment, similarly to the SSD array of the RAID level 5, the locations of the parity blocks and the locations of the data blocks are managed based on the arrangement rule, using drive member numbers, which will be described later, logically assigned to SSD-#<b>0</b> to SSD-#<b>6</b> configuring the SSD array <b>110</b>-*. Accordingly, a map table (arrangement location management table) requiring a large memory capacity is not needed for the management of the locations of the parity blocks and the locations of the data blocks.</p>
<p id="p-0063" num="0062">However, the applied arrangement rule of the SSD array <b>110</b>-* according to the embodiment differs from that of the SSD array of the RAID level 5. That is, in the SSD array <b>110</b>-* according to the embodiment, as described above, N parity blocks P are regularly arranged so as to be non-uniformly distributed to SSD-#<b>0</b> to SSD-#<b>6</b> in units of N (N=M(M&#x2212;1)/2) stripe groups. In contrast to this, in the SSD array of the RAID level 5, as shown in <figref idref="DRAWINGS">FIG. 3</figref>, the parity blocks P are regularly arranged to be uniform in SSD-#<b>0</b> to SSD-#<b>6</b> in units of M (here, M=7) stripe groups. Here, M is the number of SSDs respectively configuring the SSD array <b>110</b>-* and the SSD array of the RAID level 5.</p>
<p id="p-0064" num="0063">Hereinafter, a specific example of the arrangement of the parity blocks and the arrangement of the data blocks in the SSD array <b>110</b>-* applied to the embodiment will be described. In the description presented below, the arrangement of the parity blocks and the arrangement of the data blocks may be referred to as a parity arrangement and a data arrangement.</p>
<p id="p-0065" num="0064">First, terms relating to the SSD array <b>110</b>-* applied to the embodiment will be sequentially described.</p>
<p id="h-0006" num="0000">Number of Drives in Array (drv_num)</p>
<p id="p-0066" num="0065">The number of drives in array represents the number of drives (that is, SSDs) configuring the SSD array <b>110</b>-*. In the embodiment in which the SSD array <b>110</b>-* comprises SSD-#<b>0</b> to SSD-#<b>6</b> (more specifically, SSD-#<b>0</b> to SSD-*6 connected to the drive slots #<b>0</b> to #<b>6</b>), the number of drives in array is seven (drv_num&#x2212;7).</p>
<p id="h-0007" num="0000">Stripe Size (sb_sz)</p>
<p id="p-0067" num="0066">The stripe size represents the capacity of each stripe block.</p>
<p id="h-0008" num="0000">Stripe Group Size (sg_sz)</p>
<p id="p-0068" num="0067">The stripe group size represents the capacity of all the data blocks within each stripe group. The stripe group size, similarly to that of the array of the RAID level 5, is a value that is acquired by multiplying a value acquired by subtracting one (the number of parity blocks P within the stripe group) from the number of drives in array (drv_num) by the stripe size (sb_sz).</p>
<p id="h-0009" num="0000">Management Block</p>
<p id="p-0069" num="0068">A management block is configured in units of sets of a plurality of stripe groups having consecutive stripe group numbers. The parity arrangement (more specifically, the parity arrangement and the data arrangement) is patterned in units of management blocks. That is, the management block is a block used for managing the locations of parity blocks. The arrangement pattern (hereinafter, referred to as a parity arrangement pattern) represents the location (parity block location) of the parity block P in units of stripe groups in each management block. More specifically, the parity arrangement pattern represents the locations (the parity block location and the data block locations) of the parity block P and data blocks D<b>0</b> to D<b>5</b> in units of stripe groups in each management block.</p>
<p id="p-0070" num="0069">The feature of the parity arrangement pattern represents a parity arrangement in which the numbers of parity blocks P arranged in the management block are totally different among SSD-#<b>0</b> to SSD-#<b>6</b> configuring the SSD array <b>110</b>-*. More specifically, according to the parity arrangement pattern, the parity block P is not arranged in an SSD having a largest drive member number. In addition, according to the parity arrangement pattern, the parity blocks P are arranged such that the parity arrangement number (that is, the number of arranged parity blocks P) increases by one for each decrement of one in the drive member number.</p>
<p id="h-0010" num="0000">Management Block Size (blk_sz)</p>
<p id="p-0071" num="0070">The management block size represents the number of stripe groups configuring the management block. In the case of the above-described parity arrangement pattern, the management block size is represented as drv_num(drv_num&#x2212;1)/2. Here, drv_num represents the number of drives in array. In the embodiment in which the number of drives in array (drv_num) is seven, the management block size is 21. That is, the management block comprises 21 stripe groups having consecutive stripe group numbers sg_no.</p>
<p id="h-0011" num="0000">Management Block Boundary Offset (bdr_ofs)</p>
<p id="p-0072" num="0071">The management block boundary offset is used in a case where the location of the parity block P, and furthermore, the locations of data blocks Dj (j=0, 1, . . . , 5) are calculated in units of stripe groups. The management block boundary offset is set to zero immediately after the SSD array <b>110</b>-* is generated (or defined or constructed). In addition, the management block boundary offset increases by one every time a parity rearrangement process is started.</p>
<p id="p-0073" num="0072">The parity rearrangement process is a process for rearranging the parity block P and the data blocks Dj. The parity rearrangement process is performed in a case where an SSD to which the number of data write operations arrives at the upper limit is detected to be present in the SSD array <b>110</b>-*, and the replacement of the SSD is performed. In a case where the management boundary offset after an increase by one coincides with the management block size (the number of stripe groups in the management block), the management block boundary offset is returned to zero. That is, the management block boundary offset represents the position of a starting stripe group of the parity arrangement pattern (the management block). In addition, the management block boundary offset is also used for specifying a parity block P and a data block Dj as access targets in a case where an access to an area in which the rearrangement of the parity blocks and data blocks is completed by the parity rearrangement process is requested from the host <b>2</b>.</p>
<p id="h-0012" num="0000">Prior-Rearrangement Block Boundary Offset</p>
<p id="p-0074" num="0073">The prior-rearrangement block boundary offset (hereinafter, referred to as a PR block boundary offset) is used in a case where the location of the parity block P and, furthermore, the locations of the data blocks Dj are calculated in units of stripe groups. The PR block boundary offset set to the same value as that of the management block boundary offset immediately after the SSD array <b>110</b>-* is generated (constructed). In addition, the PR block boundary offset is set to the management block boundary offset before update when the parity rearrangement process is started. In addition, the PR block boundary offset is also used for specifying the parity block P and the data block Dj as access targets in a case where an access to an area in which the rearrangement process of the parity block and data blocks has not been performed during the period of the parity rearrangement process is requested from the host <b>2</b>.</p>
<p id="h-0013" num="0000">Stripe Group Number During Parity Rearrangement</p>
<p id="p-0075" num="0074">The stripe group number during parity rearrangement (hereinafter, referred to as an R stripe group number) represents an execution position in the parity rearrangement process, that is, a stripe group number during the execution of the parity rearrangement.</p>
<p id="h-0014" num="0000">Data Block Number</p>
<p id="p-0076" num="0075">The data block numbers represent serial numbers that are fixedly assigned to the data blocks D<b>0</b> to D<b>5</b> within the stripe group. Here, data block numbers of <b>0</b> to <b>5</b> are assigned to the data blocks D<b>0</b> to D<b>5</b>.</p>
<p id="h-0015" num="0000">Drive Slot Number</p>
<p id="p-0077" num="0076">The drive slot numbers represent serial numbers that are fixedly assigned to all the SSDs (that is, SSDs <b>11</b>-<b>0</b> to <b>11</b>-<b>6</b>) configuring the SSD <b>11</b>-*. Here, numbers <b>0</b> to <b>6</b> (SSD-#<b>0</b> to SSD-#<b>6</b>) assigned to the drive slots to which the SSDs <b>11</b>-<b>0</b> to <b>11</b>-<b>6</b> are connected are used as the drive slot numbers of the SSDs <b>11</b>-<b>0</b> to <b>11</b>-<b>6</b>.</p>
<p id="h-0016" num="0000">Drive Slot Number During Parity Rearrangement</p>
<p id="p-0078" num="0077">The drive slot number during the parity rearrangement (hereinafter, referred to as an R drive slot number) represents the execution position of the rearrangement of the parity block or data block in the stripe group during the execution of the parity rearrangement, that is, a drive slot number of the SSD (drive) that includes the stripe block during the execution of the rearrangement.</p>
<p id="h-0017" num="0000">Drive Member Number</p>
<p id="p-0079" num="0078">The drive member numbers are logical serial numbers assigned to SSDs <b>11</b>-<b>0</b> to <b>11</b>-<b>6</b> that configure the SSD array <b>110</b>-*. In the embodiment, immediately after the generation of the SSD array <b>110</b>-*, the array management unit <b>21</b> (specifically, the array generation unit <b>210</b> of the array management unit <b>21</b>) sets the drive member numbers of SSDs <b>11</b>-<b>0</b> to <b>11</b>-<b>6</b> to values that are the same as the drive slot numbers of the SSDs <b>11</b>-<b>0</b> to <b>11</b>-<b>6</b>. In addition, when the parity rearrangement process is started, the block rearrangement unit <b>250</b> sets the drive member number of the SSD to be replaced with one (that is, an SSD to which the number of data write operations arrives at the upper limit) of the SSDs <b>11</b>-<b>0</b> to <b>11</b>-<b>6</b> to zero and increases the drive member numbers of the remaining SSDs by one. The drive member number of each SSD configuring the SSD array <b>110</b>-* is used for specifying a parity block and a data block as access targets in a case where an access to an area in which the rearrangement of the parity block and data blocks has been completed by the parity arrangement process is requested from the host <b>2</b>.</p>
<p id="h-0018" num="0000">Prior-Rearrangement Member Number</p>
<p id="p-0080" num="0079">The prior-rearrangement member numbers (hereinafter, referred to as PR member numbers) represent logical serial numbers assigned to the SSDs <b>11</b>-<b>0</b> to <b>11</b>-<b>6</b> configuring the SSD array <b>110</b>-* before the parity rearrangement process is started. Immediately after the SSD array <b>110</b>-* is generated, the array management unit <b>21</b> sets the PR member number of the SSD <b>11</b>-* to the same value as the drive slot number of the SSD <b>11</b>-*. In addition, when the parity rearrangement process is started, the block rearrangement unit <b>250</b> sets the PR member number of the SSD <b>11</b>-* to the drive member number before update of the SSD <b>11</b>-*. The drive member number of each SSD configuring the SSD array <b>110</b>-* is used for specifying a parity block and a data block as access targets in a case where an access to an area in which the rearrangement process of the parity block and data blocks has not been performed during the period of the parity rearrangement process is requested from the host <b>2</b>.</p>
<p id="p-0081" num="0080"><figref idref="DRAWINGS">FIG. 4</figref> shows an example of the data structure of array management tables <b>28</b> and <b>29</b>. The array management table <b>28</b> is used for storing first SSD array management information used for managing the configuration of the SSD array <b>110</b>-*, and the array management table <b>29</b> is used for storing second SSD array management information used for managing the configuration of the SSD array <b>110</b>-*. The SSD array <b>110</b>-* is generated (defined) by the array generation unit <b>210</b> of the array management unit <b>21</b>. The array management tables <b>28</b> and <b>29</b> are generated by the array management unit <b>21</b> when the SSD array <b>110</b>-* is generated. In the embodiment, the array management tables <b>28</b> and <b>29</b> are stored in the local memory <b>126</b>.</p>
<p id="p-0082" num="0081">The first SSD array management information includes information of the number of drives in array (drv_num), the stripe size (sb_sz), the stripe group size (sg_sz), the management block size (blk_sz), the management block boundary offset (bdr_ofs), the PR block boundary offset (prior rearrangement block boundary offset), the R stripe group number (the stripe group number during the parity rearrangement), and the R drive slot number (the drive slot number during the parity rearrangement). On the other hand, the second SSD array management information represents the correspondence among the drive slot numbers of the SSDs <b>11</b>-<b>0</b> to <b>11</b>-<b>6</b> configuring the SSD array <b>110</b>-*, the drive member numbers of the SSDs <b>11</b>-<b>0</b> to <b>11</b>-<b>6</b>, and the PR member numbers (prior rearrangement drive member numbers) of the SSDs <b>11</b>-<b>0</b> to <b>11</b>-<b>6</b>.</p>
<p id="p-0083" num="0082"><figref idref="DRAWINGS">FIG. 5</figref> shows an example of the arrangement of a parity block P and data blocks D<b>0</b> to D<b>5</b> in the SSD array <b>110</b>-* in association with the array management table <b>29</b>. In the example shown in <figref idref="DRAWINGS">FIG. 5</figref>, a set of stripe groups having stripe group numbers of 0 to 20 is used as one management block. Although not shown in <figref idref="DRAWINGS">FIG. 5</figref>, in the embodiment, a set of stripe groups having stripe group numbers of 21 to 41 is also used as one management block.</p>
<p id="p-0084" num="0083">The management block is a set of a plurality of continuous stripe groups (in the embodiment, 21 stripe groups) forming a regular arrangement location pattern (arrangement pattern) of the parity block P and the data blocks D<b>0</b> to D<b>5</b>. The management block is defined by the array management unit <b>21</b>.</p>
<p id="p-0085" num="0084">In the example shown in <figref idref="DRAWINGS">FIG. 5</figref>, the SSD having a largest drive member number is SSD-#<b>6</b> (SSD <b>11</b>-<b>6</b>). In this case, in any one of 21 stripe groups configuring the management block, the parity block P is not arranged in a corresponding stripe block in the SSD-#<b>6</b>.</p>
<p id="p-0086" num="0085">On the other hand, in the other SSD-#<b>5</b> to SSD-#<b>0</b> (SSDs <b>11</b>-<b>5</b> to <b>11</b>-<b>0</b>), the parity blocks are arranged such that the parity arrangement number increases by one for each decrement of one in the drive member number compared to that of the SSD-#<b>6</b>. From this, in the management block, the numbers of parity blocks arranged in SSD-#<b>5</b>, SSD-#<b>4</b>, SSD-#<b>3</b>, SSD-#<b>2</b>, SSD-#<b>1</b>, and SSD-#<b>0</b> having drive member numbers of 5, 4, 3, 2, 1, and 0 are 1, 2, 3, 4, 5, and 6.</p>
<p id="p-0087" num="0086">Next, an exemplary procedure of the parity block location calculation process applied to the embodiment in a case where the contents of the array management tables <b>28</b> and <b>29</b> are as shown in <figref idref="DRAWINGS">FIG. 4</figref> as an example will be described with reference to a flowchart shown in <figref idref="DRAWINGS">FIG. 6</figref>. The parity block location calculation process is a process for calculating the locations (parity block locations) of the drive in which the parity block P is arranged in a target stripe group. Here, as the location of the parity block, the drive member number of the drive in which the parity block P is arranged is calculated.</p>
<p id="p-0088" num="0087">The parity block arrangement calculation unit <b>22</b> of the storage controller <b>12</b>, first, sets a target stripe group number sg_no, the number of drives in array drv_num, a management block size blk_sz, a management block boundary offset bdr_ofs, and a target stripe number in management block blk_num in the work area of the local memory <b>126</b> (Step S<b>1</b>). In Step S<b>1</b>, the parity block arrangement calculation unit <b>22</b> defines a parity continuous stripe range int_blk as a variable. The parity continuous stripe range int_blk is set in the work area.</p>
<p id="p-0089" num="0088">The target stripe group number sg_no is a stripe group number of a target stripe group (that is, a stripe group that is a target for calculating the parity block location). The number of drives in array drv_num, the management block size blk_sz, and the management block boundary offset bdr_ofs are set by referring to the array management table <b>28</b>. Immediately after the SSD array <b>110</b>-* is generated, the management block boundary offset bdr_ofs is set to an initial value of &#x201c;0&#x201d;. In the example of the array management table <b>28</b> shown in <figref idref="DRAWINGS">FIG. 4</figref>, drv_num=7, blk_sz=21, and bdr_ofs=0.</p>
<p id="p-0090" num="0089">The target stripe number in management block blk_num is a stripe number representing a relative position of the target stripe group in the management block. The range of the target stripe number in management block blk_num is 0 to 20. The target stripe number in management block blk_num is calculated using the following equation.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>blk_num=((sg_no+blk_sz)&#x2212;bdr_ofs) % blk_sz<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0091" num="0090">In this equation, symbol % represents a calculation for acquiring the remainder in a case where an integer part of the quotient is calculated by dividing ((sg_no+blk_sz)&#x2212;bdr_ofs) by blk_sz. In the arrangement shown in <figref idref="DRAWINGS">FIG. 5</figref> in which the management block boundary offset bdr_ofs is zero, for example, the target stripe number in management block blk_num of the stripe group having a stripe group number sg_no of 0 or 21 is zero. Similarly, the target stripe number in management block blk_num of the stripe group having a stripe group number sg_no of 20 is 20.</p>
<p id="p-0092" num="0091">The parity continuous stripe range int_blk represents the number (range) of continuous stripe groups (more specifically, stripe groups having consecutive stripe group numbers sg_no) in which the locations of the parity blocks P (that is, the drives in which the parity blocks P are arranged) are mutually different. The parity continuous stripe range int_blk is one of 6, 5, 4, 3, 2, and 1. A maximum value 6 of the parity continuous stripe range int_blk coincides with drive_num&#x2212;1.</p>
<p id="p-0093" num="0092">Next, the parity block arrangement calculation unit <b>22</b> sequentially repeats the following loop S<b>2</b> for a variable i that is in the range of i=0 to i=drv_num&#x2212;2 (=5). However, in a case where the determination of Step S<b>2</b><i>b </i>to be described later is &#x201c;Yes&#x201d;, the parity block arrangement calculation unit <b>22</b> exits the loop S<b>2</b>.</p>
<p id="p-0094" num="0093">First, the parity block arrangement calculation unit <b>22</b> calculates a parity continuous stripe range int_blk using the following equation (Step S<b>2</b><i>a</i>).
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>int_blk=drv_num&#x2212;1<i>&#x2212;i </i><?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0095" num="0094">In the case of i=0, the parity continuous stripe range int_blk is 6. Int_blk=6 represents a set of stripe groups, in the management block, having stripe numbers in the range of 0 to 5. In addition, int_blk=5 represents a set of stripe groups, in the management block, having stripe numbers in the range of 6 to 10, and int_blk=4 represents a set of stripe groups, in the management block, having stripe numbers in the range of 11 to 14. Similarly, int_blk=3 represents a set of stripe groups, in the management block, having stripe numbers in the range of 15 to 17, and int_blk=2 represents a set of stripe groups, in the management block, having stripe numbers in the range of 18 to 19. int_blk=1 represents a stripe group, in the management block, having a stripe number of 20.</p>
<p id="p-0096" num="0095">Next, the parity block arrangement calculation unit <b>22</b> determines whether the target stripe number in management block blk_num is less than the calculated int_blk (Step S<b>2</b><i>b</i>). This determination is performed for checking whether the target stripe group is within the range of stripe groups represented by the calculated int_blk. In a case where int_blk=6 as in the embodiment, if the target stripe number in management block blk_num is one of 0 to 5, the determination of Step S<b>2</b><i>b </i>is &#x201c;Yes&#x201d;. In such a case, the parity block arrangement calculation unit <b>22</b> exits the loop S<b>2</b> and proceeds to Step S<b>3</b>. Then, the parity block arrangement calculation unit <b>22</b> sets the target stripe number in management block blk_num as a drive member number pdrv_no representing the location of the parity block. From this, if the target stripe number in management block blk_num is 0, 1, 2, 3, 4, or 5 (that is, less than 6) (Yes in step S<b>2</b><i>b</i>), the drive member number pdrv_no representing the location of the parity block coincides with blk_num.</p>
<p id="p-0097" num="0096">In contrast to this, if the target stripe number in management block blk_num is not less than 6 (No in Step S<b>2</b><i>b</i>), the parity block arrangement calculation unit <b>22</b> determines that the target stripe group is out of the parity continuous stripe range int_blk=6 (that is, a set of stripe groups, in the management block, having stripe numbers in the range of 0 to 5). In such a case, in order to determine whether the target stripe group is within the next parity continuous stripe range int_blk=5, the parity block arrangement calculation unit <b>22</b> decrements the target stripe number in management block blk_num by the current parity continuous stripe range int_blk=6 (Step S<b>2</b><i>c</i>).</p>
<p id="p-0098" num="0097">The target stripe number in management block blk_num after the decrement in the case of i=0 represents a relative position in the management block excluding the parity continuous stripe range int_blk=6 (a set of stripe groups, in the management block, having stripe numbers in the range of 0 to 5). In this point, the target stripe number in management block blk_num after decrement is different from the target stripe number blk_num in the management block that is set in Step S<b>1</b>.</p>
<p id="p-0099" num="0098">Next, the parity block arrangement calculation unit <b>22</b> increments the variable i from i=0 to i=1 and performs Step S<b>2</b><i>a </i>again. In this case, the parity continuous stripe range int_blk is updated from 6 to 5. Then, the parity block arrangement calculation unit <b>22</b> determines whether the target stripe number in management block blk_num after decrementing is less than the current int_blk=5 (Step S<b>2</b><i>b</i>). If the target stripe number in management block (hereinafter, referred to as an initial target stripe number in management block) blk_num that is set in Step S<b>1</b> is one of 6 to 10, the determination of Step S<b>2</b><i>b </i>is &#x201c;Yes&#x201d;. In such a case, the parity block arrangement calculation unit <b>22</b> exits the loop S<b>2</b> and proceeds to Step S<b>3</b>.</p>
<p id="p-0100" num="0099">In Step S<b>3</b>, the parity block arrangement calculation unit <b>22</b> sets the current target stripe number in management block blk_num as the drive member number pdrv_no representing the parity block location. Here, it is assumed that the initial target stripe number in management block blk_num is 6, 7, 8, 9, or 10, and therefore the current target stripe number in management block blk_num is 0, 1, 2, 3, or 4 (Yes in Step S<b>2</b><i>b</i>). In this case, pdrv_no coincides with the current target stripe number in management block blk_num (that is, 0, 1, 2, 3, or 4).</p>
<p id="p-0101" num="0100">In contrast to this, if the current target stripe number in management block blk_num is not less than the current int_blk=5 (No in Step S<b>2</b><i>b</i>), the parity block arrangement calculation unit <b>22</b> determines that the target stripe group is out of the current int_blk=5 (that is, a set of stripe groups, in the management block, having stripe numbers in the range of 6 to 10). In this case, in order to determine whether the target stripe group is within the next parity continuous stripe range int_blk=4, the parity block arrangement calculation unit <b>22</b> decrements the target stripe number in management block blk_num by the current parity continuous stripe range int_blk=5 (Step S<b>2</b><i>c</i>).</p>
<p id="p-0102" num="0101">Next, the parity block arrangement calculation unit <b>22</b> increments the variable i from i=1 to i=2 and performs Step S<b>2</b><i>a </i>again. In this case, the parity continuous stripe range int_blk is updated from 5 to 4. Then, the parity block arrangement calculation unit <b>22</b> determines whether the current target stripe number in management block blk_num (after decrement) is less than the current int_blk=4 (Step S<b>2</b><i>b</i>). If the initial target stripe number in management block blk_num is one of 11 to 14, the determination of Step S<b>2</b><i>b </i>is &#x201c;Yes&#x201d;. In such a case, the parity block arrangement calculation unit <b>22</b> exits the loop S<b>2</b> and proceeds to Step S<b>3</b>. In Step S<b>3</b>, the parity block arrangement calculation unit <b>22</b> sets the current target stripe number in management block blk_num as the drive member number pdrv_no representing the parity block location. Here, it is assumed that the initial target stripe number in management block blk_num is 11, 12, 13, or 14, and therefore the current target stripe number in management block blk_num is 0, 1, 2, or 3 (Yes in Step S<b>2</b><i>b</i>). In this case, pdrv_no coincides with the current target stripe number in management block blk_num (that is, 0, 1, 2, or 3).</p>
<p id="p-0103" num="0102">In contrast to this, if the current target stripe number in management block blk_num is not less than the current int_blk=4 (No in Step S<b>2</b><i>b</i>), the parity block arrangement calculation unit <b>22</b> determines that the target stripe group is out of the current int_blk=4 (that is, a set of stripe groups, in the management block, having the stripe numbers in the range of 11 to 14). In such a case, in order to determine whether the target stripe group is within the next parity continuous stripe range int_blk=3, the parity block arrangement calculation unit <b>22</b> decrements the target stripe number in management block blk_num by the current int_blk=4 (Step S<b>2</b><i>c</i>).</p>
<p id="p-0104" num="0103">Next, the parity block arrangement calculation unit <b>22</b> increments the variable i from i=2 to i=3 and performs Step S<b>2</b><i>a </i>again. In this case, the parity continuous stripe range int_blk is updated from 4 to 3. Then, the parity block arrangement calculation unit <b>22</b> determines whether the current target stripe number in management block blk_num is less than the current int_blk=3 (Step S<b>2</b><i>b</i>). If the initial target stripe number in management block blk_num is one of 15 to 17, the determination of Step S<b>2</b><i>b </i>is &#x201c;Yes&#x201d;. In such a case, the parity block arrangement calculation unit <b>22</b> exits the loop S<b>2</b> and proceeds to Step S<b>3</b>. In Step S<b>3</b>, the parity block arrangement calculation unit <b>22</b> sets the current target stripe number in management block blk_num as the drive member number pdrv_no representing the parity block location. Here, it is assumed that the initial target stripe number in management block blk_num is 15, 16, or 17, and therefore the current target stripe number in management block blk_num is 0, 1, or 2 (Yes in Step S<b>2</b><i>b</i>). In this case, pdrv_no coincides with the current target stripe number in management block blk_num (that is, 0, 1, or 2).</p>
<p id="p-0105" num="0104">In contrast to this, if the current target stripe number in management block blk_num is not less than the current int_blk=3 (No in Step S<b>2</b><i>b</i>), the parity block arrangement calculation unit <b>22</b> determines that the target stripe group is out of the current int_blk=3 (that is, a set of stripe groups, in the management block, having the stripe numbers in the range of 15 to 17). In such a case, in order to determine whether the target stripe group is within the next parity continuous stripe range int_blk=2, the parity block arrangement calculation unit <b>22</b> decrements the target stripe number in management block blk_num by the current int_blk=3 (Step S<b>2</b><i>c</i>).</p>
<p id="p-0106" num="0105">Next, the parity block arrangement calculation unit <b>22</b> increments the variable i from i=3 to i=4 and performs Step S<b>2</b><i>a </i>again. In this case, the parity continuous stripe range int_blk is updated from 3 to 2. Then, the parity block arrangement calculation unit <b>22</b> determines whether the current target stripe number in management block blk_num is less than the current int_blk=2 (Step S<b>2</b><i>b</i>). If the initial target stripe number in management block blk_num is one of 18 and 19, the determination of Step S<b>2</b><i>b </i>is &#x201c;Yes&#x201d;. In such a case, the parity block arrangement calculation unit <b>22</b> exits the loop S<b>2</b> and proceeds to Step S<b>3</b>. In Step S<b>3</b>, the parity block arrangement calculation unit <b>22</b> sets the current target stripe number in management block blk_num as the drive member number pdrv_no representing the parity block location. Here, it is assumed that the initial target stripe number in management block blk_num is 18 or 19, and therefore the current target stripe number in management block blk_num is 0 or 1 (Yes in Step S<b>2</b><i>b</i>). In this case, pdrv_no coincides with the current target stripe number in management block blk_num (that is, 0 or 1).</p>
<p id="p-0107" num="0106">In contrast to this, when the current target stripe number in management block blk_num is not less than the current int_blk=2 (No in Step S<b>2</b><i>b</i>), the parity block arrangement calculation unit <b>22</b> determines that the target stripe group is out of the current int_blk=2 (that is, the stripe number is a set of stripe groups, in the management block, having the stripe numbers in the range 18 to 19). In such a case, in order to determine whether the target stripe group is within the next parity continuous stripe range int_blk=1, the parity block arrangement calculation unit <b>22</b> decrements the target stripe number in management block blk_num by the current int_blk=2 in Step S<b>2</b><i>c. </i></p>
<p id="p-0108" num="0107">Next, the parity block arrangement calculation unit <b>22</b> increments the variable i from i=4 to i=5 and performs Step S<b>2</b><i>a </i>again. In this case, the parity continuous stripe range int_blk is updated from 2 to 1. Then, the parity block arrangement calculation unit <b>22</b> determines whether the current target stripe number in management block blk_num is less than the current int_blk=1 in Step S<b>2</b><i>b</i>. When the initial target stripe number in management block blk_num is 20, the determination of Step S<b>2</b><i>b </i>is &#x201c;Yes&#x201d;. In such a case, the parity block arrangement calculation unit <b>22</b> exits the loop S<b>2</b> and proceeds to Step S<b>3</b>. In Step S<b>3</b>, the parity block arrangement calculation unit <b>22</b> sets the current target stripe number in management block blk_num as the drive member number pdrv_no representing the parity block location. Here, it is assumed that the initial target stripe number in management block blk_num is 20, and the current target stripe number in management block blk_num is 0 (Yes in Step S<b>2</b><i>b</i>). In this case, pdrv_no coincides with the current target stripe number in management block blk_num (that is, 0).</p>
<p id="p-0109" num="0108">The parity block arrangement calculation unit <b>22</b> performs the above-described parity block location calculation process for all the stripe groups configuring the SSD array <b>110</b>-*, whereby the parity block arrangement drive member number pdrv_no representing the parity block location for each stripe group can be calculated.</p>
<p id="p-0110" num="0109">In a case where the contents of the array management tables <b>28</b> and <b>29</b> are as shown in <figref idref="DRAWINGS">FIG. 4</figref>, based on the calculation of the parity block arrangement drive member numbers pdrv_no as described above, for example, the drive member numbers (parity block arrangement drive member numbers) pdr_no of drives in which the parity blocks P of the stripe groups having stripe group numbers sg_no of 0, 1, 2, 3, and 5 are to be arranged are 0, 1, 2, 3, 4, and 5 (see <figref idref="DRAWINGS">FIG. 5</figref>). Similarly, the parity block arrangement drive member numbers pdr_no of drives in which the parity blocks P of the stripe groups having stripe group numbers sg_no of 6, 7, 8, 9, and 10 are to be arranged are 0, 1, 2, 3, and 4 (see <figref idref="DRAWINGS">FIG. 5</figref>).</p>
<p id="p-0111" num="0110">Similarly, the parity block arrangement drive member numbers pdr_no of drives in which the parity blocks P of the stripe groups having stripe group numbers sg_no of 11, 12, 13, and 14 are to be arranged are 0, 1, 2, and 3 (see <figref idref="DRAWINGS">FIG. 5</figref>). Similarly, the parity block arrangement drive member numbers pdr_no of drives in which the parity blocks P of the stripe groups having stripe group numbers sg_no of 15, 16, and 17 are to be arranged are 0, 1, and 2 (see <figref idref="DRAWINGS">FIG. 5</figref>). Similarly, the parity block arrangement drive member numbers pdr_no of drives in which the parity blocks P of the stripe groups having stripe group numbers sg_no of 18 and 19 are to be arranged are 0 and 1 (see <figref idref="DRAWINGS">FIG. 5</figref>). Similarly, the parity block arrangement drive member number pdr_no of a drive in which the parity block P of the stripe group having a stripe group number sg_no of 20 is to be arranged is 0 (see <figref idref="DRAWINGS">FIG. 5</figref>).</p>
<p id="p-0112" num="0111">Next, an exemplary procedure of a data block location calculation process applied to the embodiment in a case where the contents of the array management tables <b>28</b> and <b>29</b> are as shown in <figref idref="DRAWINGS">FIG. 4</figref> as an example will be described with reference to a flowchart shown in <figref idref="DRAWINGS">FIG. 7</figref>. The data block location calculation process is a process for calculating the position (data block location) of a drive in which a target data block in a target stripe group is arranged. Here, as the data block location, the drive member number of the drive in which the data block is arranged is calculated.</p>
<p id="p-0113" num="0112">The data block arrangement calculation unit <b>23</b> of the storage controller <b>12</b>, first, sets a target stripe group number sg_no, a target data block number dblk_no, the number of drives in array drv_num (=7), a management block size blk_sz (=21), a management block boundary offset bdr_ofs (=0), and a target stripe number in management block blk_num in the work area of the local memory <b>126</b> (Step S<b>11</b>). The target stripe group number sg_no is a stripe group number of a stripe group (target stripe group) including a target data block Dj. The target data block number dblk_no is a data block number (j=dblk_no) of the target data block Dj. The target data block is one of six data blocks D<b>0</b> to D<b>5</b> of the target stripe group.</p>
<p id="p-0114" num="0113">Next, the data block arrangement calculation unit <b>23</b> causes the parity block arrangement calculation unit <b>22</b> to calculate the parity block location in the target stripe group represented by the target stripe group number sg_no (Step S<b>12</b>). That is, the parity block arrangement calculation unit <b>22</b> calculates the drive member number pdrv_no representing the parity block location by performing the parity block location calculation process shown in a flowchart illustrated in <figref idref="DRAWINGS">FIG. 6</figref>.</p>
<p id="p-0115" num="0114">Next, the data block arrangement calculation unit <b>23</b> calculates a data block location in which the target data block Dj (j=dblk_no) represented by the target data block number dblk_no is to be arranged, based on the calculated parity arrangement drive member number pdrv_no, the target data block number dblk_no, and the number of drives in array drv_num (=7) (Step S<b>13</b>). That is, the data block arrangement calculation unit <b>23</b> calculates a drive member number (data block arrangement drive member number) dblk_drv representing the data block location in which the data block Dj (here, j=dblk_no) is to be arranged, using the following equation.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>dblk_drv&#x2212;((pdrv_no+dblk_no+1) % drv_num<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0116" num="0115">The data block arrangement calculation unit <b>23</b> performs the above-described data block location calculation process for the data blocks D<b>0</b> to D<b>5</b> of all the stripe groups configuring the SSD array <b>110</b>-*, whereby, for each stripe group and for each data block, the data block arrangement drive member number dblk_drv representing the location of the data block can be calculated.</p>
<p id="p-0117" num="0116">According to the calculation of the data block arrangement drive member numbers dblk_drv, in a case where the contents of the array management tables <b>28</b> and <b>29</b> are as shown in <figref idref="DRAWINGS">FIG. 4</figref>, for example, the drive member numbers (data block arrangement drive member numbers) dblk_drv of drives in which the data blocks D<b>0</b>, D<b>1</b>, D<b>2</b>, D<b>3</b>, D<b>4</b>, and <b>05</b> of the stripe group having a stripe group number sg_no of &#x201c;0&#x201d; are to be arranged are 1, 2, 3, 4, 5, and 6. Similarly, for example, the drive member numbers dblk_drv of drives in which the data blocks D<b>0</b>, D<b>1</b>, D<b>2</b>, D<b>3</b>, D<b>4</b>, and D<b>5</b> of the stripe group having a stripe group number sg_no of &#x201c;1&#x201d; are to be arranged are 2, 3, 4, 5, 6, and 0.</p>
<p id="p-0118" num="0117">According to the parity block location calculation process and the data block location calculation process described above, the parity blocks P and the data blocks D<b>0</b> to D<b>5</b> are arranged in the parity arrangement pattern as shown in <figref idref="DRAWINGS">FIG. 5</figref>. In the example shown in <figref idref="DRAWINGS">FIG. 5</figref>, it should be noticed that the parity block P is not arranged in the SSD having the largest drive member number. In addition, in the example shown in <figref idref="DRAWINGS">FIG. 5</figref>, in each management block of the SSD array <b>110</b>-*, the parity block P is arranged such that the arrangement number of the parity block P increases by one every time the drive member number decreases by one. As a result of the arrangement of the parity blocks P, the arrangement numbers of the parity blocks in the management block are 6, 5, 4, 3, 2, 1, and 0 in drives (SSDs <b>11</b>-*) having drive member numbers of 0, 1, 2, 3, 4, 5, and 6. That is, according to the embodiment, the parity blocks P are arranged regularly and non-uniformly in units of management blocks.</p>
<p id="p-0119" num="0118">As above, according to the embodiment, the parity blocks P (that is, the parity blocks P that are hot spots for write accesses) are arranged non-uniformly in SSD-#<b>0</b> to SSD-#<b>6</b> (drives) included in the SSD array <b>110</b>-*. Accordingly, the respective numbers of data write operations to SSD-#<b>0</b> to SSD-#<b>6</b> are different from one another so as to be in correspondence with differences in the numbers of parity blocks P arranged in SSD-#<b>0</b> to SSD-#<b>6</b>. Accordingly, it can be prevented that the numbers of data write operations to a plurality of SSDs <b>11</b>-* included in the SSD array <b>110</b>-* arrive at the upper limit at the same time. Therefore, according to the embodiment, an array stoppage due to multiple fails of the SSDs can be prevented, which are caused as the numbers of data write operations to a plurality of SSDs <b>11</b>-* included in the SSD array <b>110</b>-* arrive at the upper limit.</p>
<p id="p-0120" num="0119">In addition, according to the embodiment, the data blocks D<b>0</b> to D<b>5</b> of each stripe group, differing from the case of the general SSD array of the RAID level 5, are arranged with the parity block location in the stripe group used as a starting point, by the data block location calculation process shown in <figref idref="DRAWINGS">FIG. 7</figref>. The reason for applying such a data block arrangement is as follows. Now, it is assumed that the number of data write operations to an SSD in which the number of arranged parity blocks P is the most arrives at the upper limit, and therefore the SSD is to be replaced. In such a case, in a general SSD array of the RAID level 5, the parity blocks P and the data blocks D<b>0</b> to D<b>5</b> need to be rearranged in all the stripe groups. In contrast to this, in the embodiment, as a result of the data block arrangement described above, by rearranging a minimum number of the parity blocks P and data blocks D<b>0</b> to D<b>5</b>, the above-described parity arrangement pattern can be maintained. The rearrangement will be described later.</p>
<p id="p-0121" num="0120">Next, an operation performed in a case where an access to the SSD array <b>110</b>-* is requested to the storage controller <b>12</b> of the storage device <b>1</b> from the host <b>2</b> will be described. This operation will be described with reference to flowcharts shown in <figref idref="DRAWINGS">FIGS. 8A and 8B</figref>. <figref idref="DRAWINGS">FIG. 8A</figref> is a diagram showing a part of a flowchart illustrating an exemplary procedure of a block specifying process applied to the embodiment, and <figref idref="DRAWINGS">FIG. 8B</figref> is a diagram showing the remaining part of the flowchart.</p>
<p id="p-0122" num="0121">The block specifying unit <b>24</b> of the storage controller <b>12</b> performs the block specifying process based on the flowchart shown in <figref idref="DRAWINGS">FIGS. 8A and 8B</figref>. This block specifying process is a process for specifying an access target data block requested from the host <b>2</b> through calculation.</p>
<p id="p-0123" num="0122">First, the block specifying unit <b>24</b> sets an access target area start address sta_adr, an access data size d_sz, the number of drives in array drv_num, and a stripe size sb_sz in the work area of the local memory <b>126</b> (Step S<b>21</b>). In Step S<b>21</b>, the block specifying unit <b>24</b> defines, as variables, a total block size in stripe group (hereinafter, referred to as a total data block size) sg_sz, an access target area end address end_adr, an access target stripe group number acs_sg[ ], an access target data block number acs_dblk[ ], an access offset address in block acs_ofs[ ], an access size in block acs_sz[ ], the number of access target data blocks acs_num, and an access target data block end address acs_end. Such variables are set in the work area.</p>
<p id="p-0124" num="0123">The access target area start address sta_adr represents a start address of an access target area requested from the host <b>2</b>. The access data size d_sz represents the size of the access target area. The total data block size sg_sz represents the size of all the data blocks in the stripe group. The access target area end address end_adr represents an address acquired by adding one to an address of the end of the access target area.</p>
<p id="p-0125" num="0124">The access target stripe group number acs_sg[ ] represents a number (stripe group number) of the access target stripe group. The access target data block number acs_dblk[ ] represents a number (data block number) of the access target data block. The access offset address in block acs_ofs[ ] represents an offset from the start of the access target data block to the access target area start address sta_adr. The access size acs_sz[ ] within the block is a size of the access target data block to be actually accessed and represents a size from the position of the access target area start address sta_adr in the data block to the end of the data block. The number of access target data blocks acs_num represents the number of access target data blocks. The access target data block end address acs_end represents an address of the end of the access target data block.</p>
<p id="p-0126" num="0125">Next, the block specifying unit <b>24</b> calculates the total data block size sg_sz based on the stripe size sb_sz and the number of drives in array drv_num, using the following equation (Step S<b>22</b>).
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>sg_sz=sb_sz&#xd7;(drv_num&#x2212;1)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0127" num="0126">In addition, the block specifying unit <b>24</b> calculates the access target area end address end_adr based on the access target area start address sta_adr and the access data size d_sz, using the following equation (Step S<b>23</b>).
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>end_adr=sta_adr+d_sz<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0128" num="0127">Next, the block specifying unit <b>24</b> performs loop S<b>24</b>, thereby generating a list of access target data blocks. Here, when the number of blocks that can be access at the same time from the storage controller <b>12</b> to the SSD array <b>110</b>-* is n, the block specifying unit <b>24</b> sequentially repeats loop S<b>24</b> for a variable i from i=0 to i=n&#x2212;1. However, in a case where a determination made in Step S<b>24</b><i>e </i>to be described later is No, the parity block arrangement calculation unit <b>22</b> exits loop S<b>24</b>.</p>
<p id="p-0129" num="0128">In loop S<b>24</b>, the block specifying unit <b>24</b> calculates the access target stripe group number acs_sg[i], the access target data block number acs_dblk[i], the access offset address in block acs_ofs[i], the access size in block acs_sz[i], and the access target data block end address acs_end (Step S<b>24</b><i>a</i>). Details of Step S<b>24</b><i>a </i>are as follows.</p>
<p id="p-0130" num="0129">First, in Step S<b>24</b><i>a</i>, the block specifying unit <b>24</b> calculates the access target stripe group number acs_sg[i] based on the access target area start address sta_adr and the total data block size sg_sz, using the following equation.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>acs_sg[i]=sta_adr/sg_sz<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0131" num="0130">In addition, in Step S<b>24</b><i>a</i>, the block specifying unit <b>24</b> calculates the access target data block number acs_dblk[i] based on the access target area start address sta_adr, the total data block size sg_sz, and the stripe size sb_sz, using the following equation.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>acs_dblk[i]=(sta_adr % sg_sz)/sb_sz<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0132" num="0131">Here, % represented in (sta_adr % sg_sz) represents a calculation for acquiring the remainder in a case where an integer part of the quotient acquired by dividing sta_adr by sg_sz is calculated.</p>
<p id="p-0133" num="0132">In addition, in Step S<b>24</b><i>a</i>, the block specifying unit <b>24</b> calculates the access offset address in block acs_ofs[i] based on the access target area start address sta_adr, the total data block size sg_sz, and the stripe size sb_sz, using the following equation.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>acs_ofs[i]&#x2212;(sta_adr % sg_sz) % sb_sz<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0134" num="0133">In addition, in Step S<b>24</b><i>a</i>, the block specifying unit <b>24</b> calculates the access size in block acs_sz[i] based on the stripe size sb_sz and the calculated access offset address in block acs_ofs[i] using the following equation.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>acs_sz[<i>i</i>]=sb_sz&#x2212;acs_ofs[<i>i]</i><?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0135" num="0134">Furthermore, in Step S<b>24</b><i>a</i>, the block specifying unit <b>24</b> calculates the access target data block end address acs_end based on the access target area address sta_adr, the calculated access offset address in block acs_ofs[i], and the stripe size sb_sz using the following equation.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>acs_end=sta_adr&#x2212;acs_ofs[<i>i</i>]+sb_sz<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0136" num="0135">Next, the block specifying unit <b>24</b> determines whether the access target data block end address acs_end calculated in Step S<b>24</b><i>a </i>is higher than the access target area end address end_adr (Step S<b>24</b><i>b</i>). If the access target data block end address acs_end is higher than the access target area end address end_adr (Yes in Step S<b>24</b><i>b</i>), the block specifying unit <b>24</b> corrects the access size in block acs_sz[i] calculated in Step S<b>24</b><i>a </i>for a difference between the access target data block end address acs_end and the access target area end address end_adr, using the following equation (Step S<b>24</b><i>c</i>).
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>acs_sz[<i>i</i>]=acs_sz[<i>i]</i>&#x2212;(acs_end&#x2212;end_adr)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0137" num="0136">Then, the block specifying unit <b>24</b> proceeds to Step S<b>24</b><i>d</i>. On the other hand, if the access target data block end address acs_end is not higher than the access target area end address end_adr (No in Step S<b>24</b><i>c</i>), the block specifying unit <b>24</b> skips Step S<b>24</b><i>c </i>and proceeds to Step S<b>24</b><i>d. </i></p>
<p id="p-0138" num="0137">In Step S<b>24</b><i>d</i>, the block specifying unit <b>24</b> updates the access target area start address sta_adr by the current access size in block acs_sz[i] using the following equation.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>sta_adr=sta_adr+acs_sz[<i>i]</i><?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0139" num="0138">Next, the block specifying unit <b>24</b> determines whether the updated access target area start address sta_adr is lower than the access target area end address end_adr (Step S<b>24</b><i>e</i>).</p>
<p id="p-0140" num="0139">If the updated access target area start address sta_adr is lower than the access target area end address end_adr (Yes in Step S<b>24</b><i>e</i>), the block specifying unit <b>24</b> determines that there is a new access target block. In such a case, the block specifying unit <b>24</b> increments the variable i. Then, when the variable i after the increment is less than the number n of blocks that can be accessed at the same time, loop S<b>24</b> is performed again using the variable i after the increment.</p>
<p id="p-0141" num="0140">On the other hand, when the updated access target area start address sta_adr is not lower than the access target area end address end_adr (No in Step S<b>24</b><i>e</i>), the block specifying unit <b>24</b> exits loop S<b>24</b> and proceeds to Step S<b>25</b>. In addition, also when the variable i after the increment is not less than the number n of blocks that can be accessed at the same time, the block specifying unit <b>24</b> exits loop S<b>24</b> and proceeds to Step S<b>25</b>.</p>
<p id="p-0142" num="0141">In Step S<b>25</b>, the block specifying unit <b>24</b> sets the number of access target data blocks acs_num to the value of the current variable i. At this time, in the work area, for each variable i from 0 to i, the access target stripe group number acs_sg[i], the access target data block number acs_dblk[i], the access offset address in block acs_ofs[i], and the access size in block acs_sz[i] are set as elements of an access target data block list.</p>
<p id="p-0143" num="0142">The block specifying unit <b>24</b> sets the access target stripe group number acs_sg[ ] as a target stripe group number sg_no for each access target stripe group number acs_sg[ ] (Step S<b>26</b>). In Step S<b>26</b>, the block specifying unit <b>24</b> sets the access target data block number acs_dblk[ ] (as the target data block number dblk_no for each target data block number acs_dblk[ ].</p>
<p id="p-0144" num="0143">In addition, in Step S<b>26</b>, the block specifying unit <b>24</b> causes the parity block arrangement calculation unit <b>22</b> to calculate the parity block location in the access target stripe group represented by the target stripe group number sg_no for each target stripe group number sg_no based on the flowchart shown in <figref idref="DRAWINGS">FIG. 6</figref>. Furthermore, in Step S<b>26</b>, the block specifying unit <b>24</b> causes the data block arrangement calculation unit <b>23</b> to calculate the location of the access target data block represented by the target data block number dblk_no for each target data block number dblk_no based on the flowchart shown in <figref idref="DRAWINGS">FIG. 7</figref>. That is, the block specifying unit <b>24</b> specifies an access target data block and a parity block (hereinafter, referred to as an access target parity block) in the stripe group in which the access target data block is present.</p>
<p id="p-0145" num="0144">The array access controller <b>27</b> accesses the specified access target data block for each target data block number dblk_no (Step S<b>27</b>). More specifically, the array access controller <b>27</b> accesses an area of a size represented by acs_sz[ ] from a location starting from acs_ofs[ ] in the specified access target data block. As such accesses, a read access and a write access differ from each other as follows.</p>
<p id="p-0146" num="0145">First, in a case where a read access is requested form the host <b>2</b>, the array access controller <b>27</b> reads data from the specified access target data block in the SSD array <b>110</b>-*. On the other hand, in a case where a write access is requested from the host <b>2</b>, the array access controller <b>27</b> reads data (old data) of the specified access target data block and data (old parity) of the specified access target parity block. The array access controller <b>27</b> updates the old data with write data as new data supplied from the host <b>2</b>. The array access controller <b>27</b> generates a new parity based on the new data and the old parity (for example, based on an exclusive-OR operation between the new data and the old parity). The array access controller <b>27</b> writes the new data to the access specified target data block and writes the new parity to the specified parity block.</p>
<p id="p-0147" num="0146">In the embodiment, the parity blocks P are arranged so as to be regularly distributed in units of management blocks such that the numbers of the parity blocks P arranged in the SSD-#<b>0</b> to SSD-#<b>6</b> (SSDs <b>11</b>-<b>0</b> to <b>11</b>-<b>6</b>) included in the SSD array <b>110</b>-* are non-uniform. In the state in which the non-uniform arrangement of the parity blocks P is maintained, the use of the SSD array <b>110</b>-* is continued.</p>
<p id="p-0148" num="0147">Then, the number of data write operations to an SSD (that is, SSD-#<b>0</b> connected to the drive slot #<b>0</b>) in which the arranged number of the parity blocks P is the largest within the SSD array <b>110</b>-* arrives at the upper limit first. In such a case, in the conventional technology, a hot spare SSD is allocated to the SSD array <b>110</b>-*, and all the data of SSD-#<b>0</b> (first SSD) is saved in the hot spare SSD. Here, the hot spare SSD is connected to the drive slot #<b>7</b> and is assumed to be in the standby state.</p>
<p id="p-0149" num="0148">When all the data of SSD-#<b>0</b> (first SSD) is saved on the hot spare SSD, SSD-#<b>0</b> is replaced with another SSD (second SSD). That is, SSD-#<b>0</b> is detached from the drive slot #<b>0</b>, and the other SSD is newly connected to the drive slot #<b>0</b>. Thereafter, the data of the hot spare SSD is rewritten to the other SSD (hereinafter, referred to as a replacement SSD). Such rewriting is called copy back, and the SSD replacement process ends in accordance with the rewriting.</p>
<p id="p-0150" num="0149">However, in such an SSD replacement process, the replacement SSD, similarly to SSD-#<b>0</b>, is an SSD in which the number of arranged parity blocks P is the largest within the SSD array <b>110</b>-*. In such a case, the number of data write operations to the SSD in the SSD array <b>110</b>-* is not in proportion to the number of parity blocks P in the SSD. That is, the number of data write operations to the SSD included in the SSD array <b>110</b>-* does not correspond to the number of parity blocks P included in the SSD. Thus, for all the SSDs included in the SSD array <b>110</b>-*, the order in which the number of data write operations to each SSD arrives at the upper limit cannot be controlled. Thus, in the embodiment, the drive replacement controller <b>25</b> (more specifically, the block rearrangement unit <b>250</b> of the drive replacement controller <b>25</b>) rearranges the parity blocks P and the data blocks D<b>0</b> to D<b>5</b> through copying in the SSD array <b>110</b>-* such that the replacement SSD is in a state in which the number of arranged parity blocks P is the smallest (specifically, a state in which the parity block P is not arranged).</p>
<p id="p-0151" num="0150"><figref idref="DRAWINGS">FIG. 9</figref> shows, in association with the array management tables <b>28</b> and <b>29</b>, an example of the state after completion of the rearrangement of parity blocks P and data blocks D<b>0</b> to D<b>5</b> arranged as shown in <figref idref="DRAWINGS">FIG. 5</figref> in block rearrangement applied to the embodiment. Here, the state of the management block boundary offset bdr_ofs, the drive member number, the parity blocks P, and the data blocks D<b>0</b> to D<b>6</b> before the rearrangement is also shown. In <figref idref="DRAWINGS">FIG. 9</figref>, description of &#x201c;A&#x2192;B&#x201d; including symbol &#x201c;&#x2192;&#x201d; represents that the state before rearrangement is A, and the state after rearrangement is B.</p>
<p id="p-0152" num="0151"><figref idref="DRAWINGS">FIG. 9</figref> is premised on a case in which SSD-#<b>0</b> connected to a drive slot #<b>0</b> is replaced, and the parity blocks P and the data blocks D<b>0</b> to D<b>5</b> are rearranged. In such block rearrangement, the block rearrangement unit <b>250</b>, as shown in <figref idref="DRAWINGS">FIG. 9</figref>, increases the management block boundary offset bdr_ofs by one. From this, the location of a beginning stripe group of the management block is shifted by one in the direction in which the stripe group number increases, compared to the state before the rearrangement (see <figref idref="DRAWINGS">FIG. 5</figref>). In such a case, the beginning stripe group of the management block before the rearrangement is an ending stripe group of the management block after the rearrangement.</p>
<p id="p-0153" num="0152">In addition, the block rearrangement unit <b>250</b> sets the drive member number of the replacement SSD (the replacement SSD may be denoted by SSD-#<b>0</b> for convenience sake) that is newly connected to the drive slot #<b>0</b> to <b>6</b>. This is equivalent to a case where the drive member number of the replacement SSD (SSD-#<b>0</b>) changes from 0 to 6 as shown in <figref idref="DRAWINGS">FIG. 9</figref>. By allocating (changing) the drive member number, the replacement SSD (SSD-#<b>0</b>) is used as a drive in which the parity block P is not arranged.</p>
<p id="p-0154" num="0153">Furthermore, the block rearrangement unit <b>250</b> decreases the drive member numbers <b>1</b> to <b>6</b> of SSD-#<b>1</b> to SSD-#<b>6</b> connected to drive slots #<b>1</b> to #<b>6</b> other than the drive slot #<b>0</b> by one. That is, the block rearrangement unit <b>250</b> changes the drive member numbers of SSD-#<b>1</b> to SSD-#<b>6</b> from &#x201c;1 to 6&#x201d; to &#x201c;0 to 5&#x201d;.</p>
<p id="p-0155" num="0154">As described above, in the embodiment, the numbers of parity blocks P arranged in SSD-#<b>0</b> to SSD-#<b>6</b> in units of management blocks depend on the drive member numbers of SSD-#<b>0</b> to SSD-#<b>6</b>. In a case where the drive member numbers of SSD-#<b>1</b> to SSD-#<b>6</b> are changed to 0 to 5 as described above, the parity blocks P and the data blocks D<b>0</b> to D<b>5</b> need to be rearranged such that the numbers of the parity blocks P arranged in SSD-#<b>11</b> to SSD-#<b>6</b> in units of management blocks are changed from &#x201c;5 to 0&#x201d; to &#x201c;6 to 1&#x201d;. In addition, the parity blocks P and the data blocks D<b>0</b> to D<b>5</b> need to be rearranged such that the number of parity blocks P arranged, in units of management blocks, in a replacement SSD (SSD-#<b>0</b>) that is newly connected to the drive slot #<b>0</b> is zero.</p>
<p id="p-0156" num="0155">That is, in the embodiment, for the rearrangement of the parity blocks P and data blocks D<b>0</b> to D<b>5</b>, the parity arrangement pattern is changed. However, when the location (that is, the location of the parity block P arranged in the beginning stripe group of the management block) that is a starting point of the parity arrangement pattern before change and the location that is a starting point of the parity arrangement pattern after change are used as references, the relative positional relation between both patterns coincides in all stripe groups within the management block. Accordingly, the parity arrangement pattern can be regarded as not being logically changed.</p>
<p id="p-0157" num="0156">In the embodiment to which the above-described parity arrangement pattern is applied in units of management blocks, by changing the drive member numbers as described above, the rearrangement of the parity blocks P and data blocks D<b>0</b> to D<b>5</b> can be suppressed to a minimal level as described below. First, in the embodiment, the replacement SSD (SSD-#O<b>0</b>) is used as a drive in which the parity block P is not arranged. Accordingly, all the parity blocks P in the replacement SSD (SSD-#<b>0</b>) and the data blocks D<b>0</b> to D<b>6</b> in the stripe group including the parity blocks P may be rearranged.</p>
<p id="p-0158" num="0157">Thus, the block rearrangement unit <b>250</b> rearranges the parity blocks P and the data blocks D<b>0</b> to D<b>5</b> only in stripe groups (hereinafter, referred to as rearrangement target stripe groups) including stripe blocks in which the parity blocks P are arranged in the replacement SSD (SSD-#<b>0</b>). The block rearrangement unit <b>250</b> performs this rearrangement in accordance with the above-described parity arrangement pattern (arrangement rule) in units of new management blocks after the management block boundary offset (bdr_ofs) is increased by one.</p>
<p id="p-0159" num="0158">In the example shown in <figref idref="DRAWINGS">FIG. 9</figref>, the rearrangement target stripe groups include stripe groups having stripe group numbers of 0, 6, 11, 15, 18, 20, and 21. Such stripe groups correspond to the stripe groups including stripe blocks in which the parity blocks P are stored in the SSD (SSD-#<b>0</b>) before replacement. The block rearrangement unit <b>250</b> rearranges (copies) the parity blocks P (parity data) arranged (stored), within the stripe groups having stripe group numbers of 0, 6, 11, 15, 18, 20, and 21, in the SSD (SSD-#<b>0</b>) before replacement in SSD-#<b>1</b>, SSD-#<b>6</b>, SSD-#<b>5</b>, SSD-#<b>4</b>, SSD-#<b>3</b>, SSD-#<b>2</b>, and SSD-#<b>1</b>.</p>
<p id="p-0160" num="0159">According to the embodiment, as shown in <figref idref="DRAWINGS">FIG. 9</figref>, while a minimum number of the parity blocks P and data blocks D<b>0</b> to D<b>5</b> are rearranged, the arrangement state of the parity blocks P and data blocks D<b>0</b> to D<b>5</b> according to the above-described parity arrangement pattern (arrangement rule) in units of new management blocks after an increase of one in the management block boundary offset (bdr_ofs) can be maintained. That is, according to the embodiment, the SSD array <b>110</b>-* after the replacement of the SDD can be maintained in a state in which the parity block location, the data block locations, the access target data block, and the access target parity block can be calculated based on the flowcharts shown in <figref idref="DRAWINGS">FIGS. 6</figref>, <b>7</b>, <b>8</b>A, and <b>8</b>B.</p>
<p id="p-0161" num="0160">According to the parity arrangement pattern (arrangement rule) applied to the embodiment, in the SSD array <b>110</b>-*, there is the only drive (SSD) in which the parity block P is not arranged. In addition, in the embodiment, the replacement SSD (SSD-#<b>0</b>) is used as a drive in which the parity block P is not arranged. Thus, according to the embodiment, stripe groups including stripe blocks, in the replacement SSD (SSD-#<b>0</b>), in which the parity blocks P are arranged can be simply specified as rearrangement target stripe groups. In addition, the block rearrangement after the replacement of the drive can be suppressed to a minimum level. Furthermore, according to the minimal block rearrangement, the regularity of the parity block location and the data block locations can be maintained. From this, the management of the parity block locations and the data block locations can be realized without using an arrangement location management table requiring a large capacity of the memory.</p>
<p id="p-0162" num="0161">In addition, according to the embodiment, the parity block P is not arranged in the replacement SSD (SSD-#<b>0</b>), and the number of parity blocks P arranged in units of management blocks in the other SSD-#<b>1</b> to SSD-#<b>6</b> increases by one from &#x201c;5 to 0&#x201d; to &#x201c;6 to 1&#x201d;. From this, according to the embodiment, for example, the order in the case of aligning SSD-#<b>0</b> to SSD-#<b>6</b> of the SSD array <b>110</b>-* in the descending order of the numbers of write operations coincides with the order in the case of aligning SSD-#<b>0</b> to SSD-#<b>6</b> in the descending order of the number of the arranged parity blocks P even after the block rearrangement. That is, the relation of the numbers of data write operations to SSD-#<b>0</b> to SSD-#<b>6</b> coincides with the relation of the numbers of arranged parity blocks P in SSD-#<b>0</b> to SSD-#<b>6</b>. Accordingly, in a case where the SSD array <b>110</b>-* is continuously operated after the block rearrangement, the SSD <b>11</b>-* of which the number of write operations arrives at the upper limit first can be maintained in the SSD in which the number of the arranged parity blocks P is the largest. From this, even after the block rearrangement, the state in which the numbers of write operations for a plurality of SSDs <b>11</b>-* in the SSD array <b>110</b>-* are close to the upper limit at the same time can be prevented from occurring.</p>
<p id="p-0163" num="0162">However, even in the middle of the rearrangement of the parity blocks P and data blocks D<b>0</b> to D<b>6</b>, a consistency relating to the calculation of the parity block location, the calculation of the data block locations, the specifying of an access target data block, and the specifying of an access target parity block need to be maintained. Accordingly, before the rearrangement of the parity blocks P and data blocks D<b>0</b> to D<b>6</b>, the block rearrangement unit <b>250</b> copies the value of the management block boundary offset (bdr_ofs) to the PR block boundary offset (the management block boundary offset before rearrangement), and copies the drive member number to the PR member number (prior-rearrangement member number). That is, the block rearrangement unit <b>250</b> sets the PR block boundary offset to the management block boundary offset before block rearrangement and sets the PR member number to the drive member number before block rearrangement.</p>
<p id="p-0164" num="0163">Then, while updating the R stripe group number (the stripe group number during rearrangement, that is, a stripe group number representing a stripe group during block rearrangement) in the ascending order of the stripe group numbers of all the rearrangement target stripe groups, the block rearrangement unit <b>250</b> rearranges the parity blocks P and the data blocks D<b>0</b> to D<b>6</b>. That is, while updating the R stripe group number in order from the stripe group number of the beginning stripe group of all the rearrangement target stripe groups, the block rearrangement unit <b>250</b> performs block rearrangement for each rearrangement target stripe group.</p>
<p id="p-0165" num="0164">The block rearrangement determination unit <b>26</b> determines, based on the R stripe group number, an area (hereinafter, referred to as a rearrangement-completed area) in which the block rearrangement in the SSD array <b>110</b>-* has been completed and an area (hereinafter, referred to as a rearrangement not-completed area) in which the block rearrangement has not been completed. In addition, the block rearrangement determination unit <b>26</b> changes, based on a result of the determination, some parameter values used in the process according to the flowcharts shown in <figref idref="DRAWINGS">FIGS. 6</figref>, <b>7</b>, <b>8</b>A, and <b>8</b>B for each determined area. Here, some parameter values described above are the parity block arrangement drive member number (pdrv_no), the data block arrangement drive member number (dblk_drv), and the management block boundary offset (bdr_ofs).</p>
<p id="p-0166" num="0165"><figref idref="DRAWINGS">FIG. 10</figref> shows an example of parameter value switching in a table form. In a case where the access target area is a rearrangement-completed area, a drive member number stored in the array management table <b>29</b> is used as the parity block arrangement drive member number (pdrv_no) and the data block arrangement drive member number (dblk_drv), and a management block boundary offset stored in the array management table <b>28</b> is used as the management block boundary offset (bdr_ofs). On the other hand, in a case where the access target area is a rearrangement not-completed area, a PR member number (a member number before rearrangement) stored in the array management table <b>29</b> is used as the parity block arrangement drive member number (pdrv_no) and the data block arrangement drive member number (dblk_drv), and a PR block boundary offset stored in the array management table <b>28</b> is used as the management block boundary offset (bdr_ofs).</p>
<p id="p-0167" num="0166">During the block rearrangement, a rearrangement-completed area and a rearrangement not-completed area are generated in the SSD array <b>110</b>-*. Accordingly, before starting the block rearrangement, the block rearrangement unit <b>250</b> stores a value of the management block boundary offset included in the array management table <b>28</b> before the start of the block rearrangement and a drive member number included in the array management table <b>29</b> before the start of the block rearrangement in the PR block boundary offset field of the array management table <b>28</b> and prior-rearrangement member number field of the array management table <b>29</b>.</p>
<p id="p-0168" num="0167">From this, even in a case where an access area designated by an access request from the host <b>2</b> is a rearrangement not-completed area, the block rearrangement determination unit <b>26</b> can specify an access target data block and an access target parity block. Here, a boundary between the rearrangement-completed area and a rearrangement not-completed area in the SSD array <b>110</b>-* is specified based on the R stripe group number included in the array management table <b>28</b>.</p>
<p id="p-0169" num="0168">In the case of an access to a stripe group represented by the R stripe group number, the block rearrangement determination unit <b>26</b> determines that the block rearrangement is being performed for the stripe group. In such a case, the block rearrangement determination unit <b>26</b> notifies the array access controller <b>27</b> so as to hold the access requested from the host <b>2</b> until the block rearrangement is completed.</p>
<p id="p-0170" num="0169">When the stripe group number of the stripe group (hereinafter, referred to as a target stripe group) in which the access target data block is present is less than the R stripe group number, the block rearrangement determination unit <b>26</b> determines the target stripe group as a rearrangement-completed area. On the other hand, when the stripe group number of the target stripe group is greater than the R stripe group number, the block rearrangement determination unit <b>26</b> determines the target stripe group as a rearrangement not-completed area. The block specifying unit <b>24</b> specifies an access target data block in the rearrangement-completed area or in the rearrangement not-completed area based on a result of the determination.</p>
<p id="p-0171" num="0170">Hereinafter, an exemplary procedure of the rearrangement completion/rearrangement incompletion determination process applied to the embodiment will be described with reference to a flowchart shown in <figref idref="DRAWINGS">FIG. 11</figref>. The rearrangement completion/rearrangement incompletion determination process is a process for determining whether the target stripe group is a rearrangement-completed area or a rearrangement not-completed area.</p>
<p id="p-0172" num="0171">First, the block rearrangement determination unit <b>26</b> sets an access target area start address sta_adr, an access data size d_sz, the number of drives in array drv_num, a stripe size sb_sz, and an R stripe group number realoc_no in the work area of the local memory <b>126</b> (Step S<b>31</b>). In Step S<b>31</b>, the block specifying unit <b>24</b> defines a total block size sg_sz, an access target beginning stripe group number sg_no_sta, and an access target final stripe group number sg_no_end as variables. Such variables are set within the work area. The access target beginning stripe group number sg_no_sta represents a stripe group number of a stripe group to which a starting end of the access target area requested from the host <b>2</b> belongs. The access target final stripe group number sg_no_end represents a stripe group number of a stripe group to which a final end of the access target area belongs.</p>
<p id="p-0173" num="0172">Next, the block rearrangement determination unit <b>26</b> calculates a total data block size sg_sz (Step S<b>32</b>). The calculation of the total data block size sg_sz, which is performed by the block rearrangement determination unit <b>26</b>, is performed similarly to the case (Step S<b>21</b>) where the total data block size sg_sz is calculated by the block specifying unit <b>24</b>.</p>
<p id="p-0174" num="0173">In Step S<b>32</b>, the block rearrangement determination unit <b>26</b> calculates an access target beginning stripe group number sg_no_sta based on the access target area start address sta_adr and the total data block size sg_sz using the following equation.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>sg_no_sta=sta_adr/sg_sz<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0175" num="0174">In addition, in Step S<b>32</b>, the block rearrangement determination unit <b>26</b> calculates an access target final stripe group number sg_no_end based on the access target area start address sta_adr, the access data size d_sz, and the total data block size sg_sz using the following equation.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>sg_no_end=(sta_adr+d_sz&#x2212;1)/sg_sz<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0176" num="0175">Next, the block rearrangement determination unit <b>26</b> determines whether the access target beginning stripe group number sg_no_sta or the access target final stripe group number sg_no_end is equal to the R stripe group number realoc_no (Step S<b>33</b>). If the determination of Step S<b>33</b> is &#x201c;No&#x201d;, the block rearrangement determination unit <b>26</b> proceeds to Step S<b>34</b>.</p>
<p id="p-0177" num="0176">In Step S<b>34</b>, the block rearrangement determination unit <b>26</b> determines whether the access target beginning stripe group number sg_no_sta is less than the R stripe group number realoc_no, and the access target final stripe group number sg_no_end is more than the R stripe group number realoc_no. If the determination of Step S<b>34</b> is &#x201c;No&#x201d;, the block rearrangement determination unit <b>26</b> proceeds to Step S<b>35</b>.</p>
<p id="p-0178" num="0177">In Step S<b>35</b>, the block rearrangement determination unit <b>26</b> determines whether the access target beginning stripe group number sg_no_sta is less than the R stripe group number realoc_no. Here, it is assumed that the determination of Step S<b>35</b> is &#x201c;Yes&#x201d;. That is, it is assumed that the access target beginning stripe group number sg_no_sta is less than the R stripe group number realoc_no, and the access target final stripe group number sg_no_end is not more than the R stripe group number realoc_no. In such a case, the block rearrangement determination unit <b>26</b> determines that the access target area is a rearrangement-completed area. Then, the block specifying unit <b>24</b> processes the access request from the host <b>2</b> as an access to a rearrangement-completed area and specifies an access target data block in the rearrangement-completed area (Step S<b>36</b>). From this, the array access controller <b>27</b> accesses the specified access target data block.</p>
<p id="p-0179" num="0178">In contrast to this, it is assumed that the determination of Step S<b>35</b> is &#x201c;No&#x201d;. That is, it is assumed that the access target beginning stripe group number sg_no_sta is more than the R stripe group number realoc_no. In such a case, the block rearrangement determination unit <b>26</b> determines that the access target area is a rearrangement not-completed area. Then, the block specifying unit <b>24</b> processes the access request from the host <b>2</b> as an access to a rearrangement not-completed area and specifies an access target data block in the rearrangement not-completed area (Step S<b>37</b>). From this, the array access controller <b>27</b> accesses the specified access target data block.</p>
<p id="p-0180" num="0179">On the other hand, it is assumed that the determination of Step S<b>33</b> is &#x201c;Yes&#x201d;. That is, it is assumed that the access target beginning stripe group number sg_no_sta or the access target final stripe group number sg_no_end is equal to the R stripe group number realoc_no. In such a case, the block rearrangement determination unit <b>26</b> determines that an area during the block rearrangement is included in the access target area and proceeds to Step S<b>38</b>.</p>
<p id="p-0181" num="0180">Next, it is assumed that the determination of Step S<b>34</b> is &#x201c;Yes&#x201d;. That is, it is assumed that the access target beginning stripe group number sg_no_sta is less than the R stripe group number realoc_no, and the access target final stripe group number sg_no_end is more than the R stripe group number realoc_no. Also in such a case, the block rearrangement determination unit <b>26</b> determines that an area during the block rearrangement is included in the access target area and proceeds to Step S<b>38</b>.</p>
<p id="p-0182" num="0181">In Step S<b>38</b>, since an area during the block rearrangement is included in the access target area, the block rearrangement determination unit <b>26</b> notifies the array access controller <b>27</b> to hold the access requested from the host <b>2</b>. Then, the block rearrangement determination unit <b>26</b> returns the process to Step S<b>31</b>.</p>
<p id="p-0183" num="0182">According to the embodiment, the locations of the parity blocks P and data blocks D<b>0</b> to D<b>6</b> in the SSD array <b>110</b>-* are managed based on a predetermined parity arrangement pattern (arrangement rule) in units of management blocks comprising a plurality of continuous stripe groups. Thus, according to the embodiment, a mapping table does not need to be used for managing the locations of the parity blocks P and data blocks D<b>0</b> to D<b>6</b> in the SSD array <b>110</b>-*, and the management thereof can be realized using a small amount of memory capacity.</p>
<p id="p-0184" num="0183">In addition, according to the embodiment, the block rearrangement is performed only in a case where an SSD for which the number of write operations arrives at the upper limit out of a plurality of SSDs (more specifically, SSD-#<b>0</b> to SSD-#<b>6</b>) configuring the SSD array <b>110</b>-* is replaced. Accordingly, an increase in the number of data write operations to each SSD due to the block rearrangement can be suppressed to a minimum level.</p>
<p id="h-0019" num="0000">[Modification]</p>
<p id="p-0185" num="0184">Next, a modification of the embodiment will be described. A feature of the modification is that automatic SSD replacement is realized using a hot spare SSD. First, in the embodiment, an overview of block rearrangement executed by the block rearrangement unit <b>250</b> will be described as below. As described above, the block rearrangement unit <b>250</b> increases the management block boundary offset (bdr_ofs) by one in the block rearrangement. In addition, the block rearrangement unit <b>250</b> sets the drive member number of the replacement SSD (SSD-#<b>0</b>) to six, thereby defining the replacement SSD (SSD-#<b>0</b>) as a drive in which the parity block P is not arranged. Furthermore, the block rearrangement unit <b>250</b> increases the drive member number of the other SSDs (SSD-#<b>1</b> to SSD-#<b>6</b>) by one. Then, the block rearrangement unit <b>250</b> rearranges the parity blocks P and the data blocks D<b>0</b> to D<b>6</b> based on the management block boundary offset (bdr_ofs) and the drive member number only for stripe groups (rearrangement target stripe group), in the replacement SSD (SSD-#<b>0</b>), including stripe blocks in which the parity blocks P are arranged.</p>
<p id="p-0186" num="0185">The block rearrangement unit <b>250</b> registers, as the R stripe group number (a stripe group number during parity rearrangement), the stripe group number of a stripe group from which the block rearrangement is started reloc_no. Then, the block rearrangement unit <b>250</b> executes the block rearrangement while registering, as the R drive slot number (a drive slot number during parity rearrangement), the drive slot number of the SSD <b>11</b>-* in which the current rearrangement target block (the parity block P or the data block Dj) is present.</p>
<p id="p-0187" num="0186">In the embodiment, a hot spare SSD (hereinafter, referred to as an SSD-SP) is connected to the drive slot #<b>7</b>. For example, in a case where SSD-#<b>0</b> is replaced in this state, the drive replacement controller <b>25</b> saves all the data of SSD-#<b>0</b> to the SSD-SP (that is, the hot spare SSD allocated to the SSD array <b>110</b>-*). After the saving of the data, when SSD-#<b>0</b> is replaced with another SSD, the drive replacement controller <b>25</b> rewrites the data of the SSD-SP to the other SSD (that is, the replacement SSD). Then, the block rearrangement unit <b>250</b> executes the above-described block rearrangement. Here, the block rearrangement unit <b>250</b> may be independent from the drive replacement controller <b>25</b>.</p>
<p id="p-0188" num="0187"><figref idref="DRAWINGS">FIG. 12</figref> shows, in association with array management tables <b>28</b> and <b>29</b>, an example of the state in the middle of the rearrangement of parity blocks P and the data blocks D<b>0</b> to D<b>5</b> arranged as shown in <figref idref="DRAWINGS">FIG. 5</figref> in the block rearrangement applied to the embodiment. In this way, <figref idref="DRAWINGS">FIG. 12</figref> shows the state in the middle of the block rearrangement, which is different from <figref idref="DRAWINGS">FIG. 9</figref> showing the state after the completion of the block rearrangement. In addition, <figref idref="DRAWINGS">FIG. 12</figref> shows the state of the SSD-SP, which is not shown in <figref idref="DRAWINGS">FIG. 9</figref>.</p>
<p id="p-0189" num="0188">In the example shown in <figref idref="DRAWINGS">FIG. 12</figref>, it is assumed that block rearrangement is executed for a stripe group having a stripe group number of 28. That is, it is assumed that the block rearrangement has been completed up to the stripe group having a stripe group number of 27. The drive member number and the PR member number (prior-rearrangement member number) of the SSD-SP are the same as those of the replacement SSD (SSD-#<b>0</b>).</p>
<p id="p-0190" num="0189">As is apparent from <figref idref="DRAWINGS">FIG. 12</figref>, when the data of the SSD-SP is included, in the SSD array <b>110</b>-*, only one set of stripe blocks having the same data is present in the same stripe group. That is, there is one set of the stripe blocks arranged in the replacement SSD (SSD-#<b>0</b>) and the stripe blocks arranged in the SSD-SP. The block rearrangement unit <b>250</b> executes the block rearrangement using the stripe blocks.</p>
<p id="p-0191" num="0190">Here, it is assumed that the block rearrangement is interrupted due to cut-off of the power of the storage device <b>1</b> in the middle of the block rearrangement process. Then, the power is assumed to be restored, whereby the storage device <b>1</b> is restarted. In such a case, the block rearrangement unit <b>250</b> resumes the block rearrangement from the state in which the block rearrangement is interrupted. In order to reliably resume the block rearrangement, the block rearrangement unit <b>250</b> executes the block rearrangement while storing at least the R stripe group number and the R drive slot number in a non-volatile storage such as a flash memory or a hard disk drive.</p>
<p id="p-0192" num="0191">In the modification, the drive replacement controller <b>25</b> builds the SSD-SP into the SSD array <b>110</b>-* as a replacement SSD, thereby realizing automatic SSD replacement. Hereinafter, this automatic SSD replacement will be described.</p>
<p id="p-0193" num="0192">Now, similarly to the embodiment, the number of data write operations to SSD-#<b>0</b> is assumed to arrive at the upper limit. In this case, the drive replacement controller <b>25</b> allocates the SSD-SP to the SSD array <b>110</b>-* and copies (saves) all the data of SSD-#<b>0</b> to the SSD-SP. Then, the drive replacement controller <b>25</b> excludes SSD-#<b>0</b> (that is, SSD-#<b>0</b> for which the number of write operations arrives at the upper limit) from the SSD array <b>110</b>-* and regards the SSD-SP as the replacement SSD (new SSD-#<b>0</b>). In this state, the block rearrangement unit <b>250</b>, similarly to the embodiment, executes block rearrangement. A difference from the embodiment is that the replacement SSD (SSD-#<b>0</b>) is the SSD-SP (hereinafter, referred to as a first SSD-SP) that is connected to the drive slot #<b>7</b>.</p>
<p id="p-0194" num="0193">When the block rearrangement is started, the block rearrangement unit <b>250</b> changes the drive member number of SSD-#<b>0</b> for which the number of write operations arrives at the upper limit from 0 to 6. In addition, the block rearrangement unit <b>250</b> sets the drive member number of the first SSD-SP regarded as the replacement SSD to 6. The block rearrangement unit <b>250</b> decreases the drive member numbers <b>1</b> to <b>6</b> of the other SSD-#<b>1</b> to SSD-#<b>6</b> by one. In <figref idref="DRAWINGS">FIG. 12</figref>, the drive member numbers after change are described.</p>
<p id="p-0195" num="0194">After the block rearrangement is completed, the drive replacement controller <b>25</b> newly redefines, as a hot spare SSD (SSD-SP), SSD-#<b>0</b> (that is, the SSD <b>11</b>-<b>0</b>) for which the number of write operations arrives at the upper limit. More specifically, the attribute of SSD-#<b>0</b> (that is, the SSD <b>11</b>-<b>0</b>) is changed from the SSD (SSD-#<b>0</b>) used in the SSD array <b>110</b>-<b>0</b> to the hot spare SSD. Here, SSD-#<b>0</b> after the attribute change is represented as an SSD-SP (specifically, a second SSD-SP). The drive replacement controller <b>25</b> handles the second SSD-SP as a failed drive due to arriving at the upper limit of the number of write operations. From this, the drive replacement controller <b>25</b> separates the second SSD-SP (that is, the SSD <b>11</b>-<b>0</b>) from the SSD array <b>110</b>-*.</p>
<p id="p-0196" num="0195">In addition, when the block rearrangement is executed, the drive replacement controller <b>25</b> defines the first SSD-SP allocated to the SSD array <b>110</b>-* as an SSD used for the SSD array <b>110</b>-* instead of SSD-#<b>0</b> (SSD <b>11</b>-<b>0</b>) for which the number of write operations arrives at the upper limit. More specifically, the attribute of the first SSD-SP is changed from the SSD-SP (hot spare SSD) to an SSD used for the SSD array <b>110</b>-*. The first SSD-SP after the attribute change is represented as SSD-#<b>0</b> for convenience sake.</p>
<p id="p-0197" num="0196">In the modification, the attributes of SSD-#<b>0</b> (that is, SSD-#<b>0</b> for which the number of write operations arrives at the upper limit) and the SSD-SP are changed as described above. Accordingly, the block rearrangement unit <b>250</b> performs block rearrangement for the SSD having the attribute change from the SSD-SP to SSD-#<b>0</b> and having the drive member number set to 6 and the drives SSD-#<b>1</b> to SSD-#<b>6</b> having the drive member numbers being changed from &#x201c;1 to 6&#x201d; to &#x201c;0 to 5&#x201d;. According to the modification, differently from the embodiment, SSD-#<b>0</b> for which the number of write operations arrives at the upper limit does not need to be replaced with another SSD (replacement SSD) for the block rearrangement. That is, according to the modification, automatic SSD replacement can be realized using the SSD-SP that is in the standby state.</p>
<p id="p-0198" num="0197"><figref idref="DRAWINGS">FIG. 13</figref> shows, in association with the array management tables <b>28</b> and <b>29</b>, an example of the state after the completion of the rearrangement of the parity blocks P and data blocks D<b>0</b> to D<b>5</b> arranged as shown in <figref idref="DRAWINGS">FIG. 5</figref> in the block rearrangement applied to the modification. Here, the state of the management block boundary offset (bdr_ofs), the parity blocks P, and the data blocks D<b>0</b> to D<b>6</b> before the rearrangement is also shown.</p>
<p id="p-0199" num="0198">According to at least one embodiment described above, a storage apparatus, a storage controller, and a method capable of managing the locations of error correcting code blocks in an array in a simple manner are provided.</p>
<p id="p-0200" num="0199">While certain embodiments have been described, these embodiments have been presented by way of example only, and are not intended to limit the scope of the inventions. Indeed, the novel embodiments described herein may be embodied in a variety of other forms; furthermore, various omissions, substitutions and changes in the form of the embodiments described herein may be made without departing from the spirit of the inventions. The accompanying claims and their equivalents are intended to cover such forms or modifications as would fall within the scope and spirit of the inventions.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A storage apparatus comprising:
<claim-text>a group of solid state drives including a plurality of solid state drives each comprising a first storage area divided into a plurality of stripe blocks; and</claim-text>
<claim-text>a storage controller configured to control the group of the solid state drives,</claim-text>
<claim-text>wherein</claim-text>
<claim-text>the storage controller comprises an array management unit configured to manage an array comprising a second storage area divided into a plurality of stripe groups, a set of an error correcting code block used for storing an error correcting code and a plurality of data blocks used for storing data being arranged in each of the plurality of stripe groups, and each of the plurality of stripe groups comprising a set of the stripe blocks of which physical positions correspond to each other in the plurality of solid state drives, and</claim-text>
<claim-text>the array management unit is configured to regularly arrange the error correcting code blocks and the data blocks in the plurality of stripe groups based on a predetermined arrangement rule such that the numbers of the arranged error correcting code blocks are non-uniform among the plurality of solid state drives and to manage locations of the error correcting code blocks based on the arrangement rule.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The storage apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the array management unit is further configured to arrange the error correcting code blocks and the data blocks in units of management blocks that are sets of a predetermined number of the stripe groups having continuous physical positions such that the numbers of arranged error correcting code blocks are non-uniform among the plurality of solid state drives.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The storage apparatus of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the storage controller further comprises a block specifying unit configured to specify locations of a first data block and a first parity block based on the arrangement rule when accesses to the first data block and the first parity block are necessary.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The storage apparatus of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein:
<claim-text>the array management unit is further configured to arrange the error correcting code blocks in the plurality of solid state drives, except one of the plurality of solid state drives; and</claim-text>
<claim-text>the storage controller further comprises</claim-text>
<claim-text>a drive replacement controller configured to monitor the numbers of write operations for the plurality of solid state drives and to control replacement of a first solid state drive for which the number of write operations arrives at a predetermined upper limit with a second solid state drive, and</claim-text>
<claim-text>a block rearrangement unit configured to rearrange the error correcting code blocks arranged in the first solid state drive to the other solid state drives excluding the second solid state drive from the plurality of solid state drives after the replacement.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The storage apparatus of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the block rearrangement unit is further configured to rearrange the error correcting code blocks arranged in the first solid state drive such that relation of the numbers of the error correcting code blocks arranged in the other solid state drives before the rearrangement is maintained after the rearrangement.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The storage apparatus of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the block rearrangement unit is further configured to shift start points of the management blocks by one stripe group when the rearrangement is started and to rearrange the error correcting code blocks such that each of first numbers of the error correcting code blocks arranged in the other solid state drives in units of the management blocks increases by one from that before the rearrangement.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The storage apparatus of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein:
<claim-text>the array management unit is further configured to allocate drive member numbers having values corresponding to the number of the error correcting code blocks arranged in the plurality of solid state drives in units of the management blocks to the plurality of solid state drives; and</claim-text>
<claim-text>the block rearrangement unit is further configured to change the drive member numbers allocated to the other solid state drives to values corresponding to the first numbers increased by one and to set the drive member number allocated to the second solid state drive to a value corresponding to a case where a second number of the error correcting code blocks arranged in the second solid state drive in units of the management blocks is zero when the rearrangement is performed.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The storage apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the storage controller further comprises a block specifying unit configured to specify locations of a first data block and a first parity block based on the arrangement rule when accesses to the first data block and the first parity block are necessary.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. A storage controller configured to control a group of solid state drives including a plurality of solid state drives each comprising a first storage area divided into a plurality of stripe blocks, the storage controller comprising:
<claim-text>an array management unit configured to manage an array comprising a second storage area divided into a plurality of stripe groups, a set of an error correcting code block used for storing an error correcting code and a plurality of data blocks used for storing data being arranged in each of the plurality of stripe groups, and each of the plurality of stripe groups comprising a set of the stripe blocks of which physical positions correspond to each other in the plurality of solid state drives,</claim-text>
<claim-text>wherein the array management unit is configured to regularly arrange the error correcting code blocks and the data blocks in the plurality of stripe groups based on a predetermined arrangement rule such that the numbers of the arranged error correcting code blocks are non-uniform among the plurality of solid state drives and to manage locations of the error correcting code blocks based on the arrangement rule.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The storage controller of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the array management unit is further configured to arrange the error correcting code blocks and the data blocks in units of management blocks that are sets of a predetermined number of the stripe groups having continuous physical positions such that the numbers of arranged error correcting code blocks are non-uniform among the plurality of solid state drives.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The storage controller of <claim-ref idref="CLM-00010">claim 10</claim-ref>, further comprising a block specifying unit configured to specify locations of a first data block and a first parity block based on the arrangement rule when accesses to the first data block and the first parity block are necessary.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The storage controller of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein:
<claim-text>the array management unit is further configured to arrange the error correcting code blocks in the plurality of solid state drives, except one of the plurality of solid state drives; and</claim-text>
<claim-text>the storage controller further comprises</claim-text>
<claim-text>a drive replacement controller configured to monitor the numbers of write operations for the plurality of solid state drives and to control replacement of a first solid state drive for which the number of write operations arrives at a predetermined upper limit with a second solid state drive, and</claim-text>
<claim-text>a block rearrangement unit configured to rearrange the error correcting code blocks arranged in the first solid state drive to the other solid state drives excluding the second solid state drive from the plurality of solid state drives after the replacement.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The storage controller of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the block rearrangement unit is further configured to rearrange the error correcting code blocks arranged in the first solid state drive such that relation of the numbers of the error correcting code blocks arranged in the other solid state drives before the rearrangement is maintained after the rearrangement.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The storage controller of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the block rearrangement unit is further configured to shift start points of the management blocks by one stripe group when the rearrangement is started and to rearrange the error correcting code blocks such that each of first numbers of the error correcting code blocks arranged in the other solid state drives in units of the management blocks increases by one from that before the rearrangement.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The storage controller of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein:
<claim-text>the array management unit is further configured to allocate drive member numbers having values corresponding to the number of the error correcting code blocks arranged in the plurality of solid state drives in units of the management blocks to the plurality of solid state drives; and</claim-text>
<claim-text>the block rearrangement unit is further configured to change the drive member numbers allocated to the other solid state drives to values corresponding to the first numbers increased by one and to set the drive member number allocated to the second solid state drive to a value corresponding to a case where a second number of the error correcting code blocks arranged in the second solid state drive in units of the management blocks is zero when the rearrangement is performed.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. A method, implemented in a storage controller, for managing locations of error correcting code blocks in an array, the storage controller being configured to control a group of solid state drives including a plurality of solid state drives, each of the plurality of solid state drives comprising a first storage area divided into a plurality of stripe blocks, the array comprising a second storage area divided into a plurality of stripe groups, a set of an error correcting code block used for storing an error correcting code and a plurality of data blocks used for storing data being arranged in each of the plurality of stripe groups, and each of the plurality of stripe groups comprising a set of the stripe blocks of which physical positions correspond to each other in the plurality of solid state drives, the method comprising:
<claim-text>regularly arranging the error correcting code blocks and the data blocks in the plurality of stripe groups based on a predetermined arrangement rule such that the numbers of the arranged error correcting code blocks are non-uniform among the plurality of solid state drives configuring the array when the array is generated; and</claim-text>
<claim-text>managing locations of the error correcting code blocks based on the arrangement rule.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the error correcting code blocks and the data blocks are arranged in units of management blocks that are sets of a predetermined number of the stripe groups having continuous physical positions such that the numbers of arranged error correcting code blocks are non-uniform among the plurality of solid state drives.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The method of <claim-ref idref="CLM-00017">claim 17</claim-ref>, further comprising specifying locations of a first data block and a first parity block based on the arrangement rule when accesses to the first data block and the first parity block are necessary.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The method of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein:
<claim-text>the error correcting code blocks are arranged in the plurality of solid state drives, except one of the plurality of solid state drives; and</claim-text>
<claim-text>the method further comprises</claim-text>
<claim-text>monitoring the numbers of write operations for the plurality of solid state drives,</claim-text>
<claim-text>controlling replacement of a first solid state drive for which the number of write operations arrives at a predetermined upper limit with a second solid state drive, and</claim-text>
<claim-text>rearranging the error correcting code blocks arranged in the first solid state drive to the other solid state drives excluding the second solid state drive from the plurality of solid state drives after the replacement.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The method of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the error correcting code blocks arranged in the first solid state drive are rearranged such that a relation of the numbers of the error correcting code blocks arranged in the other solid state drives before the rearrangement is maintained after the rearrangement. </claim-text>
</claim>
</claims>
</us-patent-grant>
