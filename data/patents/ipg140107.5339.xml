<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626440-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626440</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>11405228</doc-number>
<date>20060417</date>
</document-id>
</application-reference>
<us-application-series-code>11</us-application-series-code>
<us-term-of-grant>
<us-term-extension>784</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20110101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>19</main-group>
<subgroup>26</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>701436</main-classification>
<further-classification>340905</further-classification>
<further-classification>34099513</further-classification>
<further-classification>701117</further-classification>
<further-classification>701400</further-classification>
<further-classification>701414</further-classification>
<further-classification>701418</further-classification>
<further-classification>701425</further-classification>
</classification-national>
<invention-title id="d2e53">Data-driven 3D traffic views with the view based on user-selected start and end geographical locations</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>4937570</doc-number>
<kind>A</kind>
<name>Matsukawa et al.</name>
<date>19900600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>340905</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5379215</doc-number>
<kind>A</kind>
<name>Kruhoeffer et al.</name>
<date>19950100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>702  3</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5432895</doc-number>
<kind>A</kind>
<name>Myers</name>
<date>19950700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345419</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5583972</doc-number>
<kind>A</kind>
<name>Miller</name>
<date>19961200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345419</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5742924</doc-number>
<kind>A</kind>
<name>Nakayama</name>
<date>19980400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>701458</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>5757290</doc-number>
<kind>A</kind>
<name>Watanabe et al.</name>
<date>19980500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>34099514</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>5802492</doc-number>
<kind>A</kind>
<name>DeLorme et al.</name>
<date>19980900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>4554565</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>5864305</doc-number>
<kind>A</kind>
<name>Rosenquist</name>
<date>19990100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>340905</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>5884217</doc-number>
<kind>A</kind>
<name>Koyanagi</name>
<date>19990300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>701208</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>5908464</doc-number>
<kind>A</kind>
<name>Kishigami et al.</name>
<date>19990600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>5913918</doc-number>
<kind>A</kind>
<name>Nakano et al.</name>
<date>19990600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>5917436</doc-number>
<kind>A</kind>
<name>Endo et al.</name>
<date>19990600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>34099514</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>5936553</doc-number>
<kind>A</kind>
<name>Kabel</name>
<date>19990800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>34099514</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>5982298</doc-number>
<kind>A</kind>
<name>Lappenbusch et al.</name>
<date>19991100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>340905</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>5987377</doc-number>
<kind>A</kind>
<name>Westerlage et al.</name>
<date>19991100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>6006161</doc-number>
<kind>A</kind>
<name>Katou</name>
<date>19991200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>6154689</doc-number>
<kind>A</kind>
<name>Pereira et al.</name>
<date>20001100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>6182453</doc-number>
<kind>B1</kind>
<name>Forsberg</name>
<date>20010200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>6240369</doc-number>
<kind>B1</kind>
<name>Foust</name>
<date>20010500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>6278383</doc-number>
<kind>B1</kind>
<name>Endo et al.</name>
<date>20010800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>6360168</doc-number>
<kind>B1</kind>
<name>Shimabara</name>
<date>20020300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>701211</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>6466862</doc-number>
<kind>B1</kind>
<name>DeKock et al.</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>6520861</doc-number>
<kind>B2</kind>
<name>Shoji et al.</name>
<date>20030200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>6532414</doc-number>
<kind>B2</kind>
<name>Mintz</name>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>6553308</doc-number>
<kind>B1</kind>
<name>Uhlmann et al.</name>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>6600994</doc-number>
<kind>B1</kind>
<name>Polidi</name>
<date>20030700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>6603407</doc-number>
<kind>B2</kind>
<name>Endo et al.</name>
<date>20030800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>6650326</doc-number>
<kind>B1</kind>
<name>Huber et al.</name>
<date>20031100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>6710774</doc-number>
<kind>B1</kind>
<name>Kawasaki et al.</name>
<date>20040300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345419</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>6756919</doc-number>
<kind>B2</kind>
<name>Endo et al.</name>
<date>20040600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>US</country>
<doc-number>6845324</doc-number>
<kind>B2</kind>
<name>Smith</name>
<date>20050100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>US</country>
<doc-number>6919821</doc-number>
<kind>B1</kind>
<name>Smith</name>
<date>20050700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00033">
<document-id>
<country>US</country>
<doc-number>6956590</doc-number>
<kind>B1</kind>
<name>Barton et al.</name>
<date>20051000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00034">
<document-id>
<country>US</country>
<doc-number>7116326</doc-number>
<kind>B2</kind>
<name>Soulchin et al.</name>
<date>20061000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00035">
<document-id>
<country>US</country>
<doc-number>7221287</doc-number>
<kind>B2</kind>
<name>Gueziec et al.</name>
<date>20070500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00036">
<document-id>
<country>US</country>
<doc-number>7486201</doc-number>
<kind>B2</kind>
<name>Kelly et al.</name>
<date>20090200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00037">
<document-id>
<country>US</country>
<doc-number>2001/0008991</doc-number>
<kind>A1</kind>
<name>Hamada et al.</name>
<date>20010700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00038">
<document-id>
<country>US</country>
<doc-number>2001/0021665</doc-number>
<kind>A1</kind>
<name>Gouji et al.</name>
<date>20010900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00039">
<document-id>
<country>US</country>
<doc-number>2002/0087263</doc-number>
<kind>A1</kind>
<name>Wiener</name>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00040">
<document-id>
<country>US</country>
<doc-number>2002/0147544</doc-number>
<kind>A1</kind>
<name>Nicosia et al.</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00041">
<document-id>
<country>US</country>
<doc-number>2002/0193938</doc-number>
<kind>A1</kind>
<name>DeKock et al.</name>
<date>20021200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>701117</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00042">
<document-id>
<country>US</country>
<doc-number>2003/0125846</doc-number>
<kind>A1</kind>
<name>Yu et al.</name>
<date>20030700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00043">
<document-id>
<country>US</country>
<doc-number>2003/0181194</doc-number>
<kind>A1</kind>
<name>Litvak et al.</name>
<date>20030900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00044">
<document-id>
<country>US</country>
<doc-number>2003/0191568</doc-number>
<kind>A1</kind>
<name>Breed</name>
<date>20031000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00045">
<document-id>
<country>US</country>
<doc-number>2003/0225516</doc-number>
<kind>A1</kind>
<name>DeKock et al.</name>
<date>20031200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>701214</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00046">
<document-id>
<country>US</country>
<doc-number>2004/0030493</doc-number>
<kind>A1</kind>
<name>Pechatnikov et al.</name>
<date>20040200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>701208</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00047">
<document-id>
<country>US</country>
<doc-number>2004/0046759</doc-number>
<kind>A1</kind>
<name>Soulchin et al.</name>
<date>20040300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345440</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00048">
<document-id>
<country>US</country>
<doc-number>2004/0143385</doc-number>
<kind>A1</kind>
<name>Smyth et al.</name>
<date>20040700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00049">
<document-id>
<country>US</country>
<doc-number>2005/0143902</doc-number>
<kind>A1</kind>
<name>Soulchin et al.</name>
<date>20050600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>701117</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00050">
<document-id>
<country>US</country>
<doc-number>2006/0058950</doc-number>
<kind>A1</kind>
<name>Kato et al.</name>
<date>20060300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00051">
<document-id>
<country>US</country>
<doc-number>2006/0132482</doc-number>
<kind>A1</kind>
<name>Oh</name>
<date>20060600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00052">
<document-id>
<country>US</country>
<doc-number>2006/0158330</doc-number>
<kind>A1</kind>
<name>Gueziec</name>
<date>20060700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00053">
<document-id>
<country>US</country>
<doc-number>2006/0178807</doc-number>
<kind>A1</kind>
<name>Kato et al.</name>
<date>20060800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00054">
<document-id>
<country>US</country>
<doc-number>2006/0200302</doc-number>
<kind>A1</kind>
<name>Seko</name>
<date>20060900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00055">
<document-id>
<country>US</country>
<doc-number>2006/0247850</doc-number>
<kind>A1</kind>
<name>Cera et al.</name>
<date>20061100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00056">
<document-id>
<country>US</country>
<doc-number>2008/0030504</doc-number>
<kind>A1</kind>
<name>Brunner et al.</name>
<date>20080200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00057">
<document-id>
<country>US</country>
<doc-number>2008/0143727</doc-number>
<kind>A1</kind>
<name>Oh et al.</name>
<date>20080600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00058">
<document-id>
<country>US</country>
<doc-number>2009/0289937</doc-number>
<kind>A1</kind>
<name>Flake et al.</name>
<date>20091100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00059">
<document-id>
<country>WO</country>
<doc-number>WO 2008/060933</doc-number>
<date>20080500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00060">
<othercit>Gueziec, Andre, &#x201c;3D Traffic Visualization in Real Time,&#x201d; ACM Siggraph Technical Sketches, Conference Abstracts and Applications, p. 144, Los Angeles, CA, Aug. 2001.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00061">
<othercit>European Extended Search Report, EP Application No. 10250276.2-1232, dated Aug. 31, 2010.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>9</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>701208</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>701117</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>701118</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>701119</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>701400</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>701410</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>701414</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>701415</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>701418</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>701422</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>701423</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>701425</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>701436</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707200</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>340905</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>340990</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>340995</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3409951</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>34099512</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>34099513</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>34099514</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>34099515</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>702212</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345418</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345419</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>44</number-of-drawing-sheets>
<number-of-figures>44</number-of-figures>
</figures>
<us-related-documents>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60672413</doc-number>
<date>20050418</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20060253245</doc-number>
<kind>A1</kind>
<date>20061109</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Cera</last-name>
<first-name>Christopher D.</first-name>
<address>
<city>Havertown</city>
<state>PA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Soulchin</last-name>
<first-name>Robert M.</first-name>
<address>
<city>King of Prussia</city>
<state>PA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Smyth</last-name>
<first-name>Brian J.</first-name>
<address>
<city>West Chester</city>
<state>PA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="004" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Agree</last-name>
<first-name>Jonathan K.</first-name>
<address>
<city>Yardley</city>
<state>PA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="005" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Auxer</last-name>
<first-name>Gregory A.</first-name>
<address>
<city>Glenmore</city>
<state>PA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="006" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Matzelle</last-name>
<first-name>Brent R.</first-name>
<address>
<city>Philadelphia</city>
<state>PA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="007" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Balcerzak</last-name>
<first-name>Michal</first-name>
<address>
<city>Philadelphia</city>
<state>PA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="008" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Grubner, III</last-name>
<first-name>John B.</first-name>
<address>
<city>Fernandina Beach</city>
<state>FL</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="009" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Smith</last-name>
<first-name>Eric R.</first-name>
<address>
<city>Langhorne</city>
<state>PA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Cera</last-name>
<first-name>Christopher D.</first-name>
<address>
<city>Havertown</city>
<state>PA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Soulchin</last-name>
<first-name>Robert M.</first-name>
<address>
<city>King of Prussia</city>
<state>PA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Smyth</last-name>
<first-name>Brian J.</first-name>
<address>
<city>West Chester</city>
<state>PA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="004" designation="us-only">
<addressbook>
<last-name>Agree</last-name>
<first-name>Jonathan K.</first-name>
<address>
<city>Yardley</city>
<state>PA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="005" designation="us-only">
<addressbook>
<last-name>Auxer</last-name>
<first-name>Gregory A.</first-name>
<address>
<city>Glenmore</city>
<state>PA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="006" designation="us-only">
<addressbook>
<last-name>Matzelle</last-name>
<first-name>Brent R.</first-name>
<address>
<city>Philadelphia</city>
<state>PA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="007" designation="us-only">
<addressbook>
<last-name>Balcerzak</last-name>
<first-name>Michal</first-name>
<address>
<city>Philadelphia</city>
<state>PA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="008" designation="us-only">
<addressbook>
<last-name>Grubner, III</last-name>
<first-name>John B.</first-name>
<address>
<city>Fernandina Beach</city>
<state>FL</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="009" designation="us-only">
<addressbook>
<last-name>Smith</last-name>
<first-name>Eric R.</first-name>
<address>
<city>Langhorne</city>
<state>PA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Lempia Summerfield Katz LLC</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Navteq B.V.</orgname>
<role>03</role>
<address>
<city>Veldhoven</city>
<country>NL</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Holwerda</last-name>
<first-name>Stephen</first-name>
<department>3664</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Traffic flow data representing traffic conditions on a road system is obtained. A user selects a start geographical location and an end geographical location. An animated flythrough 3D graphical map is then created and displayed beginning at the start geographical location and navigating toward the end geographical location. The traffic flow data is continuously displayed during the flythrough.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="190.92mm" wi="249.60mm" file="US08626440-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="243.84mm" wi="192.62mm" orientation="landscape" file="US08626440-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="239.18mm" wi="189.82mm" orientation="landscape" file="US08626440-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="232.83mm" wi="185.42mm" orientation="landscape" file="US08626440-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="227.84mm" wi="168.15mm" orientation="landscape" file="US08626440-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="220.90mm" wi="171.28mm" orientation="landscape" file="US08626440-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="208.36mm" wi="173.14mm" orientation="landscape" file="US08626440-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="216.07mm" wi="182.54mm" orientation="landscape" file="US08626440-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="216.41mm" wi="181.36mm" orientation="landscape" file="US08626440-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="236.73mm" wi="191.69mm" orientation="landscape" file="US08626440-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="232.83mm" wi="178.48mm" orientation="landscape" file="US08626440-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="239.18mm" wi="177.21mm" orientation="landscape" file="US08626440-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="239.18mm" wi="187.96mm" file="US08626440-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="255.52mm" wi="196.77mm" orientation="landscape" file="US08626440-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="236.73mm" wi="139.87mm" file="US08626440-20140107-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="241.38mm" wi="153.67mm" file="US08626440-20140107-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00016" num="00016">
<img id="EMI-D00016" he="235.97mm" wi="158.50mm" file="US08626440-20140107-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00017" num="00017">
<img id="EMI-D00017" he="231.73mm" wi="154.60mm" file="US08626440-20140107-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00018" num="00018">
<img id="EMI-D00018" he="222.76mm" wi="157.56mm" file="US08626440-20140107-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00019" num="00019">
<img id="EMI-D00019" he="219.20mm" wi="150.62mm" file="US08626440-20140107-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00020" num="00020">
<img id="EMI-D00020" he="239.95mm" wi="146.64mm" file="US08626440-20140107-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00021" num="00021">
<img id="EMI-D00021" he="243.76mm" wi="203.96mm" file="US08626440-20140107-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00022" num="00022">
<img id="EMI-D00022" he="235.71mm" wi="209.63mm" file="US08626440-20140107-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00023" num="00023">
<img id="EMI-D00023" he="222.84mm" wi="195.41mm" orientation="landscape" file="US08626440-20140107-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00024" num="00024">
<img id="EMI-D00024" he="221.74mm" wi="193.80mm" orientation="landscape" file="US08626440-20140107-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00025" num="00025">
<img id="EMI-D00025" he="204.89mm" wi="162.22mm" file="US08626440-20140107-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00026" num="00026">
<img id="EMI-D00026" he="199.22mm" wi="185.76mm" file="US08626440-20140107-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00027" num="00027">
<img id="EMI-D00027" he="201.00mm" wi="167.30mm" orientation="landscape" file="US08626440-20140107-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00028" num="00028">
<img id="EMI-D00028" he="229.53mm" wi="136.48mm" orientation="landscape" file="US08626440-20140107-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00029" num="00029">
<img id="EMI-D00029" he="209.80mm" wi="193.72mm" file="US08626440-20140107-D00029.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00030" num="00030">
<img id="EMI-D00030" he="222.42mm" wi="162.98mm" file="US08626440-20140107-D00030.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00031" num="00031">
<img id="EMI-D00031" he="234.61mm" wi="173.65mm" file="US08626440-20140107-D00031.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00032" num="00032">
<img id="EMI-D00032" he="215.73mm" wi="168.57mm" file="US08626440-20140107-D00032.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00033" num="00033">
<img id="EMI-D00033" he="230.55mm" wi="197.02mm" orientation="landscape" file="US08626440-20140107-D00033.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00034" num="00034">
<img id="EMI-D00034" he="222.33mm" wi="184.49mm" orientation="landscape" file="US08626440-20140107-D00034.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00035" num="00035">
<img id="EMI-D00035" he="190.16mm" wi="141.99mm" orientation="landscape" file="US08626440-20140107-D00035.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00036" num="00036">
<img id="EMI-D00036" he="242.32mm" wi="177.88mm" file="US08626440-20140107-D00036.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00037" num="00037">
<img id="EMI-D00037" he="232.75mm" wi="180.76mm" file="US08626440-20140107-D00037.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00038" num="00038">
<img id="EMI-D00038" he="221.74mm" wi="186.35mm" orientation="landscape" file="US08626440-20140107-D00038.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00039" num="00039">
<img id="EMI-D00039" he="212.94mm" wi="181.02mm" orientation="landscape" file="US08626440-20140107-D00039.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00040" num="00040">
<img id="EMI-D00040" he="218.61mm" wi="183.47mm" orientation="landscape" file="US08626440-20140107-D00040.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00041" num="00041">
<img id="EMI-D00041" he="236.30mm" wi="198.20mm" file="US08626440-20140107-D00041.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00042" num="00042">
<img id="EMI-D00042" he="250.78mm" wi="206.50mm" file="US08626440-20140107-D00042.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00043" num="00043">
<img id="EMI-D00043" he="191.18mm" wi="160.44mm" orientation="landscape" file="US08626440-20140107-D00043.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00044" num="00044">
<img id="EMI-D00044" he="240.88mm" wi="136.40mm" orientation="landscape" file="US08626440-20140107-D00044.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATION</heading>
<p id="p-0002" num="0001">This application claims the benefit of U.S. Provisional Patent Application No. 60/672,413 Filed Apr. 18, 2005 and entitled &#x201c;Data-Driven 3-D Traffic Views and Traffic/Weather Views.&#x201d;</p>
<p id="p-0003" num="0002">This application is related to the following U.S. applications:
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0003">1. U.S. application Ser. No. 11/405,653 filed Apr. 17, 2006 entitled &#x201c;DATA-DRIVEN COMBINED TRAFFIC/WEATHER VIEWS.&#x201d;</li>
    <li id="ul0001-0002" num="0004">2. U.S. application Ser. No. 11/405,237 filed Apr. 17, 2006 entitled &#x201c;DATA-DRIVEN TRAFFIC VIEWS WITH THE VIEW BASED ON A USER-SELECTED OBJECT OF INTEREST.&#x201d;</li>
    <li id="ul0001-0003" num="0005">3. U.S. application Ser. No. 11/405,216 filed Apr. 17, 2006 entitled &#x201c;DATA-DRIVEN TRAFFIC VIEWS WITH CONTINUOUS REAL-TIME RENDERING OF TRAFFIC FLOW MAP.&#x201d;</li>
    <li id="ul0001-0004" num="0006">4. U.S. application Ser. No. 11/405,214 filed Apr. 17, 2006 entitled &#x201c;DATA-DRIVEN TRAFFIC VIEWS WITH KEYROUTE STATUS.&#x201d;</li>
</ul>
</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">COPYRIGHT NOTICE AND AUTHORIZATION</heading>
<p id="p-0004" num="0007">Portions of the documentation in this patent document contain material that is subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document or the patent disclosure as it appears in the Patent and Trademark Office file or records, but otherwise reserves all copyright rights whatsoever.</p>
<heading id="h-0003" level="1">BACKGROUND TO THE INVENTION</heading>
<p id="p-0005" num="0008">Commuters and transportation service companies have long desired to receive traffic reports that provide detailed traffic information than the generalized information (e.g., I-95 is backed up, I-495 is jammed) given with most conventional traffic reports broadcast by media outlets today. Traffic service providers such as Traffic.com, Inc. have developed highly sophisticated traffic reporting systems that now deliver such detailed information in a real-time manner. Nonetheless, there is still a need to improve upon such services to provide enhanced traffic reporting capabilities to media outlets for delivery to their customers. The present invention addresses such a need.</p>
<heading id="h-0004" level="1">BRIEF SUMMARY OF THE INVENTION</heading>
<p id="p-0006" num="0009">Different preferred embodiments of the present invention provide at least the following capabilities:</p>
<p id="p-0007" num="0010">1. Integration of weather data and weather conditions into graphical maps of road systems.</p>
<p id="p-0008" num="0011">2. Instant creation of graphical maps that show traffic flow data related to user-selected objects of interest.</p>
<p id="p-0009" num="0012">3. Viewing of traffic data along a particular travel route in a 3D flythrough mode with user control of the flythrough process.</p>
<p id="p-0010" num="0013">4. Creation of an animated traffic flow map that is continuously rendered in real time, wherein the traffic flow map immediately reflects the updated traffic data.</p>
<p id="p-0011" num="0014">5. Processes for defining congestion status along a keyroute (i.e., one or more contiguous road segments).</p>
<p id="p-0012" num="0015">6. Different zoom levels of a graphical map of a road system that presents key information in a manner that is easy to comprehend.</p>
<p id="p-0013" num="0016">7. A single display screen that shows continuously updated, real-time status of traffic flow data on one or more keyroutes, wherein the status is continuously updated on the single display screen.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0014" num="0017">The foregoing summary, as well as the following detailed description of preferred embodiments of the invention, will be better understood when read in conjunction with the appended drawings. For the purpose of illustrating the invention, there is shown in the drawings embodiments which are presently preferred. It should be understood, however, that the invention is not limited to the precise arrangements and instrumentalities shown.</p>
<p id="p-0015" num="0018">The application file contains at least one drawing executed in color. Copies of this patent application with color drawing(s) will be provided by the Office upon request and payment of the necessary fee. The color drawings are <figref idref="DRAWINGS">FIGS. 7-9</figref>, <b>21</b>-<b>22</b>, <b>30</b>-<b>31</b> and <b>36</b>-<b>38</b>.</p>
<p id="p-0016" num="0019"><figref idref="DRAWINGS">FIGS. 1-6</figref> show user interface displays for use with the present invention.</p>
<p id="p-0017" num="0020"><figref idref="DRAWINGS">FIG. 7</figref> shows a 3D fly-through map generated by the present invention.</p>
<p id="p-0018" num="0021"><figref idref="DRAWINGS">FIG. 8</figref> shows a Skyview map generated by the present invention.</p>
<p id="p-0019" num="0022"><figref idref="DRAWINGS">FIG. 9</figref> is a 2D overhead map generated by the present invention.</p>
<p id="p-0020" num="0023"><figref idref="DRAWINGS">FIG. 10</figref> is an Overview graphic generated by the present invention that provides a graphic representation of current traffic conditions on a predefined roadway.</p>
<p id="p-0021" num="0024"><figref idref="DRAWINGS">FIG. 11</figref> is a Travel Time graphic representation of the time needed to traverse a predefined stretch of a roadway.</p>
<p id="p-0022" num="0025"><figref idref="DRAWINGS">FIGS. 12 and 13</figref> show one preferred embodiment of the computer architecture of the present invention.</p>
<p id="p-0023" num="0026"><figref idref="DRAWINGS">FIGS. 14-18</figref> show different types of traffic information that can be downloaded via a TV data feed for use with the present invention.</p>
<p id="p-0024" num="0027"><figref idref="DRAWINGS">FIG. 19</figref> shows various types of objects that are created in the present invention at runtime.</p>
<p id="p-0025" num="0028"><figref idref="DRAWINGS">FIG. 20</figref> shows keyroutes used by the present invention.</p>
<p id="p-0026" num="0029"><figref idref="DRAWINGS">FIG. 21</figref> shows a 2D map generated by the present invention that simultaneously depicts traffic and weather conditions.</p>
<p id="p-0027" num="0030"><figref idref="DRAWINGS">FIG. 22</figref> shows a 3D map generated by the present invention that simultaneously depicts traffic and weather conditions.</p>
<p id="p-0028" num="0031"><figref idref="DRAWINGS">FIG. 23</figref> shows a 2D map creation process used in the present invention.</p>
<p id="p-0029" num="0032"><figref idref="DRAWINGS">FIG. 24</figref> shows a scene graph tree specified in a graphics data file for creating graphics used in the present invention.</p>
<p id="p-0030" num="0033"><figref idref="DRAWINGS">FIG. 25</figref> shows a 3D World Scene Graph for 3D Fly-Through Scene Creation used in the present invention.</p>
<p id="p-0031" num="0034"><figref idref="DRAWINGS">FIG. 26</figref> shows how to create a Travel Time Graph for use in the present invention.</p>
<p id="p-0032" num="0035"><figref idref="DRAWINGS">FIG. 27</figref> shows how weather data is processed in one preferred embodiment of the present invention.</p>
<p id="p-0033" num="0036"><figref idref="DRAWINGS">FIGS. 28-30</figref> show how radial weather data is translated into raster image format data for use in the maps generated in the present invention.</p>
<p id="p-0034" num="0037"><figref idref="DRAWINGS">FIGS. 31-32</figref> show the object-oriented architecture used to create the layers in the maps generated in the present invention.</p>
<p id="p-0035" num="0038"><figref idref="DRAWINGS">FIG. 33</figref> shows how elements of the present invention share components with one another in order to increase memory efficiency.</p>
<p id="p-0036" num="0039"><figref idref="DRAWINGS">FIGS. 34-35</figref> show the camera generation process used in the present invention.</p>
<p id="p-0037" num="0040"><figref idref="DRAWINGS">FIGS. 36-38</figref> show 2D map views that illustrate the effects of various controllers on the objects viewed by the virtual camera in accordance with the present invention.</p>
<p id="p-0038" num="0041"><figref idref="DRAWINGS">FIG. 39</figref> shows how congestion scene objects are created in the present invention.</p>
<p id="p-0039" num="0042"><figref idref="DRAWINGS">FIG. 40</figref> shows how sensor and incident scene objects are created in the present invention.</p>
<p id="p-0040" num="0043"><figref idref="DRAWINGS">FIG. 41</figref> shows a 2D World Scene Graph used in the present invention.</p>
<p id="p-0041" num="0044"><figref idref="DRAWINGS">FIG. 42</figref> shows an Overview Graph used in the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0006" level="1">DETAILED DESCRIPTION OF THE INVENTION</heading>
<p id="p-0042" num="0045">Certain terminology is used herein for convenience only and is not to be taken as a limitation on the present invention. In the drawings, the same reference letters are employed for designating the same elements throughout the several figures.</p>
<p id="p-0043" num="0046">The NeXGen&#x2122; software application (NeXGen) described herein utilizes real time traffic data to create and display data-driven maps and informational graphics of traffic conditions on a road system for display on a video device. With the NeXGen software application, traffic maps or informational graphics do not need to be pre-rendered into movies, thus providing a dynamic view of traffic data on a road system. Specifically, 2D or 3D traffic maps or informational graphics will change as traffic data changes in real-time. Also, with the NeXGen software, the show content is dynamically created to best illustrate the traffic data that the user selects. For example, a 2D map is created centered on an accident location with an icon illustrating the position of the accident and the related congestion. Furthermore, the system can also show the interaction between weather and traffic. This is done by showing weather conditions and traffic conditions on the same maps to show where bad weather is causing traffic problems.</p>
<p id="h-0007" num="0000">I. Definitions</p>
<p id="p-0044" num="0047">The following definitions are provided to promote understanding of the invention.</p>
<p id="p-0045" num="0048">VGSTN: The virtual geo-spatial traffic network is a server side application responsible for collecting and disseminating traffic information from a variety of sources. The VGSTN provides all real time traffic data to the NeXGen application. One preferred embodiment of the VGSTN is provided in U.S. Patent Application Publication No. 2004/0143385 (Smyth et al.), which is incorporated by reference herein. This patent publication also provides a description of the VGSTN and the Traffic Information Management System (&#x201c;TIMS&#x201d;) used with the VGSTN, both of which are components of the present invention.</p>
<p id="p-0046" num="0049">Traffic Data: Traffic related information that the VGSTN generates, stores and reports to the end user or application through a variety of means. Traffic data may include travel time, delay time, speed, and congestion data. Traffic data may be the same as the traffic information once inside the VGSTN.</p>
<p id="p-0047" num="0050">Road System: The actual, physical network of roads.</p>
<p id="p-0048" num="0051">Traffic Event: An occurrence on the road system which may have an impact on the flow of traffic. Traffic events include incidents, weather, construction and mass transit.</p>
<p id="p-0049" num="0052">Incident: A traffic event which is generally caused by an event, planned or unplanned, which directly or indirectly obstructs the flow of traffic on the road system or is otherwise noteworthy in reference to traffic. Incidents are generally locatable at a specific point or across a span of points. Some examples of incidents include: accidents, congestion, construction, disabled vehicles, and vehicle fires.</p>
<p id="p-0050" num="0053">Sensor Data: The data collected from roadway sensors. These sensors can be point detector sensors, toll-tag readers, etc, including probe data, where probe data is point data collected from a moving vehicle. Sensor Data indicates some combination of a vehicles' speeds, volume, occupancy (% of time a vehicle is located over a point), classification volumes (i.e., truck count, etc), or calculated derivative data (e.g., speed relative to normal).</p>
<p id="p-0051" num="0054">Traffic Information: Information about traffic events which is input to the Traffic Incident Management System (TIMS) part of the VGSTN by the traffic operator. Traffic information includes details of incidents, congestion, weather and other traffic events. Traffic information may be entered according to traffic parameters.</p>
<p id="p-0052" num="0055">Traffic Parameter: A specific detail about a traffic event, including location, police presence, injuries, damage, occurrence time, cleared time, etc.</p>
<p id="p-0053" num="0056">Traffic Flow Data: Digital data collected from independent road sensors or from human observations describing the interruption of the traffic movement. This may include some combination of Sensor Data or congestion data and can also include calculated derivative data (for example, travel time/speed relative to historical data).</p>
<p id="p-0054" num="0057">Traffic Operator: A person who gathers and enters traffic information. The traffic information may be collected through any number of traditional methods, including conversing with surveillance aircraft or vehicles and monitoring emergency scanner frequencies.</p>
<p id="p-0055" num="0058">Graphical Map: A graphical representation of the road system.</p>
<p id="h-0008" num="0000">II. Overview of Present Invention</p>
<p id="p-0056" num="0059">In one preferred embodiment of the present invention, a 2D or 3D graphical map of a road system simultaneously displays traffic data representing traffic conditions on the road system, and weather data. The traffic data may be traffic flow data associated with respective road segments, and the weather data may be weather conditions.</p>
<p id="p-0057" num="0060">In another preferred embodiment of the present invention, traffic flow data representing traffic conditions on a road system is displayed on a graphical map. The road system encompasses a predefined geographical region. A user selects an object of interest within the geographical region. The object of interest has a corresponding geographical location. The graphical map is created in a manner such that it includes, and optionally, is centered around, the geographical location of the user-selected object of interest. The graphical map may be a 3D graphical map which may be rotated around, or zoomed toward or away from, the user-selected object of interest. The view angle with respect to the ground plane may also be adjusted while maintaining focus on the object of interest. A user may select a start geographical location and an end geographical location, and a 3D animated flythrough graphical map may be created and displayed beginning at the start geographical location and navigating toward the end geographical location, wherein the 3D traffic flow data is continuously displayed during the flythrough. The start and end locations may represent a keyroute. The flythrough may be stopped at a point between the start and end geographical location while continuously displaying the 3D traffic flow data, even though the flythrough has stopped.</p>
<p id="p-0058" num="0061">In another preferred embodiment of the present invention, traffic flow data is displayed on a graphical map of a road system. The graphical map includes one or more segments, and the traffic flow data represents traffic conditions on a road system. A status of each segment on the graphical map is determined, the status corresponding to the traffic flow data associated with that segment. An animated traffic flow map of the road system is created by combining the graphical map and the status of each segment. The animated traffic flow map is created by being continuously rendered in real time. The traffic flow data is updated in real-time such that the traffic flow map immediately reflects the updated traffic data.</p>
<p id="p-0059" num="0062">In another preferred embodiment of the present invention, traffic flow data representing traffic conditions on a road system is displayed on a graphical map, wherein the graphical map includes one or more keyroutes, each keyroute being defined by one or more contiguous road segments. Congestion status of the one or more keyroutes is determined. The congestion status corresponds to traffic flow data associated with the one or more segments of each respective keyroute. The congestion status is defined by partitioning the keyroute into one or more congestion segments. Each congestion segment is defined in terms of a percentage range of the keyroute from the start of the keyroute. Each congestion segment thus has a congestion status. The traffic flow data on the graphical map of the road system displays the congestion status of the one or more keyroutes. The graphical map of the road system may be provided in either a 2D or 3D view, and each keyroute may be animated to reflect its respective status by simulating different vehicle speeds that are representative of actual vehicle speeds.</p>
<p id="p-0060" num="0063">In another preferred embodiment of the present invention, traffic flow data is displayed on a graphical map of a road system. The graphical map includes one or more segments, and the traffic flow data represents traffic conditions on a road system. A plurality of identification objects to be shown at fixed positions in the graphical map are defined, and a plurality of different views of the graphical map are defined. Each identification object is coded to be either displayed or not displayed for each of the different views. A status of each segment on the graphical map is determined, and the status corresponding to the traffic flow data is associated with that segment. A view of the graphical map is selected. The selected view determines which identification objects will appear in the graphical map. An animated traffic flow map of the road system is then created by combining the graphical map having the corresponding identification objects, and the status of each segment. The identification objects may include road shields, geographic labels, and secondary roads. At least some of the different views are predefined zoom levels.</p>
<p id="p-0061" num="0064">In another preferred embodiment of the present invention, traffic flow data representing real-time traffic conditions of one or more keyroutes on a road system is displayed. Each keyroute is defined by one or more contiguous road segments. First, one or more keyroutes is selected. Second, the real-time status of each keyroute is determined. The real-time status corresponds to the traffic flow data associated with that keyroute. Third, a single display screen displays keyroute identifying information for each selected keyroute, and its real-time status, wherein the status is continuously updated on the single display screen. The traffic flow data may represent average vehicle speeds, average travel time, or average delay time along the keyroute. The keyroute identifying information may include a roadway's name and/or route shield. The size of the displayed information on the single display screen may be automatically adjusted for each keyroute based on the number of selected keyroutes, such that a greater number of selected keyroutes causes the size of the displayed information for each keyroute to be decreased.</p>
<p id="h-0009" num="0000">III. Detailed Disclosure</p>
<p id="h-0010" num="0000">1. Video Output</p>
<p id="p-0062" num="0065">The NeXGen system renders each frame of the animated video output in real time. The term &#x201c;render&#x201d; is used in the 3D computer graphics sense of the term where it is defined as follows: Rendering is the process of generating an image from a description of three dimensional objects, by means of a software program. (See, Wikipedia, The Free Encyclopedia; http://en.wikipedia.org/wiki/Computer_rendering.) In a more technical fashion, &#x201c;render&#x201d; is described as, &#x201c;The process of converting the polygonal or data specification of an image to the image itself, including color and opacity information.&#x201d; (See, HyperVis (A project of the Association for Computing Machinery SIGGRAPH Education Committee, the National Science Foundation), and the Hypermedia and Visualization Laboratory, Georgia State University); http://www.siggraph.org/education/materials/HyperVis/vis_gloss.htm) These frames are produced and played in an NTSC video format at a sufficient rate such that the objects in the frames appear to move, thereby creating animation. This is substantially different from previous systems. In many TV graphics animation systems (including traffic and weather systems), the frames were rendered into a movie and then the movie or movies were played in sequence to produce a traffic report. This two-step process had obvious disadvantages. The rendering process could take about a minute for a simple looping 2D map to 15 minutes for a complicated 3D world fly-through. Therefore, when the movies are played in the second step, the data, which may have been current when the movie was made, was now somewhat stale. One example of a system that renders in this manner is described in U.S. Patent Publication No. 2004/0046759 (Soulchin et al.), which is incorporated by reference herein.</p>
<p id="p-0063" num="0066">There is only one step in NeXGen system. The image is rendered from the data and displayed, and then the next image is rendered and played, etc. There is no long waiting time while the frames are grouped into a movie file and then played. Therefore, in NeXGen, when new data arrives, it is immediately incorporated into the images and is displayed. For example, <figref idref="DRAWINGS">FIG. 9</figref> displays a sensor speed <b>0901</b>. When new data arrives that changes the value from 49 mph to a different value while the map in <figref idref="DRAWINGS">FIG. 9</figref> is being displayed, the numerals that are displayed will visually change immediately. This real-time data display update is a key feature of NeXGen and is available in all of the VGSTN data driven elements of the various maps and graphics.</p>
<p id="h-0011" num="0000">2. Video Output Types</p>
<p id="p-0064" num="0067">The NeXGen software application according to the present invention produces video output in a variety of formats. Among these formats is a 2 dimensional overhead map as seen in <figref idref="DRAWINGS">FIG. 9</figref>. The NeXGen user interface provides real time traffic data to the user which can be in the form of Traffic Incidents or Sensor Data. This data can then be selected by the user to create a 2D map. A user can select a piece of this data, select &#x201c;create as 2D map&#x201d; and the application will build a map around it. <figref idref="DRAWINGS">FIG. 9</figref> shows a 2D map created with sensor data from a section of Interstate 76. The NeXGen application generates this map with the sensor data located at the center point <b>0901</b>. A user may choose to include other incident or sensor data in this map, as seen in <b>0902</b>, by selecting it from the Edit Map dialog box. The map can also be adjusted by panning it in any direction or by zooming it in or out.</p>
<p id="p-0065" num="0068">Traffic flow data controls the graphical representation of vehicles on the map. Vehicle color (e.g., red, yellow, and green), animation speed and proximity to one another are dictated by this data <b>0906</b>. For example, speeds of 0 to 30 miles an hour cause a vehicle to be displayed as red; speeds of 30 to 40 miles an hour cause a vehicle to be displayed as yellow; and speeds of 40 to 60+ miles an hour cause a vehicle to be displayed as green. The specific speed ranges can be controlled by the user through a configuration file. Also, red vehicles move slowly, yellow vehicles move at moderate speed and green vehicles move quickly. Finally, slow moving, red vehicles appear densely packed together; moderate speed, yellow vehicles are less densely situated; and fast moving, green vehicles are sparsely positioned.</p>
<p id="p-0066" num="0069">Incident data, if selected, is displayed on a 2D map as icons denoting traffic events. These include but are not limited to vehicle accidents, vehicle fires, disabled vehicles, construction events and sporting events. An incident <b>0902</b> will be displayed as an icon depicting the specific traffic event it represents. Sensor Data <b>0901</b> is displayed on a map in real time, updating the numeric indicator of average vehicle speed, in miles per hour, on a specific roadway. Other forms of Sensor Data could also be displayed such as: volume, occupancy, truck volume or calculated derivative data (e.g., speed relative to normal). As mentioned above, incident and sensor data can be used as the focal or center point from which a map is created and may also be used to provide more detail to the map. Banners <b>0903</b> can be displayed on a map to show a product name or other textual/graphic information desired by a customer. Road shields <b>0904</b> and city tags <b>0905</b> are displayed on the map to provide easily identifiable markers for viewers.</p>
<p id="p-0067" num="0070">A Skyview map is another format of the NeXGen application's video output. See <figref idref="DRAWINGS">FIG. 8</figref>. A Skyview map is a 3D representation of a traffic map. Similar in structure to the aforementioned 2D map, the Skyview map adds model landmarks and terrain. A movable camera also allows a user, at map creation, to rotate and tilt the view of the map for a desired presentation. (In <figref idref="DRAWINGS">FIG. 8</figref>, the view is not parallel to the I-95 road, but rather is perpendicular to it.) These adjustments can also be made while On-Air. As with the 2D map, a Skyview map is created by selecting a piece of incident or sensor data from the NeXGen user interface <b>0806</b>. This piece of data will become the center point around which the Skyview map is created. Additional Incident and Sensor Data can then be added by selecting it from a dialog box at map creation to provide more detail to the map <b>0807</b>. Banners <b>0803</b> can be displayed on a map to show a product name or other textual/graphic information desired by a customer. Road shields/signs and city tags <b>0804</b> are displayed on the map to provide easily identifiable markers for viewers. Traffic Flow, as described above, is also displayed in a Skyview map. However, vehicles representing flow are displayed in 3D. Vehicles are modeled to resemble cars, trucks, buses, etc. <b>0805</b>.</p>
<p id="p-0068" num="0071">A 3D fly-through map is a dynamic presentation of a 3D world detailing traffic conditions along a predefined roadway or series of roadways. Fly-through routes are generally created for traffic-notable areas and include easily recognizable landmarks (see <figref idref="DRAWINGS">FIG. 7</figref>). Incident <b>0702</b> and sensor data <b>0701</b> can be added by selecting it from a dialog box at map creation to provide more detail to the map. Banners <b>0703</b> can be displayed on a map to show a product name or other textual/graphic information desired by a customer. Road shields/signs and city tags <b>0704</b> are displayed on the map to provide easily identifiable markers for viewers. Traffic flow is displayed in a fly-through in the same manner as the aforementioned Skyview map <b>0705</b>. The view moves along this route showing the traffic data and the 3D world objects. Thus, the 3D flythrough is different from the Skyview in that in a 3D flythrough, the camera moves, but the view is nominally in the direction of the traffic flow being illustrated.</p>
<p id="p-0069" num="0072">A Travel Time graphic (<figref idref="DRAWINGS">FIG. 11</figref>) is a graphic representation of the time needed to traverse a predefined stretch of a roadway. The graphic displays a roadway's name/route shield <b>1101</b> and the stretch covered <b>1102</b>. Various animation effects can be added to attract a viewer's attention. For example, a slide out animation reveals the numerals for the time needed for travel between the stretch's two points <b>1103</b>. Average speed and delay time information can also be displayed for the route. Adding display of other VGSTN data is within the scope of this invention. If enough other data is added, it also could be separated into a new type of 2D graphic. There also can be combination of products like a 2D map overlaid with this type of data. A banner <b>1104</b> can be displayed to show a product name or other textual/graphic information desired by a customer.</p>
<p id="p-0070" num="0073">The last NeXGen graphic is the Overview (<figref idref="DRAWINGS">FIG. 10</figref>). It is a graphic representation of current traffic conditions on a predefined roadway. The graphic displays a roadway's shield <b>1001</b> and the traffic conditions existing in its two directions of travel <b>1002</b>. A banner <b>1003</b> can be displayed to show a product name or other textual/graphic information desired by a customer.</p>
<p id="p-0071" num="0074">The system can also optionally display weather conditions on the various map formats to show where weather conditions are causing undesirable traffic conditions (see <figref idref="DRAWINGS">FIG. 21</figref>). The system displays the weather data as a visual layer over the map background effect under the normal map annotations showing where the weather conditions are occurring <b>2101</b>. Therefore, all of the traffic information can still be seen in the product, but the view is augmented by the addition of the weather information. The depicted embodiment shows precipitation information, but other information (snow coverage, temperature, etc.) may be shown within the scope of this invention.</p>
<p id="p-0072" num="0075">The system shows the precipitation by using a radar image that shows locations as colored according to the precipitation intensity. For example, locations with no precipitation have no colors added. Locations with low intensity have blue or green colors <b>2102</b>. Locations with higher intensity have yellow or orange colors <b>2103</b>. Locations with the highest intensity have red and violet colors. For the 3D products (3D flythrough and SkyView), the same graphical data underlay is used over the terrain and under the roads, signs, landmark models, etc. (see <figref idref="DRAWINGS">FIG. 22</figref>). However in 3D, the actual precipitation is also shown. For example if it is raining, actual rain drops <b>2201</b> are seen falling in the 3D world at the precipitation locations. The density of the snowflakes is varied based on the precipitation intensity information.</p>
<p id="h-0012" num="0000">3. User Interface</p>
<p id="p-0073" num="0076">NeXGen allows a user to view traffic information and create maps or graphics through the user interface shown in <figref idref="DRAWINGS">FIG. 1</figref>. Referring again to <figref idref="DRAWINGS">FIG. 1</figref>, the Traffic Monitor categorizes traffic information according to Incidents <b>0101</b>, Sensors <b>0102</b>, and Key Routes <b>0103</b>. This real time information is delivered to the NeXGen application from the VGSTN. Incident, Sensor and KeyRoute traffic data can be refreshed automatically at intervals ranging from 30 seconds to 5 minutes or invoked manually by the user.</p>
<p id="p-0074" num="0077">In <figref idref="DRAWINGS">FIG. 1</figref>, the Incidents tab is selected and the traffic incidents are displayed. A user can select an incident <b>0105</b> and create a 3D, 2D or skyView map by right clicking on an item or highlighting it and clicking on the desired create map icon <b>0104</b>. <figref idref="DRAWINGS">FIG. 4</figref> shows the edit map dialog box which will allow the user to adjust the map including the area shown, what data to include, and the type of map. After making selections and clicking the OK button, the map is created and added to the Rundown section <b>0107</b>.</p>
<p id="p-0075" num="0078">In <figref idref="DRAWINGS">FIG. 2</figref>, the Sensors tab <b>0201</b> is selected and sensor traffic data is displayed. Here, a user can select a sensor item <b>0202</b> from the list and build a 3D, 2D or Skyview map by right clicking on an item or highlighting it and clicking on the desired create map icon <b>0203</b>. At this point the edit map dialog box is displayed for adjustments in the same manner as described previously. After making selections and clicking the OK button, the map is created and added to the Rundown section <b>0107</b>.</p>
<p id="p-0076" num="0079">In <figref idref="DRAWINGS">FIG. 3</figref>, the Key Routes tab is selected <b>0301</b> and Key Route data pertaining to speed and travel time is displayed. Key Routes, which are predefined sections of a road system, can be selected by a user to create maps of a portion of the map or informational graphic showing the route's speed or travel time. A user can select a Key Route <b>0302</b> item from the list and build a map or informational graphic. The user can create a map by right clicking on an item or highlighting it and clicking on the desired create map icon <b>0303</b>. As described previously, the edit map dialog window is shown to allow the user to modify the map. When created from a keyroute, the 2D or skyview maps are created centered on the midpoint of the keyroute. This center point can then be adjusted in the edit map window. The 3D flythrough is create to fly over this keyroute. In the edit map window, the start and end points of the fly-through can be adjusted as desired <b>0402</b>.</p>
<p id="p-0077" num="0080">The user can also choose to create an informational graphic to display numeric data about a keyroute. Currently the system will allow the user to display a travel time and/or average speed for the keyroute. The user clicks on the travel time icon <b>0304</b> to create the travel time graphic. The Edit Travel Time window is then displayed (<figref idref="DRAWINGS">FIG. 5</figref>). The user can choose which keyroute should be displayed in each element of the graphic. The user selects the element in the element list <b>0502</b> and then chooses the desired keyroute from the element menu <b>0501</b>. The data for the keyroute is automatically displayed in the graphic <b>0503</b>. The data is also displayed in the Conditions area <b>0504</b> of the user interface as well. In an error situation (network problems, etc.), the user can override the data and the new data will be displayed. The system allows different graphical layouts of the data to be utilized by choosing from the menus in the Graphic Type section <b>0505</b>.</p>
<p id="p-0078" num="0081">The user can also choose to display an overview graphic showing general conditions on a roadway. The user clicks on the overview icon <b>0305</b> to create the overview graphic. The Edit Overview window is displayed (<figref idref="DRAWINGS">FIG. 6</figref>). In a manner similar to the overivew graphic, the user can choose which roads to display for each element of the graphic. The user selects the element in the list <b>0603</b> and chooses the desired road from the element menu <b>0601</b> and the conditions from the Conditions menu <b>0603</b>. The user can also choose different graphic layouts in the graphic type menu <b>0604</b>.</p>
<p id="p-0079" num="0082">Once a user completes map and graphic creation, the elements can be reordered as necessary using the rundown management icons <b>0108</b>. The arrows are used to move elements up or down in the play order and the &#x201c;X&#x201d; is used to delete unwanted elements. The elements can also be removed from the show but not fully deleted by clicking on the &#x201c;include&#x201d; checkbox <b>0109</b>. When the rundown order is finalized, the maps and or graphics can be launched by clicking the ON AIR button <b>0106</b>. When ON AIR is activated, NeXGen sends the graphic images to the user's computer monitor <b>1201</b> and a SDI <b>601</b> digital video signal <b>1202</b> to the TV station switcher <b>1203</b>. See <figref idref="DRAWINGS">FIG. 12</figref>. At this point, animated graphic images like that pictured in <figref idref="DRAWINGS">FIGS. 7-11</figref> will be displayed on said devices. A user or on-air talent can then play, pause or rewind the created map and or graphic content by using an input device, such as a keyboard <b>1204</b> or handheld clicker <b>1205</b>.</p>
<p id="h-0013" num="0000">4. Physical System Description</p>
<p id="p-0080" num="0083">One preferred embodiment of the NeXGen software application includes a computer architecture as seen in <figref idref="DRAWINGS">FIG. 12</figref>. The NeXGen application will typically run on a client workstation. One preferred embodiment is a Windows XP based PC workstation with dual 3.0 GHz Xeon processors and having 2 Gbytes of RAM memory. This PC is connected to the Internet via a network card to allow download of the traffic data. The client workstation includes a graphics video card <b>1206</b>, (e.g., a Nvidia Quadro FX 4000 SDI) capable of sending both an SDI digital video signal to a TV Station Switcher and a video signal to the workstation's attached monitor. The video card is also capable of accepting a TV Station genlock reference synchronization signal <b>1207</b>. Finally the client workstation uses a keyboard emulator, or switch interface, making possible the use of a handheld clicker <b>1208</b>. The actual handheld clicker <b>1205</b> will vary depending on the TV station studio environment. Any device (wireless or wired) that is capable of a simple momentary contact closure is sufficient. Other hardware equipment and or configurations may be used without departing from the spirit and scope of the present invention.</p>
<p id="h-0014" num="0000">5. Software Components Description (<figref idref="DRAWINGS">FIG. 13</figref>)</p>
<p id="p-0081" num="0084">One preferred embodiment of the NeXGen software application comprises a software architecture as seen in <figref idref="DRAWINGS">FIG. 13</figref>. The NeXGen application utilizes the Microsoft .NET framework and is run inside the .NET runtime environment <b>1301</b>. Several of the utilities of the .NET framework are also used. .NET controls are used for the user interface <b>1302</b> and .NET software utilities are used for loading data via the Internet <b>1303</b>. Gamebryo&#xae; graphic engine software by Numerical Design Limited (NDL), Chapel Hill, N.C., is used to handle the real-time geometry-processing requirements to produce the graphic show output of the NeXGen application <b>1304</b>. Other graphic software may be used without departing from the spirit and scope of the present invention.</p>
<p id="h-0015" num="0000">6. Per Station Development</p>
<p id="p-0082" num="0085">The NeXGen software application according to the present invention utilizes data to create different maps and informational graphics. While traffic data can be displayed in real time with NeXGen, a map's roads, terrain, waterways, landmarks, road shields, city tags and banners, among other things, must be developed prior to the application being used. This underlying map information is achieved through graphic artists creating a 2D world map for a customer's needs. A 3D world map is created by making adjustments to the corresponding 2D world map. 2D Informational graphics are also designed and created by graphic artists prior to NeXGen's use. In one preferred embodiment of the present invention all of this data is created with 3ds max (commercially available from Autodesk's Media and Entertainment division, formerly known as Discreet) to create a &#x2018;scene file&#x2019; for each station. The Gamebryo exporter is used to take this content from the 3ds max development environment and create a Gamebryo runtime graphics data file (referred to as a &#x201c;.nif&#x201d; file). This .nif file contains all the maps, 2D graphics, and 3D worlds that must be accessed by the running application.</p>
<p id="p-0083" num="0086">All of the objects created in 3ds max must be organized for maximum efficiency into a tree structure. This tree structure allows objects to be designated as parents, children, and sibling nodes relative to other nodes. This tree structure (known as the &#x201c;scene graph.&#x201d;) is not visible in the end product and is purely for runtime object organization and control. An example listing is shown in <figref idref="DRAWINGS">FIG. 24</figref>. This example shows the scene graph structure as it is specified in the graphics data file (i.e., a .nif file). This example shows some of the nodes for a 3D world .nif file <b>2401</b>. The 3D landmark models in the 3D world are mostly not animated. These non-animated models are grouped together under a parent node (called the non-animated node) <b>2402</b>. The code then does not have to update the animation characteristics of these models, thereby greatly improving performance.</p>
<p id="p-0084" num="0087">At runtime, the NeXGen system also creates dynamic objects that are added to the scene graph. The scene graph is utilized for functional and optimization purposes. For example, the characteristics of a node higher up in the tree are used to control the behavior of a node lower in the tree <b>2403</b>. If the node higher up in the tree is moved, all of the nodes below it are also moved. Also the nodes and leaves of the tree are grouped according to their function (e.g., all the car &#x201c;dummies&#x201d; are under the &#x201c;CarDummy&#x201d; node <b>2404</b>).</p>
<p id="p-0085" num="0088">All of the types of map graphics display traffic flow information by changing the characteristics (color, speed, etc.) of the vehicles flowing along the road <b>0705</b>. This information is displayed for the major highways in a metropolitan area. However, it is only displayed on the portions of these routes where this flow information is known. The source of this flow information includes traffic sensors, cameras, police reports, aircraft, mobile ground units, etc. These portions of highways are determined and entered in the VGSTN as keyroutes <b>2001</b>. A keyroute is defined by two endpoints on a roadway (e.g., exit A <b>2002</b> to exit F <b>2003</b> on I-76) and is made up of all the segments <b>2004</b> connecting all the points <b>2005</b> between these end points (see <figref idref="DRAWINGS">FIG. 20</figref>). The VGSTN will report congestion information to NeXGen based on its location relative to this keyroute. For example if the keyroute were 10 miles long and a one mile segment was congested starting 2 miles from the beginning, the VGSTN would report that the congestion was from Exit B <b>2006</b> (20% of the keyroute length) to Exit C <b>2007</b> (30% of the keyroute length) and the rest of the keyroute was clear.</p>
<p id="p-0086" num="0089">To display this information, a visual object matching each keyroute must be created in the map (2D and 3D). From within 3ds max, a path object for each keyroute will be created on the maps <b>2008</b>. The placement of the path's points will match the geographic characteristics of the points of the keyroute. Dummy objects are added to the path as placeholders to be overlaid at runtime with vehicle models <b>2403</b>. These vehicle models will be altered to illustrate the flow data. Continuing the earlier example, the application will make the green vehicles fast and widely spaced for the first 10% of the path and the last 80% <b>2009</b>. It will, however, make the red vehicles slow-moving and densely spaced between 10% and 20% of the path <b>2010</b>.</p>
<p id="h-0016" num="0000">7. 2D Map Creation (<figref idref="DRAWINGS">FIG. 23</figref>)</p>
<p id="p-0087" num="0090">In one preferred embodiment of the present invention the process of creating a 2D world map starts with the loading of geographic data (nominally supplied by NAVTEQ Corporation) <b>2301</b> into the MapInfo software program <b>2302</b>. One preferred MapInfo program is the MapInfo Pro version 7.0, commercially available from MapInfo Corporation, Troy, N.Y. This data is loaded for an area selected by the customer. Usually, this is an area where traffic flow data is readily available from roadside sensors or other means. MapInfo organizes data into layers which include major highways, secondary highways, waterways, oceans, and counties. From within the MapInfo program, the layer control is used to make only one of these layers selectable. After a layer is selected the user exports it as a .dxf (Autocad) file. This process is repeated to create .dxf files for each layer. Each .dxf file is a 2D representation of the map using a square-projection where x=longitude and y=latitude for each point. The map data is represented as a list of points and polylines. Polylines are a set of points that make up a segmented line. These .dxf files are then processed with a utility program <b>2303</b> that receives a .dxf file as input and extracts out these points and polylines. The clustering step examines all groups of two points, and combines their respective polylines together if the distance is greater than the threshold distance supplied by the user. Next, it examines all groups of 3 adjacent points in a polyline, and computes an angle. If the angle is below the threshold supplied by the user, the middle point is removed. Finally all coordinates are transformed using a rectangular-projection where x=longitude x cos(latitude) and y=latitude and a user-supplied scale factor is often used to eliminate the majority of floating-point errors that occur when using very small numbers of high precision. As seen in the above-noted equation, the utility program <b>2303</b> adjusts a map's longitude values to compensate for the curvature of the earth. Without this adjustment, roads would not be displayed accurately. When finished adjusting map data, files are again saved in the .dxf format.</p>
<p id="p-0088" num="0091">The &#x2018;curve-cleaned&#x2019; .dxf files for major and secondary highways are loaded into the software program 3ds max <b>2304</b>. The line tool is used to trace the roads and is converted to an editable polygon. Textures are added to the roads and UVW unwrap is performed on the roads with the textures. When all roads are mapped, they are collapsed and the file is saved. The &#x2018;curve-cleaned&#x2019; .dxf files for waterways are opened in Adobe Illustrator&#xae;. The object's path is simplified and all small lakes and single line rivers are deleted. The file is then saved as an Illustrator file <b>2305</b>. This file is then exported into 3ds max where the waterway graphics are finalized <b>2306</b>. Also, in 3ds max <b>2307</b>, Road shields, city signs <b>4101</b> and secondary roads <b>4103</b> are added and marked with a zoom level variable to control their display <b>4105</b>. This affects their visibility on the map at different zoom settings. Fewer shields are desired to be displayed at a more distant zoom level. If more of the map is shown, it becomes cluttered with shields if this is not done.</p>
<p id="p-0089" num="0092">After the main static elements of the world are established, the artists must also place the dynamic elements in the scene <b>3502</b>. These dynamic elements are placed in the scene file outside of the viewable area. At runtime, the system clones these dynamic objects and places them at the appropriate places in the animated environment <b>3504</b>. For example, the car path scene object copies the &#x201c;car&#x201d; objects from outside the viewable area <b>3506</b> and places them on the road with the appropriate color and speed. The icons that mark traffic incidents <b>2407</b> are cloned and placed on the 2D map in the same way. Lastly, the &#x201c;bubble&#x201d; containing the sensor speeds <b>2408</b> is cloned and placed at the sensor location.</p>
<p id="h-0017" num="0000">8. 3D Fly-Through Scene Creation (<figref idref="DRAWINGS">FIG. 25</figref>)</p>
<p id="p-0090" num="0093">Development of a 3D fly-through scene begins after creation of a 2D map as illustrated above. To create a third dimension, ground textures are stretched and landmarks <b>2501</b> of notable interest are added to the 2D map geometry to create the 3D world. Landmarks may include buildings, billboards, signage and any other structure located on or near a roadway. A reference image is used to model the structure of a landmark. The resultant model then has a texture/skin applied to it. The finished, textured model should closely resemble the reference image, thus making it recognizable to someone familiar with the landmark. The 3D map retains the underlying 2D map's &#x2018;curve-cleaned&#x2019; latitude and longitude point values. Landmarks are placed on the 3D map using these values. The roadway signs, town name signs, and other 2D labels are replaced by 3D versions. These changes are made in the 3ds max application to produce the completed 3D scene for a metropolitan area.</p>
<p id="p-0091" num="0094">A camera path is then created to follow each keyroute for the 3D map <b>2502</b>. This camera path object is named to reflect the keyroute's name allowing it to be referenced by the application's code. In the same way as the 2D world, the dynamic elements of the 3D world must also be created and placed outside of the viewable 3D world <b>2503</b>. This includes the flow vehicle models, the traffic incident models, and the sensor display &#x201c;bubbles&#x201d;. At runtime, the system also clones these dynamic objects and places them at the appropriate places in the animated environment <b>2504</b>.</p>
<p id="h-0018" num="0000">9. Skyview Map Creation</p>
<p id="p-0092" num="0095">A skyview map uses the same 3 dimensional world as used by the 3D fly-through explained above. No additional per station development is needed because the 3D world is already created.</p>
<p id="h-0019" num="0000">10. 2D Informational Graphics Creation</p>
<p id="p-0093" num="0096">2D informational graphics representing &#x2018;travel times&#x2019; or &#x2018;overviews&#x2019; are designed in 3 ds max as flat worlds. Each world has a texture applied to it based on a customer's needs to serve as a background for the 2D graphic. This texture can be a graphic image of a local landmark, station logo or generic traffic scene. Placeholders are inserted in the flat world for display of items such as road shields, roadway conditions and travel times.</p>
<p id="p-0094" num="0097">a. Overview Graphic (<figref idref="DRAWINGS">FIG. 42</figref>)</p>
<p id="p-0095" num="0098">A 2D overview's layout and design are created, according to station requirements, in a graphics program such as Adobe Photoshop&#xae;. Certain aspects of the design are static (e.g., the background) and others are dynamic and can be switched according the user's choice (e.g., the road shields, the traffic conditions). The static background is saved as an image file. The dynamic content portions are also created as images (e.g., one image for each road route number shield). When the user makes the choices of different roads, these will then be switched by the running system.</p>
<p id="p-0096" num="0099">The background image file <b>4201</b> is then used in 3ds max to create a 2D overview world. In 3ds max, placeholders are added to the 2D overview world to account for a roadway's name/direction of travel <b>4202</b> and traffic condition <b>4203</b>. Most overview graphics have two conditions for each road to reflect each direction of travel. The images for all the roads, directions of travel and conditions are stored outside the visible world in the 3ds max file <b>4204</b>. At runtime, this data is inserted in the placeholders within the 2d overview world according to the user's preference. Also, in the 3ds max file, &#x201c;extra data&#x201d; on each object is used to store a unique string to dynamically build the user interface drop down menu lists for the user to select a roadway, its direction(s) of travel and traffic condition. The extra data functionality is a way to annotate a graphical object with textual data and does not usually affect the visual appearance of the object. For example, in this case, the extra data is used to populate the user interface menu text allowing the user to select the desired roadway image <b>0601</b> or condition image <b>0602</b>.</p>
<p id="p-0097" num="0100">b. Travel Time Graphic (<figref idref="DRAWINGS">FIG. 26</figref>)</p>
<p id="p-0098" num="0101">The process for the travel time graphics is similar to the 2D overview graphic. A 2D travel time's layout and design are also created, according to station requirements, in a graphics program such as Adobe Photoshop&#xae;. The various background <b>2601</b>, roadway description <b>2602</b>, and condition <b>2603</b> images are then saved in image files and included in the scene graph using 3ds max to create a 2D world. In 3ds max, placeholders are added to the 2D world to account for a roadway's name <b>2604</b> and travel time <b>2605</b> or average speed <b>2606</b> or condition <b>2607</b>. The complete set of roadway name images are stored outside the visible world in the 3ds max file. At runtime, the user's selection is copied to the visible portion of the world (where the placeholders are located).</p>
<p id="p-0099" num="0102">The travel time or average speed placeholders have specially named parent nodes such that the application will generate a dynamic texture containing the numerals for speed/travel time for the selected road. The generated texture will be placed on the appropriate placeholder at runtime. All roads for travel time graphics are based on keyroutes stored in the VGSTN. Keyroutes are used in the NeXGen system for both travel time values and for the placement of congestion indication on the car paths (e.g., red and yellow car locations). In the 3ds max file, extra data is used to store a unique string to account for a keyroute's name, default road condition color indicators, and the font to be applied in creation of dynamic numeral textures. In a manner similar to the Overview 2D graphic, the extra data capability is also used to populate the user interface menu text, thereby allowing the user to select the desired roadway image <b>0501</b>.</p>
<p id="h-0020" num="0000">11. NeXGen Software Organization (<figref idref="DRAWINGS">FIG. 13</figref>)</p>
<p id="p-0100" num="0103">In one preferred embodiment of the present invention, the NeXGen software application is comprised of three layers. These layers are the user interface layer <b>1305</b>, the data layer <b>1306</b> and the animation layer <b>1307</b>. The user interface layer displays traffic data updated from the data layer thereby allowing a user to create and manage content <b>1308</b>. With this layer, a user can create 2D and 3D maps or informational graphics and control this content through the use of a rundown. The user interface layer is connected to the user through the pc monitor, keyboard and mouse. The data layer is responsible for continually downloading data from the VGSTN via the NeXGen Data Feed. It is connected to the VGSTN externally through a network access point. The data layer is used by both the user interface layer and the animation layer. Finally, the animation layer is charged with handling the graphical presentation of traffic content created with the user interface layer. It displays the selected world and its associated animations using flow, sensor and incident data. The animation layer contains a framework which serves as a bridge between the user interface layer and itself <b>1309</b>. By storing data about the world and rundown elements created, this framework manages part of the NeXGen application's animation. The animation layer uses the Data Layer by receiving the traffic data updates <b>1310</b> and then altering the data that is visually displayed as the show is playing on air.</p>
<p id="h-0021" num="0000">12. NeXGen System Activity Flow</p>
<p id="p-0101" num="0104">When the NeXGen application is started, the user interface layer checks software entitlements, instructs the animation layer to load .nif files and has the data layer start its auto-downloader. At application launch, the user interface layer checks an initialization file for a customer's entitlements. These entitlements govern the functionality of the user's software. This allows certain program features to be excluded from the user's control and use. Also, at application launch, the user interface layer instructs the animation layer to load the relevant .nif files into memory in accordance with the entitlements. The .nif files loaded are created as part of the &#x201c;per station&#x201d; development process and house the graphical world and its attributes. Depending on a customer's entitlements, these files may encompass 2D, 3D, travel time and overview worlds.</p>
<p id="p-0102" num="0105">At runtime, various types of objects are created in the system (see <figref idref="DRAWINGS">FIG. 19</figref>). Subject objects <b>1901</b> are objects that contain traffic data (e.g., sensor speeds, congestion locations, incident locations, etc.). Scene objects <b>1902</b> are a visual representation of some piece of traffic data (e.g., the incident marker for a car accident). Scene objects are associated with a subject that contains the traffic data. As the data changes, the scene object is responsible for showing the changed data. The On Air Element object <b>1903</b> is associated with an item in the rundown (e.g., a 2D map, 3D flythrough, etc.). Via the user's choices, scene objects are added to On Air Element objects (e.g., choosing to display an incident on a map).</p>
<p id="p-0103" num="0106">The runtime worlds are initialized based on the data in the .nif files. For the 2D graphic worlds (non-maps like travel times and overviews), the initialization involves accessing the data in the .nif file and setting up data in the framework. This data is provided to the user interface so it can build the menus for the user to choose the desired road, bridge, etc. <b>0601</b> to be displayed on the graphic. When the user makes the selection, the subject, scene object, and On Air element are created by the user interface.</p>
<p id="p-0104" num="0107">The system initializes the 2D and 3D worlds by reading in the graphic data from the .nif file. One of the main activities that must be done is to create the flow indication scene objects on the paths throughout the world. This is done when the first element associated with the world is created. The keyroute flow data feed subjects are accessed to color the vehicles and place the vehicles appropriately. In the .nif files, there are invisible dummy objects placed on each path by the artists when the path object was created during the &#x201c;per station&#x201d; development <b>2505</b> <b>4109</b> (<figref idref="DRAWINGS">FIG. 41</figref>). The world initialization effort determines the number of vehicles that will be needed for each path based on the current data. The animation layer clones the number of path dummies needed for each path. There are various vehicles used in the 3D visualization (e.g., cars, pickup truck, bus, etc.). These are selected via a weighted average. Referring again to <figref idref="DRAWINGS">FIG. 41</figref>, the selected vehicle <b>4106</b> is cloned and added to the world in a fashion such that the path dummy controls its location to follow the path. This is accomplished by placing the vehicle object below a cloned dummy <b>4109</b> in the scene graph.</p>
<p id="p-0105" num="0108">Finally, at program launch, the user interface layer requests the data layer to initiate its auto downloader process to provide continuous data updates <b>1308</b> from the VGSTN TV Data Feed to populate a portion of the user interface called the traffic monitor <b>0101</b>. From the NeXGen application's traffic monitor, a user may explore traffic data to create a rundown of show items <b>0107</b>. The traffic monitor updates data through the user interface layer, sending one of several possible queries to the data layer. One query can be for all data of a certain type such as incident, sensor, flow, or keyroute. Another, more limited query, can be for data relating to a specific keyroute. Data is received back from the data layer as subject system objects <b>1901</b>. The user interface layer will extract data from these subjects <b>1904</b> for display in the traffic monitor in a grid format that is easy for users to peruse, filter, and sort.</p>
<p id="p-0106" num="0109">The user then selects a single data item <b>0105</b> (incident, sensor value, or keyroute), which they would like to include in the traffic report. The user decides which type of map they would like to use to visualize this piece of data (e.g., 2D Map, 3D Skyview, 3D flythrough, etc.). After the data item is selected and the user indicates the type of map to create <b>0104</b>, the system dynamically creates a map of the specific area showing the data. Internally, the user interface layer provides this user selection information to the Animation Layer <b>1311</b>. The Animation Layer has a framework functionality <b>1309</b> that receives this information and stores it. To store the user's choice, element data objects are created and stored in the framework. The user interface also allows the user to adjust the map item.</p>
<p id="p-0107" num="0110">To facilitate these adjustments, the user interface displays a preview of the graphic in a portion of the user interface <b>0401</b>. The animation layer is responsible for producing the image for this preview. To do this, the Animation Layer creates active objects from the framework's record keeping elements. The On-Air-Element object <b>1903</b> is created and contains the information about the world on which the element is based, the camera information, and the scene objects in the on air element. Scene objects <b>1902</b> are individual objects in a scene that portray some type of traffic data. Examples of scene objects are incidents, sensor data, flow illustration cars (red, yellow, or green colored), etc.</p>
<p id="p-0108" num="0111">As mentioned in the world creation description, some scene objects are available because they are part of the world (e.g., the flow congestion cars). However, others must be explicitly created for a particular rundown item. These would be things that the user has requested to be in the item. For example, if the user requested that an incident icon and two sensor data displays be shown in a 2D map item, an incident icon scene object and two sensor data scene objects would be created based on the data in the associated subject objects. A 2D map on-air element would be created, associated to the 2D world, and associated to the scene objects. The flow indication scene objects would also be available by default as a part of the 2D world.</p>
<p id="p-0109" num="0112">The world object, the on-air-element object, and the scene objects all utilize the per-station developed artwork in the .nif file and utilize Gamebryo utilities to produce the required preview image. As the user makes adjustments, the data in the objects is adjusted to change the resulting preview image. When the user is satisfied with the appearance of the rundown item (e.g., the 2D map), the user indicates this to the system (e.g., clicks the OK button on the map definition window). The system then saves the final data off in the element data objects in the framework.</p>
<p id="p-0110" num="0113">The user can then repeat this rundown element creation for multiple items in the rundown. The result is to create the required numbers of element data objects that are stored in the framework. When the user wants to display the complete show on air, the element data objects of the framework are all converted into on-air-element objects and scene objects (animation layer objects). When the scene objects are created, they are created with the data from the corresponding subject, which has been kept updated by the auto downloader in the data layer. In this manner, the scene objects will display the latest information in the on-air graphics.</p>
<p id="p-0111" num="0114">Creation of the animation layer objects for playing an On-air show is very similar to playing the preview. However, before the graphic visualization is started, the User Interface enables the digital video output (SDI) connection <b>1202</b> and sets the screen resolution to the SDI resolution (720&#xd7;486 pixels) using NVidia video card control APIs. This allows the video to be fed into the TV-station switcher <b>1203</b> for broadcast on the air. Lastly, in the On-air show mode, the video is output to the entire screen instead of to just a small preview window that is part of the user interface. This video output is sent to both the computer monitor and to the digital video output. The screen resolution and video output type can be changed (e.g., to analog NTSC) within the scope of the present invention.</p>
<p id="h-0022" num="0000">13. Weather Related Processing</p>
<p id="p-0112" num="0115">The processing of the weather data is done in a similar manner as the traffic data, but with different specific objects (see <figref idref="DRAWINGS">FIG. 27</figref>). For example, there is a specific Weather Subject object <b>2701</b> used to process the weather data. After the data layer retrieves the weather data, the Weather Subject is responsible for parsing the radar file. For more information on the weather data, see the &#x201c;Data Layer Details&#x201d; section below. This parsing is split up into two parts: (1) extracting the raw data from the file, and (2) translating that into a format that is easier to work with.</p>
<p id="p-0113" num="0116">Referring to <figref idref="DRAWINGS">FIG. 30</figref>, extracting the raw data from the file consists of reading the metadata <b>3001</b> (latitude and longitude of the radar site, the mode of operation (precipitation or clear), the time at which the radar scan took place) and the radial image data <b>3002</b>. Before this data is all stored, it is converted from big to little endian data format. Not all of the radar data needs to be preserved since most radar scans cover a much larger area than the scene. Therefore, it reads in the bounding box of the scene in latitude/longitude coordinates from a site-specific configuration file. Then, using the latitude/longitude of the radar scan, it finds where the bounding box of the scene is in a Cartesian coordinate system with the radar scan at the origin. Only the data in this bounding box is processed further.</p>
<p id="p-0114" num="0117">The second part of setting the data in the weather subject is translating the radial data into raster image format data <b>3003</b>. The data is in a format where radials are specified via a start azimuth <b>2801</b> and an angle delta <b>2802</b>, as shown in <figref idref="DRAWINGS">FIG. 28</figref>. Each one of these radials has reflectivity levels run-length encoded from the center of the image out in a series of bins. In order to convert this into a pixel based data format, the Weather Subject iterates through each pixel in the scene <b>2901</b> and queries the radial image data to decide what its color should be. In order to speed up the &#x201c;pixel query,&#x201d; it uses the Microsoft .NET ArrayList-&#x3e;Sort method to sort the radials based on start azimuths and binary searches them to find out which radial each pixel belongs to (e.g., the highlighted wedge in FIG. <b>29</b>&#x2014;<b>2902</b>). Once it has found which radial it belongs to, it then binary searches the runs of reflectivities to find which range bin it belongs to where it can find its color (in <figref idref="DRAWINGS">FIG. 29</figref> there are 3 range bins <b>2903</b>). The point of the lower left corner of the pixel is used to determine if the pixel is in a particular bin. For example, in <figref idref="DRAWINGS">FIG. 29</figref>, the two crosshatched pixels <b>2904</b> are in the bin range in the given radial <b>2902</b>. Likewise, the horizontally marked pixels <b>2905</b> are in the second bin range and the vertically marked pixels <b>2906</b> are in the third bin range. When this has been completed for each pixel, the Weather Subject is done processing the radar data. Each pixel in the image corresponds to an intensity level on a logarithmic scale. A radar pixel with intensity levels three or below is not considered to be precipitation so they are not included in the data. Also, radar pixels are equivalent to about a square kilometer.</p>
<p id="p-0115" num="0118">Next, the weather subject determines the type of precipitation that is occurring at each pixel in the raster radar image <b>3004</b>. The weather subject uses the METAR data, which the Data Layer has downloaded. (METAR is the acronym for METeorological Aerodrome Report, which provides surface meteorological data for aviation.) The METAR data contains the observed weather conditions at that location (e.g., clear, rain, snow, etc.). Each pixel of the radar image is assigned a weather type based on the METAR data of the closest METAR data location.</p>
<p id="p-0116" num="0119">The Weather Scene Object <b>2702</b> creates the visual representation of the weather data which the Weather Subject Object has collected <b>3005</b>. The Weather Scene Object performs different functions if it is part of 2D or 3D world. If it is part of a 2D world, it first calculates the size of the texture needed by finding the smallest width and height dimension that cover the entire scene and are powers of two. The Weather Scene Object then creates a NiPixelData (Gamebryo defined object) object of this size by mapping intensity levels of the radar raster image to color values (the intensity level to color mapping is provided in a property file). The NiPixelData object is then used to create a NiTextureProperty object (Gamebryo defined object), which is placed on the ground plain of the 2D world under all the roads, route shields, signs, etc <b>2102</b>.</p>
<p id="p-0117" num="0120">If the Weather Scene Object is part of a 3D world, it must place the radar precipitation intensity image on the ground plain like in the 2D <b>2202</b> world and also must create a graphical representation of the weather (e.g., rain drops, snowflakes, etc.) <b>2201</b>. The graphical representation is accomplished by using Gamebryo particle system objects. When the scenes are created as part of the &#x201c;per station development&#x201d;, the particle systems for each weather type (rain, snow, etc.) must be created and placed in the scene out of view (same strategy as vehicles, incident markers, etc.) <b>2506</b>. Furthermore, three different particle systems with different particle density are created for each type to show different intensities of precipitation (low, medium and heavy intensities) <b>2507</b>. Three graphical representations are sufficient because it is difficult to visually distinguish between more granular intensity level representations. Each of these particle systems is created using a PArray in 3ds max and placed under the non-animated node in the scene graph.</p>
<p id="p-0118" num="0121">Some setup work must also be done to place the radar image on the 3D ground plane. At scene creation time, a bitmap of a radar image must be generated for the scene set-up purposes. This bitmap has the same dimensions as the texture dynamically created at runtime using Gamebryo functions. In order to match the texture up to the latitude/longitude of the scene, texture coordinates must be calculated to determine which part of the bitmap needs to be displayed in the scene. These texture coordinates are calculated by finding the bounding box of the scene in radar image space. Radar image space is the coordinate system where the center is the location of the NEXRAD radar and each unit is a square kilometer off of that. The bitmap contains all of the pixels in the radar image that are covered by this bounding box. To calculate the texture, first coordinates find the values for the bounding box within the bitmap, and then divide them by the size of the texture. In 3DS max, the texture is placed onto the ground plane using these coordinates. This default texture is then replaced at runtime by the actual downloaded radar data.</p>
<p id="p-0119" num="0122">Then, at runtime in the 3D world, the Weather Scene Object bins the radar raster image pixels into groups according to light, medium and heavy precipitation intensity and precipitation type (e.g., light intensity rain bin, light intensity snow bin, medium intensity rain, etc.) <b>3006</b>. The positions of these pixels are transformed from their pixel coordinate system to the latitude/longitude coordinate system of the scene. The Weather Scene Object then creates a piece of Gamebryo geometry for each bin from the points in the respective bin. Lastly it causes the predefined particle systems to emit from the vertices of the corresponding geometry (e.g., the light rain particle system emits from the light rain geometry) <b>3007</b>. In order to maintain a consistent birthrate over the entire particle system, the birthrate at each vertex is changed to be some scalar multiplied by the number of vertices in the system. This makes the visual effect independent of the number of vertices in the geometry. The intensity of the precipitation is to be based on the intensity data and not on the number of vertices.</p>
<p id="p-0120" num="0123">The next thing that the Weather Scene Object does is to add the radar texture to the ground plane <b>3005</b>. It uses the multitexture that was created when developing the scene. The scene object grabs this placeholder texture and replaces it with a texture that it creates from the radar data. However, this texture must be translated and flipped because of the way the Gamebryo exporter exports textures.</p>
<p id="h-0023" num="0000">14. Data Layer Details</p>
<p id="p-0121" num="0124">The data layer is responsible for providing updatable, real time data to the NeXGen application. After receiving a data request from the user interface layer, the data layer will query the VGSTN NeXGen Data Feed for the data. The VGSTN has the traffic data stored as part of a road network. This data can be presented to different applications in the appropriate format. This essentially provides a different view of the data for different clients of the VGSTN. For example, a textual format is a common format for a World Wide Web based application. Software was written for the VGSTN data servers that provide the traffic data in a format suitable for the NeXGen application. The majority of this data is provided based on the keyroute data structure previously discussed in various sections. For example, travel times and average speeds are provided for keyroutes. Also, congestion start and stop points are provided as percentages along these keyroutes (see the end of the &#x201c;Per Station Development&#x201d; section discussed above). Incident data that is not located on freeways is provided based on latitude/longitude data. This NeXGen Data Feed was written to format data provided by the basic VGSTN data service methods.</p>
<p id="p-0122" num="0125">The NeXGen Data Feed will send XML formatted data back to the data layer <b>1303</b>. The data layer will parse the XML file and create subjects for each individual feed item. A subject is an individual traffic item such as incident, sensor, flow or keyroute data. A subject object has associated data properties which store traffic data.</p>
<p id="p-0123" num="0126">There are five types of traffic information that can be downloaded via the TV data feed (see <figref idref="DRAWINGS">FIGS. 14-18</figref>): sensor data, keyroute data, flow data, incident data, and point data.</p>
<p id="p-0124" num="0127">The first data feed type is sensor data (see <figref idref="DRAWINGS">FIG. 14</figref>). It lists sensor data along a specific road. This data is displayed to the user in the traffic monitor portion of the user interface <b>0201</b> and can be added for display on the maps <b>0807</b>. It can be seen that the data includes information regarding the textual description for listing in the traffic monitor <b>1401</b>, the keyroute id <b>1402</b> and percent along the keyroute <b>1403</b> for display purposes, and the actual speed data <b>1404</b> for display in the traffic monitor and the visual graphic.</p>
<p id="p-0125" num="0128">The data feed also provides keyroute data (see <figref idref="DRAWINGS">FIG. 15</figref>). This data includes the processed sensor data for an entire keyroute including: average speed <b>1501</b>, travel time <b>1502</b>, delay time over free flow <b>1503</b>, etc. It also includes labeling metadata including the id <b>1504</b> and the description <b>1505</b>. This data is displayed by the user interface Traffic Monitor section and then graphically shown on Travel Time graphic.</p>
<p id="p-0126" num="0129">The flow data feed provides congestion information for the various routes (see <figref idref="DRAWINGS">FIGS. 16A and 16B</figref>). This is the data that drives the location of the red, yellow, and green colors for the vehicles moving along the roads. The main part of this feed is the specification of where the various colors are to occur on each path. The actual car path may have many sections with various colors. For example, in <figref idref="DRAWINGS">FIGS. 16A and 16B</figref>, a path has green vehicles for the first 67% <b>1601</b>. Then, there is a section that is yellow for 12% <b>1602</b>. The rest of the path is then green <b>1603</b>.</p>
<p id="p-0127" num="0130">The incident data lists traffic incidents that are occurring (see <figref idref="DRAWINGS">FIGS. 17A and 17B</figref>). This is used to display the incident list in the user interface Traffic Monitor section and is also used to place the traffic incident icons in the maps. This feed contains all of the data needed for these purposes, such as the incident type (e.g., CONST&#x2014;for construction <b>1701</b>, and ACC for accident <b>1702</b>). This data feed also has location information for drawing the incident on the maps including both in placement along a keyroute <b>1703</b> (if available) and in latitude and longitude <b>1704</b>. There is also information specifically for display in the Traffic Monitor, such as the incident description <b>1705</b>.</p>
<p id="p-0128" num="0131">The last data feed is the point data, which lists the various points <b>2005</b> along the keyroutes (see <figref idref="DRAWINGS">FIG. 18</figref>). This data is used when the user interface needs to allow the user to manipulate the system based on the various segments <b>2004</b>. For example, the system has the capability for the user to override the flow colors of the vehicles, if necessary. The user does this by specifying the color change from one point to another point from the points that are listed. For the system to do this, the feed must contain the description to display to the user <b>1801</b>, the route id <b>1802</b>, and the percent along the route <b>1803</b>.</p>
<p id="p-0129" num="0132">As a separate functionality from the user interface data request service, the data layer maintains an auto downloader which continuously queries the NeXGen Data Feed, checking for updates to the traffic data that is being used. This data is also returned as an XML formatted file, which is parsed and used to update created subjects. If subjects are updated while the rundown is playing On-Air, the subjects will update their associated scene objects and the updated data will be displayed <b>1310</b>.</p>
<p id="h-0024" num="0000">15. Weather Related Data</p>
<p id="p-0130" num="0133">The Data Layer is also responsible for getting the weather data. It contacts U.S. National Oceanic and Atmospheric Administration's (NOAA) FTP servers and retrieves a radar file (short range base reflectivity at 0.5 degree tilt is the data used in the implementation described herein) from the specified radar site for the metropolitan area <b>2703</b>. It then creates a data set to be passed to the Data Messaging Service, which will then use this data set to notify the weather subject <b>2701</b> that new data has arrived.</p>
<p id="p-0131" num="0134">In order to tell what type of precipitation is occurring, METAR data from the NOAA HTTP server is collected <b>2704</b>. An HTTP request is made to get the METAR data for each airport with METAR data in the metropolitan area. This data is requested by using the International Civil Aviation Organization (ICAO) code for each airport. For example, in Philadelphia, the airports with METAR data are Philadelphia International Airport (ICAO code: KPHL) and North East Philadelphia Airport (ICAO code: KPNE). The METAR data is then parsed to access the current conditions at the ICAO. This data is then stored in a .NET data set along with the location of the ICAO in latitude/longitude and is passed to the weather subject <b>2701</b>.</p>
<p id="h-0025" num="0000">16. Animation Layer Details</p>
<p id="p-0132" num="0135">The Animation layer heavily relies on the capabilities of the Gamebryo Graphics engine. The graphic functionality of the NeXGen system fits well with the capabilities that are provided with Gamebryo. There are several areas of the animation layer implementation that are of notable importance including both software organization areas and concrete details on the usage of Gamebryo.</p>
<p id="h-0026" num="0000">17. Object-Oriented Architecture (<figref idref="DRAWINGS">FIG. 31</figref> and <figref idref="DRAWINGS">FIG. 32</figref>)</p>
<p id="p-0133" num="0136">The NeXGen system architecture is based heavily upon the object-oriented design paradigm where inheritance is when one class extends the behavior of another class. This facilitates maximal re-use of machine logic and simplifies complex components by extending upon more generic components. For instance, the OnAirElement <b>3101</b> component exists as the base class for all six product types in the system. This abstract class defines the basic interface and implementation for creating, deleting, modifying, and updating an element. The sub-classes <b>3102</b> (e.g., GeoElement2D, GeoElement3D, etc) build upon this foundation to manage the detailed requirements of the element. These classes are further extended if the behavior is general enough to be re-used by another class of elements. For instance, the TravelTimeElement <b>3103</b>, which inherits from OnAirElement, is able to be extended to the OverviewElement class. Other examples of inheritance include the Subject <b>3104</b>, CameraConfig <b>3201</b>, Controller <b>3202</b>, and Sceneobject <b>3203</b> hierarchies.</p>
<p id="h-0027" num="0000">18. Shared Worlds (<figref idref="DRAWINGS">FIG. 33</figref>)</p>
<p id="p-0134" num="0137">Elements have the ability to share components with one another in order to increase memory efficiency. The application shares subject, scene object, and controller data structures <b>3301</b>. This sharing occurs in multiple elements based in the same world (e.g., 2D maps in the 2D world <b>3302</b>, Skyview in the 3D world <b>3303</b>, etc). Art assets are also shared even though art in one element may be temporarily hidden when the next element instance is loaded. These shared data structures and art are what constitutes a &#x201c;world&#x201d; in NeXGen. Worlds are instantiated and initialized when the application is started. This shared world description is persistent in memory until the entire application is terminated.</p>
<p id="h-0028" num="0000">19. Camera Generation (<figref idref="DRAWINGS">FIG. 34</figref> and <figref idref="DRAWINGS">FIG. 35</figref>)</p>
<p id="p-0135" num="0138">Each element requires a camera to view the 3D scene from a particular viewpoint, or a set of points in the case of an animated flythrough. Even the 2D graphics are in 3D space, but due to the camera placement looking straight down, the viewer only sees a 2D product. Depending on the type of element, the cameras may be modeled by the art staff or generated dynamically by the application. Subject-based elements generate cameras dynamically based on the position of the subject chosen in the user interface (e.g., a SkyView or 2D map focused on a traffic incident). The 2D map element creates a camera at a fixed distance directly above the subject <b>3401</b>. The camera uses the Gamebryo orthographic model representation where the viewing frustum is modified in order to change the zoom level of the scene, thereby showing more or less of the overall map <b>3402</b>. The Skyview element creates a camera <b>3501</b> represented in the Spherical coordinate system (i.e., two angles <b>3502</b> <b>3503</b> and a radius <b>3504</b>), which can be easily converted into the Cartesian coordinate system, which is needed by the Gamebryo camera. The camera is represented using the Gamebryo perspective camera model, which takes the camera's position, viewing point, and normal vector as input. The default camera positions and viewpoint can be overridden in the user interface.</p>
<p id="p-0136" num="0139">Other elements have statically defined cameras. Image-based elements (e.g., Overview, TravelTime, etc.) have a camera created by the art staff since the position of the billboard is static and thus not subject to modification <b>3403</b>. 3D flythroughs have cameras <b>3405</b> that are set to follow a predefined path <b>3406</b>. The art staff model this path taking into consideration accuracy of viewing highways and landmarks, and aesthetic appeal. Cameras constructed by the art staff use the perspective camera model since this is exported into Gamebryo more effectively.</p>
<p id="p-0137" num="0140">The system also uses controller objects that utilize the camera's properties (e.g., position, view frustum, etc.) to modify scene behavior. The viewport-clip controller detects which objects would be clipped in the viewport of the camera, and culls those objects from the scene. This keeps objects like route shields from partially being displayed. For example, if the map view in <figref idref="DRAWINGS">FIG. 36</figref> was changed to only include the map outlined by the black box <b>3601</b>, certain objects would be eliminated from the maps because they would only be partially displayed. In this case, the town signs <b>3602</b> <b>3603</b> would be eliminated because they would be partially displayed.</p>
<p id="p-0138" num="0141">The scale controller maintains a constant scale for certain objects as the camera changes position. Thus, the route shields are maintained at a constant readable size. Zooming in does not enlarge the shield to an unreasonable size and zooming out does not make the shield too small to read. <figref idref="DRAWINGS">FIG. 37</figref> shows a map with the I-87 shield <b>3701</b> and the &#x201c;Bronx&#x201d; town label <b>3702</b>, each surrounded by a box. The sizes of the boxes can also be seen off of the map <b>3703</b>. <figref idref="DRAWINGS">FIG. 38</figref> shows the same basic map area but at a more zoomed out level so that more area can be seen. Note that the I-87 shield <b>3801</b> and the &#x201c;Bronx&#x201d; town label <b>3802</b> are still the same size. The boxes <b>3803</b> were copied from one figure to the other and they clearly still are the same size as the shield and the label. The scale controller enlarges the objects so that on the zoomed-out map, the objects can remain readable. Note that not all objects are enlarged. For example, the roads are almost the same width as the shields in <figref idref="DRAWINGS">FIG. 37</figref>, but are much smaller in <figref idref="DRAWINGS">FIG. 38</figref>.</p>
<p id="p-0139" num="0142">The display-controller culls objects if the position of the camera does not fall within the artist's pre-defined zoom level boundaries where the object was designed to be visible. This is used to have more route visible shields when zooming in and less visible shields when zooming out. If this were not done, the map would be very readable at a certain zoom level. At a more zoomed-in level, there would not be enough labels and shields to identify the roads. At a more zoomed-out level, there would be too many objects that would clutter the map. For example, the area of the map in <figref idref="DRAWINGS">FIG. 37</figref> is outlined in the box <b>3804</b> on the map in <figref idref="DRAWINGS">FIG. 38</figref>. It can be seen that on the map in <figref idref="DRAWINGS">FIG. 38</figref>, there are four road shields in this area. However, on the more zoomed-in map of the same area in <figref idref="DRAWINGS">FIG. 38</figref>, there are nine full shields in this same area. If the display controller did not hide some of these shields, the zoomed out map in <figref idref="DRAWINGS">FIG. 38</figref> would be very cluttered with shields. The artist also sets a zoom level where the secondary roads <b>3704</b> are hidden. This further reduces clutter on maps at or above this zoom level.</p>
<p id="h-0029" num="0000">20. Scene Object Creation</p>
<p id="p-0140" num="0143">Subjects are created <b>1901</b> based on data obtained from the VGSTN <b>1905</b>, and serve as the basis for all scene objects <b>1902</b> contained in an on-air element. A subject is shared among all on-air elements and can have many scene object instances created from it (i.e., one-to-many relationship) <b>1906</b>. This allows the application to have different graphics and animations for each type of element while still representing consistent information across different elements. For example, the congestion-subject <b>1907</b> will have one scene object for the 2D element <b>1908</b> and another for the 3D element <b>1909</b>. In addition to congestion, other noteworthy scene objects are sensors and incidents.</p>
<p id="p-0141" num="0144">A congestion subject contains a keyroute identification string <b>3901</b>, percentages that partition the keyroute into individual congestion segments <b>3902</b>, and congestion labels representing the traffic conditions <b>3903</b> (e.g., delayed, normal, etc.) for each segment. The keyroute identification string is used to identify the keyroute geometry <b>3904</b> within the art assets of the on-air element. The art is annotated with the same string to make the association explicit between the static art assets and dynamic congestion data. Once identified, this string is stored in the Congestion Scene Object <b>3905</b>.</p>
<p id="p-0142" num="0145">The congestion subject uses keyroute congestion data (e.g., congested areas along the keyroute defined by start and stop percentages) from the VGSTN to alter the car characteristics (speed, color, and density) as they travel along the car path. In response to the arrival of new data, the congestion subject has the ability to modify its corresponding scene objects at runtime as the congested sections of the path change. If this data arrives when the traffic report graphics are actively playing, the congested sections of the car path will visually update.</p>
<p id="p-0143" num="0146">Using the laws of kinematics with the desired velocity for a congestion label, the percentages are used to calculate the starting and ending times of each segment given the car's starting location, since Gamebryo accepts input based on time. For instance using the example in <figref idref="DRAWINGS">FIG. 39</figref>, the congestion may be set to &#x201c;jammed&#x201d; between 20% along the path to 30% along the path <b>3906</b>.
<ul id="ul0002" list-style="none">
    <li id="ul0002-0001" num="0000">
    <ul id="ul0003" list-style="none">
        <li id="ul0003-0001" num="0147">1. This car starts out green so this car would be set to green and placed at 0% of the path at second 0 <b>3907</b>.</li>
        <li id="ul0003-0002" num="0148">2. If a green car travels across the total path in 20 seconds, a green car that starts at the beginning of the path will reach the congestion (20% of the path length) in 4 seconds (20% of 20 seconds=4 seconds). At 4 seconds, it would be set to the red characteristics and at the location 20% along the path <b>3908</b>.</li>
        <li id="ul0003-0003" num="0149">3. The red cars travel at a velocity of 1 so a red car would make the entire path trip in 80 seconds (green cars make the trip in 20 seconds and green cars are 4 times faster than the red cars). Thus, the red car will make it to the end of the congested area (10% further along the path) in 8 seconds (10% of 80 seconds=8 seconds). Since it reached the beginning of the congestion at 4 seconds and takes 8 seconds to reach the end of the congestion, it is changed back to green and 30% of the path at 12 seconds <b>3909</b>.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0144" num="0150">These pairs of (color, time) and (percentage of length, time) are registered with each car in terms of the path dummy defined by the artists for each car on the path with the times phase-shifted based on the start location of each car. For example, for a car starting at the beginning of the path in the example of <figref idref="DRAWINGS">FIG. 39</figref>, the pairs would be:</p>
<p id="p-0145" num="0151">0 sec.: green,</p>
<p id="p-0146" num="0152">0 sec.: 0% path length;</p>
<p id="p-0147" num="0153">4 sec.: red</p>
<p id="p-0148" num="0154">4 sec.: 20% path length</p>
<p id="p-0149" num="0155">12 sec.: green</p>
<p id="p-0150" num="0156">12 sec.: 30% path length.</p>
<p id="h-0030" num="0000">This provides Gamebryo with sufficient information to modify the velocity and color of a car as it travels along a path.</p>
<p id="p-0151" num="0157">In response to new data arrival, the congestion subject has the ability to modify its corresponding scene objects at runtime as the keyroute congestion data changes. If this data arrives when the traffic report graphics are actively playing, this will result in a change to the red, yellow, and green sections that are displayed. This is accomplished by recalculating the pairs of (color, time) and (percentage of length, time) for a new set of cars, as was done at the original world creation. Then, the old and new sets of cars are swapped in the scene graph.</p>
<p id="p-0152" num="0158">Referring to <figref idref="DRAWINGS">FIG. 40</figref>, the sensor subject contains a sensor identification string <b>4001</b>, a keyroute identification string <b>4002</b>, a percentage where it occurs along the keyroute <b>4003</b>, and a velocity measurement <b>4004</b>. Like the congestion scene objects, the keyroute identification string and the percentage are used to place the scene object along a particular keyroute. Gamebryo provides this functionality for objects along a path by setting its phase attribute, which is exactly the same as the percentage. Numerals for the velocity are created from the font specified by the artist, and are displayed on the sensor geometry (using the Gamebryo NiFont functionality). The sensor geometry <b>4005</b> (i.e., the display &#x201c;bubble&#x201d;) is cloned from the one that was created by the artist at scene creation and was placed out of the camera's view <b>2508</b>. In response to new data arrival, the sensor subject has the ability to modify its corresponding scene objects <b>4009</b> at runtime as the velocity changes. If this data arrives when the traffic report graphics are actively playing, this will result in a change to the numerals displayed showing the sensor data.</p>
<p id="p-0153" num="0159">Referring again to <figref idref="DRAWINGS">FIGS. 25 and 40</figref>, the incident subject contains an incident identification string <b>4006</b>, a latitude and longitude for global positioning <b>4007</b>, and can optionally contain a keyroute identification string and keyroute percentage <b>4008</b> for position if it exists on a roadway. The incident identification string is used as a lookup in a configuration file where it maps to the name of the incident geometry in the scene graph <b>2509</b>. The latitude and longitude are used to position the incident within the scene; otherwise it is placed along the keyroute by setting the phase attribute with the percentage. The actual visual object <b>4010</b> placed at the location is a clone of one of the incident markers in a palette of incident markers stored outside the visual world. The specific incident marker is chosen from the palette based on the runtime incident data's type. Also, in response to new data arrival, the incident subject has the ability to modify its corresponding scene objects at runtime as the incident type changes or if the incident expires. If this data arrives when the traffic report graphics are actively playing, this will result in a change to the incident marker that is visually displayed.</p>
<p id="h-0031" num="0000">21. Objects of Interest</p>
<p id="p-0154" num="0160">As part of the map creation process described above, a user selects an object of interest within the geographical region, wherein the object of interest has a corresponding geographical location. A graphical map is then created of at least a portion of a road system, and traffic flow data is displayed on the graphical map. The graphical map includes the geographical location of the user-selected object of interest and may be centered on the object of interest. The object of interest may be a specific traffic event, such as a traffic incident. Alternatively, the object of interest may be a predefined section of the road system, a roadside sensor, a landmark, or a physical address within the geographical region.</p>
<p id="h-0032" num="0000">22. Photographic Representations of the Earth</p>
<p id="p-0155" num="0161">The scope of the present invention is not limited to using graphical maps that are simulations of regions of the earth (e.g., map regions with graphically created roadways, landmarks, and the like), but includes photographic representations of the earth available from arial photography sources such as AirPhotoUSA, Phoenix, Ariz. In such embodiments, the photographic representation becomes another layer in the system sitting below the traffic data (e.g., flow, incidents, sensor data display). The roadway routes can be additionally highlighted if desired with route shields. This approach can be used in 2D and 3D products. In the 3D product, a variety of different approaches can be used. One approach is to view the photographic imagery at an angle with the Skyview and 3D fly-through products as previously described. A more enhanced approach is to use the photographic imagery as a background with the landmarks sitting on top of the imagery. In either approach, as in the 2D method, the traffic data sits on top of the imagery background.</p>
<p id="p-0156" num="0162">The present invention may be implemented with any combination of hardware and software. If implemented as a computer-implemented apparatus, the present invention is implemented using means for performing all of the steps and functions described above.</p>
<p id="p-0157" num="0163">The present invention can be included in an article of manufacture (e.g., one or more computer program products) having, for instance, computer useable media. The media has embodied therein, for instance, computer readable program code means for providing and facilitating the mechanisms of the present invention. The article of manufacture can be included as part of a computer system or sold separately.</p>
<p id="p-0158" num="0164">It will be appreciated by those skilled in the art that changes could be made to the embodiments described above without departing from the broad inventive concept thereof. It is understood, therefore, that this invention is not limited to the particular embodiments disclosed, but it is intended to cover modifications within the spirit and scope of the present invention.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is: </us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A computer-implemented method of displaying on a 3D graphical map traffic flow data representing traffic conditions on a road system, the method comprising:
<claim-text>generating a list of routes on the road system, each route on the list having a start geographical location and an end geographical location;</claim-text>
<claim-text>obtaining traffic flow data for the road system;</claim-text>
<claim-text>obtaining from a user a selection of a route from the list of routes;</claim-text>
<claim-text>obtaining from the user a selection of a start position and an end position on the selected route that are user adjustable within the start and end geographical locations of the selected route; and</claim-text>
<claim-text>creating and displaying an animated flythrough 3D graphical map for a traffic report, wherein the animated flythrough 3D graphical map has a virtual camera view that follows the route beginning at the start position and navigating toward the end position, and shows graphical representations of vehicles on the route whose movements are controlled by the traffic flow data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the virtual camera view further shows 2D informational graphics as the virtual camera view follows the route.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising:
<claim-text>stopping the virtual camera view at a point between the start and end positions while continuously displaying the graphical representations of vehicles on the route whose movements are controlled by the traffic flow data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. An article of manufacture for displaying on a 3D graphical map traffic flow data representing traffic conditions on a road system, the article of manufacture comprising a computer-readable medium holding computer-executable instructions for performing a method comprising:
<claim-text>generating a list of routes on the road system, each route on the list having a start geographical location and an end geographical location;</claim-text>
<claim-text>obtaining traffic flow data for the road system;</claim-text>
<claim-text>obtaining from a user a selection of a route from the list of routes;</claim-text>
<claim-text>obtaining from the user a selection of a start position and an end position on the selected route that are user adjustable within the start and end geographical locations of the selected route; and</claim-text>
<claim-text>creating and displaying an animated flythrough 3D graphical map for a traffic report, wherein the animated flythrough 3D graphical map has a virtual camera view that follows the route beginning at the start position and navigating toward the end position, and shows graphical representations of vehicles on the route whose movements are controlled by the traffic flow data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The article of manufacture of <claim-ref idref="CLM-00004">claim 4</claim-ref> wherein the virtual camera view further shows 2D informational graphics as the virtual camera view follows the route.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The article of manufacture of <claim-ref idref="CLM-00004">claim 4</claim-ref> further comprising:
<claim-text>stopping the virtual camera view at a point between the start and end positions while continuously displaying the graphical representations of vehicles on the route whose movements are controlled by the traffic flow data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. A computer-implemented apparatus for displaying on a 3D graphical map traffic flow data representing traffic conditions on a road system, the apparatus comprising:
<claim-text>means for generating a list of routes on the road system, each route on the list having a start geographical location and an end geographical location;</claim-text>
<claim-text>means for obtaining traffic flow data for the road system;</claim-text>
<claim-text>means for allowing a user to select a route from the list of routes;</claim-text>
<claim-text>means for allowing the user to select a start position and an end position on the selected route that are user adjustable within the start and end geographical locations of the selected route; and</claim-text>
<claim-text>means for creating and displaying an animated flythrough 3D graphical map for a traffic report, wherein the animated flythrough 3D graphical map has a virtual camera view that follows the route beginning at the start position and navigating toward the end position, and shows graphical representations of vehicles on the route whose movements are controlled by the traffic flow data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The apparatus of <claim-ref idref="CLM-00007">claim 7</claim-ref> wherein the virtual camera view further shows 2D informational graphics as the virtual camera view follows the route.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The apparatus of <claim-ref idref="CLM-00007">claim 7</claim-ref> further comprising:
<claim-text>means for stopping the virtual camera view at a point between the start and end positions while continuously displaying graphical representations of vehicles on the route whose movements are controlled by the traffic flow data.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
