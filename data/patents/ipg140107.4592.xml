<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625680-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625680</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>10680072</doc-number>
<date>20031006</date>
</document-id>
</application-reference>
<us-application-series-code>10</us-application-series-code>
<us-term-of-grant>
<us-term-extension>584</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>7</main-group>
<subgroup>12</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>11</main-group>
<subgroup>02</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>11</main-group>
<subgroup>04</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>37524027</main-classification>
<further-classification>37524029</further-classification>
</classification-national>
<invention-title id="d2e53">Bitstream-controlled post-processing filtering</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5684602</doc-number>
<kind>A</kind>
<name>Tsuchiya et al.</name>
<date>19971100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>358404</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5787203</doc-number>
<kind>A</kind>
<name>Lee et al.</name>
<date>19980700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5799113</doc-number>
<kind>A</kind>
<name>Lee</name>
<date>19980800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5970173</doc-number>
<kind>A</kind>
<name>Lee et al.</name>
<date>19991000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6125147</doc-number>
<kind>A</kind>
<name>Florencio et al.</name>
<date>20000900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524029</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6249610</doc-number>
<kind>B1</kind>
<name>Matsumoto et al.</name>
<date>20010600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6281942</doc-number>
<kind>B1</kind>
<name>Wang</name>
<date>20010800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6324301</doc-number>
<kind>B1</kind>
<name>Jacquin et al.</name>
<date>20011100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6332043</doc-number>
<kind>B1</kind>
<name>Ogata</name>
<date>20011200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382240</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6366617</doc-number>
<kind>B1</kind>
<name>Ryan</name>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524025</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6380985</doc-number>
<kind>B1</kind>
<name>Callahan</name>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6389071</doc-number>
<kind>B1</kind>
<name>Wilson</name>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>6466624</doc-number>
<kind>B1</kind>
<name>Fogg</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>6473409</doc-number>
<kind>B1</kind>
<name>Malvar</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>6665346</doc-number>
<kind>B1</kind>
<name>Lee et al.</name>
<date>20031200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524029</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>6941263</doc-number>
<kind>B2</kind>
<name>Wang et al.</name>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>7126989</doc-number>
<kind>B2</kind>
<name>Hagai et al.</name>
<date>20061000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524013</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>2001/0017944</doc-number>
<kind>A1</kind>
<name>Kalevo et al.</name>
<date>20010800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382268</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>2002/0136303</doc-number>
<kind>A1</kind>
<name>Sun et al.</name>
<date>20020900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524016</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>2002/0186890</doc-number>
<kind>A1</kind>
<name>Lee et al.</name>
<date>20021200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>2003/0053541</doc-number>
<kind>A1</kind>
<name>Sun et al.</name>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524016</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>2003/0053708</doc-number>
<kind>A1</kind>
<name>Kryukov et al.</name>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382261</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>2003/0053711</doc-number>
<kind>A1</kind>
<name>Kim</name>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382268</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>2003/0152146</doc-number>
<kind>A1</kind>
<name>Lin et al.</name>
<date>20030800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>2003/0219074</doc-number>
<kind>A1</kind>
<name>Park et al.</name>
<date>20031100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524029</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>2003/0235248</doc-number>
<kind>A1</kind>
<name>Kim et al.</name>
<date>20031200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524012</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>2003/0235250</doc-number>
<kind>A1</kind>
<name>Varma et al.</name>
<date>20031200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>2004/0032908</doc-number>
<kind>A1</kind>
<name>Hagai et al.</name>
<date>20040200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524025</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>2004/0228327</doc-number>
<kind>A1</kind>
<name>Punjabi et al.</name>
<date>20041100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>2004/0228415</doc-number>
<kind>A1</kind>
<name>Wang</name>
<date>20041100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524029</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>US</country>
<doc-number>2004/0247034</doc-number>
<kind>A1</kind>
<name>Zhong et al.</name>
<date>20041200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524029</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>US</country>
<doc-number>2005/0065780</doc-number>
<kind>A1</kind>
<name>Wiser et al.</name>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00033">
<document-id>
<country>US</country>
<doc-number>2006/0188017</doc-number>
<kind>A1</kind>
<name>Hagai et al.</name>
<date>20060800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524015</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00034">
<document-id>
<country>EP</country>
<doc-number>966841</doc-number>
<date>19991200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00035">
<document-id>
<country>EP</country>
<doc-number>1845729</doc-number>
<date>20071000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00036">
<document-id>
<country>GB</country>
<doc-number>2365647</doc-number>
<date>20020200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00037">
<document-id>
<country>JP</country>
<doc-number>05-122681</doc-number>
<date>19930500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00038">
<document-id>
<country>JP</country>
<doc-number>07-231449</doc-number>
<date>19950800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00039">
<document-id>
<country>JP</country>
<doc-number>09-023422</doc-number>
<date>19970100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00040">
<document-id>
<country>JP</country>
<doc-number>11-122624</doc-number>
<date>19990400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00041">
<document-id>
<country>JP</country>
<doc-number>2002-232881</doc-number>
<date>20020800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00042">
<document-id>
<country>JP</country>
<doc-number>07-231449</doc-number>
<date>20050800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00043">
<othercit>Kotropoulos et al., &#x201c;Adaptive LMS <i>L</i>-filters for Noise Suppression in Images,&#x201d; <i>IEEE Transactions on Image Processing</i>, vol. 5, No. 12, pp. 1596-1609 (1996). [48 pp. as' downloaded from the World Wide Web on Apr. 30, 2001.].</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00044">
<othercit>Webb, &#x201c;Postprocessing to Reduce Blocking Artifacts for Low Bit-rate Video Coding Using Chrominance Information,&#x201d; <i>Proc. ICIP</i>, vol. 2, pp. 9-12 (1996).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00045">
<othercit>Linares et al., &#x201c;JPEG Estimated Spectrum Adaptive Postfiltering Using Image-Adaptive Q-Tables and Canny Edge Detectors,&#x201d; <i>Proc. ISCAS'96</i>, Atlanta GA, May 1996.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00046">
<othercit>Lee et al., &#x201c;Blocking Effect Reduction of JPEG Images by Signal Adaptive Filtering,&#x201d; <i>IEEE Trans. on Image Processing</i>, vol. 7, pp. 229-234, Feb. 1998.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00047">
<othercit>Meier et al., &#x201c;Reduction of Blocking Artifacts in Image and Video Coding,&#x201d; <i>IEEE Trans. on Circuits and Systems for Video Technology</i>, vol. 9, No. 3, pp. 490-500, Apr. 1999.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00048">
<othercit>Chen et al., &#x201c;Adaptive post-filtering of transform coefficients for the reduction of blocking artifacts,&#x201d; <i>IEEE Transactions on Circuits and Systems for Video Technology</i>, vol. 11, No. 5, pp. 594-602, 2001.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00049">
<othercit>Choy et al., &#x201c;Reduction of coding artifacts in transform image coding by using local statistics of transform coefficients,&#x201d; <i>IEEE International Symposium on Circuits and Systems</i>, pp. 1089-1092, 1997.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00050">
<othercit>Minami et al., &#x201c;An optimization approach for removing blocking effects in transform coding,&#x201d; <i>IEEE Transactions on Circuits and Systems for Video Technology</i>, vol. 5, No. 2, pp. 74-82, 1995.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00051">
<othercit>Zhang et al., &#x201c;A new approach to reduce the &#x201c;blocking effect&#x201d; of transform coding,&#x201d; <i>IEEE Transactions on Communications</i>, vol. 41, No. 2, pp. 299-302, 1993.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00052">
<othercit>Malvar, &#x201c;Biorthogonal and Nonuniform Lapped Transforms for Transform Coding with Reduced Blocking and Ringing Artifacts,&#x201d; <i>IEEE Transactions on Signal Processing</i>, vol. 46, No. 4, pp. 1043-1053, Apr. 1998.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00053">
<othercit>Malvar, &#x201c;A pre- and post-filtering technique for the reduction of blocking effects,&#x201d; in <i>Proc. Picture Coding Symp.</i>, Stockholm, Sweden, Jun. 1987.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00054">
<othercit>Panis, &#x201c;A method for reducing block artifacts by interpolating block borders,&#x201d; available at http://www.cs.mcgill.ca/&#x2dc;gstamm/Siemensl/paper1.html.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00055">
<othercit>Panis et al., &#x201c;Reduction of block artifacts by selective removal and reconstruction of the block borders,&#x201d; Picture Coding Symposium 97, Berlin, Sep. 10-12, 1997.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00056">
<othercit>Wu et al., &#x201c;Joint estimation of forward and backward motion vectors for interpolative prediction of video,&#x201d; <i>IEEE Transactions on Image Processing </i>3(5):684-687, Sep. 1994.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00057">
<othercit>Printouts of FTP directories from http://ftp3.itu.ch , 8 pp. (downloaded from the World Wide Web on Sep. 20, 2005.).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00058">
<othercit>Park et al., &#x201c;A post processing method for reducing quantization effects in low bit-rate moving picture coding,&#x201d; <i>IEEE Trans. Circuits Syst.</i>, Video Technol., vol. 9, pp. 161-171, Feb. 1999.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00059">
<othercit>Apostolopoulos et al., &#x201c;Post-processing for very-low-bit-rate video compression,&#x201d; <i>IEEE Trans. Image Processing</i>, vol. 8, pp. 1125-1129, Aug. 1999.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00060">
<othercit>Lee et al., &#x201c;Loop filtering and post-filtering for low-bit-rates moving picture coding,&#x201d; <i>Signal Processing: Image Communication 16</i>, pp. 871-890 (2001).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00061">
<othercit>Sun et al., &#x201c;Loop Filter with Skip Mode,&#x201d; Study Group 16, Video Coding Experts Group, 8 pp. (2001).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00062">
<othercit>Cliff Reader, &#x201c;History of MPEG Video Compression&#x2014;Ver. 4.0,&#x201d; 99 pp., document marked Dec. 16, 2003.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00063">
<othercit>Segall et al., &#x201c;Pre- and Post-Processing Algorithms for Compressed Video Enhancement,&#x201d; Proc. 34<sup>th </sup>Asilomar Conf. on Signals and Systems (2000).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00064">
<othercit>&#x201c;Windows Media Video V8 Decoding Specification, Section 3 Syntax and Semantics, Section 4.2.7 Loop Filtering, Section 4.4 Loop Filtering, Annex A Processing for Coding Noise Reduction,&#x201d; 27 pp. (2003).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00065">
<othercit>U.S. Appl. No. 10/623,128, filed Jul. 18, 2003.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00066">
<othercit>&#x201c;Windows Media Video V9 Decoding Specification, revision 87, Section 3 Syntax and Semantics, Section 4.9 In-loop Deblock Filtering, Annex A Processing for Coding Noise Reduction,&#x201d; 57 pp. (2003).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00067">
<othercit>Study Group 16, Video Coding Experts Group &#x201c;Report of the Ad Hoc Committee on Test Model Enhancement and Software Development,&#x201d; 2 pp. (1997).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00068">
<othercit>Study Group 16, Video Coding Experts Group, &#x201c;Post processing with special emphasis on block edges in case Annex J is not used,&#x201d; 2 pp. (1997).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00069">
<othercit>Study Group 16, Video Coding Experts Group, &#x201c;Addition of 'comfort noise as post processing,&#x201d; 2 pp. (1997).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00070">
<othercit>Various, excerpts of reference software and documentation for H.26L JM, including &#x201c;Encoding Types and the Related Variables in H.26L TML Software,&#x201d; &#x201c;Unified Coding Style for the H.26L Reference Software,&#x201d; &#x201c;global.h,&#x201d; &#x201c;Idecod.c,&#x201d; &#x201c;output.c,&#x201d; &#x201c;b<sub>&#x2014;</sub>frame.c,&#x201d; &#x201c;image.c&#x201d; (2001-2002).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00071">
<othercit>ISO/IEC, ISO/IEC 11172-2, Information technology&#x2014;Coding of moving pictures and associated audio for digital storage media at up to about 1,5 Mbit/s&#x2014;Part 2: Video, 112 pp. (1993).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00072">
<othercit>Ebrahimi, &#x201c;MPEG-4 Video Verification Model: A video encoding/decoding algorithm based on content representation,&#x201d; 30 pp. (1997).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00073">
<othercit>ISO/IEC, &#x201c;JTC1/SC29/WG11 N2202, Information Technology&#x2014;Coding of Audio-Visual Objects: Visual, ISO/IEC 14496-2,&#x201d; 329 pp. (1998).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00074">
<othercit>ITU-T, &#x201c;ITU-T Recommendation H.261, Video Codec for Audiovisual Services at p. &#xd7;64 kbits,&#x201d; 25 pp. (1993).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00075">
<othercit>ITU-T, &#x201c;ITU-T Recommendation H.262, Information Technology&#x2014;Generic Coding of Moving Pictures and Associated Audio Information: Video,&#x201d; 205 pp. (1995).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00076">
<othercit>ITU-T, &#x201c;ITU-T Recommendation H.263, Video coding for low bit rate communication,&#x201d; 162 pp. (1998).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00077">
<othercit>Joint Video Team (JVT) of ISO/IEC MPEG &#x26; ITU-T VCEG, &#x201c;Draft ITU-T Recommendation and Final Draft International Standard of Joint Video Specification (ITU-T Rec. H.264 |ISO/IEC 14496-10 AVC,&#x201d; 253 pp. (May 2003).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00078">
<othercit>Kuo et al., &#x201c;Adaptive Postprocessor for Block Encoded Images,&#x201d; IEEE Trans. on Circuits and Systems for Video Technology, vol. 5, No. 4 (Aug. 1995).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00079">
<othercit>O'Rourke et al., &#x201c;Improved Image Decompression for Reduced Transform Coding Artifacts,&#x201d; IEEE Trans. on Circuits and Systems for Video Technology, vol. 5, No. 6, (Dec. 1995).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00080">
<othercit>Wiegand et al., &#x201c;Overview of the 11.264/AVC Video Coding Standard,&#x201d; IEEE Transactions on Circuits and Systems for Video Technology, vol. 13, No. 7, Jul. 2003, 19 pp.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00081">
<othercit>Partial Search Report from European Patent Application No. 04 01 975.3, dated Apr. 5, 2005, 5 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00082">
<othercit>Examiner's Grounds for Rejection from Korean Patent Application No. 10-2004-71449, dated Mar. 17, 2006, 7 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00083">
<othercit>Notice on Office Action from Chinese Patent Application No. 200410077189.0, dated Jul. 7, 2006, 8 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00084">
<othercit>Examination Report from European Patent Application No. 04 019 753.5, dated Sep. 27, 2006, 5 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00085">
<othercit>Notice of Rejection from Japanese Patent Application No. 2004-260264, dated Oct. 3, 2006, 4 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00086">
<othercit>Examiner's Grounds for Rejection from Korean Patent Application No. 10-2004-71449, dated Dec. 29, 2006, 3 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00087">
<othercit>Notice of Final Rejection from Japanese Patent Application No. 2004-260264, dated Aug. 24, 2007, 9 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00088">
<othercit>Second Office Action from Chinese Patent Application No. 200410077189.0, dated Mar. 6, 2009, 8 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00089">
<othercit>Notice on Grant of Patent Right for Invention from Chinese Patent Application No. 200410077189.0, dated Aug. 28, 2009, 4 pp.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00090">
<othercit>Appeal Decision from Japanese Patent Application No. 2004-260264, dated Nov. 12, 2010, 4 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00091">
<othercit>Interrogation from Japanese Patent Application No. 2004-260264, dated Jan. 8, 2010, 7 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00092">
<othercit>Notice of Grant from Korean Patent Application No. 10-2004-71449, dated Nov. 7, 2007, 3 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00093">
<othercit>Official Notice of Patent Registration from Japanese Patent Application No. 2004-260264, dated Nov. 26, 2010, 2 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00094">
<othercit>Examination Report dated Apr. 30, 2013, from European Patent Application No. 04019753.5, 4 pp.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>39</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>37524029</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>37524027</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>4</number-of-drawing-sheets>
<number-of-figures>7</number-of-figures>
</figures>
<us-related-documents>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60501081</doc-number>
<date>20030907</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20050053288</doc-number>
<kind>A1</kind>
<date>20050310</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Srinivasan</last-name>
<first-name>Sridhar</first-name>
<address>
<city>Seattle</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Lin</last-name>
<first-name>Chih-Lung</first-name>
<address>
<city>Redmond</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Hsu</last-name>
<first-name>Pohsiang</first-name>
<address>
<city>Redmond</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="004" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Holcomb</last-name>
<first-name>Thomas W.</first-name>
<address>
<city>Bothell</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="005" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Lee</last-name>
<first-name>Ming-Chieh</first-name>
<address>
<city>Bellevue</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="006" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Ribas-Corbera</last-name>
<first-name>Jordi</first-name>
<address>
<city>Redmond</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Srinivasan</last-name>
<first-name>Sridhar</first-name>
<address>
<city>Seattle</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Lin</last-name>
<first-name>Chih-Lung</first-name>
<address>
<city>Redmond</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Hsu</last-name>
<first-name>Pohsiang</first-name>
<address>
<city>Redmond</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="004" designation="us-only">
<addressbook>
<last-name>Holcomb</last-name>
<first-name>Thomas W.</first-name>
<address>
<city>Bothell</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="005" designation="us-only">
<addressbook>
<last-name>Lee</last-name>
<first-name>Ming-Chieh</first-name>
<address>
<city>Bellevue</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="006" designation="us-only">
<addressbook>
<last-name>Ribas-Corbera</last-name>
<first-name>Jordi</first-name>
<address>
<city>Redmond</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Klarquist Sparkman, LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Microsoft Corporation</orgname>
<role>02</role>
<address>
<city>Redmond</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Anyikire</last-name>
<first-name>Chikaodili E</first-name>
<department>2487</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Techniques and tools for bitstream-controlled filtering are described. For example, a video encoder puts control information into a bitstream for encoded video. A video decoder decodes the encoded video and, according to the control information, performs post-processing filtering on the decoded video with a de-ringing and/or de-blocking filter. Typically, a content author specifies the control information to the encoder. The control information itself is post-processing filter levels, filter selections, and/or some other type of information. In the bitstream, the control information is specified for a sequence, scene, frame, region within a frame, or at some other syntax level.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="99.65mm" wi="145.88mm" file="US08625680-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="217.42mm" wi="172.97mm" file="US08625680-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="215.90mm" wi="180.42mm" orientation="landscape" file="US08625680-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="215.90mm" wi="167.05mm" orientation="landscape" file="US08625680-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="218.69mm" wi="161.29mm" file="US08625680-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATION</heading>
<p id="p-0002" num="0001">This application claims the benefit of U.S. Provisional Patent Application Ser. No. 60/501,081, entitled &#x201c;VIDEO ENCODING AND DECODING TOOLS AND TECHNIQUES,&#x201d; filed Sep. 7, 2003, the disclosure of which is incorporated herein by reference.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">TECHNICAL FIELD</heading>
<p id="p-0003" num="0002">Techniques and tools for bitstream-controlled filtering are described. For example, a video encoder provides control information for post-processing filtering, and a video decoder performs bitstream-controlled post-processing filtering with a de-ringing and/or de-blocking filter.</p>
<heading id="h-0003" level="1">BACKGROUND</heading>
<p id="p-0004" num="0003">Digital video consumes large amounts of storage and transmission capacity. A typical raw digital video sequence includes 15 or 30 frames per second. Each frame can include tens or hundreds of thousands of pixels (also called pels). Each pixel represents a tiny element of the picture. In raw form, a computer commonly represents a pixel with 24 bits. Thus, the number of bits per second, or bitrate, of a typical raw digital video sequence can be 5 million bits/second or more.</p>
<p id="p-0005" num="0004">Most computers and computer networks lack the resources to process raw digital video. For this reason, engineers use compression (also called coding or encoding) to reduce the bitrate of digital video. Compression can be lossless, in which quality of the video does not suffer but decreases in bitrate are limited by the complexity of the video. Or, compression can be lossy, in which quality of the video suffers but decreases in bitrate are more dramatic. Decompression reverses compression.</p>
<p id="p-0006" num="0005">In general, video compression techniques include intraframe compression and interframe compression. Intraframe compression techniques compress individual frames, typically called I-frames or key frames. Interframe compression techniques compress frames with reference to preceding and/or following frames, which are typically called predicted frames, P-frames, or B-frames.</p>
<p id="p-0007" num="0006">Microsoft Corporation's Windows Media Video Versions 8 [&#x201c;WMV8&#x201d;] and 9 [&#x201c;WMV9&#x201d;] each include a video encoder and a video decoder. The encoders use intraframe and interframe compression, and the decoders use intraframe and interframe decompression. There are also several international standards for video compression and decompression, including the Motion Picture Experts Group [&#x201c;MPEG&#x201d;] 1, 2, and 4 standards and the H.26x standards. Like WMV8 and WMV9, these standards use a combination of intraframe and interframe compression and decompression.</p>
<p id="h-0004" num="0000">I. Block-Based Intraframe Compression and Decompression</p>
<p id="p-0008" num="0007">Many prior art encoders use block-based intraframe compression. To illustrate, suppose an encoder splits a video frame into 8&#xd7;8 blocks of pixels and applies an 8&#xd7;8 Discrete Cosine Transform [&#x201c;DCT&#x201d;] to individual blocks. The DCT converts a given 8&#xd7;8 block of pixels (spatial information) into an 8&#xd7;8 block of DCT coefficients (frequency information). The DCT operation itself is lossless or nearly lossless. The encoder quantizes the DCT coefficients, resulting in an 8&#xd7;8 block of quantized DCT coefficients. Quantization is lossy, resulting in loss of precision, if not complete loss of the information for the coefficients. The encoder then prepares the 8&#xd7;8 block of quantized DCT coefficients for entropy encoding and performs the entropy encoding, which is a form of lossless compression.</p>
<p id="p-0009" num="0008">A corresponding decoder performs a corresponding decoding process. For a given block, the decoder performs entropy decoding, inverse quantization, an inverse DCT, etc., resulting in a reconstructed block. Due to the quantization, the reconstructed block is not identical to the original block. In fact, there may be perceptible errors within reconstructed blocks or at the boundaries between reconstructed blocks.</p>
<p id="h-0005" num="0000">II. Block-Based Interframe Compression and Decompression</p>
<p id="p-0010" num="0009">Many prior art encoders use block-based motion-compensated prediction coding followed by transform coding of residuals. To illustrate, suppose an encoder splits a predicted frame into 8&#xd7;8 blocks of pixels. Groups of four 8&#xd7;8 luminance blocks and two co-located 8&#xd7;8 chrominance blocks form macroblocks. Motion estimation approximates the motion of the macroblock relative to a reference frame, for example, a previously coded, preceding frame. The encoder computes a motion vector for the macroblock. In motion compensation, the motion vector is used to compute a prediction macroblock for the macroblock using information from the reference frame. The prediction is rarely perfect, so the encoder usually encodes blocks of pixel differences (also called the error or residual blocks) between the prediction and the original macroblock. The encoder applies a DCT to the error blocks, resulting in blocks of coefficients. The encoder quantizes the DCT coefficients, prepares the blocks of quantized DCT coefficients for entropy encoding, and performs the entropy encoding.</p>
<p id="p-0011" num="0010">A corresponding decoder performs a corresponding decoding process. The decoder performs entropy decoding, inverse quantization, an inverse DCT, etc., resulting in reconstructed error blocks. In a separate motion compensation path, the decoder computes a prediction using motion vector information relative to a reference frame. The decoder combines the prediction with the reconstructed error blocks. Again, the reconstructed video is not identical to the corresponding original, and there may be perceptible errors within reconstructed blocks or at the boundaries between reconstructed blocks.</p>
<p id="h-0006" num="0000">III. Blocking Artifacts and Ringing Artifacts</p>
<p id="p-0012" num="0011">Lossy compression can result in noticeable errors in video after reconstruction. The heavier the lossy compression and the higher the quality of the original video, the more likely it is for perceptible errors to be introduced in the reconstructed video. Two common kinds of errors are blocking artifacts and ringing artifacts.</p>
<p id="p-0013" num="0012">Block-based compression techniques have benefits such as ease of implementation, but introduce blocking artifacts, which are perhaps the most common and annoying type of distortion in digital video today. Blocking artifacts are visible discontinuities around the edges of blocks in reconstructed video. Quantization and truncation (e.g., of transform coefficients from a block-based transform) cause blocking artifacts, especially when the compression ratio is high. When blocks are quantized independently, for example, one block may be quantized less or more than an adjacent block. Upon reconstruction, this can result in blocking artifacts at the boundary between the two blocks. Or, blocking artifacts may result when high-frequency coefficients are quantized, if the overall content of the blocks differs and the high-frequency coefficients are necessary to reconstruct transition detail across block boundaries.</p>
<p id="p-0014" num="0013">Ringing artifacts are caused by quantization or truncation of high-frequency transform coefficients, whether the transform coefficients are from a block-based transform or from a wavelet-based transform. Both such transforms essentially represent an area of pixels as a sum of regular waveforms, where the waveform coefficients are quantized, encoded, etc. In some cases, the contributions of high-frequency waveforms counter distortion introduced by a low-frequency waveform. If the high-frequency coefficients are heavily quantized, the distortion may become visible as a wave-like oscillation at the low frequency. For example, suppose an image area includes sharp edges or contours, and high-frequency coefficients are heavily quantized. In a reconstructed image, the quantization may cause ripples or oscillations around the sharp edges or contours.</p>
<p id="h-0007" num="0000">IV. Post-Processing Filtering</p>
<p id="p-0015" num="0014">Blocking artifacts and ringing artifacts can be reduced using de-blocking and de-ringing techniques. These techniques are generally referred to as post-processing techniques, since they are typically applied after video has been decoded. Post-processing usually enhances the perceived quality of reconstructed video.</p>
<p id="p-0016" num="0015">The WMV8 and WMV9 decoders use specialized filters to reduce blocking and ringing artifacts during post-processing. For additional information, see Annex A of U.S. Provisional Patent Application Ser. No. 60/341,674, filed Dec. 17, 2001 and Annex A of U.S. Provisional Patent Application Ser. No. 60/488,710, filed Jul. 18, 2003. Similarly, software implementing several of the MPEG and H.26x standards mentioned above has de-blocking and/or de-ringing filters. For example, see (1) the MPEG-4 de-blocking and de-ringing filters as tested in the verification model and described in Annex F, Section 15.3 of MPEG-4 draft N2202, (2) the H.263+ post-processing filter as tested in the Test Model Near-term, and (3) the H.264 JM post-processing filter. In addition, numerous publications address post-processing filtering techniques (as well as corresponding pre-processing techniques, in some cases). For example, see (1) Kuo et al., &#x201c;Adaptive Postprocessor for Block Encoded Images,&#x201d; IEEE Trans. on Circuits and Systems for Video Technology, Vol. 5, No. 4 (August 1995), (2) O'Rourke et al., &#x201c;Improved Image Decompression for Reduced Transform Coding Artifacts,&#x201d; IEEE Trans. on Circuits and Systems for Video Technology, Vol. 5, No. 6, (1995), and (3) Segall et al., &#x201c;Pre- and Post-Processing Algorithms for Compressed Video Enhancement,&#x201d; Proc. 34<sup>th </sup>Asilomar Conf. on Signals and Systems (2000).</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 1</figref> is a generalized diagram of post-processing filtering according to the prior art. A video encoder (<b>110</b>) accepts source video (<b>105</b>), encodes it, and produces a video bitstream (<b>115</b>). The video bitstream (<b>115</b>) is delivered via a channel (<b>120</b>), for example, by transmission as streaming media over a network. A video decoder (<b>130</b>) receives and decodes the video bitstream (<b>115</b>), producing decoded video (<b>135</b>). A post-processing filter (<b>140</b>) such as a de-ringing and/or de-blocking filter is used on the decoded video (<b>135</b>), producing decoded, post-processed video (<b>145</b>).</p>
<p id="p-0018" num="0017">Strictly speaking, post-processing filtering techniques are not needed to decode the video bitstream (<b>115</b>). Codec (enCOder/DECoder) engineers may decide whether to apply such techniques when designing a codec. The decision can depend, for example, on whether CPU cycles are available for a software decoder, or on the additional cost for a hardware decoder. Since post-processing filtering techniques usually enhance video quality significantly, they are commonly applied in most video decoders today. Post-processing filters are sometimes designed independently from a video codec, so the same de-blocking and de-ringing filters may be applied to different codecs.</p>
<p id="p-0019" num="0018">In prior systems, post-processing filtering is applied automatically to an entire video sequence. The assumption is that post-processing filtering will always at least improve video quality, and thus post-processing filtering should always be on. From system to system, filters may have different strengths according to the capabilities of the decoder. Moreover, some filters selectively disable or change the strength of filtering depending on decoder-side evaluation of the content of reconstructed video, but this adaptive processing is still automatically performed. There are several problems with these approaches.</p>
<p id="p-0020" num="0019">First, the assumption that post-processing filtering always at least improves video quality is incorrect. For high quality video that is compressed without much loss, post-processing de-blocking and de-ringing may eliminate texture details and noticeably blur video images, actually decreasing quality. This sometimes occurs for high definition video encoded at high bitrates.</p>
<p id="p-0021" num="0020">Second, there is no information in the video bitstream that guides post-processing filtering. The author is not allowed to control or adapt post-processing filtering by introducing information in the video bitstream to control the filtering.</p>
<p id="h-0008" num="0000">V. In-Loop Filtering</p>
<p id="p-0022" num="0021">Aside from post-processing filtering, several prior art systems use in-loop filtering. In-loop filtering involves filtering (e.g., de-blocking filtering) on reconstructed reference frames during motion compensation in the encoding and decoding processes (whereas post-processing is applied after the decoding process). By reducing artifacts in reference frames, the encoder and decoder improve the quality of motion-compensated prediction from the reference frames. For example, see (1) section 4.4 of U.S. Provisional Patent Application Ser. No. 60/341,674, filed Dec. 17, 2001, (2) section 4.9 of U.S. Provisional Patent Application Ser. No. 60/488,710, filed Jul. 18, 2003, (3) section 3.2.3 of the H.261 standard (which describes conditional low-pass filtering of macroblocks), (4) section 3.4.8 and Annex J of the H.263 standard, and (3) the relevant sections of the H.264 standard.</p>
<p id="p-0023" num="0022">In particular, the H.264 standard allows an author to turn in-loop filtering on and off, and even modify the strength of the filtering, on a scene-by-scene basis. The H.264 standard does not, however, allow the author to adapt loop filtering for regions within a frame. Moreover, the H.264 standard applies only one kind of in-loop filter.</p>
<p id="p-0024" num="0023">Given the critical importance of video compression and decompression to digital video, it is not surprising that video compression and decompression are richly developed fields. Whatever the benefits of previous video compression and decompression techniques, however, they do not have the advantages of the following techniques and tools.</p>
<heading id="h-0009" level="1">SUMMARY</heading>
<p id="p-0025" num="0024">In summary, the detailed description is directed to various techniques and tools for bitstream-controlled filtering. For example, a video encoder puts control information into a bitstream for encoded video. A video decoder decodes the encoded video and, according to the control information, performs post-processing filtering on the decoded video. With this kind of control, a human operator can allow post-processing to the extent it enhances video quality and otherwise disable the post-processing. In one scenario, the operator controls post-processing filtering to prevent excessive blurring in reconstruction of high-definition, high bitrate video.</p>
<p id="p-0026" num="0025">The various techniques and tools can be used in combination or independently.</p>
<p id="p-0027" num="0026">In one aspect, a video encoder or other tool receives and encodes video data, and outputs the encoded video data as well as control information. The control information is for controlling post-processing filtering of the video data after decoding. The post-processing filtering includes de-blocking, de-ringing, and/or other kinds of filtering. Typically, a human operator specifies control information such as post-processing filter levels (i.e., filter strengths) or filter type selections. Depending on implementation, the control information is specified for a sequence, scene, frame, region within a frame, and/or at some other level.</p>
<p id="p-0028" num="0027">In another aspect, a video decoder or other tool receives encoded video data and control information, decodes the encoded video data, and performs post-processing filtering on the decoded video data based at least in part upon the received control information. Again, the post-processing filtering includes de-blocking, de-ringing, and/or other kinds of filtering, and the control information is specified for a sequence, scene, frame, region within a frame, and/or at some other level, depending on implementation.</p>
<p id="p-0029" num="0028">Additional features and advantages will be made apparent from the following detailed description of different embodiments that proceeds with reference to the accompanying drawings.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0010" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 1</figref> is a diagram showing post-processing filtering according to the prior art.</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram of a suitable computing environment.</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 3</figref> is a block diagram of a generalized video encoder system.</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 4</figref> is a block diagram of a generalized video decoder system.</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 5</figref> is a diagram showing bitstream-controlled post-processing filtering.</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 6</figref> is a flowchart showing a technique for producing a bitstream with embedded control information for post-processing filtering.</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 7</figref> is a flowchart showing a technique for performing bitstream-controlled post-processing filtering.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0011" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0037" num="0036">The present application relates to techniques and tools for bitstream-controlled post-processing filtering for de-blocking and de-ringing reconstructed video. The techniques and tools give a human operator control over post-processing filtering, such that the operator can enable post-processing to the extent it enhances video quality and otherwise disable the post-processing. For example, the operator controls post-processing filtering to prevent excessive blurring in reconstruction of high-definition, high bitrate video.</p>
<p id="p-0038" num="0037">Among other things, the application relates to techniques and tools for specifying control information, parameterizing control information, signaling control information, and filtering according to control information. The various techniques and tools can be used in combination or independently. Different embodiments implement one or more of the described techniques and tools.</p>
<p id="p-0039" num="0038">While much of the detailed description relates directly to de-blocking and de-ringing filtering during post-processing, the techniques and tools may also be applied at other stages (e.g., in-loop filtering in encoding and decoding) and for other kinds of filtering.</p>
<p id="p-0040" num="0039">Similarly, while much of the detailed description relates to video encoders and decoders, another type of video processing tool or other tool may implement one or more of the techniques for bitstream-controlled filtering.</p>
<p id="h-0012" num="0000">I. Computing Environment</p>
<p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. 2</figref> illustrates a generalized example of a suitable computing environment (<b>200</b>) in which several of the described embodiments may be implemented. The computing environment (<b>200</b>) is not intended to suggest any limitation as to scope of use or functionality, as the techniques and tools may be implemented in diverse general-purpose or special-purpose computing environments.</p>
<p id="p-0042" num="0041">With reference to <figref idref="DRAWINGS">FIG. 2</figref>, the computing environment (<b>200</b>) includes at least one processing unit (<b>210</b>) and memory (<b>220</b>). In <figref idref="DRAWINGS">FIG. 2</figref>, this most basic configuration (<b>230</b>) is included within a dashed line. The processing unit (<b>210</b>) executes computer-executable instructions and may be a real or a virtual processor. In a multi-processing system, multiple processing units execute computer-executable instructions to increase processing power. The memory (<b>220</b>) may be volatile memory (e.g., registers, cache, RAM), non-volatile memory (e.g., ROM, EEPROM, flash memory, etc.), or some combination of the two. The memory (<b>220</b>) stores software (<b>280</b>) implementing bitstream-controlled filtering techniques for an encoder and/or decoder.</p>
<p id="p-0043" num="0042">A computing environment may have additional features. For example, the computing environment (<b>200</b>) includes storage (<b>240</b>), one or more input devices (<b>250</b>), one or more output devices (<b>260</b>), and one or more communication connections (<b>270</b>). An interconnection mechanism (not shown) such as a bus, controller, or network interconnects the components of the computing environment (<b>200</b>). Typically, operating system software (not shown) provides an operating environment for other software executing in the computing environment (<b>200</b>), and coordinates activities of the components of the computing environment (<b>200</b>).</p>
<p id="p-0044" num="0043">The storage (<b>240</b>) may be removable or non-removable, and includes magnetic disks, magnetic tapes or cassettes, CD-ROMs, DVDs, or any other medium which can be used to store information and which can be accessed within the computing environment (<b>200</b>). The storage (<b>240</b>) stores the software (<b>280</b>) implementing the bitstream-controlled filtering techniques for an encoder and/or decoder.</p>
<p id="p-0045" num="0044">The input device(s) (<b>250</b>) may be a touch input device such as a keyboard, mouse, pen, or trackball, a voice input device, a scanning device, or another device that provides input to the computing environment (<b>200</b>). For audio or video encoding, the input device(s) (<b>250</b>) may be a sound card, video card, TV tuner card, or similar device that accepts audio or video input in analog or digital form, or a CD-ROM or CD-RW that reads audio or video samples into the computing environment (<b>200</b>). The output device(s) (<b>260</b>) may be a display, printer, speaker, CD-writer, or another device that provides output from the computing environment (<b>200</b>).</p>
<p id="p-0046" num="0045">The communication connection(s) (<b>270</b>) enable communication over a communication medium to another computing entity. The communication medium conveys information such as computer-executable instructions, audio or video input or output, or other data in a modulated data signal. A modulated data signal is a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example, and not limitation, communication media include wired or wireless techniques implemented with an electrical, optical, RF, infrared, or other carrier.</p>
<p id="p-0047" num="0046">The techniques and tools can be described in the general context of computer-readable media. Computer-readable media are any available media that can be accessed within a computing environment. By way of example, and not limitation, with the computing environment (<b>200</b>), computer-readable media include memory (<b>220</b>), storage (<b>240</b>), communication media, and combinations of any of the above.</p>
<p id="p-0048" num="0047">The techniques and tools can be described in the general context of computer-executable instructions, such as those included in program modules, being executed in a computing environment on a target real or virtual processor. Generally, program modules include routines, programs, libraries, objects, classes, components, data structures, etc. that perform particular tasks or implement particular abstract data types. The functionality of the program modules may be combined or split between program modules as desired in various embodiments. Computer-executable instructions for program modules may be executed within a local or distributed computing environment.</p>
<p id="h-0013" num="0000">II. Generalized Video Encoder and Decoder</p>
<p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. 3</figref> is a block diagram of a generalized video encoder (<b>300</b>) and <figref idref="DRAWINGS">FIG. 4</figref> is a block diagram of a generalized video decoder (<b>400</b>).</p>
<p id="p-0050" num="0049">The relationships shown between modules within the encoder and decoder indicate the main flow of information in the encoder and decoder; other relationships are not shown for the sake of simplicity. In particular, <figref idref="DRAWINGS">FIGS. 3 and 4</figref> usually do not show side information indicating the encoder settings, modes, tables, etc. used for a video sequence, frame/field, macroblock, block, etc. Such side information is sent in the output bitstream, typically after entropy encoding of the side information. The format of the output bitstream can be Windows Media Video version 9 format or another format.</p>
<p id="p-0051" num="0050">The encoder (<b>300</b>) and decoder (<b>400</b>) are block-based and use a 4:2:0 macroblock format with each macroblock including 4 luminance 8&#xd7;8 luminance blocks (at times treated as one 16&#xd7;16 macroblock) and two 8&#xd7;8 chrominance blocks. The encoder (<b>300</b>) and decoder (<b>400</b>) operate on video pictures, which are video frames and/or video fields. Alternatively, the encoder (<b>300</b>) and decoder (<b>400</b>) are object-based, use a different macroblock or block format, or perform operations on sets of pixels of different size or configuration than 8&#xd7;8 blocks and 16&#xd7;16 macroblocks.</p>
<p id="p-0052" num="0051">Depending on implementation and the type of compression desired, modules of the encoder or decoder can be added, omitted, split into multiple modules, combined with other modules, and/or replaced with like modules. In alternative embodiments, encoder or decoders with different modules and/or other configurations of modules perform one or more of the described techniques.</p>
<p id="p-0053" num="0052">A. Video Encoder</p>
<p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. 3</figref> is a block diagram of a general video encoder system (<b>300</b>). The encoder system (<b>300</b>) receives a sequence of video pictures including a current picture (<b>305</b>), and produces compressed video information (<b>395</b>) as output. Particular embodiments of video encoders typically use a variation or supplemented version of the generalized encoder (<b>300</b>).</p>
<p id="p-0055" num="0054">The encoder system (<b>300</b>) compresses predicted pictures and key pictures. For the sake of presentation, <figref idref="DRAWINGS">FIG. 3</figref> shows a path for key pictures through the encoder system (<b>300</b>) and a path for forward-predicted pictures. Many of the components of the encoder system (<b>300</b>) are used for compressing both key pictures and predicted pictures. The exact operations performed by those components can vary depending on the type of information being compressed.</p>
<p id="p-0056" num="0055">A predicted picture (also called p-picture, b-picture for bi-directional prediction, or inter-coded picture) is represented in terms of prediction (or difference) from one or more other pictures. A prediction residual is the difference between what was predicted and the original picture. In contrast, a key picture (also called i-picture, intra-coded picture) is compressed without reference to other pictures.</p>
<p id="p-0057" num="0056">If the current picture (<b>305</b>) is a forward-predicted picture, a motion estimator (<b>310</b>) estimates motion of macroblocks or other sets of pixels of the current picture (<b>305</b>) with respect to a reference picture (<b>325</b>), which is the reconstructed previous picture buffered in the picture store (<b>320</b>). In alternative embodiments, the reference picture is a later picture or the current picture is bi-directionally predicted. The motion estimator (<b>310</b>) outputs as side information motion information (<b>315</b>) such as motion vectors. A motion compensator (<b>330</b>) applies the motion information (<b>315</b>) to the reference picture (<b>325</b>) to form a motion-compensated current picture prediction (<b>335</b>). The prediction is rarely perfect, however, and the difference between the motion-compensated current picture prediction (<b>335</b>) and the original current picture (<b>305</b>) is the prediction residual (<b>345</b>). Alternatively, a motion estimator and motion compensator apply another type of motion estimation/compensation.</p>
<p id="p-0058" num="0057">A frequency transformer (<b>360</b>) converts spatial domain video information into frequency domain (i.e., spectral) data. For block-based video pictures, the frequency transformer (<b>360</b>) applies DCT or variant of DCT to blocks of the pixel data or prediction residual data, producing blocks of DCT coefficients. Alternatively, the frequency transformer (<b>360</b>) applies another conventional frequency transform such as a Fourier transform or uses wavelet or subband analysis. In some embodiments, the frequency transformer (<b>360</b>) applies an 8&#xd7;8, 8&#xd7;4, 4&#xd7;8, or other size frequency transform (e.g., DCT) to prediction residuals for predicted pictures.</p>
<p id="p-0059" num="0058">A quantizer (<b>370</b>) then quantizes the blocks of spectral data coefficients. The quantizer applies uniform, scalar quantization to the spectral data with a step-size that varies on a picture-by-picture basis or other basis. Alternatively, the quantizer applies another type of quantization to the spectral data coefficients, for example, a non-uniform, vector, or non-adaptive quantization, or directly quantizes spatial domain data in an encoder system that does not use frequency transformations.</p>
<p id="p-0060" num="0059">When a reconstructed current picture is needed for subsequent motion estimation/compensation, an inverse quantizer (<b>376</b>) performs inverse quantization on the quantized spectral data coefficients. An inverse frequency transformer (<b>366</b>) then performs the inverse of the operations of the frequency transformer (<b>360</b>), producing a reconstructed prediction residual or reconstructed key picture data. If the current picture (<b>305</b>) was a key picture, the reconstructed key picture is taken as the reconstructed current picture (not shown). If the current picture (<b>305</b>) was a predicted picture, the reconstructed prediction residual is added to the motion-compensated current picture prediction (<b>335</b>) to form the reconstructed current picture. The picture store (<b>320</b>) buffers the reconstructed current picture for use in predicting the next picture. In some embodiments, the encoder (<b>300</b>) applies an in-loop de-blocking filter to the reconstructed picture to adaptively smooth discontinuities at block boundaries in the picture. For additional detail, see U.S. patent application Ser. No. 10/322,383, filed Dec. 17, 2002, and U.S. patent application Ser. No. 10/623,128, filed Jul. 18, 2003, the disclosures of which are hereby incorporated by reference.</p>
<p id="p-0061" num="0060">The entropy coder (<b>380</b>) compresses the output of the quantizer (<b>370</b>) as well as certain side information. Typical entropy coding techniques include arithmetic coding, differential coding, Huffman coding, run length coding, LZ coding, dictionary coding, and combinations of the above. The entropy coder (<b>380</b>) typically uses different coding techniques for different kinds of information, and can choose from among multiple code tables within a particular coding technique.</p>
<p id="p-0062" num="0061">The entropy coder (<b>380</b>) puts compressed video information (<b>395</b>) in the buffer (<b>390</b>). A buffer level indicator is fed back to bitrate adaptive modules. The compressed video information (<b>395</b>) is depleted from the buffer (<b>390</b>) at a constant or relatively constant bitrate and stored for subsequent streaming at that bitrate. Or, the encoder system (<b>300</b>) streams compressed video information at a variable rate.</p>
<p id="p-0063" num="0062">Before or after the buffer (<b>390</b>), the compressed video information (<b>395</b>) can be channel coded for transmission over a network. The channel coding can apply error detection and correction data to the compressed video information (<b>395</b>).</p>
<p id="p-0064" num="0063">In addition, the encoder (<b>300</b>) accepts control information for filtering operations. The control information may originate from a content author or other human operator, and may be provided to the encoder through an encoder setting or through programmatic control by an application. Or, the control information may originate from another source such as a module within the encoder (<b>300</b>) itself. The control information controls filtering operations such as post-processing de-blocking and/or de-ringing filtering, as described below. The encoder (<b>300</b>) outputs the control information at an appropriate syntax level in the compressed video information (<b>395</b>).</p>
<p id="p-0065" num="0064">B. Video Decoder</p>
<p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. 4</figref> is a block diagram of a general video decoder system (<b>400</b>). The decoder system (<b>400</b>) receives information (<b>495</b>) for a compressed sequence of video pictures and produces output including a reconstructed picture (<b>405</b>). Particular embodiments of video decoders typically use a variation or supplemented version of the generalized decoder (<b>400</b>).</p>
<p id="p-0067" num="0066">The decoder system (<b>400</b>) decompresses predicted pictures and key pictures. For the sake of presentation, <figref idref="DRAWINGS">FIG. 4</figref> shows a path for key pictures through the decoder system (<b>400</b>) and a path for forward-predicted pictures. Many of the components of the decoder system (<b>400</b>) are used for decompressing both key pictures and predicted pictures. The exact operations performed by those components can vary depending on the type of information being decompressed.</p>
<p id="p-0068" num="0067">A buffer (<b>490</b>) receives the information (<b>495</b>) for the compressed video sequence and makes the received information available to the entropy decoder (<b>480</b>). The buffer (<b>490</b>) typically receives the information at a rate that is fairly constant over time. Alternatively, the buffer (<b>490</b>) receives information at a varying rate. Before or after the buffer (<b>490</b>), the compressed video information can be channel decoded and processed for error detection and correction.</p>
<p id="p-0069" num="0068">The entropy decoder (<b>480</b>) entropy decodes entropy-coded quantized data as well as entropy-coded side information, typically applying the inverse of the entropy encoding performed in the encoder. Entropy decoding techniques include arithmetic decoding, differential decoding, Huffman decoding, run length decoding, LZ decoding, dictionary decoding, and combinations of the above. The entropy decoder (<b>480</b>) frequently uses different decoding techniques for different kinds of information, and can choose from among multiple code tables within a particular decoding technique.</p>
<p id="p-0070" num="0069">If the picture (<b>405</b>) to be reconstructed is a forward-predicted picture, a motion compensator (<b>430</b>) applies motion information (<b>415</b>) to a reference picture (<b>425</b>) to form a prediction (<b>435</b>) of the picture (<b>405</b>) being reconstructed. For example, the motion compensator (<b>430</b>) uses a macroblock motion vector to find a macroblock in the reference picture (<b>425</b>). A picture store (<b>420</b>) stores previous reconstructed pictures for use as reference pictures. Alternatively, a motion compensator applies another type of motion compensation. The prediction by the motion compensator (<b>430</b>) is rarely perfect, so the decoder (<b>400</b>) also reconstructs prediction residuals.</p>
<p id="p-0071" num="0070">An inverse quantizer (<b>470</b>) inverse quantizes entropy-decoded data. In general, the inverse quantizer (<b>470</b>) applies uniform, scalar inverse quantization to the entropy-decoded data with a step-size that varies on a picture-by-picture basis or other basis. Alternatively, the inverse quantizer (<b>470</b>) applies another type of inverse quantization to the data, for example, a non-uniform, vector, or non-adaptive inverse quantization, or directly inverse quantizes spatial domain data in a decoder system that does not use inverse frequency transformations.</p>
<p id="p-0072" num="0071">An inverse frequency transformer (<b>460</b>) converts quantized, frequency domain data into spatial domain video information. For block-based video pictures, the inverse frequency transformer (<b>460</b>) applies an inverse DCT [&#x201c;IDCT&#x201d;] or variant of IDCT to blocks of DCT coefficients, producing pixel data or prediction residual data for key pictures or predicted pictures, respectively. Alternatively, the inverse frequency transformer (<b>460</b>) applies another conventional inverse frequency transform such as an inverse Fourier transform or uses wavelet or subband synthesis. In some embodiments, the inverse frequency transformer (<b>460</b>) applies an 8&#xd7;8, 8&#xd7;4, 4&#xd7;8, or other size inverse frequency transform (e.g., IDCT) to prediction residuals for predicted pictures.</p>
<p id="p-0073" num="0072">When the decoder (<b>400</b>) needs a reconstructed picture for subsequent motion compensation, the picture store (<b>420</b>) buffers the reconstructed picture for use in the motion compensation. In some embodiments, the decoder (<b>400</b>) applies an in-loop de-blocking filter to the reconstructed picture to adaptively smooth discontinuities at block boundaries in the picture, for example, as described in U.S. patent application Ser. Nos. 10/322,383 and 10/623,128.</p>
<p id="p-0074" num="0073">The decoder (<b>400</b>) performs post-processing filtering such as de-blocking and/or de-ringing filtering. For example, the decoder performs the post-processing filtering as in the WMV8 system, WMV9 system, or other system described above.</p>
<p id="p-0075" num="0074">The decoder (<b>400</b>) receives (as part of the information (<b>495</b>)) control information for filtering operations. The control information affects operations such as post-processing de-blocking and/or de-ringing filtering, as described below. The decoder (<b>400</b>) receives the control information at an appropriate syntax level and passes the information to the appropriate filtering modules.</p>
<p id="h-0014" num="0000">III. Bitstream-Controlled Post-Processing Filtering</p>
<p id="p-0076" num="0075">In some embodiments, a video encoder allows a content author or other human operator to control the level of post-processing filtering for a particular sequence, scene, frame, or area within a frame. The operator specifies control information, which is put in the encoded bitstream. A decoder performs the post-processing filtering according to the control information. This lets the operator ensure that the post-processing enhances video quality when it is used, and that post-processing is disabled when it is not needed. For example, the operator controls post-processing filtering to prevent excessive blurring in reconstruction of high-definition, high bitrate video.</p>
<p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. 5</figref> is a generalized diagram of a system (<b>500</b>) with bitstream-controlled post-processing filtering. The details of the components, inputs, and outputs shown in <figref idref="DRAWINGS">FIG. 5</figref> vary depending on implementation.</p>
<p id="p-0078" num="0077">A video encoder (<b>510</b>) accepts source video (<b>505</b>), encodes it, and produces a video bitstream (<b>515</b>). For example, the video encoder (<b>510</b>) is an encoder such as the encoder (<b>300</b>) shown in <figref idref="DRAWINGS">FIG. 3</figref>. Alternatively, the system (<b>500</b>) includes a different video encoder (<b>510</b>).</p>
<p id="p-0079" num="0078">In addition to receiving the source video (<b>505</b>), the encoder (<b>510</b>) receives post-processing control information (<b>512</b>) that originates from input by a content author or other human operator. For example, the author provides the post-processing control information (<b>512</b>) directly to the encoder (<b>510</b>) or adjusts encoder settings for the post-processing filtering. Or, some other application receives input from the author, and that other application passes post-processing control information (<b>512</b>) to the encoder (<b>510</b>). Alternatively, instead of a human operator specifying the post-processing control information (<b>512</b>), the encoder (<b>510</b>) decides the control information (<b>512</b>) according to codec parameters or the results of video encoding. For example, the encoder (<b>510</b>) increases filter strength as the compression ratio applied increases (e.g., increasing filter strength for larger quantization step size, and vice versa; or, decreasing filter strength for greater encoded bits/pixels, and vice versa).</p>
<p id="p-0080" num="0079">The encoder (<b>510</b>) puts the post-processing control information (<b>512</b>) in the video bitstream (<b>515</b>). The encoder (<b>510</b>) formats the post-processing control information (<b>512</b>) as fixed length codes (such as 00 for level 0, 01 for level 1, 10 for level 2, etc.). Or, the encoder (<b>510</b>) uses a VLC/Huffman table to assign codes (such as 0 for level 0, 10 for level 1, 110 for level 2, etc.), or uses some other type of entropy encoding. The encoder (<b>510</b>) puts the control information (<b>512</b>) in a header at the appropriate syntax level of the video bitstream (<b>515</b>). For example, control information (<b>512</b>) for a picture is put in a picture header for the picture. For an MPEG-2 or MPEG-4 bitstream, the location in the header could be the private data section in the picture header.</p>
<p id="p-0081" num="0080">The video bitstream (<b>515</b>) is delivered via a channel (<b>520</b>), for example, by transmission as streaming media over a network. A video decoder (<b>530</b>) receives the video bitstream (<b>515</b>). The decoder (<b>530</b>) decodes the encoded video data, producing decoded video (<b>535</b>). The decoder (<b>530</b>) also retrieves the post-processing control information (<b>532</b>) (performing any necessary decoding) and passes the control information (<b>532</b>) to the post-processing filter (<b>540</b>).</p>
<p id="p-0082" num="0081">The post-processing filter (<b>540</b>) uses the control information (<b>532</b>) to apply the indicated post-processing filtering to the decoded video (<b>535</b>), producing decoded, post-processed video (<b>545</b>). The post-processing filter (<b>540</b>) is, for example, a de-ringing and/or de-blocking filter.</p>
<p id="p-0083" num="0082"><figref idref="DRAWINGS">FIG. 6</figref> shows a technique (<b>600</b>) for producing a bitstream with embedded control information for post-processing filtering. An encoder such as the encoder (<b>300</b>) shown in <figref idref="DRAWINGS">FIG. 3</figref> performs the technique (<b>600</b>).</p>
<p id="p-0084" num="0083">The encoder receives (<b>610</b>) video to be encoded and also receives (<b>630</b>) control information for post-processing filtering. The encoder encodes (<b>620</b>) the video and outputs (<b>640</b>) the encoded video and the control information. In one implementation, the encoder encodes (<b>620</b>) the video, decodes the video, and presents the results. The author then decides the appropriate post-processing strength, etc. for the control information. The decision-making process for post-processing strength and other control information may include actual post-processing in the encoder (following decoding of the encoded frame or other portion of the video), in which the encoder iterates through or otherwise evaluates different post-processing strengths, etc. until a decision is reached for the frame or other portion of the video.</p>
<p id="p-0085" num="0084">The technique (<b>600</b>) shown in <figref idref="DRAWINGS">FIG. 6</figref> may be repeated during encoding, for example, to embed control information on a scene-by-scene or frame-by-frame basis in the bitstream. More generally, depending on implementation, stages of the technique (<b>600</b>) can be added, split into multiple stages, combined with other stages, rearranged and/or replaced with like stages. In particular, the timing of the receipt (<b>630</b>) of the control information can vary depending on implementation.</p>
<p id="p-0086" num="0085"><figref idref="DRAWINGS">FIG. 7</figref> shows a technique (<b>700</b>) for performing bitstream-controlled post-processing filtering. A decoder such as the decoder (<b>400</b>) shown in <figref idref="DRAWINGS">FIG. 4</figref> performs the technique (<b>700</b>).</p>
<p id="p-0087" num="0086">The decoder receives (<b>710</b>) encoded video and control information for post-processing filtering. The decoder decodes (<b>720</b>) the video. The decoder then performs (<b>730</b>) post-processing filtering according to the control information. The technique (<b>700</b>) shown in <figref idref="DRAWINGS">FIG. 7</figref> may be repeated during decoding, for example, to retrieve and apply control information on a scene-by-scene or frame-by-frame basis. More generally, depending on implementation, stages of the technique (<b>700</b>) can be added, split into multiple stages, combined with other stages, rearranged and/or replaced with like stages.</p>
<p id="p-0088" num="0087">A. Types of Post-Processing Control Information</p>
<p id="p-0089" num="0088">There are several different possibilities for the content of the post-processing control information. The type of control information uses depends on implementation. The simplest type represents an ON/OFF decision for post-processing filtering.</p>
<p id="p-0090" num="0089">Another type of control information indicates a post-processing level (i.e., strength) of de-blocking, de-ringing, and/or other filtering. Bitstream-controlled post-processing is particularly useful when the control information represents the maximum allowed post-processing level. For example, suppose a higher level indicates stronger filtering. If the author indicates post-processing level 3 for a given video frame, the decoder may apply post-processing of level 0, 1, 2, or 3 to the given frame, but not 4 or higher. This approach provides some flexibility. If a software decoder does not have enough CPU cycles available to apply level 3 for the frame, it may only apply level 2, etc. At the same time, using a maximum allowed level achieves the main goal&#x2014;ensuring that the video will never be excessively blurred by post-processing. A smart author sets the maximum allowed level to 0 (i.e., no post-processing) or a low level when the decoded video is already of high quality, and sets a higher level when the decoded video presents more blocking and ringing artifacts.</p>
<p id="p-0091" num="0090">Alternatively, instead of maximum allowed levels, the control information represents exact levels. Such control information specifies a mandatory level of post-processing filtering, which is useful when the author wants to control post-processing exactly. Or, the control information represents minimum allowed levels. This is useful when the author wants to guarantee that at least a minimum level of post-processing filtering is applied, for example, for very low bitrate video.</p>
<p id="p-0092" num="0091">Still another type of control information represents filter type selections, instead of or in addition to filter level information for one particular filter or filters. For example, value 0 indicates no post-processing, value 1 indicates de-blocking, value 2 indicates de-ringing, value 3 indicates both de-blocking and de-ringing, etc.</p>
<p id="p-0093" num="0092">The control information alternatively includes other and/or additional types of information.</p>
<p id="p-0094" num="0093">B. Syntax Levels for Control Information</p>
<p id="p-0095" num="0094">Depending on implementation, control information is specified for a sequence, on a scene-by-scene basis within a sequence, on a frame-by-frame basis, on a region-by-region basis, or on some other basis. This allows the author to review reconstructed video and adapt the post-processing for a given sequence, scene, frame, region within a frame, etc., depending on the syntax level(s) at which control is enabled. Similarly, the bitstream includes syntax elements for control information at the appropriate syntax level(s) for sequence, scene, frame, region within a frame, etc.</p>
<p id="p-0096" num="0095">To specify control information for a region within a frame, the author may define an area such as a rectangle or ellipse, for example, and the parameters for the size and location of the area are put in the bitstream. For a rectangle, the area is definable by the sides (a, b) and top-left corner pixel location (x, y), coded using fixed length or variable length codes. The post-processing strength for the area is also put in the bitstream. Alternatively, another syntax is used to specify control information for different regions within a frame for post-processing filtering.</p>
<p id="h-0015" num="0000">IV. Extensions</p>
<p id="p-0097" num="0096">In one or more embodiments, an operator specifies control information for in-loop filtering for a sequence, on a scene-by-scene basis, on a frame-by-frame basis, on a region-by-region basis, or on some other basis. The control information includes levels (i.e., strengths) of filters, types of filters (e.g., to select from among multiple available filters), and/or other types of information.</p>
<p id="p-0098" num="0097">Having described and illustrated the principles of our invention with reference to various embodiments, it will be recognized that the various embodiments can be modified in arrangement and detail without departing from such principles. It should be understood that the programs, processes, or methods described herein are not related or limited to any particular type of computing environment, unless indicated otherwise. Various types of general purpose or specialized computing environments may be used with or perform operations in accordance with the teachings described herein. Elements of embodiments shown in software may be implemented in hardware and vice versa.</p>
<p id="p-0099" num="0098">In view of the many possible embodiments to which the principles of our invention may be applied, we claim as our invention all such embodiments as may come within the scope and spirit of the following claims and equivalents thereto.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>We claim:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. In a computer system, a computer-implemented method comprising:
<claim-text>receiving video data;</claim-text>
<claim-text>encoding the video data, wherein the encoding includes in-loop deblock filtering, and wherein the encoded video data indicates modes to be used during decoding;</claim-text>
<claim-text>parameterizing post-processing control information other than the encoded video data, including:
<claim-text>setting a filter type selection that identifies one or more of plural different types of filters for post-processing filtering, wherein the post-processing control information other than the encoded video data includes the filter type selection; and</claim-text>
<claim-text>setting filter information that varies depending on quality or bitrate of the encoded video data, wherein the post-processing control information other than the encoded video data further includes the filter information, and wherein the filter information is different than the filter type selection;</claim-text>
<claim-text>wherein the post-processing control information facilitates adjustment by a decoder of the post-processing filtering of the video data after decoding of the encoded video data, the adjustment including:
<claim-text>using the filter type selection to identify the one or more types of filters; and</claim-text>
<claim-text>using the filter information to determine filter operations that are appropriate for the quality or bitrate of the encoded video data, wherein the filter operations are for the identified one or more types of filters, and wherein, within settings defined by the post-processing control information for the post-processing filtering, the decoder can adjust the filter operations depending on processing cycles available to the decoder; and</claim-text>
</claim-text>
</claim-text>
<claim-text>outputting the encoded video data as well as the post-processing control information.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising:
<claim-text>receiving input that indicates the post-processing control information.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref> wherein a video encoder receives the input.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref> wherein an application receives the input and provides the post-processing control information to a video encoder.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein a video encoder specifies the post-processing control information depending on one or more criteria, and wherein the one or more criteria include quantization step size and/or quality of the video data.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the post-processing filtering to be performed includes applying a de-blocking filter.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the post-processing filtering to be performed includes applying a de-ringing filter.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the filter information is parameterized as a level for the post-processing filtering.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the filter information is parameterized as a maximum allowed level for the post-processing filtering.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the filter information is parameterized as a minimum allowed level for the post-processing filtering.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising entropy encoding the post-processing control information.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the adjustment of post-processing filtering to be performed comprises skipping the post-processing filtering for at least some of the video data.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the post-processing control information specifies that the decoder should skip the post-processing filtering.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the post-processing control information further comprises:
<claim-text>information indicating whether or not the post-processing filtering is to be performed after decoding; and</claim-text>
<claim-text>for each of plural pictures represented in the video data, if the post-processing filtering is to be performed for the picture, information indicating the filter type selection for the post-processing filtering for the picture.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein, when the filter information indicates lower quality or bitrate, the filter information tends to enable the post-processing filtering, and wherein, when the filter information indicates higher quality or bitrate, the filter information tends to disable the post-processing filtering.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. In a computing device that implements a video encoder, a method comprising:
<claim-text>at the computing device that implements the video encoder, receiving video data;</claim-text>
<claim-text>with the computing device that implements the video encoder, encoding the video data, wherein the encoding includes in-loop deblock filtering, and wherein the encoded video data indicates modes to be used during decoding;</claim-text>
<claim-text>with the computing device that implements the video encoder, parameterizing post-processing control information other than the encoded video data, including:
<claim-text>setting a filter type selection that identifies one or more of plural different types of filters for post-processing filtering, wherein the post-processing control information other than the encoded video data includes the filter type selection; and</claim-text>
<claim-text>setting filter information that varies depending on quality or bitrate of the encoded video data, wherein the post-processing control information other than the encoded video data further includes the filter information, and wherein the filter information is different than the filter type selection;</claim-text>
<claim-text>wherein the post-processing control information facilitates adjustment by a decoder of the post-processing filtering of the video data after decoding, the adjustment including using the filter information to determine filter operations that are appropriate for the quality or bitrate of the encoded video data, wherein the filter operations are for the one or more types of filters identified with the filter type selection, wherein, within settings defined by the post-processing control information for the post-processing filtering, the decoder can adjust the filter operations depending on processing power available to the decoder, wherein, when the filter information indicates lower quality or bitrate, the filter information tends to enable the post-processing filtering, and wherein, when the filter information indicates higher quality or bitrate, the filter information tends to disable the post-processing filtering; and</claim-text>
</claim-text>
<claim-text>with the computing device that implements the video encoder, outputting the encoded video data as well as the post-processing control information.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref> wherein a video encoder specifies the post-processing control information depending on one or more criteria, and wherein the one or more criteria include quantization step size and/or quality of the video data.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref> wherein the post-processing filtering to be performed includes applying a de-blocking filter or a de-ringing filter.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref> wherein the filter information is parameterized as a level for the post-processing filtering.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref> wherein the filter information is parameterized as a maximum allowed level or a minimum allowed level for the post-processing filtering.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref> wherein the post-processing control information further comprises:
<claim-text>information indicating whether or not the post-processing filtering is to be performed after decoding; and</claim-text>
<claim-text>for each of plural pictures represented in the video data, if the post-processing filtering is to be performed for the picture, information indicating the filter type selection for the post-processing filtering for the picture.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. A computer-implemented method comprising:
<claim-text>receiving encoded video data in a bitstream as well as post-processing control information in the bitstream for controlling post-processing filtering, the encoded video data indicating modes to be used during decoding, and the post-processing control information comprising data in the bitstream other than the encoded video data, wherein the post-processing control information includes:
<claim-text>a filter type selection that identifies one or more of plural different types of filters for the post-processing filtering; and</claim-text>
<claim-text>filter information that varies depending on quality or bitrate of the encoded video data, wherein the filter information is different than the filter type selection;</claim-text>
</claim-text>
<claim-text>decoding the encoded video data, wherein the decoding includes in-loop deblock filtering;</claim-text>
<claim-text>determining available processing cycles; and</claim-text>
<claim-text>adjusting the post-processing filtering on the decoded video data, the adjustment including using the filter information to determine filter operations that are appropriate for the quality or bitrate of the encoded video data, wherein the filter operations are for the one or more types of filters identified with the filter type selection, and wherein, within settings defined by the post-processing control information for the post-processing filtering, the decoder can adjust the filter operations depending on the processing cycles available to the decoder.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The method of <claim-ref idref="CLM-00022">claim 22</claim-ref> wherein the post-processing filtering includes applying a de-blocking filter.</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. The method of <claim-ref idref="CLM-00022">claim 22</claim-ref> wherein the post-processing filtering includes applying a de-ringing filter.</claim-text>
</claim>
<claim id="CLM-00025" num="00025">
<claim-text>25. The method of <claim-ref idref="CLM-00022">claim 22</claim-ref> wherein the filter information is parameterized as a level for the post-processing filtering.</claim-text>
</claim>
<claim id="CLM-00026" num="00026">
<claim-text>26. The method of <claim-ref idref="CLM-00022">claim 22</claim-ref> wherein the filter information is parameterized as a maximum allowed level for the post-processing filtering.</claim-text>
</claim>
<claim id="CLM-00027" num="00027">
<claim-text>27. The method of <claim-ref idref="CLM-00022">claim 22</claim-ref> wherein the filter information is parameterized as a minimum allowed level for the post-processing filtering.</claim-text>
</claim>
<claim id="CLM-00028" num="00028">
<claim-text>28. The method of <claim-ref idref="CLM-00022">claim 22</claim-ref> further comprising entropy decoding the post-processing control information.</claim-text>
</claim>
<claim id="CLM-00029" num="00029">
<claim-text>29. The method of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein determining available processing cycles comprises determining available CPU cycles for applying post-processing filtering.</claim-text>
</claim>
<claim id="CLM-00030" num="00030">
<claim-text>30. The method of <claim-ref idref="CLM-00022">claim 22</claim-ref>, further comprising performing the post-processing filtering.</claim-text>
</claim>
<claim id="CLM-00031" num="00031">
<claim-text>31. The method of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein the post-processing control information further comprises:
<claim-text>information indicating whether or not the post-processing filtering is to be performed after decoding; and</claim-text>
<claim-text>for each of plural pictures represented in the video data, if the post-processing filtering is to be performed for the picture, information indicating the filter type selection for the post-processing filtering for the picture;</claim-text>
<claim-text>the method further comprising parsing the post-processing control information.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00032" num="00032">
<claim-text>32. The method of <claim-ref idref="CLM-00031">claim 31</claim-ref>, wherein the parsing the post-processing control information comprises parsing the information indicating whether or not post-processing filtering is to be performed.</claim-text>
</claim>
<claim id="CLM-00033" num="00033">
<claim-text>33. The method of <claim-ref idref="CLM-00032">claim 32</claim-ref>, wherein the parsing the post-processing control information further comprises, for each of the plural pictures represented in the video data, if the post-processing filtering is to be performed for the picture, parsing the information indicating the filter type selection for the post-processing filtering for the picture.</claim-text>
</claim>
<claim id="CLM-00034" num="00034">
<claim-text>34. The method of <claim-ref idref="CLM-00022">claim 22</claim-ref> wherein, when the filter information indicates lower quality or bitrate, the filter information tends to enable the post-processing filtering, and wherein, when the filter information indicates higher quality or bitrate, the filter information tends to disable the post-processing filtering.</claim-text>
</claim>
<claim id="CLM-00035" num="00035">
<claim-text>35. A computer-readable medium storing computer-executable instructions for causing a computer system to perform a computer-implemented method, the computer-readable medium including one or more of non-volatile memory, a magnetic storage medium and an optical storage medium, the method comprising:
<claim-text>receiving encoded video data in a bitstream as well as post-processing control information in the bitstream for controlling post-processing filtering, wherein the encoded video data indicates modes to be used during decoding, the post-processing control information comprising data in the bitstream other than the encoded video data, wherein the post-processing control information includes:</claim-text>
<claim-text>a filter type selection that identifies one or more of plural different types of filters for the post-processing filtering; and</claim-text>
<claim-text>filter information that varies depending on quality or bitrate of the encoded video data, wherein the filter information is different than the filter type selection;</claim-text>
<claim-text>decoding the encoded video data, wherein the decoding includes in-loop deblock filtering;</claim-text>
<claim-text>using the filter type selection to select one or more of plural different types of filters for post-processing filtering;</claim-text>
<claim-text>determining available processing power; and adjusting the post-processing filtering on the decoded video data, including using the filter information to determine filter operations that are appropriate for the quality or bitrate of the encoded video data, wherein the filter operations are for the selected one or more types of filters, wherein, within settings defined by the post-processing control information for the post-processing filtering, the decoder can adjust the filter operations depending on the processing power available to the decoder, wherein, when the filter information indicates lower quality or bitrate, the filter information tends to enable the post-processing filtering, and wherein, when the filter information indicates higher quality or bitrate, the filter information tends to disable the post-processing filtering.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00036" num="00036">
<claim-text>36. The computer-readable media of <claim-ref idref="CLM-00035">claim 35</claim-ref> wherein the post-processing filtering includes applying a de-blocking filter or a de-ringing filter.</claim-text>
</claim>
<claim id="CLM-00037" num="00037">
<claim-text>37. The computer-readable media of <claim-ref idref="CLM-00035">claim 35</claim-ref> wherein the filter information is parameterized as a level for the post-processing filtering.</claim-text>
</claim>
<claim id="CLM-00038" num="00038">
<claim-text>38. The computer-readable media of <claim-ref idref="CLM-00035">claim 35</claim-ref> wherein the filter information is parameterized as a maximum allowed level or a minimum allowed level for the post-processing filtering.</claim-text>
</claim>
<claim id="CLM-00039" num="00039">
<claim-text>39. The computer-readable media of <claim-ref idref="CLM-00035">claim 35</claim-ref> wherein determining available processing power comprises determining available CPU cycles for applying post-processing filtering.</claim-text>
</claim>
</claims>
</us-patent-grant>
