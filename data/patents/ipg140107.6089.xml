<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08627213-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08627213</doc-number>
<kind>B1</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>10915939</doc-number>
<date>20040810</date>
</document-id>
</application-reference>
<us-application-series-code>10</us-application-series-code>
<us-term-of-grant>
<us-term-extension>1417</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>3</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20130101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>3</main-group>
<subgroup>048</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>15</main-group>
<subgroup>16</subgroup>
<symbol-position>L</symbol-position>
<classification-value>N</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20110101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>T</subclass>
<main-group>15</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>N</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>T</subclass>
<main-group>17</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>N</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>715758</main-classification>
<further-classification>715753</further-classification>
<further-classification>715757</further-classification>
<further-classification>715848</further-classification>
<further-classification>345419</further-classification>
<further-classification>345428</further-classification>
<further-classification>709204</further-classification>
<further-classification>709205</further-classification>
<further-classification>709206</further-classification>
<further-classification>709207</further-classification>
</classification-national>
<invention-title id="d2e53">Chat room system to provide binaural sound at a user location</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5664021</doc-number>
<kind>A</kind>
<name>Chu et al.</name>
<date>19970900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>381 92</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5736982</doc-number>
<kind>A</kind>
<name>Suzuki et al.</name>
<date>19980400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715706</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5889843</doc-number>
<kind>A</kind>
<name>Singer et al.</name>
<date>19990300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37920201</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5999208</doc-number>
<kind>A</kind>
<name>McNerney et al.</name>
<date>19991200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348 1408</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6028608</doc-number>
<kind>A</kind>
<name>Jenkins</name>
<date>20000200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345619</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6041343</doc-number>
<kind>A</kind>
<name>Nguyen et al.</name>
<date>20000300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709203</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6111957</doc-number>
<kind>A</kind>
<name>Thomasson</name>
<date>20000800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>381 15</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6188769</doc-number>
<kind>B1</kind>
<name>Jot et al.</name>
<date>20010200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>381 63</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6195099</doc-number>
<kind>B1</kind>
<name>Gardiner</name>
<date>20010200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345426</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6327567</doc-number>
<kind>B1</kind>
<name>Willehadson et al.</name>
<date>20011200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704270</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6453336</doc-number>
<kind>B1</kind>
<name>Beyda et al.</name>
<date>20020900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709204</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6559863</doc-number>
<kind>B1</kind>
<name>Megiddo</name>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715753</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>6577738</doc-number>
<kind>B2</kind>
<name>Norris et al.</name>
<date>20030600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>381 77</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>6767287</doc-number>
<kind>B1</kind>
<name>Mcquaid et al.</name>
<date>20040700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>463 42</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>6772195</doc-number>
<kind>B1</kind>
<name>Hatlelid et al.</name>
<date>20040800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709204</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>6850496</doc-number>
<kind>B1</kind>
<name>Knappe et al.</name>
<date>20050200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>370260</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>6856802</doc-number>
<kind>B1</kind>
<name>Kinnunen et al.</name>
<date>20050200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>455425</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>7124372</doc-number>
<kind>B2</kind>
<name>Brin</name>
<date>20061000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715751</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>7310802</doc-number>
<kind>B2</kind>
<name>Dani et al.</name>
<date>20071200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>718105</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>7587512</doc-number>
<kind>B2</kind>
<name>Ta et al.</name>
<date>20090900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709233</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>2001/0055398</doc-number>
<kind>A1</kind>
<name>Pachet et al.</name>
<date>20011200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>2002/0112004</doc-number>
<kind>A1</kind>
<name>Reid et al.</name>
<date>20020800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709205</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>2003/0026207</doc-number>
<kind>A1</kind>
<name>Loguinov</name>
<date>20030200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>370235</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>2003/0067877</doc-number>
<kind>A1</kind>
<name>Sivakumar et al.</name>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>370232</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>2003/0107478</doc-number>
<kind>A1</kind>
<name>Hendricks et al.</name>
<date>20030600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3403112</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>2003/0236835</doc-number>
<kind>A1</kind>
<name>Levi et al.</name>
<date>20031200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709204</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>2004/0111479</doc-number>
<kind>A1</kind>
<name>Borden et al.</name>
<date>20040600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>2004/0150646</doc-number>
<kind>A1</kind>
<name>Oka</name>
<date>20040800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345502</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>2005/0033806</doc-number>
<kind>A1</kind>
<name>Harvey et al.</name>
<date>20050200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709204</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>2005/0088981</doc-number>
<kind>A1</kind>
<name>Woodruff et al.</name>
<date>20050400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>370260</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>US</country>
<doc-number>2005/0157891</doc-number>
<kind>A1</kind>
<name>Johansen</name>
<date>20050700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>381103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>US</country>
<doc-number>2006/0045274</doc-number>
<kind>A1</kind>
<name>Aarts et al.</name>
<date>20060300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>381 17</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00033">
<document-id>
<country>US</country>
<doc-number>2006/0056638</doc-number>
<kind>A1</kind>
<name>Schobben</name>
<date>20060300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>381 17</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00034">
<document-id>
<country>US</country>
<doc-number>2006/0212147</doc-number>
<kind>A1</kind>
<name>McGrath et al.</name>
<date>20060900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>700 94</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00035">
<document-id>
<country>US</country>
<doc-number>2007/0294387</doc-number>
<kind>A1</kind>
<name>Martin</name>
<date>20071200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709224</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00036">
<document-id>
<country>US</country>
<doc-number>2011/0258507</doc-number>
<kind>A1</kind>
<name>Rideout</name>
<date>20111000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>714746</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00037">
<document-id>
<country>WO</country>
<doc-number>WO99/62189</doc-number>
<kind>A2</kind>
<date>19991200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<patcit num="00038">
<document-id>
<country>WO</country>
<doc-number>WO02/35844</doc-number>
<kind>A2</kind>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00039">
<othercit>Fernanda B. Viegas and Judith S. Donath, Chat Circles, 15-2 -May 1999, MIT Media Lab.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00040">
<othercit>Paul M. Aoki, Matthew Romaine, Margaret H. Szymanski, James D. Thornton, Daniel Wilson, and Allison Woodruff, The Mad Hatter's Cocktail Party: A Social Mobile Audio Space Supporting Multiple Simultaneous Conversations, Apr. 5-10, 2003, ACM.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00041">
<othercit>3D Audio Rendering and Evaluation Guidelines, Jun. 9, 1998, MIDI Manufactures Association.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00042">
<othercit>Thomas McGuire, Max Payne tweak guide, Aug. 22, 2001, 9 pages.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00043">
<othercit>Interactive 3D Audio Rendering Guidelines Level 2.0, Sep. 20, 1999, MIDI Manufactures Association, 29 pages.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00044">
<othercit>Thomas Funkhouser, Jean-Marc Jot, Nicolas Tsingos, &#x201c;Sounds Good to Me!&#x201d;, 2002, 43 pages.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00045">
<othercit>Nicolas Tsingos, Emmanuel Gallo and George Drettakis, Perceptual Audio Rendering of Complex Virtual Environments, Jul. 19, 2004, 11 pages.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00046">
<othercit>Thomas Funkhouser, Patrick Min, Ingrid Carlbom, Real-Time Acoustic Modeling for Distributed Virtual Environments, 1999, 11 pages.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00047">
<othercit>Steve DiPaola, David Collins, A 3D Virtual Environment for Social Telepresence, Mar. 24-27, 2002, 6 pages.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00048">
<othercit>J. H. Rindel, The Use of Computer Modeling in Room Acoustics, 2000, 6 pages.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00049">
<othercit>Malcolm J. Crocker, Handbook of Acoustics, Feb. 23, 1998, 2 pages.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00050">
<othercit>A.C.C Warnock, Reverberant Noise Control in Rooms Using Sound Absorbing Materials, Jun. 1980, 8 pages.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00051">
<othercit>Kyungmin Lee, Dongman Lee, A Scalable Dynamic Load Distribution Scheme for Multi-Server Distributed Virtual Environment Systems With Highly-Skewed User Distribution, 2003, 9 pages.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00052">
<othercit>Roger Zimmermann, Beomjoo Seo, Leslie S. Liu, Rahul S. Hampole and Brent Nash, AudioPeer: A Collaborative Distributed Audio Chat System, 2004, 7 pages.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
</us-references-cited>
<number-of-claims>22</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>715753</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>715757</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>715758</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>715848</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>709204-207</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>3</number-of-drawing-sheets>
<number-of-figures>3</number-of-figures>
</figures>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Jouppi</last-name>
<first-name>Norman Paul</first-name>
<address>
<city>Palo Alto</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Iyer</last-name>
<first-name>Subramoniam N.</first-name>
<address>
<city>Fremont</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Jouppi</last-name>
<first-name>Norman Paul</first-name>
<address>
<city>Palo Alto</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Iyer</last-name>
<first-name>Subramoniam N.</first-name>
<address>
<city>Fremont</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Hewlett-Packard Development Company, L.P.</orgname>
<role>02</role>
<address>
<city>Houston</city>
<state>TX</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Kim</last-name>
<first-name>Matt</first-name>
<department>2171</department>
</primary-examiner>
<assistant-examiner>
<last-name>Pan</last-name>
<first-name>Yongjia</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A chat room system is provided that includes sending a signal representative of a sound at a first user location to a second user location and establishing, in a chat room, a virtual first user location and a virtual second user location. It further includes establishing the orientation of a listening system at the second user location and processing the signal, using the orientation of the listening system, at a plurality of levels of realism to provide a binaural sound at the second user location having an analogous spatial relationship between the second user location and the first user location as the virtual spatial relationship between the virtual second user location and the virtual first user location.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="209.89mm" wi="183.13mm" file="US08627213-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="209.89mm" wi="170.26mm" orientation="landscape" file="US08627213-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="218.69mm" wi="184.32mm" file="US08627213-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="192.96mm" wi="136.31mm" file="US08627213-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND</heading>
<p id="p-0002" num="0001">1. Field of the Invention</p>
<p id="p-0003" num="0002">The present invention relates generally to chat room systems.</p>
<p id="p-0004" num="0003">2. Background of the Invention</p>
<p id="p-0005" num="0004">With the increasing use of digital subscriber lines (DSL), satellite and cable modems, dial-up modems, and the 802.11(a), (b), and (g) &#x201c;wi-fi&#x201d; devices, Internet access via &#x201c;web pages&#x201d; found on the World Wide Web, or the &#x201c;web&#x201d; has been constantly increasing. The speed of data transmission is such that rapid text, audio, and video communications are now possible.</p>
<p id="p-0006" num="0005">One popular communication application supported by the Internet and available on the web is the &#x201c;chat room&#x201d;. A chat room is basically an electronic bulletin board where &#x201c;posts&#x201d; or messages for anyone accessing or &#x201c;in&#x201d; the chat room are communicated substantially in real time. Thus, in a chat room, participants interested in a particular topic (often reflected in the name of the chat room) can electronically &#x201c;chat&#x201d; with other users.</p>
<p id="p-0007" num="0006">Current chat room audio systems are based on a single channel with a speakerphone-like model. People are intelligible to others just if one person at a time is speaking. This is unlike a real room where multiple people can be speaking simultaneously and still be understood. There are a couple of reasons for this and resulting disadvantages and problems. First, current telephone technology provides just a single channel. However, it is well known that by using binaural (i.e., utilizing both ears) audio, up to a 13 dB increase in effective signal-to-noise may be obtained through human perceptual abilities such as the &#x201c;Cocktail Party Effect&#x201d;. At a cocktail party, people with normal hearing can use both their ears and brain to selectively attend to and participate in multiple simultaneous conversations. In contrast, when multiple speakers are speaking over a single audio channel, the result is usually completely unintelligible (i.e., unless one speaker is much louder than the others).</p>
<p id="p-0008" num="0007">Second, conference phones often resort to half-duplex modes to reduce echoes. This can make taking turns in conversations awkward if participants from multiple locations start speaking at about the same time.</p>
<heading id="h-0002" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0009" num="0008">The present invention provides a chat room system that includes sending a signal representative of a sound at a first user location to a second user location and establishing, in a chat room, a virtual first user location and a virtual second user location. It further includes establishing the orientation of a listening system at the second user location and processing the signal, using the orientation of the listening system, at a plurality of levels of realism to provide a binaural sound at the second user location having an analogous spatial relationship between the second user location and the first user location as the virtual spatial relationship between the virtual second user location and the virtual first user location.</p>
<p id="p-0010" num="0009">Certain embodiments of the invention have other advantages in addition to or in place of those mentioned above. The advantages will become apparent to those skilled in the art from a reading of the following detailed description when taken with reference to the accompanying drawings.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 1</figref> a view of a chat room spatial audio system according to an embodiment of the present invention;</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 2</figref> is a personal computer of the chat room spatial audio system; and</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 3</figref> is a system providing chat room spatial audio according to an embodiment of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DESCRIPTION OF THE PREFERRED EMBODIMENT</heading>
<p id="p-0014" num="0013">In the following description, numerous specific details are given to provide a thorough understanding of the invention. However, it will be apparent that the invention may be practiced without these specific details. In order to avoid obscuring the present invention, some well-known systems are not disclosed in detail.</p>
<p id="p-0015" num="0014">Referring now to <figref idref="DRAWINGS">FIG. 1</figref>, therein is shown a chat room spatial audio system <b>100</b>. The chat room spatial audio system <b>100</b> includes one or more computer systems, such as Internet host servers <b>102</b>-<b>1</b> through N. The Internet host servers <b>102</b>-<b>1</b> through N are connected to a communications cloud <b>104</b>. The communications cloud <b>104</b> represents various communication systems and networks including DSL, satellite and cable modems, dial-up modems, and wi-fi Internet connections, etc. A number of computer systems, such as personal computers <b>106</b>-<b>1</b> through N, are connected to the communications cloud <b>104</b>.</p>
<p id="p-0016" num="0015">Each of the personal computers <b>106</b>-<b>1</b> through N has respective graphical user interfaces (GUIs) <b>108</b>-<b>1</b> through N, and control systems <b>110</b>-<b>1</b> through N, such as keyboards, mice, joysticks, etc. Each of the personal computers <b>106</b>-<b>1</b> through N is for a user <b>112</b>-<b>1</b> through N, respectively. Each of the users <b>112</b>-<b>1</b> through N is respectively provided with listening systems <b>114</b>-<b>1</b> through N at user locations <b>115</b>-<b>1</b> through N.</p>
<p id="p-0017" num="0016">The listening systems <b>114</b>-<b>1</b> through N could each be multiple loud speakers and can allowing listening to chat room talk in a home entertainment room, but in one embodiment the listening systems <b>114</b>-<b>1</b> through N include a communication system that uses inexpensive, light weight headsets, which are provided with audio headphones and sending systems, such as noise-canceling boom microphones <b>116</b>-<b>1</b> through N, respectively.</p>
<p id="p-0018" num="0017">The listening systems <b>114</b>-<b>1</b> through N are provided with tracking systems <b>120</b>-<b>1</b> through N, respectively, which establish orientation, such as azimuth, and optionally elevation and/or roll, of the listening systems <b>114</b>-<b>1</b> through N, respectively.</p>
<p id="p-0019" num="0018">In the chat room spatial audio system <b>100</b>, geographically distributed users <b>112</b>-<b>1</b> through N verbally communicate with each other much as if they were in a shared room. Each user <b>112</b>-<b>1</b> through N respectively wears a listening system <b>114</b>-<b>1</b> through N, which is connected to the personal computers <b>106</b>-<b>1</b> through N, respectively.</p>
<p id="p-0020" num="0019">The voice of each speaking user <b>112</b>-<b>1</b> through N is sent through the communications cloud <b>104</b> to at least one of the Internet host servers <b>102</b>-<b>1</b> through N, which simulates, with variable degrees of realism, the sound field that would occur if all the users <b>112</b>-<b>1</b> through N were actually in a single shared room; i.e., in a virtual chat room (referred to as a &#x201c;chat room&#x201d;).</p>
<p id="p-0021" num="0020">In embodiments of the present invention, it has been discovered that the Internet host servers <b>102</b>-<b>1</b> through N can be programmed to operate with a number of levels of realism as far as the subjective reality of the users <b>112</b>-<b>1</b> through N are concerned. Each increased level of realism requires more computational resources from the Internet host servers <b>102</b>-<b>1</b> through N, but generally does not require more bandwidth between the users <b>112</b>-<b>1</b> through N and the Internet host servers <b>102</b>-<b>1</b> through N.</p>
<p id="p-0022" num="0021">In another embodiment, where the Internet host server <b>102</b>-<b>1</b> is being used, it may choose its level of realism based on the number of active users in the chat room and the resulting load. For example, the fewer the speaking users, the more accurately the sound field in the chat room will be simulated, and vice versa.</p>
<p id="p-0023" num="0022">It has been discovered that three levels of realism singularly or in combination provide an excellent method to balance between maximum realism and optimal use of computer resources.</p>
<p id="p-0024" num="0023">In a first level of realism, the signal to a listening system would be processed using a frequency-independent sound model; i.e., frequencies in the model are attenuated or delayed by the same amount. A number of different computation methods may be used singularly or in combination in the frequency-independent sound model to provide a binaural listening experience that conveys to the listening user the perception of actual physical presence in the chat room.</p>
<p id="p-0025" num="0024">One method uses interaural time delay (ITD), which gives the time difference between a user's ears as a function of the angle of the user's head to the sound source, such as the user <b>112</b>-<b>1</b> to the user <b>112</b>-<b>2</b> who is virtually positioned to one side. It can be either calculated from approximation or found from a precalculated look-up table. The angle of the user's head would be established using the tracking system <b>120</b>-<b>1</b> and/or the control systems <b>110</b>-<b>1</b></p>
<p id="p-0026" num="0025">A second method uses interaural level difference (ILD), which using the level difference based on the angle (usually just azimuth, but also possibly elevation) between a user's head and the sound source, such as the user <b>112</b>-<b>1</b> virtually standing listening to the user <b>112</b>-<b>2</b> who is virtually seated. The ILD can be computed from a simple approximation.</p>
<p id="p-0027" num="0026">A third method uses angle-dependent output level (ADOL) of the user's voice calculated to represent the change in perceived volume of second user turning his/her head. For example, this reproduces to the user <b>112</b>-<b>1</b>, a user <b>112</b>-<b>2</b> who is virtually directly speaking towards the user <b>112</b>-<b>1</b> as being louder than the user <b>112</b>-<b>3</b>, who is virtually speaking directly away from the user <b>112</b>-<b>1</b>. The ADOL is calculated from a simple approximation based on the user's mouth angle relative to the virtual direction to the user <b>112</b>-<b>1</b>.</p>
<p id="p-0028" num="0027">A fourth method uses distance level dependence (DLD), which can be calculated by modeling the spread of acoustic energy with distance resulting in virtual near users <b>112</b>-<b>2</b> and <b>3</b> being perceived by the user <b>112</b>-<b>1</b> as louder than virtually distant users <b>112</b>-<b>4</b> and N. The spread of acoustic energy generally results in a 1/r<sup>2 </sup>level dependence in the near field and a less steep falloff with distance such as 1/r<sup>3/2 </sup>in enclosed spaces with floors and ceilings at distances that are large compared to the floor to ceiling spacing. In these further field situations, some of the sound energy reflects off the floors and ceilings (as well as some being absorbed) and thus can contribute to the sound perceived by the user <b>112</b>-<b>1</b>. The actual exponent in the enclosed far field case is a function of the chat room acoustics. The room acoustics could also be a function of the number of users <b>112</b>-<b>1</b> through N in the chat room.</p>
<p id="p-0029" num="0028">The perceptual shortcomings of the first level are that there may be front/back confusion with sounds generally appearing to be coming from the listener's left or right ear or somewhere between, so the sounds appear internal to the listener's head. In contrast, at higher levels of realism there is usually good externalization, except when they are close to the median plane. Also, a horizontal plane is used instead of head elevation so it is not possible to determine elevational differences in the sound.</p>
<p id="p-0030" num="0029">An even simpler level of realism is possible by just taking the sum of contributions of the nearest N speakers to the listener.</p>
<p id="p-0031" num="0030">The advantage of the first level is that the relative computational demands are low. For example, the largest number of participants in the chat room could be handled at this base level of computational demand.</p>
<p id="p-0032" num="0031">In a second level of realism, the signal to a listening system would be processed using a frequency-independent sound model and a frequency-dependent sound model; i.e., frequencies in the model are attenuated or delayed according to the frequency. A number of different computation methods may be used singularly or in combination in the frequency-dependent sound model to provide a binaural listening experience that conveys to the listening user an even more realistic perception of actual physical presence in the chat room than the first level of realism.</p>
<p id="p-0033" num="0032">One method uses head-related transfer functions (HRTFs), which are transfer functions which vary with both azimuth and elevation angle of the sound incident on the user's ears. These functions are dependent on the external structure (pinnae) of the user, and each person has a unique ear structure. However, if users are given the ability to select between several different HRTFs, they can usually find one that works quite well, especially if the HRTFs can be tuned with fixed elevation offset angles. This fixed elevation offset could be input by adjusting a thumbwheel either on a joystick or in a thumbwheel-like control as part of the control systems <b>110</b>-<b>1</b> through N.</p>
<p id="p-0034" num="0033">The HRTF calculations use discrete convolution (as used in digital signal processing) where for each output a set of contiguous input waveform samples are each multiplied by different filter coefficients and summed. Lesser levels of realism are possible by reducing the accuracy of the convolutions.</p>
<p id="p-0035" num="0034">A second method uses human voice transfer functions (HVTFs), which are transfer functions which vary with both azimuth and elevation angle of a speaker's voice. People communicate more easily when facing each other because speech is not radiated uniformly outwardly. More speech energy is radiated forward than to the sides or rear.</p>
<p id="p-0036" num="0035">A third method uses distance sound transmission transfer functions (DSTTFs), which vary with the distance between a speaker and a user. However, for distances involved in a moderate size chat room, these transfer functions are generally not significant unless the speaker and the user are in different chat rooms.</p>
<p id="p-0037" num="0036">The perceptual shortcoming of the second level is that the median plane is located inside the head.</p>
<p id="p-0038" num="0037">The relative computational demands are equal to or higher than the first level of realism, and could arbitrarily be set at approximately 100&#xd7; the first computational demand level.</p>
<p id="p-0039" num="0038">In a third level of realism, the signal to a listening system would be processed using a frequency-independent sound model, a frequency-dependent sound model, and a reflection sound model; i.e., sound reflections from structures and objects are calculated. A number of different computation methods may be used singularly or in combination in the reflection sound model to provide a binaural listening experience that conveys to the listening user the most realistic perception of actual physical presence in the chat room.</p>
<p id="p-0040" num="0039">A first method uses the reflection of sound in the chat room in addition to the direct waveform of a speaking user's voice. Reflections of a speaking user's voice are computed based on the relative position and orientation of each speaking user and listening user pair plus the virtual acoustic characteristics of the chat room floor, walls, and ceiling.</p>
<p id="p-0041" num="0040">A second method uses the reflection of sound off the human torso for the listening and speaking users.</p>
<p id="p-0042" num="0041">A third method uses multiple reflections where the initial speech wavefront will be followed by more and more reflection wavefronts coming from different directions around the listening user, each one convolved by its own HRTF specific to its incoming azimuth and elevation angles.</p>
<p id="p-0043" num="0042">The third level has no perceptual shortcomings although the level of realism could be reduced to reduce computational loading by reducing the accuracy of the reflection profile. However the computational demands would be much greater, and could arbitrarily be set at approximately 1000&#xd7; greater than the first level computational demand level.</p>
<p id="p-0044" num="0043">The first level of realism is characterized by relatively low computational demands, and simulates four effects. For each ear of each user, the voice stream of every other user who is speaking is merely offset in time using the ITD calculation and multiplied by a combined scale factor resulting from the ILD, ADOL, and DLD. The time offset is a change in sample index position. The sample index position is the location of the data being used in a discrete stream of samples. Given the samples are a time T apart, to simulate a delay of time <b>2</b>T, samples from two places earlier in the stream than normally used are used to effect the delay. Since this doesn't involve computation, but just indexing the sample stream differently, it has basically no computational cost and does not require any computation on the sound signals unless the delay is changing (in which case a limited amount of resampling or interpolation is performed). Since the ITD, ILD, ADOL, and DLD change very slowly, computation of these scale factors can be cached and hence results in insignificant computational loads.</p>
<p id="p-0045" num="0044">Thus, the first level of realism requires one simple multiplication per each combination ear/speaking user in the room, plus one addition per each combination ear/speaking user beyond one per ear to the sum of the contributions of all speaking users. For example, the first level of realism could support a hundred listening users and ten speaking users per chat room using a single Internet host server <b>102</b>-<b>1</b>.</p>
<p id="p-0046" num="0045">As a lesser level of realism, if there are a large number of speaking users and many listening users in the chat room, just the contributions of the N nearest users of each user could be summed for a variable number of N. This prevents the server load from becoming excessive and causing a system failure during a situation such as 99 users simultaneously singing happy birthday for one user.</p>
<p id="p-0047" num="0046">The first level of realism provides substantial improvement over a single audio channel speakerphone system, however it can suffer from two perceptual shortcomings.</p>
<p id="p-0048" num="0047">First, it is difficult to distinguish sounds as coming from the front or back of the user. Also, sounds emanating from along the median plane centered on the user's head appear to be coming from inside the user's head due to front/back confusion.</p>
<p id="p-0049" num="0048">Second, no perception of elevation angles between the speaking users and the user can be conveyed to the user. This is not a significant limitation for two-dimensional chat rooms, but becomes more of an issue with three-dimensional virtual chat room environments. All of these shortcomings can be remedied by higher, more accurate realism levels, but with attendant higher computational demands.</p>
<p id="p-0050" num="0049">The second level uses computations from the first level and introduces HRTFs, HVTF, and/or DSTF sound models. The second level requires convolving each signal coming into a user's ear with a HRTF, which requires a large number of multiplications and additions (e.g., 200 multiplications and additions) for each sample point. However, HRTFs can also simulate elevation differences between speaking users and listening users, and reduce front/back confusion. However, speaking users directly in front of or behind the listening user are still usually perceived inside the user's head.</p>
<p id="p-0051" num="0050">The third level of realism uses computations from the first and second levels and introduces a reflection sound model. The third level requires calculating the initial direct waveform of a speaker's voice as simulated in the first and second levels and the reflections of the room and objects in the room such as the users. This simulates full realism but has very high computational demands, since potentially tens or even hundreds of reflections could each require hundreds of computations per sample point for the HRTF calculations.</p>
<p id="p-0052" num="0051">Minor adjustments to realism at the third level can be selected by varying the number and accuracy of the reflections that are modeled.</p>
<p id="p-0053" num="0052">In one embodiment of the present invention, the users <b>112</b>-<b>1</b> through N use the listening systems <b>114</b>-<b>1</b> through N so the possibilities for echoes are limited to a speaking user's voice traveling into the chat room and back to the speaking user's own ears. In this case, simple echo cancellation with exponentially-weighted attack and decay time constants has been found to be sufficient. These time constants locally attenuate signals to the listening systems <b>114</b>-<b>1</b> through N in proportion to the volume of the signals received from their own microphones <b>116</b>-<b>1</b> through N. The amount of attenuation when a user <b>112</b>-<b>1</b> speaks may be controlled through the GUI <b>108</b>-<b>1</b> using the control system <b>110</b>-<b>1</b>.</p>
<p id="p-0054" num="0053">In another embodiment, the effect of the listening systems <b>114</b>-<b>1</b> through N may be simulated by one or more pairs of loud speakers around the user <b>112</b>-<b>1</b>. However, the range of user movement allowed while perceiving spatial audio with the loud speakers is limited to a small &#x201c;sweet spot&#x201d;. Systems need to be provided feedback and echo cancellation to avoid the microphone directly hearing the output of the speakers as well as the user. Also, a noise-canceling microphone <b>116</b>-<b>1</b> or directional microphone is spotted on the user's mouth to avoid picking up ambient sound from the user's location. The ambient sound would be background noise due to televisions, non-user speakers, and other noise sources collocated with the user.</p>
<p id="p-0055" num="0054">Referring now to <figref idref="DRAWINGS">FIGS. 1 and 2</figref>, therein are shown the personal computer <b>106</b>-N of <figref idref="DRAWINGS">FIG. 1</figref> having GUI <b>108</b>-N of the chat room spatial audio system <b>100</b>. A virtual chat room <b>200</b> appearing on the GUI <b>108</b>-N is labeled with a name <b>202</b>, for example a city location. Around the periphery of the chat room <b>200</b> are a number of labeled entries or doorways leading to other chat rooms represented by doorways <b>204</b> through <b>208</b>. The other chat rooms can be for other topics, locations, etc.</p>
<p id="p-0056" num="0055">Each user <b>112</b>-<b>1</b> through N in the chat room <b>200</b> is represented by a corresponding head icon <b>212</b>-<b>1</b> through N showing the user's name and particular features. The head icons <b>212</b>-<b>1</b> through N on the GUI <b>108</b>-N represent the respective virtual user locations of the users <b>112</b>-<b>1</b> through N in the chat room <b>200</b> and are established by the users <b>112</b>-<b>1</b> through N using their respective control systems <b>110</b>-<b>1</b> through N. The virtual relationships for the three levels of reality disclosed for <figref idref="DRAWINGS">FIG. 1</figref> are shown in <figref idref="DRAWINGS">FIG. 2</figref></p>
<p id="p-0057" num="0056">Each head icon shows a facing direction. For example, the head icon <b>212</b>-<b>5</b> shows left and right ears <b>212</b>L and <b>212</b>R, and a nose <b>212</b>N (and hence the mouth location) on the head <b>212</b>H of the user <b>112</b>-<b>5</b>. In addition, each of the users <b>112</b>-<b>1</b> through N can establish location and orientation for themselves by moving and/or rotating their respective head icons <b>212</b>-<b>1</b> through N within the chat room <b>200</b>, as well as through the various doorways <b>204</b> through <b>208</b> of the chat room <b>200</b>, or jump directly into different chat rooms through other features of the GUI <b>108</b>-<b>1</b> and their respective control systems <b>110</b>-<b>1</b> through N. In one embodiment, the user's own head icon is drawn in bold so as to allow it to be distinguished from the other users' head icons.</p>
<p id="p-0058" num="0057">Each of the tracking systems, such as the tracking system <b>120</b>-<b>1</b> allows the head icon <b>212</b>-<b>5</b> with the ears <b>212</b>L and <b>212</b>R, and the nose <b>212</b>N, to be oriented so that the user <b>112</b>-<b>5</b> can turn his head icon <b>212</b>-<b>5</b> in the chat room <b>200</b> merely by turning his own physical head. Orientation sensing may be provided by many different means, including electronic compass circuits and electronic gyroscopes. More advanced systems can put the user's head elevation angle as well as rotations in azimuth (i.e., pitch and yaw). The orientation of the user's head as shown in the GUI <b>108</b>-<b>1</b> will be automatically updated with changes in the orientation of the user's head. This can help reduce confusion along the median plane of the user's head by enabling the user <b>112</b>-<b>1</b> to rotate his/her head orientation slightly in the virtual chat room <b>200</b> so that the sound source is no longer directly on the median plane. This ability is even used in real situations, when people cock their heads to better locate a sound source.</p>
<p id="p-0059" num="0058">Usually users can just hear what is going on in the chat room in which they are located, however, it has been discovered that it is possible to model sound leakage through the doorways <b>204</b> through <b>208</b> into the chat room <b>200</b> to provide greater realism. Sound leakage through doorways <b>204</b> through <b>208</b> can be modeled simply by setting up the speaking users in the adjoining chat rooms as though they were on the other side of the doorways <b>204</b> through <b>208</b>. For example, the speaking users (or an arbitrarily set number of speaking users in the case of many speaking users) in the chat room on the other side of the doorway <b>204</b> would be treated as though they were on the other side of the doorway <b>204</b>. Factors, such as the distances of the speakers to the doorways, the angle of their heads to the doorway, etc., could be factored in and their sounds combined, attenuated, and sent into the chat room <b>200</b> as though there were a single speaking user at the doorway <b>204</b>. Sounds below a threshold value would be completely ignored (with gradual fade-in and fade-out), reducing computational demands. Furthermore, sounds propagating into the chat room <b>200</b> through the doorway <b>204</b> would not be propagated out through other doorways <b>205</b> through <b>208</b> so as to reduce computational spillover.</p>
<p id="p-0060" num="0059">A system has been discovered for providing selective private conversations in a chat room. The system has been dubbed the &#x201c;cone of silence&#x201d; from the 1965 TV show &#x201c;Get Smart&#x201d;. When a user <b>112</b>-<b>1</b> wants to have a private conversation with one or more other users, such as the users <b>112</b>-<b>2</b> and <b>112</b>-<b>3</b>, the user <b>112</b>-<b>1</b> uses the GUI <b>108</b>-<b>1</b> and the control system <b>110</b>-<b>1</b> to either select the other users or designate an area of privacy, or cone of silence <b>210</b>.</p>
<p id="p-0061" num="0060">The unselected users may either vanish or be distinguished in another fashion from the viewpoint of the users in the cone of silence <b>210</b>, and/or the users in the cone of silence <b>210</b> vanish or otherwise are distinguished from the viewpoint of the users not selected to be in the cone of silence <b>210</b>. Basically, a private chat room is formed for the selected users. The users in the cone of silence <b>210</b> cannot hear other users outside the cone of silence <b>210</b> and vice versa.</p>
<p id="p-0062" num="0061">In further embodiments, where there are three-dimensional user interfaces, similar to virtual worlds in virtual reality or a 3D user environment, the chat room spatial audio system <b>100</b> can easily be adapted to three dimensions by use of vertical orientation tracking.</p>
<p id="p-0063" num="0062">With regard to the architecture used for the chat room <b>200</b>, each user <b>112</b>-<b>1</b> through N may require anywhere from 32 Kb/sec to 200 Kb/sec bandwidth, depending on the available bandwidth of their Internet connection, the sound quality of codecs used, and the server system resources available.</p>
<p id="p-0064" num="0063">In the preferred embodiment, each chat room simulation runs on its own Internet host server, although multiple rooms with few users may be simulated by a single Internet host server. If the first level of realism is being simulated, then total bandwidth going into and out of the server may be high, approaching 20 Mb/sec in the case of 100 users. Where higher levels of realism are being simulated, due to higher per user computational loads, fewer users per chat room can be supported. As a result, the bandwidths going into and out of a server will be much lower. However, even in this case, the bandwidth will be too high for most current users to host a large chat room at home, unless the sound quality is significantly compromised by moving to low-bandwidth codecs. Most broadband connections are currently limited to less than 1 Mb/sec in both directions.</p>
<p id="p-0065" num="0064">It has been found in the case of a large chat room, it is also possible to distribute the work to several servers, such as the Internet host servers <b>102</b>-<b>2</b> through N. This is because just a small number of users are likely to be speaking at the same time (or all but the closest few can be ignored to reduce computational demands). Each server simulating a room will need to see the single voice channel of each speaking user (or those nearby), but binaural sound simulations need to be generated just for a subset of the users. In this way a large number of users in a single chat room may be partitioned among multiple servers.</p>
<p id="p-0066" num="0065">In a further embodiment, the work of the server may be completely distributed among all the users; i.e., among the personal computers <b>106</b>-<b>1</b> through N. Here, each user receives the voice of all other speaking users in the chat room (or those nearby), and simulates the sound field for binaural representation for just that user. In this decentralized embodiment, limiting audio downloads to the closest four speaking users could limit download bandwidth to 260 Kb/sec or less, even with high-quality compression. Limitations on audio upload bandwidth should also be considered. In some cases where many people are speaking near a speaking user, either multicasting or some other efficient networking technique would be used, or the maximum number of listening users may need to be limited.</p>
<p id="p-0067" num="0066">From the above, it will be evident that the computer system for the chat room spatial audio system <b>100</b> comprises all the various computer systems where the necessary processing for the present invention can be carried out any place within the system.</p>
<p id="p-0068" num="0067">Referring now to <figref idref="DRAWINGS">FIG. 3</figref>, therein is shown a system <b>300</b> providing chat room spatial audio according to an embodiment of the present invention. The system includes: sending a signal representative of a sound at a first user location to a second user location in a block <b>302</b>; establishing, in a chat room, a virtual first user location and a virtual second user location in a-block <b>304</b>; establishing the orientation of a listening system at the second user location in a block <b>306</b>, and processing the signal, using the orientation of the listening system, at a plurality of levels of realism to provide a binaural sound at the second user location having an analogous spatial relationship between the second user location and the first user location as the virtual spatial relationship between the virtual second user location and the virtual first user location in a block <b>308</b>. The system <b>300</b> replicates the Cocktail Party Effect in a virtual chat room.</p>
<p id="p-0069" num="0068">While the invention has been described in conjunction with a specific best mode, it is to be understood that many alternatives, modifications, and variations will be apparent to those skilled in the art in light of the aforegoing description. Accordingly, it is intended to embrace all such alternatives, modifications, and variations which fall within the scope of the included claims. All matters set forth herein or shown in the accompanying drawings are to be interpreted in an illustrative and non-limiting sense.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A chat room method comprising:
<claim-text>sending a signal representative of a sound at a first user location to a second user location;</claim-text>
<claim-text>establishing, in a chat room, a first user at a virtual first user location and a second user at a virtual second user location, wherein chat room acoustics are a function of a number of users in the chat room such that increasing the number of users in the chat room reduces accuracy of a sound field simulated in the chat room;</claim-text>
<claim-text>processing the signal at one or more of a plurality of levels of realism related to sound to provide a binaural sound at the second user location having an analogous spatial relationship between the second user location and the first user location as a virtual spatial relationship between the virtual second user location and the virtual first user location; and</claim-text>
<claim-text>selecting a level of realism from the plurality of levels of realism related to sound based on at least one of a number of users in the chat room and a load on computational resources hosting the chat room system,</claim-text>
<claim-text>wherein for a first number of users a first level of realism is selected, processing the signal comprises using a frequency-independent sound model with frequencies that are attenuated or delayed,</claim-text>
<claim-text>wherein for a second number of users that is less than the first number of users a second level of realism is selected, processing the signal comprises using a frequency-dependent sound model with frequencies that are attenuated or delayed, and</claim-text>
<claim-text>wherein for a third number of users that is less than the second number of users a third level of realism is selected, processing the signal comprises using a reflection sound model, wherein a reflection from a structure or object is calculated.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising:
<claim-text>sending a plurality of signals representative of additional user locations to the second user location;</claim-text>
<claim-text>establishing, in the chat room, virtual additional user locations; and</claim-text>
<claim-text>processing less than all the plurality of signals to provide the binaural sound.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising:
<claim-text>sending a plurality of signals representative of additional user locations to the second user location;</claim-text>
<claim-text>establishing, outside the chat room, virtual additional user locations;</claim-text>
<claim-text>establishing an entry into the chat room; and</claim-text>
<claim-text>processing less than all the plurality of signals and simulating sound through the entry into the chat room to provide the binaural sound.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising:
<claim-text>sending a plurality of signals representative of additional user locations to the second user location; and</claim-text>
<claim-text>processing the plurality of signals to provide the binaural sound at one of a plurality of levels of realism using head-related transfer functions and to convolve the plurality of signals at different levels of accuracy.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising:
<claim-text>sending a plurality of signals representative of additional user locations to the second user location; and</claim-text>
<claim-text>processing the plurality of signals to provide the binaural sound at one of the plurality of levels of realism using the reflection sound model for at least one of less than all of the plurality of signals, less than full accuracy modeling of all of the plurality of signals, and a combination thereof.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein processing the signal further comprises echo cancellation for sound at the second user location due to sound from the second user location to the first user location and back to the second user location.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising:
<claim-text>sending a plurality of signals representative of additional user locations to the second user location;</claim-text>
<claim-text>establishing, in a chat room, virtual additional user locations representative of the additional user locations; and</claim-text>
<claim-text>establishing a cone of silence around the virtual second user location and at least one of the virtual additional user locations to allow signals just between the second user location and at least one of the additional user locations corresponding to the at least one of the virtual additional user locations.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein if the first level of realism is selected, processing the signal comprises using the frequency-independent sound model and not using the frequency-dependent sound model and the reflection sound model.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein if the second level of realism is selected, processing the signal comprises using both the first level and the second level of realism and not the third level of realism.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein if the third level of realism is selected, processing the signal comprises using the first level, the second level, and the third level of realism.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. A chat room system comprising:
<claim-text>a sending system for sending a signal representative of a sound at a first user location to a second user location;</claim-text>
<claim-text>a listening system for providing binaural sound at the second user location; and</claim-text>
<claim-text>a computer system for establishing, in a chat room, wherein chat room acoustics are a function of a number of users in the chat room such that increasing the number of users in the chat room reduces accuracy of a sound field simulated in the chat room, a first user at a virtual first user location and a second user at a virtual second user location, the computer system for processing the signal at one or more of a plurality of levels of realism related to sound to provide the binaural sound at the second user location having an analogous spatial relationship between the second user location and the first user location as a virtual spatial relationship between the virtual second user location and the virtual first user location, and selecting a level of realism from the plurality of levels of realism related to sound based on at least one of a number of users in the chat room and a load on computational resources hosting the chat room system,</claim-text>
<claim-text>wherein for a first number of users a first level of realism is selected and the computer system uses a frequency-independent sound model to provide the binaural sound at the second user location with frequencies that are attenuated or delayed,</claim-text>
<claim-text>wherein for a second number of users that is less than the first number of users a second level of realism is selected and the computer system uses a frequency-dependent sound model to provide the binaural sound at the second user location with frequencies that are attenuated or delayed, and</claim-text>
<claim-text>wherein for a third number of users that is less than the second number of users a third level of realism is selected and the computer system uses a reflection sound model to provide the binaural sound at the second user location, wherein a reflection from a structure or object is calculated.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The system as claimed in <claim-ref idref="CLM-00011">claim 11</claim-ref> further comprising:
<claim-text>additional computer systems for sending a plurality of signals representative of additional user locations to the second user location and for establishing, in the chat room, a plurality of additional user locations each providing a plurality of signals; and</claim-text>
<claim-text>the computer system is for processing less than all the plurality of signals to provide the binaural sound.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The system as claimed in <claim-ref idref="CLM-00011">claim 11</claim-ref> further comprising:
<claim-text>additional computer systems for:</claim-text>
<claim-text>sending a plurality of signals representative of additional user locations to the second user location;</claim-text>
<claim-text>establishing, outside the chat room, virtual additional user locations;</claim-text>
<claim-text>establishing an entry into the chat room; and</claim-text>
<claim-text>the computer system for processing less than all the plurality of signals and simulating sound through the entry into the chat room to provide the binaural sound.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The system as claimed in <claim-ref idref="CLM-00011">claim 11</claim-ref> wherein the computer system further provides for processing the signal at one of the plurality of levels of realism using a sound model calculating at least one of:
<claim-text>an interaural time difference, an interaural level difference, an angle-dependent output level, a distance level dependence;</claim-text>
<claim-text>a head-related transfer function, a human voice transfer function, a distance sound transmission transfer function; and</claim-text>
<claim-text>a combination thereof.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The system as claimed in <claim-ref idref="CLM-00011">claim 11</claim-ref> further comprising:
<claim-text>additional computer systems for sending a plurality of signals representative of additional user locations to the second user location; and</claim-text>
<claim-text>the additional computer systems processing the plurality of signals to provide the binaural sound at one of a plurality of levels of realism using head-related transfer functions and to convolve the plurality of signals at different levels of accuracy.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The system as claimed in <claim-ref idref="CLM-00011">claim 11</claim-ref> wherein the computer system further provides for processing the signal at one of the plurality of levels of realism and uses at least one of:
<claim-text>an interaural time difference, an interaural level difference, an angle-dependent output level, a distance level dependence;</claim-text>
<claim-text>a head-related transfer function processing for a second level of processing;</claim-text>
<claim-text>a head-related transfer function, a human voice transfer function, a distance sound transmission transfer function;</claim-text>
<claim-text>chat room reflection modeling, human torso reflection modeling, a multiple reflection; and</claim-text>
<claim-text>a combination thereof.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The system as claimed in <claim-ref idref="CLM-00011">claim 11</claim-ref> further comprising:
<claim-text>additional computer systems for sending a plurality of signals representative of sounds at a plurality of additional user locations to the second user location; and</claim-text>
<claim-text>the computer system for processing the plurality of signals to provide the binaural sound at one of the plurality of levels of realism using at least one of chat room reflection modeling for at least one of less than all of the plurality of signals, less than full accuracy modeling of all of the plurality of signals, and a combination thereof.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The system as claimed in <claim-ref idref="CLM-00011">claim 11</claim-ref> wherein the computer system further provides for echo cancellation for sound at the second user location due to sound from the second user location to the first user location and back to the second user location.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The system as claimed in <claim-ref idref="CLM-00011">claim 11</claim-ref> further comprising:
<claim-text>additional computer systems for:</claim-text>
<claim-text>sending a plurality of signals representative of sounds at a plurality of additional user locations to the second user location;</claim-text>
<claim-text>for establishing, in a chat room, virtual additional user locations representative of the plurality of additional user locations; and</claim-text>
<claim-text>the computer system for establishing a cone of silence around the virtual second user location and at least one of the virtual additional user locations to allow signals just between the second user location and the at least one of the additional user locations corresponding to the at least one of the virtual additional user locations.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein if the first level of realism is selected, processing the signal comprises using the frequency-independent sound model and not using the frequency-dependent sound model and the reflection sound model.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein if the second level of realism is selected, processing the signal comprises using both the first level and the second level of realism and not the third level of realism.</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein if the third level of realism is selected, processing the signal comprises using the first level, the second level, and the third level of realism. </claim-text>
</claim>
</claims>
</us-patent-grant>
