<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08627333-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08627333</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13197461</doc-number>
<date>20110803</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<us-term-of-grant>
<us-term-extension>167</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>3</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>719312</main-classification>
</classification-national>
<invention-title id="d2e53">Message queuing with flexible consistency options</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>7050432</doc-number>
<kind>B1</kind>
<name>Banavar et al.</name>
<date>20060500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>370390</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>7249229</doc-number>
<kind>B2</kind>
<name>Ogasawara et al.</name>
<date>20070700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711154</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>7565443</doc-number>
<kind>B2</kind>
<name>Rossmanith et al.</name>
<date>20090700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>7788676</doc-number>
<kind>B2</kind>
<name>Caron</name>
<date>20100800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>7865606</doc-number>
<kind>B1</kind>
<name>Tewes et al.</name>
<date>20110100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709230</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>8370851</doc-number>
<kind>B2</kind>
<name>Srinivasan et al.</name>
<date>20130200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>719314</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2006/0053425</doc-number>
<kind>A1</kind>
<name>Berkman et al.</name>
<date>20060300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2007/0073821</doc-number>
<kind>A1</kind>
<name>Brail</name>
<date>20070300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2008/0077939</doc-number>
<kind>A1</kind>
<name>Harran et al.</name>
<date>20080300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2008/0222255</doc-number>
<kind>A1</kind>
<name>Hall</name>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2008/0307056</doc-number>
<kind>A1</kind>
<name>Videlov</name>
<date>20081200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2009/0228460</doc-number>
<kind>A1</kind>
<name>Martinez et al.</name>
<date>20090900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2010/0083278</doc-number>
<kind>A1</kind>
<name>Shirgaonkar</name>
<date>20100400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2010/0205271</doc-number>
<kind>A1</kind>
<name>Callaghan</name>
<date>20100800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709206</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2010/0235587</doc-number>
<kind>A1</kind>
<name>Noel</name>
<date>20100900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2011/0010340</doc-number>
<kind>A1</kind>
<name>Hung et al.</name>
<date>20110100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>707623</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>2011/0099233</doc-number>
<kind>A1</kind>
<name>Calder et al.</name>
<date>20110400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709206</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>2011/0265095</doc-number>
<kind>A1</kind>
<name>Cardona et al.</name>
<date>20111000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>718105</main-classification></classification-national>
</us-citation>
<us-citation>
<nplcit num="00019">
<othercit>IBM Technical Disclosure, &#x201c;Dynamic Workload Balancing in Clustered Message Queuing Systems,&#x201d; IPCOM000124176D; Apr. 11, 2005, www.ip.com.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00020">
<othercit>Ching-Shun, H., et al., &#x201c;Knowledge Consistency Maintenance Architecture for Bullwhip Effect in Global SCM,&#x201d; 2008 Seventh Mexican International Conference on Artificial Intelligence, 978-0/7695-3441-1/08 copyright 2008 IEEE.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>20</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>719312</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>10</number-of-drawing-sheets>
<number-of-figures>10</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20130036427</doc-number>
<kind>A1</kind>
<date>20130207</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Chen</last-name>
<first-name>Han</first-name>
<address>
<city>White Plains</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Kim</last-name>
<first-name>Minkyong</first-name>
<address>
<city>Scarsdale</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Lei</last-name>
<first-name>Hui</first-name>
<address>
<city>Scarsdale</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="004" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Ye</last-name>
<first-name>Fan</first-name>
<address>
<city>Ossining</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Chen</last-name>
<first-name>Han</first-name>
<address>
<city>White Plains</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Kim</last-name>
<first-name>Minkyong</first-name>
<address>
<city>Scarsdale</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Lei</last-name>
<first-name>Hui</first-name>
<address>
<city>Scarsdale</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="004" designation="us-only">
<addressbook>
<last-name>Ye</last-name>
<first-name>Fan</first-name>
<address>
<city>Ossining</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<last-name>Gutman</last-name>
<first-name>Jose</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
<agent sequence="02" rep-type="attorney">
<addressbook>
<orgname>Fleit Gibbons Gutman Bongini &#x26; Bianco PL</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>International Business Machines Corporation</orgname>
<role>02</role>
<address>
<city>Armonk</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Puente</last-name>
<first-name>Emerson</first-name>
<department>2196</department>
</primary-examiner>
<assistant-examiner>
<last-name>Sun</last-name>
<first-name>Charlie</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Embodiments of the invention relate to message queuing. In one embodiment, a request from an application for retrieving a message from a queue is received. The queue is stored across multiple nodes of a distributed storage system. A preference with respect to message order and message duplication associated with the queue is identified. A message sequence index associated with the queue is sampled based on the preference that has been identified. The message is selected in response to the sampling. The message that has been selected is made unavailable to other applications for a given interval of time, while maintaining the message in the queue. The message is sent to the application.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="125.05mm" wi="146.39mm" file="US08627333-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="184.49mm" wi="168.06mm" orientation="landscape" file="US08627333-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="166.03mm" wi="157.48mm" orientation="landscape" file="US08627333-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="154.52mm" wi="126.49mm" orientation="landscape" file="US08627333-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="210.90mm" wi="126.49mm" orientation="landscape" file="US08627333-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="239.18mm" wi="154.52mm" orientation="landscape" file="US08627333-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="135.38mm" wi="146.64mm" file="US08627333-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="233.93mm" wi="179.58mm" file="US08627333-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="218.44mm" wi="157.82mm" orientation="landscape" file="US08627333-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="181.53mm" wi="174.92mm" orientation="landscape" file="US08627333-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="214.46mm" wi="174.33mm" orientation="landscape" file="US08627333-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND</heading>
<p id="p-0002" num="0001">The present invention generally relates to message queuing, and more particularly relates to message queuing with flexible consistency options.</p>
<p id="p-0003" num="0002">Queuing, an asynchronous messaging paradigm, is used to connect loosely coupled components to form large-scale, highly-distributed, and fault-tolerant applications. Queuing decouples message producers from message consumers. Some current queuing systems try and offer queuing in a cloud computing environment. However, as a distributed storage system queuing is constrained by the CAP theorem by Brewer, which states that among the three qualities, consistency (C); availability (A); and network partition tolerance (P), only two can be achieved at the same time. As a cloud service, it is important to achieve A and P. This results in consistency being sacrificed. In the queuing context, consistency means that a message is delivered exactly once and in-order. Many conventional queuing systems provide at-least once delivery (no-loss) with no order. For applications that can tolerate out-of-order delivery, this semantic is sufficient for correctness. However, when these applications prefer in-order delivery this semantic is not sufficient.</p>
<heading id="h-0002" level="1">BRIEF SUMMARY</heading>
<p id="p-0004" num="0003">In one embodiment, a method for managing message queuing is disclosed. The method comprises receiving a request from an application for retrieving a message from a queue. The queue is stored across multiple nodes of a distributed storage system. A preference with respect to message order and message duplication associated with the queue is identified. A message sequence index associated with the queue is sampled based on the preference that has been identified. The message is selected in response to the sampling. The message that has been selected is made unavailable to other applications for a given interval of time, while maintaining the message in the queue. The message is sent to the application.</p>
<p id="p-0005" num="0004">In another embodiment, a message queuing system is disclosed. The system comprises a distributed storage system comprising and at least one information processing system communicatively coupled to the distributed storage system. The at least one information processing system comprises memory, a processor, and a messaging queuing system. The message queuing system is configured to perform a method. The method comprises receiving a request from an application for retrieving a message from a queue. The queue is stored across multiple nodes of a distributed storage system. A preference with respect to message order and message duplication associated with the queue is identified. A message sequence index associated with the queue is sampled based on the preference that has been identified. The message is selected in response to the sampling. The message that has been selected is made unavailable to other applications for a given interval of time, while maintaining the message in the queue. The message is sent to the application.</p>
<p id="p-0006" num="0005">In another embodiment, a computer program product for managing message queuing is disclosed. The computer program product comprises a computer readable storage medium having computer readable program code embodied therewith. The computer readable program code comprises computer readable program code configured to perform a method. The method comprises receiving a request from an application for retrieving a message from a queue. The queue is stored across multiple nodes of a distributed storage system. A preference with respect to message order and message duplication associated with the queue is identified. A message sequence index associated with the queue is sampled based on the preference that has been identified. The message is selected in response to the sampling. The message that has been selected is made unavailable to other applications for a given interval of time, while maintaining the message in the queue. The message is sent to the application.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE SEVERAL VIEWS OF THE DRAWINGS</heading>
<p id="p-0007" num="0006">The accompanying figures where like reference numerals refer to identical or functionally similar elements throughout the separate views, and which together with the detailed description below are incorporated in and form part of the specification, serve to further illustrate various embodiments and to explain various principles and advantages all in accordance with the present invention, in which:</p>
<p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram illustrating one example of an operating environment comprising a message queuing system according to one embodiment of the present invention;</p>
<p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. 2</figref> shows a detailed view of a system architecture for the message queuing system of <figref idref="DRAWINGS">FIG. 1</figref> according to one embodiment of the present invention;</p>
<p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. 3</figref> shows another detailed view of a system architecture for the message queuing system of <figref idref="DRAWINGS">FIG. 1</figref> according to another embodiment of the present invention;</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 4</figref> shows one example of a distributed storage system data store model detailed view of the a message queuing system architecture according to one embodiment of the present invention;</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 5</figref> shows one example of a sequence of operations for implementing a visibility timeout feature of the message queuing system of <figref idref="DRAWINGS">FIG. 1</figref> according to one embodiment of the present invention;</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 6</figref> is an operational flow diagram illustrating one example of managing messaging queues according to one embodiment of the present invention;</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 7</figref> is an operational flow diagram illustrating one example of managing messages in queues according to one embodiment of the present invention;</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 8</figref> illustrates one example of a cloud computing node according to one embodiment of the present invention;</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 9</figref> illustrates one example of a cloud computing environment according to one embodiment of the present invention; and</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 10</figref> illustrates abstraction model layers according to one embodiment of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0018" num="0017">Service-Oriented Architecture (SOA) has been widely adopted by the information technology industry to build large-scale software applications. SOA uses the service abstraction to promote the principle of modular software design and development and thus enhancing software reusability. As SOA continues to evolve, asynchronous messaging technologies, such as queuing and publish/subscribe, are increasingly being incorporated into solutions. Message Oriented Middleware (MOM) complements and enhances SOA in at least three different ways. First, message queues are used as reliable communication channels for synchronous request-response interactions. Protocols such as SOAP over JMS maintain the remote procedural call syntax, but address the unreliability of HTTP. Second, solutions and processes are refactored to behave asynchronously. Using MOM as an intermediary, program components are decoupled from each other. The overall system can continue to function even in the face of component failure, thus providing improved reliability. Finally, messaging is deployed as a form of connectivity to support large-scale, loosely-couple, and distributed applications. Commonly used application patterns include workload dispatching/load balancing, MapReduce-like pipelined processing, and information aggregation and dissemination.</p>
<p id="p-0019" num="0018">In order to reduce capital and operational expenses, the Information Technology industry is gradually adopting the cloud computing model. Several service providers now operate public, shared queuing services in the cloud, for example, Amazon Simple Queue Service (SQS) and Microsoft Windows Azure Queue. For reliability reasons, these systems choose to sacrifice consistency in favor of service availability and network partition tolerance. For queuing, reduced consistency means possible message loss, duplication, and out-of-order delivery. Existing systems adopt a consistency model of at-least once delivery (no-loss) with no order. This suffices for a number of applications. However, some applications, although tolerant of out-of-order delivery, still prefer in-order delivery so as to provide better service quality at application level.</p>
<p id="p-0020" num="0019">Another trend in cloud computing is the emergence of Platform-as-a-Service (PaaS), which strives to simplify application development and deployment in the cloud. PaaS provides a higher-level of abstraction than bare metal VMs in Infrastructure-as-a-Service (IaaS). Applications are modeled as elastic deployment patterns of components (such as HTTP server, database, queuing, etc.) which the PaaS runtime monitors and controls. To support the aforementioned loosely-couple, event-driven applications in a PaaS environment, it is important to create a highly elastic, embeddable queuing component.</p>
<p id="p-0021" num="0020">Therefore, one or more embodiments of the present invention provide a message queuing system that that supports both queuing and publish/subscribe services. In one embodiment, the queuing service provided by the system is a cloud-based queuing service, which meets the two requirements discussed above. The message queuing system, in one embodiment, is built on top of a distributed storage system and guarantees at-least once delivery and offers best-effort message order. A customizable parameter further allows users to specify the desired trade-off between message order and duplication. The system can be deployed as a scalable, shared cloud service. The system can also be embedded in PaaS applications as an elastic queuing component.</p>
<p id="p-0022" num="0021">Operating Environment</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 1</figref> shows one example of an operating environment <b>100</b> for implementing a message queuing system of one or more embodiments of the present invention. In particular, <figref idref="DRAWINGS">FIG. 1</figref> shows a message queuing system <b>102</b> being implemented within a cloud-computing environment. In one embodiment, the message queuing system <b>102</b> is a distributed system comprising a cluster of computer nodes as well. Implementing the message queuing system <b>102</b> in a cloud computing environment embodiment allows the message queuing system <b>102</b> to provide cloud-based message queuing services. However, it should be noted that embodiments of the present invention are not limited to cloud computing environments. Rather, various embodiments of the present invention are capable of being implemented in conjunction with any other type of computing environment now known or later developed.</p>
<p id="p-0024" num="0023">In the embodiment shown in <figref idref="DRAWINGS">FIG. 1</figref>, the message queuing system <b>102</b> is coupled to an underlying distributed storage system <b>104</b> that provides distributed persistence across a cluster of computer nodes. It should be noted that the distributed storage system <b>104</b> is also referred to herein as a &#x201c;persistence system&#x201d;, which provides reliable object store. Various distributed storage systems can be utilized by the message queuing system <b>102</b> such as, but not limited to, Apache Cassandra, WebSphere&#xae; eXtreme Scale, etc. The message queuing system <b>102</b> provides runtime instances <b>106</b> that allow various clients <b>108</b>, <b>110</b> to interact with the message queuing system <b>102</b>. In one embodiment, clients <b>108</b>, <b>110</b> interact with the message queuing system <b>102</b> utilizing an interface <b>114</b> such as, but not limited to, a Hyper Text Transfer Protocol (HTTP) based application programming interface (API) that uses a Representational State Transfer (REST) mapping.</p>
<p id="p-0025" num="0024">A runtime instance <b>106</b> of the message queuing system <b>102</b> interacts with an instance <b>112</b> of the distributed storage <b>104</b> via a distributed storage interface <b>116</b> such as, but not limited to, a Thrift interface. The above configuration of the environment <b>100</b> allows the message queuing system <b>102</b> to be highly available and partition tolerant; offer best-effort in-order delivery and no-loss guarantee (at-least-once delivery); and deployable either as a shared service or as a component in a PaaS application.</p>
<p id="p-0026" num="0025">System Architecture</p>
<p id="p-0027" num="0026">A more detailed discussion is now given with respect to the system architecture of the message queuing system <b>102</b>. Distributed persistence is provided by the distributed storage system <b>104</b>. A queue operations component <b>204</b> (<figref idref="DRAWINGS">FIG. 2</figref>) (represented by the runtime <b>106</b> in <figref idref="DRAWINGS">FIG. 1</figref>) implements a queue API using a distributed storage system API. To maximize overall system throughput, the queue operations component <b>204</b> is configured to store all states in the distributed storage/persistence layer and does not maintain state information itself. Therefore, multiple instances can be deployed to support concurrent client access. The queue operations component <b>204</b> exposes its functionality through a native API. To enable a wide variety of clients to access the service, an HTTP REST component <b>114</b> provides a RESTful (conforming to REST constraints) interface of the native queue API via HTTP binding.</p>
<p id="p-0028" num="0027">The queue operations component <b>204</b> also comprises components (not shown) such as a data model mapping component, a message store component, a message sequence component, a message visibility component, and a message retrieval component. The data model mapping component maps the queues and messages to a data model that is suitable for the distributed storage system <b>104</b>. The message store component supports message enqueuing operations. The message sequence component manages the order of messages for the queues that prefer delivery order. The message visibility component manages the temporary hiding of messages that are received, but not deleted. The message retrieval component selects the appropriate message for a message retrieval operation. These operations are discussed in greater detail below.</p>
<p id="p-0029" num="0028">In a shared service deployment, a virtual machine (VM) image <b>202</b> comprises one instance of each component <b>112</b>, <b>114</b>, <b>204</b> discussed above, as shown in <figref idref="DRAWINGS">FIG. 2</figref>. In one embodiment, the message queuing system <b>102</b> comprises a cluster of computer nodes comprising an instance <b>112</b> of the distributed storage system <b>104</b> and an instance of the queue operations component <b>204</b>, thereby creating a distributed system.</p>
<p id="p-0030" num="0029">There are two processes <b>206</b> that allow the queue operations component <b>204</b> to communicate with the REST interface <b>114</b>. In one embodiment, the distributed storage system instance <b>112</b> runs in its own address space. It should be noted that the distributed storage system instance <b>112</b> can also be implemented within its own VM as well. A separate HTTP container hosts the REST interface <b>114</b> and the queue operations <b>204</b>. The queue operations component <b>204</b> accesses the distributed storage system instance <b>112</b> via a local interface <b>116</b>.</p>
<p id="p-0031" num="0030">A dispatching mechanism <b>210</b> routes incoming client requests <b>212</b> to the REST interface instance <b>114</b>. This can be achieved using either a dedicated front-end HTTP router or round-robin DNS. To provide adequate service level to clients <b>108</b>, a separate monitoring mechanism <b>214</b> controls the dynamic scaling (e.g., instantiation of additional nodes or removal of nodes) of the VM cluster <b>216</b>. There are two different reasons for scaling the system <b>102</b>. First, more storage capacity is needed to handle increased number of queued messages. Second, more VMs are required to cope with increased request rate from clients. The monitor <b>214</b> collects information about current storage size and average response time; decides if the system <b>102</b> should be scaled up or down; and interacts with the IaaS <b>218</b> layer to carry out these actions.</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 3</figref> shows another embodiment directed to a PaaS application deployment. When used as a component in a PaaS application, the queue operations component <b>304</b> can be embedded in-process in the application logic, so that it can interact with the queuing layer directly via its native API. The distributed storage system instance <b>112</b> still runs in a separate address space in the same VM <b>302</b>. In this deployment pattern, the storage capacity and throughput capacity of the queuing component <b>304</b> increase along with the application <b>320</b>. In a more flexible pattern, the distributed storage system instance <b>112</b> can be deployed on a separate VM image <b>303</b>, whose instance number may be adjusted independently from that of the application logic. This allows a finer grained capacity control. In either case, the PaaS layer <b>318</b> controls the number of instances for each type of VM images <b>302</b>, <b>303</b>, according to predefined service level.</p>
<p id="p-0033" num="0032">Queue Operations Interface</p>
<p id="p-0034" num="0033">The message queuing system <b>102</b>, via the queue operations component <b>204</b>, provides a queue interface that deals with message operations. The design goals of this queuing interface are to provide at-least once (no-loss) guarantee over potentially unreliable HTTP and to support best-effort in-order delivery. The queue operations component <b>204</b> supports an enqueue operation such as SendMessage, which places a message on a given queue. When the message is returned, the message is guaranteed to be persisted. ReceiveMessage attempts to retrieve the oldest messages in the queue. These messages are returned to the client, but are not immediately deleted. These messages are locked and made invisible to subsequent ReceiveMessage calls during a time window herein referred to as &#x201c;visibility timeout&#x201d;. After successfully processing the messages, a client issues DeleteMessage request before the timeout occurs to actually delete the messages. If the timeout occurs before the DeleteMessage call is received, the messages reappear in the queue. This visibility timeout mechanism provides a lightweight transaction over HTTP with automatic rollback specified by the timeout value.</p>
<p id="p-0035" num="0034">To support multi-tenancy in a shared service deployment, the message queuing system <b>102</b> uses the concept of accounts. The queuing interface discussed above provides various operations such as CreateAccount, ListAccounts, and DeleteAccount. Within an account, a client can manipulate queues using operations such as CreateQueue for creating a queue, ListQueues for listing available queues, DeleteQueue for deleting a given queue (where any metadata, message sequence index for the queue, and any remaining messages in the queue are deleted from the distributed storage system <b>104</b>). As will be discussed in greater detail below, an application (through the queuing interface) can supply a preference for tradeoff between message delivery order and duplication for each queue object during creation of a given queue.</p>
<p id="p-0036" num="0035">Distributed System Data Store Configuration</p>
<p id="p-0037" num="0036">In one embodiment, the queuing operations component <b>204</b> also maps the queues and messages to a data model suitable for the distributed storage system <b>104</b>. One example of a mapping <b>400</b> is shown in <figref idref="DRAWINGS">FIG. 4</figref>. There are three main resource types in the message queuing system <b>102</b>: account, queue, and message. These resources are stored as distributed storage system <b>104</b> rows (shown in <figref idref="DRAWINGS">FIG. 4</figref>). Each row is uniquely identified by a key that takes the form of XYZ-id, where XYZ is a three character prefix and id is created by computing the base64 encoded string of 160-bit SHA 1 hash of a human readable unique name. This configuration distributes the data across the entire distributed storage system <b>104</b> cluster and allows direct access to any objects without having to search through multiple levels of indirection.</p>
<p id="p-0038" num="0037">Accounts <b>402</b> is the root object holding references to all accounts. This is a singleton with key ACT-_ACCOUNTS_. The account index is stored in column family Accounts, in the form of (AccountName, AccountKey). The shorthand (n,v,t) denotes a distributed storage system <b>104</b> column with name n, value v, and timestamp t. When timestamp is unimportant to the configuration, it may be omitted for succinctness.</p>
<p id="p-0039" num="0038">The prefix of account key is ACT and the id is the hash of the account's name, which is unique across the system. There are two column families: Metadata and Queues. The former comprises account information such as owner's name, console logon password, secret key, etc. The latter contains reference indices to the queues in the account, in the form of (QueueName, QueueKey). The prefix of queue key is QUE and the id is the hash of the fully qualified and unique queue name in the form of AccountName.QueueName. As the cornerstone of the queuing system design, several column families are defined, whose uses are described in details in subsequent sections.</p>
<p id="p-0040" num="0039">Metadata <b>404</b> stores information about the queue. The main items of interest are the default visibility timeout and the consistency level hint. Permissions <b>406</b> stores access control related information, which will not be discussed in the paper. Messages <b>408</b> stores an index of all messages in the queue in the form of (MessageKey, Handle, Timestamp). Appearances <b>410</b> stores the scheduled appearance time of each message, in the form of (Timestamp-Handle, MessageKey). The shorthand A-B means the concatenation of string A, a dash (-), and string B. Deletions <b>412</b> stores scheduled deletion time of each message, in the form of (Timestamp-Handle, MessageKey). The prefix of message key is MSG and the id is the hash of a globally unique Id. There is one column family, Data <b>414</b>, which contains a single column (Content, bytes), where bytes is the actual message content.</p>
<p id="p-0041" num="0040">In one embodiment, there is a special account with the reserved name _SYSTEM_. This special account comprises three queues, AccountGC, QueueGC, and MessageGC. When an account is deleted, a queue is deleted, or a message expires, a request is put into the corresponding queue, which is received by a background garbage collector that performs the actual cleanup of the relevant data structure.</p>
<p id="p-0042" num="0041">Message Queuing</p>
<p id="p-0043" num="0042">A more detailed discussion is now given with respect to the message queuing operations of the message queuing system <b>102</b>. Given the data store schema of the distributed storage system <b>104</b> discussed above, the message queuing system <b>102</b> realizes the queue operations by manipulating the stored data. Conceptually, realizing the visibility timeout requires maintaining a timer for each received-but-not-yet-deleted message in the system. However, a direct and naive implementation requires keeping a large number of timer events in memory, which incurs processing overhead and complicates failure recovery because the timers are in-memory states. Therefore, the message queuing system <b>102</b> uses a timestamp-based ordering algorithm (using, in one embodiment, the Appearances <b>410</b> column family of a queue) to realize timer management for visibility timeout and best-effort in-order delivery together. By using a timestamp-based ordering algorithm the message queuing system <b>102</b> does not require separate storage for locked messages and the need for time threads is eliminated.</p>
<p id="p-0044" num="0043">For example, consider the sequence of queue operations <b>500</b> shown in <figref idref="DRAWINGS">FIG. 5</figref>. When SendMessage is invoked, the system <b>102</b> creates a new unique message key K<sub>msg </sub>and a dummy handle (a unique string) H<sub>dummy</sub>. Let T<sub>now </sub>be the current time. A column (K<sub>msg</sub>, H<sub>dummy</sub>, T<sub>now</sub>) is inserted to the Messages column family of the given queue. This column in the form of (id, handle, timestamp) is also referred to as a message sequence index object.</p>
<p id="p-0045" num="0044">The system <b>102</b> then inserts a column (T<sub>now</sub>&#x2212;H<sub>dummy</sub>, K<sub>msg</sub>) into the Appearances column family. This indicates to the system that this particular message becomes visible (appears) at T<sub>now</sub>, that is, it can be received immediately. The content of the message itself is stored in a message row keyed by K<sub>msg</sub>. Note that, because the Appearances column family is sorted by column name, this preserves the message order for later delivery. <figref idref="DRAWINGS">FIG. 5</figref> shows an example where a client sends two messages m<b>1</b> and m<b>2</b> to a queue. Message m<b>1</b> is sent at time T<b>1</b> and message m<b>2</b> is sent at time T<b>2</b>. Therefore, based on the above, a column (T<sub>1</sub>&#x2212;H<sub>0</sub>, K<sub>m1</sub>) is inserted for message m<b>1</b> and a column (T<sub>2</sub>&#x2212;H<sub>0</sub>, K<sub>m2</sub>) is inserted for message m<b>2</b>.</p>
<p id="p-0046" num="0045">When ReceiveMessage is invoked, the system <b>102</b> retrieves the first column in the Appearances column family, (T&#x2212;H,K<sub>msg</sub>). Because the column family is sorted by column name, K<sub>msg </sub>refers to the oldest message that is currently in the queue. If T&#x2266;T<sub>now</sub>, this message is available to be returned. The appearance column (T&#x2212;H,K<sub>msg</sub>) is removed from the Appearances column family. A new appearance column (T&#x2032;&#x2212;H&#x2032;,K<sub>msg</sub>) is inserted, where T&#x2032;=T<sub>now</sub>+T<sub>vto </sub>(T<sub>vto </sub>is the visibility timeout value), and H&#x2032; is a unique handle that will be returned to the client as the receipt handle for this message. The corresponding column in the Messages column family is also updated with (K<sub>msg</sub>,H&#x2032;,T&#x2032;). If T&#x3e;T<sub>now</sub>, it means that all messages in the queue are currently locked, and therefore the queue is empty and nothing is returned. For example, <figref idref="DRAWINGS">FIG. 5</figref> shows that at times T<b>3</b> and T<b>4</b> two ReceiveMessage requests are received, one for message m<b>1</b> and one for message m<b>2</b>. A new appearance column (T<sub>5</sub>&#x2212;H<sub>1</sub>, K<sub>m1</sub>) is inserted for message m<b>1</b> and (T<sub>6</sub>&#x2212;H<sub>1</sub>, K<sub>m2</sub>) for message m<b>2</b>. In this example T<sub>vto </sub>was equal to 2.</p>
<p id="p-0047" num="0046">When DeleteMessage is invoked, the system <b>102</b> obtains the message key K<sub>msg </sub>and the receipt handle H from the client request. The system <b>102</b> verifies that column (K<sub>msg</sub>, H, T) is in the Messages column family. The system <b>102</b> then deletes the appearance column (T&#x2212;H, H<sub>msgs</sub>) from the Appearances column family. For example, <figref idref="DRAWINGS">FIG. 5</figref> shows that message m<b>1</b> is deleted prior to the visibility timer expiring. Therefore, the message m<b>1</b> is no longer available in the queuing system. However, if a DeleteMessage request is not received before the visibility timer expires the message is made available to another ReceiveMessage request. For example, <figref idref="DRAWINGS">FIG. 5</figref> shows that the client currently processing message m<b>2</b> has crashed and a DeleteMessage request was not received prior to the visibility timer expiring. Therefore, the system <b>102</b> makes message m<b>2</b> available once again in the queue and subsequently returns message m<b>2</b> to a client in response to receiving s ReceiveMessage received at time T<b>7</b>.</p>
<p id="p-0048" num="0047">This algorithm seamlessly integrates visibility timeout and best-effort in-order delivery. As can be seen from the above discussion, message order is recorded in an index on a per queue basis. Actual messages are distributed across all nodes in the system. All states are persisted across sessions. Therefore the system can survive node failures and still provide high availability.</p>
<p id="p-0049" num="0048">The system architecture uses multiple queue operations components to serve client requests concurrently. In order to maximize system throughput and increase reliability, no distributed locks are used among these component instances. The result is that, when multiple clients invoke ReceiveMessage operation on the same queue object using different entry points into the system, the same message may be returned to these clients. From a correctness point of view, this does not violate the at-least once delivery model. However, it may be desirable to reduce the number of duplicate messages.</p>
<p id="p-0050" num="0049">Therefore, the message queuing system <b>102</b> uses a collision avoidance algorithm to balance the probability of duplicates and the message delivery order. The change to the above algorithm involves only the ReceiveMessage operation. Instead of always retrieving the first appearance column, the system will retrieve a random column among the first K appearance columns. The larger the value of K, the less likely that concurrent receivers will obtain the same message, but the more out-of-order the returned message sequence will be. The system exposes the value of K as a configurable parameter for each queue, where a client or an admin of the system can specify the value of K. A value K=1 produces the best order with potential duplications, whereas a large value of K (e.g., K&#x3e;1) reduces duplication. Additionally, the value of K can be set for each individual request as well.</p>
<p id="p-0051" num="0050">As discussed above, data consistency in a queue means in-order and exactly-once delivery. It is possible to achieve this in a distributed environment if locks are used. Particularly, this means setting the read/write consistency level combination to one of the following, ONE/ALL, QUORUM/QUORUM, or ALL/ONE, and using distributed locks among the queue operations components. However, this reduces the system availability, because a node failure will cause queue operations to block indefinitely.</p>
<p id="p-0052" num="0051">The message queuing system <b>102</b>, in on embodiment, uses the lowest level of data consistency, ConsistencyLevel.ONE, for all read and write operations and employs no distributed locks in the queue operations layer. This gives the system the best possible performance, in terms of throughput and latency; it also provides the highest level of availability and partition tolerance, which are all highly desirable features for a cloud based service. In one embodiment, the above algorithm can be further refined to cope with the potentially inconsistent data. In particular, during ReceiveMessage operation, an Appearances column and a Messages column may represent snapshots of the queue at different time. Therefore, the algorithm can be refined to consider all possible data propagation sequences among the distributed storage system replicas and deal with each of them accordingly. Also, a distributed storage system replica may go down temporarily while accounts, queues, or messages are deleted. Therefore, it is important to have a mechanism to reconcile the difference when these replicas are back online. For this reason, garbage collection is used for all deletion operations. The grace period for garbage collecting a deleted resource is set to be long enough to accommodate any repair time of a downed storage node of the distributed storage system <b>104</b>.</p>
<p id="p-0053" num="0052">Operation Flow Diagrams</p>
<p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. 6</figref> is an operational flow diagram illustrating one example of a process for managing queues according to one embodiment of the present invention. The operational flow diagram of <figref idref="DRAWINGS">FIG. 6</figref> begins at step <b>602</b> and flows directly to step <b>604</b>. The message queuing system <b>102</b>, at step <b>604</b>, receives an application request for creating a queue. This request, in one embodiment, comprises a preference for a tradeoff between message order and duplication. This preference can be specified as a discrete set of choices such as, but not limited to, &#x201c;Favor order&#x201d;, &#x201c;Favor non-duplication&#x201d;, &#x201c;Balanced&#x201d;, etc. Alternatively, this preference can be specified as a numeric value such as, but not limited to, &#x201c;1&#x201d; for order, &#x201c;&#x221e;&#x201d; for non-duplication, and other values for tradeoff in between.</p>
<p id="p-0055" num="0054">The message queuing system <b>102</b>, at step <b>606</b>, stores metadata related to the queue in a distributed storage system <b>104</b> (persistence system) and creates a message sequence index object in the distributed storage system <b>104</b> if the tradeoff preference favors order. The message sequence index object, in one embodiment, is a list of tuples in the form of (id, handle, timestamp), where id is the message id, handle is a unique number, and timestamp is the time that the message becomes available for retrieval. This list of tuples can be sorted by ascending order of timestamp and handled (or by any other organization mechanisms).</p>
<p id="p-0056" num="0055">The message queuing system <b>102</b>, at step <b>608</b>, receives an application request for enqueuing a message to a queue. In one embodiment, the message queuing system <b>102</b> enqueues a message by inserting a tuple (id, handle, timestamp) into the queue. The message queuing system <b>102</b>, at step <b>610</b>, stores the message in the distributed storage system <b>104</b>. The message queuing system <b>102</b>, at step <b>612</b>, inserts a reference to the message in the message sequence index object if available. The control flow then exits at step <b>614</b>.</p>
<p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. 7</figref> is an operational flow diagram illustrating one example of a process for managing messages in queues according to one embodiment of the present invention. The operational flow diagram of <figref idref="DRAWINGS">FIG. 7</figref> begins at step <b>702</b> and flows directly to step <b>704</b>. The message queuing system <b>102</b>, at step <b>704</b>, receives an application request for retrieving a message from a queue.</p>
<p id="p-0058" num="0057">The message queuing system <b>102</b>, at step <b>706</b>, determines a sampling strategy based on a tradeoff preference associated with the queue. For example, the message queuing system <b>102</b> can look up the preference in the queue itself. A positive integer value K is then calculated based on the preference, where K=1 corresponds to the strongest favoring order and K=+&#x221e; corresponds to the strongest favoring of non-duplication. If K=+&#x221e; the message queuing system <b>102</b> samples a random distributed storage system node for a message belonging to the queue. Otherwise the message queuing system <b>102</b> samples a random message reference from the first K objects in the message sequence index of the queue and retrieves the message content from the distributed storage system <b>104</b> using the sampled message reference.</p>
<p id="p-0059" num="0058">The message queuing system <b>102</b>, at step <b>708</b>, samples the queue or the message sequence index and selects a message. When retrieving the message, the message queuing system <b>102</b> selects a tuple (id, handle, timestamp). The message queuing system <b>102</b>, at step <b>709</b>, determines if the timestamp is in the future. If the result of this determine is positive, the message, at step <b>711</b>, is determined to not be available and the message is not returned to the application. If the result of this determination is negative, the timestamp identifies a current point in time or a past point in time, and the message, at step <b>713</b>, is determined to be available.</p>
<p id="p-0060" num="0059">The message queuing system <b>102</b>, at step <b>714</b>, temporarily hides the selected message and initiates a visibility timer. For example, the message queuing system <b>102</b> removes the tuple (id, handle, timestamp) associated with the selected message and inserts a new tuple (id, handle, now+timeout) into the queue. The message queuing system <b>102</b>, at step <b>716</b>, returns the selected message to the application. The message queuing system <b>102</b>, at step <b>718</b>, determines if delete message request has been received prior to the visibility timer expiring (or a threshold associated with the time being reached). If the result of this determination is positive, the message queuing system <b>102</b>, at step <b>720</b> permanently removes the message from the distributed storage system <b>104</b>. If the result of this determination is negative, the message queuing system <b>102</b>, at step <b>722</b> re-enables the message in the queue after the visibility timer expires. The control flow then exits at step <b>724</b>.</p>
<p id="p-0061" num="0060">Information Processing System</p>
<p id="p-0062" num="0061">Referring now to <figref idref="DRAWINGS">FIG. 8</figref>, a schematic of an example of an information processing system <b>800</b> such as the server system <b>104</b> of <figref idref="DRAWINGS">FIG. 1</figref>. In one embodiment, the information processing system <b>800</b> is a cloud computing node. Cloud computing node <b>800</b> is only one example of a suitable cloud computing node and is not intended to suggest any limitation as to the scope of use or functionality of embodiments of the invention described herein. Regardless, cloud computing node <b>800</b> is capable of being implemented and/or performing any of the functionality set forth hereinabove.</p>
<p id="p-0063" num="0062">In the cloud computing node <b>800</b> there is a computer system/server <b>802</b>, which is operational with numerous other general purpose or special purpose computing system environments or configurations. Examples of well-known computing systems, environments, and/or configurations that may be suitable for use with computer system/server <b>802</b> include, but are not limited to, personal computer systems, server computer systems, thin clients, thick clients, hand-held or laptop devices, multiprocessor systems, microprocessor-based systems, set top boxes, programmable consumer electronics, network PCs, minicomputer systems, mainframe computer systems, and distributed cloud computing environments that include any of the above systems or devices, and the like.</p>
<p id="p-0064" num="0063">Computer system/server <b>802</b> may be described in the general context of computer system-executable instructions, such as program modules, being executed by a computer system. Generally, program modules may include routines, programs, objects, components, logic, data structures, and so on that perform particular tasks or implement particular abstract data types. Computer system/server <b>802</b> may be practiced in distributed cloud computing environments where tasks are performed by remote processing devices that are linked through a communications network. In a distributed cloud computing environment, program modules may be located in both local and remote computer system storage media including memory storage devices.</p>
<p id="p-0065" num="0064">As shown in <figref idref="DRAWINGS">FIG. 8</figref>, computer system/server <b>802</b> in cloud computing node <b>800</b> is shown in the form of a general-purpose computing device. The components of computer system/server <b>802</b> may include, but are not limited to, one or more processors or processing units <b>804</b>, a system memory <b>806</b>, and a bus <b>808</b> that couples various system components including system memory <b>806</b> to processor <b>804</b>.</p>
<p id="p-0066" num="0065">Bus <b>808</b> represents one or more of any of several types of bus structures, including a memory bus or memory controller, a peripheral bus, an accelerated graphics port, and a processor or local bus using any of a variety of bus architectures. By way of example, and not limitation, such architectures include Industry Standard Architecture (ISA) bus, Micro Channel Architecture (MCA) bus, Enhanced ISA (EISA) bus, Video Electronics Standards Association (VESA) local bus, and Peripheral Component Interconnects (PCI) bus.</p>
<p id="p-0067" num="0066">Computer system/server <b>802</b> typically includes a variety of computer system readable media. Such media may be any available media that is accessible by computer system/server <b>802</b>, and it includes both volatile and non-volatile media, removable and non-removable media.</p>
<p id="p-0068" num="0067">System memory <b>806</b>, in one embodiment, comprises a virtual machine <b>202</b> discussed above. The system memory <b>806</b> can include computer system readable media in the form of volatile memory, such as random access memory (RAM) <b>810</b> and/or cache memory <b>812</b>. Computer system/server <b>802</b> may further include other removable/non-removable, volatile/non-volatile computer system storage media. By way of example only, storage system <b>814</b> can be provided for reading from and writing to a non-removable, non-volatile magnetic media (not shown and typically called a &#x201c;hard drive&#x201d;). Although not shown, a magnetic disk drive for reading from and writing to a removable, non-volatile magnetic disk (e.g., a &#x201c;floppy disk&#x201d;), and an optical disk drive for reading from or writing to a removable, non-volatile optical disk such as a CD-ROM, DVD-ROM or other optical media can be provided. In such instances, each can be connected to bus <b>808</b> by one or more data media interfaces. As will be further depicted and described below, memory <b>806</b> may include at least one program product having a set (e.g., at least one) of program modules that are configured to carry out the functions of embodiments of the invention.</p>
<p id="p-0069" num="0068">Program/utility <b>816</b>, having a set (at least one) of program modules <b>818</b>, may be stored in memory <b>806</b> by way of example, and not limitation, as well as an operating system, one or more application programs, other program modules, and program data. Each of the operating system, one or more application programs, other program modules, and program data or some combination thereof, may include an implementation of a networking environment. Program modules <b>818</b> generally carry out the functions and/or methodologies of embodiments of the invention as described herein.</p>
<p id="p-0070" num="0069">Computer system/server <b>802</b> may also communicate with one or more external devices <b>820</b> such as a keyboard, a pointing device, a display <b>822</b>, etc.; one or more devices that enable a user to interact with computer system/server <b>802</b>; and/or any devices (e.g., network card, modem, etc.) that enable computer system/server <b>802</b> to communicate with one or more other computing devices. Such communication can occur via I/O interfaces <b>824</b>. Still yet, computer system/server <b>802</b> can communicate with one or more networks such as a local area network (LAN), a general wide area network (WAN), and/or a public network (e.g., the Internet) via network adapter <b>826</b>. As depicted, network adapter <b>826</b> communicates with the other components of computer system/server <b>802</b> via bus <b>808</b>. It should be understood that although not shown, other hardware and/or software components could be used in conjunction with computer system/server <b>802</b>. Examples, include, but are not limited to: microcode, device drivers, redundant processing units, external disk drive arrays, RAID systems, tape drives, and data archival storage systems, etc.</p>
<p id="p-0071" num="0070">Cloud Environment</p>
<p id="p-0072" num="0071">It is understood in advance that although the following is a detailed discussion on cloud computing, implementation of the teachings recited herein are not limited to a cloud computing environment. Rather, various embodiments of the present invention are capable of being implemented in conjunction with any other type of computing environment now known or later developed. For example, various embodiments of the present invention are applicable to any computing environment with a virtualized infrastructure or any other type of computing environment.</p>
<p id="p-0073" num="0072">For convenience, the Detailed Description includes the following definitions which have been derived from the &#x201c;Draft NIST Working Definition of Cloud Computing&#x201d; by Peter Mell and Tim Grance, dated Oct. 7, 2009, which is cited in an IDS filed herewith, and a copy of which is attached thereto. However, it should be noted that cloud computing environments that are applicable to one or more embodiments of the present invention are not required to correspond to the following definitions and characteristics given below or in the &#x201c;Draft NIST Working Definition of Cloud Computing&#x201d; publication. It should also be noted that the following definitions, characteristics, and discussions of cloud computing are given as non-limiting examples.</p>
<p id="p-0074" num="0073">Cloud computing is a model of service delivery for enabling convenient, on-demand network access to a shared pool of configurable computing resources (e.g. networks, network bandwidth, servers, processing, memory, storage, applications, virtual machines, and services) that can be rapidly provisioned and released with minimal management effort or interaction with a provider of the service. This cloud model may include at least five characteristics, at least three service models, and at least four deployment models.</p>
<p id="p-0075" num="0074">Characteristics are as follows:</p>
<p id="p-0076" num="0075">On-demand self-service: a cloud consumer can unilaterally provision computing capabilities, such as server time and network storage, as needed automatically without requiring human interaction with the service's provider.</p>
<p id="p-0077" num="0076">Broad network access: capabilities are available over a network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms (e.g., mobile phones, laptops, and PDAs).</p>
<p id="p-0078" num="0077">Resource pooling: the provider's computing resources are pooled to serve multiple consumers using a multi-tenant model, with different physical and virtual resources dynamically assigned and reassigned according to demand. There is a sense of location independence in that the consumer generally has no control or knowledge over the exact location of the provided resources but may be able to specify location at a higher level of abstraction (e.g., country, state, or datacenter).</p>
<p id="p-0079" num="0078">Rapid elasticity: capabilities can be rapidly and elastically provisioned, in some cases automatically, to quickly scale out and rapidly released to quickly scale in. To the consumer, the capabilities available for provisioning often appear to be unlimited and can be purchased in any quantity at any time.</p>
<p id="p-0080" num="0079">Measured service: cloud systems automatically control and optimize resource use by leveraging a metering capability at some level of abstraction appropriate to the type of service (e.g., storage, processing, bandwidth, and active user accounts). Resource usage can be monitored, controlled, and reported providing transparency for both the provider and consumer of the utilized service.</p>
<p id="p-0081" num="0080">Service Models are as follows:</p>
<p id="p-0082" num="0081">Software as a Service (SaaS): the capability provided to the consumer is to use the provider's applications running on a cloud infrastructure. The applications are accessible from various client devices through a thin client interface such as a web browser (e.g., web-based e-mail). The consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, storage, or even individual application capabilities, with the possible exception of limited user-specific application configuration settings.</p>
<p id="p-0083" num="0082">Platform as a Service (PaaS): the capability provided to the consumer is to deploy onto the cloud infrastructure consumer-created or acquired applications created using programming languages and tools supported by the provider. The consumer does not manage or control the underlying cloud infrastructure including networks, servers, operating systems, or storage, but has control over the deployed applications and possibly application hosting environment configurations.</p>
<p id="p-0084" num="0083">Infrastructure as a Service (IaaS): the capability provided to the consumer is to provision processing, storage, networks, and other fundamental computing resources where the consumer is able to deploy and run arbitrary software, which can include operating systems and applications. The consumer does not manage or control the underlying cloud infrastructure but has control over operating systems, storage, deployed applications, and possibly limited control of select networking components (e.g., host firewalls).</p>
<p id="p-0085" num="0084">Deployment Models are as follows:</p>
<p id="p-0086" num="0085">Private cloud: the cloud infrastructure is operated solely for an organization. It may be managed by the organization or a third party and may exist on-premises or off-premises.</p>
<p id="p-0087" num="0086">Community cloud: the cloud infrastructure is shared by several organizations and supports a specific community that has shared concerns (e.g., mission, security requirements, policy, and compliance considerations). It may be managed by the organizations or a third party and may exist on-premises or off-premises.</p>
<p id="p-0088" num="0087">Public cloud: the cloud infrastructure is made available to the general public or a large industry group and is owned by an organization selling cloud services.</p>
<p id="p-0089" num="0088">Hybrid cloud: the cloud infrastructure is a composition of two or more clouds (private, community, or public) that remain unique entities but are bound together by standardized or proprietary technology that enables data and application portability (e.g., cloud bursting for load-balancing between clouds).</p>
<p id="p-0090" num="0089">A cloud computing environment is service oriented with a focus on statelessness, low coupling, modularity, and semantic interoperability. At the heart of cloud computing is an infrastructure comprising a network of interconnected nodes.</p>
<p id="p-0091" num="0090">Referring now to <figref idref="DRAWINGS">FIG. 9</figref>, illustrative cloud computing environment <b>902</b> is depicted. As shown, cloud computing environment <b>902</b> comprises one or more cloud computing nodes <b>1600</b> with which local computing devices used by cloud consumers, such as, for example, personal digital assistant (PDA) or cellular telephone <b>904</b>, desktop computer <b>906</b>, laptop computer <b>908</b>, and/or automobile computer system <b>910</b> may communicate. Nodes <b>904</b>, <b>906</b>, <b>908</b>, <b>910</b> can communicate with one another. They may be grouped (not shown) physically or virtually, in one or more networks, such as Private, Community, Public, or Hybrid clouds as described hereinabove, or a combination thereof. This allows cloud computing environment <b>902</b> to offer infrastructure, platforms and/or software as services for which a cloud consumer does not need to maintain resources on a local computing device. It is understood that the types of computing devices <b>904</b>, <b>906</b>, <b>908</b>, <b>910</b> shown in <figref idref="DRAWINGS">FIG. 9</figref> are intended to be illustrative only and that computing nodes <b>800</b> and cloud computing environment <b>902</b> can communicate with any type of computerized device over any type of network and/or network addressable connection (e.g., using a web browser).</p>
<p id="p-0092" num="0091">Referring now to <figref idref="DRAWINGS">FIG. 10</figref>, a set of functional abstraction layers provided by cloud computing environment <b>902</b> (<figref idref="DRAWINGS">FIG. 9</figref>) is shown. It should be understood in advance that the components, layers, and functions shown in <figref idref="DRAWINGS">FIG. 10</figref> are intended to be illustrative only and embodiments of the invention are not limited thereto. As depicted, the following layers and corresponding functions are provided:</p>
<p id="p-0093" num="0092">Hardware and software layer <b>1002</b> includes hardware and software components. Examples of hardware components include mainframes, in one example IBM&#xae; System z&#xae; systems; RISC (Reduced Instruction Set Computer) architecture based servers, in one example IBM System p&#xae; systems; IBM System x&#xae; systems; IBM BladeCenter&#xae; systems; storage devices; networks and networking components. Examples of software components include network application server software, in one example IBM WebSphere&#xae; application server software; and database software, in one example IBM DB2&#xae; database software. (IBM, zSeries, pSeries, xSeries, BladeCenter, WebSphere, and DB2 are trademarks of International Business Machines Corporation registered in many jurisdictions worldwide)</p>
<p id="p-0094" num="0093">Virtualization layer <b>1004</b> provides an abstraction layer from which the following examples of virtual entities may be provided: virtual servers; virtual storage; virtual networks, including virtual private networks; virtual applications and operating systems; and virtual clients.</p>
<p id="p-0095" num="0094">In one example, management layer <b>1006</b> may provide the functions described below. Resource provisioning provides dynamic procurement of computing resources and other resources that are utilized to perform tasks within the cloud computing environment. Metering and Pricing provide cost tracking as resources are utilized within the cloud computing environment, and billing or invoicing for consumption of these resources. In one example, these resources may comprise application software licenses. Security provides identity verification for cloud consumers and tasks, as well as protection for data and other resources. User portal provides access to the cloud computing environment for consumers and system administrators. Service level management provides cloud computing resource allocation and management such that required service levels are met. Service Level Agreement (SLA) planning and fulfillment provide pre-arrangement for, and procurement of, cloud computing resources for which a future requirement is anticipated in accordance with an SLA.</p>
<p id="p-0096" num="0095">Workloads layer <b>1008</b> provides examples of functionality for which the cloud computing environment may be utilized. Examples of workloads and functions which may be provided from this layer include: mapping and navigation; software development and lifecycle management; virtual classroom education delivery; data analytics processing; transaction processing; and message queuing, as discussed above.</p>
<p id="p-0097" num="0096">Non-Limiting Examples</p>
<p id="p-0098" num="0097">The terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of the invention. As used herein, the singular forms &#x201c;a&#x201d;, &#x201c;an&#x201d; and &#x201c;the&#x201d; are intended to include the plural forms as well, unless the context clearly indicates otherwise. It will be further understood that the terms &#x201c;comprises&#x201d; and/or &#x201c;comprising,&#x201d; when used in this specification, specify the presence of stated features, integers, steps, operations, elements, and/or components, but do not preclude the presence or addition of one or more other features, integers, steps, operations, elements, components, and/or groups thereof.</p>
<p id="p-0099" num="0098">The corresponding structures, materials, acts, and equivalents of all means or step plus function elements in the claims below are intended to include any structure, material, or act for performing the function in combination with other claimed elements as specifically claimed. The description of the present invention has been presented for purposes of illustration and description, but is not intended to be exhaustive or limited to the invention in the form disclosed. Many modifications and variations will be apparent to those of ordinary skill in the art without departing from the scope and spirit of the invention. The embodiment was chosen and described in order to best explain the principles of the invention and the practical application, and to enable others of ordinary skill in the art to understand the invention for various embodiments with various modifications as are suited to the particular use contemplated.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method for managing message queuing, the method comprising:
<claim-text>receiving a request from an application for retrieving a message from a queue stored across multiple nodes of a distributed storage system, wherein the queue has been created by a request that comprises a specified preference for the queue for a tradeoff between message order and message duplication, metadata corresponding to the specified preference for the queue being stored in the distributed storage system;</claim-text>
<claim-text>identifying the preference associated with the queue with respect to the tradeoff between message order and message duplication;</claim-text>
<claim-text>sampling a message sequence index associated with the queue based on the preference that has been identified;</claim-text>
<claim-text>selecting, in response to the sampling, the message;</claim-text>
<claim-text>making the message that has been selected unavailable to other applications for a given interval of time, while maintaining the message in the queue; and</claim-text>
<claim-text>sending the message to the application.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the sampling further comprises:
<claim-text>calculating a positive integer K based on the preference that has been identified, where K=1 corresponds to a strong preference towards message order and K=+&#x221e;, which is indicated by K=a large integer value greater than a number of objects in the message sequence index, corresponds to a strong preference towards non-duplication.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the sampling further comprises:
<claim-text>responsive to calculating K to be K=+&#x221e;, sampling at least one random node in the distributed storage system for a message associated with the queue; and</claim-text>
<claim-text>responsive to calculating K to be K&#x3c;&#x221e;,
<claim-text>sampling a random message reference from a first K objects in the message sequence index, and</claim-text>
<claim-text>retrieving the message from the distributed storage system based on the random message reference that has been sampled.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>receiving, after sending the message to the application and prior to the given interval of time having expired, a request to delete the message from the queue; and</claim-text>
<claim-text>permanently removing the message from the distributed storage system.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>determining that the given interval of time has expired prior to receiving a request from the application to delete the message; and</claim-text>
<claim-text>making the message that has been sent to the application available to other applications.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein making the message that has been selected unavailable to other applications for a given interval of time comprises:
<claim-text>updating a timestamp associated with the message based on the given interval of time, where the message is unavailable until a future point in time corresponding to the timestamp that has been updated.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein selecting the message comprises:
<claim-text>analyzing a timestamp associated with the message;</claim-text>
<claim-text>determining if the timestamp identifies a future point in time;</claim-text>
<claim-text>responsive to the timestamp identifying a future point in time determining that the message is unavailable; and</claim-text>
<claim-text>responsive to the timestamp identifying one of a current point in time and a past point in time, determining that the message is available.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the message sequence index is a list of tuples in the form of (id, handle, timestamp), where id is an identifier of a message, handle is a unique number associated with the message, and timestamp is a time that the message is available for retrieval.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the value of the specified preference stored as metadata in the distributed storage system is a configurable parameter for the queue, the value of the parameter being exposed by the distributed storage system and configurable by a client of the distributed storage system.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. A message queuing system, the message queuing system comprising:
<claim-text>a distributed storage system comprising; and</claim-text>
<claim-text>at least one information processing system communicatively coupled to the distributed storage system, the at least one information processing system comprising memory, a processor, and a messaging queuing system configured to perform a method comprising:
<claim-text>receiving a request from an application for retrieving a message from a queue stored across multiple nodes of the distributed storage system, wherein the queue has been created by an application request that comprises a specified preference for the queue for a tradeoff between message order and message duplication, metadata corresponding to the specified preference for the queue being stored in the distributed storage system;</claim-text>
<claim-text>identifying a preference associated with the queue with respect to message order and message duplication;</claim-text>
<claim-text>sampling a message sequence index associated with the queue based on the preference that has been identified;</claim-text>
<claim-text>selecting, in response to the sampling, the message;</claim-text>
<claim-text>making the message that has been selected unavailable to other applications for a given interval of time, while maintaining the message in the queue; and</claim-text>
<claim-text>sending the message to the application.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The message queuing system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the sampling further comprises:
<claim-text>calculating a positive integer K based on the preference that has been identified, where K=1 corresponds to a strong preference towards message order and K=&#x221e; which is indicated by K=a large integer value greater than a number of objects in the message sequence index, corresponds to a strong preference towards non-duplication;</claim-text>
<claim-text>responsive to calculating K to be K=+&#x221e;, sampling at least one random node in the distributed storage system for a message associated with the queue; and</claim-text>
<claim-text>responsive to calculating K to be K&#x3c;&#x221e;,
<claim-text>sampling a random message reference from a first K objects in the message sequence index, and</claim-text>
<claim-text>retrieving the message from the distributed storage system based on the random message reference that has been sampled.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The message queuing system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the method performed by the message queuing system further comprises:
<claim-text>determining that the given interval of time has expired prior to receiving a request from the application to delete the message; and</claim-text>
<claim-text>making the message that has been sent to the application available to other applications.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The message queuing system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein making the message that has been selected unavailable to other applications for a given interval of time comprises:
<claim-text>updating a timestamp associated with the message based on the given interval of time, where the message is unavailable until a future point in time corresponding to the timestamp that has been updated.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The message queuing system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein selecting the message comprises:
<claim-text>analyzing a timestamp associated with the message;</claim-text>
<claim-text>determining if the timestamp identifies a future point in time;</claim-text>
<claim-text>responsive to the timestamp identifying a future point in time determining that the message is unavailable; and</claim-text>
<claim-text>responsive to the timestamp identifying one of a current point in time and a past point in time, determining that the message is available.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. A computer program product for managing message queuing, the computer program product comprising a non-transitory computer readable storage medium having computer readable program code embodied therewith, the computer readable program code comprising computer readable program code configured to perform a method comprising:
<claim-text>receiving a request from an application for retrieving a message from a queue stored across multiple nodes of the distributed storage system, wherein the queue has been created by an application request that comprises a specified preference for the queue for a tradeoff between message order and message duplication, metadata corresponding to the specified preference for the queue being stored in the distributed storage system;</claim-text>
<claim-text>identifying a preference associated with the queue with respect to message order and message duplication;</claim-text>
<claim-text>sampling a message sequence index associated with the queue based on the preference that has been identified;</claim-text>
<claim-text>selecting, in response to the sampling, the message;</claim-text>
<claim-text>making the message that has been selected unavailable to other applications for a given interval of time, while maintaining the message in the queue; and</claim-text>
<claim-text>sending the message to the application.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The computer program product of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the sampling further comprises:
<claim-text>calculating a positive integer K based on the preference that has been identified, where K=1 corresponds to a strong preference towards message order and K=+&#x221e; which is indicated by K=a large integer value greater than a number of objects in the message sequence index, corresponds to a strong preference towards non-duplication.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The computer program product of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the sampling further comprises:
<claim-text>responsive to calculating K to be K=+&#x221e;, sampling at least one random node in the distributed storage system for a message associated with the queue; and</claim-text>
<claim-text>responsive to calculating K to be K&#x3c;&#x221e;,
<claim-text>sampling a random message reference from a first K objects in the message sequence index, and</claim-text>
<claim-text>retrieving the message from the distributed storage system based on the random message reference that has been sampled.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The computer program product of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the method performed by the computer readable program code further comprises:
<claim-text>determining that the given interval of time has expired prior to receiving a request from the application to delete the message; and</claim-text>
<claim-text>making the message that has been sent to the application available to other applications.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The computer program product of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein making the message that has been selected unavailable to other applications for a given interval of time comprises:
<claim-text>updating a timestamp associated with the message based on the given interval of time, where the message is unavailable until a future point in time corresponding to the timestamp that has been updated.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The computer program product of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein selecting the message comprises:
<claim-text>analyzing a timestamp associated with the message;</claim-text>
<claim-text>determining if the timestamp identifies a future point in time;</claim-text>
<claim-text>responsive to the timestamp identifying a future point in time determining that the message is unavailable; and</claim-text>
<claim-text>responsive to the timestamp identifying one of a current point in time and a past point in time, determining that the message is available. </claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
