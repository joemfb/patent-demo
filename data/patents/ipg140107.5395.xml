<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626495-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626495</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12850439</doc-number>
<date>20100804</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="regional">
<country>EP</country>
<doc-number>09168699</doc-number>
<date>20090826</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>567</us-term-extension>
<disclaimer>
<text>This patent is subject to a terminal disclaimer.</text>
</disclaimer>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20130101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>10</class>
<subclass>L</subclass>
<main-group>19</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>7042001</main-classification>
<further-classification>704200</further-classification>
<further-classification>704206</further-classification>
<further-classification>704210</further-classification>
<further-classification>704222</further-classification>
<further-classification>704226</further-classification>
</classification-national>
<invention-title id="d2e73">Method of correcting errors in binary masks</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>2004/0002858</doc-number>
<kind>A1</kind>
<name>Attias et al.</name>
<date>20040100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>2006/0056647</doc-number>
<kind>A1</kind>
<name>Ramakrishnan et al.</name>
<date>20060300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>2007/0055508</doc-number>
<kind>A1</kind>
<name>Zhao et al.</name>
<date>20070300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704226</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>2008/0101653</doc-number>
<kind>A1</kind>
<name>Wang</name>
<date>20080500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>2009/0006038</doc-number>
<kind>A1</kind>
<name>Jojic et al.</name>
<date>20090100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>702190</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2010/0135511</doc-number>
<kind>A1</kind>
<name>Pontoppidan</name>
<date>20100600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>381313</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2010/0262423</doc-number>
<kind>A1</kind>
<name>Huo et al.</name>
<date>20101000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704233</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2011/0046948</doc-number>
<kind>A1</kind>
<name>Pedersen</name>
<date>20110200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704231</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>WO</country>
<doc-number>WO 2006/000103</doc-number>
<kind>A1</kind>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00010">
<othercit>Srinivasan S.; DeLiang Wang, Transforming Binary Uncertainties for Robust Speech Recognition, Sep. 2007, IEEE, vol. 15, pp. 2130-2140.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00011">
<othercit>DeLiang Wang, Time-Frequency Masking for Speech Separation and Its Potential for Hearing Aid Design, 2008, Trends in Amplification, vol. 12 No. 4, pp. 332-353.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00012">
<othercit>Aarthi M. Reddy; Bhiksha Raj, Soft Mask Estimation for Single Channel Speaker Separation, 2004, ISCA Tutorial and Research Workshop on Statistical and Perceptual Audio Processing.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00013">
<othercit>Yang Shao;DeLiang Wang, Robust Speaker Recognition Using Binary Time-Frequency Masks, 2006, IEEE, vol. 1, p. 1-4.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00014">
<othercit>Martin Cooke; Phil Green; Ljubomir Josifovski; Ascension Vizinho, Robust automatic speech recognition with missing and unreliable acoustic data, 2001, Speech Communication, vol. 34, pp. 267-285.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00015">
<othercit>Deliang Wang, on ideal binary mask as the computational goal of auditory scene analysis, 2005, Speech Separation by Humans and Machines, pp. 181-197.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00016">
<othercit>Reyes-Gomez M.J.;Raj, B.; Ellis D.R.W., Multi-channel source separation by factorial HMMs, 2003, IEEE, vol. 1, pp. 1-4.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00017">
<othercit>Sebastien Demange; Christophe Cerisara; Jean-Paul Haton, Missing data mask estimation with frequency and temporal dependencies, january 2009, Computer Speech and Language archive, vol. 23 Issue 1, pp. 25-41.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00018">
<othercit>Raphael Bornard;Emmanuelle Lecan;Louis Laborelli;Jean-Hugues Chenot, Missing data correction in still images and image sequences, 2002, Proceedings of the tenth ACM international conference on Multimedia, pp. 355-361.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00019">
<othercit>Ji Hun Park; Jae Sam Yoon; Hong Kook Kim, HMM-Based Mask Estimation for a Speech Recognition Front-End Using Computational Auditory Scene Analysis, 2008, IEEE, pp. 176-179.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00020">
<othercit>Tao Li, A general model for clustering binary data, 2005, Florida International University, pp. 188-197.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00021">
<othercit>Rabiner L.R., A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition, 1989, IEEE, vol. 77 Issue 2, pp. 257-286.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00022">
<othercit>J. C. C Segura; A. De La Torre; M. C. Benitez; A. M. Peinado, Model-based compensation of the additive noise for continuous speech recognition. Experiments using the Aurora Ii database and tasks, 2001, Eurospeech Scandinavia, pp. 1-4.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00023">
<othercit>Barker, J. et al. &#x201c;Robust ASR Based on Clean Speech Models: An Evaluation of Missing Data Techniques for Connected Digit Recognition in Noise&#x201d;. Eurospeech 2001, Sep. 2001, pp. 1-4, XP007004804.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00024">
<othercit>Li, N. et al. &#x201c;Factors influencing intelligibility of ideal binary-masked speech: Implications for noise reduction&#x201d;, J. Acoust. Soc. Am. vol. 123, No. 3, Mar. 2008. pp. 1673-1682. XP002570521.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00025">
<othercit>Shao, Y. et al. &#x201c;Sequential organization of speech in computational auditory scene analysis&#x201d;, Elsevier Science Publishers, Speech Communication vol. 51, No. 8, 2009, pp. 657-667.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00026">
<othercit>Srinivasan, S. et al. &#x201c;Binary and ratio time-frequency masks for robust speech recognition&#x201d;, Speech Communication, Elsevier Science Publishers, vol. 48, No. 11, 2006 pp. 1486-1501. XP025056863.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>27</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>702190</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>702191</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704231</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704233</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704226</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704200</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>7042001</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704206</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704210</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704222</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>5</number-of-drawing-sheets>
<number-of-figures>9</number-of-figures>
</figures>
<us-related-documents>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>61237109</doc-number>
<date>20090826</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20110051948</doc-number>
<kind>A1</kind>
<date>20110303</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Boldt</last-name>
<first-name>Jesper B&#xfc;nsow</first-name>
<address>
<city>Sm&#xf8;rum</city>
<country>DK</country>
</address>
</addressbook>
<residence>
<country>DK</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Kjems</last-name>
<first-name>Ulrik</first-name>
<address>
<city>Sm&#xf8;rum</city>
<country>DK</country>
</address>
</addressbook>
<residence>
<country>DK</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Pedersen</last-name>
<first-name>Michael Syskind</first-name>
<address>
<city>Sm&#xf8;rum</city>
<country>DK</country>
</address>
</addressbook>
<residence>
<country>DK</country>
</residence>
</us-applicant>
<us-applicant sequence="004" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Christensen</last-name>
<first-name>Mads Graesb&#xf8;ll</first-name>
<address>
<city>Klarup</city>
<country>DK</country>
</address>
</addressbook>
<residence>
<country>DK</country>
</residence>
</us-applicant>
<us-applicant sequence="005" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Jensen</last-name>
<first-name>S&#xf8;ren Holdt</first-name>
<address>
<city>Gistrup</city>
<country>DK</country>
</address>
</addressbook>
<residence>
<country>DK</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Boldt</last-name>
<first-name>Jesper B&#xfc;nsow</first-name>
<address>
<city>Sm&#xf8;rum</city>
<country>DK</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Kjems</last-name>
<first-name>Ulrik</first-name>
<address>
<city>Sm&#xf8;rum</city>
<country>DK</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Pedersen</last-name>
<first-name>Michael Syskind</first-name>
<address>
<city>Sm&#xf8;rum</city>
<country>DK</country>
</address>
</addressbook>
</inventor>
<inventor sequence="004" designation="us-only">
<addressbook>
<last-name>Christensen</last-name>
<first-name>Mads Graesb&#xf8;ll</first-name>
<address>
<city>Klarup</city>
<country>DK</country>
</address>
</addressbook>
</inventor>
<inventor sequence="005" designation="us-only">
<addressbook>
<last-name>Jensen</last-name>
<first-name>S&#xf8;ren Holdt</first-name>
<address>
<city>Gistrup</city>
<country>DK</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Birch, Stewart, Kolasch &#x26; Birch, LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Oticon A/S</orgname>
<role>03</role>
<address>
<city>Smorum</city>
<country>DK</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Han</last-name>
<first-name>Qi</first-name>
<department>2659</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">The invention relates to a method of identifying and correcting errors in a noisy binary mask. An object of the present invention is to provide a scheme for improving a binary mask representing speech. The problem is solved in that the method comprises a) providing a noisy binary mask comprising a binary representation of the power density of an acoustic signal comprising a target signal and a noise signal at a predefined number of discrete frequencies and a number of discrete time instances; b) providing a statistical model of a clean binary mask representing the power density of the target signal; and c) using the statistical model to detect and correct errors in the noisy binary mask. This has the advantage of providing an alternative and relatively simple way of improving an estimate of a binary mask representing a speech signal. The invention may e.g. be used for speech processing, e.g. in a hearing instrument.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="88.82mm" wi="116.08mm" file="US08626495-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="178.05mm" wi="115.49mm" file="US08626495-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="172.55mm" wi="110.57mm" file="US08626495-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="166.37mm" wi="117.09mm" file="US08626495-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="218.36mm" wi="145.37mm" file="US08626495-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="184.57mm" wi="137.67mm" file="US08626495-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This nonprovisional application claims the benefit of U.S. Provisional Application No. 61/237,109 filed on Aug. 26, 2009 and to European Patent Application No. 09168699.8 filed on Aug. 26, 2009. The entire contents of all of the above applications are hereby incorporated by reference into the present application.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">TECHNICAL FIELD</heading>
<p id="p-0003" num="0002">The present invention relates to signal processing. The invention relates specifically to a method of identifying and correcting errors in a noisy binary mask. The invention further relates to a data processing system, to a computer readable medium and to a hearing instrument.</p>
<p id="p-0004" num="0003">The invention may e.g. be useful in applications such as speech processing.</p>
<heading id="h-0003" level="1">BACKGROUND ART</heading>
<p id="p-0005" num="0004">The following account of the prior art relates to one of the areas of application of the present invention, hearing aids.</p>
<p id="p-0006" num="0005">Ideal binary masks have shown to be able to increase speech intelligibility significantly. The term &#x201c;ideal&#x201d; is used because knowledge about the clean target speech or clean noise signal must be available in order to calculate the ideal binary masks. In many applications only the noisy speech is available making it difficult to fulfil this requirement. Different concepts of &#x2018;the ideal binary mask&#x2019; are e.g. discussed in [Wang, 2005] and [Kjems et al., 2009]. Calculating the binary masks using noisy speech instead of clean speech introduces errors in the binary masks. The present application relates to a method for correcting these errors.</p>
<heading id="h-0004" level="1">DISCLOSURE OF INVENTION</heading>
<p id="p-0007" num="0006">The general idea is to use a statistical model to identify errors in a binary mask, e.g. representing noisy speech, and to calculate the probability for the units in the binary mask being correct. The statistical model can e.g. be based on a Hidden Markov Model (HMM) (cf. e.g. [Rabiner, 1989]) or Dynamic Time Warping (DTW) (cf. e.g. [Sakoe et al., 1978]), which is trained on binary masks from clean signals (e.g. speech) and possibly from noisy signals (e.g. speech in noise).</p>
<p id="p-0008" num="0007">The term &#x2018;clean signal&#x2019; (e.g. &#x2018;clean speech&#x2019;) is used to indicate the target (e.g. speech) signal alone without any additional (noise) signals. The term &#x2018;noisy signal&#x2019; (e.g. &#x2018;noisy speech&#x2019;) is used to describe a (clean) target (e.g. speech) signal mixed with one or more other signals (termed noise).</p>
<p id="p-0009" num="0008">Trying to correct errors in the binary domain can reduce the complexity of a processing algorithm compared to more traditional methods working on the waveforms or using a time-frequency representation of the noisy signal (e.g. speech). However, the greatly reduced (binary) domain does also make an upper limit on what can be achieved by this method. In the present work, the target binary mask (cf. [Kjems et al., 2009]) is used, but the method can potentially be used on all kinds of binary masks or other binary patterns.</p>
<p id="p-0010" num="0009">An object of the present invention is to provide a scheme for improving a binary mask representing speech (in noise). Other objects are to provide a scheme for improving a binary mask representing other signals or patterns, e.g. optical signals, e.g. in connection with character recognition.</p>
<p id="p-0011" num="0010">Objects of the invention are achieved by the invention described in the accompanying claims and as described in the following.</p>
<p id="p-0012" num="0011">An object of the invention is achieved by a method of identifying and correcting errors in a noisy binary mask. The method comprises</p>
<p id="p-0013" num="0012">a) providing a noisy binary mask comprising a binary representation of the power density of a signal, e.g. an acoustic signal, comprising a target signal mixed with a noise signal at a predefined number of discrete frequencies and a number of discrete time instances;
<br/>
b) providing a statistical model of a clean binary mask representing the power density of the target signal; and
<br/>
c) using the statistical model to detect and correct errors in the noisy binary mask.
</p>
<p id="p-0014" num="0013">The method provides an alternative and relatively simple way of improving an estimate of a binary mask representing a signal (e.g. a speech signal). In an embodiment, the noisy binary mask represents (target) speech in noise. In other embodiments, the binary mask to be improved may represent any other appropriate signal pattern. In an embodiment, the binary mask represents images, e.g. images of characters to be improved in an optical character recognition (OCR) process.</p>
<p id="p-0015" num="0014">The term &#x2018;a noisy binary mask&#x2019; is used to indicate a binary mask determined from a noisy target signal.</p>
<p id="p-0016" num="0015">The statistical model can in general be of any kind appropriate for pattern recognition and comparison. In a particular embodiment, the statistical model is based on Hidden Markov Models.</p>
<p id="p-0017" num="0016">In a particular embodiment, the step of providing a statistical model comprises providing a training set of a clean binary mask comprising a binary representation of power density of the target signal at the predefined number of discrete frequencies and a number of discrete time instances.</p>
<p id="p-0018" num="0017">In a particular embodiment, the step of providing a statistical model comprises providing a training set of a noisy binary mask comprising a binary representation of the power density of the target signal mixed with a noise signal at the predefined number of discrete frequencies and a number of discrete time instances.</p>
<p id="p-0019" num="0018">In a particular embodiment, the statistical model comprises states and observations and wherein the states are constituted by vectors representing the target signal in the binary domain at the predefined number of discrete frequencies at a number of points in time, and wherein the observations are constituted by binary vectors representing the target signal mixed with the noise signal at the predefined number of discrete frequencies at a number of points in time.</p>
<p id="p-0020" num="0019">In a particular embodiment, each state is constituted by a binary vector Q<sub>t </sub>representative of the target signal at the predefined number of discrete frequencies at a given point in time t.</p>
<p id="p-0021" num="0020">In a particular embodiment, the method provides that for each state Q<sub>t </sub>a corresponding observation X<sub>t </sub>is constituted by a vector comprising the probability of a one for each of the predefined number of discrete frequencies, given the state in question.</p>
<p id="p-0022" num="0021">In a particular embodiment, the method provides that the state transition probabilities defining a probability of changing from state Q<sub>t </sub>to Q<sub>t+1 </sub>are provided, and e.g. arranged in a square matrix A with size N&#xd7;N, where N is the total number of states and where the matrix element at the m<sup>th </sup>row and n<sup>th </sup>column in the matrix represents the probability of changing from state m to state n.</p>
<p id="p-0023" num="0022">In a particular embodiment, the observation probabilities are arranged in a matrix B with size F&#xd7;N, where F is the number of frequencies and N is the total number of states and where the matrix element at the p<sup>th </sup>row and q<sup>th </sup>column in the matrix represents the probability of a one at the p<sup>th </sup>frequency of the q<sup>th </sup>state.</p>
<p id="p-0024" num="0023">In a particular embodiment, the method comprises a quantization of the clean binary mask wherein a subset of N<sub>q </sub>states are determined, where N<sub>q</sub>&#x3c;N, each state being constituted by a binary mode vector representative of the target signal at the predefined number of discrete frequencies. Preferably, the number N<sub>q </sub>of states are selected so as to represent the clean binary mask with a minimum of errors. For a given number of states N<sub>q</sub>, the optimal binary mode vectors can e.g. be determined by the K-Means algorithm cf. e.g. [Li, 2005].</p>
<p id="p-0025" num="0024">In a particular embodiment, the state transition probabilities a<sub>ij </sub>of the matrix A are calculated from the quantized binary mask by counting the number n<sub>ij </sub>of state changes Qi to Qj, where i=1, 2, . . . , N<sub>q </sub>and j=1, 2, . . . , N<sub>q </sub>and divide each number n<sub>ij </sub>by the total number N<sub>sc,i </sub>of state changes of the quantized binary mask from state i. In other words, calculate a<sub>ij</sub>=n<sub>ij</sub>/N<sub>sc,i</sub>, where</p>
<p id="p-0026" num="0025">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mrow>
  <mrow>
    <mrow>
      <munderover>
        <mo>&#x2211;</mo>
        <mrow>
          <mi>j</mi>
          <mo>=</mo>
          <mn>1</mn>
        </mrow>
        <msub>
          <mi>N</mi>
          <mi>q</mi>
        </msub>
      </munderover>
      <mo>&#x2062;</mo>
      <msub>
        <mi>n</mi>
        <mi>ij</mi>
      </msub>
    </mrow>
    <mo>=</mo>
    <msub>
      <mi>N</mi>
      <mrow>
        <mi>sc</mi>
        <mo>,</mo>
        <mi>i</mi>
      </mrow>
    </msub>
  </mrow>
  <mo>,</mo>
  <mrow>
    <mrow>
      <mi>so</mi>
      <mo>&#x2062;</mo>
      <mstyle>
        <mspace width="0.8em" height="0.8ex"/>
      </mstyle>
      <mo>&#x2062;</mo>
      <msub>
        <mi>a</mi>
        <mi>ij</mi>
      </msub>
    </mrow>
    <mo>=</mo>
    <mrow>
      <mfrac>
        <msub>
          <mi>n</mi>
          <mi>ij</mi>
        </msub>
        <mrow>
          <munderover>
            <mo>&#x2211;</mo>
            <mrow>
              <mi>j</mi>
              <mo>=</mo>
              <mn>1</mn>
            </mrow>
            <msub>
              <mi>N</mi>
              <mi>q</mi>
            </msub>
          </munderover>
          <mo>&#x2062;</mo>
          <msub>
            <mi>n</mi>
            <mi>ij</mi>
          </msub>
        </mrow>
      </mfrac>
      <mo>.</mo>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
</p>
<p id="p-0027" num="0026">Consequently,</p>
<p id="p-0028" num="0027">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mrow>
  <mrow>
    <munderover>
      <mo>&#x2211;</mo>
      <mrow>
        <mi>j</mi>
        <mo>=</mo>
        <mn>1</mn>
      </mrow>
      <msub>
        <mi>N</mi>
        <mi>q</mi>
      </msub>
    </munderover>
    <mo>&#x2062;</mo>
    <msub>
      <mi>a</mi>
      <mi>ij</mi>
    </msub>
  </mrow>
  <mo>=</mo>
  <mn>1.</mn>
</mrow>
</math>
</maths>
</p>
<p id="p-0029" num="0028">In a particular embodiment, the observation probabilities B(k,n) of the matrix B are calculated based on two contributions B<sub>c </sub>and b<sub>n </sub>according to the formula B(k,n)=B<sub>c</sub>(k,n)+b<sub>n</sub>(k)&#x2212;B<sub>c</sub>(k,n)&#xb7;b<sub>n</sub>(k), where k=1, 2, . . . , F is the frequency index and n=1, 2, . . . , N<sub>q </sub>is the state index, where B<sub>n</sub>(k,n) are the observation probabilities defined by the clean binary mask and the quantized clean binary mask, and where the observation probabilities b<sub>n</sub>(k) defining the probability of ones at each frequency generated by the noise signal.</p>
<p id="p-0030" num="0029">In a particular embodiment, the observation probabilities b<sub>n</sub>(k) are calculated from characteristics of the of the noise signal in question.</p>
<p id="p-0031" num="0030">In a particular embodiment, the observation probabilities b<sub>n</sub>(k) are calculated during time periods where no voice signals are present (e.g. using a voice detector to detect such periods). In an embodiment, this is done in a particular training phase of the statistical model prior to an operational use of the method (e.g. in a hearing instrument). In an embodiment, this is done during operational use of the method, either instead of doing it in a particular training phase or as a supplement or update of the probabilities determined during such training phase.</p>
<p id="p-0032" num="0031">Instead of using a voice detector, the determination of time units where only noise is present can alternatively be made as discussed by [Martin, 2001].</p>
<p id="p-0033" num="0032">In a particular embodiment, the target binary mask TBM<sub>est </sub>is estimated from the following formula:</p>
<p id="p-0034" num="0033">
<maths id="MATH-US-00003" num="00003">
<math overflow="scroll">
<mrow>
  <msub>
    <mi>TBM</mi>
    <mi>est</mi>
  </msub>
  <mo>=</mo>
  <mrow>
        <mo>&#x2062;</mo>
    <mrow>
      <mo>{</mo>
      <mtable>
        <mtr>
          <mtd>
            <mrow>
              <mn>1</mn>
              <mo>;</mo>
              <mrow>
                <mrow>
                  <mi>if</mi>
                  <mo>&#x2062;</mo>
                  <mstyle>
                    <mspace width="0.8em" height="0.8ex"/>
                  </mstyle>
                  <mo>&#x2062;</mo>
                  <mfrac>
                    <mrow>
                      <mrow>
                        <mi>T</mi>
                        <mo>&#x2061;</mo>
                        <mrow>
                          <mo>(</mo>
                          <mrow>
                            <mi>&#x3c4;</mi>
                            <mo>,</mo>
                            <mi>k</mi>
                          </mrow>
                          <mo>)</mo>
                        </mrow>
                      </mrow>
                      <mo>+</mo>
                      <mrow>
                        <mi>M</mi>
                        <mo>&#x2061;</mo>
                        <mrow>
                          <mo>(</mo>
                          <mrow>
                            <mi>&#x3c4;</mi>
                            <mo>,</mo>
                            <mi>k</mi>
                          </mrow>
                          <mo>)</mo>
                        </mrow>
                      </mrow>
                    </mrow>
                    <mrow>
                      <mi>BTH</mi>
                      <mo>&#x2061;</mo>
                      <mrow>
                        <mo>(</mo>
                        <mi>k</mi>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                  </mfrac>
                </mrow>
                <mo>&#x3e;</mo>
                <mn>1</mn>
              </mrow>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mrow>
              <mrow>
                <mn>0</mn>
                <mo>;</mo>
                <mi>otherwise</mi>
              </mrow>
              <mo>,</mo>
            </mrow>
          </mtd>
        </mtr>
      </mtable>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
<br/>
where T(&#x3c4;,k) is the power of the target sound, M(&#x3c4;,k) is the power of the masker sound, and BTH(k) is a threshold value, e.g. the long-term average spectrum of the target sound, and &#x3c4; and k are time and frequency indices, respectively. The masker sound may be any sound that is considered as noise with respect to the target sound. The terms &#x2018;target sound&#x2019; and &#x2018;target signal&#x2019; are used interchangeably. Similarly, the terms &#x2018;masker sound&#x2019; and &#x2018;masker signal&#x2019; and &#x2018;noise signal&#x2019; are used interchangeably.
</p>
<p id="p-0035" num="0034">In a particular embodiment, the observation probabilities b<sub>n</sub>(k) are calculated based on a recordal of the noise signal alone. In an embodiment the observation probabilities b<sub>n</sub>(k) are calculated based on a binary mask determined as indicated by the formula above without any target signal T present (using only the recorded noise or masker signal M) with a threshold BTH determined from the target signal (e.g. a long term average), cf. also <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0036" num="0035">In an embodiment, preliminary observation probabilities b<sub>n</sub>(k) are calculated based on a recordal of the noise signal alone prior to the use of the statistical model and updated during use (e.g. in a hearing instrument), e.g. by calculating observation probabilities b<sub>n</sub>(k) during time periods where no voice signals are present.</p>
<p id="p-0037" num="0036">In an embodiment, the method comprises a step of normalization. Normalization involves e.g. adjusting the threshold to the target voice. This has the advantage of making a binary mask that is more generic (so that for example the binary masks for different voices become more alike).</p>
<p id="p-0038" num="0037">In a particular embodiment, an estimate of a noise-free (or a less noisy) binary mask is determined from the noisy binary mask as the most probable sequence of states (e.g. the most probable clean binary mask) using the Viterbi algorithm (cf. e.g. [Rabiner, 1989]).</p>
<p id="p-0039" num="0038">In a particular embodiment, the most probable sequence of states is calculated based on a number of past, the present, and a number of future observations (e.g. from the noisy binary masks). The more observations considered the more complex processing needs. The use of future observations introduces a processing delay.</p>
<p id="p-0040" num="0039">A tangible computer-readable medium storing a computer program comprising program code means for causing a data processing system to perform at least some (such as a majority or all) of the steps of the method described above, in the detailed description of &#x2018;mode(s) for carrying out the invention&#x2019; and in the claims, when said computer program is executed on the data processing system is furthermore provided. In addition to being stored on a tangible medium such as diskettes, CD-ROM-, DVD-, or hard disk media, or any other machine readable medium, the computer program can also be transmitted via a transmission medium such as a wired or wireless link or a network, e.g. the Internet, and loaded into a data processing system for being executed at a location different from that of the tangible medium.</p>
<p id="p-0041" num="0040">A data processing system comprising a processor and program code means for causing the processor to perform at least some (such as a majority or all) of the steps of the method described above, in the detailed description of &#x2018;mode(s) for carrying out the invention&#x2019; and in the claims is furthermore provided.</p>
<p id="p-0042" num="0041">In an aspect, a hearing instrument or other audio device comprising a data processing system as described above in the detailed description of &#x2018;mode(s) for carrying out the invention&#x2019; and in the claims is furthermore provided. In an embodiment, a hearing instrument comprises a forward path comprising an input transducer for receiving an external acoustic input from the environment, an AD-converter, a processing part implementing said data processing system including for providing time to time-frequency conversion and back, and typically for adapting the signal to the needs of a wearer of the hearing instrument (e.g. by applying a customized frequency dependent gain), a DA-converter (optional) and an output transducer for generating an output perceivable to a wearer of the hearing instrument or other audio device as a sound. In an embodiment, the output transducer comprises one or more speakers for a conventional hearing instrument or other audio device, electrodes for a cochlear implant or vibrators for a bone conduction device for presenting an estimate of an input sound to one or more user's.</p>
<p id="p-0043" num="0042">Further objects of the invention are achieved by the embodiments defined in the dependent claims and in the detailed description of the invention.</p>
<p id="p-0044" num="0043">As used herein, the singular forms &#x201c;a,&#x201d; &#x201c;an,&#x201d; and &#x201c;the&#x201d; are intended to include the plural forms as well (i.e. to have the meaning &#x201c;at least one&#x201d;), unless expressly stated otherwise. It will be further understood that the terms &#x201c;includes,&#x201d; &#x201c;comprises,&#x201d; &#x201c;including,&#x201d; and/or &#x201c;comprising,&#x201d; when used in this specification, specify the presence of stated features, integers, steps, operations, elements, and/or components, but do not preclude the presence or addition of one or more other features, integers, steps, operations, elements, components, and/or groups thereof. It will be understood that when an element is referred to as being &#x201c;connected&#x201d; or &#x201c;coupled&#x201d; to another element, it can be directly connected or coupled to the other element or intervening elements maybe present, unless expressly stated otherwise. Furthermore, &#x201c;connected&#x201d; or &#x201c;coupled&#x201d; as used herein may include wirelessly connected or coupled. As used herein, the term &#x201c;and/or&#x201d; includes any and all combinations of one or more of the associated listed items. The steps of any method disclosed herein do not have to be performed in the exact order disclosed, unless expressly stated otherwise.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading>
<p id="p-0045" num="0044">The invention will be explained more fully below in connection with a preferred embodiment and with reference to the drawings in which:</p>
<p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. 1</figref> illustrates an exemplary method for calculating the target binary mask (TBM), by comparing the mixture with the threshold value BTH(k), e.g. the long-term average spectrum of the target LTAS<sub>&#x3c4;</sub>(k),</p>
<p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. 2</figref> shows an example of the structure of a Hidden Markov Model comprising states (top part) and observation probabilities in each state (bottom part) at three time instants t&#x2212;1, t, t+1,</p>
<p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. 3</figref> shows a representation of a target binary mask with a limited number of binary column vectors (modes),</p>
<p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. 4</figref> shows the 20 modes (left) used in <figref idref="DRAWINGS">FIG. 3(B)</figref> and the corresponding observation matrix B<sub>c </sub>(right),</p>
<p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. 5</figref> shows examples of the quantization of the binary mask with different numbers of modes <figref idref="DRAWINGS">FIGS. 5(A-C)</figref> and different weighting of the quantization <figref idref="DRAWINGS">FIG. 5(C-E)</figref>,</p>
<p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. 6</figref> shows the influence of the number of states N<sub>q </sub>of the model on the number of wrong ones (W<b>1</b>) and wrong zeros (W<b>0</b>) for a given weighting of the quantization, <figref idref="DRAWINGS">FIG. 6</figref><i>a </i>being based on data from <figref idref="DRAWINGS">FIG. 3(C)</figref>, and <figref idref="DRAWINGS">FIGS. 5(A)</figref>, (B), (C) with N<sub>q </sub>between 16 and 64, <figref idref="DRAWINGS">FIG. 6</figref><i>b </i>presenting results for a larger range of states,</p>
<p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. 7</figref> shows an example of error correction using the HMM model, and</p>
<p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. 8</figref> shows an example of the frequency dependence of a threshold value BTH(k) for use in the estimation of the target binary mask.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<p id="p-0054" num="0053">The figures are schematic and simplified for clarity, and they just show details which are essential to the understanding of the invention, while other details are left out. Throughout, the same reference numerals are used for identical or corresponding parts.</p>
<p id="p-0055" num="0054">Further scope of applicability of the present invention will become apparent from the detailed description given hereinafter. However, it should be understood that the detailed description and specific examples, while indicating preferred embodiments of the invention, are given by way of illustration only, since various changes and modifications within the spirit and scope of the invention will become apparent to those skilled in the art from this detailed description.</p>
<heading id="h-0006" level="1">MODE(S) FOR CARRYING OUT THE INVENTION</heading>
<heading id="h-0007" level="1">The Target Binary Mask</heading>
<p id="p-0056" num="0055">The target binary mask (TBM) is e.g. obtained by comparing the energy of the time-frequency representation of the target sound with a frequency dependent threshold (BTH). The comparison is done for each unit in the time-frequency (T-F) representation. If the energy of the target sound exceeds that of the noise sound, the particular T-F unit is marked with a one in the binary mask and otherwise zero. Ones in the binary mask represent regions in time and frequency that should be kept. Instead of comparing the target sound with the noise sound, the comparison can be done directly using the threshold value BTH(k):</p>
<p id="p-0057" num="0056">
<maths id="MATH-US-00004" num="00004">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>TBM</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mi>&#x3c4;</mi>
              <mo>,</mo>
              <mi>k</mi>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mo>{</mo>
          <mtable>
            <mtr>
              <mtd>
                <mrow>
                  <mn>1</mn>
                  <mo>;</mo>
                  <mrow>
                    <mrow>
                      <mi>if</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.8em" height="0.8ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <mfrac>
                        <mrow>
                          <mi>T</mi>
                          <mo>&#x2061;</mo>
                          <mrow>
                            <mo>(</mo>
                            <mrow>
                              <mi>&#x3c4;</mi>
                              <mo>,</mo>
                              <mi>k</mi>
                            </mrow>
                            <mo>)</mo>
                          </mrow>
                        </mrow>
                        <mrow>
                          <mi>BTH</mi>
                          <mo>&#x2061;</mo>
                          <mrow>
                            <mo>(</mo>
                            <mi>k</mi>
                            <mo>)</mo>
                          </mrow>
                        </mrow>
                      </mfrac>
                    </mrow>
                    <mo>&#x3e;</mo>
                    <mn>1</mn>
                  </mrow>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mrow>
                  <mn>0</mn>
                  <mo>;</mo>
                  <mi>otherwise</mi>
                </mrow>
              </mtd>
            </mtr>
          </mtable>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>1</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where T(&#x3c4;,k) is the power of the target sound, BTH(k) is a threshold value, e.g. the long-term average spectrum of the target sound (e.g. obtained as the time averaged spectrum for one speaker or the time averaged spectrum extracted from a multitude of speakers), &#x3c4; is a time index, and k is a frequency index. The threshold value controls the number of ones in the TBM. A high threshold value gives a very sparse mask with few ones, whereas a low threshold value gives a very dense mask with many ones. The target binary mask has been shown to be able to increase speech intelligibility significantly when applied to noisy speech [Kjems et al., 2009].
</p>
<p id="p-0058" num="0057">In an embodiment, the threshold value BTH(k) is dependent on the long-term average spectrum (LTAS) of the target signal. The long-term average spectrum (LTAS) of a speech signal can e.g. be obtained by identifying time units where only noise is present (e.g. during periods of silence (no speech)), e.g. using a voice detector, estimating the noise level in the various frequency bands, and subtracting the estimated noise levels in a given time unit from the preceding time-frequency units comprising speech (e.g. assuming that the noise level is constant between the identified silent time units). The thus noise-corrected speech containing time units can then be averaged over time to provide a frequency dependent (time-invariant) LTAS(k). The determination of time units where only noise is present can alternatively be made as discussed by [Martin, 2001]. Alternatively, the long-term average spectrum of a speech signal can be determined as discussed in [Byrne et al., 1994],</p>
<p id="h-0008" num="0000">Estimation of the TBM:</p>
<p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. 1</figref> illustrates an exemplary method for calculating the target binary mask (TBM), by comparing the mixture with the threshold value BTH(k), e.g. the long-term average spectrum of the target LTAS<sub>&#x3c4;</sub>(k).</p>
<p id="p-0060" num="0059">If no masker (noise) sound is present, the target binary mask can be estimated (TBM<sub>est </sub>or (&#x201c;TBM&#x201d;(&#x201c;&#x3c4;,k&#x201d;))) without errors using the block diagram shown in <figref idref="DRAWINGS">FIG. 1</figref>. If a masker sound is present, Equation 1 changes to</p>
<p id="p-0061" num="0060">
<maths id="MATH-US-00005" num="00005">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
                <mo>=</mo>
        <mrow>
          <mo>{</mo>
          <mtable>
            <mtr>
              <mtd>
                <mrow>
                  <mn>1</mn>
                  <mo>;</mo>
                  <mrow>
                    <mrow>
                      <mi>if</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.8em" height="0.8ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <mfrac>
                        <mrow>
                          <mrow>
                            <mi>T</mi>
                            <mo>&#x2061;</mo>
                            <mrow>
                              <mo>(</mo>
                              <mrow>
                                <mi>&#x3c4;</mi>
                                <mo>,</mo>
                                <mi>k</mi>
                              </mrow>
                              <mo>)</mo>
                            </mrow>
                          </mrow>
                          <mo>+</mo>
                          <mrow>
                            <mi>M</mi>
                            <mo>&#x2061;</mo>
                            <mrow>
                              <mo>(</mo>
                              <mrow>
                                <mi>&#x3c4;</mi>
                                <mo>,</mo>
                                <mi>k</mi>
                              </mrow>
                              <mo>)</mo>
                            </mrow>
                          </mrow>
                        </mrow>
                        <mrow>
                          <mi>BTH</mi>
                          <mo>&#x2061;</mo>
                          <mrow>
                            <mo>(</mo>
                            <mi>k</mi>
                            <mo>)</mo>
                          </mrow>
                        </mrow>
                      </mfrac>
                    </mrow>
                    <mo>&#x3e;</mo>
                    <mn>1</mn>
                  </mrow>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mrow>
                  <mn>0</mn>
                  <mo>;</mo>
                  <mi>otherwise</mi>
                </mrow>
              </mtd>
            </mtr>
          </mtable>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>2</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where M(&#x3c4;,k) is the power of the masker sound, if we assume that the two signals are uncorrelated. Preferably, the target and masker signals are uncorrelated. Obviously, the masker sound will introduce errors in the TBM depending on the energy of the masker signal. The masker signal M(&#x3c4;,k) can represent one or more different noise types, e.g. other speakers, babble noise, reverberation, environmental noise, or algorithm noise. The noise can be of a stationary or a time variant nature. In connection with the determination of the target binary mask, stationary noise is preferably dealt with by adjusting the threshold BTH used in creating an estimate of the target binary mask TBM<sub>est </sub>(e.g. by adjusting the BTH relative to an estimate of the stationary part of the noise), whereas time variant noise is dealt with by the error correction algorithm (working on the TBM<sub>est</sub>).
</p>
<p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. 8</figref> shows an example of the frequency dependence of a threshold value BTH(k) for use in the estimation of the target binary mask. The graph schematically illustrates the frequency dependence between a minimum and a maximum frequency (f<sub>min</sub>, f<sub>max</sub>) of the magnitude of exemplary threshold values BTH<sub>1</sub>, BTH<sub>2</sub>, BTH<sub>3 </sub>for calculating a target binary mask TBN(&#x3c4;,k), cf. e.g. formula (1) or (2) above (cf. also <figref idref="DRAWINGS">FIG. 1</figref>). The different threshold values BTH<sub>i </sub>can be used to control the number of ones in the target binary mask. Preferably, they differ from each other only by a (frequency-independent) value &#x394;BTH (in a logarithmic description). In other words, the different threshold values used in the determination of the target binary mask have the same frequency dependence. In general, the threshold value is relatively high at intermediate frequencies (e.g. between 100 Hz and 2 KHz, such as between 250 Hz and 500 Hz) with a maximum at an intermediate frequency f<sub>m </sub>and relatively low outside this intermediate range, cf. e.g. [Byrne et al., 1994]. Preferably, the number of ones in the target binary mask is larger than 7% of the total number of time-frequency units constituting the binary mask. In a particular embodiment, the number of ones in the target binary mask is larger than 10%, such as larger than 15%. In a particular embodiment, the number of ones in the target binary mask is arranged to be in the range from 5% to 20%, such as in the range from 8% to 12%. Preferably, the threshold value BTH is estimated based on the long term average LTAS<sub>&#x3c4;</sub>(k) of the target signal, i.e. an average spectrum of the target signal over time. Typically, an LTAS-value is based on an average of different voices (e.g. comprising male and female voices). In an embodiment, time variant noise is detected (cf. e.g. [Martin, 2001]) and time-frequency units comprising such time variant noise is excluded from the determination of the threshold value BTH. In an embodiment, the BTH value is adapted to a particular persons voice (e.g. to an expected frequent conversation partner of a hearing aid user). The contribution b<sub>n </sub>to the observation probabilities matrix B from the masker sound can e.g. be dynamically calculated with a view to the above illustrated frequency dependence of the threshold value BTH(k). For example, in case a low frequency noise source (e.g. background traffic noise, e.g. from cars) is detected during use of the method (e.g. in an audio processing device, such as a hearing instrument, in operation), the b<sub>n </sub>values at relatively low frequencies f<sub>low </sub>can be increased (compared to preset average values) whereas they can be decreased at relatively higher frequencies f<sub>high</sub>. The opposite change of the b<sub>n </sub>values can advantageously be made, in case a high frequency noise source (e.g. machine noise) is detected. Relatively low and relatively high frequencies are considered in view of the audio frequency range considered by the application in question, e.g. the human audible frequency range (20 Hz to 20 kHz) or a sub-range thereof.</p>
<p id="h-0009" num="0000">Error Correction of TBM<sub>est </sub>Using Hidden Markov Models:</p>
<p id="p-0063" num="0062">A Hidden Markov Model (HMM) is a statistical model based on a Markov process which is hidden to the observer. In each state of the Markov process an observation is available, and from a sequence of observations it is possible to calculate the most probable state sequence. In the present use, the states in the HMM are the clean TBM (no masker sound) and the observations are the noisy TBM (with masker sound). From the noisy TBM it is possible to calculate the most probable clean TBM within the limitations of the HMM model. Each state in the HMM is a binary column vector with size equal to the number of frequency channels. This binary column vector describes the binary mask in the F frequency channels at time t. The observation vectors have the same size, but instead of binary numbers, the observation vectors have numbers between zero and one with the probability of a one given the state (see <figref idref="DRAWINGS">FIG. 2</figref>).</p>
<p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. 2</figref> shows an example of the structure of a Hidden Markov Model comprising states (top part) and observation probabilities in each state (bottom part) at three time instants t&#x2212;1, t, t+1. The state vectors Q<sub>t&#x2212;1</sub>, Q<sub>t</sub>, Q<sub>t+1 </sub>contain the clean binary mask of the target signal. The observation vectors contain X<sub>t&#x2212;1</sub>, X<sub>t</sub>, X<sub>t+1 </sub>contain the probability of ones, given the states Q<sub>t&#x2212;1</sub>, Q<sub>t</sub>, Q<sub>t+1</sub>, respectively.</p>
<p id="p-0065" num="0064">This type of HMM is a continuous HMM with discrete observation densities. At each time increment from t to t+1 there is a probability of changing from state Q<sub>t </sub>to Q<sub>t+1</sub>. These state transition probabilities are the elements in the matrix A which is a square matrix with size N&#xd7;N. This matrix describes the probability of changing from one state to another, and the number at the m'th row and n'th column in the matrix is the probability of changing from state m to state n. The observation vectors are arranged in a matrix B called the observation probability with F rows and N columns. Each column in the matrix corresponds to different states and each row corresponds to different frequency channels. The elements in the matrix represent the probability of ones. The last parameter of the HMM is the initial state distribution with the probabilities of starting in the different states. If we restrict the binary mask to start with an all-zero binary column vector, this parameter can be neglected. Potentially, the number of different binary column vectors in the TBM is equal to 2F (and with that an A matrix of size 2F&#xd7;2F). This is unmanageable, so the number of different binary column vectors must be limited. This is done by finding the N<sub>q </sub>binary column vectors, called modes, which best represents the binary mask (cf. <figref idref="DRAWINGS">FIG. 3</figref> and <figref idref="DRAWINGS">FIG. 4</figref>). The N<sub>q </sub>modes are the states of the HMM model and increasing N<sub>q </sub>decreases the number of errors introduced by representing the binary mask with the N<sub>q </sub>modes/states (cf. <figref idref="DRAWINGS">FIGS. 5(A)</figref>, <b>5</b>(B) and <b>5</b>(C) and <figref idref="DRAWINGS">FIG. 6</figref><i>a</i>). The process of representing the binary mask with a limited number of states is referred to as quantization of the binary mask.</p>
<heading id="h-0010" level="1">EXAMPLE</heading>
<p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. 3</figref> shows a representation of a target binary mask with a limited number of binary column vectors (modes). Each binary mask is in this example represented by F=32 frequency channels (the vertical axis denoted Frequency index [k] and with delimiting indices (channel numbers) <b>1</b>, <b>8</b>, <b>16</b>, <b>32</b>) and N<sub>Total</sub>=450 time units (the horizontal axis representing time denoted Time [&#x3c4;] and with delimiting indices <b>50</b>, <b>100</b>, <b>150</b>, <b>200</b>, <b>250</b>, <b>300</b>, <b>350</b>, <b>400</b>). The same legend is used in the binary masks shown in <figref idref="DRAWINGS">FIGS. 5 and 6</figref>. In this example, N<sub>q</sub>=20 modes are used. The image of <figref idref="DRAWINGS">FIG. 3(A)</figref> denoted Target Binary Mask is the original target binary mask, where black units represent zeros and white units represents ones. The image of <figref idref="DRAWINGS">FIG. 3(B)</figref> denoted Target Binary Mask represented using 20 binary column vectors (modes) shows the target mask of <figref idref="DRAWINGS">FIG. 3(A)</figref> represented with 20 modes found using the K-mode algorithm, where again black represents zero and white represents one. The image of <figref idref="DRAWINGS">FIG. 3(C)</figref> denoted Comparison of (A) and (B) shows the errors introduced from (A) to (B), where white is correct ones (C<b>1</b>), dark grey represent wrong ones (W<b>1</b>), black is correct zeros (C<b>0</b>), and light grey represents wrong zeros (W<b>0</b>). The same legend is used in <figref idref="DRAWINGS">FIGS. 5 and 7</figref>. The legend counts the number of units in the four categories yielding 2155 correct ones (C<b>1</b>), 318 wrong ones (W<b>1</b>), 11235 correct zeros (C<b>0</b>) and 404 wrong zeros (W<b>0</b>). In <figref idref="DRAWINGS">FIG. 4</figref> the 20 modes are shown.</p>
<p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. 4</figref> shows the 20 modes (left) used in <figref idref="DRAWINGS">FIG. 3(B)</figref> and the corresponding observation matrix B<sub>c </sub>(right). The left graph shows the 20 modes used to quantize the binary target mask in the form of N<sub>q</sub>=20 binary vectors Q<sub>1</sub>, Q<sub>2</sub>, . . . , Q<sub>20 </sub>each comprising binary values (zero or one) of the vector elements q<sub>i1</sub>, q<sub>i2</sub>, . . . , (i=1, 2, . . . , 20) at the different frequencies f<sub>1</sub>, f<sub>2</sub>, . . . , f<sub>F </sub>(F=32 in the example of <figref idref="DRAWINGS">FIGS. 3</figref>, <b>4</b>, <b>5</b>, <b>7</b>). A vector element q<sub>ij </sub>equal to zero is represented in the left graph by a black element, a one by a white element. When the clean target binary mask of <figref idref="DRAWINGS">FIG. 3(A)</figref> is approximated using only the 20 modes of the left graph of <figref idref="DRAWINGS">FIG. 4</figref> (denoted Modes) and leading to the mask in <figref idref="DRAWINGS">FIG. 3(B)</figref>, each of the 450 column vectors is approximated by a particular one of the N<sub>q</sub>=20 mode vectors. Each of the 20 mode vectors Q<sub>r </sub>(r=1, 2, . . . , 20) are used as an approximation for a given number N<sub>r </sub>of the N<sub>Total</sub>=450 column vectors</p>
<p id="p-0068" num="0067">
<maths id="MATH-US-00006" num="00006">
<math overflow="scroll">
<mrow>
  <mrow>
    <mo>(</mo>
    <mrow>
      <mrow>
        <munderover>
          <mo>&#x2211;</mo>
          <mrow>
            <mi>r</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mn>20</mn>
        </munderover>
        <mo>&#x2062;</mo>
        <msub>
          <mi>N</mi>
          <mi>r</mi>
        </msub>
      </mrow>
      <mo>=</mo>
      <mn>450</mn>
    </mrow>
    <mo>)</mo>
  </mrow>
  <mo>.</mo>
</mrow>
</math>
</maths>
<br/>
The right graph (denoted Observation matrix) graphically illustrates for each of the 20 mode vectors of the left graph the relative number of times p<sub>rj </sub>(r=1, 2, . . . , N<sub>q</sub>=20, j=1, 2, . . . , F=32) a given binary vector element (time-frequency unit) s<sub>rj </sub>of the clean binary mask deviates from the value q<sub>rj </sub>in the mode vector Q<sub>r</sub>, it is approximated to. In other words p<sub>rj</sub>=&#x394;n<sub>rj</sub>/N<sub>r</sub>, where &#x394;n<sub>rj </sub>is the absolute number of times the vector element s<sub>rj </sub>deviates from q<sub>rj</sub>. The right graph thus represents the observation matrix B<sub>c </sub>comprising observation probabilities p<sub>rj </sub>defined by the clean binary mask (<figref idref="DRAWINGS">FIG. 3(A)</figref>) and the quantized clean binary mask (<figref idref="DRAWINGS">FIG. 3(B)</figref>). The observation probabilities are indicated by a corresponding gray scaling. A legend is provided to the right of the graph, a probability of 0 being represented by black, a probability of 1 being represented by white, probabilities between 0 and 1 being indicated in steps of 0.1 by decreasing gray scaling from predominantly black to predominantly white. A fully white time-frequency unit thus indicates that the probability of a correctly estimated 1 is high. The observation probabilities indicate the probability of a 1 in the estimate of the target binary mask in the time-frequency unit in question. A fully black time-frequency unit thus indicates that the probability of a correctly estimated 0 is high. A time-frequency unit having a gray shading midway between black and white (e.g. as indicated by the legend of 0.4-0.6) thus represents a time-frequency unit for which the estimate of the target signal has a low confidence.
</p>
<p id="p-0069" num="0068"><figref idref="DRAWINGS">FIG. 5</figref> shows examples of the quantization of the binary mask with different numbers of modes <figref idref="DRAWINGS">FIG. 5(A-C)</figref> and different weighting of the quantization <figref idref="DRAWINGS">FIG. 5(C-E)</figref>.</p>
<p id="p-0070" num="0069">To find the N<sub>q </sub>modes, the K-mode algorithm is used (cf. e.g. [Huang, 1997]). This algorithm is the binary version of the widely used K-means algorithm. The result from using K-mode is N<sub>q </sub>binary column vectors which minimizes the error when the binary mask is quantized. The error can be of two types: wrong ones (W<b>1</b>), when correct zeros (C<b>0</b>) are changed to ones, and wrong zeros (W<b>0</b>) when correct ones (C<b>1</b>) are changed to zeros. It is not certain if one type of error is favourable to the other and how the errors will impact sound quality, intelligibility, noise, etc.</p>
<p id="p-0071" num="0070"><figref idref="DRAWINGS">FIG. 5(A)</figref> denoted Quantized Binary Mask using 16 modes shows the target binary mask of <figref idref="DRAWINGS">FIG. 3(A)</figref> represented by N<sub>q</sub>=16 modes. This quantization yields 2060 correct ones (C<b>1</b>), 334 wrong ones (W<b>1</b>), 11219 correct zeros (C<b>0</b>) and 499 wrong zeros (W<b>0</b>).</p>
<p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. 5(B)</figref> denoted Quantized Binary Mask using 32 modes shows the target binary mask of <figref idref="DRAWINGS">FIG. 3(A)</figref> represented by N<sub>q</sub>=32 modes. This quantization yields 2263 correct ones (C<b>1</b>), 252 wrong ones (W<b>1</b>), 11301 correct zeros (C<b>0</b>) and 296 wrong zeros (W<b>0</b>).</p>
<p id="p-0073" num="0072"><figref idref="DRAWINGS">FIG. 5(C)</figref> denoted Quantized Binary Mask using 64 modes shows the target binary mask of <figref idref="DRAWINGS">FIG. 3(A)</figref> represented by N<sub>q</sub>=64 modes. This quantization yields 2400 correct ones (C<b>1</b>), 197 wrong ones (W<b>1</b>), 11356 correct zeros (C<b>0</b>) and 159 wrong zeros (W<b>0</b>).</p>
<p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. 6</figref><i>a </i>shows the influence of the number of states of the model on the number of wrong ones (W<b>1</b>) and wrong zeros (W<b>0</b>) for a given weighting of the quantization. In the 16 states model the ratio of wrong to correct ones is W<b>1</b>/C<b>1</b>=334/2060=16%, which decreases to W<b>1</b>/C<b>1</b>=197/2400=8.2% in the 64 states model. Correspondingly, in the 16 states model the ratio of wrong to correct zeros is W<b>0</b>/C<b>0</b>=499/11219=4.4%, which decreases to W<b>0</b>/C<b>0</b>=159/11356=1.4% in the 64 states model. Although the number of wrong ones decreases from 334 to 197 (41%), and the number of wrong ones decreases from 499 to 159 (68%), the improvement in the number of model states from 16 to 64 is relatively modest considering the increase in processing complexity, from handling 16*16 matrices to handling 64*64 matrices.</p>
<p id="p-0075" num="0074"><figref idref="DRAWINGS">FIG. 6</figref><i>b </i>shows the number of erroneous unit (total wrongs, wrong ones, wrong zeros) vs. the number of modes N<sub>q </sub>from 16, 32 to 4096 (i.e. powers of 2 from 2<sup>4 </sup>to 2<sup>12</sup>). The vertical axis indicates the number of erroneous units in a range from 0 to 9*10<sup>5 </sup>(linear scale). The horizontal axis indicates the number of states N<sub>q </sub>of the statistical model in a range from 4 to 12 (log 2-scale, i.e. corresponding to N<sub>q</sub>=2<sup>4</sup>, 2<sup>5</sup>, . . . , 2<sup>12 </sup>modes). Graphs W<b>0</b> indicate wrong zeros. Graphs W<b>1</b> indicate wrong ones. Graphs W<b>0</b>+W<b>1</b> indicate the total number of wrong units (wrong zeros plus wrong ones). Data points of the graphs for a particular number of states N<sub>q </sub>are determined by 1) using the K-means algorithm to find the N<sub>q </sub>most appropriate mode vectors and 2) using the N<sub>q </sub>most frequently occurring mode vectors in the target binary mask, respectively, as indicated in <figref idref="DRAWINGS">FIG. 6</figref><i>b </i>by terms K-means (solid graphs) and Most frequent (dotted graphs), respectively. As expected, the K-means algorithm provides the lowest total number of erroneous units.</p>
<p id="p-0076" num="0075">In an embodiment, a weighting in the K-mode algorithm accepting wrong ones in favour of wrong zeros or vice versa (<figref idref="DRAWINGS">FIGS. 3(C)</figref>, <b>3</b>(D), and <b>3</b>(E)) is provided. The weighting in the K-mode algorithm is governed by changing the relative cost of a wrong one compared to a wrong zero.</p>
<p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. 5(D)</figref> denoted Quantized Binary Mask using 64 modes. Weighting with few wrong zeros shows the target binary mask of <figref idref="DRAWINGS">FIG. 3(A)</figref> represented by N<sub>q</sub>=64 modes and a weighting function providing relatively few wrong zeros. This quantization and weighting yields 2520 correct ones (C<b>1</b>), 455 wrong ones (W<b>1</b>), 11098 correct zeros (C<b>0</b>) and 39 wrong zeros (W<b>0</b>).</p>
<p id="p-0078" num="0077"><figref idref="DRAWINGS">FIG. 5(E)</figref> denoted Quantized Binary Mask using 64 modes. Weighting with few wrong ones shows the target binary mask of <figref idref="DRAWINGS">FIG. 3(A)</figref> represented by N<sub>q</sub>=64 modes and a weighting function providing relatively few wrong ones. This quantization and weighting yields 2050 correct ones (C<b>1</b>), 8 wrong ones (W<b>1</b>), 11545 correct zeros (C<b>0</b>) and 509 wrong zeros (W<b>0</b>).</p>
<p id="p-0079" num="0078">An appropriate weighting is dependent on the application. The weighting to choose in a particular situation is a compromise between loss of target units (too many wrong zeros) and too many masker/noise units (too many wrong ones). Too many wrong ones may introduce artefacts in the output signal, whereas too many wrong zeros may reduce the quality of the target signal.</p>
<p id="p-0080" num="0079">From the quantized binary mask the state transition probability matrix A is calculated by counting the number of state changes. The state transition probabilities a<sub>ij </sub>of the matrix A are calculated from the quantized binary mask by counting the number n<sub>ij </sub>of state changes Qi to Qj, where i=1, 2, . . . , N<sub>q </sub>and j=1, 2, . . . , N<sub>q </sub>and divide each number n<sub>ij </sub>by the total number N<sub>sc,i </sub>of state changes of the quantized binary mask from state i.</p>
<p id="p-0081" num="0080">To calculate the observation probabilities B, the first step is to compare the quantized binary mask with the original binary mask. If no errors were introduced by the quantization, the columns of the B matrix would be the N modes, but errors are introduced which means that the values in the B matrix are pulled towards 0.5. As an example, consider <b>10</b> (cf. N<sub>r </sub>above) binary column vectors (cf. s<sub>rj</sub>, j=1, 2, . . . , F above) which are quantized to the same mode (r). In 3 (cf. &#x394;n<sub>r5 </sub>above) of the 10 binary column vectors, a one is found in row 5 (i.e. in element s<sub>r5 </sub>of the quantized mask), which are lost due to the quantization. This means that the observation vector belonging to the mode will have a value of 3/10 in row 5 (p<sub>r5</sub>=&#x394;n<sub>r5</sub>/N<sub>r</sub>=0.3 in the above notation).</p>
<p id="p-0082" num="0081">The observation probabilities B calculated by comparing the original binary mask with the quantized binary mask is termed B<sub>c </sub>(c for clean). The second step to calculate the observation probabilities matrix B is the contribution from the masker sound or noise termed b<sub>n</sub>. This contribution is independent of B<sub>c </sub>and the states (assuming independence of the two sources), so b<sub>n</sub>, is a column vector of size F describing the probability of ones in each frequency band generated by the masker sound (noise).</p>
<p id="p-0083" num="0082">To calculate the final matrix B, the two contributions are combined:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>B</i>(<i>k,n</i>)=<i>B</i><sub>c</sub>(<i>k,n</i>)+<i>b</i><sub>n</sub>(<i>k</i>)&#x2212;<i>B</i><sub>c</sub>(<i>k,n</i>)&#xb7;<i>b</i><sub>n</sub>(<i>k</i>)&#x2003;&#x2003;(3)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
for all k and n, where k=1, 2, . . . , F is the frequency index and n=1, 2, . . . , N<sub>q </sub>is the state index.
</p>
<p id="p-0084" num="0083">To calculate the parameters for the HMM model a training set of clean and noisy binary masks must be available. It is noted that the clean binary masks are only necessary while training the statistical model. After the parameters of the HMM model have been calculated, the most probable sequence of states can be found from the noisy binary mask using the Viterbi algorithm. This sequence is an estimate of the clean binary mask.</p>
<p id="p-0085" num="0084">In a particular embodiment, the observation probabilities b<sub>n</sub>(k) are calculated during time periods where no voice signals are present. In an embodiment the observation probabilities b<sub>n</sub>(k) are calculated based on a binary mask determined as indicated by equation (2) above without any target signal T present (using only the recorded noise or masker signal M) with a threshold BTH determined from the target signal (e.g. a long term average), cf. also <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0086" num="0085">The Viterbi algorithm calculates the most probably states given the past, present and future observations. This means that the algorithm is non-causal and will introduce a delay because we use a look-ahead determined by the number of future observations used in the Viterbi algorithm. If the algorithm should be causal, only the past and present observation can be used to calculate the most probably state. Decreasing the look-ahead will reduce the performance of the algorithm.</p>
<p id="p-0087" num="0086"><figref idref="DRAWINGS">FIG. 7</figref> shows an example of error correction using the HMM model.</p>
<p id="p-0088" num="0087"><figref idref="DRAWINGS">FIG. 7(A)</figref> denoted Noisy Target Binary Mask shows a noisy target binary mask represented by N<sub>q</sub>=1024 modes. This quantization yields 1839 correct ones (C<b>1</b>), 1345 wrong ones (W<b>1</b>), 10176 correct zeros (C<b>0</b>) and 48 wrong zeros (W<b>0</b>).</p>
<p id="p-0089" num="0088"><figref idref="DRAWINGS">FIG. 7(B)</figref> denoted Error corrected Target Binary Mask shows the target binary mask resulting from the noisy target binary mask of <figref idref="DRAWINGS">FIG. 7(A)</figref> after being corrected with the method described in the present application. The corrected target binary mask yields 1470 correct ones (C<b>1</b>), 503 wrong ones (W<b>1</b>), 11018 correct zeros (C<b>0</b>) and 417 wrong zeros (W<b>0</b>).</p>
<p id="p-0090" num="0089">The amount of wrong ones (W<b>1</b>) is reduced from (A) to (B) by 63% ((1345&#x2212;503)/1345), but at the expense of a decrease in the amount of correct ones (C<b>1</b>) by 20% ((1839&#x2212;1470)/1839) and an increase in the number of wrong zeros (W<b>0</b>). However, the total number of wrong units (W<b>1</b>+W<b>0</b>) has in the present example decreased by 34% ((1345+48&#x2212;(503+417))/(1345+48)). In other words, the error corrected mask is significantly closer to the target binary mask than the noisy target binary mask.</p>
<p id="p-0091" num="0090">The number of &#x2018;new&#x2019; wrong zeros depends on the number of states of the model. The more states, the less wrong zeros are present after error correction.</p>
<p id="p-0092" num="0091">To summarize, the HMM for binary mask error correction is described by the following parameters:
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0092">F, the number of frequency channels (k is a frequency index, 1, 2, . . . , F).</li>
        <li id="ul0002-0002" num="0093">N<sub>q</sub>, the number of states or modes to represent the binary mask (n is a state index, 1, 2, . . . , N<sub>q</sub>). The terms states and modes are used interchangeably in the present application.</li>
        <li id="ul0002-0003" num="0094">A, the state transition probabilities having size N<sub>q</sub>&#xd7;N<sub>q</sub>.</li>
        <li id="ul0002-0004" num="0095">B, the observation probabilities with size F&#xd7;N<sub>q</sub>.</li>
        <li id="ul0002-0005" num="0096">&#x3c0;, the initial state distribution.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0093" num="0097">The invention is defined by the features of the independent claim(s). Preferred embodiments are defined in the dependent claims. Any reference numerals in the claims are intended to be non-limiting for their scope.</p>
<p id="p-0094" num="0098">Some preferred embodiments have been shown in the foregoing, but it should be stressed that the invention is not limited to these, but may be embodied in other ways within the subject-matter defined in the following claims. The framework of the present invention has been audio processing. It may, however, be used in other signal processing scenarios comprising a clean and a noisy target signal where the clean target signal should be estimated from the noisy target signal, for example clean-up of black and white images (e.g. photos), recognition of hand writing, etc.</p>
<heading id="h-0011" level="1">REFERENCES</heading>
<p id="p-0095" num="0000">
<ul id="ul0003" list-style="none">
    <li id="ul0003-0001" num="0099">[Wang, 2005] Wang, D. <i>On ideal binary mask as the computational goal of auditory scene analysis</i>, Divenyi P (ed): Speech Separation by Humans and Machines, pp. 181-197 (Kluwer, Norwell, Mass., 2005).</li>
    <li id="ul0003-0002" num="0100">[Kjems et al., 2009] Ulrik Kjems, Jesper B. Boldt, Michael S. Pedersen, Thomas Lunner, and DeLang Wang, <i>Role of mask pattern in intelligibility of ideal binary</i>-<i>masked noisy speech, JASA </i>(in the press).</li>
    <li id="ul0003-0003" num="0101">[Rabiner, 1989] L. R. Rabiner, A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition, Proceedings of the IEEE, Vol. 77, No. 2, February 1989, pp. 257-286.</li>
    <li id="ul0003-0004" num="0102">[Sakoe et al., 1978] Hiroaki Sakoe and Seibi Chiba, <i>Dynamic programming algorithm optimization for spoken word recognition</i>, IEEE Trans. Acoust., Speech, Signal Processing, Vol. 26, pp. 43-49, February 1978.</li>
    <li id="ul0003-0005" num="0103">[Li, 2005] Tao Li, <i>A General Model for Clustering Binary Data</i>, KDD'05, Aug. 21-24, 2005, Chicago, Ill., USA, pp. 188-197.</li>
    <li id="ul0003-0006" num="0104">[Martin, 2001] Rainer Martin, <i>Noise Power Spectral Density Estimation Based on Optimal Smoothing and Minimum Statistics</i>, IEEE Transactions on Speech and Audio Processing, Vol. 9, No. 5, July 2001, pp. 501-512.</li>
    <li id="ul0003-0007" num="0105">[Byrne et al., 1994] Dennis Byrne et al., <i>An international comparison of long</i>-<i>term average speech</i>, J. Acoust. Soc. Am., Vol. 96, No. 4, October 1994, pp. 2108-2120.</li>
    <li id="ul0003-0008" num="0106">[Huang, 1997] Zhexue Huang, <i>Clustering large data sets with mixed numeric and categorical values</i>, in The First Pacific-Asia Conference on Knowledge Discovery and Data Mining, 1997, pp. 21-34.</li>
</ul>
</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08626495-20140107-M00001.NB">
<img id="EMI-M00001" he="13.04mm" wi="76.20mm" file="US08626495-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US08626495-20140107-M00002.NB">
<img id="EMI-M00002" he="9.91mm" wi="76.20mm" file="US08626495-20140107-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00003" nb-file="US08626495-20140107-M00003.NB">
<img id="EMI-M00003" he="10.58mm" wi="76.20mm" file="US08626495-20140107-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00004" nb-file="US08626495-20140107-M00004.NB">
<img id="EMI-M00004" he="10.58mm" wi="76.20mm" file="US08626495-20140107-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00005" nb-file="US08626495-20140107-M00005.NB">
<img id="EMI-M00005" he="10.58mm" wi="76.20mm" file="US08626495-20140107-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00006" nb-file="US08626495-20140107-M00006.NB">
<img id="EMI-M00006" he="8.81mm" wi="76.20mm" file="US08626495-20140107-M00006.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00007" nb-file="US08626495-20140107-M00007.NB">
<img id="EMI-M00007" he="10.58mm" wi="76.20mm" file="US08626495-20140107-M00007.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method of identifying and correcting errors in a noisy binary mask, the method comprising
<claim-text>a) providing a noisy binary mask comprising a binary representation of the power density of an acoustic signal comprising a target signal mixed with a noise signal at a predefined number of discrete frequencies and a number of discrete time instances;</claim-text>
<claim-text>b) providing a statistical model of a clean binary mask representing the power density of the target signal; and</claim-text>
<claim-text>c) using the statistical model by a processor to detect and correct errors in the noisy binary mask.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the statistical model is based on Hidden Markov Models.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the step of providing a statistical model comprises providing a training set of a clean binary mask comprising a binary representation of power density of the target signal at the predefined number of discrete frequencies and a number of discrete time instances.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the step of providing a statistical model comprises providing a training set of a noisy binary mask comprising a binary representation of the power density of a mixture of the target signal and a noise signal at the predefined number of discrete frequencies and a number of discrete time instances.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the statistical model comprises states and observations and wherein the states are constituted by vectors representing the target signal at the predefined number of discrete frequencies at a number of points in time, and wherein the observations are constituted by vectors representing the target signal mixed with the noise signal at the predefined number of discrete frequencies at a number of points in time.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. A method according to <claim-ref idref="CLM-00005">claim 5</claim-ref> wherein each state is constituted by a binary vector Q<sub>t </sub>representative of the target signal at the predefined number of discrete frequencies in a given time unit t.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. A method according to <claim-ref idref="CLM-00005">claim 5</claim-ref> wherein for each state Q<sub>t </sub>a corresponding observation X<sub>t </sub>is constituted by a vector comprising the probability of a one for each of the predefined number of discrete frequencies, given the state in question.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. A method according to <claim-ref idref="CLM-00007">claim 7</claim-ref> wherein the observation probabilities are arranged in a matrix B with size F&#xd7;N, where F is the number of frequencies and N is the total number of states and where the matrix element at the p<sup>th </sup>row and q<sup>th </sup>column in the matrix represents the probability of a one at the p<sup>th </sup>frequency of the q<sup>th </sup>state.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. A method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein
<claim-text>state transition probabilities defining a probability of changing from state Q<sub>t </sub>to Q<sub>t+1 </sub>are provided, and arranged in a square matrix A with size N&#xd7;N, where</claim-text>
<claim-text>N is the total number of states and where the matrix element at the m<sup>th </sup>row and n<sup>th </sup>column in the matrix represents the probability of changing from state m to state n.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein
<claim-text>state transition probabilities defining a probability of changing from state Q<sub>t </sub>to Q<sub>t+1 </sub>are arranged in a square matrix A with size N&#xd7;N, where N is the total number of states and where the matrix element at the m<sup>th </sup>row and n<sup>th </sup>column in the matrix represents the probability of changing from state m to state n.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. A method according to <claim-ref idref="CLM-00005">claim 5</claim-ref> comprising a quantization of the clean binary mask wherein a subset of N<sub>q </sub>states are determined, where N<sub>q</sub>&#x3c;N, each state being constituted by a binary mode vector representative of the target signal at the predefined number of discrete frequencies.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. A method according to <claim-ref idref="CLM-00011">claim 11</claim-ref> wherein the state transition probabilities a<sub>ij </sub>of the matrix A are calculated from the quantized binary mask by counting the number n<sub>ij </sub>of state changes Qi to Qj, where i=1, 2, . . . , N<sub>q </sub>and j=1, 2, . . . , N<sub>q </sub>and divide each number n<sub>ij </sub>by the total number N<sub>sc,i </sub>of state changes of the quantized binary mask from state i.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. A method according to <claim-ref idref="CLM-00011">claim 11</claim-ref> wherein the observation probabilities B(k,n) of the matrix B are calculated based on two contributions B<sub>c </sub>and b<sub>m </sub>according to the formula B(k,n)=B<sub>c</sub>(k,n)+b<sub>n</sub>(k)&#x2212;B<sub>c</sub>(k,n)&#xb7;b<sub>n</sub>(k), where f=1, 2, . . . , F is the frequency index and k=1, 2, . . . , N<sub>q </sub>is the state index, where B<sub>c</sub>(k,n) are the observation probabilities defined by the clean binary mask and the quantized clean binary mask, and where the observation probabilities b<sub>n</sub>(k) defining the probability of ones at each frequency generated by the noise signal.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. A method according to <claim-ref idref="CLM-00013">claim 13</claim-ref> where the observation probabilities b<sub>n</sub>(k) are calculated from characteristics of the of the noise signal in question.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. A method according to <claim-ref idref="CLM-00014">claim 14</claim-ref> where the observation probabilities b<sub>n</sub>(k) are calculated during time periods where no voice signals are present.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. A method according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein
<claim-text>the observation probabilities are calculated based on a record of the noise signal alone prior to the use of the statistical model to detect and correct errors in the noisy binary mask.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein
<claim-text>an estimate of a noise-free binary mask is determined from the noisy binary mask as the most probable sequence of states using the Viterbi algorithm,</claim-text>
<claim-text>the states being constituted by vectors representing the target signal in the binary domain at the predefined number of discrete frequencies at a number of points in time.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. A method according to <claim-ref idref="CLM-00017">claim 17</claim-ref> wherein
<claim-text>the most probable sequence of states are calculated based on a number of past, the present, and a number of future observations,</claim-text>
<claim-text>the observations being constituted by binary vectors representing the target signal mixed with the noise signal at the predefined number of discrete frequencies at a number of points in time.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein
<claim-text>the noisy binary mask represents speech in noise.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein
<claim-text>a target binary mask TBM<sub>est </sub>is estimated from the following formula:</claim-text>
</claim-text>
<claim-text>
<maths id="MATH-US-00007" num="00007">
<math overflow="scroll">
<mrow>
    <mo>=</mo>
  <mrow>
    <mo>{</mo>
    <mtable>
      <mtr>
        <mtd>
          <mrow>
            <mn>1</mn>
            <mo>;</mo>
            <mrow>
              <mrow>
                <mi>if</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.8em" height="0.8ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mfrac>
                  <mrow>
                    <mrow>
                      <mi>T</mi>
                      <mo>&#x2061;</mo>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <mi>&#x3c4;</mi>
                          <mo>,</mo>
                          <mi>k</mi>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                    <mo>+</mo>
                    <mrow>
                      <mi>M</mi>
                      <mo>&#x2061;</mo>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <mi>&#x3c4;</mi>
                          <mo>,</mo>
                          <mi>k</mi>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                  </mrow>
                  <mrow>
                    <mi>BTH</mi>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mi>k</mi>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                </mfrac>
              </mrow>
              <mo>&#x3e;</mo>
              <mn>1</mn>
            </mrow>
          </mrow>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mrow>
            <mrow>
              <mn>0</mn>
              <mo>;</mo>
              <mi>otherwise</mi>
            </mrow>
            <mo>,</mo>
          </mrow>
        </mtd>
      </mtr>
    </mtable>
  </mrow>
</mrow>
</math>
</maths>
<claim-text>where T(&#x3c4;,k) is the power of a target sound, M(&#x3c4;,k) is the power of a masker sound, BTH(k) is a threshold value, &#x3c4; is a time index, and k is a frequency index.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The method according to <claim-ref idref="CLM-00020">claim 20</claim-ref>, wherein
<claim-text>the threshold value BTH(k) is the long-term average spectrum of the target sound.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. A non-transitory tangible computer-readable medium storing a computer program comprising instructions for causing a data processing system to perform the steps of the method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, when said computer program is executed on the data processing system.</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. An audio device, comprising:
<claim-text>a data processing system including a processor configured to perform steps of a method including
<claim-text>providing a noisy binary mask comprising a binary representation of the power density of an acoustic signal comprising a target signal mixed with a noise signal at a predefined number of discrete frequencies and a number of discrete time instances,</claim-text>
<claim-text>providing a statistical model of a clean binary mask representing the power density of the target signal, and</claim-text>
<claim-text>using the statistical model to detect and correct errors in the noisy binary mask; and</claim-text>
</claim-text>
<claim-text>a forward path including
<claim-text>an input transducer configured to receive an external acoustic input from an environment and output the acoustic signal,</claim-text>
<claim-text>an AD-converter, and</claim-text>
<claim-text>an output transducer configured to generate an output perceivable to a wearer of the audio device as sound.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. The audio device according to <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein
<claim-text>the output transducer includes one or more speakers for presenting an estimate of the external acoustic input to said wearer.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00025" num="00025">
<claim-text>25. The audio device according to <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein
<claim-text>the output transducer includes one or more electrodes implantable in a cochlea for presenting an estimate of the external acoustic input to said wearer.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00026" num="00026">
<claim-text>26. The audio device according to <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein
<claim-text>the output transducer includes one or more bone conduction vibrators for presenting an estimate of the external acoustic input to said wearer.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00027" num="00027">
<claim-text>27. The audio device according to <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein
<claim-text>the audio device is a hearing instrument, and</claim-text>
<claim-text>the processor is configured to adapt the acoustic signal to needs of the wearer by applying a customized frequency dependent gain to the acoustic signal.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
