<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625931-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625931</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13046638</doc-number>
<date>20110311</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<us-term-of-grant>
<us-term-extension>174</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>36</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382285</main-classification>
<further-classification>382195</further-classification>
</classification-national>
<invention-title id="d2e53">Light space graphical model in shape from shading</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>7379195</doc-number>
<kind>B2</kind>
<name>Yoon et al.</name>
<date>20080500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>356625</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>7620309</doc-number>
<kind>B2</kind>
<name>Georgiev</name>
<date>20091100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>7756325</doc-number>
<kind>B2</kind>
<name>Vetter et al.</name>
<date>20100700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382154</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>7872796</doc-number>
<kind>B2</kind>
<name>Georgiev</name>
<date>20110100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>7948514</doc-number>
<kind>B2</kind>
<name>Sato et al.</name>
<date>20110500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348 46</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>7949252</doc-number>
<kind>B1</kind>
<name>Georgiev</name>
<date>20110500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>7956924</doc-number>
<kind>B2</kind>
<name>Georgiev</name>
<date>20110600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>7962033</doc-number>
<kind>B2</kind>
<name>Georgiev et al.</name>
<date>20110600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>8009880</doc-number>
<kind>B2</kind>
<name>Zhang et al.</name>
<date>20110800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382118</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>8019215</doc-number>
<kind>B2</kind>
<name>Georgiev et al.</name>
<date>20110900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>8094964</doc-number>
<kind>B2</kind>
<name>Hadap et al.</name>
<date>20120100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382274</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>8155456</doc-number>
<kind>B2</kind>
<name>Babacan et al.</name>
<date>20120400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>8189065</doc-number>
<kind>B2</kind>
<name>Georgiev et al.</name>
<date>20120500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>8189089</doc-number>
<kind>B1</kind>
<name>Georgiev</name>
<date>20120500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>8228417</doc-number>
<kind>B1</kind>
<name>Georgiev et al.</name>
<date>20120700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>8238738</doc-number>
<kind>B2</kind>
<name>Georgiev</name>
<date>20120800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>8244058</doc-number>
<kind>B1</kind>
<name>Intwala et al.</name>
<date>20120800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>8289318</doc-number>
<kind>B1</kind>
<name>Hadap et al.</name>
<date>20121000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345420</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>8290358</doc-number>
<kind>B1</kind>
<name>Georgiev</name>
<date>20121000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>8315476</doc-number>
<kind>B1</kind>
<name>Georgiev et al.</name>
<date>20121100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>8363929</doc-number>
<kind>B2</kind>
<name>Kojima et al.</name>
<date>20130100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382154</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>2010/0074551</doc-number>
<kind>A1</kind>
<name>Lin et al.</name>
<date>20100300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382264</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>2010/0085359</doc-number>
<kind>A1</kind>
<name>Wu et al.</name>
<date>20100400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345426</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>2013/0127859</doc-number>
<kind>A1</kind>
<name>Hadap et al.</name>
<date>20130500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00025">
<othercit>Zhang, R., et al., &#x201c;Shape from shading: a survey&#x201d;, IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 21, No. 8, Aug. 1999.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00026">
<othercit>Castelan, M., et al., &#x201c;Example-based face shape recovery using the zenith angle of the surface normal,&#x201d; MICAI'07 Proceedings of the artificial intelligence 6th Mexican international conference on Advances in artificial intelligence, pp. 758-768, 2007.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00027">
<othercit>Emmanuel Prados, Olivier Faugeras &#x201c;Shape From Shading&#x201d; published in Handbook of Mathematical Models in Computer Vision Springer (Ed.) (2006) pp. 375-388.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00028">
<othercit>Jan J. Koenderink, Andrea J. van Doom &#x201c;Shape and Shading&#x201d; The Visual Neurosciences, 2003 pp. 1-28.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00029">
<othercit>V. S. Ramachandran &#x201c;Perception of shape from shading&#x201d; Nature.com, University of California,California,USA 1988 pp. 1-2.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00030">
<othercit>U.S. Appl. No. 12/957,312, filed Nov. 30, 2010, 91 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00031">
<othercit>U.S. Appl. No. 12/957,316, filed Oct. 30, 2010, 66 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00032">
<othercit>U.S. Appl. No. 12/957,320, filed Nov. 30, 2010, 58 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00033">
<othercit>U.S. Appl. No. 12/957,322, filed Nov. 30, 2010, 53 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00034">
<othercit>Georgiev, U.S. Appl. No. 12/790,677, filed May 28, 2010, 48 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00035">
<othercit>Georgiev, et al., U.S. Appl. No. 12/628,437, filed Dec. 1, 2009, 114 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00036">
<othercit>&#x201c;Final Office Action&#x201d;, U.S. Appl. No. 13/046,637, Jul. 18, 2013, 5 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00037">
<othercit>&#x201c;Non-Final Office Action&#x201d;, U.S. Appl. No. 13/046,637, Mar. 29, 2013, 12 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00038">
<othercit>Cho, Taeg S., et al., &#x201c;The Patch Transform and Its Applications to Image Editing&#x201d;, <i>Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on </i>(Jun. 23, 2008), 8 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00039">
<othercit>Huang, et al., &#x201c;Examplar-based Shape from Shading&#x201d;, <i>Proceedings of the Sixth International Conference on 3-D Digital Imaging and Modeling</i>, (Aug. 2007), 8 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>20</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>345420</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345426</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>356625</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382118</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382154</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382264</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382274</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>14</number-of-drawing-sheets>
<number-of-figures>16</number-of-figures>
</figures>
<us-related-documents>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>61380172</doc-number>
<date>20100903</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20130127860</doc-number>
<kind>A1</kind>
<date>20130523</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Hadap</last-name>
<first-name>Sunil</first-name>
<address>
<city>San Jose</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Panagopoulos</last-name>
<first-name>Alexandros</first-name>
<address>
<city>Astoria</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Hadap</last-name>
<first-name>Sunil</first-name>
<address>
<city>San Jose</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Panagopoulos</last-name>
<first-name>Alexandros</first-name>
<address>
<city>Astoria</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Wolfe-SBMC</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Adobe Systems Incorporated</orgname>
<role>02</role>
<address>
<city>San Jose</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Bali</last-name>
<first-name>Vikkram</first-name>
<department>2668</department>
</primary-examiner>
<assistant-examiner>
<last-name>Chen</last-name>
<first-name>Xuemei</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Methods and apparatus for construction of an object shape from an image using a light-space graphical model are disclosed. A set of normal vectors for a set of pixels in an image is defined. Each normal vector is defined in terms of an azimuth and a zenith measured in a spherical coordinate system centered on a light source illuminating the image. The zenith of each normal vector is constrained based on an observed shading of a respective pixel. A shape is constructed from the image. Constructing the shape includes minimizing an energy function to specify an azimuth value and a zenith value of each normal vector. Minimizing the energy function further includes constraining the azimuth of each normal vector based on an image gradient of the image at each respective pixel to enforce a coplanar assumption between the image gradient expressed in a three-dimensional space and the respective normal vector.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="226.40mm" wi="152.74mm" file="US08625931-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="246.04mm" wi="151.89mm" file="US08625931-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="134.87mm" wi="83.99mm" file="US08625931-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="251.80mm" wi="184.57mm" file="US08625931-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="186.69mm" wi="179.32mm" file="US08625931-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="244.09mm" wi="154.09mm" file="US08625931-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="158.75mm" wi="115.15mm" file="US08625931-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="246.55mm" wi="167.81mm" file="US08625931-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="209.80mm" wi="159.34mm" file="US08625931-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="230.46mm" wi="162.48mm" file="US08625931-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="233.60mm" wi="174.41mm" file="US08625931-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="226.99mm" wi="158.33mm" file="US08625931-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="237.49mm" wi="173.40mm" file="US08625931-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="195.07mm" wi="167.39mm" file="US08625931-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="200.66mm" wi="188.81mm" file="US08625931-20140107-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CLAIM FOR PRIORITY TO PROVISIONAL APPLICATION</heading>
<p id="p-0002" num="0001">This application claims benefit of priority of U.S. Provisional Application Ser. No. 61/380,172 entitled &#x201c;Systems and Methods for Shape from Shading&#x201d; filed Sep. 3, 2010, the content of which is incorporated by reference herein in its entirety.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">BACKGROUND</heading>
<heading id="h-0003" level="1">Description of the Related Art</heading>
<p id="p-0003" num="0002">Research in computerized graphics processing has long sought to extract data for modeling three-dimensional features of a feature portrayed in an image from two-dimensional images of the feature. Researchers have sought to express data for modeling three-dimensional features of an item in several ways: depth maps, surface normal vectors, surface gradients, and surface slant and tilt. Depth can be considered either as the relative distance from a camera to surface points or'the relative surface height above the x-y plane. Surface normal vectors are the orientation of a vector perpendicular to a tangent plane on the object surface.</p>
<p id="p-0004" num="0003">In computer vision, the techniques to recover shape are called shape-from-X techniques, where X can be shading, stereo, motion, texture, etc. Shape from shading (SFS) deals with the recovery of shape from a gradual variation of shading in the image. A common model of image formation is the Lambertian model, in which the gray level at a pixel in the image depends on a light source direction and the surface normal vector. In SFS, given a gray level image, the aim is to recover the light source and the surface shape at each pixel in the image.</p>
<heading id="h-0004" level="1">SUMMARY</heading>
<p id="p-0005" num="0004">Various embodiments of methods and apparatus for construction of an object shape from an image using a light-space graphical model are disclosed. A set of normal vectors for a set of pixels in an image is defined. Each normal vector is defined in terms of an azimuth and a zenith measured in a spherical coordinate system centered on a light source illuminating the image. The zenith of each normal vector is constrained based on an observed shading of a respective pixel. A shape is constructed from the image. Constructing the shape includes minimizing an energy function to specify an azimuth value and a zenith value of each normal vector. Minimizing the energy function further includes constraining the azimuth of each normal vector based on an image gradient of the image at each respective pixel to enforce a coplanar assumption between the image gradient expressed in a three-dimensional space and the respective normal vector.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. 1</figref> depicts a module that may implement methods to derive shape from shading of images according to some embodiments.</p>
<p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. 2</figref> illustrates a subregion dictionary that may be used to implement patch-based methods to derive shape from shading of images according to some embodiments.</p>
<p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. 3</figref> depicts a set of test image files for developing a subregion dictionary that may be used to implement machine learning techniques in the context of patch-based methods to derive shape from shading of images according to some embodiments.</p>
<p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. 4A</figref> illustrates a sample image on which methods to derive shape from shading of images according to some embodiments may be used.</p>
<p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. 4B</figref> depicts a depth field that may result from use of methods to derive shape from shading of images according to some embodiments.</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 5A</figref> illustrates light striking a surface and the estimation of a surface normal vector at a single pixel location according to some embodiments.</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 5B</figref> depicts surface normal vectors at multiple pixel locations according to some embodiments.</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 5C</figref> illustrates surface normal vectors at multiple subregion locations, each subregion comprising multiple pixels, according to some embodiments.</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 6A</figref> depicts operations that may be performed in the context of light-space methods to derive shape from shading of images according to some embodiments.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 6B</figref> illustrates additional operations that may be performed in the context of light-space methods to derive shape from shading of images according to some embodiments.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 6C</figref> depicts operations that may be performed to support calculations useful in light-space methods to derive shape from shading of images according to some embodiments.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 7A</figref> illustrates operations that may be performed in the context of patch-based methods to derive shape from shading of images according to some embodiments.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 7B</figref> depicts operations that may be performed in the context of machine-learning methods for developing a subregion dictionary that may be used to implement machine learning techniques in the context of patch-based methods to derive shape from shading of images according to some embodiments.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 8</figref> illustrates additional operations that may be performed in the context of patch-based methods to derive shape from shading of images according to some embodiments.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 9</figref> depicts operations that may be performed to support calculations useful in patch-based methods to derive shape from shading of images according to some embodiments.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 10</figref> illustrates an example computer system that may be used in embodiments.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<p id="p-0022" num="0021">While the invention is described herein by way of example for several embodiments and illustrative drawings, those skilled in the art will recognize that the invention is not limited to the embodiments or drawings described. It should be understood, that the drawings and detailed description thereto are not intended to limit the invention to the particular form disclosed, but on the contrary, the intention is to cover all modifications, equivalents and alternatives falling within the spirit and scope of the present invention. The headings used herein are for organizational purposes only and are not meant to be used to limit the scope of the description. As used throughout this application, the word &#x201c;may&#x201d; is used in a permissive sense (i.e., meaning having the potential to), rather than the mandatory sense (i.e., meaning must). Similarly, the words &#x201c;include&#x201d;, &#x201c;including&#x201d;, and &#x201c;includes&#x201d; mean including, but not limited to.</p>
<heading id="h-0006" level="1">DETAILED DESCRIPTION OF EMBODIMENTS</heading>
<p id="p-0023" num="0022">Various embodiments of a system and methods for determining a shape of an object from its appearance, and more particularly, its shading, as it appears in an image are described. In one embodiment of a subregion-based method, an image is divided into a set of image subregions. For each image subregion, a set of subregion dictionary entries is identified. Each of the set of subregion dictionary entries includes a subregion entry appearance matching an appearance of the respective image subregion and a subregion entry geometry. A set of optimal subregion dictionary entries is identified. Identifying the set of optimal subregion dictionary entries includes minimizing an energy function of the sets of subregion dictionary entries for all image subregions. Each optimal subregion dictionary entry is, for a respective one of the image subregions, a subregion entry associated with a minimum of the energy function. Such optimal subregion entries are selected as on the basis of their usefulness, as a set, in reconstructing the entire image. The shape includes a shape construction parameter from a subregion geometry entry of each optimal subregion geometry entry of the set of optimal subregion geometry entries. The shape can be reconstructed using the subregion geometry entries of the optimal subregion dictionary entry for each image subregion. In some embodiments, shape reconstruction may employ the estimation of extra parameters in order to singularly determine the subregion shape from the optimal subregion geometry entry. These parameters may also be part of the model energy, and therefore determined by the minimization of that energy.</p>
<p id="p-0024" num="0023">In one embodiment of a light-space graphical model based method, a set of normal vectors or other shape construction parameters corresponding to a set of pixels in an image is defined. The defining the set of normal vectors includes defining each normal vector in terms of the spherical coordinates (zenith and azimuth) in the coordinate system of the light source, such that the orientation of the light source corresponds to a zenith value of 0. The zenith of each normal vector is constrained based on an observed shading of a respective pixel. A shape is constructed from the image. Constructing the shape includes minimizing an energy function to specify an azimuth value and a zenith value of each normal vector. The minimizing the energy function further includes constraining the azimuth of each normal vector based on an image gradient of the image at each respective pixel. The constraining the azimuth enforces a coplanar assumption between the image gradient expressed in a three-dimensional space and the respective normal vector. In some embodiments, light-space and subregion based methods may be used serially to refine an estimate of a shape from a single image.</p>
<p id="p-0025" num="0024">In the following detailed description, numerous specific details are set forth to provide a thorough understanding of claimed subject matter. However, it will be understood by those skilled in the art that claimed subject matter may be practiced without these specific details. In other instances, methods, apparatuses or systems that Would be known by one of ordinary skill have not been described in detail so as not to obscure claimed subject matter.</p>
<p id="p-0026" num="0025">Some portions of the detailed description which follow are presented in terms of algorithms or symbolic representations of operations on binary digital signals stored within a memory of a specific apparatus or special purpose computing device or platform. In the context of this particular specification, the term specific apparatus or the like includes a general purpose computer once it is programmed to perform particular functions pursuant to instructions from program software. Algorithmic descriptions or symbolic representations are examples of techniques used by those of ordinary skill in the signal processing or related arts to convey the substance of their work to others skilled in the art. An algorithm is here, and is generally, considered to be a self-consistent sequence of operations or similar signal processing leading to a desired result. In this context, operations or processing involve physical manipulation of physical quantities. Typically, although not necessarily, such quantities may take the form of electrical or magnetic signals capable of being stored, transferred, combined, compared or otherwise manipulated. It has proven convenient at times, principally for reasons of common usage, to refer to such signals as bits, data, values, elements, symbols, characters, terms, numbers, numerals or the like. It should be understood, however, that all of these or similar terms are to be associated with appropriate physical quantities and are merely convenient labels. Unless specifically stated otherwise, as apparent from the following discussion, it is appreciated that throughout this specification discussions utilizing terms such as &#x201c;processing,&#x201d; &#x201c;computing,&#x201d; &#x201c;calculating,&#x201d; &#x201c;determining&#x201d; or the like refer to actions or processes of a specific apparatus, such as a special purpose computer or a similar special purpose electronic computing device. In the context of this specification, therefore, a special purpose computer or a similar special purpose electronic computing device is capable of manipulating or transforming signals, typically represented as physical electronic or magnetic quantities within memories, registers, or other information storage devices, transmission devices, or display devices of the special purpose computer or similar special purpose electronic computing device.</p>
<p id="h-0007" num="0000">Introduction to Patch-Based Data-driven Shape from Shading</p>
<p id="p-0027" num="0026">A system and method are provided to retrieve the shape of an object from its appearance, and more particularly, appearance variations due to the interaction of illumination and the object's shape, as the object appears in a single image. Image patches, alternatively called subregions herein, serve as a source of information for the purpose of extracting a shape from shading. The method may include retrieving the orientation of three-dimensional surfaces using learned patch dictionaries. In some embodiments, the term dictionary refers to a collection of patches. Each patch may consist of the corresponding appearance and geometry. The dictionary may further include collected statistics and other information on properties of the patches stored in the dictionary, or encountered during the training phase. The term image patch, or subregion, refers to a sample of an image consisting of more than one pixel of the image. The terms subregion and patch, as used herein, are interchangeable, and some methods are interchangeably described as being patch-based or subregion-based. In some embodiments, subregions are rectangular, and are composed of only a few pixels. The method may include a training phase and a testing phase. An embodiment of training operations is discussed below with respect to <figref idref="DRAWINGS">FIG. 7B</figref>. During training, a patch or subregion dictionary is constructed from a set of training images. In the testing phase, the methods discussed below, particularly with respect to <figref idref="DRAWINGS">FIGS. 7A</figref>, <b>8</b> and <b>9</b>, may construct a surface geometry that explains a given test image. In some embodiments, such a surface geometry is provided as a map of normal vectors of the surface.</p>
<p id="p-0028" num="0027">In the training phase described below with respect to <figref idref="DRAWINGS">FIG. 7B</figref>, a set of training images and corresponding known normal maps are used as input. An example set of training images and normal maps is described below with respect to <figref idref="DRAWINGS">FIG. 3</figref>. Each training image and the corresponding normal map are divided into (possibly overlapping) patches or subregions of a given size (e.g., m&#xd7;n or m&#xd7;m). Training images are accompanied by known geometry (in some embodiments, in the form a normal map), and are only used for the construction of the dictionary and to extract relevant statistics about patches (that may also be stored in the dictionary). Test images, by contrast, involve only appearance, and embodiments determine the geometry, such that, as used herein, a test image is any image we will apply our algorithm on after training is performed. Each patch or subregion is added to a subregion dictionary (&#x3a9;) if the subregion differs substantially from the patches or subregions already stored in the subregion dictionary. Metrics and thresholds for determining whether the subregion differs substantially from the patches or subregions already stored in the subregion dictionary will vary between embodiments. One of skill in the art will readily comprehend, in light of having read the present disclosure, that parameters such as thresholds for determining whether the subregion differs substantially from the patches or subregions already stored in the subregion dictionary or subregion geometry will vary between embodiments in order to control or optimize various aspects of performance of the methods disclosed herein.</p>
<p id="p-0029" num="0028">In some embodiments, a dictionary entry includes a subregion appearance and a subregion geometry. The subregion appearance may be a simple image, the similarity of which to a selected subregion of an image from which a shape is being constructed may be ascertained using conventional techniques that will be well-known to one of skill in the art in light of having read the present disclosure. In some embodiments, the subregion geometry may be decomposed in components, in order to make the dictionary more compact and remove ambiguities. In some embodiments, the geometry of each subregion dictionary entry in the subregion dictionary may be defined up to some</p>
<p id="p-0030" num="0029">parameters (e.g., an azimuth angle). Such an azimuth angle, for example, may be determined when reconstructing a surface represented by the subregion.</p>
<p id="p-0031" num="0030">In the testing phase, a new input image with unknown underlying geometry is provided. The method reconstructs the underlying geometry as a normal map, using the dictionary learned during the training phase. First, the test image is divided into a set of overlapping patches. For each image patch, the method finds a set of the k entries in the dictionary with the most similar appearance to the observed patch appearance. To define the normal map which underlies the given image, the method may select one of the k dictionary matches for each image patch, and the parameters (e.g., azimuth angle) that disambiguate the geometry of that patch. This may be treated as a labeling problem on a graphical model.</p>
<p id="p-0032" num="0031">In some embodiments, a graphical model for patch-based shape from shading contains one node for each subregion in the input image. A label of each node corresponds to a selection of one of k subregion dictionary matches and the accompanying parameters that best explain this subregion. An energy E(x) is defined for each label assignment x on this model. The x that minimizes this energy is the most probable set of patch and parameter choices, given the input image. In such an embodiment, the model energy has the following general form:</p>
<p id="p-0033" num="0032">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mrow>
  <mrow>
    <mrow>
      <mi>E</mi>
      <mo>&#x2061;</mo>
      <mrow>
        <mo>(</mo>
        <mi>x</mi>
        <mo>)</mo>
      </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
      <mrow>
        <munder>
          <mo>&#x2211;</mo>
          <mi>i</mi>
        </munder>
        <mo>&#x2062;</mo>
        <mstyle>
          <mspace width="0.3em" height="0.3ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mrow>
          <msub>
            <mi>&#x3c6;</mi>
            <mi>i</mi>
          </msub>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <msubsup>
              <mi>x</mi>
              <mi>i</mi>
              <mi>A</mi>
            </msubsup>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mrow>
      <mo>+</mo>
      <mrow>
        <munder>
          <mo>&#x2211;</mo>
          <mi>i</mi>
        </munder>
        <mo>&#x2062;</mo>
        <mrow>
          <msub>
            <mi>&#x3c8;</mi>
            <mi>i</mi>
          </msub>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <msubsup>
              <mi>x</mi>
              <mi>i</mi>
              <mi>&#x3b8;</mi>
            </msubsup>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mrow>
      <mo>+</mo>
      <mrow>
        <munder>
          <mo>&#x2211;</mo>
          <mrow>
            <mi>i</mi>
            <mo>,</mo>
            <mi>j</mi>
          </mrow>
        </munder>
        <mo>&#x2062;</mo>
        <mrow>
          <msub>
            <mi>&#x3be;</mi>
            <mrow>
              <mi>i</mi>
              <mo>,</mo>
              <mi>j</mi>
            </mrow>
          </msub>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <msubsup>
                <mi>x</mi>
                <mi>i</mi>
                <mrow>
                  <mi>G</mi>
                  <mo>,</mo>
                  <mi>&#x3b8;</mi>
                </mrow>
              </msubsup>
              <mo>,</mo>
              <msubsup>
                <mi>x</mi>
                <mi>j</mi>
                <mrow>
                  <mi>G</mi>
                  <mo>,</mo>
                  <mi>&#x3b8;</mi>
                </mrow>
              </msubsup>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mrow>
      <mo>+</mo>
      <mrow>
        <munder>
          <mo>&#x2211;</mo>
          <mrow>
            <mi>i</mi>
            <mo>,</mo>
            <mi>j</mi>
          </mrow>
        </munder>
        <mo>&#x2062;</mo>
        <mrow>
          <msub>
            <mi>&#x3b6;</mi>
            <mrow>
              <mi>i</mi>
              <mo>,</mo>
              <mi>j</mi>
            </mrow>
          </msub>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <msubsup>
                <mi>x</mi>
                <mi>i</mi>
                <mrow>
                  <mi>G</mi>
                  <mo>,</mo>
                  <mi>&#x3b8;</mi>
                </mrow>
              </msubsup>
              <mo>,</mo>
              <msubsup>
                <mi>x</mi>
                <mi>j</mi>
                <mrow>
                  <mi>G</mi>
                  <mo>,</mo>
                  <mi>&#x3b8;</mi>
                </mrow>
              </msubsup>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mrow>
    </mrow>
  </mrow>
  <mo>&#x2062;</mo>
  <mstyle>
    <mspace width="0.3em" height="0.3ex"/>
  </mstyle>
</mrow>
</math>
</maths>
</p>
<p id="p-0034" num="0033">The first term (&#x3c6;<sub>i </sub>corresponds to the difference between the appearance x<sub>i</sub><sup>A </sup>of selected match x<sub>i </sub>and the observed appearance of patch i. The first term (&#x3c6;<sub>i </sub>penalizes dictionary matches that correspond to appearances considerably different from the observed appearance. This difference can be calculated as: &#x3c6;<sub>i</sub>(x<sub>i</sub><sup>A</sup>)=w<sub>1</sub>&#x2225;x<sub>i</sub><sup>A</sup>&#x2212;I<sub>i</sub>&#x2225;<sub>2</sub>, where I<sub>i </sub>is the appearance of patch i in the input image and w<sub>i </sub>is a weight.</p>
<p id="p-0035" num="0034">The second term &#x3c8;<sub>i</sub>(x<sub>i</sub><sup>&#x3b8;</sup>) corresponds to the prior probability of parameter choices x<sub>i</sub><sup>&#x3b8;</sup>given the learned dictionary, penalizing parameter choices that are far from the observed values of these parameters while training. This term can be calculated as: &#x3c8;<sub>i</sub>(x<sub>i</sub><sup>&#x3b8;</sup>) =&#x2212;w<sub>2</sub>log (P(x<sub>i</sub><sup>74 </sup>|<img id="CUSTOM-CHARACTER-00001" he="3.13mm" wi="2.46mm" file="US08625931-20140107-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/>)), where w<sub>2 </sub>is a weight and P(x<sub>i</sub><sup>&#x3b8;</sup>|<img id="CUSTOM-CHARACTER-00002" he="3.13mm" wi="2.46mm" file="US08625931-20140107-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/>) is the probability of the parameter values indicated by x<sub>i</sub><sup>&#x3b8;</sup>, based on the values observed while training dictionary <img id="CUSTOM-CHARACTER-00003" he="3.13mm" wi="2.46mm" file="US08625931-20140107-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/>. The probability P(x<sub>i</sub><sup>&#x3b8;</sup>|<img id="CUSTOM-CHARACTER-00004" he="3.13mm" wi="2.46mm" file="US08625931-20140107-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/>) is calculated, in some embodiments, by fitting a Gaussian mixture model to the set of observed values for each parameter of interest.</p>
<p id="p-0036" num="0035">The third term &#x3be;<sub>i,j</sub>(x<sub>i</sub><sup>G,&#x3b8;</sup>,x<sub>j</sub><sup>G,&#x3b8;</sup>) enforces compatibility between matches x<sub>i </sub>and x<sub>j </sub>for overlapping patches i and j, by penalizing disagreements between the geometry component of the two matches in the region of overlap between i and j. This term can be calculated by the sum of square difference between normals of the two patches that coincide in the image:</p>
<p id="p-0037" num="0036">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mrow>
  <mrow>
    <mrow>
      <msub>
        <mi>&#x3be;</mi>
        <mrow>
          <mi>i</mi>
          <mo>,</mo>
          <mi>j</mi>
        </mrow>
      </msub>
      <mo>&#x2061;</mo>
      <mrow>
        <mo>(</mo>
        <mrow>
          <msubsup>
            <mi>x</mi>
            <mi>i</mi>
            <mrow>
              <mi>G</mi>
              <mo>,</mo>
              <mi>&#x3b8;</mi>
            </mrow>
          </msubsup>
          <mo>,</mo>
          <msubsup>
            <mi>x</mi>
            <mi>j</mi>
            <mrow>
              <mi>G</mi>
              <mo>,</mo>
              <mi>&#x3b8;</mi>
            </mrow>
          </msubsup>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
      <msub>
        <mi>w</mi>
        <mn>3</mn>
      </msub>
      <mo>&#x2062;</mo>
      <mrow>
        <munder>
          <mo>&#x2211;</mo>
          <mrow>
            <mi>s</mi>
            <mo>,</mo>
            <mi>t</mi>
          </mrow>
        </munder>
        <mo>&#x2062;</mo>
        <msup>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mrow>
                <msubsup>
                  <mi>n</mi>
                  <mi>i</mi>
                  <mrow>
                    <mi>G</mi>
                    <mo>,</mo>
                    <mi>&#x3b8;</mi>
                  </mrow>
                </msubsup>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>(</mo>
                  <mi>s</mi>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mo>-</mo>
              <mrow>
                <msubsup>
                  <mi>n</mi>
                  <mi>j</mi>
                  <mrow>
                    <mi>G</mi>
                    <mo>,</mo>
                    <mi>&#x3b8;</mi>
                  </mrow>
                </msubsup>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>(</mo>
                  <mi>t</mi>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mrow>
            <mo>)</mo>
          </mrow>
          <mn>2</mn>
        </msup>
      </mrow>
    </mrow>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
</maths>
<br/>
where a pixel s of patch i corresponds to the same image pixel as pixel t of patch j, and n<sub>k</sub><sup>G,&#x3b8;</sup> (u) is the normal vector at pixel u of patch k indicated by label x<sub>k</sub><sup>G,&#x3b8;</sup> (u), which is reconstructed by the information stored in the dictionary for patch k and by the mean azimuth angle x<sub>k</sub><sup>&#x3b8;</sup> (which is included in label x<sub>k</sub><sup>G,&#x3b8;</sup> (u). w<sub>3 </sub>is a weight.
</p>
<p id="p-0038" num="0037">A fourth term &#x3b6;<sub>i,j</sub>(x<sub>i</sub><sup>G,&#x3b8;</sup>, x<sub>j</sub><sup>G,&#x3b8;</sup>) penalizes violations of surface integrability in the region of overlap between patches i and j. Enforcing integrability for the estimated normal map ensures that the normal map corresponds to a plausible 3D surface. In order to compute term &#x3b6;<sub>i,j</sub>(x<sub>i</sub><sup>G,&#x3b8;</sup>, x<sub>j</sub><sup>G,&#x3b8;</sup>), embodiments first compute the normal map n<sub>i,j </sub>resulting from the combination of patches indicated by x<sub>i</sub><sup>G,&#x3b8;</sup>and x<sub>j</sub><sup>G,&#x3b8;</sup>in the area of overlap of those two patches. The normal map N<sub>i,j </sub>can be computed as a weighted average of the two patches for each pixel where they overlap. Embodiments then compute the vector field that is equivalent to normal map N<sub>i,j </sub>for the applicable region. Vector Field G ={(g<sub>x</sub>, g<sub>y</sub>)} is associated with the equivalent normal map N ={n} by g<sub>x </sub>=&#x2202;n<sub>z</sub>/&#x2202;n<sub>y</sub>, &#x11d;<sub>y </sub>=&#x2202;n<sub>z</sub>/&#x2202;n<sub>y</sub>. Having defined G<sub>i,j</sub>, we can define &#x3b6;<sub>i,j</sub>(x<sub>i</sub><sup>G,&#x3b8;</sup>, x<sub>j</sub><sup>G,&#x3b8;</sup>) as:</p>
<p id="p-0039" num="0038">
<maths id="MATH-US-00003" num="00003">
<math overflow="scroll">
<mrow>
  <mrow>
    <msub>
      <mi>&#x3b6;</mi>
      <mrow>
        <mi>i</mi>
        <mo>,</mo>
        <mi>j</mi>
      </mrow>
    </msub>
    <mo>(</mo>
    <mstyle>
      <mspace width="0.em" height="0.ex"/>
    </mstyle>
    <mo>&#x2062;</mo>
    <mrow>
      <msubsup>
        <mi>x</mi>
        <mi>i</mi>
        <mrow>
          <mi>G</mi>
          <mo>,</mo>
          <mi>&#x3b8;</mi>
        </mrow>
      </msubsup>
      <mo>,</mo>
      <msubsup>
        <mi>x</mi>
        <mi>j</mi>
        <mrow>
          <mi>G</mi>
          <mo>,</mo>
          <mi>&#x3b8;</mi>
        </mrow>
      </msubsup>
    </mrow>
    <mo>)</mo>
  </mrow>
  <mo>=</mo>
  <mrow>
    <msub>
      <mi>w</mi>
      <mn>4</mn>
    </msub>
    <mo>&#x2062;</mo>
    <mrow>
      <munder>
        <mo>&#x2211;</mo>
        <mrow>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mi>x</mi>
              <mo>,</mo>
              <mi>y</mi>
            </mrow>
            <mo>)</mo>
          </mrow>
          <mo>&#x2062;</mo>
          <mi>&#x3b5;&#x39e;</mi>
        </mrow>
      </munder>
      <mo>&#x2062;</mo>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mrow>
            <mrow>
              <msub>
                <mi>g</mi>
                <mi>x</mi>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>x</mi>
                  <mo>,</mo>
                  <mi>y</mi>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>-</mo>
            <mrow>
              <msub>
                <mi>g</mi>
                <mi>x</mi>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>x</mi>
                  <mo>,</mo>
                  <mrow>
                    <mi>y</mi>
                    <mo>+</mo>
                    <mn>1</mn>
                  </mrow>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>-</mo>
            <mrow>
              <msub>
                <mi>g</mi>
                <mi>y</mi>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>x</mi>
                  <mo>,</mo>
                  <mi>y</mi>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>+</mo>
            <mrow>
              <msub>
                <mi>g</mi>
                <mi>y</mi>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mrow>
                    <mi>x</mi>
                    <mo>+</mo>
                    <mn>1</mn>
                  </mrow>
                  <mo>,</mo>
                  <mi>y</mi>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
          <mo>,</mo>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
</p>
<p id="p-0040" num="0039">where w<sub>4 </sub>is a weight. One or more of these terms may be optional in some embodiments.</p>
<p id="p-0041" num="0040">Minimizing the above energy function yields, in some embodiments, a highly-probable geometry for the input image, given the dictionary &#x3a9;. In some embodiments, overlapping patches are used. Because such embodiments generate a geometry by combining overlapping patches, disagreements may result in loss of fine detail and &#x201c;seams.&#x201d; In such embodiments, a final s tage employing an alternative shape from shading model or other image reconstruction model can improve local detail to result in an accurate surface reconstruction. The final stage may be a refinement stage, where the reconstructed normal vectors are smoothed across the seams or may be another shape from shading algorithm, which will use the reconstructed surface from the present method as extra information or prior information.</p>
<p id="h-0008" num="0000">Light-Space Graphical Model for Shape from Shading</p>
<p id="p-0042" num="0041">Some embodiments may retrieve the shape of an object from its appearance, and more particularly, its shading, as it appears in a single image. In some embodiments, the shape may be retrieved in terms of per-pixel normal vectors or other shape construction parameters. Embodiments may employ an energy minimization approach, based on a graphical model. Each normal vector n may be defined by its spherical coordinates (&#x3c6;&#x3b8;) in the coordinate system of the light source (assuming a single light source). If it is assumed that the object exhibits Lambertian reflectance, a shape may be constructed based on an assumption that the observed image intensities only depend on the zenith angle &#x3c6;. Such an assumption is based on the definition of Lambertian reflectance, implying that the zenith angle &#x3c6; for each normal vector may be strongly and directly constrained based on the observed shading, while having a different set of constraints for the azimuth angles &#x3b8;.</p>
<p id="p-0043" num="0042">In particular, azimuth angles may be constrained based on the gradients of the observed image, by assuming that the normal vector and the two-dimensional image gradient expressed in three-dimensional space exhibit co-planar behavior. Furthermore, different smoothness constraints may be used for zenith and azimuth angles, to take into account that different reliability of the information for each of them.</p>
<p id="p-0044" num="0043">Therefore, the normal at each pixel is defined by two scalar values, &#x3c6; and &#x3b8;. The most probable set of values to reconstruct the geometry underlying an input image may be inferred by formulating the problem as a graphical model. The graphical model contains two nodes for each pixel, one node for &#x3c6; and one node for &#x3b8;. The energy of this model may be defined, given an assignment of labels x =(&#x3c6;<sub>i</sub>,&#x3b8;<sub>i</sub>, by a set of potential functions as:</p>
<p id="p-0045" num="0044">
<maths id="MATH-US-00004" num="00004">
<math overflow="scroll">
<mrow>
  <mrow>
    <mrow>
      <mi>E</mi>
      <mo>&#x2061;</mo>
      <mrow>
        <mo>(</mo>
        <mi>x</mi>
        <mo>)</mo>
      </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
      <mrow>
        <munder>
          <mo>&#x2211;</mo>
          <mi>i</mi>
        </munder>
        <mo>&#x2062;</mo>
        <mrow>
          <msub>
            <mi>&#x3c7;</mi>
            <mi>i</mi>
          </msub>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <msub>
              <mi>&#x3d5;</mi>
              <mi>i</mi>
            </msub>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mrow>
      <mo>+</mo>
      <mrow>
        <munder>
          <mo>&#x2211;</mo>
          <mi>i</mi>
        </munder>
        <mo>&#x2062;</mo>
        <mrow>
          <msub>
            <mi>&#x3b6;</mi>
            <mi>i</mi>
          </msub>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <msub>
              <mi>&#x3b8;</mi>
              <mi>i</mi>
            </msub>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mrow>
      <mo>+</mo>
      <mrow>
        <munder>
          <mo>&#x2211;</mo>
          <mrow>
            <mi>i</mi>
            <mo>,</mo>
            <mi>j</mi>
          </mrow>
        </munder>
        <mo>&#x2062;</mo>
        <mrow>
          <msub>
            <mi>&#x3c8;</mi>
            <mrow>
              <mi>i</mi>
              <mo>,</mo>
              <mi>j</mi>
            </mrow>
          </msub>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <msub>
                <mi>&#x3d5;</mi>
                <mi>i</mi>
              </msub>
              <mo>,</mo>
              <msub>
                <mi>&#x3d5;</mi>
                <mi>j</mi>
              </msub>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mrow>
      <mo>+</mo>
      <mrow>
        <munder>
          <mo>&#x2211;</mo>
          <mrow>
            <mi>i</mi>
            <mo>,</mo>
            <mi>j</mi>
          </mrow>
        </munder>
        <mo>&#x2062;</mo>
        <mrow>
          <mrow>
            <msub>
              <mi>&#x3be;</mi>
              <mrow>
                <mi>i</mi>
                <mo>,</mo>
                <mi>j</mi>
              </mrow>
            </msub>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <msub>
                  <mi>&#x3b8;</mi>
                  <mi>i</mi>
                </msub>
                <mo>,</mo>
                <msub>
                  <mi>&#x3b8;</mi>
                  <mi>j</mi>
                </msub>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>.</mo>
        </mrow>
      </mrow>
    </mrow>
  </mrow>
  <mo>&#x2062;</mo>
  <mstyle>
    <mspace width="2.2em" height="2.2ex"/>
  </mstyle>
</mrow>
</math>
</maths>
</p>
<p id="p-0046" num="0045">The first term &#x3c7;<sub>i</sub>(&#x3c6;<sub>i</sub>) is a potential function which penalizes values of the zenith angle &#x3c6;<sub>i </sub>that correspond to shading at pixel i that is far from the observed image intensity I at pixel i. In embodiments where Lambertian reflectance is assumed, this term may have the form: &#x3c7;<sub>i </sub>(&#x3c6;<sub>i</sub>)=w<sub>x</sub>(max{cos(&#x3c6;<sub>i</sub>),0}-1<sub>i</sub>)<sup>2</sup>, where w<sub>x </sub>is a weight.</p>
<p id="p-0047" num="0046">The second term &#x3b6;<sub>i </sub>expresses the dependency of the azimuth value at pixel i to the observed image gradient at pixel i. Let g<sub>i</sub>, be the image gradient at pixel i. The projection of the normal vector at pixel i onto the image plane will be a 2D vector depending on the azimuth angle, n<sub>2</sub>(&#x3b8;<sub>i</sub>). Then the term takes the form: &#x3b6;=w<sub>&#x3b6;</sub>&#x2225;g<sub>i</sub>-n<sub>2</sub>(&#x3b8;<sub>i</sub>)&#x2225;, where w<sub>&#x3b6;</sub> is a weight.</p>
<p id="p-0048" num="0047">The third term &#x3c8;<sub>i,j </sub>corresponds to a smoothness assumption for the zeniths of neighboring pixels i and j. Then the term &#x3c8;<sub>i,j </sub>may take the form: &#x3c8;<sub>i,j</sub>(&#x3c6;<sub>i</sub>,&#x3c6;<sub>j</sub>)=w<sub>100 </sub>(<img id="CUSTOM-CHARACTER-00005" he="3.56mm" wi="1.02mm" file="US08625931-20140107-P00002.TIF" alt="custom character" img-content="character" img-format="tif"/>&#x3c6;<sub>i</sub>-&#x3c6;<sub>i</sub>-&#x3c6;<sub>j</sub><img id="CUSTOM-CHARACTER-00006" he="3.56mm" wi="1.02mm" file="US08625931-20140107-P00003.TIF" alt="custom character" img-content="character" img-format="tif"/>)<sup>2</sup>, where w<sub>100 </sub> is a weight and the difference <img id="CUSTOM-CHARACTER-00007" he="3.56mm" wi="1.02mm" file="US08625931-20140107-P00002.TIF" alt="custom character" img-content="character" img-format="tif"/>&#x3c6;<sub>i</sub>-&#x3c6;<sub>j</sub><img id="CUSTOM-CHARACTER-00008" he="3.56mm" wi="1.02mm" file="US08625931-20140107-P00003.TIF" alt="custom character" img-content="character" img-format="tif"/> may be calculated taking into account that &#x3c6;<sub>i </sub>and &#x3c6;<sub>j </sub>are angles in the [-&#x3c0;, &#x3c0;] domain. The fourth term, &#x3be;<sub>i,j</sub>, corresponds to a smoothness assumption for the azimuths of neighboring pixels i and j, and may take a form similar to term &#x3c8;<sub>i,j</sub>: &#x3be;<sub>i,j</sub>(&#x3b8;<sub>i</sub>,&#x3b8;<sub>j</sub>)=w<sub>&#x3c6;</sub>(<img id="CUSTOM-CHARACTER-00009" he="3.56mm" wi="1.02mm" file="US08625931-20140107-P00002.TIF" alt="custom character" img-content="character" img-format="tif"/>&#x3c6;<sub>i</sub>-&#x3c6;<sub>j</sub><img id="CUSTOM-CHARACTER-00010" he="3.56mm" wi="1.02mm" file="US08625931-20140107-P00003.TIF" alt="custom character" img-content="character" img-format="tif"/>)<sup>2</sup>, where w<sub>&#x3b8;</sub> is a weight.</p>
<p id="p-0049" num="0048">The two smoothness terms &#x3c8;<sub>i,j </sub>and &#x3be;<sub>i,j </sub>are, in some embodiments, modified to use weights w<sub>&#x3c6;</sub>, w<sub>&#x3c6;</sub> that vary by taking into account the local image gradient. In some embodiments, another set of terms &#x3c7;<sub>i,j,k </sub>, can be added to express integrability constraints, penalizing calculated normals that deviated from a valid three-dimensional surface by enforcing a zero-curl assumption. Such embodiments let i be a pixel with image coordinates (x,y). Potential u<sub>i,j,k </sub>is defined so that pixel j will have image coordinates (x,y+1) and pixel k image coordinates (x+1,y). Given the spherical coordinates &#x3c6;,&#x3b8; of the normal vectors at pixels i, j and k, embodiments can compute the Cartesian coordinates of these normal vectors n<sub>i</sub>, n<sub>j </sub>, n<sub>k</sub>. Let g be the vector field corresponding to the normal map N={n} defined by some set of labels x. Vector field G ={(g<sub>x</sub>, g<sub>y</sub>)} is associated with the equivalent normal map N by g<sub>x</sub>=&#x2202;n<sub>z</sub>/&#x2202;n<sub>x</sub>, g<sub>y </sub>=&#x2202;n<sub>z</sub>/&#x2202;n<sub>y</sub>. Given the zenith and azimuth angles at pixels i,j and k, embodiments can define the integrability constraint term u<sub>i,j,k </sub>as:</p>
<p id="p-0050" num="0049">
<maths id="MATH-US-00005" num="00005">
<math overflow="scroll">
<mrow>
  <mrow>
    <msub>
      <mi>u</mi>
      <mrow>
        <mi>i</mi>
        <mo>,</mo>
        <mi>j</mi>
        <mo>,</mo>
        <mi>k</mi>
      </mrow>
    </msub>
    <mo>&#x2061;</mo>
    <mrow>
      <mo>(</mo>
      <mrow>
        <msub>
          <mi>&#x3d5;</mi>
          <mi>i</mi>
        </msub>
        <mo>,</mo>
        <msub>
          <mi>&#x3b8;</mi>
          <mi>i</mi>
        </msub>
        <mo>,</mo>
        <msub>
          <mi>&#x3d5;</mi>
          <mi>j</mi>
        </msub>
        <mo>,</mo>
        <msub>
          <mi>&#x3b8;</mi>
          <mi>j</mi>
        </msub>
        <mo>,</mo>
        <msub>
          <mi>&#x3d5;</mi>
          <mi>k</mi>
        </msub>
        <mo>,</mo>
        <msub>
          <mi>&#x3b8;</mi>
          <mi>k</mi>
        </msub>
      </mrow>
      <mo>)</mo>
    </mrow>
  </mrow>
  <mo>=</mo>
  <mrow>
    <msub>
      <mi>w</mi>
      <mi>u</mi>
    </msub>
    <mo>&#x2062;</mo>
    <mrow>
      <munder>
        <mo>&#x2211;</mo>
        <mrow>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mi>x</mi>
              <mo>,</mo>
              <mi>y</mi>
            </mrow>
            <mo>)</mo>
          </mrow>
          <mo>&#x2062;</mo>
          <mi>&#x3b5;&#x39e;</mi>
        </mrow>
      </munder>
      <mo>&#x2062;</mo>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mrow>
            <mrow>
              <msub>
                <mi>g</mi>
                <mi>x</mi>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mi>i</mi>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>-</mo>
            <mrow>
              <msub>
                <mi>g</mi>
                <mi>x</mi>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mi>j</mi>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>-</mo>
            <mrow>
              <msub>
                <mi>g</mi>
                <mi>y</mi>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mi>i</mi>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>+</mo>
            <mrow>
              <msub>
                <mi>g</mi>
                <mi>y</mi>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mi>k</mi>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
          <mo>,</mo>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
</p>
<p id="p-0051" num="0050">where w<sub>u </sub>is a weight and g<sub>x</sub>(t), g<sub>y</sub>(t) are the x- and y-components of the gradient field G as described above. One or more of the above-described terms may be optional in some embodiments.</p>
<p id="p-0052" num="0051">Some embodiments are built to select values of normal vectors based on the inference that the most probable labels for the above graphical model correspond to minimizing the energy E, and inference may be performed by a variety of different inference algorithms, such as graph-cut or message-passing algorithms. Performing inference on a graphical model such as the one described above includes finding an approximation to the most probable labels, corresponding to the (approximately) most probable assignment of values to the random variables that characterize the problem. Inference on this model corresponds to finding a (local) minimum of the energy E. Finding such a minimum yields the set of normal vectors that approximate the most probable normal map to explain the test image, given the described model.</p>
<p id="p-0053" num="0052">In various embodiments employing the above approach, the zenith and azimuth are treated separately, with different smoothness constraints and different priors expressed by the corresponding potential functions. Furthermore, the potential functions may take any form, allowing flexibility in the modeling of the problem and providing the ability to assume specific probability distributions underlying each constraint.</p>
<p id="h-0009" num="0000">Example Shape from Shading Module, Data Structures, and Images</p>
<p id="p-0054" num="0053">Various embodiments of methods and apparatus for determining a shape of an object from its appearance, and more particularly, its shading, as it appears in an image are implemented through a shape modeling module, as described below. Some embodiments may include a means for accessing a two-dimensional input image and providing as output a three-dimensional vector field representing a shape associated with the image. For example, some embodiments may implement a patch-based shape from shading method for generating a three-dimensional vector field representing a shape associated with a two-dimensional image. Some embodiments may implement a light-space-based shape from shading method for generating a three-dimensional vector field representing a shape associated with a two-dimensional image. Some embodiments may include both light-spaced and patch-based methods for retrieving three dimensional shape from a two-dimensional image, and some embodiments may use one to refine results presented by the other.</p>
<p id="p-0055" num="0054">The shape from shading modules implementing embodiments described herein may in some embodiments be implemented by a non-transitory, computer-readable storage medium and one or more processors (e.g., CPUs and/or GPUs) of a computing apparatus. The computer-readable storage medium may store program instructions executable by the one or more processors to cause the computing apparatus to perform receiving input identifying a digital image or a portion of a digital image on which shape from shading methods are to be performed, and loading such a digital image and storing a shape reconstruction data structure as described herein. Other embodiments of the shape from shading methods and module may be at least partially implemented by hardware circuitry and/or firmware stored, for example, in a non-volatile memory or other non-transitory storage medium.</p>
<p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. 1</figref> illustrates a shape modeling module that may implement one or more of the shape from shading image editing techniques and tools illustrated in <figref idref="DRAWINGS">FIGS. 2 through 9</figref>. Shape modeling module <b>120</b> may, for example, implement one or more of a light-space model shape from shading tool and a subregion-based shape from shading tool. <figref idref="DRAWINGS">FIG. 10</figref> illustrates an example computer system on which embodiments of shape modeling module <b>120</b> may be implemented. Shape modeling module <b>120</b> receives as input one or more digital input images <b>110</b>. An example image is shown in <figref idref="DRAWINGS">FIG. 4A</figref>. Shape modeling module <b>120</b> may receive user input <b>112</b> activating one or more of a light-space model shape from shading tool and a patch-based shape from shading tool. Shape modeling module <b>120</b> then constructs three-dimensional shape from the input image(s) <b>110</b>, according to, in some embodiments, preferences expressed in user input <b>112</b> received via user interface <b>122</b>, using the activated light-space model shape from shading tool or the patch-based shape from shading tool. The user may activate a different one of light-space model shape from shading tool or the patch-based shape from shading tool and further refine the shape, and/or may activate other tools and further refine the shape. Shape modeling module <b>120</b> generates as output one or more modified output shapes <b>130</b>. Output shape(s) <b>130</b> may, for example, be stored to a storage medium <b>140</b>, such as system memory, a disk drive, DVD, CD, etc.</p>
<p id="p-0057" num="0056">In some embodiments, shape modeling module <b>120</b> may provide user interface <b>122</b> via which a user may interact with the shape modeling module <b>120</b>, for example to activate one or more of a light-space model shape from shading tool and a patch-based shape from shading tool, to select input image <b>110</b>, and to select parameters for performing a shape construction method as described herein. In some embodiments, the user interface may provide user interface elements whereby the user may select options including, but not limited to, dictionary selection constraints (e.g., the size of k or thresholds for inclusion into a set of possible subregion matches), degree of overlap in patch selection, known borders of an object for shape measurement, sensitivity of particular constraint operations, sequence of iteration of subregion based and light-space based refinements of a shape model, and/or options to balance image accuracy against processing requirements. In some embodiments, the user interface may provide user interface elements whereby the user may specify boundaries of shapes or edges or other discontinuities of shapes, or whether the tool is to use an entire image or a subset of an image. In some embodiments, the user interface may provide user interface elements whereby the user may specify which layers data is to be sampled from.</p>
<p id="p-0058" num="0057">In some embodiments, a patch-based shape from shading module <b>160</b> performs a patch-based shape from shading method as described above and further described below with respect to <figref idref="DRAWINGS">FIGS. 2</figref>, <b>3</b>, <b>5</b>C, and <b>7</b>A-<b>9</b>. In such a patch-based shape from shading method, input image <b>110</b> is divided into a set of image subregions, as discussed below with respect to <figref idref="DRAWINGS">FIG. 5C</figref>. For each image subregion, a set of subregion dictionary entries is identified as a possible match for the subregion from a subregion dictionary <b>150</b>, which is described below with respect to <figref idref="DRAWINGS">FIG. 2</figref>. Each of the set of subregion dictionary entries includes a subregion entry appearance matching an appearance of the respective image subregion and a subregion entry geometry. The matching may be calculated using conventional image comparison techniques. A set of optimal subregion dictionary entries is identified for respective ones of the image subregions. The image subregions may overlap. Identifying the set of optimal subregion dictionary entries includes minimizing an energy function, as described above, of the sets of subregion dictionary entries for all image subregions. Each optimal subregion dictionary entry is, for a respective one of the image subregions, a subregion entry associated with a minimum of the energy function. Output shape <b>130</b> includes a shape construction parameter, such as a normal vector, from a subregion geometry entry of each optimal subregion geometry entry of the set of optimal subregion geometry entries.</p>
<p id="p-0059" num="0058">In some embodiments, patch-based shape from shading module <b>160</b> uses and updates a subregion dictionary <b>150</b> generated by a subregion learning module <b>170</b>. Subregion learning module <b>170</b> is configured to generate subregion dictionary <b>150</b> from training image files <b>190</b> and their corresponding normal maps by dividing a training image and a corresponding normal map into training subregions, comparing an appearance of a test subregion to entries of the subregion dictionary, and, in response to failure to find an entry of the subregion dictionary with an appearance matching an appearance of the test subregion, creating a new entry of the subregion dictionary comprising the appearance of the test subregion and a shape construction parameter of the test subregion. Statistics tracking the likelihood of appearance of a particular subregion dictionary entry and/or associated parameters are tracked and updated during both the training phase in which subregion dictionary <b>150</b> is generated. Such statistics are, in some embodiments, used for calculating a probability based on prior choices from a subregion dictionary of a selected subregion dictionary entry being an optimal subregion dictionary entry</p>
<p id="p-0060" num="0059">In some embodiments, a light-space graphical model shape from shading module <b>180</b> performs a light-space graphical model shape from shading method as described above and further described below with respect to FIGS. <b>5</b>B and <b>6</b>A-<b>6</b>C. A set of normal vectors corresponding to a set of pixels in input image <b>110</b> is defined. Each normal vector is defined in terms of the spherical coordinates (zenith and azimuth) in the coordinate system of the light source, such that the orientation of the light source corresponds to a zenith value of 0. The zenith of each normal vector is constrained based on an observed shading of a respective pixel of input image <b>110</b>. An output shape <b>130</b> is constructed from input image <b>110</b>. Constructing output shape <b>130</b> includes minimizing an energy function to specify an azimuth value and a zenith value of each normal vector in output shape <b>130</b>. The minimizing the energy function may further include constraining the azimuth of each normal vector in output shape <b>130</b> based on an image gradient of input image <b>110</b> at each respective pixel. The constraining the azimuth enforces a coplanar assumption between the image gradient in input image <b>110</b> expressed in a three-dimensional space and the respective normal vector in output shape <b>130</b>.</p>
<p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. 2</figref> illustrates a subregion dictionary that may be used to implement patch-based methods to derive shape from shading of images according to some embodiments. Subregion dictionary <b>150</b> contains a set of subregion dictionary entries <b>152</b><i>a</i>-<b>152</b><i>n</i>. In one embodiment, each of subregion dictionary entries <b>152</b><i>a</i>-<b>152</b><i>n </i>includes a subregion entry appearance <b>154</b><i>a</i>-<b>154</b><i>n </i>for matching an appearance of the respective image subregion, and a subregion dictionary entry geometry <b>156</b><i>a</i>-<b>156</b><i>n </i>comprising a shape construction parameter for constructing an output shape from an image. The subregion appearances <b>154</b><i>a</i>-<b>154</b><i>n </i>may be simple images, the similarity of which to a selected subregion of an image from which a shape is being constructed may be ascertained using conventional techniques that will be well-known to one of skill in the art in light of having read the present disclosure. In some embodiments, the subregion dictionary entry geometries <b>156</b><i>a</i>-<b>156</b><i>n </i>may be decomposed in components, in order to make the dictionary more compact and remove ambiguities. In some embodiments, the geometry <b>156</b><i>a</i>-<b>156</b><i>n </i>of each subregion dictionary entry <b>152</b><i>a</i>-<b>152</b><i>n </i>in subregion dictionary <b>150</b> may be defined up to some parameters (e.g., an azimuth angle). Such an azimuth angle, for example, may be determined when reconstructing a surface represented by a subregion.</p>
<p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. 3</figref> depicts a set of test image files for developing a subregion dictionary that may be used to implement machine learning techniques in the context of patch-based methods to derive shape from shading of images according to some embodiments. Each of training image entries <b>182</b><i>a</i>-<b>182</b><i>n </i>contains a test image <b>184</b><i>a</i>-<b>184</b><i>n </i>and a normal map <b>186</b><i>a</i>-<b>186</b><i>n</i>. In the -training phase described below with respect to <figref idref="DRAWINGS">FIG. 7B</figref>, the set of training image entries <b>184</b><i>a</i>-<b>184</b><i>n </i>and corresponding known normal maps <b>186</b><i>a</i>-<b>186</b><i>n </i>are used as input. Each of the set of training image entries <b>184</b><i>a</i>-<b>184</b><i>n </i>and the corresponding one of normal maps <b>186</b><i>a</i>-<b>186</b><i>n </i>is divided into (possibly overlapping) patches or subregions of a given size (e.g., m&#xd7;n or m&#xd7;m). Each patch or subregion is added to a subregion dictionary, such as subregion dictionary <b>150</b>, if the subregion differs substantially from the patches or subregions already stored in subregion dictionary <b>150</b>. Metrics and thresholds for determining whether the subregion differs substantially from the patches or subregions already stored in subregion dictionary <b>150</b> will vary between embodiments without departing from the scope of the present disclosure.</p>
<p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. 4A</figref> illustrates a sample image on which methods to derive shape from shading of images according to some embodiments may be used. An input image <b>110</b> provides the input data from which a shape from shading method can be used to generate an output shape <b>140</b>. Input image <b>110</b> is composed of pixels (not individually segmented in figure). In some embodiments, a light-space graphical model is used to derive a surface normal vector from each pixel, as described above with respect to light-space graphical model shape-from shading module <b>180</b> and further described below with respect to figures FIGS. <b>5</b>B and <b>6</b>A-<b>6</b>C. In some embodiments, a patch-based shape from shading method is used to derive a surface normal vector from groups of pixels as described above with respect to patch-based shape from shading module <b>160</b> and further described below with respect to <figref idref="DRAWINGS">FIGS. 2</figref>, <b>3</b>, <b>5</b>C, and <b>7</b>A-<b>9</b>. In some embodiments, both light-space and patch-based methods may be used to generate and refine output shape <b>140</b>.</p>
<p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. 4B</figref> depicts a depth field that may result from use of methods to derive shape from shading of images according to some embodiments. Output shape <b>130</b> is composed of a set of surface normal vectors (not individually segmented in figure) derived from input image <b>110</b>. In some embodiments, a light-space graphical model is used to derive the surface normal vectors from pixels of an input image, as described above with respect to light-space graphical model shape-from shading module <b>180</b> and further described below with respect to figures FIGS. <b>5</b>B and <b>6</b>A-<b>6</b>C. In some embodiments, a patch-based shape from shading method is used to derive the surface normal vectors from groups of pixels as described above with respect to patch-based shape from shading module <b>160</b> and further described below with respect to <figref idref="DRAWINGS">FIGS. 2</figref>, <b>3</b>, <b>5</b>C, and <b>7</b>A-<b>9</b>.</p>
<p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. 5A</figref> illustrates light striking a surface and the estimation of a surface normal vector at a single pixel location according to some embodiments. Incident light <b>510</b> strikes a surface <b>500</b> at a pixel location <b>515</b>. A normal vector <b>520</b> at the pixel location <b>515</b> is a vector perpendicular to surface <b>500</b> at pixel location <b>515</b>. Embodiments of the invention attempt to identify normal vector <b>520</b> based on the assumption that surface <b>500</b> exhibits Lambertian (i.e., exclusively diffuse) reflectance, such that the brightness of the surface at pixel location <b>500</b> depends on the orientation of the surface at pixel location <b>515</b> to the incident light <b>510</b> from a light source.</p>
<p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. 5B</figref> depicts surface normal vectors at multiple pixel locations according to some embodiments. In a given region of input image <b>110</b>, each of pixels <b>540</b><i>a</i>-<b>540</b><i>i </i>has a respective one of normal vectors <b>550</b><i>a</i>-<b>550</b><i>i</i>. Embodiments of the invention attempt to identify normal vectors <b>550</b><i>a</i>-<b>550</b><i>i </i>based on the assumption that surface <b>500</b> exhibits Lambertian (i.e., exclusively diffuse) reflectance. Some embodiments may retrieve the shape of an object (e.g., normal vectors <b>550</b><i>a</i>-<b>550</b><i>i</i>) from its appearance, and more particularly, its shading (e.g., brightness of pixels <b>540</b><i>a</i>-<b>540</b><i>i</i>), as it appears in a single image. In some embodiments, the shape may be retrieved in terms of per-pixel normal vectors (e.g., normal vectors <b>550</b><i>a</i>-<b>550</b><i>i</i>), as described above with respect to light-space graphical model shape-from shading module <b>180</b> and further described below with respect to <figref idref="DRAWINGS">FIGS. 6A-6C</figref>.</p>
<p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. 5C</figref> illustrates surface normal vectors at multiple subregion locations, each subregion comprising multiple pixels, according to some embodiments. For each of several subregions (not labeled) of input image <b>110</b>, one of several subregion normal vectors <b>570</b><i>a</i>-<b>570</b><i>n </i>exists. The size and configuration of pixel groups used as subregions will vary between embodiments, and subregions may overlap. For instance, in one embodiment, a subregion associated with normal <b>570</b><i>a </i>may include pixels <b>560</b><i>a</i>-<b>560</b><i>i</i>. In such an embodiment, a subregion associated with normal <b>570</b><i>b </i>may include pixels <b>560</b><i>j</i>-<b>650</b><i>r</i>. A subregion associated with normal <b>570</b><i>c </i>may include pixels <b>560</b><i>s</i>-<b>560</b><i>aa</i>, and a subregion associated with normal <b>570</b><i>d </i>may include pixels <b>560</b><i>ab</i>-<b>560</b><i>aj</i>. In an embodiment that accommodates overlapping subregions, a subregion associated with normal <b>570</b><i>a </i>may include pixels <b>560</b><i>a</i>-<b>560</b><i>i </i>as well as pixels <b>560</b><i>j</i>, <b>560</b><i>m</i>, <b>560</b><i>p</i>, <b>560</b><i>ab</i>, and <b>560</b><i>s</i>-<i>u</i>. In such an embodiment, a subregion associated with normal <b>570</b><i>b </i>may include pixels <b>560</b><i>j</i>-<b>560</b><i>r </i>as well as pixels <b>560</b><i>c</i>, <b>560</b><i>f</i>, <b>560</b><i>i</i>, <b>560</b><i>u</i>, and <b>560</b><i>ab</i>-<i>ad</i>. A subregion associated with normal <b>570</b><i>c </i>may include pixels <b>560</b><i>s</i>-<b>560</b><i>aa </i>as well as pixels <b>560</b><i>ab</i>, <b>560</b><i>ae</i>, <b>560</b><i>ah</i>, <b>560</b><i>p</i>, and <b>560</b><i>g</i>-<i>i</i>, and a subregion associated with normal <b>570</b><i>d </i>may include pixels <b>560</b><i>ab</i>-<b>560</b><i>aj </i>as well as pixels <b>560</b><i>u</i>, <b>560</b><i>x</i>, <b>560</b><i>aa</i>, <b>560</b><i>i</i>, and <b>560</b><i>p</i>-<i>r</i>. In some embodiments, a patch-based shape from shading method is performed as described above with respect to patch-based shape from shading module <b>160</b> and further described below with respect to <figref idref="DRAWINGS">FIGS. 7A-9</figref>.</p>
<p id="h-0010" num="0000">Light-Space Model Operations</p>
<p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. 6A</figref> depicts operations that may be performed in the context of light-space methods to derive shape from shading of images according to some embodiments. With respect to <figref idref="DRAWINGS">FIG. 6A</figref>, and more generally with respect to <figref idref="DRAWINGS">FIGS. 6A-9</figref>, while the operations shown various blocks are given a sequential appearance and explanation for the sake of clarity, one of skill in the art will readily understand in light of having read the present disclosure that the sequential presentation is not intended to imply sequential execution in actual embodiments. More specifically, embodiments may perform steps of some flowcharts in order other than the order of presentation and may omit some steps or add other steps without departing from the scope and intent of the present disclosure.</p>
<p id="p-0069" num="0068">A light source location for an image is estimated (optional) (block <b>605</b>). A set of normal vectors corresponding to a set of pixels in the image is defined (block <b>610</b>). One of skill in the art will readily understand, in light of having read the present disclosure, that while the vectors are defined, in some embodiments, at block <b>610</b>, their values are defined subsequently. A zenith of each normal vector is constrained based on an observed shading of a respective pixel of the set of pixels (block <b>620</b>). A shape is constructed from the image by minimizing an energy function of the normal vectors (block <b>630</b>). Refinements to the shape are performed (optional) (block <b>635</b>) as described above.</p>
<p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. 6B</figref> illustrates additional operations that may be performed in the context of light-space methods to derive shape from shading of images according to some embodiments. The azimuth of each normal vector is constrained based on an image gradient of the image at each respective pixel to enforce a coplanar assumption between the image gradient expressed in three-dimensional space and the respective normal vector (block <b>640</b>). An integrability constraint is enforced (block <b>650</b>). An energy function of the set of normal vectors is minimized to specify an azimuth value and a zenith value, respectively, of each normal vector (block <b>655</b>).</p>
<p id="p-0071" num="0070"><figref idref="DRAWINGS">FIG. 6C</figref> depicts operations that may be performed to support calculations useful in light-space methods to derive shape from shading of images according to some embodiments. A potential function constraining a reconstructed intensity of each respective pixel given the zenith value of each respective pixel to an observed value of intensity of each respective pixel in the image is calculated (block <b>660</b>). A smoothness function constraining the zenith of each of the normal vectors to converge to a respective zenith value minimizing a difference between the zenith of each of the normal vectors and the zeniths of one or more neighboring normal vectors is calculated (block <b>670</b>). A smoothness function constraining the azimuth of each of the normal vectors to converge to a respective azimuth value minimizing a difference between the azimuth of each of the normal vectors and the azimuths of one or more neighboring normal vectors is calculated (block <b>680</b>). A function expressing a dependency of each azimuth value on an observed image gradient at a respective pixel is calculated (block <b>690</b>).</p>
<p id="h-0011" num="0000">Patch-Based Model Operations</p>
<p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. 7A</figref> illustrates operations that may be performed in the context of patch-based methods to derive shape from shading of images according to some embodiments. An image is divided into a set of image subregions (block <b>700</b>). A set of subregion dictionary entries matching an appearance of each subregion is identified from a subregion dictionary (block <b>710</b>). Optimal subregion dictionary entries for each subregion are identified by minimizing an energy function of the normal vectors of the optimal subregion dictionary entries (block <b>720</b>). Refinements are performed (optional) as described above (block <b>730</b>).</p>
<p id="p-0073" num="0072"><figref idref="DRAWINGS">FIG. 7B</figref> depicts operations that may be performed in the context of machine-learning methods for developing a subregion dictionary that may be used to implement machine learning techniques in the context of patch-based methods to derive shape from shading of images according to some embodiments. A training image and a corresponding normal map are divided into subregions (block <b>740</b>). For a given subregion, a subregion appearance is calculated (block <b>750</b>). The subregion appearance is compared to subregion appearances for dictionary entries (block <b>760</b>). Statistics reflecting whether the subregion appearance matched a subregion appearance of an existing dictionary entry are updated (block <b>770</b>). If no dictionary entry matches a subregion appearance, a subregion is added to a subregion dictionary as a new entry comprising a subregion appearance and a set of shape parameters, which may include geometry, a set of normal vectors (block <b>780</b>).</p>
<p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. 8</figref> illustrates additional operations that may be performed in the context of patch-based methods to derive shape from shading of images according to some embodiments. An image is divided into n overlapping test patches (block <b>800</b>). For each test patch, k patches in a dictionary with the most similar appearance are found (block <b>810</b>). A global optimization is performed. For each test patch, one of the k dictionary matches and accompanying parameters are found to minimize the model energy E (block <b>820</b>). A surface geometry is reconstructed from selected patches and patch parameters (block <b>830</b>). The surface geometry is refined to improve reconstruction of details, smoothness, and/or integrability (block <b>840</b>).</p>
<p id="p-0075" num="0074"><figref idref="DRAWINGS">FIG. 9</figref> depicts operations that may be performed to support calculations useful in patch-based methods to derive shape from shading of images according to some embodiments. A term representing a difference between, for each image subregion, a subregion entry appearance of an optimal subregion dictionary entry and an appearance of the respective image subregion is calculated (block <b>900</b>). A term representing a probability based on prior choices from a subregion dictionary of a selected subregion dictionary entry being an optimal subregion dictionary is calculated (block <b>910</b>). A term representing an incongruity between an optimal subregion dictionary entry for a selected image subregion and optimal subregion dictionary entries of adjacent image subregions is calculated (block <b>920</b>). A term penalizing violations of surface integrability along shared spaces of image subregions is calculated (block <b>930</b>).</p>
<heading id="h-0012" level="1">EXAMPLE SYSTEM</heading>
<p id="p-0076" num="0075">Embodiments of a shape from shading module and/or the various shape from shading techniques as described herein may be executed on one or more computer systems, which may interact with various other devices. One such computer system is illustrated by <figref idref="DRAWINGS">FIG. 10</figref>. In different embodiments, computer system <b>1000</b> may be any of various types of devices, including, but not limited to, a personal computer system, desktop computer, laptop, notebook, or netbook computer, mainframe computer system, handheld computer, workstation, network computer, a camera, a set top box, a mobile device, a consumer device, video game console, handheld video game device, application server, storage device, a peripheral device such as a switch, modem, router, or in general any type of computing or electronic device.</p>
<p id="p-0077" num="0076">In the illustrated embodiment, computer system <b>1000</b> includes one or more processors <b>1010</b> coupled to a system memory <b>1020</b> via an input/output (I/O) interface <b>1030</b>. Computer system <b>1000</b> further includes a network interface <b>1040</b> coupled to I/O interface <b>1030</b>, and one or more input/output devices <b>1050</b>, such as cursor control device <b>1060</b>, keyboard <b>1070</b>, and display(s) <b>1080</b>. In some embodiments, it is contemplated that embodiments may be implemented using a single instance of computer system <b>1000</b>, while in other embodiments multiple such systems, or multiple nodes making up computer system <b>1000</b>, may be configured to host different portions or instances of embodiments. For example, in one embodiment some elements may be implemented via one or more nodes of computer system <b>1000</b> that are distinct from those nodes implementing other elements.</p>
<p id="p-0078" num="0077">In various embodiments, computer system <b>1000</b> may be a uniprocessor system including one processor <b>1010</b>, or a multiprocessor system including several processors <b>1010</b> (e.g., two, four, eight, or another suitable number). Processors <b>1010</b> may be any suitable processor capable of executing instructions. For example, in various embodiments, processors <b>1010</b> may be general-purpose or embedded processors implementing any of a variety of instruction set architectures (ISAs), such as the x86, PowerPC, SPARC, or MIPS ISAs, or any other suitable ISA. In multiprocessor systems, each of processors <b>1010</b> may commonly, but not necessarily, implement the same ISA.</p>
<p id="p-0079" num="0078">In some embodiments, at least one processor <b>1010</b> may be a graphics processing unit. A graphics processing unit or GPU may be considered a dedicated graphics-rendering device for a personal computer, workstation, game console or other computing or electronic device. Modern GPUs may be very efficient at manipulating and displaying computer graphics, and their highly parallel structure may make them more effective than typical CPUs for a range of complex graphical algorithms. For example, a graphics processor may implement a number of graphics primitive operations in a way that makes executing them much faster than drawing directly to the screen with a host central processing unit (CPU). In various embodiments, the image processing methods disclosed herein may, at least in part, be implemented by program instructions configured for execution on one of, or parallel execution on two or more of, such GPUs. The GPU(s) may implement one or more application programmer interfaces (APIs) that permit programmers to invoke the functionality of the GPU(s). Suitable GPUs may be commercially available from vendors such as NVIDIA Corporation, ATI Technologies (AMD), and others.</p>
<p id="p-0080" num="0079">System memory <b>1020</b> may be configured to store program instructions and/or data accessible by processor <b>1010</b>. In various embodiments, system memory <b>1020</b> may be implemented using any suitable memory technology, such as static random access memory (SRAM), synchronous dynamic RAM (SDRAM), nonvolatile/Flash-type memory, or any other type of memory. In the illustrated embodiment, program instructions and data implementing desired functions, such as those described above for embodiments of a shape from shading module are shown stored within system memory <b>1020</b> as program instructions <b>1025</b> and data storage <b>1035</b>, respectively. In other embodiments, program instructions and/or data may be received, sent or stored upon different types of computer-accessible media or on similar media separate from system memory <b>1020</b> or computer system <b>1000</b>. Generally speaking, a computer-accessible medium may include storage media or memory media such as magnetic or optical media, e.g., disk or CD/DVD-ROM coupled to computer system <b>1000</b> via I/O interface <b>1030</b>. Program instructions and data stored via a computer-accessible medium may be transmitted by transmission media or signals such as electrical, electromagnetic, or digital signals, which may be conveyed via a communication medium such as a network and/or a wireless link, such as may be implemented via network interface <b>1040</b>.</p>
<p id="p-0081" num="0080">In one embodiment, I/O interface <b>1030</b> may be configured to coordinate I/O traffic between processor <b>1010</b>, system memory <b>1020</b>, and any peripheral devices in the device, including network interface <b>1040</b> or other peripheral interfaces, such as input/output devices <b>1050</b>. In some embodiments, I/O interface <b>1030</b> may perform any necessary protocol, timing or other data transformations to convert data signals from one component (e.g., system memory <b>1020</b>) into a format suitable for use by another component (e.g., processor <b>1010</b>). In some embodiments, I/O interface <b>1030</b> may include support for devices attached through various types of peripheral buses, such as a variant of the Peripheral Component Interconnect (PCI) bus standard or the Universal Serial Bus (USB) standard, for example. In some embodiments, the function of I/O interface <b>1030</b> may be split into two or more separate components, such as a north bridge and a south bridge, for example. In addition, in some embodiments some or all of the functionality of I/O interface <b>1030</b>, such as an interface to system memory <b>1020</b>, may be incorporated directly into processor <b>1010</b>.</p>
<p id="p-0082" num="0081">Network interface <b>1040</b> may be configured to allow data to be exchanged between computer system <b>1000</b> and other devices attached to a network, such as other computer systems, or between nodes of computer system <b>1000</b>. In various embodiments, network interface <b>1040</b> may support communication via wired or wireless general data networks, such as any suitable type of Ethernet network, for example; via telecommunications/telephony networks such as analog voice networks or digital fiber communications networks; via storage area networks such as Fibre Channel SANs, or via any other suitable type of network and/or protocol.</p>
<p id="p-0083" num="0082">Input/output devices <b>1050</b> may, in some embodiments, include one or more display terminals, keyboards, keypads, touchpads, scanning devices, voice or optical recognition devices, or any other devices suitable for entering or retrieving data by one or more computer system <b>1000</b>. Multiple input/output devices <b>1050</b> may be present in computer system <b>1000</b> or may be distributed on various nodes of computer system <b>1000</b>. In some embodiments, similar input/output devices may be separate from computer system <b>1000</b> and may interact with one or more nodes of computer system <b>1000</b> through a wired or wireless connection, such as over network interface <b>1040</b>.</p>
<p id="p-0084" num="0083">As shown in <figref idref="DRAWINGS">FIG. 10</figref>, memory <b>1020</b> may include program instructions <b>1025</b>, configured to implement embodiments of a shape from shading module as described herein, and data storage <b>1035</b>, comprising various data accessible by program instructions <b>1025</b>. In one embodiment, program instructions <b>1025</b> may include software elements of embodiments of a shape from shading module as illustrated in the above Figures. Data storage <b>1035</b> may include data that may be used in embodiments. In other embodiments, other or different software elements and data may be included.</p>
<p id="p-0085" num="0084">Those skilled in the art will appreciate that computer system <b>1000</b> is merely illustrative and is not intended to limit the scope of a shape from shading module as described herein. In particular, the computer system and devices may include any combination of hardware or software that can perform the indicated functions, including a computer, personal computer system, desktop computer, laptop, notebook, or netbook computer, mainframe computer system, handheld computer, workstation, network computer, a camera, a set top box, a mobile device, network device, internet appliance, PDA, wireless phones, pagers, a consumer device, video game console, handheld video game device, application server, storage device, a peripheral device such as a switch, modem, router, or in general any type of computing or electronic device. Computer system <b>1000</b> may also be connected to other devices that are not illustrated, or instead may operate as a stand-alone system. In addition, the functionality provided by the illustrated components may in some embodiments be combined in fewer components or distributed in additional components. Similarly, in some embodiments, the functionality of some of the illustrated components may not be provided and/or other additional functionality may be available.</p>
<p id="p-0086" num="0085">Those skilled in the art will also appreciate that, while various items are illustrated as being stored in memory or on storage while being used, these items or portions of them may be transferred between memory and other storage devices for purposes of memory management and data integrity. Alternatively, in other embodiments some or all of the software components may execute in memory on another device and communicate with the illustrated computer system via inter-computer communication. Some or all of the system components or data structures may also be stored (e.g., as instructions or structured data) on a computer-accessible medium or a portable article to be read by an appropriate drive, various examples of which are described above. In some embodiments, instructions stored on a computer-accessible medium separate from computer system <b>1000</b> may be transmitted to computer system <b>1000</b> via transmission media or signals such as electrical, electromagnetic, or digital signals, conveyed via a communication medium such as a network and/or a wireless link. Various embodiments may further include receiving, sending or storing instructions and/or data implemented in accordance with the foregoing description upon a computer-accessible medium. Accordingly, the present invention may be practiced with other computer system configurations.</p>
<p id="h-0013" num="0000">Conclusion</p>
<p id="p-0087" num="0086">Various embodiments may further include receiving, sending or storing instructions and/or data implemented in accordance with the foregoing description upon a computer-accessible medium. Generally speaking, a computer-accessible medium may include storage media or memory media such as magnetic or optical media, e.g., disk or DVD/CD-ROM, volatile or non-volatile media such as RAM (e.g. SDRAM, DDR, RDRAM, SRAM, etc.), ROM, etc., as well as transmission media or signals such as electrical, electromagnetic, or digital signals, conveyed via a communication medium such as network and/or a wireless link.</p>
<p id="p-0088" num="0087">The various methods as illustrated in the Figures and described herein represent example embodiments of methods. The methods may be implemented in software, hardware, or a combination thereof. The order of method may be changed, and various elements may be added, reordered, combined, omitted, modified, etc.</p>
<p id="p-0089" num="0088">Various modifications and changes may be made as would be obvious to a person skilled in the art having the benefit of this disclosure. It is intended that the invention embrace all such modifications and changes and, accordingly, the above description to be regarded in an illustrative rather than a restrictive sense.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08625931-20140107-M00001.NB">
<img id="EMI-M00001" he="7.45mm" wi="76.20mm" file="US08625931-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US08625931-20140107-M00002.NB">
<img id="EMI-M00002" he="7.03mm" wi="76.20mm" file="US08625931-20140107-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00003" nb-file="US08625931-20140107-M00003.NB">
<img id="EMI-M00003" he="7.45mm" wi="76.20mm" file="US08625931-20140107-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00004" nb-file="US08625931-20140107-M00004.NB">
<img id="EMI-M00004" he="7.45mm" wi="76.20mm" file="US08625931-20140107-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00005" nb-file="US08625931-20140107-M00005.NB">
<img id="EMI-M00005" he="7.45mm" wi="76.20mm" file="US08625931-20140107-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A computer-implemented method, comprising:
<claim-text>defining a set of normal vectors corresponding to a set of pixels in an image, each normal vector of the set of normal vectors defined in terms of an azimuth and a zenith measured in a spherical coordinate system centered on a light source illuminating the image;</claim-text>
<claim-text>constraining the zenith of each normal vector based on an observed shading of a respective pixel of the set of pixels; and</claim-text>
<claim-text>constructing a shape from the image that comprises minimizing an energy function of the set of normal vectors, the azimuth and the zenith treated separately to specify constraints on an azimuth value and a zenith value for the azimuth and the zenith, respectively, of each normal vector,</claim-text>
<claim-text>the minimizing the energy function further comprises constraining the azimuth of each normal vector based on an image gradient of the image at each respective pixel to enforce a coplanar assumption between the image gradient expressed in a 3-dimensional space and the respective normal vector.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the minimizing the energy function further comprises the energy function including a potential function constraining a reconstructed intensity of each respective pixel given the zenith value of each respective pixel to an observed value of intensity of each respective pixel in the image.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the minimizing the energy function further comprises the energy function including a smoothness function constraining the zenith of each of the normal vectors to converge to a respective zenith value minimizing a difference between the zenith of each of the normal vectors and the zeniths of one or more neighboring normal vectors.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the minimizing the energy function further comprises the energy function including a smoothness function constraining the azimuth of each of the normal vectors to converge to a respective azimuth value minimizing a difference between the azimuth of each of the normal vectors and the azimuths of one or more neighboring normal vectors.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the minimizing the energy function further comprises the energy function including a function expressing a dependency of each azimuth value on an observed image gradient at a respective pixel.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the minimizing the energy function further comprises the energy function including a function enforcing an integrability constraint.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising estimating a location of the light source illuminating the image.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. A system, comprising:
<claim-text>at least one processor; and</claim-text>
<claim-text>a memory comprising program instructions, wherein the program instructions are executable by the at least one processor to:</claim-text>
<claim-text>define a set of normal vectors corresponding to a set of pixels in an image, each normal vector of the set of normal vectors defined in terms of an azimuth and a zenith measured in a spherical coordinate system centered on a light source illuminating the image;</claim-text>
<claim-text>constrain the zenith of each normal vector based on an observed shading of a respective pixel of the set of pixels; and</claim-text>
<claim-text>construct a shape from the image that comprises minimizing an energy function of the set of normal vectors, the azimuth and the zenith treated separately to specify constraints on an azimuth value and a zenith value for the azimuth and the zenith, respectively, of each normal vector,</claim-text>
<claim-text>said minimizing the energy function further comprises constraining the azimuth of each normal vector based on an image gradient of the image at each respective pixel to enforce a coplanar assumption between the image gradient expressed in a 3-dimensional space and the respective normal vector.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the program instructions executable by the at least one processor to minimize the energy function further comprise the energy function including a potential function configured to constrain a reconstructed intensity of each respective pixel given the zenith value of each respective pixel to an observed value of intensity of each respective pixel in the image.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the program instructions executable by the at least one processor to minimize the energy function further comprise the energy function including a smoothness function configured to constrain the zenith of each of the normal vectors to converge to a respective zenith value minimizing a difference between the zenith of each of the normal vectors and the zeniths of one or more neighboring normal vectors.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the program instructions executable by the at least one processor to minimize the energy function further comprise the energy function including a smoothness function configured to constrain the azimuth of each of the normal vectors to converge to a respective azimuth value minimizing a difference between the azimuth of each of the normal vectors and the azimuths of one or more neighboring normal vectors.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the program instructions executable by the at least one processor to minimize the energy function further comprise the energy function including a function expressing a dependency of each azimuth value on an observed image gradient at a respective pixel.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the program instructions executable by the at least one processor to minimize the energy function further comprise the energy function including a function enforcing an integrability constraint.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising program instructions executable by the at least one processor to estimate a location of the light source illuminating the image.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. A non-transitory computer-readable storage medium storing program instructions, wherein the program instructions are computer-executable to implement:
<claim-text>defining a set of normal vectors corresponding to a set of pixels in an image, each normal vector of the set of normal vectors defined in terms of an azimuth and a zenith measured in a spherical coordinate system centered on a light source illuminating the image;</claim-text>
<claim-text>constraining the zenith of each normal vector based on an observed shading of a respective pixel of the set of pixels; and</claim-text>
<claim-text>constructing a shape from the image, that comprises minimizing an energy function of the set of normal vectors, the azimuth and the zenith treated separately to specify constraints on an azimuth value and a zenith value for the azimuth and the zenith, respectively, of each normal vector,</claim-text>
<claim-text>the minimizing the energy function further comprises constraining the azimuth of each normal vector based on an image gradient of the image at each respective pixel to enforce a coplanar assumption between the image gradient expressed in a 3-dimensional space and the respective normal vector.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the program instructions are computer executable to implement said minimizing the energy function including a potential function constraining a reconstructed intensity of each respective pixel given the zenith value of each respective pixel to an observed value of intensity of each respective pixel in the image.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the program instructions are computer executable to implement said minimizing the energy function including a smoothness function constraining the zenith of each of the normal vectors to converge to a respective zenith value minimizing a difference between the zenith of each of the normal vectors and the zeniths of one or more neighboring normal vectors.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the program instructions are computer executable to implement said minimizing the energy function including a smoothness function constraining the azimuth of each of the normal vectors to converge to a respective azimuth value minimizing a difference between the azimuth of each of the normal vectors and the azimuths of one or more neighboring normal vectors.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the program instructions are computer executable to implement said minimizing the energy function including a function expressing a dependency of each azimuth value on an observed image gradient at a respective pixel.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the program instructions are computer executable to implement said minimizing the energy function including a function enforcing an integrability constraint.</claim-text>
</claim>
</claims>
</us-patent-grant>
