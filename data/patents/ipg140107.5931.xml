<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08627040-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08627040</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13731760</doc-number>
<date>20121231</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>12</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>711203</main-classification>
<further-classification>711103</further-classification>
<further-classification>711118</further-classification>
<further-classification>711154</further-classification>
<further-classification>710 22</further-classification>
<further-classification>709217</further-classification>
</classification-national>
<invention-title id="d2e43">Processor-bus-connected flash storage paging device using a virtual memory mapping table and page faults</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>7421557</doc-number>
<kind>B2</kind>
<name>Lee et al.</name>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>2007/0088867</doc-number>
<kind>A1</kind>
<name>Cho et al.</name>
<date>20070400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>2009/0240869</doc-number>
<kind>A1</kind>
<name>O'Krafka et al.</name>
<date>20090900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>2010/0293420</doc-number>
<kind>A1</kind>
<name>Kapil et al.</name>
<date>20101100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>2011/0055458</doc-number>
<kind>A1</kind>
<name>Kuehne</name>
<date>20110300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00006">
<othercit>Caulfield, A. M., Grupp, L. M., and Swanson, S., Gordon: Using Flash Memory to Build Fast, Power-efficient Clusters for Data-intensive Applications, ASPLOS'09, Mar. 7-11, 2009, pp. 1-12.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>9</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>711103</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>711118</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>711154</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>711203</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>710 22</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>709217</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>8</number-of-drawing-sheets>
<number-of-figures>8</number-of-figures>
</figures>
<us-related-documents>
<division>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>13345410</doc-number>
<date>20120106</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>8370533</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13731760</doc-number>
</document-id>
</child-doc>
</relation>
</division>
<division>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>12572189</doc-number>
<date>20091001</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>8176220</doc-number>
<date>20120508</date>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13345410</doc-number>
</document-id>
</child-doc>
</relation>
</division>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20130145086</doc-number>
<kind>A1</kind>
<date>20130606</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only" applicant-authority-category="assignee">
<addressbook>
<orgname>Oracle International Corporation</orgname>
<address>
<city>Redwood City</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Koka</last-name>
<first-name>Pranay</first-name>
<address>
<city>Austin</city>
<state>TX</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>McCracken</last-name>
<first-name>Michael Oliver</first-name>
<address>
<city>Austin</city>
<state>TX</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Schwetman, Jr.</last-name>
<first-name>Herbert Dewitt</first-name>
<address>
<city>Austin</city>
<state>TX</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="004" designation="us-only">
<addressbook>
<last-name>Bonebakker</last-name>
<first-name>Jan Lodewijk</first-name>
<address>
<city>Amersfoort</city>
<country>NL</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Osha Liang LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Oracle America, Inc.</orgname>
<role>02</role>
<address>
<city>Redwood City</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Kim</last-name>
<first-name>Hong</first-name>
<department>2188</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A method for accessing a virtual memory of a processor using a processor-bus-connected flash storage module (PFSM) as a first paging device and a hard disk drive (HDD) as a second paging device, the method including: allocating a first address partition and a second address partition of a virtual memory for a software application of a processor to the first paging device and the second paging device, respectively, identifying a virtual memory page in the first paging device responsive to a page fault of the virtual memory triggered by the software application, sending a page access request to the PFSM for accessing the virtual memory page responsive to the page fault, and receiving the virtual memory page from the PFSM based on a command of the processor bus issued by the PFSM in conjunction with performing a flash memory access in the flash memory using a flash page address.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="236.81mm" wi="181.78mm" file="US08627040-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="248.41mm" wi="158.83mm" file="US08627040-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="232.58mm" wi="178.22mm" file="US08627040-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="248.75mm" wi="194.06mm" file="US08627040-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="246.80mm" wi="161.12mm" file="US08627040-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="229.62mm" wi="170.69mm" file="US08627040-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="245.11mm" wi="158.50mm" file="US08627040-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="254.68mm" wi="185.17mm" file="US08627040-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="219.79mm" wi="173.31mm" file="US08627040-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application is a divisional application of U.S. patent application Ser. No. 13/345,410, filed on Jan. 6, 2012. U.S. patent application Ser. No. 13/345,410 is a divisional application of U.S. patent application Ser. No. 12/572,189, filed on Oct. 1, 2009. Accordingly, this application claims benefit of the filing dates of U.S. patent application Ser. No. 13/345,410 and U.S. patent application Ser. No. 12/572,189, under 35U.S.C. &#xa7;120. Both U.S. patent application Ser. No. 13/345,410 and U.S. patent application Ser. No. 12/572,189 are hereby incorporated by reference in their entirety.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">BACKGROUND</heading>
<p id="p-0003" num="0002">Over the years improvements in input/output (I/O) performance have not kept pace with those of processor or memory performance. As a result, computing systems are becoming largely I/O limited. Typical storage devices attached to computing nodes are hard disk drives (HDD) with spinning disk media and are attached to the computing nodes through I/O channels (e.g., PCI-express, etc.) as a local disk storage or through a storage area network (SAN) as a shared disk storage. The HDD based storage systems have the following disadvantages:</p>
<p id="p-0004" num="0003">(a) Long disk access latencies (e.g., on the order of milliseconds).</p>
<p id="p-0005" num="0004">(b) Long I/O bus and interface latencies.</p>
<p id="p-0006" num="0005">(c) Inefficient latency overhead for small size data access causing HDD storage to be unsuitable for random access of small data elements.</p>
<p id="p-0007" num="0006">(d) Limited capacity of HDD-integrated DRAM/SRAM caches due to space and power constraints.</p>
<p id="p-0008" num="0007">Paging is a method in a computer operating system to store and retrieve data from secondary storage for use in main memory. In particular, the operating system retrieves data from the secondary storage in same size blocks called pages. Paging is an important part of virtual memory implementation in most contemporary general-purpose operating systems, allowing them to use disk storage for data that does not fit into physical main memory. High performance computing (HPC) applications exhibit a wide range of memory access patterns from sequential to completely random accesses. As a result, the memory working sets also vary widely with different applications. From a virtual memory paging perspective, the memory access patterns can be classified as:</p>
<p id="p-0009" num="0008">(i) Sequential/random access patterns over a memory working set that does not exceed the capacity of the main memory (DRAM).</p>
<p id="p-0010" num="0009">(ii) Random access pattern over a large working set (i.e., a large number of memory pages) that exceed the capacity of the main memory (DRAM).</p>
<p id="p-0011" num="0010">Application workloads of category (i) benefit from high speed access to memory (e.g., to avoid long stalls) but do not require high speed paging devices. Application workloads of category (ii) benefit from high speed paging devices more than from fast memory. Generally speaking, application workloads of category (ii) are usually executed using a truncated datasets to avoid random access pattern exceeding the capacity of the main memory (DRAM) and the undesirable result of paging to long latency storage device.</p>
<p id="p-0012" num="0011">Flash memory is a non-volatile computer memory that can be electrically erased and rewritten in large blocks. HyperTransport is a processor interconnection technology with bidirectional serial/parallel high-bandwidth, low-latency point-to-point links and is promoted and developed by the HyperTransport Consortium. The technology is used by various vendors, for example in the form of !HyperTransport&#xae; (!HyperTransport&#xae; is a registered trademark of Advanced Micro Devices, Sunnyvale, Calif.). The Intel&#xae; Quickpath Interconnect (QPI) (Intel&#xae; is a registered trademark of Intel Corporation, Santa Clara, Calif.) is a point-to-point processor interconnect developed by Intel to compete with HyperTransport. Prior to the announcement of the name, Intel referred to it as Common System Interface (CSI). Earlier incarnations were known as YAP (Yet Another Protocol) and YAP+.</p>
<heading id="h-0003" level="1">SUMMARY</heading>
<p id="p-0013" num="0012">In general, in one aspect, the invention relates to a system includes multiple nodes coupled using a network of processor buses. The multiple nodes include a first processor node, including one or more processing cores and main memory, and a flash memory node coupled to the first processor node via a first processor bus of the network of processor buses. The flash memory node includes a flash memory including flash pages, a first memory including a cache partition for storing cached flash pages for the flash pages in the flash memory and a control partition for storing cache control data and contexts of requests to access the flash pages, and a logic module including a direct memory access (DMA) register and configured to receive a first request from the first processor node via the first processor bus to access the flash pages, wherein the first request is received using the DMA register that is mapped into an address space of the first processor node, store one or more parameters of the first request as a first context of the contexts stored in the control partition of the first memory, schedule a DMA operation responsive to the first request, and perform the DMA operation based on the first context, wherein the DMA operation transfers data between the flash memory and the first processor node and includes accessing the cache partition in the first memory when a portion of the flash pages is cached in the cache partition according to the cache control data.</p>
<p id="p-0014" num="0013">In general, in one aspect, the invention relates to a method for accessing a processor-bus-connected flash storage module (PFSM) operatively coupled to a processor via a processor bus. The PFSM includes a flash memory, a buffer memory, and a processor accessible command register mapped into an address space of the processor. The method includes receiving a page access request from the processor via the processor bus using the processor accessible command register, storing one or more parameters of the page access request in the buffer memory as a context of the page access request, scheduling a flash memory access responsive to receiving a trigger parameter of the one or more parameters, performing the scheduled flash memory access based on the context, and issuing a command of the processor bus in conjunction with performing the scheduled flash memory access to transfer data between the PFSM and the processor.</p>
<p id="p-0015" num="0014">In general, in one aspect, the invention relates to a method for accessing a virtual memory of a processor using a processor-bus-connected flash storage module (PFSM) as a first paging device and a hard disk drive (HDD) as a second paging device. The PFSM is operatively coupled to the processor via a processor bus. The PFSM includes a flash memory and a virtual address mapping table. The method includes allocating a first address partition and a second address partition of the virtual memory for a software application of the processor to the first paging device and the second paging device, respectively, identifying a virtual memory page in the first paging device responsive to a page fault of the virtual memory triggered by the software application, sending a page access request to the PFSM for accessing the virtual memory page responsive to the page fault, wherein the page access request is sent via the processor bus and comprises a virtual address of the virtual memory page, and receiving the virtual memory page from the PFSM based on a command of the processor bus issued by the PFSM in conjunction with performing a flash memory access in the flash memory using a flash page address, wherein the virtual address mapping table translates the virtual address of the virtual memory page to the flash page address in the flash memory.</p>
<p id="p-0016" num="0015">Other aspects of the invention will be apparent from the following description and the appended claims.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIGS. 1-3</figref> depict a block diagram of a system in accordance with one or more embodiments of the invention.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIGS. 4 and 5</figref> depict method flowcharts in accordance with one or more embodiments of the invention.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIGS. 6 and 7</figref> depict examples in accordance with one or more embodiments of the invention.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 8</figref> depicts a computer system in accordance with one or more embodiments of the invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0021" num="0020">Specific embodiments of the invention will now be described in detail with reference to the accompanying Figures. Like elements in the various Figures are denoted by like reference numerals for consistency.</p>
<p id="p-0022" num="0021">In the following detailed description of embodiments of the invention, numerous specific details are set forth in order to provide a more thorough understanding of the invention. However, it will be apparent to one of ordinary skill in the art that the invention may be practiced without these specific details. In other instances, well-known features have not been described in detail to avoid unnecessarily complicating the description.</p>
<p id="p-0023" num="0022">In general, embodiments of the invention provide a system and method for a flash memory (e.g., NOR (i.e., negation of OR operation) flash memory, NAND (i.e., negation of AND operation) flash memory, etc.) storage module that is attached directly to the host processor bus. Examples of such processor bus include HyperTransport, QPI, etc. The low latency access characteristics of the flash memory compared to HDD along with the high bandwidth, low-overhead characteristics of the processor bus result in a high performance storage system.</p>
<p id="p-0024" num="0023">Further, embodiments of the invention provide a system and method for optimizing performance on software workloads with various paging requirements based on a combination of a fast (flash memory based) virtual memory paging device and a slow (HDD based) paging device without affecting the performance of regular memory operations.</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIGS. 1-3</figref> depict a block diagram of a system (<b>100</b>) in accordance with one or more embodiments of the invention. Those skilled in the art, having the benefit of this detailed description, will appreciate the components shown in <figref idref="DRAWINGS">FIG. 1</figref> may differ among embodiments of the invention, and that one or more of the components may be optional. In one or more embodiments of the invention, one or more of the components shown in <figref idref="DRAWINGS">FIG. 1</figref> may be omitted, repeated, supplemented, and/or otherwise modified from that shown in <figref idref="DRAWINGS">FIG. 1</figref>. Accordingly, the specific arrangement of components shown in <figref idref="DRAWINGS">FIG. 1</figref> should not be construed as limiting the scope of the invention.</p>
<p id="p-0026" num="0025">As shown in <figref idref="DRAWINGS">FIG. 1</figref>, the system (<b>100</b>) includes a number of nodes, namely processor node (<b>101</b>), node A (<b>101</b>), node B (<b>102</b>), node C (<b>103</b>), node D (<b>104</b>), etc. coupled using an interconnection network (e.g., including segments <b>111</b>, <b>112</b>, <b>113</b>, <b>114</b>, etc.). The dotted lines represent additional nodes and interconnections not specifically shown in <figref idref="DRAWINGS">FIG. 1</figref>. At least one of the nodes is a processor node (<b>101</b>) of which more details are shown in <figref idref="DRAWINGS">FIG. 2</figref>. In one or more embodiments of the invention, other nodes (<b>102</b>, <b>103</b>, <b>104</b>, etc.) include other processor nodes and one or more processor-bus-connected flash storage module (PFSM) node of which more details are shown as the PFSM (<b>300</b>) in <figref idref="DRAWINGS">FIG. 3</figref>. In such embodiments, the PFSM (<b>300</b>) is connected to one or more such interconnection network segments and is accessible by processor nodes in the system (<b>100</b>) either via direct point-to-point connectivity or via forwarding. Further, interconnection network segment may be omitted among any adjacent PFSMs in the system (<b>100</b>).</p>
<p id="p-0027" num="0026">In one or more embodiments, segments (e.g., <b>111</b>, <b>112</b>, <b>113</b>, <b>114</b>, etc.) of the interconnection network may be configured in various configurations such as a nearest-neighbor configuration, a bus configuration, a star configuration, a switch configuration, or other suitable configurations. In one or more embodiments, the nodes (e.g., <b>101</b>, <b>102</b>, <b>103</b>, <b>104</b>, etc.) of the system (<b>100</b>) communicate with each other using segments (e.g., <b>111</b>, <b>112</b>, <b>113</b>, <b>114</b>, etc.) of the interconnection network according to a communication protocol, which is a convention or standard that controls and enables the connection, communication, and data transfer between any two nodes in communication. Generally speaking, communication protocols may be implemented by hardware, software, or combinations thereof.</p>
<p id="p-0028" num="0027">In one or more embodiments, each segment (e.g., <b>111</b>, <b>112</b>, <b>113</b>, <b>114</b>, etc.) of the interconnection network includes processor bus (e.g., HyperTransport, QPI, etc.) and the communication protocol is a processor bus protocol specific to the processor bus.</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 2</figref> depict a block diagram of the processor node (<b>101</b>) in accordance with one or more embodiments of the invention. Those skilled in the art, having the benefit of this detailed description, will appreciate the components shown in <figref idref="DRAWINGS">FIG. 2</figref> may differ among embodiments of the invention, and that one or more of the components may be optional. In one or more embodiments of the invention, one or more of the components shown in <figref idref="DRAWINGS">FIG. 2</figref> may be omitted, repeated, supplemented, and/or otherwise modified from that shown in <figref idref="DRAWINGS">FIG. 2</figref>. Accordingly, the specific arrangement of components shown in <figref idref="DRAWINGS">FIG. 2</figref> should not be construed as limiting the scope of the invention.</p>
<p id="p-0030" num="0029">As shown in <figref idref="DRAWINGS">FIG. 2</figref>, the processor node (<b>101</b>), which can also be used as any of the nodes (<b>102</b>, <b>103</b>, <b>104</b>, etc.) of <figref idref="DRAWINGS">FIG. 1</figref>, includes a CPU (central processing unit) (<b>200</b>), an I/O bus (<b>202</b>) for interfacing to an attached I/O device (<b>201</b>), and main memory (<b>203</b>) coupled to the CPU (<b>200</b>) via a memory bus (<b>204</b>). The CPU (<b>202</b>) further includes one or more processor cores, namely processor core A (<b>214</b>), processor core B (<b>213</b>), etc., an I/O bridge (<b>215</b>) for communicating with the I/O device (<b>210</b>), a memory controller and interface (<b>216</b>) for communicating with the main memory (<b>203</b>), and one or more processor bus ports (<b>211</b>, <b>212</b>, etc.) for communicating with other nodes (<b>102</b>, <b>103</b>, <b>104</b>, etc.) in the system (<b>100</b>) via segments (<b>111</b>, <b>112</b>, <b>113</b>, <b>114</b>, etc.) of the interconnection network described in reference to <figref idref="DRAWINGS">FIG. 1</figref> above. For example, the processor ports (<b>211</b>, <b>212</b>, etc.) may be attached to the processor bus connectors (<b>210</b>) for connecting to the segments (<b>111</b>, <b>112</b>, <b>113</b>, <b>114</b>, etc.). In one or more embodiments of the invention, the CPU (<b>202</b>) further includes a cache hierarchy (not shown), which together with the main memory (<b>203</b>) forms a CPU memory hierarchy.</p>
<p id="p-0031" num="0030">In one or more embodiments, the I/O bridge (<b>201</b>) is an I/O controller hub. Examples of the I/O device (<b>201</b>) includes data storage (e.g., HDD, solid state disk drive, etc.), communication (e.g., networking), and other peripheral devices. Generally speaking, the I/O device (<b>201</b>) may be external to the processor node (<b>101</b>) depending on the physical size. In particular, the processor bus and the processor bus protocol are optimized for communication among processor nodes (e.g., <b>101</b>) and are distinct from the I/O bus (<b>202</b>), the memory bus (<b>204</b>), and the respective associated communication protocols, which are optimized for I/O and memory accesses. Accordingly, bus interface functionalities of the processor bus port (<b>211</b>, <b>212</b>, etc.) are distinct from those of the I/O bridge (<b>215</b>) and the memory controller and interface (<b>216</b>).</p>
<p id="p-0032" num="0031">In one or more embodiments, the main memory stores instructions for execution by the processor cores (<b>213</b>, <b>214</b>, etc.) including functionalities such as accessing the PFSM as a flash disk, a heterogeneous paging device, etc. More details of such functionalities are described in reference to <figref idref="DRAWINGS">FIGS. 4-7</figref> below. In one or more embodiments, each of the processor cores (<b>213</b>, <b>214</b>, etc.) may execute one or more application threads of the aforementioned instructions.</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 3</figref> depict a block diagram of a PSFM (<b>300</b>) in accordance with one or more embodiments of the invention. Those skilled in the art, having the benefit of this detailed description, will appreciate the components shown in <figref idref="DRAWINGS">FIG. 3</figref> may differ among embodiments of the invention, and that one or more of the components may be optional. In one or more embodiments of the invention, one or more of the components shown in <figref idref="DRAWINGS">FIG. 3</figref> may be omitted, repeated, supplemented, and/or otherwise modified from that shown in <figref idref="DRAWINGS">FIG. 3</figref>. Accordingly, the specific arrangement of components shown in <figref idref="DRAWINGS">FIG. 3</figref> should not be construed as limiting the scope of the invention.</p>
<p id="p-0034" num="0033">As shown in <figref idref="DRAWINGS">FIG. 3</figref>, the PSFM (<b>300</b>), which can be used as any of the nodes (<b>102</b>, <b>103</b>, <b>104</b>, etc.) of <figref idref="DRAWINGS">FIG. 1</figref>, includes a logic module (<b>301</b>), one or more processor bus interfaces, namely processor bus interface A (<b>302</b>), processor bus interface B (<b>306</b>), etc., DRAM (dynamic random access memory) (<b>303</b>), SRAM (static random access memory) (<b>304</b>), and flash memory (<b>305</b>). In one or more embodiments of the invention, the logic module (<b>301</b>) is implemented as an ASIC (application specific integrated circuit) while the DRAM (<b>303</b>), the SRAM (<b>304</b>), and the flash memory (<b>305</b>) are memory arrays implemented using commercially available memories devices.</p>
<p id="p-0035" num="0034">As shown in <figref idref="DRAWINGS">FIG. 3</figref>, the logic module (<b>301</b>) further includes one or more register sets, namely register set A (<b>313</b>), register set B (<b>314</b>), etc., a DRAM controller for controlling the DRAM (<b>303</b>), and a flash controller (<b>312</b>) for controlling the flash memory (<b>305</b>). In one or more embodiments, the flash controller (<b>312</b>) is configured to optimize wear characteristics of flash memory cells in the flash memory (<b>305</b>).</p>
<p id="p-0036" num="0035">As shown in <figref idref="DRAWINGS">FIG. 3</figref>, the flash memory (<b>305</b>) further includes flash pages, namely flash page A (<b>331</b>), flash page B (<b>332</b>), etc., which are smallest size blocks of flash memory cells individually accessible (e.g., for read access, write access, etc.). For example, each of the flash pages (<b>331</b>, <b>332</b>, etc.) may consist of 2K-bit blocks, 4K-bit blocks, 8K-bit blocks, or other size blocks depending on the underlying design and technology of the flash memory (<b>305</b>).</p>
<p id="p-0037" num="0036">As shown in <figref idref="DRAWINGS">FIG. 3</figref>, the DRAM (<b>305</b>) further includes a cache partition (<b>341</b>) consisting of cached flash pages, namely cached flash page (<b>342</b>), cached flash page (<b>343</b>), etc. storing images of recently accessed flash pages (e.g., <b>331</b>, <b>332</b>, etc.) in the flash memory (<b>305</b>) according to a pre-determined cache policy, a cache replacement list (<b>344</b>) for storing information pertaining to the aforementioned cache policy, an address mapping table (<b>345</b>) for mapping addresses to ultimately reference physical locations in the flash memory (<b>305</b>), and register context (<b>346</b>) for extending the register sets (<b>313</b>, <b>314</b>, etc.). In one or more embodiments, the cache partition (<b>341</b>) is configured as a cache to the flash memory (<b>305</b>) with the cache replacement list (<b>344</b>) being a LRU (least recently used) list. In such embodiments, read accesses are first checked in the DRAM (<b>303</b>) before being read from the flash memory (<b>305</b>) while write accesses are first written to the DRAM (<b>303</b>) and subsequently transferred to the flash memory (<b>305</b>). In addition, cached flash pages may be evicted and replaced according to the aforementioned cache policy.</p>
<p id="p-0038" num="0037">As shown in <figref idref="DRAWINGS">FIG. 3</figref>, the SRAM (<b>304</b>) further includes a SRAM cache (<b>321</b>) for caching control data in the DRAM (<b>303</b>) outside of the cache partition (<b>341</b>). In one or more embodiments, the register context (<b>346</b>) are allocated in the DRAM (<b>303</b>) with accesses (reads/writes) to the contexts being performed via the SRAM (<b>304</b>) for faster access.</p>
<p id="p-0039" num="0038">In one or more embodiments, the processor bus interfaces (<b>302</b>, <b>306</b>, etc.) are configured to perform signaling, error detection, flow control, etc. over the attached segment (e.g., <b>111</b>, <b>112</b>, <b>113</b>, <b>114</b>, etc.) of the interconnection network described in reference to <figref idref="DRAWINGS">FIG. 1</figref>. For example, the processor bus interfaces (<b>302</b>, <b>306</b>, etc.) may be attached to the processor bus connectors (<b>310</b>) for connecting to the segments (<b>111</b>, <b>112</b>, <b>113</b>, <b>114</b>, etc.). In one or more embodiments, the processor bus connectors (<b>310</b>) are substantially the same as the processor bus connectors (<b>210</b>) of <figref idref="DRAWINGS">FIG. 2</figref>. As discussed above, in one or more embodiments, each segment (e.g., <b>111</b>, <b>112</b>, <b>113</b>, <b>114</b>, etc.) of the interconnection network consists of a processor bus. In such embodiments, the processor bus interface (<b>302</b>) is configured to perform signaling, error detection, flow control, etc. according to the processor bus protocol specific to the processor bus.</p>
<p id="p-0040" num="0039">In one or more embodiments, the logic module (<b>301</b>) is the processing engine of the PFSM (<b>300</b>). Specifically, the logic module (<b>301</b>) is configured to decode and/or issue commands using the processor bus protocol associated with the processor bus connecting the PFSM (<b>300</b>) and other processor node (e.g., <b>101</b>) of <figref idref="DRAWINGS">FIG. 1</figref>. In particular, the logic module (<b>301</b>) is configured to transfer data between the memory hierarchy (i.e., the flash memory (<b>305</b>), the DRAM (<b>303</b>), and/or SRAM (<b>304</b>)) and the processor bus in response to processor requests (e.g., for read access, write access, etc.) of processor nodes in the system (<b>100</b>).</p>
<p id="p-0041" num="0040">In one or more embodiments, the logic module (<b>301</b>) is further configured to perform data transfers between the flash memory (<b>305</b>) and the DRAM (<b>303</b>) according to the aforementioned cache policy. In particular, the logic module (<b>301</b>) is configured to perform replacement of cached flash pages (e.g., <b>342</b>, <b>343</b>, etc.) based on the cache replacement list (<b>344</b>) when cache miss occurs.</p>
<p id="p-0042" num="0041">In one or more embodiments, the logic module (<b>301</b>) is further configured to perform an address mapping function for the aforementioned data transfers between the memory hierarchy (i.e., the flash memory (<b>305</b>), the DRAM (<b>303</b>), and/or SRAM (<b>304</b>)) and the processor bus as well as the aforementioned data transfers between the flash memory (<b>305</b>) and the DRAM (<b>303</b>). More details of the address mapping function are described in the examples depicted in <figref idref="DRAWINGS">FIGS. 6-7</figref> below.</p>
<p id="p-0043" num="0042">In one or more embodiments, each of the register sets (<b>313</b>, <b>314</b>, etc.) includes four command registers, namely CMDREG_<b>1</b>, CMDREG_<b>2</b>, CMDREG_<b>3</b>, and CMDREG_<b>4</b> (not shown). Specifically, these four command registers are mapped to the processor address space of the processor node requesting access to the PFSM (<b>300</b>).</p>
<p id="p-0044" num="0043">In one or more embodiments, CMDREG_<b>1</b> is configured to store a command type field and a size field. For example, the command type field distinguishes between read and write commands while the size field specifies the size (e.g., number of flash pages) of data requested.</p>
<p id="p-0045" num="0044">In one or more embodiments, CMDREG_<b>2</b> is configured to store the address of the first flash page to be read or written to in the flash memory (<b>305</b>).</p>
<p id="p-0046" num="0045">In one or more embodiments, CMDREG_<b>3</b> is configured to store an address of the first page in main memory of the processor node requesting access to the PFSM (<b>300</b>).</p>
<p id="p-0047" num="0046">In one or more embodiments, CMDREG_<b>4</b> is configured as a trigger register that is used to initiate an operation in the logic module (<b>301</b>). In one or more embodiments, CMDREG_<b>4</b> is written to only after contents of CMDREG_<b>1</b>, CMDREG_<b>2</b>, and CMDREG_<b>3</b> are properly updated for the operation. In one or more embodiments, the operation is a DMA (direct memory access) operation while CMDREG_<b>1</b>, CMDREG_<b>2</b>, CMDREG_<b>3</b>, and CMDREG_<b>4</b> are DMA control registers.</p>
<p id="p-0048" num="0047">In one or more embodiments, multiple processor nodes in the system (<b>100</b>) are allowed to access the PFSM (<b>300</b>) in a concurrent manner. In one or more embodiments, multiple register sets (<b>313</b>, <b>314</b>, etc.) are multiplexed to support operations (e.g., DMA operations) for multiple threads/processor cores/processor nodes in the system (<b>100</b>). In one or more embodiments, the register sets (<b>313</b>, <b>314</b>, etc.) are extended for supporting concurrent accesses to the PFSM (<b>300</b>) from additional thread executions by storing additional copies of the four register values as context information in the register context (<b>346</b>).</p>
<p id="p-0049" num="0048">In such embodiments, the logic module (<b>301</b>) is further configured to allocate and maintain multiple contexts in the register context (<b>346</b>) for each of such additional thread executions to support multiple outstanding requests from requesting threads/processor cores/processor nodes in the system (<b>100</b>). In one or more embodiments, such multiple contexts in the register context (<b>346</b>) are cached in the SRAM (<b>304</b>) and allocated dynamically by indexing using a processor node ID, a processor core ID, a thread ID, and a request ID. In such embodiments, flow control between the processor cores and PFSM (<b>300</b>) is managed using the processor bus flow control mechanisms of the processor bus and the processor bus protocol.</p>
<p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. 4</figref> depicts a flowchart of read access method in accordance with one or more embodiments of the invention. In one or more embodiments of the invention, one or more of the steps shown in <figref idref="DRAWINGS">FIG. 4</figref> may be omitted, repeated, and/or performed in a different order. Accordingly, embodiments of the invention should not be considered limited to the specific arrangements of steps shown in <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0051" num="0050">In one or more embodiments of the invention, the method depicted in <figref idref="DRAWINGS">FIG. 4</figref> may be practiced using the system (<b>100</b>), the processor node (<b>101</b>), and the PFSM (<b>300</b>) described above with respect to <figref idref="DRAWINGS">FIGS. 1</figref>, <b>2</b>, and <b>3</b>, respectively.</p>
<p id="p-0052" num="0051">As shown in <figref idref="DRAWINGS">FIG. 4</figref>, initially in Step <b>401</b>, a page read request is received, for example by an operating system from an application both of which may be executing on at least the processor node (<b>101</b>) in the system (<b>100</b>) of <figref idref="DRAWINGS">FIG. 1</figref> above. Responsive to receiving such request, a PFSM operation (e.g., a DMA operation) may be scheduled, for example by a portion of the operating system (i.e., PFSM driver) that stores DMA control registers using parameters of the page read request. In one or more embodiments, the parameters of the page read request are stored in the DMA control registers using a CPU instruction (e.g., &#x201c;store&#x201d; instruction executed by the processor node (<b>101</b>)). In one or more embodiments, the DMA control registers includes the fours CMDREG registers as well as the register contexts described in reference to <figref idref="DRAWINGS">FIG. 3</figref> above.</p>
<p id="p-0053" num="0052">In Step <b>402</b>, a read command type, the size of requested data in number of flash pages, and the request ID are stored to a CMDREG_<b>1</b> register. In one or more embodiments, such stored information is obtained by parsing the page read request.</p>
<p id="p-0054" num="0053">In Step <b>403</b>, a requested address is stored to the CMDREG_<b>2</b> register. In one or more embodiments, the requested address is the address of the first flash page to be read. In one or more embodiments, such stored information is obtained by parsing the page read request.</p>
<p id="p-0055" num="0054">In Step <b>404</b>, a memory address of the requesting processor node is stored to the CMDREG_<b>2</b> register. In one or more embodiments, the memory address is the address of the first page in the main memory (of the processor sending the page read request) to receive the read data. In one or more embodiments, such stored information is obtained by parsing the page read request.</p>
<p id="p-0056" num="0055">In Step <b>405</b>, a trigger is stored to the CMDREG_<b>4</b>, for example as a signal to initiate the PFSM DMA operation.</p>
<p id="p-0057" num="0056">In one or more embodiments, at least a portion of the four DMA control register contents stored in Steps <b>402</b>-<b>405</b> are further stored in a DRAM buffer (e.g., the cache partition of <figref idref="DRAWINGS">FIG. 3</figref>) as a context for a requesting thread of the requesting processor node. In one or more embodiments, the context area in the DRAM buffer is allocated during Step <b>402</b>. In one or more embodiments, the contexts stored in the DRAM buffer is cached in a SRAM (e.g., <b>304</b> of <figref idref="DRAWINGS">FIG. 3</figref>).</p>
<p id="p-0058" num="0057">In Step <b>406</b>, a context is read based on a pre-determined schedule to initiate a corresponding PFSM operation. For example, the pre-determined schedule may be a polling schedule, an event driven schedule, etc. In one or more embodiments, upon receiving the store to CMDREG_<b>4</b>, the context is read to determine a read command for further processing.</p>
<p id="p-0059" num="0058">In Step <b>407</b>, data is read using a flash page address or a cached flash page address based on an address mapping table. For example, the address mapping table of <figref idref="DRAWINGS">FIG. 3</figref> may be checked to determine if the requested data is cached in the cache partition of the DRAM buffer of <figref idref="DRAWINGS">FIG. 3</figref>.</p>
<p id="p-0060" num="0059">In Step <b>408</b>, a cached flash page may be optionally evicted for replacement. For example, the page eviction may be performed when data is read from the flash memory that is not cached in the DRAM as described in reference to <figref idref="DRAWINGS">FIG. 3</figref> above. In such example, the flash page containing the read data may be cached where the evicted page resided while a cache replacement list and the address mapping table may be updated.</p>
<p id="p-0061" num="0060">In Step <b>409</b>, a processor-bus command is issued to transfer the data to processor memory when the data read operation is completed. In addition, a signal may be generated to notify the requesting processor node (specifically the requesting thread executing on a processor core of the requesting processor node) of the completion. For example, the processor-bus command and the signal may be based on the processor bus protocol described in reference to <figref idref="DRAWINGS">FIGS. 1-3</figref> above. In one or more embodiments, the signal is an interrupt signal.</p>
<p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. 5</figref> depicts a flowchart of write access method in accordance with one or more embodiments of the invention. In one or more embodiments of the invention, one or more of the steps shown in <figref idref="DRAWINGS">FIG. 5</figref> may be omitted, repeated, and/or performed in a different order. Accordingly, embodiments of the invention should not be considered limited to the specific arrangements of steps shown in <figref idref="DRAWINGS">FIG. 5</figref>.</p>
<p id="p-0063" num="0062">In one or more embodiments of the invention, the method depicted in <figref idref="DRAWINGS">FIG. 5</figref> may be practiced using the system (<b>100</b>), the processor node (<b>101</b>), and the PFSM (<b>300</b>) described above with respect to <figref idref="DRAWINGS">FIGS. 1</figref>, <b>2</b>, and <b>3</b>, respectively.</p>
<p id="p-0064" num="0063">As shown in <figref idref="DRAWINGS">FIG. 5</figref>, initially in Step <b>501</b>, a page write request is received, for example by an operating system from an application both of which may be executing on at least the processor node (<b>101</b>) in the system (<b>100</b>) of <figref idref="DRAWINGS">FIG. 1</figref> above. Responsive to receiving such request, in Step <b>502</b>, a PFSM read operation (e.g., as described in reference to <figref idref="DRAWINGS">FIG. 4</figref> above) may be scheduled, for example by a portion of the operating system (i.e., PFSM driver) that brings the flash page requested by the write request into the cache hierarchy as a cached page. For example, the cache hierarchy may include the cache hierarchy in the processor node of <figref idref="DRAWINGS">FIG. 2</figref> and the cached flash page in the cache partition of <figref idref="DRAWINGS">FIG. 3</figref>. Once the cached page corresponding to the requested flash page is in the cache hierarchy, in one or more embodiments, the page write request is performed to the cached page where the written page is marked dirty. In one or more embodiments, the dirty page is written back to the flash memory upon being evicted according to the cache policy governing the processor cache hierarchy.</p>
<p id="p-0065" num="0064">Steps <b>503</b>-<b>508</b> describe the ensuing write back operation. In Step <b>503</b>, a write command type, the size of requested data in number of flash pages, and the request ID are stored to a CMDREG_<b>1</b> register. In one or more embodiments, such stored information is obtained by parsing the page write request.</p>
<p id="p-0066" num="0065">In Step <b>504</b>, other CMDREG are stored in a similar manner as Steps <b>403</b>-<b>405</b> described in reference to <figref idref="DRAWINGS">FIG. 4</figref> above with the parameters consistent with the direction of DMA data transfer for the write operation, i.e., data is read from processor memory and written into the flash memory. In one or more embodiments, the parameters of the page write request are stored in the DMA control registers using a CPU instruction (e.g., &#x201c;store&#x201d; instruction executed by the processor node (<b>101</b>)).</p>
<p id="p-0067" num="0066">In one or more embodiments, at least a portion of the four DMA control register contents stored in Steps <b>503</b> and <b>504</b> are further stored in a DRAM buffer (e.g., the cache partition of <figref idref="DRAWINGS">FIG. 3</figref>) as a context for the aforementioned write back operation following the page eviction. In one or more embodiments, the context area in the DRAM buffer is allocated during Step <b>503</b>. In one or more embodiments, the contexts stored in the DRAM buffer is cached in a SRAM (e.g., <b>304</b> of <figref idref="DRAWINGS">FIG. 3</figref>).</p>
<p id="p-0068" num="0067">In Step <b>505</b>, a processor-bus command is issued to transfer data from the dirty page to be evicted from the cache hierarchy of the processor node (i.e., in the processor memory hierarchy) to a location in a PFSM DRAM buffer. In one or more embodiments, in Step <b>506</b>, cached flash page eviction is optionally performed if such location already stores a cached flash page described in reference to <figref idref="DRAWINGS">FIG. 3</figref> above. In such scenario, data write back to the flash memory is performed if the evicted cached flash page is marked dirty. At the same time, a cache replacement list and the address mapping table may be updated accordingly in Step <b>507</b>.</p>
<p id="p-0069" num="0068">In Step <b>508</b>, a signal may be generated upon completion of the data write back to the flash memory. For example, the processor-bus command of the Step <b>505</b> and the signal of the Step <b>508</b> may be based on the processor bus protocol described in reference to <figref idref="DRAWINGS">FIGS. 1-3</figref> above. In one or more embodiments, the signal is an interrupt signal to a particular processor core associated with the eviction of the processor page and the resulting write back.</p>
<p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. 6</figref> depicts an example in accordance with one or more embodiments of the invention. Those skilled in the art, having the benefit of this detailed description, will appreciate the components shown in <figref idref="DRAWINGS">FIG. 6</figref> may differ among embodiments of the invention, and that one or more of the components may be optional. In one or more embodiments of the invention, one or more of the components shown in <figref idref="DRAWINGS">FIG. 6</figref> may be omitted, repeated, supplemented, and/or otherwise modified from that shown in <figref idref="DRAWINGS">FIG. 6</figref>. Accordingly, the specific arrangement of components shown in <figref idref="DRAWINGS">FIG. 6</figref> should not be construed as limiting the scope of the invention.</p>
<p id="p-0071" num="0070">In the example of <figref idref="DRAWINGS">FIG. 6</figref>, the application (<b>601</b>) (e.g., a HPC application such as a scientific research or engineering application) and the I/O software stack (<b>600</b>) are instructions stored in various memory (e.g., main memory, cache hierarchy, and/or other system memory) of the processor node (<b>101</b>), depicted in <figref idref="DRAWINGS">FIG. 1</figref> above. For example, certain task of the application (<b>601</b>) may be executing in a thread, which in turn executes on one of the cores in the processor node (<b>101</b>). Generally speaking, the I/O software stack (<b>600</b>) is part of an operating system (not shown) environment supporting the application (<b>601</b>) for accessing the physical flash memory device, such as the flash memory (<b>655</b>) embedded in the PFSM (<b>650</b>) depicted in <figref idref="DRAWINGS">FIG. 3</figref> above. As discussed in reference to <figref idref="DRAWINGS">FIGS. 4 and 5</figref> above, the application (<b>601</b>) may access the flash memory (<b>305</b>) using at least the processor bus commands (<b>610</b>) described in reference to <figref idref="DRAWINGS">FIGS. 4 and 5</figref> above. As shown in <figref idref="DRAWINGS">FIG. 6</figref>, the I/O software stack (<b>600</b>) includes the system call layer (<b>602</b>), the virtual file system framework (<b>603</b>), the file system driver (<b>604</b>), the PFSM driver (<b>605</b>), and the platform specific infrastructure (<b>606</b>), which together generates such processor bus commands (<b>610</b>) when the application (<b>601</b>) accesses the flash memory (<b>305</b>). Various details of the processor node (<b>101</b>) and the PFSM (<b>300</b>) are not specifically shown in <figref idref="DRAWINGS">FIG. 6</figref> for clarity.</p>
<p id="p-0072" num="0071">For example during the course of execution, the application (<b>601</b>) may generate a request to access a data structure (not shown) through the I/O software stack (<b>600</b>) while a portion of such data structure may reside in the flash memory (<b>655</b>) of the PFSM (<b>650</b>), which is configured as a block device (i.e., accessible in data blocks) in the operating system environment. While the file system driver (<b>604</b>) contains the file system specific code (e.g., caching etc), the PFSM layer (i.e., the PFSM driver (<b>605</b>) and the platform specific infrastructure (<b>606</b>)) performs the functionality of a block device driver to interface with the PFSM (<b>650</b>). For different operating systems, the implementation of the I/O software stack can retain the existing block device driver and build the PFSM layer beneath the block device layer. As illustrated in the general layered structure of the I/O software stack (<b>600</b>), this invention does not depend upon a specific file system (e.g., the virtual file system framework (<b>603</b>) and the file system driver (<b>604</b>)) and does not affect the semantics of the file system operation (e.g., the system call layer (<b>602</b>)).</p>
<p id="p-0073" num="0072">As an example, the file system describes data storage in terms of pages (i.e., file system pages) with size &#x201c;P&#x201d; bytes. The flash memory (<b>655</b>) may have small block sizes (i.e., flash page size), for example &#x201c;B&#x201d; bytes, where &#x201c;B&#x201d;&#x3c;&#x3c;&#x201c;P&#x201d;. Since the PFSM (<b>650</b>) is configured as a block device, dirty blocks inside a page are tracked in the same manner as other existing block devices in the operating system environment. Other implementation specific bookkeeping can be maintained in the PFSM driver (<b>605</b>) and the platform specific infrastructure (<b>606</b>), which may be implemented as the PFSM layer beneath the generic block device driver.</p>
<p id="p-0074" num="0073">In the example of a read call, the application (<b>601</b>) invokes a system call in the system call layer (<b>602</b>) that calls appropriate handler in the file system (i.e., the virtual file system framework (<b>603</b>) and the file system driver (<b>604</b>)). The file system checks its cache for the presence of the requested page of the Read call. On a page miss, the file system identifies a victim page for eviction from the cache hierarchy of the processor node (<b>651</b>) and schedules a write back (e.g., described in reference to <figref idref="DRAWINGS">FIG. 5</figref> above) if the victim page is dirty. With the cache location of the evicted page now available, the file system schedules a page-read by calling the PFSM driver (<b>605</b>) with a page read request, which initiates the method steps described in reference to <figref idref="DRAWINGS">FIG. 4</figref> above.</p>
<p id="p-0075" num="0074">In this example, referring back to <figref idref="DRAWINGS">FIG. 3</figref>, the logic module (<b>301</b>) may be an ASIC that maintains the address mapping table (<b>345</b>) as a block-table at the flash block (i.e., flash page) granularity. The block table provides the mapping between the flash block (i.e., flash page) address and the cached flash page address in the cache partition (<b>341</b>). Since the flash block size &#x201c;B&#x201d; is less than the file system page size &#x201c;P&#x201d;, this helps to reduce the number of reads to the flash memory (<b>305</b>) for improved performance. Based on the block table, the ASIC schedules reads to the flash blocks that are not in the PFSM DRAM (<b>303</b>) and updates the cache replacement list (<b>344</b>), for example a LRU (i.e., least recently used) list of cached pages in the DRAM cache partition (<b>341</b>). On completion of the reads, the PFSM (<b>300</b>) issues processor-bus commands to transfer the data to processor memory (e.g., main memory (<b>203</b> in <figref idref="DRAWINGS">FIG. 2</figref>) of the processor node (<b>101</b> in <figref idref="DRAWINGS">FIG. 2</figref>)).</p>
<p id="p-0076" num="0075">Returning to <figref idref="DRAWINGS">FIG. 6</figref>, in the example of a write call, the application (<b>601</b>) invokes a system call in the system call layer (<b>602</b>) that calls the appropriate handler in the file system. The file system checks its cache for the presence of the requested page of the write call. On a page hit, the file system driver writes the application data to the file system cache and marks the block (i.e., the written file system page) dirty. On a page miss, the file system first schedules a page-read with the PFSM driver (<b>605</b>), then continues with the write back operation, i.e., when the file system evicts a dirty page it schedules a write back with the PFSM driver (<b>300</b>) using the page number of the page being evicted.</p>
<p id="p-0077" num="0076">To perform the write back operation, the PFSM driver (<b>605</b>) performs one or several DMA-write transactions to the PFSM (<b>650</b>) to write back the dirty blocks (i.e., the written file system page). Each DMA-write writes back multiple contiguous flash pages to the flash memory (<b>655</b>). The PFSM driver (<b>605</b>) performs a DMA-write by issuing four stores to the DMA register set in the PFSM (<b>650</b>) as described above. Responsive to the PFSM driver (<b>605</b>), the PFSM (<b>650</b>) allocates a write context on the first store. On receiving the fourth store, the ASIC issues processor bus commands to read data from the processor memory (e.g., main memory (<b>203</b> in <figref idref="DRAWINGS">FIG. 2</figref>) of the processor node (<b>101</b> in <figref idref="DRAWINGS">FIG. 2</figref>)). In particular, the ASIC allocates blocks in the cache-partition (<b>341</b> in <figref idref="DRAWINGS">FIG. 3</figref>) to cache the incoming data. The ASIC updates its block-table (<b>345</b> in <figref idref="DRAWINGS">FIG. 3</figref>) to reflect the mapping between corresponding flash pages and the cached flash pages written with the incoming data in the DRAM cache partition (<b>341</b> in <figref idref="DRAWINGS">FIG. 3</figref>). Dirty blocks (i.e., cached flash pages) evicted from the cache-partition (<b>341</b> in <figref idref="DRAWINGS">FIG. 3</figref>) are the written to the flash memory (<b>305</b> in <figref idref="DRAWINGS">FIG. 3</figref>). Further, the ASIC updates the LRU-list and marks the cached flash pages written with the incoming data dirty as well as interrupts the core executing the application (<b>601</b>) to indicate the completion of DMA.</p>
<p id="p-0078" num="0077">Since the processor bus is designed to support small sized data transfers of a cache line size or less, small updates representing partial updates to pages can be transferred from processor memory to PFSM (<b>650</b>) individually, without additional overheads. Hence, when using the PFSM (<b>650</b>), large sequential I/O accesses are not required to obtain good performance. This makes PFSM (<b>650</b>) suitable to random-patterned small writes such as in-node updates.</p>
<p id="p-0079" num="0078"><figref idref="DRAWINGS">FIG. 7</figref> depicts another example in accordance with one or more embodiments of the invention. Those skilled in the art, having the benefit of this detailed description, will appreciate that one or more of the steps shown in <figref idref="DRAWINGS">FIG. 7</figref> may be omitted, repeated, and/or performed in a different order. Accordingly, embodiments of the invention should not be considered limited to the specific arrangements of steps shown in <figref idref="DRAWINGS">FIG. 7</figref>.</p>
<p id="p-0080" num="0079">In an example, the operating system environment described in reference to <figref idref="DRAWINGS">FIG. 6</figref> above may support a virtual memory environment for the application (<b>601</b> in <figref idref="DRAWINGS">FIG. 6</figref>). <figref idref="DRAWINGS">FIG. 7</figref> shows a method for the application (<b>601</b> in <figref idref="DRAWINGS">FIG. 6</figref>) to access certain data structure in the virtual memory environment using a heterogeneous paging system including a combination of a hard disk (HDD) paging device (e.g., the I/O device (<b>201</b> in <figref idref="DRAWINGS">FIG. 2</figref>) and another paging device based on the PFSM (<b>300</b> in <figref idref="DRAWINGS">FIG. 3</figref>). For example, portions of the data or instructions of the application (<b>601</b> in <figref idref="DRAWINGS">FIG. 6</figref>) may reside in the main memory (<b>203</b> in <figref idref="DRAWINGS">FIG. 2</figref>), the HDD I/O device (e.g., <b>201</b> in <figref idref="DRAWINGS">FIG. 2</figref>), and/or the PFSM (<b>300</b> in <figref idref="DRAWINGS">FIG. 3</figref>). In such virtual memory environment, portions of the data or instructions of the application (<b>601</b> in <figref idref="DRAWINGS">FIG. 6</figref>) may be swapped in/out of the main memory (<b>203</b> in <figref idref="DRAWINGS">FIG. 2</figref>) based on paging operations described in reference to <figref idref="DRAWINGS">FIG. 7</figref> below.</p>
<p id="p-0081" num="0080">In the example of <figref idref="DRAWINGS">FIG. 7</figref>, instead of the block-table described above, the address mapping table (<b>345</b> in <figref idref="DRAWINGS">FIG. 3</figref>) of the PFSM (<b>300</b> in <figref idref="DRAWINGS">FIG. 3</figref>) is maintained as a VA table (virtual address table) consisting of mappings from virtual addresses in the CPU virtual memory address space of the processor node (<b>101</b> in <figref idref="DRAWINGS">FIG. 2</figref>) to flash page addresses and cached flash page addresses in the DRAM cache partition (<b>341</b> in <figref idref="DRAWINGS">FIG. 3</figref>).</p>
<p id="p-0082" num="0081">As shown in the example flowchart of <figref idref="DRAWINGS">FIG. 7</figref>, initially in Step <b>701</b>, first and second partitions of the virtual memory (i.e., the CPU virtual memory address space) is defined. For example, the operating system divides the virtual memory region into two subsets. One subset uses HDD as associated paging device and the other uses PFSM (<b>300</b> in <figref idref="DRAWINGS">FIG. 3</figref>) as associated paging device. The division of the virtual memory region may be illustrated in the following three examples.</p>
<p id="p-0083" num="0082">(1) The virtual memory partition is configured at system startup on a system basis. The partitioning in this case is static. The virtual memory region of every process (or thread) executing in the operating system environment is divided in fast VM (i.e., virtual memory based on PFSM (<b>300</b> in <figref idref="DRAWINGS">FIG. 3</figref>)) and slow VM (i.e., virtual memory based on HDD) in the same preset proportions.</p>
<p id="p-0084" num="0083">(2) The virtual memory partition is configured at the time of start of an application (e.g., Application (<b>601</b> in <figref idref="DRAWINGS">FIG. 6</figref>)) on a per-application basis. The fraction of fast VM to slow VM is specified at the time of process startup. This method provides more flexibility in a multi-programmed environment.</p>
<p id="p-0085" num="0084">(3) The virtual memory partition is configured based on an application controlled VM allocation. Application (e.g., Application (<b>601</b> in <figref idref="DRAWINGS">FIG. 6</figref>)) can specify fast or slow VM for every memory allocation request. This method provides the maximum flexibility for applications. An example mechanism may depend upon the operating system implementation and complexity constraints.</p>
<p id="p-0086" num="0085">In one or more of the three above example, the partition may be determined based on a memory access pattern of the application (e.g., Application (<b>601</b> in <figref idref="DRAWINGS">FIG. 6</figref>)) running under the operating system environment.</p>
<p id="p-0087" num="0086">Returning to the flowchart in <figref idref="DRAWINGS">FIG. 7</figref>, in Step <b>702</b>, the first and second partitions of the virtual memory are allocated to the fast VM paging device (e.g., based on PFSM (<b>300</b> in <figref idref="DRAWINGS">FIG. 3</figref>)) and the slow VM paging device (e.g., based on HDD). For example, the operating system maintains the following additional information in its data structures:</p>
<p id="p-0088" num="0087">(1) Virtual address space partition information:</p>
<p id="p-0089" num="0088">Depending on the partition scheme used (as discussed above), the operating system maintains the paging device ID (i.e., handle) for different address ranges. On a page fault the operating system refers to this data structure to determine the appropriate paging device. The type and form of the data structure to store the above information is implementation specific.</p>
<p id="p-0090" num="0089">(2) An operating system device driver is required to communicate to the PFSM (<b>300</b> in <figref idref="DRAWINGS">FIG. 3</figref>) as the fast VM paging device. The virtual memory subsystem (e.g., implemented using the virtual file system framework (<b>603</b> in <figref idref="DRAWINGS">FIG. 6</figref>) and the file system driver (<b>604</b> in <figref idref="DRAWINGS">FIG. 6</figref>)) uses this driver to transfer pages from the flash based paging device to main memory. For example, such operating system device driver may be a variation of the PFSM driver (<b>605</b> in <figref idref="DRAWINGS">FIG. 6</figref>) described in the example of <figref idref="DRAWINGS">FIG. 6</figref> above.</p>
<p id="p-0091" num="0090">Once the virtual memory partitions are defined and allocated to VM paging devices, the software application (e.g., Application (<b>601</b> in <figref idref="DRAWINGS">FIG. 6</figref>)) executes in the operating system environment similar to the example of <figref idref="DRAWINGS">FIG. 6</figref> above (Step <b>703</b>). From time to time a virtual memory page fault may be detected (Step <b>704</b>). Typically, the main memory (e.g., <b>203</b> in <figref idref="DRAWINGS">FIG. 2</figref>) may be accessed by the application (<b>601</b> in <figref idref="DRAWINGS">FIG. 6</figref>) and return to Step <b>703</b> when there is no page fault (Step <b>707</b>). However, when it is determined that a page fault occurred, a determination is made in Step <b>705</b> for the following two cases:</p>
<p id="p-0092" num="0091">Case1: The virtual memory page resides in disk storage (e.g., in the second partition), in which case the operating system handles this as a normal page fault using the slow VM paging device (Step <b>708</b>) and return to Step <b>703</b>.</p>
<p id="p-0093" num="0092">Case2: The virtual memory page resides in PFSM (<b>650</b> in <figref idref="DRAWINGS">FIG. 6</figref>), in which case the Step <b>706</b> is performed, before returning to Step <b>703</b>, by the following operations:</p>
<p id="p-0094" num="0093">(a) The operating system uses the paging device ID to route the request to the underlying device drivers (<b>604</b> and <b>605</b> in <figref idref="DRAWINGS">FIG. 6</figref>).</p>
<p id="p-0095" num="0094">(b) The PFSM driver (<b>605</b> in <figref idref="DRAWINGS">FIG. 6</figref>) issues a page read/write command as a set of store instructions to the DMA registers in the PFSM (<b>300</b> in <figref idref="DRAWINGS">FIG. 3</figref>).</p>
<p id="p-0096" num="0095">(c) Similar to the example of <figref idref="DRAWINGS">FIG. 6</figref>, the set of store instructions specify the operation type, physical main memory address and the page size. However, the physical flash page address specified in the store instructions of <figref idref="DRAWINGS">FIG. 6</figref> is now virtual address for the example of <figref idref="DRAWINGS">FIG. 7</figref>.</p>
<p id="p-0097" num="0096">(d) The PFSM (<b>300</b> in <figref idref="DRAWINGS">FIG. 3</figref>) refers to the VA table to translate the virtual address to the flash page address and the DRAM buffer address (i.e., cached flash page address in DRAM cache partition (<b>341</b> in <figref idref="DRAWINGS">FIG. 3</figref>)). All flash memory transfers are buffered in the DRAM buffer (i.e., the cache partition (<b>341</b> in <figref idref="DRAWINGS">FIG. 3</figref>)) in the PFSM device.</p>
<p id="p-0098" num="0097">(e) If the page exists in the DRAM cache partition (<b>341</b> in <figref idref="DRAWINGS">FIG. 3</figref>), the PFSM (<b>300</b> in <figref idref="DRAWINGS">FIG. 3</figref>) performs the read/write operation on the DRAM (<b>303</b> in <figref idref="DRAWINGS">FIG. 3</figref>). On a DRAM miss, a flash memory transfer is initiated.</p>
<p id="p-0099" num="0098">(c) The PFSM (<b>300</b> in <figref idref="DRAWINGS">FIG. 3</figref>) schedules DMAs to transfer the data to/from physical memory (main memory (<b>203</b> in <figref idref="DRAWINGS">FIG. 2</figref>)) using processor bus commands (<b>610</b> in <figref idref="DRAWINGS">FIG. 6</figref>).</p>
<p id="p-0100" num="0099">(d) On completing the operation PFSM (<b>300</b> in <figref idref="DRAWINGS">FIG. 3</figref>) interrupts the processor.</p>
<p id="p-0101" num="0100">The heterogeneous paging device configuration described above may be used as a part of a HPC cluster, for example used for scientific research and engineering applications such as computational fluid dynamics and the building and testing of virtual prototypes as well as business applications such as data warehousing, line-of-business (LOB) applications, and transaction processing.</p>
<p id="p-0102" num="0101">Embodiments of the invention may be implemented on virtually any type of computer regardless of the platform being used. For example, as shown in <figref idref="DRAWINGS">FIG. 8</figref>, a computer system (<b>800</b>) includes one or more processor(s) (<b>802</b>), associated memory (<b>804</b>) (e.g., random document access memory (RAM), cache memory, flash memory, etc.), a storage device (<b>806</b>) (e.g., a hard disk, an optical drive such as a compact disk drive or digital video disk (DVD) drive, a flash memory stick, etc.), and numerous other elements and functionalities typical of today's computers (not shown). The computer (<b>800</b>) may also include input means, such as a keyboard (<b>808</b>), a mouse (<b>810</b>), or a microphone (not shown). Further, the computer (<b>800</b>) may include output means, such as a monitor (<b>812</b>) (e.g., a liquid crystal display (LCD), a plasma display, or cathode ray tube (CRT) monitor). The computer system (<b>800</b>) may be connected to a network (not shown) (e.g., a local area network (LAN), a wide area network (WAN) such as the Internet, or any other similar type of network) with wired and/or wireless segments via a network interface connection (not shown). Those skilled in the art will appreciate that many different types of computer systems exist, and the aforementioned input and output means may take other forms. Generally speaking, the computer system (<b>800</b>) includes at least the minimal processing, input, and/or output means necessary to practice embodiments of the invention.</p>
<p id="p-0103" num="0102">Further, those skilled in the art will appreciate that one or more elements of the aforementioned computer system (<b>800</b>) may be located at a remote location and connected to the other elements over a network. Further, embodiments of the invention may be implemented on a distributed system having a plurality of nodes, where each portion of the invention (e.g., various modules of <figref idref="DRAWINGS">FIG. 1</figref>) may be located on a different node within the distributed system. In one embodiments of the invention, the node corresponds to a computer system. Alternatively, the node may correspond to a processor with associated physical memory. The node may alternatively correspond to a processor with shared memory and/or resources. Further, software instructions for performing embodiments of the invention may be stored on a computer readable medium such as a compact disc (CD), a diskette, a tape, a file, or any other computer readable storage device.</p>
<p id="p-0104" num="0103">One or more embodiments of the invention exhibit one or more of the following advantages. By accessing flash based solid state disk drive directly via the processor bus, the I/O bus and associated interface overheads are eliminated to improve (e.g., by an order of magnitude) bandwidth and latency over the scheme of using the I/O bus to transfer data. For example, the PFSM achieves higher performance as a paging device in virtual memory systems because the address translation overhead for each paging command is reduced compared to IO-bus-connected solid state disk drive. Accordingly, applications with frequent paging from very large datasets can now be executed with orders of magnitude improvement in performance. At the same time, the memory performance of workloads that do not require the fast paging is not adversely affected. Said in other words, embodiments of the invention provides a heterogeneous paging configuration that is advantageous over schemes using only one type of paging device because of the ability to selectively direct pages to flash based paging device or HDD based paging device based on software instruction access pattern of the application. Furthermore, because the PFSM interfaces directly with the processor bus, partial updates can be performed to enable higher degree of random access at increased efficiency and lower latency. In particular, the aforementioned advantage is achieved without requiring any modifications to the processor or the memory controller of existing processor nodes in the system.</p>
<p id="p-0105" num="0104">While the invention has been described with respect to a limited number of embodiments, those skilled in the art, having benefit of this disclosure, will appreciate that other embodiments can be devised which do not depart from the scope of the invention as disclosed herein. Accordingly, the scope of the invention should be limited only by the attached claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method for accessing a virtual memory of a processor using a processor-bus-connected flash storage module (PFSM) as a first paging device and a hard disk drive (HDD) as a second paging device, the PFSM operatively coupled to the processor via a processor bus, the PFSM comprising a flash memory and a virtual address mapping table, the method comprising:
<claim-text>allocating a first address partition and a second address partition of the virtual memory for a software application of the processor to the first paging device and the second paging device, respectively;</claim-text>
<claim-text>identifying a virtual memory page in the first paging device responsive to a page fault of the virtual memory triggered by the software application;</claim-text>
<claim-text>sending a page access request to the PFSM for accessing the virtual memory page responsive to the page fault, wherein the page access request is sent via the processor bus and comprises a virtual address of the virtual memory page; and</claim-text>
<claim-text>receiving the virtual memory page from the PFSM based on a command of the processor bus issued by the PFSM in conjunction with performing a flash memory access in the flash memory using a flash page address, wherein the virtual address mapping table translates the virtual address of the virtual memory page to the flash page address in the flash memory.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein defining the first address partition and the second address partition of the virtual memory is based on a memory access pattern of the software application.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>defining the first address partition and the second address partition of the virtual memory when an operating system of the processor is starting up.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>defining the first address partition and the second address partition of the virtual memory when the software application of the processor is starting up.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>defining the first address partition and the second address partition of the virtual memory when the software application of the processor requests memory from an operating system of the processor.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the PFSM further comprises a buffer memory and a processor accessible command register mapped into an address space of the processor, the method further comprising:
<claim-text>storing, by the PFSM, one or more parameters of the page access request in the buffer memory as a context of the page access request, wherein the one or more parameters are received by the PFSM using the processor accessible command register; and</claim-text>
<claim-text>scheduling, by the PFSM, a direct memory access (DMA) operation responsive to receiving a trigger parameter of the one or more parameters,</claim-text>
<claim-text>wherein the DMA operation comprises performing the flash memory access and the command of the processor bus.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising:
<claim-text>generating, by the PSFM, an interrupt to the processor when the DMA operation is completed.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the flash memory and a cache partition of the buffer memory forms a memory hierarchy of the PFSM, the method further comprising:
<claim-text>performing a cache operation by the PFSM according to a cache policy of the memory hierarchy responsive to receiving the trigger parameter, wherein the flash memory access is scheduled as a result of the cache operation.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising:
<claim-text>identifying a cached flash page address in the cache partition for the virtual address based on the virtual address mapping table,</claim-text>
<claim-text>wherein the cache operation is based on the cached flash page address in the cache partition. </claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
