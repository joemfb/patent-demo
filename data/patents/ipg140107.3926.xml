<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08624994-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08624994</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13362978</doc-number>
<date>20120131</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>JP</country>
<doc-number>2008-051121</doc-number>
<date>20080229</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>140</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>5</main-group>
<subgroup>228</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>3482221</main-classification>
<further-classification>348335</further-classification>
<further-classification>348340</further-classification>
<further-classification>348345</further-classification>
<further-classification>348348</further-classification>
</classification-national>
<invention-title id="d2e71">Information processing apparatus, eye open/closed degree determination method, computer-readable storage medium, and image sensing apparatus</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5878156</doc-number>
<kind>A</kind>
<name>Okumura</name>
<date>19990300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>7039233</doc-number>
<kind>B2</kind>
<name>Mori et al.</name>
<date>20060500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>7054850</doc-number>
<kind>B2</kind>
<name>Matsugu</name>
<date>20060500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>7274819</doc-number>
<kind>B2</kind>
<name>Matsugu</name>
<date>20070900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>2006/0074653</doc-number>
<kind>A1</kind>
<name>Mitari et al.</name>
<date>20060400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2006/0115157</doc-number>
<kind>A1</kind>
<name>Mori et al.</name>
<date>20060600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382190</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2006/0204053</doc-number>
<kind>A1</kind>
<name>Mori et al.</name>
<date>20060900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2006/0228005</doc-number>
<kind>A1</kind>
<name>Matsugu et al.</name>
<date>20061000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2007/0201724</doc-number>
<kind>A1</kind>
<name>Steinberg et al.</name>
<date>20070800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2007/0242856</doc-number>
<kind>A1</kind>
<name>Suzuki et al.</name>
<date>20071000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2008/0219516</doc-number>
<kind>A1</kind>
<name>Suzuki et al.</name>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2009/0089235</doc-number>
<kind>A1</kind>
<name>Torii et al.</name>
<date>20090400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>DE</country>
<doc-number>19614975</doc-number>
<kind>A1</kind>
<date>19970100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>JP</country>
<doc-number>9-044685</doc-number>
<kind>A</kind>
<date>19970200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>JP</country>
<doc-number>11-242744</doc-number>
<kind>A</kind>
<date>19990900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>JP</country>
<doc-number>2004-192551</doc-number>
<kind>A</kind>
<date>20040700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>JP</country>
<doc-number>2007-241937</doc-number>
<kind>A</kind>
<date>20070900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>WO</country>
<doc-number>2006/092022</doc-number>
<kind>A1</kind>
<date>20060900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00019">
<othercit>U.S. Appl. No. 12/392,917, filed Feb. 25, 2009, Applicants: Kaneda, et al.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00020">
<othercit>U.S. Appl. No. 12/485,020, filed Jun. 15, 2009, Applicants: Sato, et al.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00021">
<othercit>European Search Report dated Oct. 4, 2013 in corresponding European Patent Application No. 09153599.7.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>7</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>3482221</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348335</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348340</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348345</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348348</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348349</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348356</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382117</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382118</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382291</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382104</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>340575</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>340576</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3405731</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>22</number-of-drawing-sheets>
<number-of-figures>22</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>12392917</doc-number>
<date>20090225</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>8130281</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13362978</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120148159</doc-number>
<kind>A1</kind>
<date>20120614</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Kaneda</last-name>
<first-name>Yuji</first-name>
<address>
<city>Kawasaki</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Matsugu</last-name>
<first-name>Masakazu</first-name>
<address>
<city>Yokohama</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Kaneda</last-name>
<first-name>Yuji</first-name>
<address>
<city>Kawasaki</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Matsugu</last-name>
<first-name>Masakazu</first-name>
<address>
<city>Yokohama</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Fitzpatrick, Cella, Harper &#x26; Scinto</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Canon Kabushiki Kaisha</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Aggarwal</last-name>
<first-name>Yogesh</first-name>
<department>2661</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">An information processing apparatus inputs an image, detects the face of a person from the input image, and calculates a feature amount associated with the open/closed state of eyes of the detected face. In addition, the information processing apparatus calculates, as a feature-change amount, the difference between the calculated feature amount and a predetermined feature amount, and calculates the eye open/closed degree of eyes of the detected face on the basis of the feature amount and the feature-change amount.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="136.23mm" wi="230.29mm" file="US08624994-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="236.64mm" wi="151.47mm" orientation="landscape" file="US08624994-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="214.88mm" wi="171.37mm" file="US08624994-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="160.53mm" wi="164.25mm" file="US08624994-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="236.39mm" wi="139.36mm" file="US08624994-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="230.12mm" wi="127.93mm" orientation="landscape" file="US08624994-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="99.40mm" wi="94.32mm" file="US08624994-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="96.60mm" wi="77.13mm" file="US08624994-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="109.47mm" wi="131.15mm" file="US08624994-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="98.04mm" wi="102.02mm" file="US08624994-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="213.78mm" wi="171.87mm" file="US08624994-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="130.89mm" wi="166.12mm" file="US08624994-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="215.39mm" wi="167.47mm" file="US08624994-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="110.32mm" wi="155.96mm" file="US08624994-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="210.57mm" wi="191.26mm" file="US08624994-20140107-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="207.94mm" wi="176.36mm" file="US08624994-20140107-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00016" num="00016">
<img id="EMI-D00016" he="219.71mm" wi="155.96mm" file="US08624994-20140107-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00017" num="00017">
<img id="EMI-D00017" he="178.05mm" wi="115.99mm" file="US08624994-20140107-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00018" num="00018">
<img id="EMI-D00018" he="131.40mm" wi="166.96mm" file="US08624994-20140107-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00019" num="00019">
<img id="EMI-D00019" he="215.22mm" wi="169.42mm" file="US08624994-20140107-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00020" num="00020">
<img id="EMI-D00020" he="214.46mm" wi="175.43mm" file="US08624994-20140107-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00021" num="00021">
<img id="EMI-D00021" he="156.55mm" wi="169.33mm" file="US08624994-20140107-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00022" num="00022">
<img id="EMI-D00022" he="208.62mm" wi="175.68mm" file="US08624994-20140107-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<p id="p-0002" num="0001">This application is a continuation of U.S. application Ser. No. 12/392,917, filed Feb. 25, 2009 (allowed), the contents of which are incorporated by reference herein.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">1. Field of the Invention</p>
<p id="p-0004" num="0003">The present invention relates to an information processing apparatus, eye open/closed degree determination method, computer-readable storage medium, and image sensing apparatus.</p>
<p id="p-0005" num="0004">2. Description of the Related Art</p>
<p id="p-0006" num="0005">A technique of detecting a face from an image including still and moving images has conventionally been known (Mitarai, Y., Mori, K., Matsugu, M. &#x201c;Robust Face Detection System Based on Convolutional Neural Networks Using Selective Activation of Modules&#x201d;, FIT (Forum of Information Technology), L1-013, 2003). Further, techniques have been proposed for determining the open/closed degree of eyes in a detected face.</p>
<p id="p-0007" num="0006">In this regard, Japanese Patent Laid-Open No. 09-044685 proposes a technique of comparing the distance between the eyebrow and the pupil or the area ratio of the eyebrow and pupil with a predetermined reference, and calculating the change amount of the distance, thereby determining the eye open/closed degree.</p>
<p id="p-0008" num="0007">Japanese Patent Laid-Open No. 11-242744 proposes a technique of grouping eye regions into a white region and black (pupil) region, and when the black region changes, determining that the object blinks.</p>
<p id="p-0009" num="0008">Japanese Patent Laid-Open No. 2004-192551 proposes a technique of preparing a plurality of processes to determine the eye open/closed degree in order to distinguish an eye state immediately before closing eyes from an eye state upon a change of the expression such as a smile, and determining the eye open/closed degree on the basis of a result using the ANDs of respective determination results.</p>
<p id="p-0010" num="0009">There is an individual difference in eye size (e.g., the distance between upper and lower eyelids or the appearance of the pupil) in an expressionless state while the eyes are fully open. For example, the pupil of person A shown in <figref idref="DRAWINGS">FIG. 19</figref> is completely seen in an expressionless state (state <b>1</b>) while the eyes are fully opened. In contrast, the pupil of person B shown in <figref idref="DRAWINGS">FIG. 19</figref> is partially hidden in an expressionless state (state <b>4</b>) while the eyes are fully opened.</p>
<p id="p-0011" num="0010">In this manner, there is an individual difference in eye size in an expressionless state. If the eye open/closed degree is determined from only one image, state <b>2</b> of person A shown in <figref idref="DRAWINGS">FIG. 19</figref> and state <b>4</b> of person B shown in <figref idref="DRAWINGS">FIG. 19</figref> are determined to have the same eye open/closed degree. For example, when state <b>2</b> of person A shown in <figref idref="DRAWINGS">FIG. 19</figref> is determined as a shooting failure upon shooting by an image sensing apparatus, state <b>4</b> of person B shown in <figref idref="DRAWINGS">FIG. 19</figref> is also determined as a shooting failure.</p>
<p id="p-0012" num="0011">When determining the eye open/closed degree using the amount of change between a plurality of images, the amount of change from state <b>1</b> to state <b>2</b> of person A shown in <figref idref="DRAWINGS">FIG. 19</figref> equals that of change from state <b>4</b> to state <b>5</b> of person B shown in <figref idref="DRAWINGS">FIG. 19</figref>. It is determined that state <b>2</b> of person A and state <b>5</b> of person B are the same. For example, when state <b>2</b> of person A shown in <figref idref="DRAWINGS">FIG. 19</figref> is determined as a shooting success upon shooting by an image sensing apparatus, state <b>5</b> of person B shown in <figref idref="DRAWINGS">FIG. 19</figref> is also determined as a shooting success.</p>
<p id="p-0013" num="0012">Determination of whether an image shot by an image sensing apparatus is a failure or success changes depending on the user of the image sensing apparatus. It is necessary to detect the eye open/closed degree at high precision regardless of individual differences in the eye size. For example, the above-mentioned technique disclosed in Japanese Patent Laid-Open No. 2004-192551 determines the eye open/closed degree by using a combination of eye open/closed degree determination results. However, each eye open/closed degree determination result is a binary output representing that the eyes are open or closed. For this reason, even this technique cannot detect the eye open/closed degree at high precision regardless of individual differences in eye size.</p>
<p id="p-0014" num="0013">A case where the eye open/closed degree is determined by a method using one image, and a case where it is determined by a method using the amount of change between a plurality of images will be examined. A case where the eye open/closed degree is determined when it changes from state <b>1</b> to state <b>2</b> shown in <figref idref="DRAWINGS">FIG. 20</figref>, and a case where the eye open/closed degree is determined when it changes from state <b>4</b> to state <b>5</b> shown in <figref idref="DRAWINGS">FIG. 20</figref> will be exemplified.</p>
<p id="p-0015" num="0014">When the eyes change from state <b>1</b> (black eye area <b>10</b>) to state <b>2</b> (black eye area <b>5</b>) shown in <figref idref="DRAWINGS">FIG. 20</figref>, the change amount of the black eye area is <b>5</b>, and the absolute amount of the black eye area in state <b>2</b> is <b>5</b>. When an absolute amount threshold for determining that the eyes are closed is set to 6 (it is determined that the eyes are closed when the absolute amount is smaller than 6), and a change amount threshold for determining that the eyes are closed is set to 4 (it is determined that the eyes are closed when the change amount is larger than 4), both determination results based on the absolute amount and change amount represent that the eyes are closed. When the eyes change from state <b>4</b> (black eye area <b>5</b>) to state <b>5</b> (black eye area <b>0</b>) shown in <figref idref="DRAWINGS">FIG. 20</figref> and the same determinations as those described above are made, the same results are obtained, and both determination results based on the absolute amount and change amount represent that the eyes are closed. In this manner, according to the conventionally proposed eye open/closed degree determination method, state <b>2</b> and state <b>5</b> shown in <figref idref="DRAWINGS">FIG. 20</figref> are determined to have the same eye open/closed degree.</p>
<heading id="h-0002" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0016" num="0015">The present invention enables to provide an information processing apparatus, eye open/closed degree determination method, computer-readable storage medium, and image sensing apparatus capable of detecting the eye open/closed degree at high precision regardless of individual differences in eye size.</p>
<p id="p-0017" num="0016">According to a first aspect of the present invention there is provided an information processing apparatus comprising: an input unit configured to input an image; a face detection unit configured to detect a face of a person from the image input from the input unit; a first calculation unit configured to calculate a feature amount associated with an eye open/closed state from the face detected by the face detection unit; a second calculation unit configured to calculate, as a feature-change amount, a difference between the feature amount calculated by the first calculation unit and a feature amount calculated from a face detected from an image input before a predetermined time; and a third calculation unit configured to calculate an eye open/closed degree of eyes of the face detected by the face detection unit on the basis of the feature amount and the feature-change amount.</p>
<p id="p-0018" num="0017">According to a second aspect of the present invention there is provided an eye open/closed degree determination method comprising: inputting an image;</p>
<p id="p-0019" num="0018">detecting a face of a person from the image input in the inputting the image; calculating a feature amount associated with an eye open/closed state from the face detected in the detecting the face; calculating, as a feature-change amount, a difference between the feature amount calculated in the calculating the feature amount and a feature amount calculated from a face detected from an image input before a predetermined time; and calculating an eye opening/closing degree of eyes of the face detected in the detecting the face on the basis of the feature amount and the feature-change amount.</p>
<p id="p-0020" num="0019">According to a third aspect of the present invention there is provided a computer-readable storage medium storing a computer program, the program causing a computer to function as an input unit configured to input an image, a face detection unit configured to detect a face of a person from the image input from the input unit, a first calculation unit configured to calculate a feature amount associated with an eye open/closed state from the face detected by the face detection unit, a second calculation unit configured to calculate, as a feature-change amount, a difference between the feature amount calculated by the first calculation unit and a feature amount calculated from a face detected from an image input before a predetermined time, and a third calculation unit configured to calculate an eye open/closed degree of eyes of the face detected by the face detection unit on the basis of the feature amount and the feature-change amount.</p>
<p id="p-0021" num="0020">According to a fourth aspect of the present invention there is provided an image sensing apparatus comprising: an input unit configured to input an image; a face detection unit configured to detect a face of a person from the image input from the input unit; a first calculation unit configured to calculate a feature amount associated with an eye open/closed state from the face detected by the face detection unit; a second calculation unit configured to calculate, as a feature-change amount, a difference between the feature amount calculated by the first calculation unit and a feature amount calculated from a face detected from an image input before a predetermined time; a third calculation unit configured to calculate an eye open/closed degree of eyes of the face detected by the face detection unit on the basis of the feature amount and the feature-change amount; a determination unit configured to perform eye open/closed determination by performing threshold processing for the eye open/closed degree calculated by the third calculation unit; and an image sensing unit configured to shoot on the basis of a result of eye open/closed degree determination by the determination unit.</p>
<p id="p-0022" num="0021">Further features of the present invention will be apparent from the following description of exemplary embodiments with reference to the attached drawings.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram showing an example of the arrangement of an image sensing apparatus <b>100</b> according to an embodiment of the present invention;</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIGS. 2A and 2B</figref> are a flowchart showing an example of a processing sequence in the image sensing apparatus <b>100</b> shown in <figref idref="DRAWINGS">FIG. 1</figref>;</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 3</figref> is a flowchart showing an example of the sequence of eye open/closed degree determination in step S<b>113</b> shown in <figref idref="DRAWINGS">FIG. 2B</figref>;</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 4</figref> is a view showing an example of an outline of image normalization;</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 5</figref> is a view showing an example of an outline of processing when setting a region to undergo detection of a pupil feature;</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 6</figref> is a view showing an example of a pupil feature detection result;</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 7</figref> is a graph showing an example of the distribution of eye open/closed degree values calculated by a predetermined function func<sub>1</sub>;</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 8</figref> is a graph showing an example of the distribution of eye open/closed degree values calculated by a predetermined function func<sub>2</sub>;</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 9</figref> is a first view for explaining an example of the effects of the first embodiment;</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 10</figref> is a second view for explaining an example of the effects of the first embodiment;</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 11</figref> is a flowchart showing an example of the sequence of eye open/closed degree determination according to the second embodiment;</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 12</figref> is a graph showing an example of a change of the pupil-feature amount within a predetermined time;</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 13</figref> is a first view for explaining an example of the effects of the second embodiment;</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIGS. 14A and 14B</figref> are a flowchart showing an example of a processing sequence in an image sensing apparatus <b>100</b> according to the third embodiment;</p>
<p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. 15</figref> is a flowchart showing an example of the sequence of eye open/closed degree determination in step S<b>416</b> shown in <figref idref="DRAWINGS">FIG. 14B</figref>;</p>
<p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. 16</figref> is a flowchart showing an example of the sequence of weight determination in step S<b>502</b> shown in <figref idref="DRAWINGS">FIG. 15</figref>;</p>
<p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. 17</figref> is a first view for explaining an example of the effects of the third embodiment;</p>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. 18</figref> is a second view for explaining an example of the effects of the third embodiment;</p>
<p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. 19</figref> is a first view for explaining a conventional problem; and</p>
<p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. 20</figref> is a second view for explaining the conventional problem.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DESCRIPTION OF THE EMBODIMENTS</heading>
<p id="p-0043" num="0042">Preferred embodiments of the present invention will now be described in detail with reference to the drawings. It should be noted that the relative arrangement of the components, the numerical expressions and numerical values set forth in these embodiments do not limit the scope of the present invention unless it is specifically stated otherwise.</p>
<heading id="h-0005" level="1">First Embodiment</heading>
<p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram showing an example of the arrangement of an image sensing apparatus <b>100</b> according to an embodiment of the present invention. The first embodiment will exemplify a case where the image sensing apparatus <b>100</b> is an electronic still camera.</p>
<p id="p-0045" num="0044">The image sensing apparatus <b>100</b> includes an image sensing lens unit <b>101</b>, and a light amount adjustment device <b>102</b> having a stop and shutter. An image sensor <b>103</b> is, for example, a CCD (Charge-Coupled Device) or CMOS (Complementary Metal Oxide Semiconductor) sensor for converting a light beam of an object image having passed through the image sensing lens unit into an electrical signal. An analog signal processing circuit <b>104</b> performs clamp, gain, and the like for an analog signal output from the image sensor <b>103</b>. An A/D converter <b>105</b> converts an output from the analog signal processing circuit <b>104</b> into a digital signal.</p>
<p id="p-0046" num="0045">A digital signal processing circuit <b>107</b> performs predetermined pixel interpolation, color conversion, and the like for data from the A/D converter <b>105</b> or data from a memory control circuit <b>106</b>. The digital signal processing circuit <b>107</b> also performs predetermined arithmetic using sensed image data, and TTL AWB (Auto White Balance) processing based on the obtained arithmetic result. Further, the digital signal processing circuit <b>107</b> executes specific object face detection and eye open/closed degree determination for the sensed image data. The specific object face detection and eye open/closed degree determination use data stored in a memory (a memory <b>108</b> to be described later).</p>
<p id="p-0047" num="0046">A system control circuit <b>112</b> performs TTL (Through The Lens) AF (Auto Focus), AE (Auto Exposure), and EF (pre-Electronic Flash) on the basis of the above-mentioned arithmetic results to control an exposure control circuit <b>113</b> and focus control circuit <b>114</b>.</p>
<p id="p-0048" num="0047">The memory control circuit <b>106</b> controls the analog signal processing circuit <b>104</b>, the A/D converter <b>105</b>, the digital signal processing circuit <b>107</b>, the memory <b>108</b>, and a digital/analog (to be referred to as D/A hereinafter) converter <b>109</b>. Data A/D-converted by the A/D converter <b>105</b> is written in the memory <b>108</b> via the digital signal processing circuit <b>107</b> and memory control circuit <b>106</b>. Data A/D-converted by the A/D converter <b>105</b> is sometimes written in the memory <b>108</b> directly via the memory control circuit <b>106</b>.</p>
<p id="p-0049" num="0048">The memory <b>108</b> stores, for example, data to be displayed on a display apparatus <b>110</b>. Data stored in the memory <b>108</b> is output to the display apparatus <b>110</b> such as a TFT or LCD via the D/A converter <b>109</b>, and displayed on it. The memory <b>108</b> stores images including sensed still images and moving images. The memory <b>108</b> has a capacity enough to store a predetermined number of still images and a moving image of a predetermined time. Even in continuous shooting or panoramic shooting of successively shooting a plurality of still images, many images can be written in the memory <b>108</b> at high speed. The memory <b>108</b> is also available as the work area of the system control circuit <b>112</b>. Sensed still images and moving images may also be written in a storage medium such as a CD-ROM, Floppy Disk&#xae;, hard disk, magnetic tape, magneto-optical disk, or nonvolatile memory card by using an interface (I/F) <b>111</b>.</p>
<p id="p-0050" num="0049">The display apparatus <b>110</b> can sequentially display sensed image data, and in this case, functions as an electronic viewfinder. The display apparatus <b>110</b> can arbitrarily turn on/off the display in accordance with an instruction from the system control circuit <b>112</b>. When the display is turned off, power consumption of the image sensing apparatus <b>100</b> can be reduced more greatly than when the display is ON. The display apparatus <b>110</b> displays an operating state, message, and the like using a text, image, or the like in accordance with execution of a program in the system control circuit <b>112</b>.</p>
<p id="p-0051" num="0050">The I/F <b>111</b> interfaces the image sensing apparatus <b>100</b> with a storage medium such as a memory card or hard disk. By using the I/F <b>111</b>, the image sensing apparatus <b>100</b> can exchange image data and management information accessory to the image data with another computer or a peripheral device such as a printer. When the I/F <b>111</b> complies with the standard of a PCMCIA card, CF (Compact Flash&#xae;) card, or the like, the I/F <b>111</b> functions as a communication interface upon connecting various communication cards. These communication cards include a LAN card, modem card, USB card, IEEE1394 card, P1284 card, SCSI card, and PHS communication card.</p>
<p id="p-0052" num="0051">The system control circuit <b>112</b> controls the overall operation of the image sensing apparatus <b>100</b>. The memory in the system control circuit <b>112</b> stores constants, variables, programs, and the like for operating the system control circuit <b>112</b>, or recognizing the face of a specific object and the eye open/closed degree. The Constants, variables, programs, and the like stored in the internal memory of the system control circuit <b>112</b> can be changed using a CD-ROM, Floppy Disk&#xae;, hard disk, magnetic tape, magneto-optical disk, nonvolatile memory card, or the like.</p>
<p id="p-0053" num="0052">The exposure control circuit <b>113</b> controls the stop and shutter of the light amount adjustment device <b>102</b>. The focus control circuit <b>114</b> controls focusing and zooming of the image sensing lens unit <b>101</b>. The exposure control circuit <b>113</b> and focus control circuit <b>114</b> are controlled by TTL. The system control circuit <b>112</b> controls the exposure control circuit <b>113</b> and focus control circuit <b>114</b> on the basis of the result of executing calculation for sensed image data by the digital signal processing circuit <b>107</b>.</p>
<p id="p-0054" num="0053">An example of a processing sequence in the image sensing apparatus <b>100</b> shown in <figref idref="DRAWINGS">FIG. 1</figref> will be explained with reference to <figref idref="DRAWINGS">FIGS. 2A and 2B</figref>. A program for executing this processing is stored in, for example, the internal memory of the system control circuit <b>112</b>, and executed under the control of the system control circuit <b>112</b>.</p>
<p id="p-0055" num="0054">This processing starts upon power-on or the like. When the processing starts, the system control circuit <b>112</b> initializes various flags, control variables, and the like in the internal memory (step S<b>100</b>). Then, the system control circuit <b>112</b> detects the mode setting state of the image sensing apparatus <b>100</b>. An example of the mode is an eye closing prevention shooting mode.</p>
<p id="p-0056" num="0055">If the system control circuit <b>112</b> detects a mode other than a shooting mode (another mode in step S<b>101</b>), the image sensing apparatus <b>100</b> executes processing corresponding to the selected mode (step S<b>102</b>), and then the process returns to step S<b>101</b>. If the system control circuit <b>112</b> detects a shooting mode such as the eye closing prevention shooting mode (shooting mode in step S<b>101</b>), it determines whether an error has occurred in the remaining battery level or operating state. If an error has occurred (NO in step S<b>103</b>), the system control circuit <b>112</b> issues a predetermined warning by an image or sound using the display apparatus <b>110</b> (step S<b>104</b>), and then the process returns to step S<b>101</b>. If no error has occurred (YES in step S<b>103</b>), the system control circuit <b>112</b> determines whether the storage medium operating state is improper for the operation of the image sensing apparatus <b>100</b>, particularly an image data recording/playback operation to the storage medium.</p>
<p id="p-0057" num="0056">If the storage medium operating state is improper (NO in step S<b>105</b>), the system control circuit <b>112</b> issues a predetermined warning by an image or sound using the display apparatus <b>110</b> similarly to the above-mentioned warning (step S<b>104</b>), and then the process returns to step S<b>101</b>. If the storage medium is proper (YES in step S<b>105</b>), the system control circuit <b>112</b> displays a user interface (to be referred to as a UI hereinafter) representing various setting states of the image sensing apparatus <b>100</b> by an image or sound using the display apparatus <b>110</b> (step S<b>106</b>). When the image display of the display apparatus <b>110</b> is ON, the UI representing various setting states of the image sensing apparatus <b>100</b> may also be displayed by an image or sound using the display apparatus <b>110</b>. In this fashion, the user makes various settings.</p>
<p id="p-0058" num="0057">The system control circuit <b>112</b> sets ON the image display of the display apparatus <b>110</b> (step S<b>107</b>). The system control circuit <b>112</b> sets a through display state to sequentially display sensed image data (step S<b>108</b>). In the through display state, the display apparatus <b>110</b> sequentially displays data written in the memory <b>108</b>, implementing an electronic viewfinder function.</p>
<p id="p-0059" num="0058">After that, the image sensing apparatus <b>100</b> determines whether the user such as a photographer has pressed the shutter switch. If the user has not pressed the shutter switch (NO in step S<b>109</b>), the process returns to step S<b>101</b>. If the user has pressed the shutter switch (YES in step S<b>109</b>), the system control circuit <b>112</b> executes face detection (step S<b>110</b>). In the face detection, image data input from the image sensor <b>103</b> is compressed to low resolution, and the face of a person is detected from the compressed image data. More specifically, image data is compressed to a low resolution of, for example, 640&#xd7;480 pixels by executing thinning or the like, and then a face is detected. A known example of the face detection method is convolutional neural network (CNN) of hierarchically detecting high-order features such as an eye and mouth from low-order features such as an edge, and finally detecting the barycenter of the face (Mitarai, Y., Mori, K., Matsugu, M. &#x201c;Robust Face Detection System Based on Convolutional Neural Networks Using Selective Activation of Modules&#x201d;, FIT (Forum of Information Technology), L1-013, 2003). By using the CNN, the barycenter of an eye or mouth can be obtained.</p>
<p id="p-0060" num="0059">After detecting the face of a person in this way, the system control circuit <b>112</b> executes predetermined AE/AF control for the detected person's face (step S<b>111</b>), and through-displays a sensed image (step S<b>112</b>). At this time, the system control circuit <b>112</b> determines the eye open/closed degree of each person by using the eye/face position detected in step S<b>110</b> (step S<b>113</b>). Details of the eye open/closed degree determination will be explained later.</p>
<p id="p-0061" num="0060">The system control circuit <b>112</b> determines, based on the result of the eye open/closed degree determination, whether to shoot by the image sensing apparatus <b>100</b>. More specifically, if the system control circuit <b>112</b> determines that the eyes are closed; it does not cause the image sensing apparatus <b>100</b> to shoot (NO in step S<b>114</b>), and determines whether to forcibly terminate shooting. If the system control circuit <b>112</b> determines to forcibly terminate shooting (YES in step S<b>119</b>), the process ends. If the system control circuit <b>112</b> determines not to forcibly terminate shooting (NO in step S<b>119</b>), it performs face detection again (step S<b>118</b>), and the process returns to step S<b>112</b>.</p>
<p id="p-0062" num="0061">If the system control circuit <b>112</b> determines that the eyes are open, it causes the image sensing apparatus <b>100</b> to shoot (step S<b>115</b> after YES in step S<b>114</b>). After shooting, the system control circuit <b>112</b> displays a quick review of the sensed image (step S<b>116</b>), encodes the sensed high-resolution image, and records the encoded image on a storage medium such as a flash memory (step S<b>117</b>). That is, a low-resolution image compressed by thinning or the like is used for face detection, whereas a high-resolution image is used for recording.</p>
<p id="p-0063" num="0062">Details of the eye open/closed degree determination in step S<b>113</b> shown in <figref idref="DRAWINGS">FIG. 2B</figref> will be explained with reference to <figref idref="DRAWINGS">FIG. 3</figref>.</p>
<p id="p-0064" num="0063">When this processing starts, the system control circuit <b>112</b> executes image normalization using an eye or face position detected in step S<b>110</b> (step S<b>200</b>). In the normalization, the orientation of each face in an input image <b>300</b> compressed to low resolution is aligned to be erect, as shown in <figref idref="DRAWINGS">FIG. 4</figref>. Then, affine transformation and extraction are done to change the width between two eyes to a predetermined number of pixels (e.g., 50 pixels) and change the width and height of an extracted image to a predetermined number of pixels (e.g., 120&#xd7;120 pixels). In an example shown in <figref idref="DRAWINGS">FIG. 4</figref>, face images <b>301</b> and <b>302</b> are obtained as a result of normalizing the input image <b>300</b>.</p>
<p id="p-0065" num="0064">Thereafter, the system control circuit <b>112</b> sets an eye region as shown in <figref idref="DRAWINGS">FIG. 5</figref> in the normalized image (step S<b>201</b>). In an example shown in <figref idref="DRAWINGS">FIG. 5</figref>, the coordinate point of the upper left corner of an eye region <b>303</b> is set to (1,10), and that of its lower right corner is set to (120,60).</p>
<p id="p-0066" num="0065">After setting the eye region, the system control circuit <b>112</b> performs image correction (e.g., luminance correction) for the eye region (step S<b>202</b>). In the luminance correction, the luminance histogram of the eye region is created and expanded to change the luminance value of each pixel.</p>
<p id="p-0067" num="0066">After the luminance correction, the system control circuit <b>112</b> detects a pupil feature from the eye region of the luminance-corrected image (step S<b>203</b>). The pupil feature is detected using a pupil detection CNN which has learned in advance to detect the pupil, similar to face detection. The pupil detection CNN is given a pupil region as a right answer and a region other than the pupil region as a wrong answer, and learns to output a large value from each neuron of the CNN in only a region where the pupil exists. If the image having undergone luminance correction in step S<b>202</b> is input to the pupil detection CNN, for example, neuron output values as shown in FIG. <b>6</b> are obtained.</p>
<p id="p-0068" num="0067">Then, the system control circuit <b>112</b> calculates a pupil-feature amount using the pupil-feature detection amount (step S<b>204</b>). The pupil-feature amount in the first embodiment is represented by an output value counter(t) which becomes equal to or larger than a predetermined threshold I<sub>th </sub>at time t[s]:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>if <i>I</i>(<i>x,y</i>)&#x2267;<i>I</i><sub>th</sub><img id="CUSTOM-CHARACTER-00001" he="2.79mm" wi="3.13mm" file="US08624994-20140107-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/>Counter(<i>t</i>)+=1&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
Instead of the pupil-feature amount, the distance between edges corresponding to upper and lower eyelids is also available. A pupil-feature amount extracted from one image to undergo determination of the eye open/closed degree is represented by Feature<b>1</b>(<i>t</i>):
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>Feature1(<i>t</i>)=Counter(<i>t</i>)&#x2003;&#x2003;(2)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0069" num="0068">After the feature amount calculation, the system control circuit <b>112</b> calculates a feature-change amount Feature<b>2</b>(<i>t</i>) of the calculated pupil-feature amount Feature<b>1</b>(<i>t</i>) (step S<b>205</b>). The feature-change amount Feature<b>2</b>(<i>t</i>) is calculated by calculating the difference between the pupil-feature amount Feature<b>1</b>(<i>t</i>) and a predetermined feature amount (in this case, a previously calculated pupil-feature amount Feature<b>1</b>(<i>t</i><sub>n</sub>):
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>Feature2(<i>t</i>)=|Counter(<i>t</i><sub>n</sub>)&#x2212;Counter(<i>t</i>)|&#x2003;&#x2003;(3)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
The previously calculated pupil-feature amount Feature<b>1</b>(<i>t</i><sub>n</sub>) is a pupil-feature amount calculated a predetermined time (e.g., 500 ms) before calculating the pupil-feature amount Feature<b>1</b>(<i>t</i>). In equation (3), n is an average time when the man blinks unconsciously. For example, when the average time when the man closes an open eye is 500 ms, n=500 ms is set.
</p>
<p id="p-0070" num="0069">After the feature-change amount calculation, the system control circuit <b>112</b> calculates eye open/closed degrees for the calculated pupil-feature amount Feature<b>1</b>(<i>t</i>) and feature-change amount Feature<b>2</b>(<i>t</i>), respectively (steps S<b>206</b> and S<b>207</b>). In the first embodiment, the eye open/closed degree will be explained using an eye closing degree BlinkDegree(t). However, the eye open/closed degree may also be represented by the eye opening degree.</p>
<p id="p-0071" num="0070">An eye open/closed degree BlinkDegree<b>1</b>(<i>t</i>) based on the pupil-feature amount Feature<b>1</b>(<i>t</i>) is calculated by substituting the pupil-feature amount Feature<b>1</b>(<i>t</i>) into a predetermined function fun<sub>1</sub>:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>BlinkDegree1(<i>t</i>)=func<sub>1</sub>(Feature1(<i>t</i>))&#x2003;&#x2003;(4)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
An eye open/closed degree BlinkDegree<b>2</b>(<i>t</i>) based on the feature-change amount Feature<b>2</b>(<i>t</i>) is calculated by substituting the feature-change amount Feature<b>2</b>(<i>t</i>) into a predetermined function func<sub>2</sub>:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>BlinkDegree2(<i>t</i>)=func<sub>2</sub>(Feature2(<i>t</i>))&#x2003;&#x2003;(5)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0072" num="0071">As represented by equation (6), the system control circuit <b>112</b> adds weights w<sub>1 </sub>and w<sub>2 </sub>to the eye open/closed degrees BlinkDegree<b>1</b>(<i>t</i>) and BlinkDegree<b>2</b>(<i>t</i>), and then adds the weighted eye open/closed degrees BlinkDegree<b>1</b>(<i>t</i>) and BlinkDegree<b>2</b>(<i>t</i>) (step S<b>208</b>), thereby calculating a final eye open/closed degree BlinkDegree(t) (step S<b>209</b>):
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>BlinkDegree(<i>t</i>)=<i>w</i><sub>1</sub>&#xd7;BlinkDegree1(<i>t</i>)+<i>w</i><sub>2</sub>&#xd7;BlinkDegree2(<i>t</i>)&#x2003;&#x2003;(6)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where the weights w<sub>1 </sub>and w<sub>2 </sub>in the embodiment are 1:1.
</p>
<p id="p-0073" num="0072"><figref idref="DRAWINGS">FIG. 7</figref> shows the distribution of eye open/closed degree values calculated by the predetermined function func<sub>1</sub>. As shown in <figref idref="DRAWINGS">FIG. 7</figref>, as the pupil-feature amount Feature<b>1</b>(<i>t</i>) calculated from one image to undergo determination of the eye open/closed degree becomes larger, the eye open/closed degree BlinkDegree<b>1</b>(<i>t</i>) becomes smaller. That is, as the pupil-feature amount Feature<b>1</b>(<i>t</i>) becomes larger, the possibility of determining that the eyes are open becomes higher.</p>
<p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. 8</figref> shows the distribution of eye open/closed degree values calculated by the predetermined function func<sub>2</sub>. As shown in <figref idref="DRAWINGS">FIG. 8</figref>, as the feature-change amount Feature<b>2</b>(<i>t</i>) becomes larger, the eye open/closed degree BlinkDegree<b>2</b>(<i>t</i>) becomes larger. That is, as the feature-change amount Feature<b>2</b>(<i>t</i>) becomes larger, the possibility of determining that the eyes are kept closed or being opened or closed becomes higher.</p>
<p id="p-0075" num="0074">Referring back to <figref idref="DRAWINGS">FIG. 3</figref>, after executing the final eye open/closed degree calculation, the system control circuit <b>112</b> makes a binary determination of whether the eyes are closed or open. This determination is achieved by executing threshold processing for the eye open/closed degree BlinkDegree(t) (step S<b>210</b>). The system control circuit <b>112</b> sends back a determination result &#x201c;eyes open&#x201d; or &#x201c;eye closed&#x201d; based on the relationship (equal to and larger or smaller than the threshold) between BlinkDegree(t) and the threshold (step S<b>211</b>). Then, the process ends.</p>
<p id="p-0076" num="0075"><figref idref="DRAWINGS">FIGS. 9 and 10</figref> are views for explaining effects obtained using the pupil-feature amount Feature<b>1</b>(<i>t</i>) calculated from an image to undergo determination of the eye open/closed degree and the corresponding feature-change amount Feature<b>2</b>(<i>t</i>) when determining the eye open/closed state.</p>
<p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. 9</figref> is a view showing results of calculating the eye open/closed degrees in respective states of person A having a large eye open/closed width and those of person B having a small eye open/closed width. As described above, the eye open/closed degree BlinkDegree<b>1</b>(<i>t</i>) is calculated based on the pupil-feature amount Feature<b>1</b>(<i>t</i>) of an image to undergo determination of the eye open/closed degree. The eye open/closed degree BlinkDegree<b>2</b>(<i>t</i>) is calculated based on the feature-change amount Feature<b>2</b>(<i>t</i>) representing the difference between the pupil-feature amount Feature<b>1</b>(<i>t</i>) and the previously calculated pupil-feature amount Feature<b>1</b>(<i>t</i><sub>n</sub>). BlinkDegree(t) represents the final eye open/closed degree which is calculated by adding the weighted BlinkDegree<b>1</b>(<i>t</i>) and BlinkDegree<b>2</b>(<i>t</i>).</p>
<p id="p-0078" num="0077">A case where the eyes of person A change from state <b>1</b> (eye open state) to state <b>1</b>&#x2032; (eye open state) will be examined. In this case, the pupil is clearly seen in state <b>1</b>&#x2032; (eye open state), so the pupil-feature amount Feature<b>1</b>(<i>t</i>) takes a large value. Hence, the eye open/closed degree BlinkDegree<b>1</b>(<i>t</i>) takes a small value. When the eyes of person A change from state <b>1</b> (eye open state) to state <b>1</b>&#x2032; (eye open state), the pupil-feature amount Feature<b>1</b>(<i>t</i>) hardly changes. Thus, the eye open/closed degree BlinkDegree<b>2</b>(<i>t</i>) calculated based on the feature-change amount Feature<b>2</b>(<i>t</i>) between the pupil-feature amount Feature<b>1</b>(<i>t</i>) and the previously calculated pupil-feature amount Feature<b>1</b>(<i>t</i><sub>n</sub>) also takes a small value. As a result, the eye open/closed degree BlinkDegree(t) finally calculated after adding the weights=1:1 becomes &#x201c;0.0&#x201d;.</p>
<p id="p-0079" num="0078">A case where the eyes of person A change from state <b>1</b> (eye open state) to state <b>2</b> (eye half-open state) will be examined. In this case, the pupil region is partially seen in state <b>2</b> (eye half-open state), so the pupil-feature amount Feature<b>1</b>(<i>t</i>) takes an intermediate value (e.g., a value between 0 and 1). Hence, the eye open/closed degree BlinkDegree<b>1</b>(<i>t</i>) also takes an intermediate value. When the eyes of person A change from state <b>1</b> (eye open state) to state <b>2</b> (eye half-open state), the pupil-feature amount Feature<b>1</b>(<i>t</i>) slightly changes. Thus, the eye open/closed degree BlinkDegree<b>2</b>(<i>t</i>) calculated based on the feature-change amount Feature<b>2</b>(<i>t</i>) between the pupil-feature amount Feature<b>1</b>(<i>t</i>) and the previously calculated pupil-feature amount Feature<b>1</b>(<i>t</i><sub>n</sub>) also takes an intermediate value. The eye open/closed degree BlinkDegree(t) finally calculated after adding the weights=1:1 becomes &#x201c;1.0&#x201d;.</p>
<p id="p-0080" num="0079">Similarly, a case where the eyes of person A change from state <b>1</b> (eye open state) to state <b>3</b> (eye closed state) will be examined. In this case, the eye open/closed degree BlinkDegree<b>1</b>(<i>t</i>) calculated based on the pupil-feature amount Feature<b>1</b>(<i>t</i>) takes a large value. Also, the eye open/closed degree BlinkDegree<b>2</b>(<i>t</i>) calculated based on the feature-change amount Feature<b>2</b>(<i>t</i>) also takes a large value. The eye open/closed degree BlinkDegree(t) finally calculated after adding the weights=1:1 becomes &#x201c;2.0&#x201d;.</p>
<p id="p-0081" num="0080">In contrast, a case where the eyes of person B change from state <b>4</b> (eye open state) to state <b>4</b>&#x2032; (eye open state) will be examined. In this case, the eye open/closed degree BlinkDegree<b>1</b>(<i>t</i>) calculated based on the pupil-feature amount Feature<b>1</b>(<i>t</i>) takes an intermediate value. The eye open/closed degree BlinkDegree<b>2</b>(<i>t</i>) calculated based on the feature-change amount Feature<b>2</b>(<i>t</i>) takes a small value. The eye open/closed degree BlinkDegree(t) finally calculated after adding the weights=1:1 becomes &#x201c;0.5&#x201d;.</p>
<p id="p-0082" num="0081">A case where the eyes of person B change from state <b>4</b> (eye open state) to state <b>5</b> (eye closed state) will be examined. In this case, the eye open/closed degree BlinkDegree<b>1</b>(<i>t</i>) calculated based on the pupil-feature amount Feature<b>1</b>(<i>t</i>) takes a large value. The eye open/closed degree BlinkDegree<b>2</b>(<i>t</i>) calculated based on the feature-change amount Feature<b>2</b>(<i>t</i>) takes an intermediate value. The eye open/closed degree BlinkDegree(t) finally calculated after adding the weights=1:1 becomes &#x201c;1.5&#x201d;.</p>
<p id="p-0083" num="0082"><figref idref="DRAWINGS">FIG. 10</figref> shows the distribution of the calculated eye open/closed degree BlinkDegree(t). As shown in <figref idref="DRAWINGS">FIG. 10</figref>, calculated eye open/closed degrees in the respective states of persons A and B take different values. In other words, the eye open/closed degree can be detected at high precision regardless of individual differences in eye size.</p>
<p id="p-0084" num="0083">As described above, according to the first embodiment, eye open/closed degrees calculated based on a pupil-feature amount obtained from an image to be determined, and a feature-change amount representing the difference between the pupil-feature amount and a previously calculated pupil-feature amount are weighted and added to calculate the final eye open/closed degree. As a result, the eye open/closed state can be detected at high precision regardless of individual differences in eye size.</p>
<p id="p-0085" num="0084">In the above-described first embodiment, the final eye open/closed degree is calculated after weighting. However, weighting is not indispensable, and the final eye open/closed degree may also be calculated without weighting.</p>
<heading id="h-0006" level="1">Second Embodiment</heading>
<p id="p-0086" num="0085">The second embodiment will now be explained. The arrangement of an apparatus and the sequence of the overall operation in the second embodiment are the same as those in <figref idref="DRAWINGS">FIGS. 1 and 2</figref> described in the first embodiment, a description thereof will be omitted, and only differences from the first embodiment will be explained. The second embodiment is different from the first embodiment in eye open/closed degree determination described with reference to <figref idref="DRAWINGS">FIG. 3</figref>.</p>
<p id="p-0087" num="0086">The sequence of eye open/closed degree determination according to the second embodiment will be explained with reference to <figref idref="DRAWINGS">FIG. 11</figref>. In the second embodiment, to prevent a repetitive description, a description of detailed processing (e.g., methods of detecting and calculating a pupil-feature amount) described in the first embodiment will be omitted, and only a rough sequence representing the difference will be explained.</p>
<p id="p-0088" num="0087">When this processing starts, a system control circuit <b>112</b> estimates a state in which the eyes of a face detected in step S<b>110</b> (<figref idref="DRAWINGS">FIG. 2A</figref>) are open. More specifically, the system control circuit <b>112</b> estimates a state in which the eyes are open, and checks whether this eye state corresponds to a predetermined eye state (narrow eyes in the second embodiment). Even when a person with narrow eyes normally opens his eyes, most part of the pupil region is hidden.</p>
<p id="p-0089" num="0088">To estimate an eye state when the eyes are open, the system control circuit <b>112</b> calculates a pupil-feature amount Feature<b>1</b>(<i>t</i>) by the same method as that in the first embodiment (step S<b>300</b>). The pupil-feature amount Feature<b>1</b>(<i>t</i>) is repetitively calculated till the lapse of a predetermined time (NO in step S<b>301</b>). That is, the system control circuit <b>112</b> calculates pupil-feature amounts Feature<b>1</b>(<i>t</i>) from respective frame images sequentially input within the predetermined time. The predetermined time is an average time taken to blink. It suffices to always blink within a predetermined time (e.g., 5 sec).</p>
<p id="p-0090" num="0089">Upon the lapse of the predetermined time (YES in step S<b>301</b>), the system control circuit <b>112</b> estimates an eye state on the basis of the pupil-feature amount Feature<b>1</b>(<i>t</i>) calculated from a plurality of frame images in step S<b>300</b> (step S<b>302</b>). More specifically, the system control circuit <b>112</b> estimates an eye open state by referring to pupil-feature amounts Feature<b>1</b>(<i>t</i><sub>1</sub>), . . . , Feature<b>1</b>(<i>t</i><sub>n1</sub>) calculated within the predetermined time (time t=t<sub>1 </sub>[s] to time t=t<sub>n1</sub>[s]).</p>
<p id="p-0091" num="0090"><figref idref="DRAWINGS">FIG. 12</figref> is a graph showing the pupil-feature amounts Feature<b>1</b>(<i>t</i><sub>1</sub>), . . . , Feature<b>1</b>(<i>t</i><sub>n1</sub>) calculated from time t=t<sub>1 </sub>[s] to time t=t<sub>n1 </sub>[s].</p>
<p id="p-0092" num="0091">Whether the eyes correspond to a predetermined eye state (narrow eyes) is determined based on the width L (=Feature<b>1</b>Max&#x2212;Feature<b>1</b>Min) of the pupil-feature amount Feature<b>1</b>(<i>t</i>) calculated from t=t<sub>1 </sub>[s] to time t=t<sub>n1 </sub>[s]. More specifically, if the width L of the pupil-feature amount Feature<b>1</b>(<i>t</i>) is larger than a predetermined threshold L<sub>th</sub>, it is determined that most part of the pupil region is not hidden when the eyes are open. If the width L of the pupil-feature amount Feature<b>1</b>(<i>t</i>) is equal to or smaller than the predetermined threshold L<sub>th</sub>, it is determined that most part of the pupil region is hidden when the eyes are open.</p>
<p id="p-0093" num="0092">If the eyes are not in the predetermined eye state (NO in step S<b>303</b>), the system control circuit <b>112</b> calculates BlinkDegree(t<sub>n</sub>) using only the pupil-feature amount Feature<b>1</b>(<i>t</i><sub>n1</sub>) calculated from one image at time t=t<sub>n1 </sub>[s] (step S<b>304</b>). Calculating the eye open/closed degree BlinkDegree(t<sub>n1</sub>) using only the pupil-feature amount Feature<b>1</b>(<i>t</i><sub>n1</sub>) equals setting the weight w<sub>2 </sub>to 0.</p>
<p id="p-0094" num="0093">If the system control circuit <b>112</b> determines that the eyes are in the predetermined eye state, that is, the eyes are narrow (YES in step S<b>303</b>), it calculates the eye open/closed degree by using both the pupil-feature amount and feature-change amount by the same method as that in the first embodiment (step S<b>305</b>). More specifically, the system control circuit <b>112</b> calculates the eye open/closed degree BlinkDegree(t<sub>n1</sub>) by using the pupil-feature amount Feature<b>1</b>(<i>t</i><sub>n1</sub>) calculated from one image at time t=t<sub>n1</sub>[s] and the feature-change amount Feature<b>2</b>(<i>t</i><sub>n1</sub>). The feature-change amount Feature<b>2</b>(<i>t</i><sub>n1</sub>) represents the difference between the pupil-feature amount Feature<b>1</b>(<i>t</i><sub>n1</sub>) and a pupil-feature amount Feature<b>1</b>(<i>t</i><sub>n1-n1&#x2032;</sub>) (1&#x2266;n<sub>1&#x2032;</sub>&#x3c;n<sub>1</sub>) calculated from a previously input image. In this case, the weights w<sub>1 </sub>and w<sub>2 </sub>are set to 1:1.</p>
<p id="p-0095" num="0094">After that, the system control circuit <b>112</b> determines the eye open/closed state by executing threshold processing for the calculated eye open/closed degree BlinkDegree(t<sub>n1</sub>, and sends back the determination result (eye closed or eye open) (step S<b>306</b>). Then, the process ends.</p>
<p id="p-0096" num="0095">As described above, according to the second embodiment, if the eyes of a person, like person A shown in <figref idref="DRAWINGS">FIG. 13</figref>, are not in a predetermined eye state, that is, are not narrow, the eye open/closed degree BlinkDegree(t<sub>n1</sub>) is calculated using only a feature amount obtained from one image. If the eyes of a person, like person B shown in <figref idref="DRAWINGS">FIG. 13</figref>, are in a predetermined eye state, that is, are narrow, the eye open/closed degree BlinkDegree(t<sub>n1</sub>) is calculated using a feature amount obtained from one image and a feature-change amount. By switching the eye open/closed degree calculation method on the basis of eye state, the eye open/closed degree can be determined at high precision regardless of individual differences in eye size. Accordingly, high-precision detection of the eye open/closed degree can be achieved at minimum processing cost.</p>
<heading id="h-0007" level="1">Third Embodiment</heading>
<p id="p-0097" num="0096">The third embodiment will be explained. The arrangement of an apparatus in the third embodiment is the same as that in <figref idref="DRAWINGS">FIG. 1</figref> described in the first embodiment, a description thereof will be omitted, and only differences from the first embodiment will be explained.</p>
<p id="p-0098" num="0097"><figref idref="DRAWINGS">FIGS. 14A and 14B</figref> show an example of a processing sequence in an image sensing apparatus <b>100</b> according to the third embodiment. Processes in step S<b>400</b> to S<b>408</b> are the same as those in steps S<b>100</b> to S<b>108</b> in <figref idref="DRAWINGS">FIG. 2A</figref> in the first embodiment, and a description thereof will be omitted.</p>
<p id="p-0099" num="0098">If the user presses the first switch (focusing switch) in step S<b>409</b> (YES in step S<b>409</b>), face detection, AE/AF control, and through display processing are executed similarly to the first embodiment (steps S<b>410</b> to S<b>412</b>). After the through display, the pupil-feature amount is calculated for a predetermined time (step S<b>413</b>). More specifically, by the same method as that in the second embodiment, the pupil-feature amount is calculated from frame images sequentially input within a predetermined time (time t=t<sub>2 </sub>[s] to time t=t<sub>n2 </sub>[s].</p>
<p id="p-0100" num="0099">If the user presses the second switch (shooting switch) (YES in step S<b>414</b>), shooting is done (step S<b>415</b>). In this case, shooting is performed regardless of the eye open/closed state. At this time, a sensed image is temporarily stored in a memory <b>108</b> shown in <figref idref="DRAWINGS">FIG. 1</figref>. In addition to the sensed high-resolution image, the memory <b>108</b> also stores a low-resolution image compressed for use in eye open/closed determination.</p>
<p id="p-0101" num="0100">If the user does not press the second switch in step S<b>414</b> (NO in step S<b>414</b>), face detection (step S<b>420</b>) and through display (step S<b>421</b>) are executed again. Then, the process returns to step S<b>413</b>.</p>
<p id="p-0102" num="0101">If shooting is done in step S<b>415</b>, a system control circuit <b>112</b> performs eye open/closed determination using the compressed low-resolution image (step S<b>416</b>). Details of the eye open/closed degree determination will be explained later.</p>
<p id="p-0103" num="0102">The system control circuit <b>112</b> displays a quick review on the image sensing apparatus <b>100</b> (step S<b>417</b>). At this time, if it is determined in the eye open/closed degree determination that the eyes are closed, a message such as &#x201c;would you like to shoot again?&#x201d; is displayed to prompt the user to shoot again. It is also possible to force the user to shoot without any display which prompts him to shoot again.</p>
<p id="p-0104" num="0103">If the user does not want to shoot again (NO in step S<b>418</b>), the system control circuit <b>112</b> records, in a flash memory or the like, the high-resolution sensed image stored in the memory <b>108</b> (step S<b>419</b>). Then, the process ends. If the user wants to shoot again (YES in step S<b>418</b>), the system control circuit <b>112</b> performs face detection for the next frame image (step S<b>420</b>), and a through display (step S<b>421</b>). After that, the process returns to step S<b>413</b>.</p>
<p id="p-0105" num="0104">Details of the eye open/closed degree determination in step S<b>416</b> shown in <figref idref="DRAWINGS">FIG. 14B</figref> will be explained with reference to <figref idref="DRAWINGS">FIG. 15</figref>. In the third embodiment, to prevent a repetitive description, a description of detailed processing (e.g., methods of detecting and calculating a pupil-feature amount) described in the first and second embodiments will be omitted, and only a rough sequence representing the difference will be explained.</p>
<p id="p-0106" num="0105">When this processing starts, the system control circuit <b>112</b> refers to pupil-feature amounts Feature<b>1</b>(<i>t</i><sub>2</sub>), . . . , Feature<b>1</b>(<i>t</i><sub>n2</sub>) calculated in step S<b>413</b> (<figref idref="DRAWINGS">FIG. 14B</figref>) from time t=t<sub>2 </sub>[s] to time t=t<sub>n2 </sub>[s]. The system control circuit <b>112</b> estimates a state in which the eyes of a face detected in step S<b>410</b> (<figref idref="DRAWINGS">FIG. 14A</figref>) or step S<b>420</b> (<figref idref="DRAWINGS">FIG. 14B</figref>) are open (step S<b>500</b>). Similar to the second embodiment, whether the eyes are in a predetermined eye state is determined by comparing, with a predetermined threshold L<sub>th</sub>, the width L between the maximum and minimum values of the pupil-feature amount Feature<b>1</b>(<i>t</i>) within a predetermined time. Note that the pupil-feature amounts Feature<b>1</b>(<i>t</i><sub>2</sub>), . . . , Feature<b>1</b>(<i>t</i><sub>n2</sub>) from time t=t<sub>2 </sub>[s] to time t=t<sub>n2 </sub>[s] are stored in the memory <b>108</b> in <figref idref="DRAWINGS">FIG. 1</figref>, as described above.</p>
<p id="p-0107" num="0106">If the system control circuit <b>112</b> determines that the eyes are in the predetermined eye state, that is, are narrow (YES in step S<b>501</b>), it executes weight determination (step S<b>502</b>). Details of the weight determination will be described later. After the weight determination, the system control circuit <b>112</b> performs processing such as weighting similarly to the first embodiment, and then calculates a final eye open/closed degree BlinkDegree(t<sub>n2</sub>) (step S<b>503</b>). That is, the system control circuit <b>112</b> calculates the eye open/closed degree using both the pupil-feature amount and feature-change amount.</p>
<p id="p-0108" num="0107">If the eyes are not in the predetermined eye state, that is, are not narrow (NO in step S<b>501</b>), the system control circuit <b>112</b> performs processing such as weighting similarly to the first embodiment, and then calculates the final eye open/closed degree BlinkDegree(t<sub>n2</sub>) (step S<b>503</b>). Also in this case, the system control circuit <b>112</b> calculates the eye open/closed degree using both the pupil-feature amount and feature-change amount.</p>
<p id="p-0109" num="0108">Thereafter, the system control circuit <b>112</b> determines the eye open/closed state by executing threshold processing for the calculated eye open/closed degree BlinkDegree(t<sub>n2</sub>), and sends back the determination result (eye closed or eye open) (step S<b>504</b>). Then, the process ends.</p>
<p id="p-0110" num="0109">Details of the weight determination in step S<b>502</b> shown in <figref idref="DRAWINGS">FIG. 15</figref> will be explained with reference to <figref idref="DRAWINGS">FIG. 16</figref>. In the following description, thresholds which satisfy a relation &#x201c;L<sub>th1</sub>&#x3e;L<sub>th2</sub>&#x201d; are used to determine weights. Note that the weights are determined using a table prepared in advance.</p>
<p id="p-0111" num="0110">When this processing starts, the system control circuit <b>112</b> determines whether the width L of the pupil-feature amount Feature<b>1</b>(<i>t</i>) calculated within a predetermined time in step S<b>413</b> (<figref idref="DRAWINGS">FIG. 14B</figref>) satisfies L&#x3e;L<sub>th1</sub>. If L&#x3e;L<sub>th1 </sub>(YES in step S<b>600</b>), the weights w<sub>1 </sub>and w<sub>2 </sub>are determined to be 1:1 (step S<b>601</b>), and then the process ends.</p>
<p id="p-0112" num="0111">If L&#x2266;L<sub>th1 </sub>(NO in step S<b>600</b>), the system control circuit <b>112</b> determines whether L<sub>th1</sub>&#x2267;L&#x3e;L<sub>th2 </sub>holds. If L<sub>th1</sub>&#x2267;L&#x3e;L<sub>th2 </sub>(YES in step S<b>602</b>), the weights w<sub>1 </sub>and w<sub>2 </sub>are determined to be 1:2 (step S<b>603</b>), and then the process ends. If L&#x2266;L<sub>th2 </sub>(NO in step S<b>602</b>), the weights w<sub>1 </sub>and w<sub>2 </sub>are determined to be 1:5 (step S<b>604</b>), and then the process ends. That is, as the width L between the maximum and minimum values of the pupil-feature amount Feature<b>1</b>(<i>t</i>) within a predetermined time becomes smaller, the weight w<b>2</b> of the eye open/closed degree BlinkDegree<b>2</b>(<i>t</i><sub>n2</sub>) is set larger.</p>
<p id="p-0113" num="0112">The processes described with reference to <figref idref="DRAWINGS">FIGS. 15 and 16</figref> change the set weights only when the eyes correspond to a predetermined eye state (narrow eyes), but the present invention is not limited to this. For example, when the eyes of a person are not in a predetermined eye state, for example, are large, the weight w<sub>1 </sub>may also be set larger than the weight w<sub>2</sub>.</p>
<p id="p-0114" num="0113"><figref idref="DRAWINGS">FIGS. 17 and 18</figref> are views for explaining effects obtained by setting the weight w<sub>2 </sub>of the eye open/closed degree BlinkDegree<b>2</b>(<i>t</i><sub>n2</sub>) calculated based on the feature-change amount Feature<b>2</b>(<i>t</i><sub>n2</sub>) upon detecting a predetermined eye state.</p>
<p id="p-0115" num="0114">For person A whose eyes are not in a predetermined eye state, that is, are not narrow, as shown in <figref idref="DRAWINGS">FIG. 17</figref>, the weight w<sub>1 </sub>of BlinkDegree<b>1</b>(<i>t</i><sub>n2</sub>) and the weight w<sub>2 </sub>of the eye open/closed degree BlinkDegree<b>2</b>(<i>t</i><sub>n2</sub>) are set to 1:1.</p>
<p id="p-0116" num="0115">In contrast, for a person whose eyes are in a predetermined eye state, that is, are narrow, like person B shown in <figref idref="DRAWINGS">FIG. 17</figref>, the weight w<sub>1 </sub>of BlinkDegree<b>1</b>(<i>t</i><sub>n2</sub>) and the weight w<sub>2 </sub>of the eye open/closed degree BlinkDegree<b>2</b>(<i>t</i><sub>n2</sub>) are set to 1:2. That is, the weight w<sub>2 </sub>is set larger. With this setting, state <b>2</b> of person A and state <b>4</b>&#x2032; of person B can be distinguished from each other. Further, state <b>3</b> of person A whose eyes are not narrow and state <b>5</b> of person B whose eyes are narrow can be determined to have the same eye open/closed degree.</p>
<p id="p-0117" num="0116">For a person whose eyes are very narrow, like person B shown in <figref idref="DRAWINGS">FIG. 18</figref>, the weight w<sub>2 </sub>of the eye open/closed degree BlinkDegree<b>2</b>(<i>t</i><sub>n2</sub>) calculated based on the feature-change amount Feature<b>2</b>(<i>t</i><sub>n2</sub>) is set much larger. With this setting, the eye open/closed degree can be determined regardless of individual differences in eye size.</p>
<p id="p-0118" num="0117">As described above, according to the third embodiment, it is determined from a change of the pupil-feature amount within a predetermined time whether the eye size of a person to undergo determination of the eye open/closed state is in a predetermined eye state (narrow eyes). Weights are changed based on the determination result, and then the eye open/closed degree is determined. As a result, an eye state can be detected at higher precision regardless of eye size.</p>
<p id="p-0119" num="0118">Typical embodiments of the present invention have been described above. However, the present invention is not limited to the above-described embodiments and embodiments shown in the accompanying drawings, and can be properly modified without departing from the scope of the appended claims.</p>
<p id="p-0120" num="0119">For example, in the second and third embodiments, a state in which the eyes are narrow is determined as a predetermined eye state. However, the present invention is not limited to this, and a state in which the eyes are large may also be determined as a predetermined eye state. Also in this case, the same processing as that described above can be adopted.</p>
<p id="p-0121" num="0120">In the second and third embodiments, a predetermined eye state is estimated by comparing, with the predetermined threshold L<sub>th</sub>, the width L between the maximum and minimum values of the pupil-feature amount Feature<b>1</b>(<i>t</i>) within a predetermined time. However, the present invention is not limited to this. For example, a predetermined eye state may also be estimated by comparing, with a predetermined threshold, the average value of the pupil-feature amount Feature<b>1</b>(<i>t</i>) within a predetermined time.</p>
<p id="p-0122" num="0121">Some or all processes described in the first to third embodiments may also be combined. For example, the processing described in the second embodiment to switch the method of determining the eye open/closed degree, and the weight determination described in the third embodiment may also be executed in combination.</p>
<p id="p-0123" num="0122">The present invention also includes a case where the functions of the above-described embodiments are achieved by supplying a software program to a system or apparatus directly or from a remote place, and reading out and executing the supplied program codes by a computer incorporated in the system or apparatus. In this case, the supplied program is a computer program corresponding to the flowcharts shown in the drawings in the embodiments.</p>
<p id="p-0124" num="0123">Hence, the program codes installed in the computer to implement functional processing of the present invention by the computer also implement the present invention. That is, the present invention also includes the computer program for implementing functional processing of the present invention. In this case, the program may take the form of an object code, a program executed by an interpreter, or script data supplied to an OS (Operating System) as long as the functions of the program can be provided.</p>
<p id="p-0125" num="0124">Various types of the computer-readable storage media may be used for supplying the computer program.</p>
<p id="p-0126" num="0125">The program can also be supplied by downloading the computer program of the present invention from an Internet homepage to a storage medium such as a hard disk. The program can also be supplied by grouping program codes which form the program of the present invention into a plurality of files, and downloading the files from different homepages.</p>
<p id="p-0127" num="0126">Further, the functions of the above-described embodiments are also implemented in cooperation with the OS or the like running on the computer on the basis of the instructions of the program.</p>
<p id="p-0128" num="0127">While the present invention has been described with reference to exemplary embodiments, it is to be understood that the invention is not limited to the disclosed exemplary embodiments. The scope of the following claims is to be accorded the broadest interpretation so as to encompass all such modifications and equivalent structures and functions.</p>
<p id="p-0129" num="0128">This application claims the benefit of Japanese Patent Application No. 2008-051121 filed on Feb. 29, 2008, which is hereby incorporated by reference herein in its entirety.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A sensing apparatus comprising:
<claim-text>an input unit configured to input an image;</claim-text>
<claim-text>a face detection unit configured to detect a face of a person from the image input from said input unit;</claim-text>
<claim-text>a first calculation unit configured to calculate a pupil-feature amount associated with an eye open/closed state from faces detected from a plurality of images input from said input unit within a predetermined time;</claim-text>
<claim-text>a second calculation unit configured to calculate, as a feature-change amount, a difference between the pupil-feature amount calculated by said first calculation unit and a feature amount calculated from a face detected from an image input before a predetermined time;</claim-text>
<claim-text>a weighting unit configured to determine weights for the pupil-feature amount calculated by said first calculation unit and to determine weights for the feature-change amount calculated by said second calculation unit, wherein the weights are determined on the basis of a plurality of pupil-feature amounts calculated by said first calculation unit from the plurality of images, and wherein the weighting unit is further configured to set the determined weights for the pupil-feature amount and for the feature-change amount;</claim-text>
<claim-text>a determination unit configured to determine whether the eyes of the face detected by said face detection unit are open or closed on the basis of the pupil-feature amount and the feature-change amount for which said weighting unit sets the weights; and</claim-text>
<claim-text>a recording unit configured to record the input image if the eyes are determined to be open,</claim-text>
<claim-text>wherein said weighting unit sets the weight for the feature-change amount to be larger than the weight set for the pupil-feature amount when a difference between a maximum value and a minimum value in a plurality of pupil-feature amounts calculated by said first calculation unit from the plurality of images is smaller than a predetermined threshold.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said weighting unit sets the weight for the feature amount to be larger than the weight set for the feature-change amount when a difference between a maximum value and a minimum value in a plurality of feature amounts calculated by said first calculation unit from the plurality of images is larger than a predetermined threshold.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. A sensing apparatus comprising:
<claim-text>an input unit configured to input an image;</claim-text>
<claim-text>a face detection unit configured to detect a face of a person from the image input from said input unit;</claim-text>
<claim-text>a first calculation unit configured to calculate a feature amount associated with an eye open/closed state from faces detected from a plurality of images input from said input unit within a predetermined time;</claim-text>
<claim-text>a second calculation unit configured to calculate, as a feature-change amount, a difference between the feature amount calculated by said first calculation unit and a feature amount calculated from a face detected from an image input before a predetermined time;</claim-text>
<claim-text>a weighting unit configured to determine weights for the feature amount calculated by said first calculation unit and to determine weights for the feature-change amount calculated by said second calculation unit, wherein the weights are determined on the basis of a plurality of feature amounts calculated by said first calculation unit from the plurality of images, and wherein the weighting unit is further configured to set the determined weights for the feature amount and the feature-change amount;</claim-text>
<claim-text>a third calculation unit configured to calculate an eye open/closed degree of eyes of the face detected by said face detection unit on the basis of the feature amount and the feature-change amount for which said weighting unit sets the weights; and</claim-text>
<claim-text>a determination unit configured to determine whether the eyes of the face detected by said face detection unit are open or closed by performing threshold processing for the eye open/closed degree calculated by said third calculation unit,</claim-text>
<claim-text>wherein said third calculation unit calculates the eye open/closed degree of eyes of the face detected by said face detection unit on the basis of only the weighted feature amount when a difference between a maximum value and a minimum value in a plurality of feature amounts calculated by said first calculation unit from the plurality of images has a predetermined relation with a preset threshold.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. A sensing method for a sensing apparatus comprising:
<claim-text>an input step of inputting an image;</claim-text>
<claim-text>a face detection step of detecting a face of a person from the image input in said input step;</claim-text>
<claim-text>a first calculation step of calculating a pupil-feature amount associated with an eye open/closed state from faces detected from a plurality of images input in said input step within a predetermined time;</claim-text>
<claim-text>a second calculation step of calculating, as a feature-change amount, a difference between the pupil-feature amount calculated in said first calculation step and a feature amount calculated from a face detected from an image input before a predetermined time;</claim-text>
<claim-text>a weighting step of determining weights for the pupil-feature amount calculated in said first calculation step and of determining weights for the feature-change amount calculated in said second calculation step, wherein the weights are determined on the basis of a plurality of pupil-feature amounts calculated in said first calculation step from the plurality of images, and wherein the weighting step further sets the determined weights for the pupil-feature amount and for the feature-change amount;</claim-text>
<claim-text>a determination step of determining whether the eyes of the face detected in said face detection step are open or closed on the basis of the pupil-feature amount and the feature-change amount for which the weights are set in said weighting step; and</claim-text>
<claim-text>a recording step of recording the input image if the eyes are determined to be open,</claim-text>
<claim-text>wherein said weighting step sets the weight for the feature-change amount to be larger than the weight set for the pupil-feature amount when a difference between a maximum value and a minimum value in a plurality of pupil-feature amounts calculated by said first calculation step from the plurality of images is smaller than a predetermined threshold.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. A non-transitory computer-readable storage medium storing a computer program for causing a computer to execute each step of a sensing method for a sensing apparatus according to <claim-ref idref="CLM-00004">claim 4</claim-ref>.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. A sensing method for a sensing apparatus comprising:
<claim-text>an input step of inputting an image;</claim-text>
<claim-text>a face detection step of detecting a face of a person from the image input from said input step;</claim-text>
<claim-text>a first calculation step of calculating a feature amount associated with an eye open/closed state from faces detected from a plurality of images input from said input step within a predetermined time;</claim-text>
<claim-text>a second calculation step of calculating, as a feature-change amount, a difference between the feature amount calculated by said first calculation step and a feature amount calculated from a face detected from an image input before a predetermined time;</claim-text>
<claim-text>a weighting step of determining weights for the feature amount calculated by said first calculation step and of determining weights for the feature-change amount calculated by said second calculation step, wherein the weights are determined on the basis of a plurality of feature amounts calculated by said first calculation step from the plurality of images, and wherein the weighting step sets the determined weights for the feature amount and the feature-change amount;</claim-text>
<claim-text>a third calculation step of calculating an eye open/closed degree of eyes of the face detected by said face detection step on the basis of the feature amount and the feature-change amount for which said weighting step sets the weights; and</claim-text>
<claim-text>a determination step of determining whether the eyes of the face detected by said face detection step are open or closed by performing threshold processing for the eye open/closed degree calculated by said third calculation step,</claim-text>
<claim-text>wherein said third calculation step calculates the eye open/closed degree of eyes of the face detected by said face detection step on the basis of only the weighted feature amount when a difference between a maximum value and a minimum value in a plurality of feature amounts calculated by said first calculation step from the plurality of images has a predetermined relation with a preset threshold.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. A non-transitory computer-readable storage medium storing a computer program for causing a computer to execute the sensing method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>. </claim-text>
</claim>
</claims>
</us-patent-grant>
