<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625859-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625859</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12902744</doc-number>
<date>20101012</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>JP</country>
<doc-number>P2009-242771</doc-number>
<date>20091021</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>643</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382118</main-classification>
<further-classification>382203</further-classification>
<further-classification>382218</further-classification>
</classification-national>
<invention-title id="d2e71">Information processing apparatus, information processing method, and program</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>6381346</doc-number>
<kind>B1</kind>
<name>Eraslan</name>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382118</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6504546</doc-number>
<kind>B1</kind>
<name>Cosatto et al.</name>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345473</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6801641</doc-number>
<kind>B2</kind>
<name>Eraslan</name>
<date>20041000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382118</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>7184047</doc-number>
<kind>B1</kind>
<name>Crampton</name>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345473</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>RE42205</doc-number>
<kind>E</kind>
<name>Jung et al.</name>
<date>20110300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>8111281</doc-number>
<kind>B2</kind>
<name>Sangberg et al.</name>
<date>20120200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348 1401</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2004/0175041</doc-number>
<kind>A1</kind>
<name>Miller</name>
<date>20040900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382190</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2005/0162419</doc-number>
<kind>A1</kind>
<name>Kim et al.</name>
<date>20050700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345419</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2006/0050933</doc-number>
<kind>A1</kind>
<name>Adam et al.</name>
<date>20060300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382118</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2009/0002479</doc-number>
<kind>A1</kind>
<name>Sangberg et al.</name>
<date>20090100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348 1402</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2009/0202114</doc-number>
<kind>A1</kind>
<name>Morin et al.</name>
<date>20090800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382118</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2010/0149177</doc-number>
<kind>A1</kind>
<name>Miller</name>
<date>20100600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345419</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2011/0022965</doc-number>
<kind>A1</kind>
<name>Lawrence et al.</name>
<date>20110100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715747</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2012/0113106</doc-number>
<kind>A1</kind>
<name>Choi et al.</name>
<date>20120500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345419</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2013/0045804</doc-number>
<kind>A1</kind>
<name>Ruke</name>
<date>20130200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>463 42</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2013/0129141</doc-number>
<kind>A1</kind>
<name>Wang et al.</name>
<date>20130500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>2013/0129158</doc-number>
<kind>A1</kind>
<name>Wang et al.</name>
<date>20130500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382118</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>JP</country>
<doc-number>08-305836</doc-number>
<date>19961100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>JP</country>
<doc-number>10-255017</doc-number>
<date>19980900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>19</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382118</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382203</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382218-222</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>25</number-of-drawing-sheets>
<number-of-figures>55</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20110091071</doc-number>
<kind>A1</kind>
<date>20110421</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Sabe</last-name>
<first-name>Kohtaro</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Ohashi</last-name>
<first-name>Takeshi</first-name>
<address>
<city>Kanagawa</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Sabe</last-name>
<first-name>Kohtaro</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Ohashi</last-name>
<first-name>Takeshi</first-name>
<address>
<city>Kanagawa</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Oblon, Spivak, McClelland, Maier &#x26; Neustadt, L.L.P.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Sony Corporation</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Carter</last-name>
<first-name>Aaron W</first-name>
<department>2665</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">An information processing apparatus including an image acquisition unit that acquires a target image; a face part extraction unit that extracts a face region including a face part from the target image; an identification unit that identifies a model face part by comparing the face part to a plurality of model face parts stored in a storage unit; and an illustration image determination unit that determines an illustration image corresponding to the identified model face part.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="119.55mm" wi="223.01mm" file="US08625859-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="264.16mm" wi="177.29mm" orientation="landscape" file="US08625859-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="225.47mm" wi="140.04mm" file="US08625859-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="228.68mm" wi="150.45mm" orientation="landscape" file="US08625859-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="237.41mm" wi="133.10mm" orientation="landscape" file="US08625859-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="217.09mm" wi="151.30mm" orientation="landscape" file="US08625859-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="141.82mm" wi="143.93mm" file="US08625859-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="178.22mm" wi="152.65mm" orientation="landscape" file="US08625859-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="238.08mm" wi="158.58mm" orientation="landscape" file="US08625859-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="243.67mm" wi="164.25mm" orientation="landscape" file="US08625859-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="202.78mm" wi="163.75mm" orientation="landscape" file="US08625859-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="200.74mm" wi="164.51mm" orientation="landscape" file="US08625859-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="103.63mm" wi="132.33mm" file="US08625859-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="174.75mm" wi="137.58mm" file="US08625859-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="241.64mm" wi="151.30mm" orientation="landscape" file="US08625859-20140107-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="193.46mm" wi="156.80mm" orientation="landscape" file="US08625859-20140107-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00016" num="00016">
<img id="EMI-D00016" he="170.18mm" wi="124.97mm" file="US08625859-20140107-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00017" num="00017">
<img id="EMI-D00017" he="138.60mm" wi="159.17mm" file="US08625859-20140107-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00018" num="00018">
<img id="EMI-D00018" he="139.87mm" wi="132.84mm" file="US08625859-20140107-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00019" num="00019">
<img id="EMI-D00019" he="237.91mm" wi="123.95mm" orientation="landscape" file="US08625859-20140107-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00020" num="00020">
<img id="EMI-D00020" he="235.29mm" wi="133.77mm" orientation="landscape" file="US08625859-20140107-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00021" num="00021">
<img id="EMI-D00021" he="139.70mm" wi="178.99mm" file="US08625859-20140107-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00022" num="00022">
<img id="EMI-D00022" he="216.66mm" wi="152.74mm" orientation="landscape" file="US08625859-20140107-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00023" num="00023">
<img id="EMI-D00023" he="221.32mm" wi="157.82mm" orientation="landscape" file="US08625859-20140107-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00024" num="00024">
<img id="EMI-D00024" he="236.39mm" wi="138.35mm" orientation="landscape" file="US08625859-20140107-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00025" num="00025">
<img id="EMI-D00025" he="189.82mm" wi="149.52mm" orientation="landscape" file="US08625859-20140107-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application claims the benefit of priority under 35 U.S.C. &#xa7;119 from Japanese Priority Patent Application JP 2009-242771 filed in the Japan Patent Office on Oct. 21, 2009, the entire contents of which is hereby incorporated by reference.</p>
<heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">1. Field of the Invention</p>
<p id="p-0004" num="0003">The present invention relates to an information processing apparatus, an information processing method, and a program, and more particularly, to an information processing apparatus, an information processing method, and a program capable of creating a transformed image on which the feature of a face image is perceived without an operation of a user.</p>
<p id="p-0005" num="0004">2. Description of the Related Art</p>
<p id="p-0006" num="0005">In recent years, with development of a face image recognition technique, there has been actualized a technique for creating a similar face picture image from a face image contained in a captured image such as a photograph.</p>
<p id="p-0007" num="0006">As a method of creating this similar face picture image, there is generally used a method of creating a similar face picture image in accordance with ways using a positional relationship, sizes, ratios, and the like of face part regions extracted from contour images of parts such as eyes, eyebrows, a nose, and a mouth obtained by executing an edge extraction process or the like on a face image in a captured image.</p>
<heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0008" num="0007">In the techniques according to a related art, however, the transformed part images for similar face picture images are selected in accordance with the geometric sizes or ratios of part images organizing a face image. Therefore, the features of the face image may not be completely perceived, and a user has to execute an operation when a similar face picture image is created. For this reason, there is a necessity of a technique capable of generating a similar face picture image on which the features of a face image are perceived without an operation of a user.</p>
<p id="p-0009" num="0008">It is desirable to provide a technique capable of creating a transformed image, such as a similar face picture image, on which the features of a face image are perceived without an operation of a user.</p>
<p id="p-0010" num="0009">Accordingly, in an exemplary embodiment, the present invention is directed to an information processing apparatus, system, method and/or computer-readable medium that acquires a target image; extracts a face region including a face part from the target image; identifies a model face part by comparing the face part to a plurality of model face parts stored in a storage unit; and determines an illustration image corresponding to the identified model face part.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 1</figref> is a diagram illustrating the configuration of a similar face picture image generation device according to an embodiment of the invention;</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 2</figref> is a flowchart illustrating a similar face picture image generation process;</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIGS. 3A to 3H</figref> are diagrams illustrating the overall routine of the similar face picture image generation process;</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIGS. 4A to 4C</figref> are diagrams illustrating the details of a hair region extraction process;</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIGS. 5A to 5D</figref> are diagrams illustrating the details of a drawing process;</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 6</figref> is a flowchart illustrating the details of an illustration image selection process;</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 7</figref> is a diagram illustrating an example of part region definition information;</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 8</figref> is a diagram illustrating the details of the illustration image selection process;</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIGS. 9A and 9B</figref> are diagrams illustrating the details of an illustration image drawing process;</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIGS. 10A to 10F</figref> are diagrams illustrating examples of a similar face picture image;</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 11</figref> is a diagram illustrating an exemplary configuration of a preliminary process device;</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 12</figref> is a flowchart illustrating the preliminary process;</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 13</figref> is a flowchart illustrating a learning process;</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 14</figref> is a diagram illustrating the details of a K-class determination unit;</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 15</figref> is a diagram illustrating the details of calculation order of a K-dimensional score vector;</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 16</figref> is a flowchart illustrating a generation process;</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 17</figref> is a diagram illustrating examples of eye model images;</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 18</figref> is a flowchart illustrating a setting process;</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIGS. 19A to 19L</figref> are diagrams illustrating examples of eye illustration images;</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 20</figref> is a diagram illustrating an example where the eye model images are matched to the eye illustration images;</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIGS. 21A and 21B</figref> are diagrams illustrating examples where the illustration image is expanded for drawing;</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 22</figref> is a diagram illustrating an example where eyebrow model images are matched to eyebrow illustration images;</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 23</figref> is a diagram illustrating an example where nose model images are matched to nose illustration images;</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 24</figref> is a diagram illustrating an example where mouth model images are matched to mouth illustration images; and</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 25</figref> is a diagram illustrating the configuration of a computer.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading>
<p id="p-0036" num="0035">Hereinafter, an embodiment of the invention will be described with reference to the drawings.</p>
<p id="h-0006" num="0000">Exemplary Configuration of Similar Face Picture Image Generating Device</p>
<p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. 1</figref> is a diagram illustrating the configuration of a similar face picture image generation device according to an embodiment of the invention.</p>
<p id="p-0038" num="0037">A similar face picture image generation device <b>1</b> shown in <figref idref="DRAWINGS">FIG. 1</figref> generates a transformed image such as a similar face picture image from a target image containing a face image and displays the transformed image on a display device <b>2</b> such as an LCD (Liquid Crystal Display).</p>
<p id="p-0039" num="0038">As shown in <figref idref="DRAWINGS">FIG. 1</figref>, the similar face picture image generation device <b>1</b> includes a target image acquisition unit <b>11</b>, a face detection unit <b>12</b>, a feature point detection unit <b>13</b>, an attribute determination unit <b>14</b>, a contour region extraction unit <b>15</b>, a hair region extraction unit <b>16</b>, a drawing generation unit <b>17</b>, and an illustration image selection process unit <b>18</b>.</p>
<p id="p-0040" num="0039">The target image acquisition unit <b>11</b> acquires a target image and supplies the target image to the face detection unit <b>12</b> and the contour region extraction unit <b>15</b>.</p>
<p id="p-0041" num="0040">For example, when the target image acquisition unit <b>11</b> is a camera including an image device such as a lens or a CCD (Charge Coupled Devices), the target image is a captured image captured by the camera. Alternatively, the target image may be an image acquired by reading image data recorded in a record medium such as a memory card or an image acquired from a device connected to a network such as the Internet via the network. That is, the target image may be acquired by any method, as long as the target image contains a face image.</p>
<p id="p-0042" num="0041">The face detection unit <b>12</b> executes a predetermined image process on the target image supplied from the target image acquisition unit <b>11</b> to detect the face region. Then, the face detection unit <b>12</b> supplies information regarding a face image of the face region obtained by the predetermined image process to the feature point detection unit <b>13</b>, the contour region extraction unit <b>15</b>, the hair region extraction unit <b>16</b>, and the illustration image selection process unit <b>18</b>.</p>
<p id="p-0043" num="0042">The feature point detection unit <b>13</b> executes a predetermined image process on the face images of the face regions supplied from the face detection unit <b>12</b> to detect feature points specifying the contours of parts such as eyes, eyebrows, a nose, and a mouth. Then, the feature point detection unit <b>13</b> supplies information regarding the feature point obtained by the predetermined image process to the attribute determination unit <b>14</b> and the contour region extraction unit <b>15</b>.</p>
<p id="p-0044" num="0043">The attribute determination unit <b>14</b> executes a predetermined image process on a face image pattern obtained by positioning the face region in accordance with the feature point supplied from the feature point detection unit <b>13</b> to determine a predetermined attribute and determines whether glasses are contained in the face image in the face region, for example. The determination result of the attribute is supplied to the drawing generation unit <b>17</b>.</p>
<p id="p-0045" num="0044">The target image from the target image acquisition unit <b>11</b>, information regarding the face image of the face region from the face detection unit <b>12</b>, and information regarding the feature point from the feature point detection unit <b>13</b> are supplied to the contour region extraction unit <b>15</b>. The contour region extraction unit <b>15</b> executes a re-size process of cutting the region, which is formed by enlarging mainly the face region only at a predetermined magnification ratio, from the target image and matching the cut region to the image size of the similar face picture image to be generated.</p>
<p id="p-0046" num="0045">When the re-size process is executed, the contour region extraction unit <b>15</b> executes a process of converting the position (x, y) of the feature point detected by the feature point detection unit <b>13</b> into a position (x, y) in the cut region. The information regarding the contour calculated by the contour region extraction unit <b>15</b> is supplied to the drawing generation unit <b>17</b>.</p>
<p id="p-0047" num="0046">The hair region extraction unit <b>16</b> executes a predetermined image process on the face image (or the target image) in the face region supplied from the face detection unit <b>12</b> to extract the hair region. Then, the hair region extraction unit <b>16</b> supplies information regarding the hair region obtained in this manner to the drawing generation unit <b>17</b>.</p>
<p id="p-0048" num="0047">The illustration image selection process unit <b>18</b> executes a process of selecting an illustration image corresponding to a predetermined part such as eyes, eyebrows, a nose, and a mouth in the face region.</p>
<p id="p-0049" num="0048">The illustration image selection process unit <b>18</b> includes a part region extraction unit <b>31</b>, a K-class determination unit <b>32</b>, a similarity determination unit <b>33</b>, an illustration image selection unit <b>34</b>, and a database <b>35</b>.</p>
<p id="p-0050" num="0049">The part region extraction unit <b>31</b> extracts a part region containing a predetermined part from the face image in the face region supplied from the face detection unit <b>12</b> on the basis of part region definition information. Then, the part region extraction unit <b>31</b> supplies the extracted part region to the K-class determination unit <b>32</b>. The part region definition information is registered in advance in the database <b>35</b>, for example. Therefore, the part region definition information is acquired from the database <b>35</b> by the part region extraction unit <b>31</b>.</p>
<p id="p-0051" num="0050">The K-class determination unit <b>32</b> requests a K-dimensional score (hereinafter, also referred to as a K-dimensional score vector) regarding the part image in the part region supplied from the part region extraction unit <b>31</b>. Then, the K-class determination unit <b>32</b> supplies the calculated K-dimensional score vector to the similarity determination unit <b>33</b>.</p>
<p id="p-0052" num="0051">The K-class determination unit <b>32</b> is prepared for each part and is calculated by the preliminary process device <b>101</b> shown in <figref idref="DRAWINGS">FIG. 11</figref>, which is described below. The detailed description is made below.</p>
<p id="p-0053" num="0052">The similarity determination unit <b>33</b> specifies the K-dimensional score vector which is the most similar to the K-dimensional score vector supplied from the K-class determination unit <b>32</b> among the K-dimensional score vectors of model images corresponding to part matching information registered in the database <b>35</b>. The similarity determination unit <b>33</b> determines the most similar model image and supplies the determination result to the illustration image selection unit <b>34</b>.</p>
<p id="p-0054" num="0053">The part matching information is information matching the illustrating image to the model image and is registered in the database <b>35</b>. The illustration image is an example of the transformed part image formed by transforming and displaying each part and is an image formed by the drawings (illustrations) of drawing each part. The model image is an image generated by classifying part images of input images, which are generated by AdaBoostECOC (Error Correct Output Coding) learning using the image feature amounts of plural sample images and are output from a multi-class determiner (the K-class determination unit <b>32</b> shown in <figref idref="DRAWINGS">FIG. 11</figref>, which is described below), into plural prototypes on the basis of multi-dimensional score vectors for the input images and by calculating an average image of a part image group belonging to each prototype.</p>
<p id="p-0055" num="0054">The part matching information is calculated by the preliminary process device <b>101</b> shown in <figref idref="DRAWINGS">FIG. 11</figref>. The detailed description is made below.</p>
<p id="p-0056" num="0055">The illustration image selection unit <b>34</b> selects an illustration image matched to the model image determined to be the most similar among the part matching information registered in the database <b>35</b> on the basis of the determination result supplied from the similarity determination unit <b>33</b>. Then, the illustration image selection unit <b>34</b> supplies the selected illustration image to the drawing generation unit <b>17</b>.</p>
<p id="p-0057" num="0056">The database <b>35</b> stores various kinds of information necessary for creating the similar face picture image such as anchor point definition information, as well as the part region definition information and the part matching information described above. The anchor point definition information defines a point (hereinafter, also referred to as an anchor point) on the illustration image matched to the position (hereinafter, also referred to as a part point) of each part detected from the face image contained in the target image. The anchor point definition information is calculated by the preliminary process device <b>101</b> shown in <figref idref="DRAWINGS">FIG. 11</figref>. The detailed description is made below.</p>
<p id="p-0058" num="0057">The determination result from the attribute determination unit <b>14</b>, the information regarding the contour from the contour region extraction unit <b>15</b>, the information regarding the hair region from the hair region extraction unit <b>16</b>, the illustration image from the illustration image selection unit <b>34</b>, and the anchor point definition information from the database <b>35</b> are supplied to the drawing generation unit <b>17</b>.</p>
<p id="p-0059" num="0058">The drawing generation unit <b>17</b> paints the entire region of the sum sets of the contour region and the hair region in a predetermined skin color on the basis of the information regarding the contours and the hair region and paints the entire hair region in a predetermined hair color to draw an image (hereinafter, also referred to as a base image) formed only by the skin region and the hair region. Subsequently, the drawing generation unit <b>17</b> executes a predetermined image process such as rotation or scaling (re-sizing) on the illustration image so that the anchor point defined on the basis of the anchor point definition information is matched to the part point. Then, the drawing generation unit <b>17</b> draws and disposes the illustration obtained in this manner to the base image to generate the similar face picture image.</p>
<p id="p-0060" num="0059">The drawing generation unit <b>17</b> further disposes a glasses image on the basis of the anchor point definition information, when the determination result from the attribute determination unit <b>14</b> indicates glasses.</p>
<p id="p-0061" num="0060">The similar face picture image generated in this manner is displayed on the display device <b>2</b> by the drawing generation unit <b>17</b>.</p>
<p id="p-0062" num="0061">The similar face picture image generation device <b>1</b> has the above-described configuration.</p>
<p id="h-0007" num="0000">Description of Similar Face Picture Image Generation Process</p>
<p id="p-0063" num="0062">Next, a process (similar face picture image generation process) of generating the similar face picture image, which is executed by the similar face picture image generation device <b>1</b> shown in <figref idref="DRAWINGS">FIG. 1</figref>, will be described with reference to the flowchart of <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0064" num="0063">In the description made with reference to <figref idref="DRAWINGS">FIG. 2</figref>, the overall routine of <figref idref="DRAWINGS">FIG. 3</figref> is frequently referred to facilitate understanding of the process executed in each step.</p>
<p id="p-0065" num="0064">In step S<b>11</b>, the target image acquisition unit <b>11</b> acquires the target image containing the face image such as a captured image. For example, the target image containing the face image shown in <figref idref="DRAWINGS">FIG. 3A</figref> is input to the target image acquisition unit <b>11</b>.</p>
<p id="p-0066" num="0065">In step S<b>12</b>, the face detection unit <b>12</b> detects the position (x, y, w, h) of the face region in the target image by scanning a face pattern of the target image supplied from the target image acquisition unit <b>11</b>. For example, the face detection unit <b>12</b> detects a region, which is indicated by a rectangle overlapping with a face of <figref idref="DRAWINGS">FIG. 3B</figref>, as the face region.</p>
<p id="p-0067" num="0066">In step S<b>13</b>, the feature point detection unit <b>13</b> detects the position (x, y) of the feature point by scanning the pattern of each feature point of the face image in the face region detected by the face detection unit <b>12</b>. The part and contour are specified by the feature point. For example, the feature point detection unit <b>13</b> detects the feature point indicated by the point overlapping on the face of <figref idref="DRAWINGS">FIG. 3C</figref>.</p>
<p id="p-0068" num="0067">In step S<b>14</b>, the attribute determination unit <b>14</b> determines the attribute of the face image pattern obtained by positioning the face region in accordance with the feature point of eyes or the like detected by the feature point detection unit <b>13</b>. Then, the attribute determination unit <b>14</b> determines whether the glasses exist. For example, as shown in <figref idref="DRAWINGS">FIG. 3D</figref>, the attribute determination unit <b>14</b> determines the glasses attribute from the face image pattern. In this case, the attribute determination unit <b>14</b> determines that the glasses exist.</p>
<p id="p-0069" num="0068">In step S<b>15</b>, the contour region extraction unit <b>15</b> executes the re-size process of cutting the region, which is formed by enlarging mainly the face region detected by the face detection unit <b>12</b>, from the target image and matching the cut region to the image size of the similar face picture image to be generated. As for the re-size, the region is extracted to the degree that the entire face slightly protrudes in the face region detected by the face detection unit <b>12</b>. Therefore, for example, the region which is about 1.6 times the face region is cut from the target image.</p>
<p id="p-0070" num="0069">At this time, for example, when it is assumed that the image size of the similar face picture image desired to be calculated is 300 by 300 pixels, the face image in the detected face region is re-sized to this image size. At this time, the contour region extraction unit <b>15</b> transforms the position of the feature point of the face region detected by the feature point detection unit <b>13</b> to the position of the cut image. Then, the contour region extraction unit <b>15</b> supplies the transformed position of the feature point to the drawing generation unit <b>17</b>.</p>
<p id="p-0071" num="0070">In step S<b>16</b>, the hair region extraction unit <b>16</b> clusters RGB values, which are obtained from the image of a region (region of the top of a head) of the upper half of the face image in the face region (or the target image) detected by the face detection unit <b>12</b>, using a k-means algorithm, for example. Then, the hair region extraction unit <b>16</b> extracts a predetermined region as the hair region.</p>
<p id="p-0072" num="0071"><figref idref="DRAWINGS">FIGS. 4A to 4C</figref> are diagrams illustrating the details of a hair region extraction process executed by the hair region extraction unit <b>16</b>.</p>
<p id="p-0073" num="0072">In the hair region extraction process, as shown in <figref idref="DRAWINGS">FIG. 4A</figref>, an upper face region A<sub>U </sub>corresponding to the upper half of the face region is first extracted. When all pixels (r, g, b) of the upper face region A<sub>U </sub>are clustered into three colors using the k-means algorithm, all pixels belong to any one of three classes. Subsequently, when it is determined whether the labels of pixels adjacent to each other are identical to each other, the upper face region is divided into three regions of background, hair, and skin regions, as shown <figref idref="DRAWINGS">FIG. 4B</figref>.</p>
<p id="p-0074" num="0073">In this division process, the upper face region may be divided into three or more regions. However, as shown in <figref idref="DRAWINGS">FIG. 4C</figref>, the hair region extraction unit <b>16</b> may extract the region with the largest area as the hair region from the region closest to black, for example.</p>
<p id="p-0075" num="0074">Returning to the flowchart of <figref idref="DRAWINGS">FIG. 2</figref>, in step S<b>17</b>, the drawing generation unit <b>17</b> paints the entire region of the sum set of the contour region and the hair region in a predetermined skin color and paints the entire hair region in a predetermined hair color to draw a face base image. As shown in <figref idref="DRAWINGS">FIG. 3E</figref>, the extracted contour region is first drawn. Thereafter, as shown in <figref idref="DRAWINGS">FIG. 3F</figref>, the base image is drawn by drawing the extracted hair region.</p>
<p id="p-0076" num="0075"><figref idref="DRAWINGS">FIGS. 5A to 5D</figref> are diagrams illustrating the details of a base image drawing process executed by the drawing generation unit <b>17</b>.</p>
<p id="p-0077" num="0076">As shown in <figref idref="DRAWINGS">FIG. 5A</figref>, the feature points of the detected contour can be connected to each other by a predetermined interpolation process such as spline curve interpolation. When the region surrounded by a curve is calculated, the contour region shown in <figref idref="DRAWINGS">FIG. 5B</figref> is obtained. As shown in <figref idref="DRAWINGS">FIG. 5C</figref>, the entire contour region and the inside of the hair region extracted by the hair region extraction unit <b>16</b> are painted in the skin color, and then only the hair region is painted in black (hair color). In this way, the base image shown in <figref idref="DRAWINGS">FIG. 5D</figref> can be obtained.</p>
<p id="p-0078" num="0077">Returning to the flowchart of <figref idref="DRAWINGS">FIG. 2</figref>, in step S<b>18</b>, the illustration image selection process unit <b>18</b> executes an illustration image selection process. The illustration image selection process is a process corresponding to selection of the illustration image shown in <figref idref="DRAWINGS">FIG. 3G</figref>.</p>
<p id="p-0079" num="0078">Hereinafter, the details of the illustration image selection process of step S<b>18</b> shown in <figref idref="DRAWINGS">FIG. 2</figref> will be described with reference to the flowchart of <figref idref="DRAWINGS">FIG. 6</figref>. Here, to facilitate the description, an example will mainly be described in which the illustration image of eyes is selected among parts of eyes, eyebrows, a nose, and a mouth.</p>
<p id="p-0080" num="0079">In step S<b>31</b>, the illustration image selection process unit <b>18</b> re-sizes the image size of the face region of the target image to a predetermined image size. Then, the part region extraction unit <b>31</b> cuts out the part region from the face region re-sized to the predetermined image size in accordance with the part region definition information.</p>
<p id="p-0081" num="0080"><figref idref="DRAWINGS">FIG. 7</figref> is a diagram illustrating an example of the part region definition information.</p>
<p id="p-0082" num="0081">As shown in <figref idref="DRAWINGS">FIG. 7</figref>, when respective part regions are cut from the face region re-sized to the size of 64 by 64 pixels, a rectangular region where an arbitrary point (x<sub>1</sub>, y<sub>1</sub>) and an arbitrary point (x<sub>2</sub>, y<sub>2</sub>) in the face region are diagonal corners is extracted. For example, since an eye part region is a rectangular region where the point (<b>10</b>, <b>23</b>) and the point (<b>55</b>, <b>30</b>) are diagonal corners, the part region extraction unit <b>31</b> cuts and extracts this region as the eye part region.</p>
<p id="p-0083" num="0082">Returning to the flowchart of <figref idref="DRAWINGS">FIG. 6</figref>, in step S<b>33</b>, the K-class determination unit <b>32</b> for the eye part region calculates a K-dimensional score vector corresponding to the part image in the eye part region.</p>
<p id="p-0084" num="0083">In step S<b>34</b>, the similarity determination unit <b>33</b> specifies the K-dimensional score vector, which is the most similar to the K-dimensional score vector obtained from the part image in the eye part region calculated by the K-class determination unit <b>32</b> for the eye part region among the K-dimensional score vectors of the model images matched to the illustration images on the basis of the part matching information registered in the database <b>35</b>. Then, the similarity determination unit <b>33</b> determines the most similar model image. Moreover, in the similarity between the K-dimensional score vectors, Euclidean distance is used.</p>
<p id="p-0085" num="0084">In step S<b>35</b>, the illustration image selection unit <b>34</b> selects the illustration image matched to the model image determined to be the most similar from the part matching information registered in the database <b>35</b> on the basis of the determination result of the similarity determination unit <b>33</b>.</p>
<p id="p-0086" num="0085"><figref idref="DRAWINGS">FIG. 8</figref> is a diagram illustrating the details of the illustration image selection process.</p>
<p id="p-0087" num="0086">As shown in <figref idref="DRAWINGS">FIG. 8</figref>, when the part region extraction unit <b>31</b> extracts the part image in the eye part region from the face region of the target image, the similarity determination unit <b>33</b> determines the similarity between the part image and the plural model images on the basis of the calculation result of the K-class determination unit <b>32</b> and determines the model image with the highest similarity. Since the model images and the illustration images are matched to each other in advance in the part matching information, the illustration image selection unit <b>34</b> can select one illustration image matched to the model image with the highest similarity.</p>
<p id="p-0088" num="0087">In this way, one illustration image matched to the model image which is the most similar to the part image in the eye part region extracted from the face region is selected from the prepared plural illustration images in advance. Then, the process returns to step S<b>18</b> of <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0089" num="0088">In step S<b>19</b>, the drawing generation unit <b>17</b> executes an image process, such as rotation or scaling, on the illustration image selected by the illustration image selection unit <b>34</b> so that the anchor points defined by the anchor point definition information are identical to the corresponding part points. Then, the drawing generation unit <b>17</b> draws the illustration image obtained in this manner on the base image obtained in step S<b>17</b>.</p>
<p id="p-0090" num="0089"><figref idref="DRAWINGS">FIGS. 9A and 9B</figref> are diagrams illustrating the details of an illustration image drawing process.</p>
<p id="p-0091" num="0090">As shown in <figref idref="DRAWINGS">FIGS. 9A and 9B</figref>, anchor points P<sub>A </sub>of the eye illustration image selected by the illustration image selection unit <b>34</b> can be matched to part points Pp on the similar face picture image (base image) corresponding to the feature points of the face region detected by the feature point detection unit <b>13</b>, as described above.</p>
<p id="p-0092" num="0091">That is, as shown in <figref idref="DRAWINGS">FIG. 9A</figref>, since one anchor point P<sub>A1 </sub>is matched to the part point P<sub>P1 </sub>and the other anchor point P<sub>A2 </sub>is matched to the part point P<sub>P2</sub>, the drawing generation unit <b>17</b> executes the image process such as rotation or scaling on the illustration image so that the eye illustration image is drawn on the base image by matching these points. Then, the drawing generation unit <b>17</b> executes drawings so that the anchor points P<sub>A </sub>are matched to the part points P<sub>P</sub>. In this way, when the drawing is executed, as shown in <figref idref="DRAWINGS">FIG. 9B</figref>, a right eye illustration image is drawn on the base image.</p>
<p id="p-0093" num="0092">At this time, since the skin region is drawn beforehand as the base image, only the pixels corresponding to the illustration image are overwritten. The part points P<sub>P </sub>may be points matching the detected feature points or may be points set separately from the feature points.</p>
<p id="p-0094" num="0093">Returning to the flowchart of <figref idref="DRAWINGS">FIG. 2</figref>, in step S<b>20</b>, the illustration image selection process unit <b>18</b> determines whether all illustration images such as eyes, eyebrows, a nose, a mouth, and a forehead are drawn on the base image.</p>
<p id="p-0095" num="0094">When it is determined all illustration images are not drawn in step S<b>20</b>, the process returns to step S<b>18</b>, and then the illustration image selection drawing process from step S<b>18</b> to S<b>20</b>, which are described above, is repeated.</p>
<p id="p-0096" num="0095">That is, by repeating the illustration image selection drawing process, an eyebrow part region (point (<b>8</b>, <b>15</b>)-point (<b>57</b>, <b>22</b>)), a nose part region (point (<b>21</b>, <b>31</b>)-point (<b>44</b>, <b>45</b>)), a mouth part region (point (<b>18</b>, <b>46</b>)-point (<b>47</b>, <b>59</b>)), and a forehead part region (point (<b>1</b>, <b>1</b>)-point (<b>64</b>, <b>14</b>)) other than the eye part region defined in the part region definition information in <figref idref="DRAWINGS">FIG. 7</figref> are respectively extracted. Then, the similarity with the model images is determined using the K-class determination unit <b>32</b> prepared in each part region, and each illustration image matched to the model image with the highest similarity is selected. The selected illustration image is subjected to the image process on the basis of the anchor point definition information. Then, the illustration image is drawn and disposed at a predetermined position on the base image.</p>
<p id="p-0097" num="0096">Alternatively, when all illustration images are completely drawn, the process proceeds to step S<b>21</b>. In step S<b>21</b>, the drawing generation unit <b>17</b> draws an image of a frame of the glasses at a predetermined position on the base image on the basis of the anchor point definition information, when the determination result of the glasses attribute from the attribute determination unit <b>14</b> represents that the glasses exist. In this way, the similar face picture image shown in <figref idref="DRAWINGS">FIG. 3H</figref> is generated.</p>
<p id="p-0098" num="0097">The similar face picture image generated in this manner is exemplified as follows. That is, <figref idref="DRAWINGS">FIGS. 10A to 10F</figref> are diagrams illustrating examples where the similar face picture image (transformed image) generated by the drawing generation unit <b>17</b> is displayed on the display device <b>2</b>.</p>
<p id="p-0099" num="0098">As indicated by combinations of the target images (left side) and the similar face picture images (right side) shown in <figref idref="DRAWINGS">FIGS. 10A to 10F</figref>, each illustration image matched to each part region of the face image contained in the target image on the left side is selected separately by executing the above-described similar face picture image generation process. The selected illustration image is drawn on the base image. Then, each similar face picture image on the right side is generated.</p>
<p id="p-0100" num="0099">That is, the similar face picture image is generated from the illustration image selected not on the basis of the similarity between the part image in each part region and the illustration image but on the basis of the similarity between the part image and the model image. Therefore, the illustration image matched to the model image which is the most similar to each part image of the target image can be selected directly from the appearance of the face image and thus the similar face picture image on which the features of the face image is perceived can be generated. That is, it is difficult to directly calculate the similarity between the part image and the illustration image. In this embodiment, however, the illustration image is matched in advance to the model image, the similarity between the part image and the model image is calculated, and the illustration image matched to the model image which is the most similar to the part image is selected.</p>
<p id="p-0101" num="0100">In this way, the similar face picture image generation process is executed.</p>
<p id="h-0008" num="0000">Exemplary Configuration of Preliminary Process Device</p>
<p id="p-0102" num="0101">Next, a learning process of generating the K-class determination unit <b>32</b>, a generation process of generating the model image, and a setting process of setting various kinds of information such as the part matching information and the anchor point definition information will be described which are executed as preliminary processes necessary for executing the similar face picture image generation process by the similar face picture image generation device <b>1</b> in <figref idref="DRAWINGS">FIG. 1</figref>. The preliminary processes are executed by the preliminary process device.</p>
<p id="p-0103" num="0102"><figref idref="DRAWINGS">FIG. 11</figref> is a diagram illustrating an exemplary configuration of the preliminary process device.</p>
<p id="p-0104" num="0103">As shown in <figref idref="DRAWINGS">FIG. 11</figref>, the preliminary process device <b>101</b> includes a learning system <b>111</b> executing a learning process, a generation system <b>112</b> executing a generation process, and a setting system <b>113</b> executing a setting process.</p>
<p id="p-0105" num="0104">A learning sample acquisition unit <b>121</b>, a preliminary image process unit <b>122</b>, a learning unit <b>123</b>, and a K-class determination unit <b>32</b> belong to the learning system <b>111</b>. A generation image acquisition unit <b>124</b>, a preliminary image process unit <b>125</b>, the K-class determination unit <b>32</b>, a generation unit <b>126</b>, and the database <b>35</b> belong to the generation system <b>112</b>. An illustration image acquisition unit <b>127</b>, a setting unit <b>128</b>, and the database <b>35</b> belong to the setting system <b>113</b>.</p>
<p id="p-0106" num="0105">In <figref idref="DRAWINGS">FIG. 11</figref>, the same reference numerals are given to the elements corresponding to the elements of <figref idref="DRAWINGS">FIG. 1</figref>, and the description is appropriately omitted. That is, in <figref idref="DRAWINGS">FIG. 11</figref>, the same K-class determination device <b>32</b> and the same database <b>35</b> shown in <figref idref="DRAWINGS">FIG. 1</figref> are described. However, in the preliminary process device <b>101</b>, the K-class determination unit <b>32</b> forming the similar face picture image generation device <b>1</b> is generated and various kinds of information are set to execute a process of registering the K-class determination unit <b>32</b> and the various kinds of information in the database <b>35</b>.</p>
<p id="p-0107" num="0106">First, the learning system <b>111</b> will be described.</p>
<p id="p-0108" num="0107">The learning sample acquisition unit <b>121</b> acquires a face image (hereinafter, also referred to as a sample image) prepared in various variations for K (where K=1, 2, . . . , K) sample persons and supplies the acquired face image to the preliminary image process unit <b>122</b>.</p>
<p id="p-0109" num="0108">The preliminary image process unit <b>122</b> executes a process of detecting the face region from the sample image supplied from the learning sample acquisition unit <b>121</b> and re-sizing the face region to a predetermined image size. In the re-size process, the face region is re-sized to the image size defined in the above-described part region definition information (see <figref idref="DRAWINGS">FIG. 7</figref>). The preliminary image process unit <b>122</b> cuts out the part region on the basis of the part region definition information from the re-sized face region and supplies the part region to the learning unit <b>123</b>.</p>
<p id="p-0110" num="0109">The learning unit <b>123</b> calculates the image feature amount of the part image in the part region supplied from the preliminary image process unit <b>122</b>, learns plural weak determiners using AdaBoostECOC (Error Correct Output Coding), and generates the K-class determination unit <b>32</b> formed by the plural weak determiners.</p>
<p id="p-0111" num="0110">Next, the generation system <b>112</b> will be described.</p>
<p id="p-0112" num="0111">The generation image acquisition unit <b>124</b> acquires several face images (hereinafter, also referred to as generation images) extracted at random to generate the model images and supplies the face images to the preliminary image process unit <b>125</b>.</p>
<p id="p-0113" num="0112">The preliminary image process unit <b>125</b> re-sizes the generation image from the generation image acquisition unit <b>124</b>, like the re-size process of the sample image by the preliminary image process unit <b>122</b>, cuts the part regions, and supplies the cut regions to the K-class determination unit <b>32</b>.</p>
<p id="p-0114" num="0113">The K-class determination unit <b>32</b> calculates the K-dimensional score vector corresponding to the part image in the part region supplied from the preliminary image process unit <b>125</b> and supplies the K-dimensional score vector to the generation unit <b>126</b>.</p>
<p id="p-0115" num="0114">The generation unit <b>126</b> classifies the part images into N (where N=1, 2, . . . , N) prototypes on the basis of the K-dimensional score vector supplied from the K-class determination unit <b>32</b> and generates the model image by calculating the average image of a part image group belonging to each prototype. The model image is registered in the database <b>35</b>.</p>
<p id="p-0116" num="0115">Next, the setting system <b>113</b> will be described.</p>
<p id="p-0117" num="0116">The illustration image acquisition unit <b>127</b> acquires the illustration image of each part and supplies the illustration image to the setting unit <b>128</b>.</p>
<p id="p-0118" num="0117">The setting unit <b>128</b> sets the part matching information by matching the illustration images supplied from the illustration image acquisition unit <b>127</b> to the model images registered in the database <b>35</b>. Then, the setting unit <b>128</b> registers the part matching information in the database <b>35</b>. In addition, the setting unit <b>128</b> sets the anchor point definition information and registers the anchor point definition information in the database <b>35</b>.</p>
<p id="p-0119" num="0118">The preliminary process device <b>101</b> has the above-described configuration.</p>
<p id="h-0009" num="0000">Description of Preliminary Process</p>
<p id="p-0120" num="0119">Next, the preliminary process executed by the preliminary process device <b>101</b> in <figref idref="DRAWINGS">FIG. 11</figref> will be described with reference to the flowchart of <figref idref="DRAWINGS">FIG. 12</figref>.</p>
<p id="p-0121" num="0120">In step S<b>51</b>, the learning system <b>111</b> of the preliminary process device <b>101</b> executes the learning process to generate the K-class determination unit <b>32</b>.</p>
<p id="p-0122" num="0121">In step S<b>52</b>, the generation system <b>112</b> of the preliminary process device <b>101</b> generates the model images in response to the execution of the generation process and registers the model images in the database <b>35</b>.</p>
<p id="p-0123" num="0122">In step S<b>53</b>, the setting system <b>113</b> of the preliminary process device <b>101</b> sets the part matching information and the anchor point definition information by executing the setting process and registers the result in the database <b>35</b>.</p>
<p id="p-0124" num="0123">Hereinafter, the above-described process from steps S<b>51</b> to S<b>53</b> of <figref idref="DRAWINGS">FIG. 12</figref> will be described in detail.</p>
<p id="h-0010" num="0000">Details of Learning Process</p>
<p id="p-0125" num="0124"><figref idref="DRAWINGS">FIG. 13</figref> is a flowchart illustrating the learning process in detail.</p>
<p id="p-0126" num="0125">In step S<b>71</b>, the learning sample acquisition unit <b>121</b> acquires the sample images prepared in various variations for K sample persons.</p>
<p id="p-0127" num="0126">The preliminary image process unit <b>122</b> detects the face region from the sample image acquired by the learning sample acquisition unit <b>121</b> in step S<b>72</b>, re-sizes the detected face region to a predetermined image size (process of step S<b>73</b>) on the basis of the part region definition information, and then cuts the part region from the re-sized face region (process of step S<b>74</b>).</p>
<p id="p-0128" num="0127">In this re-size process, the face region is re-sized to the size of 64 by 64 pixels, for example, in accordance with the image size defined in the part region definition information shown in <figref idref="DRAWINGS">FIG. 7</figref>. For example, the eye part regions (point (<b>10</b>, <b>23</b>)-point (<b>55</b>, <b>30</b>)) are cut.</p>
<p id="p-0129" num="0128">In the learning unit <b>123</b>, the image feature amount of the part image is calculated in step S<b>75</b>, and then the plural weak determiners are learned in accordance with AdaBoostECOC in step S<b>76</b> to generate the K-class determination unit <b>32</b>.</p>
<p id="p-0130" num="0129">In the plural weak determiners determining whether to belong to each class, the image feature amount of the sample image (part image) is used as determination reference. As the image feature amount, for example, the pixel difference feature (PixDif Feature) suggested by the inventor may be used.</p>
<p id="p-0131" num="0130">The pixel difference feature is disclosed in &#x201c;Kohtaro Sabe and Kenichi Idai, &#x201c;Learning Real-Time Arbitrary Position Face Detector Using Pixel Difference Feature&#x201d;, Proceedings of the 10th Symposium on Sensing, pp. 547 to 552, 2004&#x201d; and Japanese Unexamined Patent Application Publication No. 2005-157679.</p>
<p id="p-0132" num="0131"><figref idref="DRAWINGS">FIG. 14</figref> is a diagram illustrating the details of the K-class determination unit <b>32</b> generated by the learning unit <b>123</b>.</p>
<p id="p-0133" num="0132">As shown in <figref idref="DRAWINGS">FIG. 14</figref>, the sample images of K persons, such as &#x201c;person A&#x201d;, &#x201c;person B&#x201d; and &#x201c;person C&#x201d;, are acquired in various variations and the face regions of the sample images are re-sized. Thereafter, respective part regions are cut from the re-sized face regions, as described in the process from steps S<b>71</b> to S<b>74</b> of <figref idref="DRAWINGS">FIG. 13</figref>.</p>
<p id="p-0134" num="0133">In the learning unit <b>123</b>, the image feature amounts of the respective part images of the face images of the K sample persons are calculated in accordance with the pixel difference feature, the plural weak determiners are learned in accordance with AdaBoostECOC using the image feature amounts, and the K-class determination unit <b>32</b> is generated as the multi-class determiner.</p>
<p id="p-0135" num="0134">The K-class determination unit <b>32</b> generated in this manner calculates a score expressing to what degree an input face image is similar to each of the K sample persons. Moreover, the input face image is more similar, as the value of the score is larger. Accordingly, as described above, the K-class determination unit <b>32</b> outputs the K-dimensional score vector as a K-dimensional score.</p>
<p id="p-0136" num="0135">In this way, K-dimensional score spaces are obtained in each part. For example, when an input part image belongs to the same person, a distance in the K-dimensional score spaces is close. When an input face image belongs to an unknown person &#x201c;X&#x201d;, each part of the person X&#x2033; can be expressed numerically to show to what degree each part is similar to each part of the sample persons &#x201c;A&#x201d;, &#x201c;B&#x201d;, &#x201c;C&#x201d;, and the like. Therefore, the similarity of each part can be determined in accordance with the distance in the K-dimensional score spaces of each part.</p>
<p id="p-0137" num="0136">More specifically, the process in <figref idref="DRAWINGS">FIG. 15</figref> is executed. That is, a difference (I<sub>1</sub>&#x2212;I<sub>2</sub>) between pixel values (luminance value) I<sub>1 </sub>and I<sub>2 </sub>of two pixels on the face image (Face Image A) is calculated to obtain the pixel difference feature (PixDif Feature). In a binary weak determiner h(x) corresponding to combination of two pixels, as indicated by Expression (1), true (+1) or false (&#x2212;1) is determined by the pixel difference feature (I<sub>1</sub>&#x2212;I<sub>2</sub>) and a threshold value Th.</p>
<p id="p-0138" num="0137">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mtable>
        <mtr>
          <mtd>
            <mrow>
              <mrow>
                <mi>h</mi>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>(</mo>
                  <mi>x</mi>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mo>=</mo>
              <mrow>
                <mo>-</mo>
                <mn>1</mn>
              </mrow>
            </mrow>
          </mtd>
          <mtd>
            <mrow>
              <mrow>
                <mrow>
                  <mi>if</mi>
                  <mo>&#x2062;</mo>
                  <mstyle>
                    <mspace width="0.8em" height="0.8ex"/>
                  </mstyle>
                  <mo>&#x2062;</mo>
                  <msub>
                    <mi>I</mi>
                    <mn>1</mn>
                  </msub>
                </mrow>
                <mo>-</mo>
                <msub>
                  <mi>I</mi>
                  <mn>2</mn>
                </msub>
              </mrow>
              <mo>&#x2264;</mo>
              <mi>Th</mi>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mrow>
              <mrow>
                <mi>h</mi>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>(</mo>
                  <mi>x</mi>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mo>=</mo>
              <mrow>
                <mo>+</mo>
                <mn>1</mn>
              </mrow>
            </mrow>
          </mtd>
          <mtd>
            <mrow>
              <mrow>
                <mrow>
                  <mi>if</mi>
                  <mo>&#x2062;</mo>
                  <mstyle>
                    <mspace width="0.8em" height="0.8ex"/>
                  </mstyle>
                  <mo>&#x2062;</mo>
                  <msub>
                    <mi>I</mi>
                    <mn>1</mn>
                  </msub>
                </mrow>
                <mo>-</mo>
                <msub>
                  <mi>I</mi>
                  <mn>2</mn>
                </msub>
              </mrow>
              <mo>&#x3e;</mo>
              <mi>Th</mi>
            </mrow>
          </mtd>
        </mtr>
      </mtable>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>1</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0139" num="0138">An ECOC bit (which is a value (+1 or &#x2212;1) of a K-th column stored in an ECOC table of first row and K-th column) defined in each class is compared to h(x) obtained from Expression (1). When the determination result is equal to the ECOC bit, the score of the class is increased by reliance degree &#x3b1;. In contrast, when the determination result is not equal to the ECOC bit, the score of the class is decreased by reliance degree &#x3b1;.</p>
<p id="p-0140" num="0139">When this process is repeated by the number of image feature amounts, H(<b>1</b>), H(<b>2</b>), H(<b>3</b>), H(<b>4</b>), H(<b>5</b>), and the like can be calculated as the K-dimensional score vector (score of K-class).</p>
<p id="p-0141" num="0140">Returning to the flowchart of <figref idref="DRAWINGS">FIG. 13</figref>, in step S<b>77</b>, the learning unit <b>123</b> determines whether the generation of the K-class determination unit <b>32</b> ends for all parts.</p>
<p id="p-0142" num="0141">In step S<b>77</b>, when it is determined that the generation of the K-class determination unit <b>32</b> does not end for all parts, the process returns to step S<b>74</b> and the generation process (process from steps S<b>74</b> to S<b>77</b>), which is described above, is repeated.</p>
<p id="p-0143" num="0142">That is, when the generation process is repeated, part regions such as an eye part region, an eyebrow part region, a nose part region, a mouth part region, and a forehead part region are separately extracted from the re-sized face region on the basis of the part region definition information shown in <figref idref="DRAWINGS">FIG. 7</figref>. Then, each K-class determination unit <b>32</b> is separately generated for each of the part regions.</p>
<p id="p-0144" num="0143">The K-class determination unit <b>32</b> is obtained for each part region defined by the part region definition information shown in <figref idref="DRAWINGS">FIG. 7</figref>, and then the learning process ends.</p>
<p id="p-0145" num="0144">The K-class determination unit <b>32</b> for each part generated in this manner can express the image feature amount of the input face image (part image) by using the K-dimensional score vector. For example, among the sample persons &#x201c;A&#x201d;, &#x201c;B&#x201d;, &#x201c;C&#x201d;, and the like, each part of the unknown person &#x201c;X&#x201d; can be expressed numerically to what degree each part of the unknown person &#x201c;X&#x201d; is similar to each part of the sample persons &#x201c;A&#x201d;, &#x201c;B&#x201d;, &#x201c;C&#x201d;, and the like.</p>
<p id="h-0011" num="0000">Details of Generation Process</p>
<p id="p-0146" num="0145"><figref idref="DRAWINGS">FIG. 16</figref> is a flowchart illustrating the details of the generation process.</p>
<p id="p-0147" num="0146">Steps S<b>91</b> to S<b>94</b> are the same as steps S<b>71</b> to S<b>74</b> of <figref idref="DRAWINGS">FIG. 13</figref>. The face region of the generation image acquired by the generation image acquisition unit <b>124</b> is detected by the preliminary image process unit <b>125</b>, and the part region is cut from the re-sized face region. As the generation image, for example, images containing face images extracted at random from 10000 images are used.</p>
<p id="p-0148" num="0147">The part image obtained in this manner is input to the K-class determination unit <b>32</b> for each part. In step S<b>95</b>, the K-class determination unit <b>32</b> determines the pixel difference feature of the input part image as a threshold value, compares the pixel difference feature to the ECOC bit defined in each class to calculate the K-dimensional score vector.</p>
<p id="p-0149" num="0148">In step S<b>96</b>, the generation unit <b>126</b> clusters the set of the K-dimensional score vectors corresponding to the part images calculated by the K-class determination unit <b>32</b> in the K-dimensional score spaces by use of the k-means algorithm, for example, and divides the set of the K-dimensional score vector into N subsets. In this way, since the part images are classified into N prototypes, the generation unit <b>126</b> generates the model images by calculating the average image of the part image group belonging to each of the classified prototypes, and registers the model images in the database <b>35</b>.</p>
<p id="p-0150" num="0149"><figref idref="DRAWINGS">FIG. 17</figref> is a diagram illustrating examples of eye model images.</p>
<p id="p-0151" num="0150">As shown in <figref idref="DRAWINGS">FIG. 17</figref>, when eye part images as a cluster of eyes are classified into twenty four prototypes, for example, an eye model image is formed by averaging the part image group belonging to each prototype. In the example of <figref idref="DRAWINGS">FIG. 17</figref>, in particular, the shapes of eyes are different for the model images of the prototypes.</p>
<p id="p-0152" num="0151">Returning to the flowchart of <figref idref="DRAWINGS">FIG. 16</figref>, in step S<b>97</b>, the generation unit <b>126</b> determines whether the generation of the model images ends for all parts.</p>
<p id="p-0153" num="0152">When it is determined that the generation of the model images does not end for all parts in step S<b>97</b>, the process returns to step S<b>94</b> and the generation process (process from steps S<b>94</b> to S<b>97</b>), which is described above, is repeated.</p>
<p id="p-0154" num="0153">That is, when the generation process is repeated, the part regions such as an eyebrow part region, a nose part region, a mouth part region, and a forehead part region other than the above-described eye part region are separately extracted from the re-sized face region on the basis of the part region definition information shown in <figref idref="DRAWINGS">FIG. 7</figref>. Then, N model images are separately generated for each part region and are registered in the database <b>35</b>.</p>
<p id="p-0155" num="0154">In this way, N model images are obtained for each of the part regions defined by the part region definition information shown in <figref idref="DRAWINGS">FIG. 7</figref> (in this case, it is not necessary for the number of model images to be equal for each part region). Then, the generation process ends.</p>
<p id="h-0012" num="0000">Setting Process</p>
<p id="p-0156" num="0155"><figref idref="DRAWINGS">FIG. 18</figref> is a flowchart illustrating the setting process in detail.</p>
<p id="p-0157" num="0156">In step S<b>101</b>, the illustration image acquisition unit <b>127</b> acquires the illustration image of each part. For example, when eye illustration images are set, illustration images varied in various forms of eyes among the parts forming the similar face picture image are prepared. For example the illustration images shown in <figref idref="DRAWINGS">FIGS. 19A to 19L</figref> are prepared.</p>
<p id="p-0158" num="0157">In step S<b>102</b>, the setting unit <b>128</b> acquires the model images registered in the database <b>35</b>. The model images are generated by the generation system <b>112</b> and are registered in the database <b>35</b> (the generation process in <figref idref="DRAWINGS">FIG. 16</figref>). For example, when the eye illustration images are set, the eye model images in <figref idref="DRAWINGS">FIG. 17</figref> are acquired among the model images generated for respective part regions and registered in the database <b>35</b>.</p>
<p id="p-0159" num="0158">In step S<b>103</b>, the setting unit <b>128</b> sets the part matching information by matching the acquired illustration image to the model image and registers the part matching information in the database <b>35</b>.</p>
<p id="p-0160" num="0159">As a matching method, there are two methods: a first matching method of executing an image process to match a certain model image to the most similar illustration image in a group of candidate illustration images; and a second matching method of confirming a model image visually by a user, selecting the illustration image which is the most similar model image in a group of candidate illustration images by the user.</p>
<p id="p-0161" num="0160">When the illustration image is matched to the model image by either of these methods, the illustration images (see <figref idref="DRAWINGS">FIGS. 19A to 19L</figref>) are matched to the model images (see <figref idref="DRAWINGS">FIG. 17</figref>), for example, as in <figref idref="DRAWINGS">FIG. 20</figref>. In the example of <figref idref="DRAWINGS">FIG. 20</figref>, each 6 by 4 illustration image can be matched to each 6 by 4 model image in accordance with the eye appearance. In this way, since the similarity can be calculated by patterns of the appearances of the part images and the model images rather than the geometric size or ratio of the parts, the similarity on which the subjective view of a person is perceived can be defined.</p>
<p id="p-0162" num="0161">When a user executes the matching visually, the setting unit <b>128</b> can, for example, intentionally match a model image having a featured mouth to an illustration image having a feature in a mouth, as in <figref idref="DRAWINGS">FIG. 21A</figref>. Moreover, the setting unit <b>128</b> can intentionally match a model image having featured eyes to an illustration image having a feature in eyes, as in <figref idref="DRAWINGS">FIG. 21B</figref>.</p>
<p id="p-0163" num="0162">In this way, when the part matching information is set, an illustration image having an expression more exaggerated than a real appearance may be allocated for the model image. Then, the similar face picture image (which is a similar face picture image in which facial features are further emphasized) having facial features of the subjective view of a person can be generated, when the similar face picture image is generated.</p>
<p id="p-0164" num="0163">Returning to the flowchart of <figref idref="DRAWINGS">FIG. 18</figref>, in step S<b>104</b>, the setting unit <b>128</b> sets the anchor point definition information for each illustration image in accordance with an instruction of a user, for example, and registers the anchor point definition in the database <b>35</b>.</p>
<p id="p-0165" num="0164">As for the anchor point definition information, as described with reference to <figref idref="DRAWINGS">FIGS. 9A and 9B</figref>, when the part points P<sub>P1 </sub>and P<sub>P2 </sub>are disposed by setting the positions of the anchor points P<sub>A1 </sub>and P<sub>A2 </sub>more inwardly and further allowing the distance between anchor points P<sub>A1 </sub>and P<sub>A2 </sub>to be narrower than the distance between the part points P<sub>P1 </sub>and P<sub>P2</sub>, for example, larger eyes can be expressed even in the same eye illustration image due to the fact that the eye illustration image is expanded and then drawn. Alternatively, when the positions of the anchor points P<sub>A1 </sub>and P<sub>A2 </sub>are set more outwardly, the distance between the anchor points P<sub>A1 </sub>and P<sub>A2 </sub>is larger than the distance between the part points P<sub>P1 </sub>and P<sub>P2</sub>. Therefore, smaller eyes can be expressed even in the same illustration image due to the fact that, for example, the eye illustration image is contracted and then drawn.</p>
<p id="p-0166" num="0165">In this way, since a part can be expressed in various forms even in the same illustration image just by changing the positions of the anchor points, it is possible to reduce the number of illustration images to be prepared. Moreover, it is possible to provide the transformed part image on which the feature of each part is perceived.</p>
<p id="p-0167" num="0166">Moreover, the number of anchor points is not limited to two, but may be set in accordance with the number of corresponding part points.</p>
<p id="p-0168" num="0167">In step S<b>105</b>, the setting unit <b>128</b> determines whether the setting of the part matching information and the anchor point definition information of all parts ends.</p>
<p id="p-0169" num="0168">When it is determined the setting of the part matching information and the anchor point definition information of all parts does not end in step S<b>105</b>, the process returns to step S<b>101</b> and the setting process (the process from steps S<b>101</b> to S<b>105</b>), which is described above, is repeated.</p>
<p id="p-0170" num="0169">That is, by repeating the setting process, each illustration image can be matched to each model image generated for each part region. Moreover, the anchor points of the illustration image can also be set separately.</p>
<p id="p-0171" num="0170"><figref idref="DRAWINGS">FIGS. 22 to 24</figref> show matching examples of the model images and the illustration images. <figref idref="DRAWINGS">FIG. 22</figref> shows the matching example of eyebrows. <figref idref="DRAWINGS">FIG. 23</figref> shows the matching example of a nose. <figref idref="DRAWINGS">FIG. 24</figref> shows the matching example of a mouth.</p>
<p id="p-0172" num="0171">As shown in <figref idref="DRAWINGS">FIG. 22</figref>, the eyebrow model images on the upper side of <figref idref="DRAWINGS">FIG. 22</figref> are obtained by classifying the eyebrow part image into twelve prototypes as a cluster of eyebrows and averaging the part image group belonging to the prototypes. In the example of <figref idref="DRAWINGS">FIG. 22</figref>, in particular, the shapes of the eyebrows are different for the model images of the prototypes. The eyebrow illustration images on the lower side of <figref idref="DRAWINGS">FIG. 22</figref> can be separately matched to the eyebrow model images by executing the setting process.</p>
<p id="p-0173" num="0172">The same is applied to the examples of <figref idref="DRAWINGS">FIGS. 23 and 24</figref>, as in the example of the eyebrows in <figref idref="DRAWINGS">FIG. 22</figref>. That is, the nose illustration images can be separately matched to the nose model images in <figref idref="DRAWINGS">FIG. 23</figref>. Likewise, the mouth illustration images can be separately matched to the mouth model images in <figref idref="DRAWINGS">FIG. 24</figref>.</p>
<p id="p-0174" num="0173">The information used for the matching by the setting process is registered as the part matching information in the database <b>35</b>. In addition, the part matching information of all parts and the anchor point definition information are set, and then the setting process ends.</p>
<p id="p-0175" num="0174">In the preliminary process device <b>101</b>, the K-class determination unit <b>32</b> is generated for each part, and the part matching information and the anchor point definition information are set and registered in advance in the database <b>35</b>. Then, the similar face picture image generation device <b>1</b> using the K-class determination unit <b>32</b> and the database <b>35</b> can generate the similar face picture image from the target image containing the face image without an operation of a user.</p>
<p id="h-0013" num="0000">Other Examples of Class Division</p>
<p id="p-0176" num="0175">Hitherto, the illustration images are matched to the model images in accordance with the part matching information. However, attribute information of sample persons may be given as class labels to the sample images and may be each learned by the K-class determination unit <b>32</b>. An example of the attribute information of the sample persons is information belonging to the same attribute, as long as the information relates to a sample person of the same race, age classification, sex, presence or absence of glasses, or the like.</p>
<p id="p-0177" num="0176">When the K-class determination unit <b>32</b> learning the attribute information as the class label is used, the part image and the model image can be featured in more detail in comparison to the case where the similarity between the part image and the model image is calculated. Therefore, since the similarity between the part image and the model image is calculated more accurately, an accurate illustration image can be selected.</p>
<p id="p-0178" num="0177">In this embodiment, the similar face picture image generation device <b>1</b> in <figref idref="DRAWINGS">FIG. 1</figref> is a different device from the preliminary process device <b>101</b> in <figref idref="DRAWINGS">FIG. 11</figref>. However, the similar face picture image generation device <b>1</b> and the preliminary process device <b>101</b> may be considered as one device including a similar face picture image generation unit as one process unit corresponding to the similar face picture image generation device <b>1</b> in <figref idref="DRAWINGS">FIG. 1</figref> and a preliminary process unit as one process unit corresponding to the preliminary process device <b>101</b> in <figref idref="DRAWINGS">FIG. 11</figref>. In this case, the K-class determination unit <b>32</b> and the database <b>35</b> included in this device are generated (set) by the preliminary process unit and are used in the similar face picture image generation process of the similar face picture image generation unit.</p>
<p id="p-0179" num="0178">In this embodiment, the similar face picture image generation device <b>1</b> and the display device <b>2</b> in <figref idref="DRAWINGS">FIG. 1</figref> are different from each other. However, a display unit as one process unit corresponding to the display device <b>2</b> may be included in the similar face picture image generation device <b>1</b>. In this case, the drawing generation unit <b>17</b> displays the generated similar face picture image on a screen of the display unit.</p>
<p id="p-0180" num="0179">The similar face picture image generated by the drawing generation unit <b>17</b> is displayed on the screen of the display device <b>2</b>. Moreover, the similar face picture image may also be compressed by a predetermined compression method such as the JPEG method and may be stored as a file in a predetermined record medium.</p>
<p id="p-0181" num="0180">The above-described series of processes may be executed by hardware or software. When the series of processes are executed by software, the program configured by the software is installed from a program record medium in a computer embedded with exclusive-use hardware or, for example, a general personal computer capable of executing various functions by installing various programs.</p>
<p id="p-0182" num="0181"><figref idref="DRAWINGS">FIG. 25</figref> is a diagram illustrating an exemplary configuration of a personal computer executing the above-described series of processes in accordance with a program. A CPU (Central Processing Unit) <b>211</b> executes various processes in accordance with a program recorded in a ROM (Read-Only Memory) <b>212</b> or a memory unit <b>218</b>. The RAM (Random Access Memory) <b>213</b> appropriately stores the program executed by the CPU <b>211</b> or data. The CPU <b>211</b>, the ROM <b>212</b>, and the RAM <b>213</b> are connected to each other via a bus <b>214</b>.</p>
<p id="p-0183" num="0182">An input/output interface <b>215</b> is connected to the CPU <b>211</b> via the bus <b>214</b>. An input unit <b>216</b> formed by a microphone or the like and an output unit <b>217</b> formed by a display, a speaker, or the like are connected to the input/output interface <b>215</b>. The CPU <b>211</b> executes various processes in accordance with an instruction input from the input unit <b>216</b>. The CPU <b>211</b> outputs the process result to the output unit <b>217</b>.</p>
<p id="p-0184" num="0183">The memory unit <b>218</b> connected to the input/output interface <b>215</b> is formed by a hard disk drive or the like and stores the program executed by the CPU <b>211</b> or a variety of data. A communication unit <b>219</b> communicates with an external device via a network such as the Internet or a local area network.</p>
<p id="p-0185" num="0184">A program may be acquired via the communication unit <b>219</b> and may be stored in the memory unit <b>218</b>.</p>
<p id="p-0186" num="0185">When a removable media <b>221</b> such as a magnetic disk, an optical disk, a magneto-optical disk, or a semiconductor memory is mounted, a drive <b>220</b> connected to the input/output interface <b>215</b> drives the removable media <b>221</b> and acquires a program, data, or the like stored in the removable media <b>221</b>. The acquired program or data is transmitted and stored in the memory unit <b>218</b>, as necessary.</p>
<p id="p-0187" num="0186">As shown in <figref idref="DRAWINGS">FIG. 25</figref>, a program record medium storing a program installed in a computer and executable by the computer includes the removable media <b>221</b> as a package media formed by a magnetic disk (including a flexible disk), an optical disk (including a CD-ROM (Compact Disc-Read Only Memory) and a DVD (Digital Versatile Disc)), a magneto-optical disk, a semiconductor memory, or the like, the ROM <b>212</b> storing a program temporarily or permanently, and the hard disk drive forming the memory unit <b>218</b>. The program may be stored in the program record medium using a wired or wireless communication medium such as a local area network, the Internet, or digital satellite broadcasting via the communication unit <b>219</b> which is an interface such as a router or a modem, as necessary.</p>
<p id="p-0188" num="0187">In the specification, steps describing the program stored in the record medium include not only a step which is executed in time-series in recorded order but also a step which is not necessarily executed in time-series but is executed in parallel or separately.</p>
<p id="p-0189" num="0188">It should be understood by those skilled in the art that various modifications, combinations, sub-combinations and alterations may occur depending on design requirements and other factors insofar as they are within the scope of the appended claims or the equivalents thereof.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08625859-20140107-M00001.NB">
<img id="EMI-M00001" he="7.45mm" wi="76.20mm" file="US08625859-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. An information processing apparatus, comprising:
<claim-text>circuitry configured to
<claim-text>acquire a two-dimensional (2D) target image;</claim-text>
<claim-text>extract a face region including a face part from the 2D target image;</claim-text>
<claim-text>identify a 2D real-image model face part by comparing the face part to a plurality of 2D real-image model face parts stored in a storage unit; and</claim-text>
<claim-text>determine a 2D illustration image corresponding to the 2D real-image model face part identified by comparing the face part to the plurality of 2D real-image model face parts stored in the storage unit.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the circuitry is configured to extract the face region by scanning a face pattern of the 2D target image.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the circuitry is configured to determine that glasses exist in the 2D target image.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The information processing apparatus according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the circuitry is configured to determine an illustration image corresponding to the glasses.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the circuitry is configured to extract a hair region from the 2D target image.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The information processing apparatus according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the circuitry is configured to determine an illustration image corresponding to the hair region.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the circuitry is configured to extract a face contour from the face region.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the circuitry is configured to resize the face region including the face part, and cut the face part from the resized face region.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The information processing apparatus according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the circuitry is configured to calculate a score vector corresponding to the face part cut from the resized face region.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The information processing apparatus according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the circuitry is configured to identify the 2D real-image model face part by comparing the calculated score vector to a plurality of score vectors corresponding to each of the plurality of 2D real-image model face parts stored in the storage unit.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the circuitry is configured to rotate or resize the 2D illustration image so that a plurality of reference points on a contour of the 2D illustration image match a plurality of points on a contour of the extracted face part.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. A non-transitory computer-readable medium including computer program instructions, which when executed by an information processing apparatus, cause the information processing apparatus to perform a method, the method comprising:
<claim-text>acquiring a two-dimensional (2D) target image;</claim-text>
<claim-text>extracting a face region including a face part from the 2D target image;</claim-text>
<claim-text>identifying a 2D real-image model face part by comparing the face part to a plurality of stored 2D real-image model face parts;</claim-text>
<claim-text>determining a 2D illustration image corresponding to the 2D real-image model face part identified by comparing the face part to the plurality of stored 2D real-image model face parts.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The non-transitory computer-readable medium according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein:
<claim-text>the identifying further includes resizing the face region including the face part, and cutting the face part from the resized face region.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The non-transitory computer-readable medium according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein:
<claim-text>the identifying further includes calculating a score vector corresponding to the face part cut from the resized face region.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The non-transitory computer-readable medium according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein:
<claim-text>the identifying further includes identifying the model face part by comparing the calculated score vector to a plurality of score vectors corresponding to each of the plurality of model face parts stored in the storage unit.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. An image generating method performed by an information processing apparatus, the method comprising:
<claim-text>acquiring, by circuitry of the information processing apparatus, a two-dimensional (2D) target image;</claim-text>
<claim-text>extracting, by the circuitry, a face region including a face part from the 2D target image;</claim-text>
<claim-text>identifying, by the circuitry, a 2D real-image model face part by comparing the face part to a plurality of 2D real-image model face parts stored in a storage unit; and</claim-text>
<claim-text>determining, by the circuitry, a 2D illustration image corresponding to the 2D real-image model face part identified by comparing the face part to the plurality of 2D real-image model face parts stored in the storage unit.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The method according to <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein:
<claim-text>the identifying further includes resizing the face region including the face part, and cutting the face part from the resized face region.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The method according to <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein:
<claim-text>the identifying further includes calculating a score vector corresponding to the face part cut from the resized face region, and identifying the 2D real-image model face part by comparing the calculated score vector to a plurality of score vectors corresponding to each of the plurality of 2D real-image model face parts stored in the storage unit.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. An information processing apparatus comprising:
<claim-text>means for acquiring a two-dimensional (2D) target image;</claim-text>
<claim-text>means for extracting a face region including a face part from the 2D target image;</claim-text>
<claim-text>means for identifying a 2D real-image model face part by comparing the face part to a plurality of 2D real-image model face parts stored in a storage unit;</claim-text>
<claim-text>means for determining an illustration image corresponding to the 2D real-image model face part identified by comparing the face part to the plurality of 2D real-image model face parts stored in the storage unit.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
