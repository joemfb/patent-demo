<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625005-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625005</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12940128</doc-number>
<date>20101105</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<us-term-of-grant>
<us-term-extension>692</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20110101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>5</main-group>
<subgroup>217</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>5</main-group>
<subgroup>228</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>348241</main-classification>
<further-classification>3482221</further-classification>
</classification-national>
<invention-title id="d2e53">First-in-first-out (FIFO) buffered median scene non-uniformity correction method</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5721427</doc-number>
<kind>A</kind>
<name>White et al.</name>
<date>19980200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5903659</doc-number>
<kind>A</kind>
<name>Kilgore</name>
<date>19990500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5925880</doc-number>
<kind>A</kind>
<name>Young et al.</name>
<date>19990700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>2502521</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6184527</doc-number>
<kind>B1</kind>
<name>Young</name>
<date>20010200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6803945</doc-number>
<kind>B1</kind>
<name>Needham</name>
<date>20041000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482071</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6934421</doc-number>
<kind>B2</kind>
<name>Gindele et al.</name>
<date>20050800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382260</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>7016550</doc-number>
<kind>B2</kind>
<name>Alderson et al.</name>
<date>20060300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382274</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>7035475</doc-number>
<kind>B1</kind>
<name>Chen et al.</name>
<date>20060400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382254</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>7280596</doc-number>
<kind>B2</kind>
<name>Cho</name>
<date>20071000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524012</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2004/0017891</doc-number>
<kind>A1</kind>
<name>Endo</name>
<date>20040100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>378 988</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2004/0113052</doc-number>
<kind>A1</kind>
<name>Johanneson et al.</name>
<date>20040600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>250214 R</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2005/0157942</doc-number>
<kind>A1</kind>
<name>Chen et al.</name>
<date>20050700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382275</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2008/0151081</doc-number>
<kind>A1</kind>
<name>Frank</name>
<date>20080600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348241</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2008/0204592</doc-number>
<kind>A1</kind>
<name>Jia et al.</name>
<date>20080800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3484021</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2010/0074554</doc-number>
<kind>A1</kind>
<name>Gyotoku</name>
<date>20100300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2010/0188535</doc-number>
<kind>A1</kind>
<name>Mitsuya et al.</name>
<date>20100700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348241</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>EP</country>
<doc-number>2 015 562</doc-number>
<kind>A1</kind>
<date>20090100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>WO</country>
<doc-number>2002/067575</doc-number>
<kind>A2</kind>
<date>20020800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>WO</country>
<doc-number>2007/123453</doc-number>
<kind>A2</kind>
<date>20071100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00020">
<othercit>Narendra; &#x201c;Scene-based nonuniformity compensation for imaging sensors&#x201d;; IEEE Trans Pattern Anal Mach Intell.; 4 (1):57-61 (1982)[Abstract only].</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>25</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>3482221</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348241</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345545</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>11</number-of-drawing-sheets>
<number-of-figures>25</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120113299</doc-number>
<kind>A1</kind>
<date>20120510</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Palik</last-name>
<first-name>Stephen M.</first-name>
<address>
<city>Playa Vista</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Palik</last-name>
<first-name>Stephen M.</first-name>
<address>
<city>Playa Vista</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Pierce Atwood LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
<agent sequence="02" rep-type="attorney">
<addressbook>
<last-name>Maraia</last-name>
<first-name>Joseph M.</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Raytheon Company</orgname>
<role>02</role>
<address>
<city>Waltham</city>
<state>MA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Velez</last-name>
<first-name>Roberto</first-name>
<department>2662</department>
</primary-examiner>
<assistant-examiner>
<last-name>Le</last-name>
<first-name>Tuan</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A buffered scene-based non-uniformity correction method includes receiving a plurality of frames of video image data from an image detector; determining relative movement of a current image frame with respect to a previous image frame and responsive to a determination of substantial movement, adding the current image frame to a buffer memory sized to store a predetermined number of video frames; averaging pixel values of the frames in the buffer to determine a mean (or weighted mean) value for each pixel of a reference image; determining correction terms for each pixel of the current image frame by determining the difference between the current image frame pixel values and the corresponding reference image pixels; and correcting the current image frame using the correction terms. A scene-based non-uniformity correction system is also disclosed.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="176.45mm" wi="226.48mm" file="US08625005-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="107.70mm" wi="174.33mm" file="US08625005-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="251.88mm" wi="140.97mm" orientation="landscape" file="US08625005-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="191.09mm" wi="102.11mm" orientation="landscape" file="US08625005-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="150.62mm" wi="148.00mm" file="US08625005-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="109.56mm" wi="203.12mm" file="US08625005-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="230.04mm" wi="185.17mm" orientation="landscape" file="US08625005-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="233.26mm" wi="187.79mm" orientation="landscape" file="US08625005-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="229.45mm" wi="191.60mm" orientation="landscape" file="US08625005-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="220.47mm" wi="181.36mm" orientation="landscape" file="US08625005-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="196.09mm" wi="185.84mm" orientation="landscape" file="US08625005-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="111.51mm" wi="195.41mm" file="US08625005-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND</heading>
<p id="p-0002" num="0001">This application generally relates to digital image processing, and more particular, to a first-in-first-out (FIFO) buffered median scene non-uniformity correction method.</p>
<p id="p-0003" num="0002">Digital sensors exhibit certain noise characteristics that result in some degree of image degradation in the final output. One image degradation problem of concern is residual Fixed Pattern Noise (FPN) which may be present after typical Non-Uniformity Correction algorithms have been performed. Distinguishing and effectively removing this noise traditionally has been difficult without degrading the high frequency spatial content of the image.</p>
<p id="p-0004" num="0003">Conventional non-uniformity correction (NUC) methods have attempted to identify high frequency scene content areas and avoid further processing in those areas in a given frame. These methods converge slowly to minimize the effect of temporal noise and scene content impacting FPN correcting terms. Rapid changes in input scene intensity thus result in correspondingly rapid changes in FPN, which are not fully corrected. An additional problem with these approaches is that outlying pixels may be difficult to distinguish from high frequency content and, as a result, residual FPN artifacts can be present in an image. These artifacts may look like random &#x201c;spots&#x201d;, &#x201c;lines&#x201d; and/or other visible artifacts to the end-user.</p>
<p id="p-0005" num="0004">Thus, an improved scene non-uniformity correction method is desired which preserves the true image spatial context while removing noise.</p>
<heading id="h-0002" level="1">SUMMARY</heading>
<p id="p-0006" num="0005">In an embodiment, a buffered scene-based non-uniformity correction method comprises: receiving a plurality of frames of video image data from an image detector; determining relative movement of a current image frame with respect to a previous image frame; if there is substantial movement determined, adding the current image frame to a buffer in a memory sized to store a predetermined number of video frames; averaging pixel values of the frames in the buffer to determine a mean value for each pixel of a reference image; determining correction terms for each pixel of the current image frame by determining the difference between the current image frame pixel value and the reference image; and correcting the current image frame using the correction terms.</p>
<p id="p-0007" num="0006">In another embodiment, a buffered scene-based non-uniformity correction system comprises: a buffer memory sized to store a predetermined number of video frames; and a processor configured to: receive a plurality of frames of video image data from an image detector; determine relative movement of a current image frame with respect to a previous image frame; if there is substantial movement determined, add the current image frame to the buffer; average pixel values of the frames in the buffer to determine a mean value for each pixel of a reference image; determine correction terms for each pixel of the current image frame by determining the difference between the current image frame pixel value and the reference image; and correct the current image frame using the correction terms.</p>
<p id="p-0008" num="0007">These and other aspects of this disclosure, as well as the methods of operation and functions of the related elements of structure and the combination of parts and economies of manufacture, will become more apparent upon consideration of the following description and the appended claims with reference to the accompanying drawings, all of which form a part of this specification, wherein like reference numerals designate corresponding parts in the various figures. It is to be expressly understood, however, that the drawings are for the purpose of illustration and description only and are not a limitation of the invention. In addition, it should be appreciated that structural features shown or described in any one embodiment herein can be used in other embodiments as well.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. 1</figref> shows a general overview of a FIFO buffered median scene non-uniformity correction system in accordance with an embodiment.</p>
<p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. 2</figref> shows a general overview of a FIFO buffered median scene non-uniformity correction method in accordance with an embodiment. <figref idref="DRAWINGS">FIG. 2A</figref> shows an example of the correction method being applied to a frame of video data.</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 3</figref> shows a general overview of a method for determining reference frames that may be used in the FIFO buffered median scene non-uniformity correction method in accordance with an embodiment.</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 4</figref> shows an exemplary method for determining relative movement of a new frame to a previous frame of video data in the FIFO buffer according to an embodiment.</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIGS. 5A and 5B</figref> depict exemplary context windows for determining correction term values according to an embodiment.</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 6</figref> shows an exemplary FIFO buffered median scene non-uniformity correction algorithm in accordance with an embodiment.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIGS. 7-10</figref> show exemplary video frames both before and after having been subjected to FIFO buffered median scene non-uniformity correction method in accordance with an embodiment.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 11</figref> is a plot showing performance of the FIFO-buffered median scene non-uniformity based correction algorithm in accordance with an embodiment as compared with other conventional non-uniformity correction algorithms.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0017" num="0016">A first-in-first-out (FIFO) buffered median scene non-uniformity correction methodology is described which uses an algorithm that is configured to detect and to remove scene based non-uniformities from video images. The methodology is configured to preserve true image spatial content of the video images, while correcting for fixed pattern noise (FPN) therein. According to one or more embodiments, a FIFO buffer is used to store recent video images, with video frames being added only to the FIFO buffer when a minimum amount of spatial movement is detected. Weighted averaged FIFO buffer frames may be used to create a reference frame which is used to determine correction terms which are then used to correct image frames.</p>
<p id="p-0018" num="0017">As used herein, &#x201c;first-in-first-out&#x201d; and &#x201c;FIFO&#x201d; refer to the prioritization and ordering of data in a queue having a predetermined size which can only accommodate a predetermined amount of data, in this case video images or frames. For instance, data added to the tail of the queue will continually be advanced to the head of the queue, as newer data is added to the tail and older data is removed from head of the queue. That being said, newer data being added at the tail may be thought of as pushing out older data from the head of queue.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 1</figref> shows a general overview of FIFO buffered median scene non-uniformity correction system <b>100</b> in accordance with an embodiment.</p>
<p id="p-0020" num="0019">System <b>100</b> generally includes image detector <b>110</b>, image processor <b>120</b>, and FIFO buffer <b>130</b>. Processed video image data <b>140</b> from image processor <b>120</b> may be output for display on a suitable display device and/or stored in memory (for later use).</p>
<p id="p-0021" num="0020">Image detector <b>110</b> may be configured to generate video image data of scene S. For instance, in some implementations, image detector <b>110</b> may be a conventional video camera. It will be appreciated, though, that image detector <b>110</b> may be any device configured to receive electromagnetic energy (light) in one or more spectra, such as, for instance, ultraviolet (UV), visible (VIS) and/or infrared (IR) from scene S and to generate a plurality of consecutive images in relatively rapid succession. Together the images form video data.</p>
<p id="p-0022" num="0021">Image processor <b>120</b> receives video image data from image detector <b>110</b> and processes individual frames of the video image data. In particular, image processor <b>120</b> is configured to perform a FIFO median scene non-uniformity correction to the video image data to correct for residual Fixed Pattern Noise (FPN) as disclosed herein. In various embodiments, image processor <b>120</b> may include dedicated hardware (such as, microprocessor, central processing unit (CPU), an application specific integrated circuit (ASIC) or field programmable gate array (FPGA)), software (or firmware), or a combination of dedicated hardware and software. Of course, it will be appreciated that any number of hardware and/or software implementations, programming languages, and operating platforms may be used. As such, the description or recitation of any specific hardware or software implementation, programming language, and operating platform herein is exemplary only and should not be viewed as limiting. For the different applications of the embodiments disclosed herein, the programming and/or configuration may vary. Processor <b>120</b> may be also integrated within any machine (not shown) in some cases. Also, while processor <b>120</b> is illustrated as being connected to image detector <b>110</b>, it will be appreciated that processor <b>120</b> may be integrated into image detector <b>110</b>, or alternatively be remote to it (with data being transferred, for instance, via one or more networks). Of course, video data may also be stored in an electronic memory and retrieved later for processing via processor <b>120</b>.</p>
<p id="p-0023" num="0022">Algorithm <b>600</b> (<figref idref="DRAWINGS">FIG. 6</figref>) may be stored on a computer- or machine-readable storage media having computer- or machine-executable instructions that are executable by processor <b>120</b>. In one implementation, algorithm <b>600</b> may reside on a memory, such as, for example, any non-volatile electronic memory device (e.g., flash memory, EEPROM, etc.) or other memory device (e.g., disk drive, writable optical disk, etc.).</p>
<p id="p-0024" num="0023">FIFO buffer <b>130</b> is configured to store plural frames of video data for use by image processor <b>120</b>, which are used to perform image correction. FIFO buffer <b>130</b> may include any electronic memory device, for example, any non-volatile electronic memory device (e.g., flash memory, EEPROM, etc.) or other memory device (e.g., disk drive, writable optical disk, etc.) for storing video data. In one implementation, FIFO <b>130</b> buffer may be configured to store five frames of video data. Of course, FIFO buffer <b>130</b> might be smaller or larger depending on available memory and/or processing resources.</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 2</figref> shows a general overview of FIFO buffered median scene non-uniformity correction method <b>200</b> in accordance with an embodiment. <figref idref="DRAWINGS">FIG. 2A</figref> shows an example of the correction method <b>200</b> being applied to a frame of video data.</p>
<p id="p-0026" num="0025">In step <b>210</b>, raw video image data is provided from image detector <b>110</b> to image processor <b>120</b>. The raw image data may include individual frames of video data in various analog and/or digital video formats, such as, for example, raw digital frames of video from image detector <b>110</b> in either progressive or interlaced formats, either sent as complete frame or as lines of video. Exemplary raw video image frame <b>240</b> is shown in <figref idref="DRAWINGS">FIG. 2A</figref>.</p>
<p id="p-0027" num="0026">Next, in step <b>220</b>, image processor <b>120</b> calculates correction terms for the raw image data using a FIFO buffered median scene non-uniformity correction method in accordance with one or more embodiments. Correction terms may be determined and individually applied on a pixel-by-pixel basis. Taken together, the correction terms may be referred to as a &#x201c;blurry&#x201d; reference frame, and may be generated using a FIFO buffer (of which may contain frames of video with a minimum spatial difference&#x2014;relative scene movement) as discussed below. The correction terms are quickly updated by using a small FIFO buffer (e.g., five frames). Blurry reference frame <b>250</b> is shown in <figref idref="DRAWINGS">FIG. 2A</figref>. High spatial frequencies due to scene content are not present in blurry reference frame <b>250</b>. As a result, the method used to generate correction terms does not produce terms that contain information on scene content and thus only remove FPN (and do not reduce spatial frequencies in the image due to genuine scene content).</p>
<p id="p-0028" num="0027">To detect and remove residual FPN, processor <b>120</b> may be configured to look at an average of the frame values in FIFO buffer <b>130</b> for each pixel. In some implementations, this may be a weighted average. Motion detection is used to ensure that frames in FIFO buffer <b>130</b> are spatially different.</p>
<p id="p-0029" num="0028">The difference between a given pixel value (in the averaged FIFO buffer <b>130</b>) and the median of its nearest neighbors may be considered a good approximation of the FPN for that pixel. This difference represents the amount of offset needed to correct the FPN for this pixel in the image. FPN correction terms can be routinely updated when FIFO buffer <b>130</b> changes as a new frame replaces an older frame.</p>
<p id="p-0030" num="0029">Next, in step <b>230</b>, the correction terms calculated in step <b>120</b> are applied to the raw image data to generate a corrected video image. Corrected video frame <b>260</b> is shown in <figref idref="DRAWINGS">FIG. 2A</figref>. Additional image processing on the corrected video frames may also be performed (as necessary).</p>
<p id="p-0031" num="0030">Method <b>200</b> may be configured, in some instances, not to update correction terms in areas of high-frequency and/or dynamic-range scene content. This may prevent improper correction terms from being created. Once the high-frequency and/or dynamic-range scene content moves to a different region, the correction terms for that region may again be regularly updated. However, this is not generally active for the vast majority of scene content and may prevent regions with high pixel-to-pixel differences from becoming a problem. Examples of scenes with high &#x201c;delta&#x201d; values, i.e., large differences in pixel intensities, can include an airplane engine, sky to hot object border, etc.</p>
<p id="p-0032" num="0031">For determining motion, an image motion tracker may be used. And, for some image scenes, method <b>200</b> may be more aggressively performed, based on motion and/or other factors. Moreover, the method <b>200</b> may allow greater refinement of correction terms while viewing uniform scenes.</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 3</figref> shows a general overview of method <b>300</b> for determining reference frames that may be used in FIFO buffered median scene non-uniformity correction method <b>100</b> in accordance with an embodiment.</p>
<p id="p-0034" num="0033">Method <b>300</b> uses FIFO buffer <b>130</b> to store recent video frames. Each new frame of video image data from image detector <b>110</b> is analyzed by image processor <b>120</b> in step <b>310</b>. If the current frame under consideration is determined by image processor <b>120</b> to be spatially different than the previous frame in the video steam, then that frame will be added to FIFO buffer <b>130</b> in step <b>320</b>.</p>
<p id="p-0035" num="0034">A motion detection technique, for instance, can be used to ensure that frames in FIFO buffer <b>130</b> are spatially different, that is, having greater spatial variance than a predetermined minimum threshold, as discussed herein. In one instance, the predetermined minimum threshold may be 3 pixels having movement since the last moving frame detected.</p>
<p id="p-0036" num="0035">According to one or more implementations, frames may also be added to FIFO buffer <b>130</b> when (i) there is a uniform scene with low spatial frequency content; and/or (ii) there was continuous motion which has recently stopped.</p>
<p id="p-0037" num="0036">For the frames stored in FIFO buffer <b>130</b>, an average sum is routinely computed by image processor <b>120</b> on a pixel-by-pixel basis, which together are used to form a reference frame in step <b>330</b>. The reference frame may be configured to blur scene content, but not FPN. For instance, in one embodiment, the frames stored in FIFO buffer <b>130</b> may be processed so as to create a &#x201c;blur&#x201d; radius sufficient to remove the high spatial frequencies of the imaged scene but not the sensor FPN.</p>
<p id="p-0038" num="0037">FIFO buffer <b>130</b> is heavily used in algorithm <b>600</b> (<figref idref="DRAWINGS">FIG. 6</figref>). The FIFO average is used to compute the FPN correction terms. FPN can be identified in all portions of the image (including areas of scene S with high spatial frequencies) since the &#x201c;blurry&#x201d; reference frame is used to detect FPN.</p>
<p id="p-0039" num="0038">In one implementation, correction terms for FPN may be continuously evaluated and updated by image processor <b>120</b>, for instance, at approximately 30 frames per second (fps). Thus, for a 5-frame FIFO buffer, the correction would be about 16.6 ms which enables handling of rapidly changing FPN levels. Recovery from any &#x201c;negative&#x201d; effect can be very quick and limited in duration to only about 16.6 ms. For example, large direct current (DC) offset pixels in the raw image (which appear as FPN in the reference frame) can then be corrected.</p>
<p id="p-0040" num="0039">According to this methodology, the temporal noise in the reference frame (as compared to a single frame) may be reduced by approximately the square root of n, where n is the number of frames stored in FIFO buffer <b>130</b>. This allows FPN to be more precisely quantified and corrected.</p>
<p id="p-0041" num="0040">One or more optional modes may be provided in handling video image data.</p>
<p id="p-0042" num="0041">In some cases, image motion detection may not be able to easily detect movement while viewing a uniform scene, such as the sky. Thus, an optional &#x201c;Sky Mode&#x201d; may be provided which is configured to determine if the video contains a uniform scene. The &#x201c;Sky Mode&#x201d; can be activated, for instance, when low scene structure is detected (i.e., no high spatial frequency content) and no motion is occurring. One low scene structure threshold may be determined applying a median filter to the image (kernel size in one implementation is 3&#xd7;3 pixels), then convolving a two axis sobel kernel on a video frame, next generating the resulting sobel image, and finally determining the value of this resultant image. In one implementation a value of 350 can be used as a minimum threshold to determine the presence of low scene structure. No motion in one implementation may be considered when less than one-half pixel of motion is present. When this mode is activated (low scene threshold is met and no motion is present), frames may be added to the FIFO buffer to improve the calculation of correction terms. This optional processing allows scenes with very low contrast and scene structure to have residual FPN removed where conventional algorithms would not. In some implementations, the algorithm can run the FPN correction updates during a uniform scene. One or more tests (e.g., image scene structure, movement, etc.) may be performed to automatically prevent this mode from running, if any high frequency content exists. One scene structure metric is discussed in more detail below in step <b>630</b> (<figref idref="DRAWINGS">FIG. 6</figref>). When the Sky Mode is activated, though, the algorithm can add additional frames to the FIFO buffer and improve the correction terms.</p>
<p id="p-0043" num="0042">Additionally, in some embodiments, an option may be selected to address stopped motion. For example, when there has been continuous movement that abruptly stops in the current frame, several additional frames (e.g., up to 50% of the FIFO buffer) may be added to the FIFO buffer. If this option is enabled, this allows the FIFO buffer to more accurately reflect current scene values, but to still contain sufficient blur. The end result of this optional processing is that Image Quality (IQ) in substantially static or non-motion conditions can be improved.</p>
<p id="p-0044" num="0043">Another option may be to identify and remove blinking pixels from the video frames. One criterion to detect blinking pixels is to determine if a particular pixel changes quickly in time, but its neighboring pixels do not. To help detect blinking pixels, the algorithm looks at the change in the correction terms from one instance to the next. One of several strong indicators (metrics) that may indicate that the pixel is a &#x201c;blinker,&#x201d; is if a correction term radically changes values (e.g. from one update of the correction term to the next). As a result of detecting a blinking pixel, the detected blinking pixels can be removed, and replaced with, for example, a median value of its neighbors. Thus, blinking pixels may be identified and removed substantially in real-time. In some instances, the algorithm may be configured to remove a few hundred blinking pixels in an image (e.g., less than 0.1% of the total pixels).</p>
<p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. 4</figref> shows exemplary method <b>400</b> for determining relative moment of a new frame to a previous frame of video data in the FIFO buffer according to an embodiment.</p>
<p id="p-0046" num="0045">For determining motion, an image motion tracking function may be used. This ensures that high frequency spatial frequencies due to scene content are not present in the blurry reference frame. For some image scenes, this method may be more aggressively performed so as to allow greater refinement of correction terms while viewing uniform scenes.</p>
<p id="p-0047" num="0046">Exemplary reference image <b>410</b> is schematically illustrated as an 8&#xd7;6 array of pixels, for explanation purposes only. Of course, the size of an image is not limiting and may vary.</p>
<p id="p-0048" num="0047">Method <b>400</b> correlates the X- and Y-axis sums of pixel values which reduces the number of operations by a factor of a square root of the number of pixels compared to most image correlation trackers. The result is a very fast, accurate tracking method with high temporal noise suppression. In addition, there may also be one or more error checking schemes built in to improve results and accuracy.</p>
<p id="p-0049" num="0048">To begin, row vector <b>420</b> and column vector <b>430</b> corresponding to the size of the image <b>410</b> dimensions may be allocated in memory. Next, row and column vector sums &#x3a3; may be computed while reading in video data. For instance, as each pixel of image <b>410</b> is read in a line of video, its value may be added to the corresponding element value of row vector <b>420</b>. When the end of the line of video is reached, the &#x201c;row sum&#x201d; &#x3a3;<sub>R </sub>is determined for that row and may be stored in the corresponding element of the row vector <b>420</b> for that particular row. The &#x201c;column sum&#x201d; &#x3a3;<sub>C </sub>may be calculated in an analogous manner as the row sums &#x3a3;<sub>R</sub>.</p>
<p id="p-0050" num="0049">Once computed, these vectors are compared with a previous frame (or other frame) to determine any potential correlation. To improve algorithm speed, the size of the correlation region or search size may be reduced. Also, by extending the correlation region, local regional movements versus entire image shifts may be found.</p>
<p id="p-0051" num="0050">In some implementations, the resulting row and column vectors <b>420</b>, <b>430</b> may be scaled to allow for sub-pixel correlation. For instance, a cubic interpolation function may be optionally applied to scale the size of the resulting vectors by a scaling factor Z. In one embodiment, the scaling factor Z may be 2. Although, it will be appreciated that values between 2 and 4 may be most common. Of course, other interpolation functions may also be used.</p>
<p id="p-0052" num="0051">Consider, for example, an image size of M&#xd7;N, where M is the number or rows and N is the number of columns. A search size D from a current pixel may be defined as the maximum distance (in pixels) to search for correlation/movement. If scaled, M, N and D will be each multiplied by the scale factor Z.</p>
<p id="p-0053" num="0052">To determine the search size D for the image, the first step may be to take a segment of the reference scaled vector centered, of size M*Z&#x2212;N*Z&#x2212;1. Next, a segment of equal length (but indexed from the first index position of the comparative vector) is considered. The vectors can be compared using, for instance, a mean absolute difference method in which all corresponding elements in the vectors are subtracted, absolute value taken, and averaged. This metric is then recorded.</p>
<p id="p-0054" num="0053">This process may be then repeated, but with the comparative vector starting at a next position (e.g., index position two). Again, the metric is recorded. This process is continued until the comparative sub-vector has considered the entire comparative vector and metrics for each position have been computed. There should be 2*D*Z+1 iterations of the loop. Once completed, the minimum value of the vector (in the case of a mean absolute different metric being used) may correspond to the &#x201c;best&#x201d; correlation. The &#x201c;offset&#x201d; applied (i.e., index offset within search range) indicates the movement in pixels that was detected. If scaled, the resultant value may be divided by Z to get the proper correlation in the original image dimensions.</p>
<p id="p-0055" num="0054"><figref idref="DRAWINGS">FIGS. 5A and 5B</figref> depict exemplary context windows for determining correction term values according to an embodiment.</p>
<p id="p-0056" num="0055">Each pixel in the reference frame may be individually processed to determine updated correction terms. Correction terms may be calculated using context windows in the vicinity of a pixel under consideration. For instance, the vicinity be may defined as plus or minus a few pixels (e.g., up/down and left/right) from the pixel under consideration. The process may require very little in the terms of memory resources.</p>
<p id="p-0057" num="0056">Both a &#x201c;near region&#x201d; and &#x201c;far region&#x201d; from a current pixel under consideration may be defined. The near and far regions may each include eight pixels. Each of the near region and the far region may be further divided into two sub-regions (e.g., 1 and 2), which each contain four pixels. The first region may include the four pixels orthogonally oriented up/down and left/right with respect to the pixel under consideration. The second region may include the four pixels which are diagonally oriented with respect to the pixel under consideration.</p>
<p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. 5A</figref> shows context window <b>510</b> for determining the near region (NR) median value. The NR median value may be determined by calculating the median of the &#x201c;near region&#x201d; around each pixel adjacent to the pixel under consideration in the reference frame. In this example, centrally located pixel CP is the pixel under consideration in context window <b>510</b>. The near region is illustrated as the eight pixels NR<b>1</b><sub>1</sub>, NR<b>1</b><sub>2</sub>, NR<b>1</b><sub>3</sub>, NR<b>1</b><sub>4</sub>, NR<b>2</b><sub>1</sub>, NR<b>2</b><sub>2</sub>, NR<b>2</b><sub>3</sub>, NR<b>2</b><sub>4 </sub>immediately surrounding central pixel CP under consideration.</p>
<p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. 5B</figref> shows context window <b>520</b> for determining the far region (FR) median value. The FR median value may be determined by calculating the median of the &#x201c;outer edge region&#x201d; around each pixel under consideration in the reference frame. In this example, centrally located pixel CP is the pixel under consideration. The outer edge region is illustrated as the eight pixels FR<b>1</b><sub>1</sub>, FR<b>1</b><sub>2</sub>, FR<b>1</b><sub>3</sub>, FR<b>1</b><sub>4</sub>, FR<b>2</b><sub>1</sub>, FR<b>2</b><sub>2</sub>, FR<b>2</b><sub>3</sub>, FR<b>2</b><sub>4 </sub>surrounding central pixel CP under consideration. As shown, outer edge pixels are located three pixels away up/down and left/right from the pixel under consideration.</p>
<p id="p-0060" num="0059">For each sub-region, the four pixel values may be stored as a vector. The vector may be ordered to ensure that the first and last elements are the highest and lowest values and the center two values are averaged (i.e., taking the median of the four values). This results in:</p>
<p id="p-0061" num="0060">Near Region <b>1</b> Median=Median (NR<b>1</b><sub>1</sub>, NR<b>1</b><sub>2</sub>, NR<b>1</b><sub>3</sub>, NR<b>1</b><sub>4</sub>)</p>
<p id="p-0062" num="0061">Near Region <b>2</b> Median=Median (NR<b>2</b><sub>1</sub>, NR<b>2</b><sub>2</sub>, NR<b>2</b><sub>3</sub>, NR<b>2</b><sub>4</sub>)</p>
<p id="p-0063" num="0062">Far Region <b>1</b> Median=Median (FR<b>1</b><sub>1</sub>, FR<b>1</b><sub>2</sub>, FR<b>1</b><sub>3</sub>, FR<b>1</b><sub>4</sub>)</p>
<p id="p-0064" num="0063">Far Region <b>2</b> Median=Median (FR<b>1</b><sub>1</sub>, FR<b>1</b><sub>2</sub>, FR<b>1</b><sub>3</sub>, FR<b>1</b><sub>4</sub>)</p>
<p id="p-0065" num="0064">Near Region Avg Median=(Near Region <b>1</b> Median+Near Region <b>2</b> Median)/2</p>
<p id="p-0066" num="0065">Far Region Avg Median=(Far Region <b>1</b> Median+Far Region <b>2</b> Median)/2</p>
<p id="p-0067" num="0066">Additionally, the absolute &#x201c;delta&#x201d; (or difference) of the center two values (of four) for the NR is calculated. This yields, Near Region <b>1</b> Center Value Delta and Near Region <b>2</b> Center Value Delta.</p>
<p id="p-0068" num="0067">Updated correction terms may be determined using the NR and FR median values. The correction value can be determined by looking at the difference between the FIFO buffer average pixel value and the Near Region median pixel values. A weight, coef_update_weight, may be used to apply more or less emphasis on the new value. For instance, the NR may receive a two-thirds weighting and the FR receiving a one-third weighting. For each pixel, the correction term can then be computed using the following pseudo-code logic:</p>
<p id="p-0069" num="0068">
<tables id="TABLE-US-00001" num="00001">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="left"/>
<thead>
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry>correction_last = correction[current_pixel]</entry>
</row>
<row>
<entry>GIVEN:</entry>
</row>
<row>
<entry>Threshold &#x2192; Edge detection threshold value</entry>
</row>
<row>
<entry>Inner_Outer_Median_Thresh &#x2192; Edge detection threshold value</entry>
</row>
<row>
<entry>coef_update_weight &#x2192; Weight to update new correction term by</entry>
</row>
<row>
<entry>stack_avg &#x2192; FIFO stack average</entry>
</row>
<row>
<entry>correction_old = correction[current_pixel]</entry>
</row>
<row>
<entry>IF</entry>
</row>
<row>
<entry>Near Region 1 Center Value Delta &#x3c; Threshold</entry>
</row>
<row>
<entry>And</entry>
</row>
<row>
<entry>Near Region 2 Center Value Delta &#x3c; Threshold</entry>
</row>
<row>
<entry>And</entry>
</row>
<row>
<entry>Abs(Near Region 1 Center Value Delta &#x2212; Far Region Avg Median) &#x3c;</entry>
</row>
<row>
<entry>Inner_Outer_Median_Thresh</entry>
</row>
<row>
<entry>THEN</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="14pt" align="left"/>
<colspec colname="1" colwidth="203pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>correction[current_pixel] =</entry>
</row>
<row>
<entry/>
<entry>(</entry>
</row>
<row>
<entry/>
<entry>coef_update_weight *</entry>
</row>
<row>
<entry/>
<entry>( stack_avg[current_pixel] &#x2212; (Near Region 2 Median/1.5 + Near</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="left"/>
<tbody valign="top">
<row>
<entry>Region 1 Median/3) )</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="14pt" align="left"/>
<colspec colname="1" colwidth="203pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>+ correction[current_pixel]</entry>
</row>
<row>
<entry/>
<entry>)</entry>
</row>
<row>
<entry/>
<entry>/</entry>
</row>
<row>
<entry/>
<entry>(1+coef_update_weight)</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="left"/>
<tbody valign="top">
<row>
<entry>ELSE</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="14pt" align="left"/>
<colspec colname="1" colwidth="203pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>correction[current_pixel] = correction[current_pixel]</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="left"/>
<tbody valign="top">
<row>
<entry>outer_mean_old[current_pixel] = Far Region Avg Median</entry>
</row>
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0070" num="0069">The FIFO Average, stack_avg, can be calculated substantially simultaneously (i.e., &#x201c;on the fly&#x201d;) as the most recent frame is being added. If, in a real-time scenario, any of the &#x201c;conditionals&#x201d; were met with the previous frame, then the FIFO Average might also be recomputed for each pixel as it is read in (although this may require a large amount of memory to store all the frames in the FIFO buffer). Using this method of computing FIFO Averages on the fly for the most recent frame can also (with a few video lines delay) compute correction terms without having to wait for the entire image to be read in. This allows the correction terms to be computed and applied with only a few lines of video delay to the system.</p>
<p id="p-0071" num="0070">To compute correction terms, a delay of roughly three lines of video may be necessary before computing terms. This delay is directly proportional to the largest &#x201c;kernel&#x201d; size used in the correction terms (thus for a 7&#xd7;7 kernel, the center of the kernel would require a three pixel clock delay plus three video line delay). To support a near-real time implementation, the correction terms may be computed by this amount of delay versus the &#x201c;FIFO Average&#x201d; being calculated for the given frame.</p>
<p id="p-0072" num="0071">If the blinking pixel option is activated, blinking pixels may be identified and corrected at this time. For instance, the pseudo-code below can be invoked directly after the correction term for a pixel is calculated. It assumes the same pre-computed variables that the correction term calculations needed have been generated. And, if the conditional statements are satisfied, then the blinker timer value for the pixel is set to the max value, e.g., 100 frames in one implementation.</p>
<p id="p-0073" num="0072">
<tables id="TABLE-US-00002" num="00002">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="left"/>
<thead>
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry>GIVEN:</entry>
</row>
<row>
<entry>Near Region 1 &#x2192; Already computed and sorted in Correction Term</entry>
</row>
<row>
<entry>Computation</entry>
</row>
<row>
<entry>Near Region 2 &#x2192; Already computed and sorted in Correction Term</entry>
</row>
<row>
<entry>Computation</entry>
</row>
<row>
<entry>outer_mean &#x2192; Already computed and sorted in Correction Term</entry>
</row>
<row>
<entry>Computation</entry>
</row>
<row>
<entry>outer_mean_old &#x2192; Already computed and sorted in Correction Term</entry>
</row>
<row>
<entry>Computation</entry>
</row>
<row>
<entry>correction_old &#x2192; Already computed and sorted in Correction Term</entry>
</row>
<row>
<entry>Computation</entry>
</row>
<row>
<entry>blinker_thresh, subterm_thresh &#x2192; Threshold to help detect blinkers</entry>
</row>
<row>
<entry>blinker_time &#x2192; Max Blinker Timer Value</entry>
</row>
<row>
<entry>main_pix_delta = correction[current_pixel] &#x2212; correction_old</entry>
</row>
<row>
<entry>other_delta = outer_mean_old[current_pixel] &#x2212; outer_mean</entry>
</row>
<row>
<entry>IF</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="14pt" align="left"/>
<colspec colname="1" colwidth="203pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>(abs(main_pix_delta) &#x3e; blinker_thresh</entry>
</row>
<row>
<entry/>
<entry>AND</entry>
</row>
<row>
<entry/>
<entry>fabs(other_delta) &#x3c; blinker_thresh_low</entry>
</row>
<row>
<entry/>
<entry>AND max(Near Region 1) &#x2212; min(Near Region 1) &#x3c; subterm_thresh</entry>
</row>
<row>
<entry/>
<entry>AND max(Near Region 2) &#x2212; min(Near Region 1) &#x3c; subterm_thresh</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="left"/>
<tbody valign="top">
<row>
<entry>THEN</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="14pt" align="left"/>
<colspec colname="1" colwidth="203pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>blink_timer[current_pixel] = blinker_time</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="left"/>
<tbody valign="top">
<row>
<entry>IF</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="14pt" align="left"/>
<colspec colname="1" colwidth="203pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>blink_timer[current_pixel] &#x3e; 0</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="left"/>
<tbody valign="top">
<row>
<entry>THEN</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="14pt" align="left"/>
<colspec colname="1" colwidth="203pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>blink_timer[current_pixel] = blink_timer[current_pixel] &#x2212; 1</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. 6</figref> shows exemplary FIFO buffered median scene non-uniformity correction algorithm <b>600</b> in accordance with an embodiment. Algorithm <b>600</b> may be executed by image processor <b>120</b> (<figref idref="DRAWINGS">FIG. 1</figref>). Algorithm <b>600</b> does not replace pixels due to non-uniformities, but rather corrects residual FPN. Merely replacing pixels can lead to image blur, which reduces Image Quality (IQ) and is generally not desirable.</p>
<p id="p-0075" num="0074">Thus, to help high frequency scene content (and prevent correction terms from being inappropriately updated) algorithm <b>600</b> looks at several factors.</p>
<p id="p-0076" num="0075">Algorithm <b>600</b> begins with current frame <b>605</b> of video data being considered. In step <b>610</b>, the relative movement of current frame <b>605</b> is determined with respect to the previous frame. This may be performed by determining vectors for rows and columns respectively corresponding to the size of the image, for example, as discussed above, with respect to <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0077" num="0076">A determination is made in step <b>615</b> whether the determined movement is above a predetermined threshold. For instance, the predetermined threshold may be 3 pixels of scene movement since the last moving frame detected. Alternatively, this step might look to see if movement is within a certain number (e.g., one) of frames of movement falling below threshold.</p>
<p id="p-0078" num="0077">If the determined movement is above a predetermined threshold then algorithm <b>600</b> continues to step <b>645</b>, discussed below. Otherwise, algorithm <b>600</b> continues to step <b>620</b>. In step <b>620</b> a determination is made whether the &#x201c;Sky Mode&#x201d; is activated. If the &#x201c;Sky Mode&#x201d; is not activated, then algorithm <b>600</b> proceeds to step <b>640</b>.</p>
<p id="p-0079" num="0078">If so, then in step <b>625</b>, a further determination is made whether movement is less than the sky mode movement threshold. For instance, this may be 0.5 pixels relative movement. If so, then in step <b>630</b> a scene structure metric is generated on the current frame. Otherwise, if the sky mode movement threshold is not met, then the process proceeds to <b>640</b>.</p>
<p id="p-0080" num="0079">The scene structure metric generated in step <b>630</b> is configured to determine the structure of the scene. This results in a number that is proportional to scene content (with a higher value meaning more scene structure). The metric can be computed by taking the mean of all the resulting Net Sobel values. Sobel values may be computed using a known Sobel operator function.</p>
<p id="p-0081" num="0080">First, the algorithm takes an incoming image and applies a median filter process to it. In one example, a 3&#xd7;3 median filter may be used. Next, the image has both an X- and Y-axis Sobel operator kernel independently applied. The resulting value for each pixel is determined using the following formula:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>Net_Sobel_pix=|<i>X</i>Sobel_pix|+|<i>Y</i>Sobel_pix|<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0082" num="0081">The median filter removes very high frequency noise (such as, FPN/Temporal) that is generally not of any consequence, and then the Sobel works as an edge detection method.</p>
<p id="p-0083" num="0082">In step <b>635</b>, a determination is made whether the scene structure metric is less than a threshold value. For instance, this may be 77 (value is proportional to the degree of high spatial frequency content, with lower values indicating less high spatial frequencies) to ensure that the algorithm does not update correction terms in areas of high frequency or dynamic range scene content. If the scene structure metric is less than the threshold, then the processing continues to step <b>645</b>. Otherwise, if the scene structure metric is not met, then the processing proceeds to <b>640</b>.</p>
<p id="p-0084" num="0083">In step <b>640</b>, the last frame in the FIFO buffer is replaced with the current frame and the process continues to step <b>650</b>. It will be appreciated that this step does not increment the FIFO buffer.</p>
<p id="p-0085" num="0084">Returning to step <b>645</b>, the FIFO buffer is incremented with a new frame being added to the FIFO buffer. In some implementations, the size of the FIFO buffer is five frames. In step <b>650</b>, an average reference frame is created from the FIFO buffer. One way to create the reference frame is to average corresponding pixel values for all the frames within the FIFO buffer.</p>
<p id="p-0086" num="0085">In step <b>655</b>, a mask of good reference frame regions is created. The mask may be created by taking the difference of the maximum and minimum value of each pixel in the reference frame and determining if the difference in each pixel is less than a predetermined threshold, i.e., max (Ref. Frame(pixel)&#x2212;min (Ref. Frame(pixel))&#x3c;Threshold. This method may be used to determine cases where the scene intensity for a pixel is varying greatly across the frames in the FIFO. Ideally, the threshold may be set to call correction terms for each pixel &#x201c;good&#x201d; when the maximum minus minimum value for each pixel in the FIFO is below a threshold amount. Large variance for a pixel intensity for frames in the FIFO may potentially result in a poor correction term being applied. Thus, the mask is created to identify pixels that do not meet this condition and the correction term calculated can be applied. To do so, for instance, a threshold value may be set to 5% of the dynamic range of a pixel value.</p>
<p id="p-0087" num="0086">For all pixels in an image, the process proceeds to step <b>660</b> in which a determination is made whether the current pixel is on a high frequency edge in the reference frame. To determine this, the difference of the Near Region Avg Median and the Far Region Avg Median is compared for each pixel in the reference frame. If the absolute value of the difference between these values is less than a threshold then an &#x201c;edge&#x201d; is detected. If this is the case, the process goes to step <b>680</b>, discussed below. On the other hand, if this is not the case, the process proceeds to step <b>665</b>, where a correction term is updated, for example, as discussed above, with respect to <figref idref="DRAWINGS">FIG. 5</figref>.</p>
<p id="p-0088" num="0087">In step <b>670</b>, another determination may be made whether the current pixel is a blinking pixel. Blinking pixel detection is an optional feature and may be implemented during the correction term computation. The purpose is to detect pixels that are varying in value quickly over time not due to scene change. To detect blinking pixels, the algorithm looks at the change in the correction terms from one instance to the next. If the correction term radically changes values (from one update of the correction term to the next) this may be one indication that the pixel is blinking.</p>
<p id="p-0089" num="0088">Another check that may be performed makes sure that the correction terms of the surrounding neighbors (passed through a median process) also did not change quickly. If this is the case then it is likely not a blinking pixel and is a valid correction change. A final check may be performed to determine that the immediate surrounding pixels (that were passed through a median process) are relatively the same. If they are not the same, then this may indicate scene content and not a blinking pixel. If they are very close and the two above tests pass, then the pixel may be assumed to be a blinking pixel.</p>
<p id="p-0090" num="0089">If not, the process goes to step <b>680</b>, discussed below. For a blinking pixel, in step <b>675</b>, a blinker timer for the pixel is set to its maximum value. For instance, this may be 100 frames of video.</p>
<p id="p-0091" num="0090">Next, in step <b>680</b>, the blinker timer is decremented for all pixels in the current frame. The blinker timer may be decreased each iteration by 1. In step <b>685</b>, the correction terms are applied to the current frame. This may be accomplished by taking the difference between the reference frame and the product of the correction terms and the good mask reference, i.e., Frame&#x2212;(correction_terms*good_ref mask). Continuing to step <b>690</b>, if the &#x201c;blinker mode&#x201d; is activated then the pixels are replaced (or substituted) with pixels having a blinker timer value greater than zero. Processed frame <b>695</b> is output from process <b>600</b>, and processing returns to step <b>610</b> for another frame <b>605</b> of video data.</p>
<p id="p-0092" num="0091">Implementing using the above method is possible on many systems that operate on entire frames (images) at once. Many legacy systems, however, operate on individual video lines as they are read in. The above algorithm therefore may need to be modified slightly to conform to these system. For example, to modify the algorithm, several key items will now have a one frame &#x201c;lag&#x201d;. Those items may be: Movement Analysis through Scene Structure Metric. The entire frame method allows in x (typically 2) frames extra into the FIFO buffer once movement has stopped. This number may need to decrease by one frame (to account for the lag). The scene structure method will not measurably affect performance if it is one frame behind. Given these changes, the rest of the algorithm will calculate the following with only three video lines of delay (e.g., determined by largest kernel size used in the algorithms).</p>
<p id="p-0093" num="0092"><figref idref="DRAWINGS">FIGS. 7-10</figref> show exemplary video frames both before and after having been subjected to a FIFO buffered median scene non-uniformity correction method in accordance with an embodiment. Portions of the frames have been magnified for clarity.</p>
<p id="p-0094" num="0093"><figref idref="DRAWINGS">FIGS. 7A-10A</figref> show the raw videos frame prior to processing. The residual FPN appears in the raw image frame as random &#x201c;spots&#x201d;, &#x201c;lines&#x201d; and/or other visible artifacts (indicated in the figures with small surrounding circles).</p>
<p id="p-0095" num="0094">And, <figref idref="DRAWINGS">FIGS. 7B-10B</figref> show the same video frame after processing. As will be appreciated, the random &#x201c;spots&#x201d;, &#x201c;lines&#x201d; and/or other visible artifacts visible in <figref idref="DRAWINGS">FIGS. 7-10</figref> have generally been removed by the processing. FPN may be due to non-linear responsivity of pixels not being fully corrected by a conventional NUC methods. The FPN has been actively reduced, despite the presence of high frequency scene spatial frequencies.</p>
<p id="p-0096" num="0095"><figref idref="DRAWINGS">FIG. 11</figref> is a plot showing performance of the FIFO-buffered median scene based non-uniformity correction algorithm in accordance with an embodiment as compared with other conventional non-uniformity correction algorithms.</p>
<p id="p-0097" num="0096">Conventional high-frequency scene based non-uniformity correction (HFSBNUC) and nearest neighbor scene based non-uniformity correction (NNSBNUC) algorithms were compared to a FIFO-buffered median scene based non-uniformity correction method (FMSBNUC) algorithm in accordance with an embodiment.</p>
<p id="p-0098" num="0097">The evaluations were based on 31 &#x201c;standard&#x201d; video sets, including 16 long-wave infrared (LW IR) hot and cold scenes and 15 mid-wave infrared (MW IR) hot and cold scenes. The hot and cold scenes were measured at temperatures of approximately 0 and 40 degrees C., respectively. In general, evaluation scenes were chosen as to provide a large range of scene intensity values across the dynamic range of the imaging sensor used in test.</p>
<p id="p-0099" num="0098">The FMSBNUC performance (reduction in FPN) was approximately 50-60% higher than that of the HFSBNUC and the NNSBNUC. In the plot, the higher the average performance level indicated a better improvement. In addition, there was a significant improvement in the measured errors for FMSBNUC. For HFSBNUC, there were 6 minor errors measured and no moderate errors. For NNSBNUC, there were 45 minor errors and 18 moderate errors. And, for FMSBNUC, there was one moderate error and no mirror errors. Both the HFSBNUC and the NNSBNUC has &#x201c;zero&#x201d; measured improvement for the MW IR scenes.</p>
<p id="p-0100" num="0099">Advantages of the FIFO buffered median scene non-uniformity correction method, described above, include (i) removal of high spatial frequency FPN; (ii) handling both regions of high spatial frequency scene content and regions of uniform scene in images; (iii) removal of column amplifier DC offsets which may be relatively high in spatial frequency; and (iv) identifies and corrects blinking (and/or other noisy pixels). In one embodiment, the FIFO buffered median scene non-uniformity correction method never replaces pixels due to non-uniformities under any circumstances, and instead always tries to correct residual FPN By contrast, replacing pixels always causes image blur which reduces image quality and is not desired.</p>
<p id="p-0101" num="0100">Other embodiments, uses and advantages of the inventive concept will be apparent to those skilled in the art from consideration of the above disclosure and the following claims. The specification should be considered non-limiting and exemplary only, and the scope of the inventive concept is accordingly intended to be limited only by the scope of the following claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A buffered scene-based non-uniformity correction method comprising:
<claim-text>receiving a plurality of frames of video image data from an image detector;</claim-text>
<claim-text>determining relative movement of a current image frame with respect to a previous image frame;</claim-text>
<claim-text>if there is substantial movement determined, adding the current image frame to a buffer in a memory sized to store a predetermined number of video frames;</claim-text>
<claim-text>averaging pixel values of the frames in the buffer to determine a mean value for each pixel of a reference image;</claim-text>
<claim-text>determining correction terms for each pixel of the current image frame by determining the difference between the current image frame pixel value and the reference image; and</claim-text>
<claim-text>correcting the current image frame using the correction terms.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the buffer is a first-in-first-out (FIFO) buffer.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the buffer is configured to hold five frames of video.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>responsive to a determination that the current frame includes a uniform scene with low spatial frequency content, adding the current image frame to the buffer.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>responsive to a determination that there has been continuous motion in previous frames which has stopped in the current frame, adding one or more additional frames to the buffer corresponding to the current frame.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>identifying and removing blinking pixels from the images.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining correction term values for each pixel comprises:
<claim-text>determining pixels in a near region to the current pixel;</claim-text>
<claim-text>determining pixels in a far region to the current pixel; and</claim-text>
<claim-text>analyzing the near and far region pixels.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining relative movement of a current image frame with respect to the previous image frame comprises:
<claim-text>determining row and column sum vectors for the current image from and the previous image frame; and</claim-text>
<claim-text>analyzing the determined row and column sum vectors to determine a correlation between the current image frame and the previous image frame.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein analyzing the determined row and column sum vectors comprises:
<claim-text>searching for movement between the current image frame and the previous image frame within a predetermined distance from each current pixel.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein regions in the current frame which have high-frequency or dynamic range scene content are not corrected.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the pixel values of the frames in the buffer are continuously averaged when new frames are added to the buffer.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the mean value is a weighted mean value.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. A buffered scene-based non-uniformity correction system comprising:
<claim-text>a buffer memory sized to store a predetermined number of video frames; and</claim-text>
<claim-text>a processor configured to:
<claim-text>receive a plurality of frames of video image data from an image detector;</claim-text>
<claim-text>determine relative movement of a current image frame with respect to a previous image frame;</claim-text>
<claim-text>if there is substantial movement determined, add the current image frame to the buffer;</claim-text>
<claim-text>average pixel values of the frames in the buffer to determine a mean value for each pixel of a reference image;</claim-text>
<claim-text>determine correction terms for each pixel of the current image frame by determining the difference between the current image frame pixel value and the reference image; and</claim-text>
<claim-text>correct the current image frame using the correction terms.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The system according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising:
<claim-text>a image detector configured to generated the video data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The system according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the buffer is a first-in-first-out (FIFO) buffer.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The system according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the buffer is configured to hold five frames of video.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The system according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the processor is configured to:
<claim-text>responsive to a determination that the current frame includes a uniform scene with low spatial frequency content, add the current image frame to the buffer.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The system according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the processor is configured to:
<claim-text>responsive to a determination that there has been continuous motion in previous frames which has stopped in the current frame, add one or more additional frames to the buffer corresponding to the current frame.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The system according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the processor is configured to:
<claim-text>Identify and remove blinking pixels from the images.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The system according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein, in determining correction term values for each pixel, the processor is configured to:
<claim-text>determine pixels in a near region to the current pixel;</claim-text>
<claim-text>determine pixels in a far region to the current pixel; and</claim-text>
<claim-text>analyze the near and far region pixels.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The system according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein, in determining relative movement of a current image frame with respect to the previous image frame, the processor is configured to:
<claim-text>determine row and column sum vectors for the current image from and the previous image frame; and</claim-text>
<claim-text>analyze the determined row and column sum vectors to determine a correlation between the current image frame and the previous image frame.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The system according to <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein, in analyzing the determined row and column sum vectors, the processor is configured to:
<claim-text>search for movement between the current image frame and the previous image frame within a predetermined distance from each current pixel.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The system according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein regions in the current frame which have high-frequency or dynamic range scene content are not corrected.</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. The system according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the pixel values of the frames in the buffer are continuously averaged when new frames are added to the buffer.</claim-text>
</claim>
<claim id="CLM-00025" num="00025">
<claim-text>25. The system according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the mean value is a weighted mean value. </claim-text>
</claim>
</claims>
</us-patent-grant>
