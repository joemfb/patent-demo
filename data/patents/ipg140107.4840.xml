<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625933-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625933</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12571658</doc-number>
<date>20091001</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>JP</country>
<doc-number>2008-258452</doc-number>
<date>20081003</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>1133</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>32</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382298</main-classification>
<further-classification>382181</further-classification>
<further-classification>382291</further-classification>
</classification-national>
<invention-title id="d2e71">Image processing apparatus and method for the same</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>6169966</doc-number>
<kind>B1</kind>
<name>Miura et al.</name>
<date>20010100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>702142</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>7113616</doc-number>
<kind>B2</kind>
<name>Ito et al.</name>
<date>20060900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>7548269</doc-number>
<kind>B2</kind>
<name>Yata</name>
<date>20090600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348352</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>7783184</doc-number>
<kind>B2</kind>
<name>Chi</name>
<date>20100800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>396 80</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>7899208</doc-number>
<kind>B2</kind>
<name>Kondo et al.</name>
<date>20110300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2010/0085385</doc-number>
<kind>A1</kind>
<name>Nagamasa</name>
<date>20100400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345660</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>JP</country>
<doc-number>2003-235035</doc-number>
<kind>A</kind>
<date>20030800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>JP</country>
<doc-number>2004-171490</doc-number>
<kind>A</kind>
<date>20040600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>JP</country>
<doc-number>2005-267512</doc-number>
<kind>A</kind>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>JP</country>
<doc-number>2007-025899</doc-number>
<kind>A</kind>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>JP</country>
<doc-number>2007-304852</doc-number>
<kind>A</kind>
<date>20071100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>15</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382181</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382291</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>10</number-of-drawing-sheets>
<number-of-figures>16</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20100085385</doc-number>
<kind>A1</kind>
<date>20100408</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Nagamasa</last-name>
<first-name>Yoshinobu</first-name>
<address>
<city>Kawasaki</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Nagamasa</last-name>
<first-name>Yoshinobu</first-name>
<address>
<city>Kawasaki</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Canon U.S.A., Inc. IP Division</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Canon Kabushiki Kaisha</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Desire</last-name>
<first-name>Gregory M</first-name>
<department>2668</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">An image processing apparatus can detect a predetermined target object from image data. The image processing apparatus includes an image zooming unit configured to generate a plurality of pieces of zoomed image data that are mutually different in magnification from the image data input by an image inputting unit, a detection unit configured to extract a partial area from the plurality of pieces of zoomed image data generated by the image zooming unit, and detect the predetermined target object by performing collation to determine whether the extracted partial area coincides with a detection pattern stored in a detected pattern storage unit, and a detected information storage unit configured to store detection information including magnification information of the zoomed image data from which the predetermined target object is detected by the detection unit. In a case where the detection information is stored in the detected information storage unit, the image zooming unit determines a magnification of the zoomed image data based on the detection information and generates at least one piece of zoomed image data whose total number is smaller compared to a case where the detection information is not stored.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="101.52mm" wi="230.63mm" file="US08625933-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="238.93mm" wi="127.00mm" orientation="landscape" file="US08625933-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="242.15mm" wi="172.47mm" file="US08625933-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="204.81mm" wi="171.20mm" file="US08625933-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="241.72mm" wi="133.86mm" orientation="landscape" file="US08625933-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="240.28mm" wi="176.61mm" file="US08625933-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="207.09mm" wi="170.69mm" file="US08625933-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="244.43mm" wi="154.26mm" orientation="landscape" file="US08625933-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="252.56mm" wi="170.43mm" file="US08625933-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="227.16mm" wi="149.78mm" orientation="landscape" file="US08625933-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="243.08mm" wi="176.19mm" file="US08625933-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0002" num="0001">1. Field of the Invention</p>
<p id="p-0003" num="0002">The present invention relates to image processing, and more particularly to a technique capable of detecting an imaging object pattern from a moving image.</p>
<p id="p-0004" num="0003">2. Description of the Related Art</p>
<p id="p-0005" num="0004">Conventionally, an image processing method which can automatically detect a predetermined imaging object pattern from a moving image has been discussed and the method can be used to determine, for example, a human face or the like. This kind of image processing method can be used in a video conference, a monitoring system, or the like.</p>
<p id="p-0006" num="0005">Such techniques that can be used to detect a predetermined imaging object pattern from an image are, for example, discussed in Japanese Patent Application Laid-Open No. 2007-25899, Japanese Patent Application Laid-Open No. 2004-171490, and Japanese Patent Application Laid-Open No. 2003-235035 according to which the predetermined imaging object pattern can be detected using a template matching technique.</p>
<p id="p-0007" num="0006">However, according to the technique discussed in Japanese Patent Application Laid-Open No. 2007-25899, a burden in calculation processing increases if the system generates a plurality of reduced images from an input image on a frame-by-frame basis and performs matching between a predetermined imaging object pattern and the template. Accordingly, unless an employed hardware has high-speed processing capabilities, it will be required to use the method discussed in Japanese Patent Application Laid-Open No. 2004-171490 to perform detection processing according to the template matching at constant frame intervals so that the calculations for processing can be smoothly executed.</p>
<p id="p-0008" num="0007">In this case, for example, if a zoomed-up image of a predetermined imaging object is captured by a camera, the system may not perform tracking using panning and tilting functions when the predetermined imaging object exits from the frame between non-processed frames.</p>
<p id="p-0009" num="0008">Further, according to the technique discussed in Japanese Patent Application Laid-Open No. 2004-171490, the system performs normal detection processing at constant frame intervals and inserts a frame that detects only a position adjacent to a coordinate of the already detected imaging object between two frames at which the normal detection processing is performed. This system intends to reduce the burden in calculations for the template matching processing.</p>
<p id="p-0010" num="0009">However, the imaging object may not be captured in a detection area if a moving speed of the imaging object is inappropriate. The imaging object cannot be detected in this case. Further, in a zoom-up operation of the camera, or in a situation where a distance between the imaging object and the camera becomes shorter, the imaging object may not be detected based on only the detection at or near the coordinate position of the previous detection frame.</p>
<p id="p-0011" num="0010">Further, according to the technique discussed in Japanese Patent Application Laid-Open No. 2003-235035, the system obtains a difference between an input image and a background image and registers the obtained difference as a template. In this case, a predetermined imaging object in the template is identical to a predetermined imaging object in the original input image.</p>
<p id="p-0012" num="0011">If the camera performs a zooming control, the system performs conversion in resolution based on the zooming parameter to equalize in size the predetermined imaging object of the input image with the predetermined imaging object of the template, so that the imaging object pattern can be detected.</p>
<p id="p-0013" num="0012">However, this technique is only effective in an operation for tracking an imaging object that has been captured in the input image and cannot be used in a situation where the distance between the imaging object and the camera is variable. Accordingly, detection accuracy deteriorates when the distance between the imaging object and the camera is changed.</p>
<heading id="h-0002" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0014" num="0013">Exemplary embodiments of the present invention are directed to a technique capable of reducing a burden in calculations for detecting an imaging object pattern and capable of maintaining accuracy in detection of the imaging object pattern.</p>
<p id="p-0015" num="0014">According to an aspect of the present invention, an image processing apparatus can detect a predetermined target object from image data. The image processing apparatus includes an image inputting unit configured to input the image data, a detected pattern storage unit configured to store a detection pattern representing the predetermined target object, an image zooming unit configured to generate a plurality of pieces of zoomed image data that are mutually different in magnification from the image data input by the image inputting unit, a detection unit configured to extract a partial area from the plurality of pieces of zoomed image data generated by the image zooming unit, and detect the predetermined target object by performing collation to determine whether the extracted partial area coincides with the detection pattern stored in the detected pattern storage unit, and a detected information storage unit configured to store detection information including magnification information of the zoomed image data from which the predetermined target object is detected by the detection unit. In a case where the detection information is stored in the detected information storage unit, the image zooming unit determines a magnification of the zoomed image data based on the detection information and generates at least one piece of zoomed image data whose total number is smaller compared to a case where the detection information is not stored.</p>
<p id="p-0016" num="0015">Further features and aspects of the present invention will become apparent from the following detailed description of exemplary embodiments with reference to the attached drawings.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0017" num="0016">The accompanying drawings, which are incorporated in and constitute a part of the specification, illustrate exemplary embodiments, features, and aspects of the invention and, together with the description, serve to explain the principles of the invention.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram illustrating an example of the configuration of an image processing apparatus according to a first exemplary embodiment of the present invention.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIGS. 2A and 2B</figref> are flowcharts illustrating an example of processing procedure for detecting a predetermined pattern which can be performed by the image processing apparatus according to the first exemplary embodiment.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIGS. 3A and 3B</figref> illustrate an example of collating processing to be performed to check whether a collation object pattern coincides with a detection pattern with respect to various reduced images that are different in size according to the first exemplary embodiment.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 4</figref> is a block diagram illustrating an example of the configuration of an image processing apparatus according to a second exemplary embodiment of the present invention.</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIGS. 5A and 5B</figref> are flowcharts illustrating an example of a processing procedure for detecting a predetermined pattern which can be performed by the image processing apparatus according to the second exemplary embodiment.</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIGS. 6A and 6B</figref> illustrate an example of collating processing to be performed to check whether a collation object pattern coincides with a detection pattern with respect to various reduced images that are different in size according to the second exemplary embodiment.</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 7</figref> is a block diagram illustrating an example of the configuration of an image processing apparatus according to a third exemplary embodiment of the present invention.</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIGS. 8A and 8B</figref> are flowcharts illustrating an example of a processing procedure for detecting a predetermined pattern which can be performed by the image processing apparatus according to the third exemplary embodiment.</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 9</figref> is a block diagram illustrating an example of the configuration of an image processing apparatus according to a fourth exemplary embodiment of the present invention.</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIGS. 10A and 10B</figref> are flowcharts illustrating an example of a processing procedure for detecting a predetermined pattern which can be performed by image processing apparatus according to the fourth exemplary embodiment.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DESCRIPTION OF THE EMBODIMENTS</heading>
<p id="p-0028" num="0027">Various exemplary embodiments, features, and aspects of the present invention will now be herein described in detail below with reference to the drawings. It is to be noted that the relative arrangement of the components, the numerical expressions, and numerical values set forth in these embodiments are not intended to limit the scope of the present invention.</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram illustrating an example of the configuration of an image processing apparatus <b>1</b> according to an exemplary embodiment of the present invention. First, the configuration of the image processing apparatus <b>1</b> according to the present exemplary embodiment is described below.</p>
<p id="p-0030" num="0029">In <figref idref="DRAWINGS">FIG. 1</figref>, an image inputting unit <b>11</b> receives image data. The image inputting unit <b>11</b> has a decoding processing function for receiving image data that may be conformable to a specific communication method and compatible with a marker code-attached protocol or compression processed image data such as Joint Photographic Experts Group (JPEG)/Motion Joint Photographic Experts Group 4 (MPEG4). In the present exemplary embodiment, the image data include photographic image data captured and generated by an imaging apparatus and image data stored in an external storage apparatus. An output source is not limited to a specific device.</p>
<p id="p-0031" num="0030">An image memory <b>12</b> can be configured by a random access memory (RAM) or other readable and writable storage apparatus. The image memory <b>12</b> stores the image data received by the image inputting unit <b>11</b> and zooming processed image data supplied from an image zooming unit <b>13</b>. The image zooming unit <b>13</b> can read the image data stored in the image memory <b>12</b> and generate reduced image data (i.e., zoomed image data). The image zooming unit <b>13</b> can write the reduced image data into the image memory <b>12</b>.</p>
<p id="p-0032" num="0031">A collation object pattern extraction unit <b>14</b> can move a rectangular area which has a predetermined size on the reduced image data stored in the image memory <b>12</b> and successively extract and output a portion (i.e., a pixel group) included in the rectangular area as a collation object pattern. In the present exemplary embodiment, an original image can be regarded as an image having a reduced size of 1/1 and can be interpreted as one of the reduced images.</p>
<p id="p-0033" num="0032">A detected pattern storage unit <b>15</b> can store beforehand a detection pattern that represents a predetermined pattern (target object) to be detected from an input image. A pattern detection unit <b>16</b> performs collation based on the detection pattern stored in the detected pattern storage unit <b>15</b> to determine whether the collation object pattern coincides with the predetermined pattern. A detected information storage unit <b>17</b> can receive magnification information relating to a zooming ratio of the reduced image which is used when the predetermined pattern is detected by the pattern detection unit <b>16</b>. The detected information storage unit <b>17</b> stores the magnification information received from the pattern detection unit <b>16</b>.</p>
<p id="p-0034" num="0033">Next, an example of processing that can be realized by the functional components illustrated in <figref idref="DRAWINGS">FIG. 1</figref> is described below with reference to flowcharts illustrated in <figref idref="DRAWINGS">FIGS. 2A and 2B</figref>. The image processing apparatus <b>1</b> can provide two types of detection modes (i.e., a normal detection mode and a high-speed detection mode) which can be selected in detection of the predetermined pattern. It is now assumed that the image processing apparatus <b>1</b> starts its operation in the normal detection mode.</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 2A</figref> is a flowchart illustrating an example of a processing procedure for detecting a predetermined pattern, which can be performed by the image processing apparatus <b>1</b> that operates in the normal detection mode according to the present exemplary embodiment.</p>
<p id="p-0036" num="0035">First, after starting the processing, in step S<b>101</b>, the image inputting unit <b>11</b> receives image data. In this case, if the received image data is the image data that may be conformable to the specific communication method and compatible with the marker code-attached protocol or the compression processed image data such as JPEG/MPEG4, the image inputting unit <b>11</b> performs decoding processing on the received image data.</p>
<p id="p-0037" num="0036">Next, in step S<b>102</b>, the image inputting unit <b>11</b> writes the processing result as input image data into the image memory <b>12</b>. When the decoding processing is not required, the image inputting unit <b>11</b> directly writes the received image data as input image data into the image memory <b>12</b>.</p>
<p id="p-0038" num="0037">Next, in step S<b>103</b>, the image zooming unit <b>13</b> reads the input image data stored in the image memory <b>12</b> and generates n (n&#x2267;2) pieces of reduced image data. The image zooming unit <b>13</b> writes the generated reduced image data into the image memory <b>12</b>.</p>
<p id="p-0039" num="0038">For example, it is assumed that the input image is of a Video Graphic Array (VGA) size (640 pixels in the horizontal direction and 480 pixels in the vertical direction). The image zooming unit <b>13</b> reduces the input VGA data and generates first reduced image A<b>1</b> data which has a size of 320 pixels in the horizontal direction and 240 pixels in the vertical direction. The image zooming unit <b>13</b> writes the first reduced image A<b>1</b> data into the image memory <b>12</b>.</p>
<p id="p-0040" num="0039">Moreover, the image zooming unit <b>13</b> reads the reduced image A<b>1</b> data from the image memory <b>12</b> and generates second reduced image A<b>2</b> data which has a size comparable to 0.8 times the reduced image A<b>1</b> data in both the horizontal and vertical directions. The image zooming unit <b>13</b> writes the second reduced image A<b>2</b> data into the image memory <b>12</b>. The image zooming unit <b>13</b> repeats similar reduction processing until n-th reduced image An data can be obtained.</p>
<p id="p-0041" num="0040">In the present exemplary embodiment, a zooming ratio (i.e., a magnification of an image relative to the input image) in generation of the reduced image data is a mere example. Further, the value n can be changed considering various conditions, such as the size of the input image or the size of the detection pattern. Alternatively, the value n can be a fixed value. Hereinafter, the zooming ratios for generating the data of the reduced images A<b>1</b> to An can be referred to as magnifications A<b>1</b> to An.</p>
<p id="p-0042" num="0041">Next, in step S<b>104</b>, the collation object pattern extraction unit <b>14</b> moves a rectangular area having a predetermined size on the data of the reduced images A<b>1</b> to An in a direction indicated by an arrow, as illustrated in <figref idref="DRAWINGS">FIG. 3A</figref>, and successively extracts a portion (i.e., a pixel group) included in the rectangular area as a collation object pattern. Then, the portions (i.e., pixel groups) extracted by the collation object pattern extraction unit <b>14</b> are output to the pattern detection unit <b>16</b>.</p>
<p id="p-0043" num="0042">In the present exemplary embodiment, the &#x201c;predetermined size&#x201d; can be arbitrarily determined to be a size equal to or greater than the reduced image An, although the rectangular area illustrated in <figref idref="DRAWINGS">FIG. 3A</figref> is similar in size to the detection pattern. The procedure indicated by the arrow is a mere example. The procedure for the successive extraction may be arbitrarily changed.</p>
<p id="p-0044" num="0043">The pattern detection unit <b>16</b> successively performs collation to determine whether each collation object pattern extracted by the collation object pattern extraction unit <b>14</b> coincides with the predetermined pattern, referring to the detection pattern stored in the detected pattern storage unit <b>15</b>. As described above, in the normal detection mode, the image processing apparatus <b>1</b> can generate a total of n pieces of reduced image data and perform collation on the generated n pieces of reduced image data.</p>
<p id="p-0045" num="0044">Next, in step S<b>105</b>, it is determined whether the predetermined pattern has been detected referring to a result of the collation performed on the reduced images A<b>1</b> to An. If it is determined that the predetermined pattern has not been detected (NO in step S<b>105</b>), the processing returns to step S<b>101</b> to repeat the processing of steps S<b>101</b> to S<b>104</b>. On the other hand, if it is determined that the predetermined pattern has been detected (YES in step S<b>105</b>), the processing proceeds to step S<b>106</b>.</p>
<p id="p-0046" num="0045">For example, it is assumed that in step S<b>105</b> the pattern detection unit <b>16</b> has detected the predetermined pattern from the reduced image A<b>6</b> (i.e., the sixth reduced image). In this case, in step S<b>106</b>, the detected information storage unit <b>17</b> receives information indicating a magnification A<b>6</b> of the reduced image A<b>6</b>, which is a zooming ratio relative to the input image, from the pattern detection unit <b>16</b>. The detected information storage unit <b>17</b> stores the received zooming ratio information. Then, after the zooming ratio information is stored in the detected information storage unit <b>17</b>, the image processing apparatus <b>1</b> terminates the operation in the normal detection mode and shifts its operation mode to the high-speed detection mode.</p>
<p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. 2B</figref> is a flowchart illustrating an example of a processing procedure for detecting a predetermined pattern, which can be performed by the image processing apparatus <b>1</b> that operates in the high-speed detection mode according to the present exemplary embodiment.</p>
<p id="p-0048" num="0047">First, in step S<b>107</b>, the image inputting unit <b>11</b> receives image data of the next latest frame. Next, in step S<b>108</b>, similar to step S<b>102</b>, the image inputting unit <b>11</b> writes the received image data as input image data into the image memory <b>12</b>.</p>
<p id="p-0049" num="0048">Next, in step S<b>109</b>, the image zooming unit <b>13</b> reads the input image data stored in the image memory <b>12</b> and generates m (n&#x3e;m&#x2267;1) pieces of reduced image data. The image zooming unit <b>13</b> writes the generated reduced image data into the image memory <b>12</b>. The reduction magnification for generating the reduced image data can be determined based on the zooming ratio stored in the detected information storage unit <b>17</b>.</p>
<p id="p-0050" num="0049">Alternatively, the detected information storage unit <b>17</b> may store image size information instead of the zooming ratio information and can determine a reduction magnification based on the image size information. In the present exemplary embodiment, the zooming ratio of the reduced image from which the predetermined pattern has been detected is the magnification A<b>6</b>. Thus, the detected information storage unit <b>17</b> stores the information indicating the magnification A<b>6</b>.</p>
<p id="p-0051" num="0050">For example, if m=1, the image zooming unit <b>13</b> generates a piece of reduced image B<b>1</b> (=reduced image Bm) data from the input image data using the magnification A<b>6</b> stored in the detected information storage unit <b>17</b>. The image zooming unit <b>13</b> writes the generated reduced image B<b>1</b> data into the image memory <b>12</b>.</p>
<p id="p-0052" num="0051">Further, if m=3, the image zooming unit <b>13</b> generates the reduced image B<b>1</b> data from the input image data using a magnification A<b>5</b> that is a zooming ratio comparable to 1.25 times the magnification A<b>6</b> stored in the detected information storage unit <b>17</b>. The image zooming unit <b>13</b> writes the generated reduced image B<b>1</b> data into the image memory <b>12</b>.</p>
<p id="p-0053" num="0052">Then, the image zooming unit <b>13</b> reads the reduced image B<b>1</b> data from the image memory <b>12</b> and generates reduced image B<b>2</b> data which has a size comparable to 0.8 times the reduced image B<b>1</b> data in both the horizontal and vertical directions. The image zooming unit <b>13</b> writes the second reduced image B<b>2</b> data into the image memory <b>12</b>. The zooming ratio in this case is equal to the magnification A<b>6</b> stored in the detected information storage unit <b>17</b>.</p>
<p id="p-0054" num="0053">Subsequently, the image zooming unit <b>13</b> reads the reduced image B<b>2</b> data from the image memory <b>12</b> and generates reduced image B<b>3</b> (=reduced image Bm) data which has a size comparable to 0.8 times the reduced image B<b>2</b> data in both the horizontal and vertical directions. The image zooming unit <b>13</b> writes the generated third reduced image B<b>3</b> data into the image memory <b>12</b>. The zooming ratio in this case is equal to the magnification A<b>7</b>.</p>
<p id="p-0055" num="0054">As described above, if m&#x2260;1, the image zooming unit <b>13</b> generates m pieces of reduced image data using a zooming ratio adjacent to the zooming ratio stored in the detected information storage unit <b>17</b> among the n pieces of zooming ratios in the generation of n pieces of reduced image data.</p>
<p id="p-0056" num="0055">Next, in step S<b>110</b>, the collation object pattern extraction unit <b>14</b> moves a rectangular area having a predetermined size on the data of the reduced images B<b>1</b> to Bm in a direction indicated by an arrow, as illustrated in <figref idref="DRAWINGS">FIG. 3B</figref>, and successively extracts a portion (i.e., a pixel group) included in the rectangular area as a collation object pattern. Then, the portions (i.e., pixel groups) extracted by the collation object pattern extraction unit <b>14</b> are output to the pattern detection unit <b>16</b>.</p>
<p id="p-0057" num="0056">The pattern detection unit <b>16</b> successively performs collation to determine whether each collation object pattern extracted by the collation object pattern extraction unit <b>14</b> coincides with the predetermined pattern, referring to the detection pattern stored in the detected pattern storage unit <b>15</b>.</p>
<p id="p-0058" num="0057">The processing in step S<b>110</b> is different from the above-described processing in step S<b>104</b> in that the total number of the reduced image data is m that is smaller than n.</p>
<p id="p-0059" num="0058">As described above, in the high-speed detection mode, the image processing apparatus <b>1</b> can generate a total of m pieces of reduced image data and perform collation on the generated m pieces of reduced image data.</p>
<p id="p-0060" num="0059">Next, in step S<b>111</b>, it is determined whether the predetermined pattern has been detected referring to a result of the collation performed on the reduced images B<b>1</b> to Bm. If it is determined that the predetermined pattern has not been detected (NO in step S<b>111</b>), the image processing apparatus <b>1</b> terminates the operation in the high-speed detection mode. Then, the detected information storage unit <b>17</b> deletes the stored information indicating the magnification A<b>6</b>. The image processing apparatus <b>1</b> starts its operation in the normal detection mode. On the other hand, if it is determined that the predetermined pattern has been detected (YES in step S<b>111</b>), the processing proceeds to step S<b>112</b>.</p>
<p id="p-0061" num="0060">For example, it is assumed that in step S<b>111</b> the pattern detection unit <b>16</b> has detected the predetermined pattern from the reduced image B<b>3</b> (=reduced image Bm) that is the third reduced image (m=3). In this case, in step S<b>112</b>, the detected information storage unit <b>17</b> deletes the stored information indicating the magnification A<b>6</b>. The detected information storage unit <b>17</b> receives information indicating a magnification B<b>3</b> of the reduced image B<b>3</b>, which is a zooming ratio relative to the input image, from the pattern detection unit <b>16</b>. The detected information storage unit <b>17</b> newly stores the received zooming ratio information. Then, the processing returns to step S<b>107</b> to execute the processing of steps S<b>107</b> to S<b>111</b>, similar to the above-described procedure.</p>
<p id="p-0062" num="0061">In the present exemplary embodiment, the pattern detection unit <b>16</b> may have luminance correction processing, density correction processing, and background removal processing functions, which are conventionally known. In this case, the pattern detection unit <b>16</b> can perform the luminance correction processing, the density correction processing, and the background removal processing on the collation object patterns. This is effective to improve accuracy of the collation performed by the pattern detection unit <b>16</b>.</p>
<p id="p-0063" num="0062">Further, it is useful to separately provide an image processing unit configured to perform the luminance correction processing, the density correction processing, and the background removal processing on the input image or the reduced images. This is also effective to improve the accuracy of the collation performed by the pattern detection unit <b>16</b>.</p>
<p id="p-0064" num="0063">Further, in the collation performed by the pattern detection unit <b>16</b>, it is useful to use only the luminance component of an image. In this case, the image zooming unit <b>13</b> can extract only the luminance component and generate reduced image data based on the extracted luminance component.</p>
<p id="p-0065" num="0064">As described above, if there is no zooming ratio stored in the detected information storage unit <b>17</b>, the present exemplary embodiment uses a total of n (n&#x2267;2) pieces of reduced images that are sufficient for detecting a predetermined pattern on condition that the size of a predetermined imaging object on the input image is unclear.</p>
<p id="p-0066" num="0065">For example, when the image inputting unit <b>11</b> receives the first frame, it is determined that no zooming ratio is present (stored) in the detected information storage unit <b>17</b>. On the other hand, if there is any zooming ratio stored in the detected information storage unit <b>17</b>, the present exemplary embodiment uses m (n&#x3e;m&#x2267;1) pieces of reduced images to detect the predetermined pattern considering the situation that the size of the predetermined imaging object on the input image is already known.</p>
<p id="p-0067" num="0066">As described above, the present exemplary embodiment limits the zooming ratio to be used to generate reduced image data referring to the zooming ratio stored in the detected information storage unit <b>17</b>. Therefore, the present exemplary embodiment can reduce the amount of reduced image data to be generated to detect the predetermined pattern. Therefore, the present exemplary embodiment can reduce a burden in calculation processing by decreasing the processing amount in the zooming processing as well as in the collation. Thus, the present exemplary embodiment can speedily detect the predetermined imaging object.</p>
<p id="p-0068" num="0067">Further, the present exemplary embodiment can maintain the detection accuracy because of m&#x3e;1 even when the zooming processing amount is decreased and when the size of predetermined imaging object on the input image is changed. Moreover, the image processing apparatus <b>1</b> according to the present exemplary embodiment is configured to receive image data from an external device. However, similar effects can be obtained even in a case where the image processing apparatus <b>1</b> is integrated with an imaging apparatus or other image data outputting source.</p>
<p id="p-0069" num="0068">A second exemplary embodiment is described below. <figref idref="DRAWINGS">FIG. 4</figref> is a block diagram illustrating an example of the configuration of an image processing apparatus <b>20</b> according to the present exemplary embodiment. First, an example of the configuration of the image processing apparatus <b>20</b> according to the present exemplary embodiment is described below.</p>
<p id="p-0070" num="0069">In <figref idref="DRAWINGS">FIG. 4</figref>, an image inputting unit <b>21</b> receives image data from an external imaging apparatus <b>2</b>. Further, the image inputting unit <b>21</b> has a decoding processing function for receiving image data that may be conformable to a specific communication method and compatible with a marker code-attached protocol or compression processed image data such as JPEG/MPEG4.</p>
<p id="p-0071" num="0070">The imaging apparatus <b>2</b> is a general camera that includes a charge-coupled device (CCD) or a complementary metal oxide semiconductor (CMOS) configured to receive incident light via a zoom lens that can change a photographic angle of view. The imaging apparatus <b>2</b> can generate photographic image data constituted by digital data that are photo-electrically converted by the CCD or CMOS. The imaging apparatus <b>2</b> can output photographic parameters including at least a zooming magnification, together with the photographic image data, to the image processing apparatus <b>20</b>.</p>
<p id="p-0072" num="0071">A photographic parameter storage unit <b>28</b> can receive the photographic parameters output from the imaging apparatus <b>2</b> and store the received photographic parameters as parameter information. A general network line, such as a local area network (LAN), or a dedicated cable can be used to transmit or receive various data between the imaging apparatus <b>2</b> and the image processing apparatus <b>20</b>.</p>
<p id="p-0073" num="0072">An image memory <b>22</b> can be configured by a readable and writable storage apparatus (e.g., a RAM). The image memory <b>22</b> stores the image data received by the image inputting unit <b>21</b> and zooming processed image data supplied from an image zooming unit <b>23</b>. The image zooming unit <b>23</b> can read the image data stored in the image memory <b>22</b> and generate reduced image data (i.e., zoomed image data). The image zooming unit <b>23</b> can write the reduced image data into the image memory <b>22</b>.</p>
<p id="p-0074" num="0073">A collation object pattern extraction unit <b>24</b> can move a rectangular area having a predetermined size on the reduced image data stored in the image memory <b>22</b> and successively extract and output a portion (i.e., a pixel group) included in the rectangular area as a collation object pattern. In the present exemplary embodiment, an original image can be regarded as an image having a reduced size of 1/1 and can be interpreted as one of the reduced images.</p>
<p id="p-0075" num="0074">A detected pattern storage unit <b>25</b> can store beforehand a detection pattern that represents a predetermined pattern (target object) to be detected from an input image. A pattern detection unit <b>26</b> performs collation based on the detection pattern stored in the detected pattern storage unit <b>25</b> to determine whether the collation object pattern coincides with the predetermined pattern. A detected information storage unit <b>27</b> can receive magnification information relating to a zooming ratio of the reduced image which is used when the predetermined pattern is detected by the pattern detection unit <b>26</b>. The detected information storage unit <b>27</b> stores the magnification information received from the pattern detection unit <b>26</b>.</p>
<p id="p-0076" num="0075">Next, an example of processing that can be realized by the functional components illustrated in <figref idref="DRAWINGS">FIG. 4</figref> is described below with reference to flowcharts illustrated in <figref idref="DRAWINGS">FIGS. 5A and 5B</figref>. The image processing apparatus <b>20</b> can provide two types of detection modes (i.e., the normal detection mode and the high-speed detection mode) which can be selected in detection of the predetermined pattern. It is now assumed that the image processing apparatus <b>20</b> starts its operation in the normal detection mode.</p>
<p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. 5A</figref> is a flowchart illustrating an example of a processing procedure for detecting a predetermined pattern, which can be performed by the image processing apparatus <b>20</b> that operates in the normal detection mode according to the present exemplary embodiment.</p>
<p id="p-0078" num="0077">First, after starting the processing, in step S<b>201</b>, the image inputting unit <b>21</b> receives image data from the imaging apparatus <b>2</b>. The photographic parameter storage unit <b>28</b> receives the photographic parameters relating to the image data received by the image inputting unit <b>21</b>.</p>
<p id="p-0079" num="0078">Next, in step S<b>202</b>, the image inputting unit <b>21</b> writes the input image data into the image memory <b>22</b>. In this case, if the received image data is the image data that may be conformable to the specific communication method and compatible with the marker code-attached protocol, the image inputting unit <b>21</b> performs marker code removal or similar decoding processing on the received image data. The image inputting unit <b>21</b> writes the processing result as input image data into the image memory <b>22</b>.</p>
<p id="p-0080" num="0079">Further, if the received image data is the compression processed image data such as JPEG/MPEG4, the image inputting unit <b>21</b> performs decompression processing or similar decoding processing on the received image data. The image inputting unit <b>21</b> writes the processing result as input image data into the image memory <b>22</b>.</p>
<p id="p-0081" num="0080">When the decoding processing is not required, the image inputting unit <b>21</b> directly writes the received image data as input image data into the image memory <b>22</b>. Further, the photographic parameter storage unit <b>28</b> stores the received photographic parameters.</p>
<p id="p-0082" num="0081">Next, in step S<b>203</b>, the image zooming unit <b>23</b> reads the input image data stored in the image memory <b>22</b> and generates n (n&#x2267;2) pieces of reduced image data. The image zooming unit <b>23</b> writes the generated reduced image data into the image memory <b>22</b>.</p>
<p id="p-0083" num="0082">For example, it is assumed that the input image is of VGA size (640 pixels in the horizontal direction and 480 pixels in the vertical direction). The image zooming unit <b>23</b> reduces the input VGA data and generates first reduced image A<b>1</b> data which has a size of 320 pixels in the horizontal direction and 240 pixels in the vertical direction. The image zooming unit <b>23</b> writes the first reduced image A<b>1</b> data into the image memory <b>22</b>.</p>
<p id="p-0084" num="0083">Moreover, the image zooming unit <b>23</b> reads the reduced image A<b>1</b> data from the image memory <b>22</b> and generates second reduced image A<b>2</b> data which has a size comparable to 0.8 times the reduced image A<b>1</b> data in both the horizontal and vertical directions. The image zooming unit <b>23</b> writes the second reduced image A<b>2</b> data into the image memory <b>22</b>. The image zooming unit <b>23</b> repeats similar reduction processing until n-th reduced image An data can be obtained.</p>
<p id="p-0085" num="0084">In the present exemplary embodiment, a zooming ratio (i.e., a magnification of an image relative to the input image) in the generation of the reduced image data is a mere example. Further, the value n can be changed considering various conditions, such as the size of the input image or the size of the detection pattern. Alternatively, the value n can be a fixed value. Hereinafter, the zooming ratios for generating the data of the reduced images A<b>1</b> to An can be referred to as magnifications A<b>1</b> to An.</p>
<p id="p-0086" num="0085">Next, in step S<b>204</b>, the collation object pattern extraction unit <b>24</b> moves a rectangular area having a predetermined size on the data of the reduced images A<b>1</b> to An in a direction indicated by an arrow, as illustrated in <figref idref="DRAWINGS">FIG. 6A</figref>, and successively extracts a portion (i.e., a pixel group) included in the rectangular area as a collation object pattern. Then, the portions (i.e., pixel groups) extracted by the collation object pattern extraction unit <b>24</b> are output to the pattern detection unit <b>26</b>.</p>
<p id="p-0087" num="0086">In the present exemplary embodiment, the &#x201c;predetermined size&#x201d; can be arbitrarily determined to be a size equal to or greater than the reduced image An, although the rectangular area illustrated in <figref idref="DRAWINGS">FIG. 6A</figref> is similar in size to the detection pattern. The procedure indicated by the arrow is a mere example. The procedure for the successive extraction may be arbitrarily changed.</p>
<p id="p-0088" num="0087">The pattern detection unit <b>26</b> successively performs collation to determine whether each collation object pattern extracted by the collation object pattern extraction unit <b>24</b> coincides with the predetermined pattern, referring to the detection pattern stored in the detected pattern storage unit <b>25</b>. As described above, in the normal detection mode, the image processing apparatus <b>20</b> can generate a total of n pieces of reduced image data and perform collation on the generated n pieces of reduced image data.</p>
<p id="p-0089" num="0088">Next, in step S<b>205</b>, it is determined whether the predetermined pattern is detected based on the result of the collation performed on the reduced images A<b>1</b> to An. If it is determined that the predetermined pattern is not detected (NO in step S<b>205</b>), the processing returns to step S<b>201</b> to repeat the processing of steps S<b>201</b> to S<b>204</b>. On the other hand, if it is determined that the predetermined pattern is detected (YES in step S<b>205</b>), the processing proceeds to step S<b>206</b>.</p>
<p id="p-0090" num="0089">For example, it is assumed that in step S<b>205</b> the pattern detection unit <b>26</b> has detected the predetermined pattern from the reduced image A<b>6</b> (i.e., the sixth reduced image). In this case, in step S<b>206</b>, the detected information storage unit <b>27</b> receives information indicating a magnification A<b>6</b> of the reduced image A<b>6</b>, which is a zooming ratio relative to the input image, from the pattern detection unit <b>26</b>. The detected information storage unit <b>27</b> stores the received zooming ratio information. Then, after the zooming ratio information is stored in the detected information storage unit <b>27</b>, the image processing apparatus <b>20</b> terminates the operation in the normal detection mode and shifts its operation mode to the high-speed detection mode.</p>
<p id="p-0091" num="0090"><figref idref="DRAWINGS">FIG. 5B</figref> is a flowchart illustrating an example of a processing procedure for detecting a predetermined pattern, that can be performed by the image processing apparatus <b>20</b> that operates in the high-speed detection mode according to the present exemplary embodiment.</p>
<p id="p-0092" num="0091">First, in step S<b>207</b>, the image inputting unit <b>21</b> receives image data of the next latest frame from the imaging apparatus <b>2</b>. The photographic parameter storage unit <b>28</b> receives photographic parameters relating to the image data received by the image inputting unit <b>21</b>. Next, in step S<b>208</b>, similar to step S<b>202</b>, the image inputting unit <b>21</b> writes the received image data as input image data into the image memory <b>22</b>. Moreover, the photographic parameter storage unit <b>28</b> stores the photographic parameters of the latest frame so as to be added to the photographic parameters of a preceding frame.</p>
<p id="p-0093" num="0092">Next, in step S<b>209</b>, the image zooming unit <b>23</b> reads the input image data stored in the image memory <b>22</b> and generates m (n&#x3e;m&#x2267;1) pieces of reduced image data. The image zooming unit <b>23</b> writes the generated reduced image data into the image memory <b>22</b>. The reduction magnification for generating the reduced image data can be determined based on the zooming ratio stored in the detected information storage unit <b>27</b> and the photographic parameters (including the zooming magnification of the imaging apparatus <b>2</b>) stored in the photographic parameter storage unit <b>28</b>. In the present exemplary embodiment, the zooming ratio of the reduced image from which the predetermined pattern has been detected is the magnification A<b>6</b>. Thus, the detected information storage unit <b>27</b> stores the information indicating the magnification A<b>6</b>.</p>
<p id="p-0094" num="0093">First, as an example, an operation in the case of m=1 is described below. It is assumed that the magnification A<b>6</b> stored in the detected information storage unit <b>27</b> is 0.4, the zooming magnification of the preceding frame is 2, and the zooming magnification of the latest frame is 5. In this case, a zooming ratio A<b>6</b>&#x2032; for generating the reduced image of the latest frame can be obtained according to the following formula.</p>
<p id="p-0095" num="0094">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>A</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <msup>
            <mn>6</mn>
            <mi>&#x2032;</mi>
          </msup>
        </mrow>
        <mo>=</mo>
        <mi/>
        <mo>&#x2062;</mo>
        <mrow>
          <mi>A</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>6</mn>
          <mo>&#xd7;</mo>
          <mrow>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mi>zooming</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.8em" height="0.8ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mi>magnification</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.8em" height="0.8ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mi>of</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.8em" height="0.8ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mi>preceding</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.8em" height="0.8ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mi>frame</mi>
              </mrow>
              <mo>)</mo>
            </mrow>
            <mo>&#xf7;</mo>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mi/>
        <mo>&#x2062;</mo>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mi>zooming</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.8em" height="0.8ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>magnification</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.8em" height="0.8ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>of</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.8em" height="0.8ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>latest</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.8em" height="0.8ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>frame</mi>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mo>=</mo>
        <mi/>
        <mo>&#x2062;</mo>
        <mrow>
          <mn>0.4</mn>
          <mo>&#xd7;</mo>
          <mrow>
            <mn>2</mn>
            <mo>&#xf7;</mo>
            <mn>5</mn>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mo>=</mo>
        <mi/>
        <mo>&#x2062;</mo>
        <mn>0.16</mn>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0096" num="0095">The image zooming unit <b>23</b> generates a piece of reduced image B<b>1</b> (=reduced image Bm) data from the input image data using the magnification A<b>6</b>&#x2032; and writes the generated reduced image B<b>1</b> data into the image memory <b>22</b>. If the magnifications that can be processed by the image zooming unit <b>23</b> are limited to fixed values, the image zooming unit <b>23</b> can use a value adjacent to the magnification A<b>6</b>&#x2032;.</p>
<p id="p-0097" num="0096">Next, an operation in the case of m=3 is described below. It is assumed that the magnification A<b>6</b> stored in the detected information storage unit <b>27</b> is 0.4, the zooming magnification of the preceding frame is 2, and the zooming magnification of the latest frame is 5. First, the image zooming unit <b>23</b> obtains the magnification A<b>6</b>&#x2032; according to a procedure similar to the above-described procedure in the case of m=1. Then, the image zooming unit <b>23</b> generates the reduced image B<b>1</b> data from the input image data using a zooming ratio comparable to 1.25 times the magnification A<b>6</b>&#x2032;.</p>
<p id="p-0098" num="0097">The image zooming unit <b>23</b> writes the generated reduced image B<b>1</b> data into the image memory <b>22</b>. Next, the image zooming unit <b>23</b> reads the reduced image B<b>1</b> data from the image memory <b>22</b> and generates reduced image B<b>2</b> data which has a size comparable to 0.8 times the reduced image B<b>1</b> data in both the horizontal and vertical directions. The image zooming unit <b>23</b> writes the second reduced image B<b>2</b> data into the image memory <b>22</b>. The zooming ratio in this case is equal to the magnification A<b>6</b>&#x2032;.</p>
<p id="p-0099" num="0098">Subsequently, the image zooming unit <b>23</b> reads the reduced image B<b>2</b> data from the image memory <b>22</b> and generates reduced image B<b>3</b> (=reduced image Bm) data which has a size comparable to 0.8 times the reduced image B<b>2</b> data in both the horizontal and vertical directions. The image zooming unit <b>23</b> writes the generated reduced image B<b>3</b> into the image memory <b>22</b>.</p>
<p id="p-0100" num="0099">As described above, if m&#x2260;1, the image zooming unit <b>23</b> generates m pieces of reduced image data using a zooming ratio adjacent to the zooming ratio that can be determined based on the zooming ratio stored in the detected information storage unit <b>27</b> and the photographic parameters stored in the photographic parameter storage unit <b>28</b>.</p>
<p id="p-0101" num="0100">Next, in step S<b>210</b>, the collation object pattern extraction unit <b>24</b> moves a rectangular area having a predetermined size on the data of the reduced images B<b>1</b> to Bm in a direction indicated by an arrow, as illustrated in <figref idref="DRAWINGS">FIG. 6B</figref>, and successively extracts a portion (i.e., a pixel group) included in the rectangular area as a collation object pattern. Then, the portions (i.e., pixel groups) extracted by the collation object pattern extraction unit <b>24</b> are output to the pattern detection unit <b>26</b>.</p>
<p id="p-0102" num="0101">The pattern detection unit <b>26</b> successively performs collation to determine whether each collation object pattern extracted by the collation object pattern extraction unit <b>24</b> coincides with the predetermined pattern, referring to the detection pattern stored in the detected pattern storage unit <b>25</b>.</p>
<p id="p-0103" num="0102">The processing in step S<b>210</b> is different from the above-described processing in step S<b>204</b> in that the total number of the reduced image data is m that is smaller than n.</p>
<p id="p-0104" num="0103">As described above, in the high-speed detection mode, the image processing apparatus <b>20</b> can generate a total of m pieces of reduced image data and perform collation on the generated m pieces of reduced image data.</p>
<p id="p-0105" num="0104">Next, in step S<b>211</b>, it is determined whether the predetermined pattern has been detected referring to a result of the collation performed on the reduced images B<b>1</b> to Bm. If it is determined that the predetermined pattern has not been detected (NO in step S<b>211</b>), the image processing apparatus <b>20</b> terminates the operation in the high-speed detection mode. Then, the detected information storage unit <b>27</b> deletes the stored information indicating the magnification A<b>6</b>. The image processing apparatus <b>20</b> starts its operation in the normal detection mode. On the other hand, if it is determined that the predetermined pattern has been detected (YES in step S<b>211</b>), the processing proceeds to step S<b>212</b>.</p>
<p id="p-0106" num="0105">For example, it is assumed that in step S<b>211</b> the pattern detection unit <b>26</b> has detected the predetermined pattern from the reduced image B<b>3</b> (=reduced image Bm) that is the third reduced image (m=3). In this case, in step S<b>212</b>, the detected information storage unit <b>27</b> deletes the stored information indicating the magnification A<b>6</b>. The detected information storage unit <b>27</b> receives information indicating a magnification B<b>3</b> of the reduced image B<b>3</b>, which is a zooming ratio relative to the input image, from the pattern detection unit <b>26</b>. The detected information storage unit <b>27</b> newly stores the received zooming ratio information. Then, the processing returns to step S<b>207</b> to execute the processing of steps S<b>207</b> to S<b>211</b>, similar to the above-described procedure.</p>
<p id="p-0107" num="0106">In the present exemplary embodiment, the photographic parameter storage unit <b>28</b> stores two photographic parameters. However, the photographic parameter storage unit <b>28</b> can store a change amount between two photographic parameters. For example, if the zooming magnification of the preceding frame is 2 and the zooming magnification of the latest frame is 5, the photographic parameter storage unit <b>28</b> can store 2.5 as a change amount. In this case, the zooming ratio for generating the reduced image of the latest frame can be calculated according to the following formula.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>Zooming ratio=(stored zooming ratio)&#xf7;(change amount)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0108" num="0107">In the present exemplary embodiment, the pattern detection unit <b>26</b> may have luminance correction processing, density correction processing, and background removal processing functions, which are conventionally known. In this case, the pattern detection unit <b>26</b> can perform the luminance correction processing, the density correction processing, and the background removal processing on the collation object patterns. This is effective to improve the accuracy of the collation performed by the pattern detection unit <b>26</b>.</p>
<p id="p-0109" num="0108">Further, it is useful to separately provide an image processing unit configured to perform the luminance correction processing, the density correction processing, and the background removal processing on the input image or the reduced images. This is also effective to improve the accuracy of the collation performed by the pattern detection unit <b>26</b>.</p>
<p id="p-0110" num="0109">Further, in the collation performed by the pattern detection unit <b>26</b>, it is useful to use only the luminance component of an image. In this case, the image zooming unit <b>23</b> can extract only the luminance component and generate reduced image data based on the extracted luminance component.</p>
<p id="p-0111" num="0110">As described above, if there is no zooming ratio stored in the detected information storage unit <b>27</b>, the present exemplary embodiment uses a total of n (n&#x2267;2) pieces of reduced images that are sufficient for detecting a predetermined pattern on condition that the size of a predetermined imaging object on the input image is unclear.</p>
<p id="p-0112" num="0111">For example, when the image inputting unit <b>21</b> receives the first frame, it is determined that no zooming ratio is present (stored) in the detected information storage unit <b>17</b>. On the other hand, if there is any zooming ratio stored in the detected information storage unit <b>27</b>, the present exemplary embodiment uses m (n&#x3e;m&#x2267;1) pieces of reduced images to detect the predetermined pattern considering the situation that the size of the predetermined imaging object on the input image is already known.</p>
<p id="p-0113" num="0112">In this case, the present exemplary embodiment limits the zooming ratio to be used to generate reduced image data referring to the zooming ratio stored in the detected information storage unit <b>27</b> and the photographic parameter (i.e., the zooming magnification) stored in the photographic parameter storage unit <b>28</b>. Therefore, the present exemplary embodiment can reduce the amount of reduced image data to be generated to detect the predetermined pattern even in a case where the size of the imaging object on the input image is changed by a zooming operation of the imaging apparatus <b>2</b>.</p>
<p id="p-0114" num="0113">Moreover, the present exemplary embodiment can reduce the burden in calculation processing by decreasing the processing amount in the zooming processing as well as in the collation. Thus, the present exemplary embodiment can speedily detect the predetermined imaging object.</p>
<p id="p-0115" num="0114">Further, the present exemplary embodiment can maintain the detection accuracy because of m&#x3e;1 even when the zooming processing amount is decreased and when the size of predetermined imaging object on the input image is changed due to change in a distance between the imaging object and the imaging apparatus. The image processing apparatus <b>20</b> according to the present exemplary embodiment is connected to the imaging apparatus <b>2</b> via a general network line or a dedicated cable. However, similar effects can be obtained even in a case where the image processing apparatus <b>20</b> is integrated with the imaging apparatus <b>2</b>.</p>
<p id="p-0116" num="0115">A third exemplary embodiment is described below. <figref idref="DRAWINGS">FIG. 7</figref> is a block diagram illustrating an example of the configuration of an image processing apparatus <b>30</b> according to the present exemplary embodiment. First, an example of the configuration of the image processing apparatus <b>30</b> according to the present exemplary embodiment is described below.</p>
<p id="p-0117" num="0116">In <figref idref="DRAWINGS">FIG. 7</figref>, an image inputting unit <b>31</b> can receive image data from the external imaging apparatus <b>2</b>. Further, the image inputting unit <b>31</b> has a decoding processing function for receiving image data that may be conformable to a specific communication method and compatible with a marker code-attached protocol or compression processed image data such as JPEG/MPEG4.</p>
<p id="p-0118" num="0117">The imaging apparatus <b>2</b> is a general camera that includes a CCD or a CMOS configured to receive incident light via a zoom lens that can change a photographic angle of view. The imaging apparatus <b>2</b> can generate photographic image data constituted by digital data that are photo-electrically converted by the CCD or CMOS.</p>
<p id="p-0119" num="0118">The imaging apparatus <b>2</b> can output photographic parameters including at least a zooming magnification, together with the photographic image data, to the image processing apparatus <b>30</b>. A photographic parameter storage unit <b>38</b> can receive the photographic parameters output from the imaging apparatus <b>2</b> and store the received photographic parameters as parameter information. A general network line, such as LAN, or a dedicated cable can be used to transmit or receive various data between the imaging apparatus <b>2</b> and the image processing apparatus <b>30</b>.</p>
<p id="p-0120" num="0119">An image memory <b>32</b> can be configured by a readable and writable storage apparatus (e.g., a RAM). The image memory <b>22</b> stores the image data received by the image inputting unit <b>31</b> and zooming processed image data supplied from an image zooming unit <b>33</b>. The image zooming unit <b>33</b> can read the image data stored in the image memory <b>32</b> and generate reduced image data (i.e., zoomed image data). The image zooming unit <b>33</b> can write the reduced image data into the image memory <b>32</b>.</p>
<p id="p-0121" num="0120">A collation object pattern extraction unit <b>34</b> can move a rectangular area having a predetermined size on the reduced image data stored in the image memory <b>32</b> and successively extract and output a portion (i.e., a pixel group) included in the rectangular area as a collation object pattern. In the present exemplary embodiment, an original image can be regarded as an image having a reduced size of 1/1 and can be interpreted as one of the reduced images.</p>
<p id="p-0122" num="0121">A detected pattern storage unit <b>35</b> can store beforehand a detection pattern that represents a predetermined pattern (target object) to be detected from an input image. A pattern detection unit <b>36</b> performs collation based on the detection pattern stored in the detected pattern storage unit <b>35</b> to determine whether the collation object pattern coincides with the predetermined pattern.</p>
<p id="p-0123" num="0122">A detected information storage unit <b>37</b> can receive magnification information relating to a zooming ratio of the reduced image and position information of the collation object pattern which are used when the predetermined pattern is detected by the pattern detection unit <b>36</b>. The detected information storage unit <b>37</b> stores the magnification information and the position information received from the pattern detection unit <b>36</b>. Moreover, the detected information storage unit <b>37</b> transmits the position information of the collation object pattern to the imaging apparatus <b>2</b>.</p>
<p id="p-0124" num="0123">Next, an example of processing that can be realized by the functional components illustrated in <figref idref="DRAWINGS">FIG. 7</figref> is described below with reference to flowcharts illustrated in <figref idref="DRAWINGS">FIGS. 8A and 8B</figref>. The image processing apparatus <b>30</b> can provide two types of detection modes (i.e., the normal detection mode and the high-speed detection mode) which can be selected in detection of the predetermined pattern. It is now assumed that the image processing apparatus <b>30</b> starts its operation in the normal detection mode.</p>
<p id="p-0125" num="0124"><figref idref="DRAWINGS">FIG. 8A</figref> is a flowchart illustrating an example of a processing procedure for detecting a predetermined pattern, which can be performed by the image processing apparatus <b>30</b> that operates in the normal detection mode according to the present exemplary embodiment.</p>
<p id="p-0126" num="0125">After starting the processing, first, in step S<b>301</b>, the image inputting unit <b>31</b> receives image data from the imaging apparatus <b>2</b>. The photographic parameter storage unit <b>38</b> receives the photographic parameters relating to image data received by the image inputting unit <b>31</b>.</p>
<p id="p-0127" num="0126">Next, in step S<b>302</b>, the image inputting unit <b>31</b> writes the input image data into the image memory <b>32</b>. In this case, if the received image data is the image data that may be conformable to the specific communication method and compatible with the marker code-attached protocol, the image inputting unit <b>31</b> performs marker code removal or similar decoding processing on the received image data. The image inputting unit <b>31</b> writes the processing result as input image data into the image memory <b>32</b>.</p>
<p id="p-0128" num="0127">Further, if the received image data is the compression processed image data such as JPEG/MPEG4, the image inputting unit <b>31</b> performs decompression processing or similar decoding processing on the received image data. The image inputting unit <b>31</b> writes the processing result as input image data into the image memory <b>32</b>.</p>
<p id="p-0129" num="0128">When the decoding processing is not required, the image inputting unit <b>31</b> directly writes the received image data as input image data into the image memory <b>32</b>. Further, the photographic parameter storage unit <b>38</b> stores the received photographic parameters.</p>
<p id="p-0130" num="0129">Next, in step S<b>303</b>, the image zooming unit <b>33</b> reads the input image data stored in the image memory <b>32</b> and generates n (n&#x2267;2) pieces of reduced image data. The image zooming unit <b>33</b> writes the generated reduced image data into the image memory <b>32</b>.</p>
<p id="p-0131" num="0130">For example, it is assumed that the input image is of VGA size (640 pixels in the horizontal direction and 480 pixels in the vertical direction). The image zooming unit <b>33</b> reduces the input VGA data and generates first reduced image A<b>1</b> data which has a size of 320 pixels in the horizontal direction and 240 pixels in the vertical direction. The image zooming unit <b>33</b> writes the first reduced image A<b>1</b> data into the image memory <b>32</b>.</p>
<p id="p-0132" num="0131">Moreover, the image zooming unit <b>33</b> reads the reduced image A<b>1</b> data from the image memory <b>32</b> and generates second reduced image A<b>2</b> data which has a size comparable to 0.8 times the reduced image A<b>1</b> data in both the horizontal and vertical directions. The image zooming unit <b>33</b> writes the second reduced image A<b>2</b> data into the image memory <b>32</b>. The image zooming unit <b>33</b> repeats similar reduction processing until n-th reduced image An data can be obtained.</p>
<p id="p-0133" num="0132">In the present exemplary embodiment, a zooming ratio (i.e., a magnification of an image relative to the input image) in the generation of the reduced image data is a mere example. Further, the value n can be changed considering various conditions, such as the size of the input image or the size of the detection pattern. Alternatively, the value n can be a fixed value. Hereinafter, the zooming ratios for generating the data of the reduced images A<b>1</b> to An can be referred to as magnifications A<b>1</b> to An.</p>
<p id="p-0134" num="0133">Next, in step S<b>304</b>, the collation object pattern extraction unit <b>34</b> moves a rectangular area having a predetermined size on the data of the reduced images A<b>1</b> to An in a direction indicated by an arrow, as illustrated in <figref idref="DRAWINGS">FIG. 6A</figref>, and successively extracts a portion (i.e., a pixel group) included in the rectangular area as a collation object pattern. Then, the portions (i.e., pixel groups) extracted by the collation object pattern extraction unit <b>34</b> are output to the pattern detection unit <b>36</b>.</p>
<p id="p-0135" num="0134">In the present exemplary embodiment, the &#x201c;predetermined size&#x201d; can be arbitrarily determined to be a size equal to or greater than the reduced image An, although the rectangular area illustrated in <figref idref="DRAWINGS">FIG. 6A</figref> is similar in size to the detection pattern. The procedure indicated by the arrow is a mere example. The procedure for the successive extraction may be arbitrarily changed.</p>
<p id="p-0136" num="0135">Moreover, the pattern detection unit <b>36</b> successively performs collation to determine whether each collation object pattern extracted by the collation object pattern extraction unit <b>34</b> coincides with the predetermined pattern, referring to the detection pattern stored in the detected pattern storage unit <b>35</b>. As described above, in the normal detection mode, the image processing apparatus <b>30</b> can generate a total of n pieces of reduced image data and perform collation on the generated n pieces of reduced image data.</p>
<p id="p-0137" num="0136">Next, in step S<b>305</b>, it is determined whether the predetermined pattern is detected based on the result of the collation performed on the reduced images A<b>1</b> to An. If it is determined that the predetermined pattern is not detected (NO in step S<b>505</b>), the processing returns to step S<b>301</b> to repeat the processing of steps S<b>301</b> to S<b>304</b>. On the other hand, if it is determined that the predetermined pattern is detected (YES in step S<b>305</b>), the processing proceeds to step S<b>306</b>.</p>
<p id="p-0138" num="0137">For example, it is assumed that in step S<b>305</b> the pattern detection unit <b>36</b> has detected the predetermined pattern from the reduced image A<b>6</b> (i.e., the sixth reduced image). In this case, in step S<b>306</b>, the detected information storage unit <b>37</b> receives information indicating a magnification A<b>6</b> of the reduced image A<b>6</b>, which is a zooming ratio relative to the input image, and position information of the collation object pattern from the pattern detection unit <b>36</b>. The detected information storage unit <b>37</b> stores the zooming ratio information and the position information received from the pattern detection unit <b>36</b>.</p>
<p id="p-0139" num="0138">Next, in step S<b>307</b>, the detected information storage unit <b>37</b> calculates a position of the predetermined pattern on the input image based on the position of the collation object pattern in the reduced image A<b>6</b> and the magnification A<b>6</b>. The detected information storage unit <b>37</b> transmits the calculation result as detection information to the imaging apparatus <b>2</b>. Then, after the zooming ratio information is stored in the detected information storage unit <b>37</b> and the detection information is transmitted to the imaging apparatus <b>2</b>, the image processing apparatus <b>30</b> terminates the operation in the normal detection mode and shifts its operation mode to the high-speed detection mode.</p>
<p id="p-0140" num="0139"><figref idref="DRAWINGS">FIG. 8B</figref> is a flowchart illustrating an example of a processing procedure for detecting a predetermined pattern, which can be performed by the image processing apparatus <b>30</b> that operates in the high-speed detection mode according to the present exemplary embodiment.</p>
<p id="p-0141" num="0140">First, in step S<b>308</b>, the image inputting unit <b>31</b> receives image data of the next latest frame from the imaging apparatus <b>2</b>. The photographic parameter storage unit <b>38</b> receives photographic parameters relating to the image data received by the image inputting unit <b>31</b>. Next, in step S<b>309</b>, similar to step S<b>302</b>, the image inputting unit <b>31</b> writes the received image data as input image data into the image memory <b>32</b>. The photographic parameter storage unit <b>38</b> stores the photographic parameters of the latest frame so as to be added to the photographic parameters of a preceding frame.</p>
<p id="p-0142" num="0141">Next, in step S<b>310</b>, the image zooming unit <b>33</b> reads the input image data stored in the image memory <b>32</b> and generates m (n&#x3e;m&#x2267;1) pieces of reduced image data. The image zooming unit <b>33</b> writes the generated reduced image data into the image memory <b>32</b>. The reduction magnification for generating the reduced image data can be determined based on the zooming ratio stored in the detected information storage unit <b>37</b> and the photographic parameters (including the zooming magnification of the imaging apparatus <b>2</b>) stored in the photographic parameter storage unit <b>38</b>.</p>
<p id="p-0143" num="0142">In the present exemplary embodiment, the zooming ratio of the reduced image from which the predetermined pattern has been detected is the magnification A<b>6</b>. Thus, the detected information storage unit <b>37</b> stores the information indicating the magnification A<b>6</b>.</p>
<p id="p-0144" num="0143">First, as an example, an operation in the case of m=1 is described below. It is assumed that the magnification A<b>6</b> stored in the detected information storage unit <b>37</b> is 0.4, the zooming magnification of the preceding frame is 2, the zooming magnification of the latest frame is 5. In this case, a zooming ratio A<b>6</b>&#x2032; for generating the reduced image of the latest frame can be obtained according to the following formula.</p>
<p id="p-0145" num="0144">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>A</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <msup>
            <mn>6</mn>
            <mi>&#x2032;</mi>
          </msup>
        </mrow>
        <mo>=</mo>
        <mi/>
        <mo>&#x2062;</mo>
        <mrow>
          <mi>A</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>6</mn>
          <mo>&#xd7;</mo>
          <mrow>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mi>zooming</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.8em" height="0.8ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mi>magnification</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.8em" height="0.8ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mi>of</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.8em" height="0.8ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mi>preceding</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.8em" height="0.8ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mi>frame</mi>
              </mrow>
              <mo>)</mo>
            </mrow>
            <mo>&#xf7;</mo>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mi/>
        <mo>&#x2062;</mo>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mi>zooming</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.8em" height="0.8ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>magnification</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.8em" height="0.8ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>of</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.8em" height="0.8ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>latest</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.8em" height="0.8ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>frame</mi>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mo>=</mo>
        <mi/>
        <mo>&#x2062;</mo>
        <mrow>
          <mn>0.4</mn>
          <mo>&#xd7;</mo>
          <mrow>
            <mn>2</mn>
            <mo>&#xf7;</mo>
            <mn>5</mn>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mo>=</mo>
        <mi/>
        <mo>&#x2062;</mo>
        <mn>0.16</mn>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0146" num="0145">The image zooming unit <b>33</b> generates a piece of reduced image B<b>1</b> (=reduced image Bm) data from the input image data using the magnification A<b>6</b>&#x2032; and writes the generated reduced image B<b>1</b> into the image memory <b>32</b>. If the magnifications that can be processed by the image zooming unit <b>33</b> are limited to fixed values, the image zooming unit <b>33</b> can use a value adjacent to the magnification A<b>6</b>&#x2032;.</p>
<p id="p-0147" num="0146">Next, an operation in the case of m=3 is described below. It is assumed that the magnification A<b>6</b> stored in the detected information storage unit <b>37</b> is 0.4, the zooming magnification of the preceding frame is 2, and the zooming magnification of the latest frame is 5. First, the image zooming unit <b>33</b> obtains the magnification A<b>6</b>&#x2032; according to a procedure similar to the above-described procedure in the case of m=1.</p>
<p id="p-0148" num="0147">Then, the image zooming unit <b>33</b> generates reduced image B<b>1</b> data from the input image data using a zooming ratio comparable to 1.25 times the magnification A<b>6</b>&#x2032;. The image zooming unit <b>33</b> writes the generated reduced image B<b>1</b> data into the image memory <b>32</b>. Next, the image zooming unit <b>33</b> reads the reduced image B<b>1</b> data from the image memory <b>32</b> and generates reduced image B<b>2</b> data which has a size comparable to 0.8 times the reduced image B<b>1</b> data in both the horizontal and vertical directions. The image zooming unit <b>33</b> writes the second reduced image B<b>2</b> data into the image memory <b>32</b>. The zooming ratio in this case is equal to the magnification A<b>6</b>&#x2032;.</p>
<p id="p-0149" num="0148">Subsequently, the image zooming unit <b>33</b> reads the reduced image B<b>2</b> data from the image memory <b>32</b> and generates reduced image B<b>3</b> (=reduced image Bm) data which has a size comparable to 0.8 times the reduced image B<b>2</b> data in both the horizontal and vertical directions. The image zooming unit <b>33</b> writes the generated reduced image B<b>3</b> into the image memory <b>32</b>.</p>
<p id="p-0150" num="0149">As described above, if m&#x2260;1, the image zooming unit <b>33</b> generates m pieces of reduced image data using a zooming ratio adjacent to the zooming ratio that can be determined based on the zooming ratio stored in the detected information storage unit <b>37</b> and the photographic parameters stored in the photographic parameter storage unit <b>38</b>.</p>
<p id="p-0151" num="0150">Next, in step S<b>311</b>, the collation object pattern extraction unit <b>34</b> moves a rectangular area having a predetermined size on the data of the reduced images B<b>1</b> to Bm in a direction indicated by an arrow, as illustrated in <figref idref="DRAWINGS">FIG. 6B</figref>, and successively extracts a portion (i.e., a pixel group) included in the rectangular area as a collation object pattern. Then, the portions (i.e., pixel groups) extracted by the collation object pattern extraction unit <b>34</b> are output to the pattern detection unit <b>36</b>.</p>
<p id="p-0152" num="0151">The pattern detection unit <b>36</b> successively performs collation to determine whether each collation object pattern extracted by the collation object pattern extraction unit <b>34</b> coincides with the predetermined pattern, referring to the detection pattern stored in the detected pattern storage unit <b>35</b>.</p>
<p id="p-0153" num="0152">The processing in step S<b>311</b> is different from the above-described processing in step S<b>304</b> in that the total number of the reduced image data is m that is smaller than n.</p>
<p id="p-0154" num="0153">As described above, in the high-speed detection mode, the image processing apparatus <b>30</b> can generate a total of m pieces of reduced image data and perform collation on the generated m pieces of reduced image data.</p>
<p id="p-0155" num="0154">Next, in step S<b>312</b>, it is determined whether the predetermined pattern has been detected referring to a result of the collation performed on the reduced images B<b>1</b> to Bm. If it is determined that the predetermined pattern has not been detected (NO in step S<b>312</b>), the image processing apparatus <b>30</b> terminates the operation in the high-speed detection mode. Then, the detected information storage unit <b>37</b> deletes the stored information indicating the magnification A<b>6</b>. The image processing apparatus <b>30</b> starts its operation in the normal detection mode. On the other hand, if it is determined that the predetermined pattern has been detected (YES in step S<b>312</b>), the processing proceeds to step S<b>313</b>.</p>
<p id="p-0156" num="0155">For example, it is assumed that in step S<b>312</b> the pattern detection unit <b>36</b> has detected the predetermined pattern from the reduced image B<b>3</b> (=reduced image Bm) that is the third reduced image (m=3). In this case, in step S<b>313</b>, the detected information storage unit <b>37</b> deletes the stored information indicating the magnification A<b>6</b> and the position information of the collation object pattern. Then, the detected information storage unit <b>37</b> receives information indicating a magnification B<b>3</b> of the reduced image B<b>3</b>, which is a zooming ratio relative to the input image, and position information of the collation object pattern from the pattern detection unit <b>36</b>. The detected information storage unit <b>37</b> newly stores the received zooming ratio information and the position information.</p>
<p id="p-0157" num="0156">Next, in step S<b>314</b>, the detected information storage unit <b>37</b> calculates the position of the predetermined pattern on the input image based on the position of the collation object pattern in the reduced image B<b>3</b> and the magnification B<b>3</b>. The detected information storage unit <b>37</b> transmits the calculation result as detection information to the imaging apparatus <b>2</b>. Then, the processing returns to step S<b>308</b> to execute the processing of steps S<b>308</b> to S<b>312</b> as described above.</p>
<p id="p-0158" num="0157">In the present exemplary embodiment, the photographic parameter storage unit <b>38</b> stores two photographic parameters. However, the photographic parameter storage unit <b>38</b> can store a change amount between two photographic parameters. For example, if the zooming magnification of the preceding frame is 2 and the zooming magnification of the latest frame is 5, the photographic parameter storage unit <b>38</b> can store 2.5 as a change amount. In this case, the zooming ratio for generating the reduced image of the latest frame can be calculated according to the following formula.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>Zooming ratio=(stored zooming ratio)&#xf7;(change amount)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0159" num="0158">In the present exemplary embodiment, the pattern detection unit <b>36</b> may have luminance correction processing, density correction processing, and background removal processing functions, which are conventionally known. In this case, the pattern detection unit <b>36</b> can perform the luminance correction processing, the density correction processing, and the background removal processing on the collation object patterns. This is effective to improve the accuracy of the collation performed by the pattern detection unit <b>36</b>.</p>
<p id="p-0160" num="0159">Further, it is useful to separately provide an image processing unit configured to perform the luminance correction processing, the density correction processing, and the background removal processing on the input image or the reduced images. This is also effective to improve the accuracy of the collation performed by the pattern detection unit <b>36</b>.</p>
<p id="p-0161" num="0160">Further, in the collation performed by the pattern detection unit <b>36</b>, it is useful to use only the luminance component of an image. In this case, the image zooming unit <b>33</b> can extract only the luminance component and generate reduced image data based on the extracted luminance component.</p>
<p id="p-0162" num="0161">As described above, if there is no zooming ratio stored in the detected information storage unit <b>37</b>, the present exemplary embodiment uses a total of n (n&#x2267;2) pieces of reduced images that are sufficient for detecting a detection pattern stored in the detected pattern storage unit <b>35</b> on condition that the size of a predetermined imaging object on the input image is unclear.</p>
<p id="p-0163" num="0162">For example, when the image inputting unit <b>31</b> receives the first frame, it is determined that no zooming ratio is present (stored) in the detected information storage unit <b>37</b>. On the other hand, if there is any zooming ratio stored in the detected information storage unit <b>37</b>, the present exemplary embodiment uses m (n&#x3e;m&#x2267;1) pieces of reduced images to detect the predetermined pattern considering the situation that the size of the predetermined imaging object on the input image is already known.</p>
<p id="p-0164" num="0163">In this case, the present exemplary embodiment limits the zooming ratio to be used to generate reduced image data referring to the zooming ratio stored in the detected information storage unit <b>37</b> and the photographic parameter (i.e., the zooming magnification) stored in the photographic parameter storage unit <b>38</b>. Therefore, the present exemplary embodiment can reduce the amount of reduced image data to be generated to detect the predetermined pattern even in a case where the size of the imaging object on the input image is changed by a zooming operation of the imaging apparatus <b>2</b>.</p>
<p id="p-0165" num="0164">Moreover, the present exemplary embodiment can reduce the burden in calculation processing by decreasing the processing amount in the zooming processing as well as in the collation. Thus, the present exemplary embodiment can speedily detect the predetermined imaging object.</p>
<p id="p-0166" num="0165">Further, the present exemplary embodiment can maintain the detection accuracy because of m&#x3e;1 even when the zooming processing amount is decreased and when the size of predetermined imaging object on the input image is changed due to change in a distance between the imaging object and the imaging apparatus.</p>
<p id="p-0167" num="0166">Further, the present exemplary embodiment can detect a predetermined imaging object and can transmit position information of a predetermined pattern on the input image (i.e., the captured image) as detection information to the imaging apparatus <b>2</b>. Thus, the present exemplary embodiment enables the imaging apparatus <b>2</b> to perform tracking operation of the predetermined imaging object.</p>
<p id="p-0168" num="0167">In the present exemplary embodiment, the imaging apparatus <b>2</b> can perform the tracking operation to continuously capture the predetermined pattern in the imaging field by using a panning mechanism, a tilting mechanism, a zoom mechanism, or various functions including image segmenting processing that are provided thereto.</p>
<p id="p-0169" num="0168">In the tracking operation, if the predetermined imaging object is zoomed up, the angle of view becomes narrower. Further, if the image processing apparatus <b>30</b> operates in the normal detection mode, a relatively long time is required to detect the imaging object. The imaging object may exit from the frame.</p>
<p id="p-0170" num="0169">However, the present exemplary embodiment starts the tracking operation after the predetermined pattern is first detected. Thus, the present exemplary embodiment can transmit detection information of the predetermined pattern obtained in the high-speed detection mode. The imaging apparatus <b>2</b> can constantly capture the predetermined pattern in the imaging field.</p>
<p id="p-0171" num="0170">Moreover, the image processing apparatus <b>30</b> according to the present exemplary embodiment is connected to the imaging apparatus <b>2</b> via a general network line or a dedicated cable. However, similar effects can be obtained even in a case where the image processing apparatus <b>30</b> is integrated with the imaging apparatus <b>2</b>.</p>
<p id="p-0172" num="0171">Further, as described in the first exemplary embodiment, even in a case where the image processing apparatus has no photographic parameter storage unit, the image processing apparatus can transmit the position information of the predetermined pattern to a transmission destination if the transmission destination of the input image data can be identified.</p>
<p id="p-0173" num="0172">A fourth exemplary embodiment is described below. <figref idref="DRAWINGS">FIG. 9</figref> is a block diagram illustrating an example of the configuration of an image processing apparatus <b>40</b> according to the present exemplary embodiment. First, an example of the configuration of the image processing apparatus <b>40</b> according to the present exemplary embodiment is described below.</p>
<p id="p-0174" num="0173">In <figref idref="DRAWINGS">FIG. 9</figref>, an image inputting unit <b>41</b> receives image data. Further, the image inputting unit <b>41</b> has a decoding processing function for receiving image data that may be conformable to a specific communication method and compatible with a marker code-attached protocol or compression processed image data such as JPEG/MPEG4. In the present exemplary embodiment, the image data include photographic image data captured and generated by an imaging apparatus and image data stored in an external storage apparatus. An output source is not limited to a specific device.</p>
<p id="p-0175" num="0174">An image memory <b>42</b> can be configured by a readable and writable storage apparatus (e.g., a RAM). The image memory <b>42</b> stores the image data received by the image inputting unit <b>41</b> and zooming processed image data supplied from an image zooming unit <b>43</b>. The image zooming unit <b>43</b> can read the image data stored in the image memory <b>42</b> and generate reduced image data (i.e., zoomed image data). The image zooming unit <b>43</b> can write the reduced image data into the image memory <b>42</b>.</p>
<p id="p-0176" num="0175">A collation object pattern extraction unit <b>44</b> can move a rectangular area having a predetermined size on the reduced image data stored in the image memory <b>42</b> and successively extract and output a portion (i.e., a pixel group) included in the rectangular area as a collation object pattern. In the present exemplary embodiment, an original image can be regarded as an image having a reduced size of 1/1 and can be interpreted as one of the reduced images.</p>
<p id="p-0177" num="0176">A detected pattern storage unit <b>45</b> can store beforehand a detection pattern that represents a predetermined pattern (target object) to be detected from an input image. A pattern detection unit <b>46</b> performs collation based on the detection pattern stored in the detected pattern storage unit <b>45</b> to determine whether the collation object pattern coincides with the predetermined pattern. Moreover, the pattern detection unit <b>46</b> obtains YUV values for respective pixels that can be converted from the pixels obtained from the detected predetermined pattern.</p>
<p id="p-0178" num="0177">A detected information storage unit <b>47</b> can receive magnification information relating to a zooming ratio of the reduced image and color information which are used when the predetermined pattern is detected by the pattern detection unit <b>46</b>. The detected information storage unit <b>47</b> stores the magnification information and the color information received from the pattern detection unit <b>46</b>.</p>
<p id="p-0179" num="0178">Next, an example of processing that can be realized by the functional components illustrated in <figref idref="DRAWINGS">FIG. 9</figref> is described below with reference to flowcharts illustrated in <figref idref="DRAWINGS">FIGS. 10A and 10B</figref>. The image processing apparatus <b>40</b> can provide two types of detection modes (i.e., the normal detection mode and the high-speed detection mode) which can be selected in detection of the predetermined pattern. It is now assumed that the image processing apparatus <b>40</b> starts its operation in the normal detection mode.</p>
<p id="p-0180" num="0179"><figref idref="DRAWINGS">FIG. 10A</figref> is a flowchart illustrating an example of a processing procedure for detecting a predetermined pattern, which can be performed by the image processing apparatus <b>40</b> that operates in the normal detection mode according to the present exemplary embodiment.</p>
<p id="p-0181" num="0180">After starting the processing, first in step S<b>401</b>, the image inputting unit <b>41</b> receives image data. In this case, if the received image data is the image data that may be conformable to the specific communication method and compatible with the marker code-attached protocol or the compression processed image data such as JPEG/MPEG4, the image inputting unit <b>41</b> performs decoding processing on the received image data.</p>
<p id="p-0182" num="0181">Next, in step S<b>402</b>, the image inputting unit <b>41</b> writes the processing result as input image data into the image memory <b>42</b>. When the decoding processing is not required, the image inputting unit <b>41</b> directly writes the received image data as input image data into the image memory <b>42</b>.</p>
<p id="p-0183" num="0182">Next, in step S<b>403</b>, the image zooming unit <b>43</b> reads the input image data stored in the image memory <b>42</b> and generates n (n&#x2267;2) pieces of reduced image data. The image zooming unit <b>43</b> writes the generated reduced image data into the image memory <b>42</b>.</p>
<p id="p-0184" num="0183">For example, it is assumed that the input image is of the VGA size (640 pixels in the horizontal direction and 480 pixels in the vertical direction). The image zooming unit <b>43</b> reduces the input VGA data and generates first reduced image A<b>1</b> data which has a size of 320 pixels in the horizontal direction and 240 pixels in the vertical direction. The image zooming unit <b>43</b> writes the first reduced image A<b>1</b> data into the image memory <b>42</b>.</p>
<p id="p-0185" num="0184">Moreover, the image zooming unit <b>43</b> reads the reduced image A<b>1</b> data from the image memory <b>42</b> and generates a second reduced image A<b>2</b> data which has a size comparable to 0.8 times the reduced image A<b>1</b> data in both the horizontal and vertical directions. The image zooming unit <b>43</b> writes the second reduced image A<b>2</b> data into the image memory <b>42</b>. The image zooming unit <b>43</b> repeats similar reduction processing until n-th reduced image An data can be obtained.</p>
<p id="p-0186" num="0185">In the present exemplary embodiment, a zooming ratio (i.e., a magnification of an image relative to the input image) in the generation of the reduced image data is a mere example. Further, the value n can be changed considering various conditions, such as the size of the input image or the size of the detection pattern. Alternatively, the value n can be a fixed value. Hereinafter, the zooming ratios for generating the data of the reduced images A<b>1</b> to An can be referred to as magnifications A<b>1</b> to An.</p>
<p id="p-0187" num="0186">Next, in step S<b>404</b>, the collation object pattern extraction unit <b>44</b> moves a rectangular area having a predetermined size on the data of the reduced images A<b>1</b> to An in a direction indicated by an arrow, as illustrated in <figref idref="DRAWINGS">FIG. 3A</figref>, and successively extracts a portion (i.e., a pixel group) included in the rectangular area as a collation object pattern. Then, the portions (i.e., pixel groups) extracted by the collation object pattern extraction unit <b>44</b> are output to the pattern detection unit <b>46</b>.</p>
<p id="p-0188" num="0187">In the present exemplary embodiment, the &#x201c;predetermined size&#x201d; can be arbitrarily determined to be a size equal to or greater than the reduced image An, although the rectangular area illustrated in <figref idref="DRAWINGS">FIG. 3A</figref> is similar in size to the detection pattern. The procedure indicated by the arrow is a mere example. The procedure for the successive extraction may be arbitrarily changed.</p>
<p id="p-0189" num="0188">The pattern detection unit <b>46</b> successively performs collation to determine whether each collation object pattern extracted by the collation object pattern extraction unit <b>44</b> coincides with the predetermined pattern, referring to the detection pattern stored in the detected pattern storage unit <b>45</b>.</p>
<p id="p-0190" num="0189">As described above, in the normal detection mode, the image processing apparatus <b>40</b> can generate a total of n pieces of reduced image data and perform collation on the generated n pieces of reduced image data.</p>
<p id="p-0191" num="0190">Next, in step S<b>405</b>, it is determined whether the predetermined pattern is detected based on the result of the collation performed on the reduced images A<b>1</b> to An. If it is determined that the predetermined pattern is not detected (NO in step S<b>405</b>), the processing returns to step S<b>401</b> to repeat the processing of steps S<b>401</b> to S<b>404</b>. On the other hand, if it is determined that the predetermined pattern is detected (YES in step S<b>405</b>), the processing proceeds to step S<b>406</b>.</p>
<p id="p-0192" num="0191">For example, it is assumed that in step S<b>405</b> the pattern detection unit <b>46</b> has detected the predetermined pattern from the reduced image A<b>6</b> (i.e., the sixth reduced image). In this case, in step S<b>406</b>, the detected information storage unit <b>47</b> receives information indicating a magnification A<b>6</b> of the reduced image A<b>6</b> which is a zooming ratio relative to the input image, from the pattern detection unit <b>46</b>. The detected information storage unit <b>47</b> stores the received zooming ratio information.</p>
<p id="p-0193" num="0192">Moreover, the pattern detection unit <b>46</b> obtains YUV values for respective pixels that can be converted from the pixels obtained from the detected predetermined pattern. Then, the detected information storage unit <b>47</b> receives, from the pattern detection unit <b>46</b>, color information including upper and lower limits of U and V values that represent color-difference information. The detected information storage unit <b>47</b> stores the received color information.</p>
<p id="p-0194" num="0193">Then, after the zooming ratio information and the color information are stored in the detected information storage unit <b>47</b>, the image processing apparatus <b>40</b> terminates the operation in the normal detection mode and shifts its operation mode to the high-speed detection mode.</p>
<p id="p-0195" num="0194"><figref idref="DRAWINGS">FIG. 10B</figref> is a flowchart illustrating an example of a processing procedure for detecting a predetermined pattern, which can be performed by the image processing apparatus <b>40</b> that operates in the high-speed detection mode according to the present exemplary embodiment.</p>
<p id="p-0196" num="0195">First, in step S<b>407</b>, the image inputting unit <b>41</b> receives image data of the next latest frame. Next, in step S<b>408</b>, similar to step S<b>402</b>, the image inputting unit <b>41</b> writes the received image data as input image data into the image memory <b>42</b>.</p>
<p id="p-0197" num="0196">Next, in step S<b>409</b>, the image zooming unit <b>43</b> reads the input image data stored in the image memory <b>42</b> and generates m (n&#x3e;m&#x2267;1) pieces of reduced image data. The image zooming unit <b>43</b> writes the generated reduced image data into the image memory <b>42</b>.</p>
<p id="p-0198" num="0197">The reduction magnification for generating the reduced image data can be determined based on the zooming ratio stored in the detected information storage unit <b>47</b>. Alternatively, the detected information storage unit <b>47</b> may store image size information instead of the zooming ratio information and can determine a reduction magnification based on the image size information.</p>
<p id="p-0199" num="0198">In the present exemplary embodiment, the zooming ratio of the reduced image from which the predetermined pattern has been detected is the magnification A<b>6</b>. Thus, the detected information storage unit <b>47</b> stores the information indicating the magnification A<b>6</b>.</p>
<p id="p-0200" num="0199">For example, if m=1, the image zooming unit <b>43</b> generates a piece of reduced image B<b>1</b> (=reduced image Bm) data from the input image data using the magnification A<b>6</b> stored in the detected information storage unit <b>47</b>. The image zooming unit <b>43</b> writes the generated reduced image B<b>1</b> data into the image memory <b>42</b>.</p>
<p id="p-0201" num="0200">Further, if m=3, the image zooming unit <b>43</b> generates the reduced image B<b>1</b> data from the input image data using a magnification A<b>5</b> that is a zooming ratio comparable to 1.25 times the magnification A<b>6</b> stored in the detected information storage unit <b>47</b>. The image zooming unit <b>43</b> writes the generated reduced image B<b>1</b> data into the image memory <b>42</b>.</p>
<p id="p-0202" num="0201">Then, the image zooming unit <b>43</b> reads the reduced image B<b>1</b> data from the image memory <b>42</b> and generates reduced image B<b>2</b> data which has a size comparable to 0.8 times the reduced image B<b>1</b> data in both the horizontal and vertical directions. The image zooming unit <b>43</b> writes the second reduced image B<b>2</b> data into the image memory <b>42</b>. The zooming ratio in this case is equal to the magnification A<b>6</b> stored in the detected information storage unit <b>47</b>.</p>
<p id="p-0203" num="0202">Subsequently, the image zooming unit <b>43</b> reads the reduced image B<b>2</b> data from the image memory <b>42</b> and generates reduced image B<b>3</b> (=reduced image Bm) data which has a size comparable to 0.8 times the reduced image B<b>2</b> data in both the horizontal and vertical directions. The image zooming unit <b>43</b> writes the generated third reduced image B<b>3</b> data into the image memory <b>42</b>. The zooming ratio in this case is equal to the magnification A<b>7</b>.</p>
<p id="p-0204" num="0203">As described above, if m&#x2260;1, the image zooming unit <b>43</b> generates m pieces of reduced image data using a zooming ratio adjacent to the zooming ratio stored in the detected information storage unit <b>47</b> among the n pieces of zooming ratios in the generation of n pieces of reduced image data.</p>
<p id="p-0205" num="0204">Next, in step S<b>410</b>, the collation object pattern extraction unit <b>44</b> moves a rectangular area having a predetermined size on the data of the reduced images B<b>1</b> to Bm in a direction indicated by an arrow, as illustrated in <figref idref="DRAWINGS">FIG. 3B</figref>, and successively extracts a portion (i.e., a pixel group) included in the rectangular area as a collation object pattern. Then, the portions (i.e., pixel groups) extracted by the collation object pattern extraction unit <b>44</b> are output to the pattern detection unit <b>46</b>. Then, the pattern detection unit <b>46</b> successively performs the following processing on the collation object pattern.</p>
<p id="p-0206" num="0205">First, the pattern detection unit <b>46</b> detects, from a target collation object pattern, pixels included in a range between the upper and lower limits of the U and V values of the color information stored in the detected information storage unit <b>47</b>.</p>
<p id="p-0207" num="0206">If a ratio of the pixels included in the range between the upper and lower limits of the U and V values is less than a predetermined value, the pattern detection unit <b>46</b> does not perform collation between the target collation object pattern and the detection pattern stored in the detected pattern storage unit <b>45</b>.</p>
<p id="p-0208" num="0207">Then, only when the ratio of the pixels included in the range between the upper and lower limits of the U and V values is equal to or greater than the predetermined value, the pattern detection unit <b>46</b> performs collation to determine whether the collation object pattern coincides with the detection pattern stored in the detected pattern storage unit <b>45</b>. The processing in step S<b>410</b> is different from the above-described processing in step S<b>404</b> in that the total number of the reduced image data is m that is smaller than n.</p>
<p id="p-0209" num="0208">As described above, in the high-speed detection mode, the image processing apparatus <b>40</b> can generate a total of m pieces of reduced image data and perform collation on the generated m pieces of reduced image data.</p>
<p id="p-0210" num="0209">Next, in step S<b>411</b>, it is determined whether the predetermined pattern has been detected referring to a result of the collation performed on the reduced images B<b>1</b> to Bm. If it is determined that the predetermined pattern has not been detected (NO in step S<b>411</b>), the image processing apparatus <b>40</b> terminates the operation in the high-speed detection mode.</p>
<p id="p-0211" num="0210">Then, the detected information storage unit <b>47</b> deletes the stored information indicating the magnification A<b>6</b> and the color information. The image processing apparatus <b>40</b> starts its operation in the normal detection mode. On the other hand, if it is determined that the predetermined pattern has been detected (YES in step S<b>411</b>), the processing proceeds to step S<b>412</b>.</p>
<p id="p-0212" num="0211">For example, it is assumed that in step S<b>411</b> the pattern detection unit <b>46</b> has detected the predetermined pattern from the reduced image B<b>3</b> (=reduced image Bm) that is the third reduced image (m=3). In this case, in step S<b>412</b>, the detected information storage unit <b>47</b> deletes the stored information indicating the magnification A<b>6</b> and the color information.</p>
<p id="p-0213" num="0212">Then, the detected information storage unit <b>47</b> receives, from the pattern detection unit <b>46</b>, information indicating a magnification B<b>3</b> of the reduced image B<b>3</b>, which is a zooming ratio relative to the input image, and color information including upper and lower limits of U and V values that can be obtained from the detected predetermined pattern. The detected information storage unit <b>47</b> newly stores the received zooming ratio information and the color information. Then, the processing returns to step S<b>407</b> to execute the processing of steps S<b>407</b> to S<b>411</b>, similar to the above-described procedure.</p>
<p id="p-0214" num="0213">In the present exemplary embodiment, the detected information storage unit <b>47</b> stores the color information including the upper and lower limits of the U and V values obtained from the detected predetermined pattern. Alternatively, the detected information storage unit <b>47</b> can store color information (e.g., luminance and RGB values) and can limit collation object patterns to be compared with the detection pattern.</p>
<p id="p-0215" num="0214">In the present exemplary embodiment, the pattern detection unit <b>46</b> may have luminance correction processing, density correction processing, and background removal processing functions, which are conventionally known. In this case, the pattern detection unit <b>46</b> can perform the luminance correction processing, the density correction processing, and the background removal processing on the collation object patterns. This is effective to improve the accuracy of the collation performed by the pattern detection unit <b>46</b>.</p>
<p id="p-0216" num="0215">Further, it is useful to separately provide an image processing unit configured to perform the luminance correction processing, the density correction processing, and the background removal processing on the input image or the reduced images. This is also effective to improve the accuracy of the collation performed by the pattern detection unit <b>46</b>.</p>
<p id="p-0217" num="0216">Further, in the collation performed by the pattern detection unit <b>46</b>, it is useful to use only the luminance component of an image. In this case, the image zooming unit <b>43</b> can extract only the luminance component and generate reduced image data based on the extracted luminance component.</p>
<p id="p-0218" num="0217">As described above, similar to the first exemplary embodiment, the present exemplary embodiment limits the zooming ratio to be used to generate reduced image data referring to the zooming ratio stored in the detected information storage unit <b>47</b>. Therefore, the present exemplary embodiment can reduce the amount of reduced image data to be generated to detect the predetermined pattern. Therefore, the present exemplary embodiment can reduce the burden in calculation processing by decreasing the processing amount in the zooming processing as well as in the collation. Thus, the present exemplary embodiment can speedily detect the predetermined imaging object.</p>
<p id="p-0219" num="0218">Moreover, in the present exemplary embodiment, the detected information storage unit <b>47</b> stores color information including upper and lower limits of U and V values obtained from the detected predetermined pattern. Then, the present exemplary embodiment performs collation to check whether the collation object pattern coincides with the detection pattern stored in the detected pattern storage unit <b>45</b> only when the ratio of the pixels of the collation object pattern in the range between the upper and lower limits of the U and V values is equal to or greater than a predetermined value. Thus, the present exemplary embodiment can further reduce the number of collations. Therefore, the present exemplary embodiment can reduce the burden in calculation processing and can speedily detect the predetermined imaging object.</p>
<p id="p-0220" num="0219">Further, the present exemplary embodiment can maintain the detection accuracy because of m&#x3e;1 even when the zooming processing amount is decreased and when the size of predetermined imaging object on the input image is changed according to the distance between the imaging object and the imaging apparatus.</p>
<p id="p-0221" num="0220">Moreover, the image processing apparatus <b>40</b> according to the present exemplary embodiment is configured to receive image data from an external device. However, similar effects can be obtained even in a case where the image processing apparatus <b>40</b> is integrated with an imaging apparatus or other image data outputting source. The image processing apparatus <b>40</b> according to the present exemplary embodiment can be combined with the image processing apparatus described in the second or third exemplary embodiment.</p>
<p id="p-0222" num="0221">Aspects of the present invention can also be realized by a computer of a system or apparatus (or devices such as a CPU or MPU) that reads out and executes a program recorded on a memory device to perform the functions of the above-described embodiments, and by a method, the steps of which are performed by a computer of a system or apparatus by, for example, reading out and executing a program recorded on a memory device to perform the functions of the above-described embodiments. For this purpose, the program is provided to the computer for example via a network or from a recording medium of various types serving as the memory device (e.g., computer-readable medium).</p>
<p id="p-0223" num="0222">While the present invention has been described with reference to exemplary embodiments, it is to be understood that the invention is not limited to the disclosed exemplary embodiments. The scope of the following claims is to be accorded the broadest interpretation so as to encompass all such modifications and equivalent structures and functions.</p>
<p id="p-0224" num="0223">This application claims the benefit of Japanese Patent Application No. 2008-258452, filed Oct. 3, 2008, which is hereby incorporated by reference herein in its entirety.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08625933-20140107-M00001.NB">
<img id="EMI-M00001" he="15.92mm" wi="76.20mm" file="US08625933-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US08625933-20140107-M00002.NB">
<img id="EMI-M00002" he="15.92mm" wi="76.20mm" file="US08625933-20140107-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. An image processing apparatus that can detect a predetermined target object from image data, the image processing apparatus comprising:
<claim-text>an image inputting unit configured to input the image data;</claim-text>
<claim-text>an image zooming unit configured to generate a plurality of pieces of zoomed image data that are mutually different in magnification from the image data input by the image inputting unit;</claim-text>
<claim-text>a detection unit configured to detect the predetermined target object by determining whether a partial area in the plurality of pieces of zoomed image data coincides with a predetermined detection pattern; and</claim-text>
<claim-text>a detected information storage unit configured to store detection information including magnification information of the zoomed image data from which the predetermined target object is detected by the detection unit,</claim-text>
<claim-text>wherein, in a case where the detection unit detects the predetermined target object from first zoomed image data generated from first image data, the image zooming unit determines a magnification of second zoomed image data generated from second image data based on the magnification information of the first zoomed image data and generates at least one piece of the second zoomed image data whose total number is smaller compared to a case where the detection unit does not detect the predetermined target object from the first zoomed image data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The image processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein in the case where the detection unit detects the predetermined target object from the first zoomed image data, the image zooming unit generates at least one piece of the second zoomed image data which has a magnification similar to the magnification included in the detection information.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The image processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein in the case where the detection unit detects the predetermined target object from the first zoomed image data, the image zooming unit generates a plurality of pieces of the second zoomed image data which have mutually different magnifications so that the magnification contained in the detection information can be included as a central value.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The image processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising a parameter storage unit configured to store parameter information of the image data input by the image inputting unit,
<claim-text>wherein, in the case where the detection unit detects the predetermined target object from the first zoomed image data, the image zooming unit determines the magnification of the second zoomed image data based on the detection information and the parameter information stored in the parameter storage unit and generates at least one piece of the second zoomed image data whose total number is smaller compared to the case where the detection unit does not detect the predetermined target object from the first zoomed image data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The image processing apparatus according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein in the case where the detection unit detects the predetermined target object from the first zoomed image data, the image zooming unit generates at least one piece of the second zoomed image data calculated based on the magnification included in the detection information and a magnification included in the parameter information stored in the parameter storage unit.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The image processing apparatus according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein in the case where the detection unit detects the predetermined target object from the first zoomed image data, the image zooming unit generates a plurality of pieces of the second zoomed image data which have mutually different magnifications so that the magnification calculated based on the magnification included in the detection information and the magnification included in the parameter information stored in the parameter storage unit can be included as a central value.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The image processing apparatus according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the parameter information stored in the parameter storage unit includes information indicating a zooming magnification of an imaging apparatus that has generated the first image data input by the image inputting unit.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The image processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the detection information stored in the detected information storage unit further includes position information of the predetermined target object in the zoomed image data from which the predetermined target object is detected, and
<claim-text>the image processing apparatus further comprises a transmission unit configured to transmit the position information to an imaging apparatus that has generated the input image data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The image processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising a deletion unit configured to delete the detection information stored in the detected information storage unit in a case where the predetermined target object is not detected by the detection unit.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The image processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the detection information stored in the detected information storage unit further includes color information of the predetermined target object, and the detection unit uses the color information to narrow down a collation target area.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The image processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising a zooming ratio input unit configured to input a zooming ratio for capturing an image to generate the image data to be inputted from the image inputting unit,
<claim-text>wherein the image zooming unit determines the magnification of the second zoomed image data based on the magnification information of the first zoomed image data, a first zooming ratio for capturing the image to generate the first image data, and a second zooming ratio for capturing the image to generate the second image data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. A method performed in an imaging processing apparatus for processing an image which detects a predetermined target object from image data, wherein the imaging processing apparatus includes an image inputting unit, image zooming unit, detection unit and storage detection unit, the method comprising:
<claim-text>via the image inputting unit, inputting the image data;</claim-text>
<claim-text>via the image zooming unit, generating a plurality of pieces of zoomed image data that are mutually different in magnification from the input image data;</claim-text>
<claim-text>via the detection unit, detecting the predetermined target object by determining whether a partial area in the plurality of pieces of zoomed image data coincides with a detection pattern representing the predetermined target object; and</claim-text>
<claim-text>via the storage detection unit, storing detection information including magnification information of the zoomed image data from which the predetermined target object is detected in a storage unit,</claim-text>
<claim-text>wherein in the generation of a second zoomed image data, in a case where the predetermined target object is detected from first zoomed image data generated from first image data, the method further comprises determining a magnification of the second zoomed image data generated from second image data based on the magnification information of the first zoomed image data and generating the second zoomed image data whose total number is smaller compared to a case where the predetermined target object is not detected from the first zoomed image data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. A non-transitory computer-readable storage medium that stores a program for instructing a computer to implement the method for image processing according to <claim-ref idref="CLM-00012">claim 12</claim-ref>.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The computer-readable storage medium according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising inputting a zooming ratio for capturing an image to generate the image data to be inputted from the image inputting unit,
<claim-text>wherein the magnification of the second zoomed image data is determined based on the magnification information of the first zoomed image data, a first zooming ratio for capturing the image to generate the first image data, and a second zooming ratio for capturing the image to generate the second image data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The method according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, further comprising inputting a zooming ratio for capturing an image to generate the image data to be inputted from the image inputting unit,
<claim-text>wherein the magnification of the second zoomed image data is determined based on the magnification information of the first zoomed image data, a first zooming ratio for capturing the image to generate the first image data, and a second zooming ratio for capturing the image to generate the second image data. </claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
