<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08624912-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08624912</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12508889</doc-number>
<date>20090724</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>JP</country>
<doc-number>2008-194218</doc-number>
<date>20080728</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>1189</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>T</subclass>
<main-group>17</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>09</class>
<subclass>G</subclass>
<main-group>5</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>09</class>
<subclass>G</subclass>
<main-group>5</main-group>
<subgroup>02</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>T</subclass>
<main-group>1</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>09</class>
<subclass>G</subclass>
<main-group>5</main-group>
<subgroup>36</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>40</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>20</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>60</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>345589</main-classification>
<further-classification>345420</further-classification>
<further-classification>345593</further-classification>
<further-classification>345619</further-classification>
<further-classification>345501</further-classification>
<further-classification>345549</further-classification>
<further-classification>382165</further-classification>
<further-classification>382254</further-classification>
<further-classification>382282</further-classification>
<further-classification>382305</further-classification>
<further-classification>382307</further-classification>
</classification-national>
<invention-title id="d2e71">Program, image generation device, and image generation method</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5909220</doc-number>
<kind>A</kind>
<name>Sandow</name>
<date>19990600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345589</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>7233338</doc-number>
<kind>B2</kind>
<name>Orihara</name>
<date>20070600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>2001/0027129</doc-number>
<kind>A1</kind>
<name>Harima</name>
<date>20011000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>463 32</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>2006/0217008</doc-number>
<kind>A1</kind>
<name>Higashino et al.</name>
<date>20060900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>2006/0285755</doc-number>
<kind>A1</kind>
<name>Hager et al.</name>
<date>20061200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382224</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2010/0001993</doc-number>
<kind>A1</kind>
<name>Finn et al.</name>
<date>20100100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345419</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2010/0020083</doc-number>
<kind>A1</kind>
<name>Kumakura et al.</name>
<date>20100100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345441</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2010/0194776</doc-number>
<kind>A1</kind>
<name>Chong et al.</name>
<date>20100800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345594</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2010/0234106</doc-number>
<kind>A1</kind>
<name>Kojima et al.</name>
<date>20100900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>JP</country>
<doc-number>A-2004-105532</doc-number>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>JP</country>
<doc-number>A-2006-268406</doc-number>
<date>20061000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>JP</country>
<doc-number>A-2007-260197</doc-number>
<date>20071000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00013">
<othercit>U.S. Appl. No. 12/508,825, filed Jul. 24, 2009 in the name of Kumakura et al.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00014">
<othercit>U.S. Appl. No. 12/508,826, filed Jul. 24, 2009 in the name of Kumakura et al.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>13</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>345418-420</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345581</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345589-590</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345593-594</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345597</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345600-601</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345619</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345636</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345501</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345549</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>358518-519</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>358537-538</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>358452-453</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>358540</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>358448</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382152</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382165-167</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>254254</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>254274</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>254276</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>254282</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>254285</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>254305</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>254307</main-classification>
</classification-national>
<classification-cpc-combination-text>G06T 2219/2012</classification-cpc-combination-text>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>12</number-of-drawing-sheets>
<number-of-figures>12</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20100020077</doc-number>
<kind>A1</kind>
<date>20100128</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Kumakura</last-name>
<first-name>Takashi</first-name>
<address>
<city>Matsudo</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Hiyama</last-name>
<first-name>Noriyuki</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Kumakura</last-name>
<first-name>Takashi</first-name>
<address>
<city>Matsudo</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Hiyama</last-name>
<first-name>Noriyuki</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Oliff &#x26; Berridge, PLC</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Namco Bandai Games Inc.</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Sajous</last-name>
<first-name>Wesner</first-name>
<department>2677</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A program causing a computer to select a first color range from a plurality of color ranges based on input information, and to determine a color within the first color range to be a color of one of part objects corresponding to categories that form main parts of a model object under a predetermined condition.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="159.85mm" wi="235.80mm" file="US08624912-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="237.57mm" wi="160.70mm" orientation="landscape" file="US08624912-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="160.61mm" wi="146.13mm" file="US08624912-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="167.56mm" wi="145.71mm" file="US08624912-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="243.59mm" wi="157.56mm" orientation="landscape" file="US08624912-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="147.07mm" wi="139.36mm" file="US08624912-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="188.47mm" wi="152.48mm" file="US08624912-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="152.06mm" wi="149.35mm" file="US08624912-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="229.45mm" wi="153.42mm" orientation="landscape" file="US08624912-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="194.31mm" wi="99.23mm" orientation="landscape" file="US08624912-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="99.14mm" wi="128.35mm" file="US08624912-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="208.87mm" wi="148.00mm" file="US08624912-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="120.65mm" wi="140.21mm" file="US08624912-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<p id="p-0002" num="0001">Japanese Patent Application No. 2008-194218, filed on Jul. 28, 2008, is hereby incorporated by reference in its entirety.</p>
<heading id="h-0001" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">The present invention relates to a program, an image generation device, and an image generation method.</p>
<p id="p-0004" num="0003">An image generation device (game device) that generates an image viewed from a virtual camera (given viewpoint) in an object space (virtual three-dimensional space) has been known. Such an image generation device is very popular as a system that allows experience of virtual reality.</p>
<p id="p-0005" num="0004">An image generation device that allows the player to arbitrarily customize the player's character has been known (JP-A-2006-268406). A related-art image generation device allows the player to select desired part objects to create an original model object.</p>
<p id="p-0006" num="0005">However, a related-art image generation device does not allow the player to create a unified model object while reflecting the preference of the player.</p>
<heading id="h-0002" level="1">SUMMARY</heading>
<p id="p-0007" num="0006">According to a first aspect of the invention, there is provided a program that is stored in a computer-readable information storage medium and generates an image of a model object formed by a plurality of part objects, the program causing a computer to function as:</p>
<p id="p-0008" num="0007">a storage section that stores the plurality of part objects, each of the plurality of part objects being classified into one of a plurality of categories;</p>
<p id="p-0009" num="0008">a setting section that selects a part object from the plurality of part objects for each of the plurality of categories, and sets the selected part object as an element of the model object;</p>
<p id="p-0010" num="0009">a color range determination section that selects a first color range from a plurality of color ranges based on input information; and</p>
<p id="p-0011" num="0010">a color determination section that determines a color within the first color range to be a color of one of part objects corresponding to categories that form main parts of the model object under a predetermined condition.</p>
<p id="p-0012" num="0011">According to a second aspect of the invention, there is provided a program that is stored in a computer-readable information storage medium and generates an image of a model object formed by a plurality of part objects, the program causing a computer to function as:</p>
<p id="p-0013" num="0012">a storage section that stores the plurality of part objects, each of the plurality of part objects being classified into one of a plurality of categories;</p>
<p id="p-0014" num="0013">a setting section that selects a part object from the plurality of part objects for each of the plurality of categories, and sets the selected part object as an element of the model object;</p>
<p id="p-0015" num="0014">a selectable design information determination section that determines a plurality of pieces of selectable design information based on input information; and</p>
<p id="p-0016" num="0015">a design information determination section that selects one piece of design information from the plurality of pieces of selectable design information under a predetermined condition, and determines the selected piece of design information to be design information of one of part objects corresponding to categories that form main parts of the model object.</p>
<p id="p-0017" num="0016">According to a third aspect of the invention, there is provided a program that is stored in a computer-readable information storage medium and generates an image of a model object formed by a plurality of part objects, the program causing a computer to function as:</p>
<p id="p-0018" num="0017">a storage section that stores the plurality of part objects, each of the plurality of part objects being classified into one of a plurality of categories;</p>
<p id="p-0019" num="0018">a setting section that selects a part object from the plurality of part objects for each of the plurality of categories, and sets the selected part object as an element of the model object;</p>
<p id="p-0020" num="0019">a selectable texture determination section that determines a plurality of selectable textures based on input information; and</p>
<p id="p-0021" num="0020">a texture determination section that selects one texture from the plurality of selectable textures under a predetermined condition, and determines the selected texture to be a texture to be mapped onto one of part objects corresponding to categories that form main parts of the model object.</p>
<p id="p-0022" num="0021">According to a fourth aspect of the invention, there is provided an image generation device that generates an image of a model object formed by a plurality of part objects, the image generation device comprising:</p>
<p id="p-0023" num="0022">a storage section that stores the plurality of part objects, each of the plurality of part objects being classified into one of a plurality of categories;</p>
<p id="p-0024" num="0023">a setting section that selects a part object from the plurality of part objects for each of the plurality of categories, and sets the selected part object as an element of the model object;</p>
<p id="p-0025" num="0024">a color range determination section that selects a first color range from a plurality of color ranges based on input information; and</p>
<p id="p-0026" num="0025">a color determination section that determines a color within the first color range to be a color of one of part objects corresponding to categories that form main parts of the model object under a predetermined condition.</p>
<p id="p-0027" num="0026">According to a fifth aspect of the invention, there is provided an image generation device that generates an image of a model object formed by a plurality of part objects, the image generation device comprising:</p>
<p id="p-0028" num="0027">a storage section that stores the plurality of part objects, each of the plurality of part objects being classified into one of a plurality of categories;</p>
<p id="p-0029" num="0028">a setting section that selects a part object from the plurality of part objects for each of the plurality of categories, and sets the selected part object as an element of the model object;</p>
<p id="p-0030" num="0029">a selectable design information determination section that determines a plurality of pieces of selectable design information based on input information; and</p>
<p id="p-0031" num="0030">a design information determination section that selects one piece of design information from the plurality of pieces of selectable design information under a predetermined condition, and determines the selected piece of design information to be design information of one of part objects corresponding to categories that form main parts of the model object.</p>
<p id="p-0032" num="0031">According to a sixth aspect of the invention, there is provided an image generation device that generates an image of a model object formed by a plurality of part objects, the image generation device comprising:</p>
<p id="p-0033" num="0032">a storage section that stores the plurality of part objects, each of the plurality of part objects being classified into one of a plurality of categories;</p>
<p id="p-0034" num="0033">a setting section that selects a part object from the plurality of part objects for each of the plurality of categories, and sets the selected part object as an element of the model object;</p>
<p id="p-0035" num="0034">a selectable texture determination section that determines a plurality of selectable textures based on input information; and</p>
<p id="p-0036" num="0035">a texture determination section that selects one texture from the plurality of selectable textures under a predetermined condition, and determines the selected texture to be a texture to be mapped onto one of part objects corresponding to categories that form main parts of the model object.</p>
<p id="p-0037" num="0036">According to a seventh aspect of the invention, there is provided an image generation method of generating an image of a model object formed by a plurality of part objects, the image generation method, performed by an image generation device having a processor, comprising:</p>
<p id="p-0038" num="0037">storing the plurality of part objects in a storage section, each of the plurality of part objects being classified into one of a plurality of categories;</p>
<p id="p-0039" num="0038">selecting a part object from the plurality of part objects for each of the plurality of categories, and sets the selected part object as an element of the model object;</p>
<p id="p-0040" num="0039">selecting a first color range from a plurality of color ranges based on input information; and</p>
<p id="p-0041" num="0040">determining a color within the first color range to be a color of one of part objects corresponding to categories that form main parts of the model object under a predetermined condition.</p>
<p id="p-0042" num="0041">According to an eighth aspect of the invention, there is provided an image generation method of generating an image of a model object formed by a plurality of part objects, the image generation method, performed by an image generation device having a processor, comprising:</p>
<p id="p-0043" num="0042">storing the plurality of part objects in a storage section, each of the plurality of part objects being classified into one of a plurality of categories;</p>
<p id="p-0044" num="0043">selecting a part object from the plurality of part objects for each of the plurality of categories, and sets the selected part object as an element of the model object;</p>
<p id="p-0045" num="0044">determining a plurality of pieces of selectable design information based on input information; and</p>
<p id="p-0046" num="0045">selecting one piece of design information from the plurality of pieces of selectable design information under a predetermined condition, and determining the selected piece of design information to be design information of one of part objects corresponding to categories that form main parts of the model object.</p>
<p id="p-0047" num="0046">According to a ninth aspect of the invention, there is provided an image generation method of generating an image of a model object formed by a plurality of part objects, the image generation method, performed by an image generation device having a processor, comprising:</p>
<p id="p-0048" num="0047">storing the plurality of part objects in a storage section, each of the plurality of part objects being classified into one of a plurality of categories;</p>
<p id="p-0049" num="0048">selecting a part object from the plurality of part objects for each of the plurality of categories, and sets the selected part object as an element of the model object;</p>
<p id="p-0050" num="0049">determining a plurality of selectable textures based on input information; and</p>
<p id="p-0051" num="0050">selecting one texture from the plurality of selectable textures under a predetermined condition, and determining the selected texture to be a texture to be mapped onto one of part objects corresponding to categories that form main parts of the model object.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE SEVERAL VIEWS OF THE DRAWING</heading>
<p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. 1</figref> is a functional block diagram of an image generation device according to one embodiment of the invention.</p>
<p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. 2</figref> is a diagram illustrating a slot.</p>
<p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. 3</figref> is a diagram illustrating a slot.</p>
<p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. 4</figref> illustrates a table illustrating the relationship among a category, a part object, and the like.</p>
<p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. 5</figref> illustrates an example of a part object selection screen according to one embodiment of the invention.</p>
<p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. 6</figref> illustrates an example of a part object color selection screen.</p>
<p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. 7</figref> illustrates an example of a model object body information setting screen.</p>
<p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. 8</figref> illustrates an example of a model object creation screen.</p>
<p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. 9</figref> is a table illustrating a method of determining the color of a part object.</p>
<p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. 10</figref> is a diagram for describing a method of determining the colors of the skin and the hair of a model object.</p>
<p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. 11</figref> is a flowchart illustrating an example of a process according to one embodiment of the invention.</p>
<p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. 12</figref> illustrates a model object application example.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION OF THE EMBODIMENT</heading>
<p id="p-0064" num="0063">The invention may provide a program, an image generation device, and an image generation method that can determine the color of a part object so that a unified model object is created while reflecting the preference of the player.</p>
<p id="p-0065" num="0064">(1) According to one embodiment of the invention, there is provided a program that is stored in a computer-readable information storage medium and generates an image of a model object formed by a plurality of part objects, the program causing a computer to function as:</p>
<p id="p-0066" num="0065">a storage section that stores the plurality of part objects, each of the plurality of part objects being classified into one of a plurality of categories;</p>
<p id="p-0067" num="0066">a setting section that selects a part object from the plurality of part objects for each of the plurality of categories, and sets the selected part object as an element of the model object;</p>
<p id="p-0068" num="0067">a color range determination section that selects a first color range from a plurality of color ranges based on input information; and</p>
<p id="p-0069" num="0068">a color determination section that determines a color within the first color range to be a color of one of part objects corresponding to categories that form main parts of the model object under a predetermined condition.</p>
<p id="p-0070" num="0069">According to one embodiment of the invention, there is provided a computer-readable information storage medium storing a program that causes a computer to function as the above-described sections. According to one embodiment of the invention, there is provided an image generation device comprising the above-described sections.</p>
<p id="p-0071" num="0070">According to the above embodiments, similar colors are assigned to the main parts of the model object. Therefore, the color of the part object can be determined so that a unified model object is created while reflecting the preference of the player.</p>
<p id="p-0072" num="0071">(2) In each of the above program, information storage medium and image generation device,</p>
<p id="p-0073" num="0072">the color determination section may randomly select a color within the first color range and determine the selected color to be a color of one of the part objects.</p>
<p id="p-0074" num="0073">The model object becomes monotonous if an identical color is assigned to all of the part objects belonging to the categories forming main parts of the model object. According to the above embodiment, however, since the colors of the part objects belonging to the categories forming the main parts of the model object can be caused to differ within the same color range, the color of the part object can be determined so that the color arrangement of the part objects does not become monotonous while creating a unified model object.</p>
<p id="p-0075" num="0074">(3) In each of the above program, information storage medium and image generation device,</p>
<p id="p-0076" num="0075">the color range determination section may select a second color range that differs from the first color range from the plurality of color ranges; and</p>
<p id="p-0077" num="0076">the color determination section may determine a color within the second color range to be a color of one of part objects corresponding to categories that form secondary parts of the model object under a predetermined condition.</p>
<p id="p-0078" num="0077">According to the above embodiment, since similar colors are assigned to the part objects belonging to the categories forming secondary parts of the model object while assigning different colors to the main parts and the secondary parts, the colors of the part objects can be determined so that a unified model object is created while ensuring a wide variety of combinations with regard to the colors of the main parts and the secondary parts.</p>
<p id="p-0079" num="0078">(4) In each of the above program, information storage medium and image generation device,</p>
<p id="p-0080" num="0079">the color determination section may randomly select a color within the second color range and determine the selected color to be a color of one of the part objects.</p>
<p id="p-0081" num="0080">According to the above embodiment, since the colors of the part objects belonging to the categories forming the secondary parts of the model object can be caused to differ within the same color range, the colors of the part objects can be determined so that the color arrangement of the part objects does not become monotonous while creating a unified model object.</p>
<p id="p-0082" num="0081">(5) In each of the above program, information storage medium and image generation device,</p>
<p id="p-0083" num="0082">the color range determination section may select a third color range that differs from the first color range and the second color range from among the plurality of color ranges;</p>
<p id="p-0084" num="0083">the color determination section may determine a color within the first color range to be a color of a first area of one of the part objects corresponding to categories that form the main parts of the model object under a predetermined condition, and determine a color within the third color range to be a color of a second area of one of the part objects under a predetermined condition, the second area being smaller than the first area; and</p>
<p id="p-0085" num="0084">the color determination section may determine a color within the second color range to be a color of a third area of one of the part objects corresponding to categories that form the secondary parts of the model object under a predetermined condition, and determine a color within the third color range to be a color of a fourth area of one of the part objects under a predetermined condition, the fourth area being smaller than the third area.</p>
<p id="p-0086" num="0085">According to the above embodiment, the number of combinations of the colors assigned to the first area and the second area and the colors assigned to the third area and the fourth area can be increased in addition to the number of combinations of the colors assigned to the main parts and the secondary parts. According to the above embodiment, since similar colors are assigned to the second area and the fourth area, the number of combinations of colors can be increased while creating a unified model object.</p>
<p id="p-0087" num="0086">(6) According to one embodiment of the invention, there is provided a program that is stored in a computer-readable information storage medium and generates an image of a model object formed by a plurality of part objects, the program causing a computer to function as:</p>
<p id="p-0088" num="0087">a storage section that stores the plurality of part objects, each of the plurality of part objects being classified into one of a plurality of categories;</p>
<p id="p-0089" num="0088">a setting section that selects a part object from the plurality of part objects for each of the plurality of categories, and sets the selected part object as an element of the model object;</p>
<p id="p-0090" num="0089">a selectable design information determination section that determines a plurality of pieces of selectable design information based on input information; and</p>
<p id="p-0091" num="0090">a design information determination section that selects one piece of design information from the plurality of pieces of selectable design information under a predetermined condition, and determines the selected piece of design information to be design information of one of part objects corresponding to categories that form main parts of the model object.</p>
<p id="p-0092" num="0091">According to one embodiment of the invention, there is provided a computer-readable information storage medium storing a program that causes a computer to function as the above-described sections. According to one embodiment of the invention, there is provided an image generation device comprising the above-described sections.</p>
<p id="p-0093" num="0092">(7) According to one embodiment of the invention, there is provided a program that is stored in a computer-readable information storage medium and generates an image of a model object formed by a plurality of part objects, the program causing a computer to function as:</p>
<p id="p-0094" num="0093">a storage section that stores the plurality of part objects, each of the plurality of part objects being classified into one of a plurality of categories;</p>
<p id="p-0095" num="0094">a setting section that selects a part object from the plurality of part objects for each of the plurality of categories, and sets the selected part object as an element of the model object;</p>
<p id="p-0096" num="0095">a selectable texture determination section that determines a plurality of selectable textures based on input information; and</p>
<p id="p-0097" num="0096">a texture determination section that selects one texture from the plurality of selectable textures under a predetermined condition, and determines the selected texture to be a texture to be mapped onto one of part objects corresponding to categories that form main parts of the model object.</p>
<p id="p-0098" num="0097">According to one embodiment of the invention, there is provided a computer-readable information storage medium storing a program that causes a computer to function as the above-described sections. According to one embodiment of the invention, there is provided an image generation device comprising the above-described sections.</p>
<p id="p-0099" num="0098">(8) According to one embodiment of the invention, there is provided an image generation method of generating an image of a model object formed by a plurality of part objects, the image generation method, performed by an image generation device having a processor, comprising:</p>
<p id="p-0100" num="0099">storing the plurality of part objects in a storage section, each of the plurality of part objects being classified into one of a plurality of categories;</p>
<p id="p-0101" num="0100">selecting a part object from the plurality of part objects for each of the plurality of categories, and sets the selected part object as an element of the model object;</p>
<p id="p-0102" num="0101">selecting a first color range from a plurality of color ranges based on input information; and</p>
<p id="p-0103" num="0102">determining a color within the first color range to be a color of one of part objects corresponding to categories that form main parts of the model object under a predetermined condition.</p>
<p id="p-0104" num="0103">(9) According to one embodiment of the invention, there is provided an image generation method of generating an image of a model object formed by a plurality of part objects, the image generation method, performed by an image generation device having a processor, comprising:</p>
<p id="p-0105" num="0104">storing the plurality of part objects in a storage section, each of the plurality of part objects being classified into one of a plurality of categories;</p>
<p id="p-0106" num="0105">selecting a part object from the plurality of part objects for each of the plurality of categories, and sets the selected part object as an element of the model object;</p>
<p id="p-0107" num="0106">determining a plurality of pieces of selectable design information based on input information; and</p>
<p id="p-0108" num="0107">selecting one piece of design information from the plurality of pieces of selectable design information under a predetermined condition, and determining the selected piece of design information to be design information of one of part objects corresponding to categories that form main parts of the model object.</p>
<p id="p-0109" num="0108">(10) According to one embodiment of the invention, there is provided an image generation method of generating an image of a model object formed by a plurality of part objects, the image generation method, performed by an image generation device having a processor, comprising:</p>
<p id="p-0110" num="0109">storing the plurality of part objects in a storage section, each of the plurality of part objects being classified into one of a plurality of categories;</p>
<p id="p-0111" num="0110">selecting a part object from the plurality of part objects for each of the plurality of categories, and sets the selected part object as an element of the model object;</p>
<p id="p-0112" num="0111">determining a plurality of selectable textures based on input information; and</p>
<p id="p-0113" num="0112">selecting one texture from the plurality of selectable textures under a predetermined condition, and determining the selected texture to be a texture to be mapped onto one of part objects corresponding to categories that form main parts of the model object.</p>
<p id="p-0114" num="0113">Embodiments of the invention will now be described below. Note that the embodiments described below do not unduly limit the scope of the invention as stated in the claims. Also, not all the elements described below should be taken as essential requirements of the invention.</p>
<heading id="h-0005" level="1">1. Configuration</heading>
<p id="p-0115" num="0114"><figref idref="DRAWINGS">FIG. 1</figref> illustrates an example of a functional block diagram of an image generation device (game device or terminal) according to one embodiment of the invention. Note that the image generation device according to this embodiment may have a configuration in which some of the elements shown in <figref idref="DRAWINGS">FIG. 1</figref> are omitted.</p>
<p id="p-0116" num="0115">An operation section <b>160</b> allows the player (operator) to input operation data relating to a player's object (i.e., a player's character operated by the player). The function of the operation section <b>160</b> may be implemented by a lever, a button, a steering wheel, a microphone, a touch panel display, a casing, or the like.</p>
<p id="p-0117" num="0116">A storage section <b>170</b> serves as a work area for a processing section <b>100</b>, a communication section <b>196</b>, and the like. The function of the storage section <b>170</b> may be implemented by a RAM (VRAM) or the like.</p>
<p id="p-0118" num="0117">A slot state data storage section <b>177</b> stores slot state data. The term &#x201c;slot state data&#x201d; refers to data that specifies the presence or absence of part object data disposed in each slot and the type of part object data disposed in each slot. Each part object is formed by one or more pieces of part object data. Each piece of part object data is associated with one of the slots. Specifically, the slot state data refers to information for managing the part object data disposed in each slot (i.e., in slot units).</p>
<p id="p-0119" num="0118">An object data storage section <b>179</b> stores object data. The object data storage section <b>179</b> according to this embodiment includes a model object data storage section <b>179</b><i>a </i>and a part object data storage section <b>179</b><i>b. </i></p>
<p id="p-0120" num="0119">The model object data storage section <b>179</b><i>a </i>stores model object data that is formed by a plurality of part objects. The model object data storage section <b>179</b><i>a </i>according to this embodiment includes a plurality of buffers in order to store a plurality of model objects, and stores the model object data in each buffer.</p>
<p id="p-0121" num="0120">The part object data storage section <b>179</b><i>b </i>stores a plurality of part objects. Each of the part objects is classified into one of a plurality of categories and stored in the part object data storage section <b>179</b><i>b. </i></p>
<p id="p-0122" num="0121">The term &#x201c;category&#x201d; refers to an attribute for each part of the model object, such as a head, a neck, or an arm.</p>
<p id="p-0123" num="0122">Designation information that designates a category that differs from the category of the part object is stored in association with a given part object among the plurality of part objects stored in the part object data storage section <b>179</b><i>b. </i></p>
<p id="p-0124" num="0123">The term &#x201c;category of the part object&#x201d; refers to the category to which the part object belongs. The designation information designates the category of the part object that may adversely affect an image. Specifically, the designation information designates the restriction target category for which a simultaneous setting may be restricted.</p>
<p id="p-0125" num="0124">In this embodiment, the part object may be downloaded from a server based on input information (e.g., operation information from the operation section <b>160</b>) input by the player, and stored in the part object data storage section <b>179</b><i>b </i>and an information storage medium <b>180</b>.</p>
<p id="p-0126" num="0125">The information storage medium <b>180</b> (computer-readable medium) stores a program, data, and the like. The function of the information storage medium <b>180</b> may be implemented by an optical disk (CD or DVD), a magneto-optical disk (MO), a magnetic disk, a hard disk, a magnetic tape, a memory (ROM), a memory card, or the like.</p>
<p id="p-0127" num="0126">The processing section <b>100</b> performs various processes according to this embodiment based on a program (data) stored in the information storage medium <b>180</b>.</p>
<p id="p-0128" num="0127">Specifically, a program that causes a computer to function as each section according to this embodiment (i.e., a program that causes a computer to execute the process of each section) is stored in the information storage medium <b>180</b>. A player's personal data, game save data, and the like may be stored in the information storage medium <b>180</b>.</p>
<p id="p-0129" num="0128">In this embodiment, the model object data and the part object data may be stored in the information storage medium <b>180</b>, and loaded into the model object data storage section <b>179</b><i>a </i>and the part object data storage section <b>179</b><i>b</i>, if necessary.</p>
<p id="p-0130" num="0129">A display section <b>190</b> outputs an image generated according to this embodiment. The function of the display section <b>190</b> may be implemented by a CRT, an LCD, a touch panel display, a head mount display (HMD), or the like. A sound output section <b>192</b> outputs sound generated according to this embodiment. The function of the sound output section <b>192</b> may be implemented by a speaker, a headphone, or the like.</p>
<p id="p-0131" num="0130">The communication section <b>196</b> performs various types of control for communicating with the outside (e.g., host device or another image generation device). The function of the communication section <b>196</b> may be implemented by hardware such as a processor or a communication ASIC, a program, and the like.</p>
<p id="p-0132" num="0131">A program (data) that causes a computer to function as each section according to this embodiment may be downloaded from a server through a network, and stored in the storage section <b>170</b> or the information storage medium <b>180</b>. The scope of the invention also includes the program stored in a storage section of the server.</p>
<p id="p-0133" num="0132">The processing section <b>100</b> (processor) performs a game process, an image generation process, a sound generation process, and the like based on operation data from the operation section <b>160</b>, a program, and the like. The game process includes starting the game when game start conditions have been satisfied, proceeding with the game, disposing an object such as a character or a map, displaying an object, calculating the game result, finishing the game when game end conditions have been satisfied, and the like. The processing section <b>100</b> performs various processes using a main storage section <b>171</b> of the storage section <b>170</b> as a work area. The function of the processing section <b>100</b> may be implemented by hardware such as a processor (e.g., CPU or DSP) or an ASIC (e.g., gate array), or a program.</p>
<p id="p-0134" num="0133">The processing section <b>100</b> includes an object space setting section <b>110</b>, a movement/motion processing section <b>111</b>, a virtual camera control section <b>112</b>, a reception section <b>113</b>, a setting section <b>114</b>, a slot state update section <b>115</b>, a model object data update section <b>116</b>, a selectable design information determination section <b>117</b>, a design information determination section <b>118</b>, a drawing section <b>120</b>, and a sound generation section <b>130</b>. The processing section <b>100</b> may have a configuration in which some of these sections are omitted.</p>
<p id="p-0135" num="0134">The object space setting section <b>110</b> disposes an object (i.e., an object formed by a primitive such as a polygon, a free-form surface, or a subdivision surface) that represents a display object (e.g., model object, character, building, stadium, car, tree, pillar, wall, or map (topography)) in an object space. Specifically, the object space setting section <b>110</b> determines the position and the rotational angle (synonymous with orientation or direction) of the model object in a world coordinate system, and disposes the object at the determined position (X, Y, Z) and the determined rotational angle (rotational angles around X, Y, and Z axes).</p>
<p id="p-0136" num="0135">The movement/motion processing section <b>111</b> calculates the movement/motion (movement/motion simulation) of the object (e.g., character, car, or airplane). Specifically, the movement/motion processing section <b>111</b> causes the object to move or make a motion (animation) in the object space based on the operation data input by the player using the operation section <b>160</b>, a program (movement/motion algorithm), various types of data (motion data), and the like. More specifically, the movement/motion processing section <b>111</b> performs a simulation process that sequentially calculates movement information (position, rotational angle, speed, or acceleration) and motion information (position or rotational angle of each part object) of the object every frame ( 1/60th of a second). Note that the term &#x201c;frame&#x201d; refers to a time unit when performing an object movement/motion process (simulation process) or an image generation process.</p>
<p id="p-0137" num="0136">The virtual camera control section <b>112</b> controls a virtual camera (viewpoint) for generating an image viewed from a given (arbitrary) viewpoint in the object space. Specifically, the virtual camera control section <b>112</b> controls the position (X, Y, Z) or the rotational angle (rotational angles around X, Y, and Z axes) of the virtual camera (i.e., controls the viewpoint position or the line-of-sight direction).</p>
<p id="p-0138" num="0137">For example, when photographing the object (e.g., character, ball, or car) from behind using the virtual camera, the virtual camera control section <b>112</b> controls the position or the rotational angle (direction) of the virtual camera so that the virtual camera follows a change in position or rotation of the object. In this case, the virtual camera control section <b>112</b> may control the virtual camera based on information (e.g., position, rotational angle, or speed) of the object obtained by the movement/motion processing section <b>111</b>. Alternatively, the virtual camera control section <b>112</b> may rotate the virtual camera by a predetermined rotational angle, or may move the virtual camera along a predetermined path. In this case, the virtual camera control section <b>112</b> controls the virtual camera based on virtual camera data that specifies the position (moving path) or the rotational angle of the virtual camera. When a plurality of virtual cameras (viewpoints) are provided, the virtual camera control section <b>112</b> performs the above-described control process for each virtual camera.</p>
<p id="p-0139" num="0138">The reception section <b>113</b> receives selection of the part object for each category. Specifically, the reception section <b>113</b> receives selection of one category from among a plurality of categories displayed on the screen based on input information input by the player, and receives selection of one part object among a plurality of part objects belonging to the selected category. Specifically, the reception section <b>113</b> according to this embodiment receives selection of one part object belonging to one category based on input information input by the player.</p>
<p id="p-0140" num="0139">The setting section <b>114</b> determines the part object for each category, and sets the determined part object as an element of the model object. For example, the setting section <b>114</b> may determine the part object that has been selected and received by the reception section <b>113</b> to be the part object of each category.</p>
<p id="p-0141" num="0140">The setting section <b>114</b> may select a plurality of categories from all of the categories under a predetermined condition based on a given algorithm, determine one part object for each of the selected categories under a predetermined conditions and set the determined part object as an element of the model object.</p>
<p id="p-0142" num="0141">When the category of a specific part object that has been set as an element of the model object has been stored as the designation information of the selected part object and the category of the selected part object has been stored as the designation information of the specific part object, the setting section <b>114</b> according to this embodiment sets the selected part object as an element of the model object in place of the specific part object, or, the setting of the selected part object is restricted (and maintains the specific part object as an element of the model object). Specifically, when the category of the specific part object has been stored as the designation information of the selected part object and the category of the selected part object has been stored as the designation information of the specific part object, the selected part object and the already-set part object cannot be set together as elements of the model object.</p>
<p id="p-0143" num="0142">When the selected part object is set to be an element of the model object and another part object data has been disposed in the slot that corresponds to each of one or more pieces of part object data that forms the selected part object, the slot state update section <b>115</b> determines whether or not to update the slot state data based on part object priority information that determines the disposition priority of each part object data, and updates the slot state data stored in the slot state data storage section <b>177</b> based on the determination result. Specifically, the slot state update section <b>115</b> disposes (sets) the part object data for each slot.</p>
<p id="p-0144" num="0143">The term &#x201c;priority information&#x201d; refers to information for disposing (setting) the part object data with a higher disposition priority (i.e., higher-order part object data) in the slot preferentially over the part object data with a lower disposition priority (i.e., lower-order part object data). The priority information is set for each category.</p>
<p id="p-0145" num="0144">The model object data update section <b>116</b> updates the model object data stored in the model object data storage section <b>179</b><i>a </i>based on the slot state data stored in the slot state data storage section <b>177</b>.</p>
<p id="p-0146" num="0145">Specifically, the model object data update section <b>116</b> may read the part object data disposed in the slot for which the slot state data has been updated from the part object data storage section <b>179</b><i>b</i>, and store the part object data in a first buffer (update buffer) of the model object data storage section <b>179</b><i>a</i>. The model object data update section <b>116</b> may copy the part object data disposed in the slot for which the slot state data has not been updated to the first buffer from a second buffer of the model object data storage section <b>179</b><i>a </i>that stores the model object data that has been disposed. Note that the model object data update section <b>116</b> may read the part object data disposed in the slot for which the slot state data has not been updated from the part object data storage section <b>179</b><i>b</i>, and store the part object data in the first buffer.</p>
<p id="p-0147" num="0146">The selectable design information determination section <b>117</b> determines a plurality of pieces of selectable design information based on input information input by the player. The term &#x201c;a plurality of pieces of design information&#x201d; refers to a color range, a plurality of textures, and the like.</p>
<p id="p-0148" num="0147">A color range determination section <b>117</b><i>a </i>selects a first color range from a plurality of color ranges based on input information input by the player. The color range determination section <b>117</b><i>a </i>also selects a second color range that differs from the first color range from the plurality of color ranges. The color range determination section <b>117</b><i>a </i>also selects a third color range that differs from the first color range and the second color range from the plurality of color ranges.</p>
<p id="p-0149" num="0148">The term &#x201c;color range&#x201d; refers to a color range classified by hue, brightness, and saturation. For example, the plurality of color ranges include a red color range, a yellow color range, a green color range, a blue color range, and a purple color range.</p>
<p id="p-0150" num="0149">The color range determination section <b>117</b><i>a </i>selects the first color range, the second color range, and the third color range from the plurality of color ranges classified by hue, brightness, and saturation based on input information input by the player.</p>
<p id="p-0151" num="0150">The color range determination section <b>117</b><i>a </i>may select the color range based on the saturation (vividness) at an arbitrary level. For example, the color range determination section <b>117</b><i>a </i>determines the saturation based on input information input by the player, and selects the first color range, the second color range, and the third color range from a plurality of color ranges of the determined saturation.</p>
<p id="p-0152" num="0151">The color range determination section <b>117</b><i>a </i>may select the color range based on brightness at an arbitrary level. For example, the color range determination section <b>117</b><i>a </i>determines a brightness based on input information input by the player, and selects the first color range, the second color range, and the third color range from a plurality of color ranges of the determined brightness.</p>
<p id="p-0153" num="0152">A selectable texture determination section <b>117</b><i>b </i>determines a plurality of selectable textures based on input information input by the player.</p>
<p id="p-0154" num="0153">The design information determination section <b>118</b> selects one piece of design information from a plurality of pieces of selectable design information under a predetermined condition, and determines the selected design information to be the design information of a one of a plurality of part objects belonging to one of a plurality of categories forming main parts of the model object. The term &#x201c;design information&#x201d; refers to a color, a texture, and the like.</p>
<p id="p-0155" num="0154">A color determination section <b>118</b><i>a </i>determines a color within the first color range to be a color of one of a plurality of part objects under a predetermined condition, the part object belonging to one of a plurality of categories forming main parts of the model object.</p>
<p id="p-0156" num="0155">For example, the color determination section <b>118</b><i>a </i>randomly selects a color within the first color range and determines the selected color to be a color of one of a plurality of part objects belonging to one of a plurality of categories forming main parts of the model object.</p>
<p id="p-0157" num="0156">The color determination section <b>118</b><i>a </i>determines a color within the second color range to be a color of one of a plurality of part objects under a predetermined condition, the part object belonging to one of a plurality of categories forming secondary parts of the model object.</p>
<p id="p-0158" num="0157">The color determination section <b>118</b><i>a </i>randomly selects a color within the second color range, and determines the selected color to be a color of one of a plurality of part objects belonging to one of a plurality of categories forming the secondary parts.</p>
<p id="p-0159" num="0158">The color determination section <b>118</b><i>a </i>determines a color within the first color range to be a color of a first area of one of a plurality of part objects under a predetermined condition and determines a color within the third color range to be a color of a second area of the part object under another predetermined condition, the part object belonging to one of a plurality of categories forming the main parts of the model object, and the second area being smaller than the first area. The color determination section <b>118</b><i>a </i>determines a color within the second color range to be a color of a third area of one of a plurality of part objects under a predetermined condition and determines a color within the third color range to be a color of a fourth area of the part object under another predetermined condition, the part object belonging to one of a plurality of categories forming the secondary parts of the model object, and the fourth area being smaller than the third area.</p>
<p id="p-0160" num="0159">The color determination section <b>118</b><i>a </i>according to this embodiment stores the determined color of the part object (i.e., an element of the model object) in the object data storage section <b>179</b><i>b </i>as the vertex color of a polygon that forms the part object or the color of a texture mapped onto the part object.</p>
<p id="p-0161" num="0160">The texture determination section <b>118</b><i>b </i>selects one texture from a plurality of selectable textures under a predetermined condition, and determines the selected texture to be a texture to be mapped onto one of a plurality of part objects belonging to one of a plurality of categories forming the main parts of the model object.</p>
<p id="p-0162" num="0161">The drawing section <b>120</b> performs a drawing process based on the results of various processes (game process) performed by the processing section <b>100</b> to generate an image, and outputs the generated image to the display section <b>190</b>. When generating a three-dimensional game image, the drawing section <b>120</b> performs a geometric process such as coordinate transformation (world coordinate transformation or camera coordinate transformation), clipping, or perspective transformation, and creates drawing data (e.g., position coordinates of primitive vertices, texture coordinates, color data, normal vector, or alpha (&#x3b1;) value) based on the processing results. The drawing section <b>120</b> draws the object (one or more primitives) subjected to perspective transformation (geometric process) in an image buffer <b>173</b> (i.e., a buffer that can store image information in pixel units (e.g., frame buffer or intermediate buffer (work buffer)); VRAM) based on the drawing data (object data). The drawing section <b>120</b> thus generates an image viewed from the virtual camera (given viewpoint) in the object space. When a plurality of virtual cameras (viewpoints) are provided, the drawing section <b>120</b> performs the drawing process so that images viewed from the virtual cameras can be displayed on one screen as divided images.</p>
<p id="p-0163" num="0162">The drawing section <b>120</b> includes a geometric processing section <b>122</b>, a shading processing section <b>124</b>, an alpha blending section <b>126</b>, and a hidden surface removal section <b>128</b>.</p>
<p id="p-0164" num="0163">The geometric processing section <b>122</b> performs the geometric process on the object. Specifically, the geometric processing section <b>122</b> performs the geometric process such as coordinate transformation, clipping, perspective transformation, and light source calculations. The object data (e.g., object's vertex position coordinates, texture coordinates, color data (luminance data), normal vector, or alpha value) after the geometric process (perspective transformation) is stored in the object data storage section <b>179</b>.</p>
<p id="p-0165" num="0164">The shading processing section <b>124</b> performs a shading process that shades in the object. Specifically, the shading processing section <b>124</b> adjusts the luminance of the drawing pixel of the object based on the results (shade information) of light source calculations performed by the geometric processing section <b>122</b>. The shading processing section <b>124</b> may perform light source calculations in place of the geometric processing section <b>122</b>. The shading processing section <b>124</b> may subject the object to flat shading or smooth shading (e.g., Gouraud shading or Phong shading) as the shading process.</p>
<p id="p-0166" num="0165">The alpha blending section <b>126</b> performs a translucent blending process (e.g., normal alpha blending, additive alpha blending, or subtractive alpha blending) based on the alpha (&#x3b1;) value (A<sub>value</sub>). When performing normal alpha blending, the alpha blending section <b>126</b> performs a process shown by the following expressions (1) to (3).
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>R</i><sub>Q</sub>=(1&#x2212;&#x3b1;)&#xd7;<i>R</i><sub>1</sub><i>+&#x3b1;&#xd7;R</i><sub>2</sub>&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>G</i><sub>Q</sub>=(1&#x2212;&#x3b1;)&#xd7;<i>G</i><sub>1</sub><i>+&#x3b1;&#xd7;G</i><sub>2</sub>&#x2003;&#x2003;(2)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>B</i><sub>Q</sub>=(1&#x2212;&#x3b1;)&#xd7;<i>B</i><sub>1</sub><i>+&#x3b1;&#xd7;B</i><sub>2</sub>&#x2003;&#x2003;(3)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0167" num="0166">When performing additive alpha blending, the alpha blending section <b>126</b> performs a process shown by the following expressions (4) to (6).
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>R</i><sub>Q</sub><i>=R</i><sub>1</sub><i>+&#x3b1;&#xd7;R</i><sub>2</sub>&#x2003;&#x2003;(4)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>G</i><sub>Q</sub><i>=G</i><sub>1</sub><i>+&#x3b1;&#xd7;G</i><sub>2</sub>&#x2003;&#x2003;(5)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>B</i><sub>Q</sub><i>=B</i><sub>1</sub><i>+&#x3b1;&#xd7;B</i><sub>2</sub>&#x2003;&#x2003;(6)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0168" num="0167">When performing subtractive alpha blending, the alpha blending section <b>126</b> performs a process shown by the following expressions (7) to (9).
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>R</i><sub>Q</sub><i>=R</i><sub>1</sub><i>&#x2212;&#x3b1;&#xd7;R</i><sub>2</sub>&#x2003;&#x2003;(7)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>G</i><sub>Q</sub><i>=G</i><sub>1</sub><i>&#x2212;&#x3b1;&#xd7;G</i><sub>2</sub>&#x2003;&#x2003;(8)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>B</i><sub>Q</sub><i>=B</i><sub>1</sub><i>&#x2212;&#x3b1;&#xd7;B</i><sub>2</sub>&#x2003;&#x2003;(9)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0169" num="0168">R<sub>1</sub>, G<sub>1</sub>, and B<sub>1 </sub>represent the RGB components of the image (original image) that has been drawn in the image buffer <b>173</b>, and R<sub>2</sub>, G<sub>2</sub>, and B<sub>2 </sub>represent the RGB components of the image to be drawn in the image buffer <b>173</b>. R<sub>Q</sub>, G<sub>Q</sub>, and B<sub>Q </sub>represent the RGB components of the image obtained by alpha blending. Note that the alpha value is information that can be stored in association with each pixel (texel or dot), such as additional information other than the color information. The alpha value may be used as mask information, translucency (equivalent to transparency or opacity), bump information, or the like.</p>
<p id="p-0170" num="0169">The hidden surface removal section <b>128</b> performs a hidden surface removal process by a Z-buffer method (depth comparison method or Z-test) using a Z-buffer <b>175</b> (depth buffer) that stores the Z-value (depth information) of the drawing pixel. Specifically, the bidden surface removal section <b>128</b> refers to the Z-value stored in the Z-buffer <b>175</b> when drawing the drawing pixel corresponding to the primitive of the object. The hidden surface removal section <b>128</b> compares the Z-value stored in the Z-buffer <b>175</b> with the Z-value of the drawing pixel of the primitive. When the Z-value of the drawing pixel is the Z-value in front of the virtual camera (e.g., a small Z-value), the hidden surface removal section <b>128</b> draws the drawing pixel and updates the Z-value stored in the Z-buffer <b>175</b> with a new Z-value.</p>
<p id="p-0171" num="0170">The sound generation section <b>130</b> performs a sound generation process based on the results of various processes performed by the processing section <b>100</b> to generate game sound such as background music (BGM), effect sound, or voice, and outputs the generated game sound to the sound output section <b>192</b>.</p>
<p id="p-0172" num="0171">The image generation device according to this embodiment may be an image generation device dedicated to a single-player mode that allows only one player to play the game, or may be an image generation device provided with a multi-player mode that allows a plurality of players to play the game.</p>
<p id="p-0173" num="0172">When a plurality of players play the game, the game image and the game sound provided to the players may be generated by one terminal.</p>
<p id="p-0174" num="0173">The image generation device according to this embodiment may exchange data (e.g., input information) with one or more image generation devices connected through a network (transmission line or communication channel) to implement an online game.</p>
<heading id="h-0006" level="1">2. Outline of Method According to This Embodiment</heading>
<p id="h-0007" num="0000">2-1. Slot</p>
<p id="p-0175" num="0174">In this embodiment, the part object data is disposed (set) in each of a plurality of slots that form the model object.</p>
<p id="p-0176" num="0175">The term &#x201c;slot&#x201d; refers to a part object data disposition space set in a model coordinate system, and corresponds to a component part of the model object. In this embodiment, body slots A<b>1</b> to A<b>14</b> (first slots in a broad sense) in <figref idref="DRAWINGS">FIG. 2</figref> that correspond to the component parts of a human body, and item slots B<b>1</b> to B<b>10</b> (second slots in a broad sense) in <figref idref="DRAWINGS">FIG. 3</figref> that correspond to the component parts of an ornament attached to the human body, are provided.</p>
<p id="p-0177" num="0176">In <figref idref="DRAWINGS">FIG. 2</figref>, fourteen body slots A<b>1</b> to A<b>14</b> correspond to the component parts of the human body. For example, the arm is made up of four body slots A<b>4</b> to A<b>7</b>. The body slot A<b>4</b> corresponds to the upper arm, the body slot A<b>5</b> corresponds to the elbow, the body slot A<b>6</b> corresponds to the forearm, and the body slot A<b>7</b> corresponds to the hand, for example. A part object such as a shirt or a jacket can also be disposed in the body slots A<b>1</b> to A<b>14</b> instead of the part object corresponding to the component part of the human body. For example, parts of a short sleeve T-shirt can be formed using the body slots A<b>3</b>, A<b>4</b>, A<b>8</b>, and A<b>9</b>, and parts of a long sleeve T-shirt can be formed using the body slots A<b>3</b> to A<b>6</b>, A<b>8</b>, and A<b>9</b>.</p>
<p id="p-0178" num="0177">In <figref idref="DRAWINGS">FIG. 3</figref>, the item slots B<b>1</b> to B<b>10</b> are disposed to overlap one or more body slots. The item slots B<b>1</b> to B<b>10</b> are used when it is necessary to dispose a plurality of pieces of part object data in layers, such as when dressing the model object in two or more suits of clothes one over the other. In other words, the item slots B<b>1</b> to B<b>10</b> may be referred to as overlapping slots. For example, the item slot B<b>2</b> is used when disposing a beard on the face. Full armor may be set using the item slots B<b>1</b>, B<b>3</b> to B<b>6</b>, and B<b>8</b> to B<b>10</b>. The part object data that is given disposition priority over the part object data disposed in the body slots A<b>1</b> to A<b>14</b> is basically disposed in the item slots B<b>1</b> to B<b>10</b>. The part object data disposed in one item slot may be given disposition priority over the part object data disposed in another item slot. For example, the item slot B<b>7</b> is used when the model object wears a belt over a uniform or wears a belted skirt. The part object data that is given disposition priority over the part object data disposed in the item slot B<b>8</b> is disposed in the item slot B<b>7</b>.</p>
<p id="p-0179" num="0178">According to this embodiment, an image of a model object that wears a plurality of part objects in layers can be generated by providing the body slots A<b>1</b> to A<b>14</b> and the item slots B<b>1</b> to B<b>10</b> and disposing the part object data in each slot to form a model object. An image in which a joint is observed through the opening in armor can be generated by disposing the part object data in the item slots B<b>1</b> to B<b>10</b> and disposing the part object data in the body slots A<b>1</b> to A<b>14</b> in layers, for example.</p>
<p id="h-0008" num="0000">2-2. Categorization Method</p>
<p id="p-0180" num="0179">In this embodiment, each part object is stored in the part object data storage section <b>179</b><i>b </i>in association with one of a plurality of categories.</p>
<p id="p-0181" num="0180">Specifically, one or more pieces of part object data (slot-unit part object data) disposed in one or more body slots or item slots are stored in the part object data storage section <b>179</b><i>b </i>in association with one category.</p>
<p id="p-0182" num="0181">The relationship between the part objects is described below with reference to <figref idref="DRAWINGS">FIG. 4</figref>. For example, part objects &#x201c;samurai helmet&#x201d;, &#x201c;burgonet&#x201d;, and &#x201c;top hat&#x201d; are classified into a category &#x201c;Head&#x201d; that determines the upper part of the head of the model object, and disposed in the body slot A<b>1</b>.</p>
<p id="p-0183" num="0182">A part object &#x201c;Venetian mask&#x201d; is classified into a category &#x201c;Mask&#x201d; that determines the mask of the model object, and disposed in the body slot A<b>2</b>.</p>
<p id="p-0184" num="0183">In this embodiment, the priority information that indicates the disposition priority is set in advance for each category, as illustrated in <figref idref="DRAWINGS">FIG. 4</figref>. The priority information is used when updating the slot state data that determines the part objects disposed in the body slots A<b>1</b> to A<b>14</b> or the item slots B<b>1</b> to B<b>10</b>. Note that the slot state data may be exceptionally updated regardless of the priority information.</p>
<p id="p-0185" num="0184">In this embodiment, the part object data (slot-unit part object data) corresponding to each slot is stored in the part object data storage section <b>179</b><i>b</i>, as illustrated in <figref idref="DRAWINGS">FIG. 4</figref>. For example, part object data Inner_A<b>3</b>_<b>1</b> corresponding to the slot A<b>3</b>, part object data Inner_A<b>4</b>_<b>1</b> corresponding to the slot A<b>4</b>, part object data Inner_A<b>8</b>_<b>1</b> corresponding to the slot A<b>8</b>, and part object data Inner_A<b>9</b>_<b>1</b> corresponding to the slot A<b>9</b> are stored in the part object data storage section <b>179</b><i>b </i>for a part object &#x201c;T-shirt&#x201d; in a category &#x201c;Inner&#x201d;.</p>
<p id="p-0186" num="0185">In this embodiment, when disposing the higher-order part object data, disposition clear information for clearing the setting of the slot corresponding to the lower-order part object data may be stored.</p>
<p id="p-0187" num="0186">The disposition clear information specifies the slot for which the part object data is hidden behind the part object data disposed in the item slot and need not be disposed when the part object data is disposed in the body slot (e.g., A<b>8</b>) and the item slot (e.g., B<b>3</b>) in layers. When updating the slot state data, the part object data is not disposed in the slot specified by the disposition clear information, or the part object data that has been disposed is cleared based on the disposition clear information.</p>
<p id="h-0009" num="0000">2-3. Selection of Part Object</p>
<p id="p-0188" num="0187">In this embodiment, selection of the part object from each category is received based on input information input by the player, and the selected part object is set to be an element of the model object.</p>
<p id="p-0189" num="0188"><figref idref="DRAWINGS">FIG. 5</figref> illustrates an example of a part object selection screen. In this embodiment, selection of one of a plurality of categories is received, and selection of one of a plurality of part objects in the selected category is received, for example. <figref idref="DRAWINGS">FIG. 5</figref> illustrates an example of an image in which the category &#x201c;Head&#x201d; is selected from a plurality of categories, and the part object &#x201c;burgonet&#x201d; is selected from a plurality of part objects that belong to the category &#x201c;Head&#x201d;.</p>
<p id="p-0190" num="0189">In this embodiment, a parameter (e.g., power skill value and impact skill value) used for game calculations is set for each part object, and a skill that gives an advantage to the player can be set based on the parameter. This makes it possible to change the part objects on the model object taking account of the strategy of the game.</p>
<p id="h-0010" num="0000">2-4. Selection of Color of Part Object</p>
<p id="p-0191" num="0190">In this embodiment, the color of each part object that forms the model object is determined based on input information input by the player.</p>
<p id="p-0192" num="0191"><figref idref="DRAWINGS">FIG. 6</figref> illustrates an example of a color selection screen for selecting the color of the part object. In this embodiment, the color of the part object of each of categories that forms the model object is determined based on input information input by the player, for example. In <figref idref="DRAWINGS">FIG. 6</figref>, a color Head_C<b>1</b> assigned to an area <b>1</b> of the part object (e.g., &#x201c;top hat&#x201d;) in the category &#x201c;Head&#x201d; and a color Head_C<b>2</b> assigned to an area <b>2</b> of the part object are displayed, for example.</p>
<p id="p-0193" num="0192">In this embodiment, a plurality of levels of saturation (vividness) are provided, and a color can be determined based on the saturation. For example, red having a saturation level <b>1</b> is dark red, and red having a saturation level <b>9</b> is clear and bright red.</p>
<p id="p-0194" num="0193">In this embodiment, when selection of the area of a given part object (e.g., the area <b>1</b> of the part object &#x201c;top hat&#x201d;) has been received, color palette coordinates (hue and saturation) for determining the color (e.g., Head_C<b>1</b>) of the selected area are displayed on the display section <b>190</b>. The color of the selected area is determined based on the hue and the saturation indicated by the coordinate values input by the player.</p>
<p id="h-0011" num="0000">2-5. Setting of Body Information</p>
<p id="p-0195" num="0194">In this embodiment, body information of the model object can be determined based on input information input by the player. <figref idref="DRAWINGS">FIG. 7</figref> illustrates an example of a model object body information setting screen.</p>
<p id="p-0196" num="0195">In this embodiment, a hair color Hair_C<b>1</b>, an eyebrow/beard color Hair_C<b>2</b>, an eye color Eye_C<b>1</b>, and a skin color Skin_C<b>1</b> of the model object can be determined using a method similar to the pail object color determination method.</p>
<p id="h-0012" num="0000">2-6. Slot State Data Update Process</p>
<p id="p-0197" num="0196">In this embodiment, the part object data of the part object that forms the model object is disposed in the slot by a slot state data update process.</p>
<p id="p-0198" num="0197">In this embodiment, it is determined that the slot state data of the slot corresponding to the part object data of the part object selected by the player must be updated when no part object is disposed in the slot that corresponds to the part object selected by the player, and the part object data of the part object selected by the player is disposed in the slot that corresponds to the part object data of the part object selected by the player to update the slot state data.</p>
<p id="p-0199" num="0198">For example, when the player has selected the part object &#x201c;top hat&#x201d; from a plurality of part objects that belong to the category &#x201c;Head&#x201d; and the part object in the category &#x201c;Head&#x201d; is not disposed, the part object data of the part object &#x201c;top hat&#x201d; is disposed to update the slot state data.</p>
<p id="p-0200" num="0199">In this embodiment, when the part object data of the part object that is other than the part object selected by the player and belongs to the same category as the part object selected by the player has been disposed, the part object data of the part object selected by the player is disposed in place of the part object data of the part object other than the part object selected by the player to update the slot state data.</p>
<p id="p-0201" num="0200">For example, when the player has selected the part object &#x201c;top hat&#x201d; from the part objects that belong to the category &#x201c;Head&#x201d; and the part object data of the part object &#x201c;samurai helmet&#x201d; that belongs to the category &#x201c;Head&#x201d; has been disposed in the slot, the part object data of the part object &#x201c;top hat&#x201d; is disposed in place of the part object data of the part object &#x201c;samurai helmet&#x201d; to update the slot state data.</p>
<p id="p-0202" num="0201">In this embodiment, when the part object data of the part object that belongs to a category differing from the category of the part object data of the part object selected by the player has been disposed in the slot that corresponds to the part object data of the part object selected by the player, the disposition priorities of the categories determined based on the priority information are compared in slot units to update the slot state data.</p>
<p id="p-0203" num="0202">Note that whether or not to update the slot state data is determined in slot units corresponding to the part object data of the part object even when a plurality of slots are designated for the part object, and the slot state data is updated in slot units. This minimizes the amount of object data that forms the model object while appropriately dressing the model object in two or more suits of clothes one over the other.</p>
<p id="p-0204" num="0203">Specifically, the slot state data may be updated as described in (A) to (C) given below.</p>
<p id="p-0205" num="0204">(A) When the part object data of another part object (i.e., a part object that belongs to a category differing from the category of the part object data of the part object selected by the player) has been disposed in the slot that corresponds to the part object data of the part object selected by the player and the part object data of the part object selected by the player is given a disposition priority higher than that of the other part object, it is determined that the slot state data must be updated, and the part object data of the part object selected by the player is disposed in the slot in place of the part object data of the other part object to update the slot state data.</p>
<p id="p-0206" num="0205">(B) When the part object data of another part object (i.e., a part object that belongs to a category differing from the category of the part object data of the part object selected by the player) has been disposed in the slot that corresponds to the part object data of the part object selected by the player and the part object data of the part object selected by the player is given a disposition priority lower than that of the other part object, it is determined that the slot state data should not be updated. In this case, the part object data of the other part object is continuously disposed without updating the slot state data of the slot.</p>
<p id="p-0207" num="0206">(C) When the part object data of another part object (i.e., a part object that belongs to a category differing from the category of the part object data of the part object selected by the player) has been disposed in the slot that corresponds to the part object data of the part object selected by the player and the part object data of the part object selected by the player is given a disposition priority equal to that of the other part object, the part object data of the part object selected by the player is disposed in the slot in place of the part object data of the other part object to update the slot state data.</p>
<heading id="h-0013" level="1">3. Method of Generating Model Object</heading>
<p id="p-0208" num="0207">The method according to this embodiment allows the player to create a desired model object. However, the process of selecting the part object for each category and determining the color of each part object may be troublesome depending on the player. In this embodiment, a model object that reflects the preference of the player is created in advance so that the player can easily create an original character. Specifically, the character of the model object is roughly determined based on input information input by the player, and the part object set as an element of the model object is determined based on the character (parameter).</p>
<p id="p-0209" num="0208">In this embodiment, when the part object set as an element of the model object has been determined, the default slot state data of the model object provided in advance is updated to dispose the (slot-unit) part object data of the part object set as an element of the model object in each slot.</p>
<p id="p-0210" num="0209"><figref idref="DRAWINGS">FIG. 8</figref> illustrates an example of a model object creation screen. In this embodiment, selection of parameters for items &#x201c;equipment&#x201d;, &#x201c;battle&#x201d;, and &#x201c;principle&#x201d; is received based on input information input by the player, and the part object that forms the model object is determined based on the selected parameters. For example, a parameter &#x201c;light-armed&#x201d;, &#x201c;heavy-armed&#x201d;, or the like is set for the item &#x201c;equipment&#x201d;. A parameter &#x201c;attack&#x201d;, &#x201c;defense&#x201d;, &#x201c;none&#x201d;, or the like is set for the item &#x201c;battle&#x201d;. A parameter &#x201c;good&#x201d; or &#x201c;evil&#x201d; is set for the item &#x201c;principle&#x201d;. In this embodiment, the part object is determined based on these parameters.</p>
<p id="p-0211" num="0210">For example, a weight parameter is set in advance for each part object. When the parameter set for the item &#x201c;equipment&#x201d; is &#x201c;light-armed&#x201d;, one part object is randomly selected from a plurality of part objects for which the weight parameter is within a predetermined range (0&#x2266;weight parameter&#x3c;100). When the parameter set for the item &#x201c;equipment&#x201d; is &#x201c;heavy-armed&#x201d;, one part object is randomly selected from a plurality of part objects for which the weight parameter is within a predetermined range (100&#x2266;weight parameter). A lottery process may be performed using a given algorithm based on the parameter of the item &#x201c;equipment&#x201d; selected by the player, and the part object may be selected based on the lottery result. For example, when the player has selected the parameter &#x201c;light-armed&#x201d;, the part object corresponding to the parameter &#x201c;light-armed&#x201d; may be selected with a predetermined win probability (e.g., 80%).</p>
<p id="p-0212" num="0211">In this embodiment, when selection of the parameter &#x201c;attack&#x201d; for the item &#x201c;battle&#x201d; has been received, one part object is randomly selected from a plurality of part objects having a high attack capability (e.g., a plurality of part objects for which the power skill value is equal to or larger than a predetermined value). When selection of the parameter &#x201c;defense&#x201d; for the item &#x201c;battle&#x201d; has been received, one part object is randomly selected from a plurality of part objects having a high defense capability. A lottery process may be performed based on a given algorithm based on the parameter of the item &#x201c;battle&#x201d; selected by the player, and the part object may be selected based on the lottery result.</p>
<p id="p-0213" num="0212">Identification information &#x201c;good&#x201d; or &#x201c;evil&#x201d; is set in advance corresponding to each part object, for example. When the parameter &#x201c;good&#x201d; has been selected for the item &#x201c;principle&#x201d;, one part object is randomly selected from a plurality of part objects corresponding to the identification information &#x201c;good&#x201d;. When the parameter &#x201c;evil&#x201d; has been selected for the item &#x201c;principle&#x201d;, one part object is randomly selected from a plurality of part objects corresponding to the identification information &#x201c;evil&#x201d;.</p>
<p id="p-0214" num="0213">In this embodiment, the part object of to each category is determined based on all of the parameters selected by the player for the items &#x201c;equipment&#x201d;, &#x201c;battle&#x201d;, and &#x201c;principle&#x201d;. In the example of <figref idref="DRAWINGS">FIG. 8</figref>, one part object is randomly selected from a plurality of part objects for which the weight parameter is within a predetermined range (0&#x2266;weight parameter&#x3c;100) and which correspond to the identification information &#x201c;good&#x201d;.</p>
<p id="p-0215" num="0214">In this embodiment, the part object need not necessarily be determined for each of the categories. For example, only the part objects of given categories (e.g., categories &#x201c;Head&#x201d;, &#x201c;Shoulder&#x201d;, &#x201c;Inner&#x201d;, &#x201c;Lower&#x201d;, and &#x201c;Feet&#x201d;) may be determined, as illustrated in <figref idref="DRAWINGS">FIG. 8</figref>.</p>
<p id="p-0216" num="0215">In this embodiment, the body information of the model object is also randomly selected (determined).</p>
<heading id="h-0014" level="1">4. Color Adjustment Method</heading>
<p id="p-0217" num="0216">In this embodiment, the part object set as an element of the model object is determined. However, since the color of the part object is not uniform, a model object that lacks unity may be generated. Therefore, this embodiment employs a method that determines the color of the part object set as an element of the model object.</p>
<p id="h-0015" num="0000">4-1. Method of Determining Color of Part Object</p>
<p id="p-0218" num="0217">In this embodiment, the first color range, the second color range, and the third color range are selected (determined) from a plurality of color ranges.</p>
<p id="p-0219" num="0218">The term &#x201c;color range&#x201d; refers to a color range (scale, extent, limits) classified by hue, brightness, and saturation. For example, the plurality of color ranges include a red color range, a yellow color range, a green color range, a blue color range, and a purple color range. In this embodiment, the color range is selected (determined) based on the levels of saturation (vividness). For example, a red color range, a yellow color range, a green color range, a blue color range, and a purple color range of a saturation level <b>1</b> and a red color range, a yellow color range, a green color range, a blue color range, and a purple color range of a saturation level <b>9</b> are provided in advance.</p>
<p id="p-0220" num="0219">The first color range is a color range for determining the color of the primary area (the primary surface area, the major surface area) of each of a plurality of part objects belonging to categories forming main parts (main regions, large parts, large regions) of the model object. The main parts are parts of the model object each having a large surface area such as a trunk part. The categories (the category group) forming the main parts include the categories &#x201c;Inner&#x201d;, &#x201c;Upper&#x201d;, and &#x201c;Lower&#x201d;, for example. Specifically, the first color range corresponds to the basic colors or the concept colors of the model object.</p>
<p id="p-0221" num="0220">The primary area is a largest area among areas of a part object, each having a single color. In the example of <figref idref="DRAWINGS">FIG. 8</figref>, the area of the part object of the category &#x201c;Inner&#x201d; to which a color &#x201c;Inner_C<b>1</b>&#x201d; is assigned is the primary area. In <figref idref="DRAWINGS">FIG. 8</figref>, the area of the part object of the category &#x201c;Shoulder&#x201d; to which a color &#x201c;Shoulder_C<b>1</b>&#x201d; is assigned is the primary area.</p>
<p id="p-0222" num="0221">The second color range is a color range for determining the color of the primary area of each of a plurality of part objects belonging to categories forming secondary parts (secondary regions, sub parts, small parts, small regions) of the model object. The secondary parts are parts of the model object each having a surface area smaller than the main parts (e.g., end part). The categories (the category group) forming the secondary parts include the categories &#x201c;Head&#x201d;, &#x201c;Mask&#x201d;, &#x201c;Neck&#x201d;, &#x201c;Shoulder&#x201d;, and &#x201c;Feet&#x201d;, for example. Specifically, the second color range corresponds to the secondary color of the model object.</p>
<p id="p-0223" num="0222">The third color range is a color range for determining the color of the secondary area (the secondary surface area, the minor surface area) of each of a plurality of part objects belonging to categories forming the main parts and the secondary parts of the model object. The secondary area is one of areas of a part object having a single color smaller than the primary area. When a plurality of areas having a surface area smaller than the primary area exist, one of the areas is determined to be the secondary area. In the example of <figref idref="DRAWINGS">FIG. 8</figref>, the area of the part object of the category &#x201c;Inner&#x201d; to which a color &#x201c;Inner_C<b>2</b>&#x201d; is assigned is the secondary area. In <figref idref="DRAWINGS">FIG. 8</figref>, the area of the part object of the category &#x201c;Shoulder&#x201d; to which a color &#x201c;Shoulder_C<b>2</b>&#x201d; is assigned is the secondary area. Specifically, the third color range corresponds to the accent color of the model object.</p>
<p id="p-0224" num="0223">The method of determining the first color range, the second color range, and the third color range according to this embodiment is described in detail below. In this embodiment, the saturation is determined based on the parameter &#x201c;good&#x201d; or &#x201c;evil&#x201d; selected for the item &#x201c;principle&#x201d; based on input information input by the player. For example, the saturation level <b>9</b> is determined to be the saturation when the parameter &#x201c;good&#x201d; has been selected for the item &#x201c;principle&#x201d;, and the saturation level <b>1</b> is determined to be the saturation when the parameter &#x201c;evil&#x201d; has been selected for the item &#x201c;principle&#x201d;. In the example of <figref idref="DRAWINGS">FIG. 8</figref>, since the parameter &#x201c;good&#x201d; has been selected for the item &#x201c;principle&#x201d;, the saturation level <b>9</b> is determined to be the saturation.</p>
<p id="p-0225" num="0224">In this embodiment, the first color range, the second color range, and the third color range are selected from a plurality of color ranges of the determined saturation. In this embodiment, the first color range, the second color range, and the third color range differ from one another. For example, the first color range, the second color range, and the third color range differing from one another are randomly selected from the red color range, the yellow color range, the green color range, the blue color range, and the purple color range of the saturation level <b>9</b>.</p>
<p id="p-0226" num="0225">A process of determining the colors of the primary area and the secondary area of each part object based on the first color range, the second color range, and the third color range thus selected is described below with reference to <figref idref="DRAWINGS">FIG. 9</figref>.</p>
<p id="p-0227" num="0226">In this embodiment, a color within the first color range is randomly selected for each of a plurality of part objects belonging to one of the categories forming the main parts of the model object, and the selected color is determined to be the color of the primary area of a corresponding part object.</p>
<p id="p-0228" num="0227">For example, the blue color range of the saturation level <b>9</b> has been determined to be the first color range. In this case, a color (e.g., dark blue) randomly selected from the blue color range of the saturation level <b>9</b> is determined to be the color of the primary area of the part object corresponding to the category &#x201c;Inner&#x201d; (main part). A color (e.g., light blue) randomly selected from the blue color range of the saturation level <b>9</b> is determined to be the color of the primary area of the part object of the category &#x201c;Lower&#x201d; (main part).</p>
<p id="p-0229" num="0228">Therefore, the concept colors of the model object are unified into blue (i.e., a unified color tone is achieved). In this embodiment, since a color within the first color range is randomly selected for each of a plurality of part objects belonging to one of the categories forming the main parts of the model object, the colors of the primary areas of the part objects of the categories &#x201c;Inner&#x201d; and &#x201c;Lower&#x201d; may differ from each other within the blue color range. Therefore, the color arrangement does not become monotonous.</p>
<p id="p-0230" num="0229">In this embodiment, a color within the second color range is randomly selected for each of a plurality of part objects belonging to one of the categories forming the secondary parts of the model object, and the selected color is determined to be the color of the primary area of a corresponding part object.</p>
<p id="p-0231" num="0230">For example, the red color range of the saturation level <b>9</b> has been determined to be the second color range. In this case, a color (e.g., dark red) randomly selected from the red color range of the saturation level <b>9</b> is determined to be the color of the primary area of the part object of the category &#x201c;Head&#x201d; (secondary part). A color (e.g., red close to pink) randomly selected from the red color range of the saturation level <b>9</b> is determined to be the color of the primary area of the part object of the category &#x201c;Shoulder&#x201d; (secondary part). A color (e.g., wine red) randomly selected from the red color range of the saturation level <b>9</b> is determined to be the color of the primary area of the part object of the category &#x201c;Feet&#x201d; (secondary part).</p>
<p id="p-0232" num="0231">Therefore, the secondary color of the model object is unified into red (i.e., a unified color tone is achieved). In this embodiment, since a color within the second color range is randomly selected for each of a plurality of part objects belonging to one of the categories forming the secondary parts of the model object, the colors of the primary areas of the part objects of the categories &#x201c;Head&#x201d;, &#x201c;Shoulder&#x201d;, and &#x201c;Feet&#x201d; may differ from each other within the red color range. Therefore, the color arrangement does not become monotonous.</p>
<p id="p-0233" num="0232">In this embodiment, a color within the third color range is randomly selected for each of a plurality of part objects belonging to one of the categories forming the main parts and the secondary parts of the model object, and the selected color is determined to be the color of the secondary area of a corresponding part object.</p>
<p id="p-0234" num="0233">For example, the green color range of the saturation level <b>9</b> has been determined to be the third color range. In this case, a color (e.g., green blue) randomly selected from the green color range of the saturation level <b>9</b> is determined to be the color of the secondary area of the part object of the category &#x201c;Inner&#x201d; (main part). A color (e.g., yellowish green) randomly selected from the green color range of the saturation level <b>9</b> is determined to be the color of the secondary area of the part object of the category &#x201c;Lower&#x201d; (main part). A color randomly selected from the green color range of the saturation level <b>9</b> is determined to be the color of the secondary area of the part object of the category &#x201c;Feet&#x201d; (secondary part). A color is similarly determined for the categories &#x201c;Shoulder&#x201d; and &#x201c;Feet&#x201d;.</p>
<p id="p-0235" num="0234">Therefore, the accent color of the model object is unified into green (i.e., a unified color tone is achieved). Moreover, since the accent colors of the part objects may differ to some extent from each other within the green color range, the color arrangement does not become monotonous.</p>
<p id="p-0236" num="0235">When the colors of the primary area and the secondary area of each part object have been determined based on the first color range, the second color range, and the third color range, the determined colors are stored in a model buffer of the model object data storage section <b>179</b><i>a. </i></p>
<p id="p-0237" num="0236">For example, when the color of the primary area of the part object &#x201c;top hat&#x201d; belonging to the category &#x201c;Head&#x201d; has been determined, the determined color is stored in the model buffer as a color &#x201c;Head_C<b>1</b>&#x201d; for the model object. The determined color is used as the vertex color of a polygon that forms the primary area of the part object &#x201c;top hat&#x201d; of the model object or the color of a texture mapped onto the primary area of the part object &#x201c;top hat&#x201d; of the model object when the drawing section <b>120</b> performs the drawing process, for example.</p>
<p id="h-0016" num="0000">4-2. Method of Determining Skin Color</p>
<p id="p-0238" num="0237">In this embodiment, one skin color range is randomly selected from three skin color ranges when creating the model object, and a color within the selected skin color range is determined to be a color &#x201c;Skin_C<b>1</b>&#x201d; of the skin of the model object. For example, a human white skin color range, a human black skin color range, and a non-human skin color range (green or purple) are provided, and one of the skin color ranges is randomly selected. In this embodiment, a color randomly selected within the selected skin color range is determined to be the color of the skin of the model object. In particular, since the human white skin color range and the human black skin color range are provided in advance, a color that is easily recognized to be a human skin color can be set.</p>
<p id="h-0017" num="0000">4-3. Method of Determining Colors of Hair and Eyebrows/Beard</p>
<p id="p-0239" num="0238">In this embodiment, a plurality of hair color ranges (e.g., red, brown, black, and blond) are provided, and one hair color range is randomly selected from the plurality of hair color ranges when creating the model object. As illustrated in <figref idref="DRAWINGS">FIG. 10</figref>, a color (e.g., dark brown) randomly selected within the hair color range is determined to be a hair color &#x201c;Hair_C<b>1</b>&#x201d;, for example. Likewise, a color (e.g., chestnut) randomly selected within the hair color range is determined to be an eyebrows/beard color &#x201c;Hair_C<b>2</b>&#x201d;. Therefore, since similar colors are assigned to the hair and the eyebrows/beard, a unified model object can be created.</p>
<heading id="h-0018" level="1">5. Process According to This Embodiment</heading>
<p id="p-0240" num="0239">A process of determining the color of each part object set as an element of the model object according to this embodiment is described below with reference to <figref idref="DRAWINGS">FIG. 11</figref>.</p>
<p id="p-0241" num="0240">The first color range, the second color range, and the third color range are selected from a plurality of color ranges (step S<b>10</b>). The color determination target part object is then determined (step S<b>20</b>). Whether or not the category of the part object is included in the categories that form the main parts is determined (step S<b>30</b>). When the category of the part object is included in the categories that form the main parts (Y in step S<b>30</b>), a color within the first color range is determined to be the color of the primary area of the part object (step S<b>40</b>). When the category of the part object is not included in the categories that form the main parts (N in step S<b>30</b>), the category of the part object is determined to be included in the categories that form the secondary parts, and a color within the second color range is determined to be the color of the primary area of the part object (step S<b>50</b>).</p>
<p id="p-0242" num="0241">A color within the third color range is then determined to be the color of the secondary area of the part object (step S<b>60</b>). Whether or not the colors of all of the part objects have been determined is determined (step S<b>70</b>). When the colors of all of the part objects have been determined (Y in step S<b>70</b>), the process ends. When the colors of all of the part objects have not been determined (N in step S<b>70</b>), the process returns to the step S<b>20</b>. The process is thus completed.</p>
<heading id="h-0019" level="1">6. Application Example</heading>
<p id="p-0243" num="0242">This embodiment may be applied to an animal model object, a monster model object, or the like instead of a human model object. As illustrated in <figref idref="DRAWINGS">FIG. 12</figref>, a skin color (Skin_C<b>1</b>) and a whisker color (Hair_C<b>2</b>) of a cat object may be determined in the same manner as described above, for example.</p>
<p id="p-0244" num="0243">In this embodiment, the color of the part object set as an element of the model object customized by the player may be determined.</p>
<p id="p-0245" num="0244">In this embodiment, the colors of some part objects (e.g., only the part objects belonging to the categories &#x201c;Head&#x201d; and &#x201c;Feet&#x201d;) among the part objects set as elements of the model object may be determined.</p>
<p id="p-0246" num="0245">When three or more colors are assigned to the part object, the color range may be selected for each area, and a color within the selected color range may be determined for each area.</p>
<p id="p-0247" num="0246">For example, when a part object among a plurality of part objects that form the model object does not have a primary area, but has four secondary areas (i.e., colors are assigned to four areas of the part object), a fourth color range and a fifth color range are selected in addition to the first color range, the second color range, and the third color range.</p>
<p id="p-0248" num="0247">A color within the first color range is determined to be the color of the first secondary area, a color within the third color range is determined to be the color of the second secondary area, a color within the fourth color range is determined to be the color of the third secondary area, and a color within the fifth color range is determined to be the color of the fourth secondary area for each part object belonging to one of the categories forming the main parts of the model object. A color within the second color range is determined to be the color of the first secondary area, a color within the third color range is determined to be the color of the second secondary area, a color within the fourth color range is determined to be the color of the third secondary area, and a color within the fifth color range is determined to be the color of the fourth secondary area for each part object belonging to one of the categories forming the secondary parts of the model object. Note that the relationship &#x201c;first secondary area&#x2267;second secondary area&#x2267;third secondary area&#x2267;fourth secondary area&#x201d; is satisfied.</p>
<p id="p-0249" num="0248">In this embodiment, the above-described method may be applied to determine a texture. For example, a plurality of textures are selected from a plurality of selectable textures based on input information input by the player, one texture is selected from the selected textures, and the selected texture is determined to be the texture to be mapped onto the part object.</p>
<p id="p-0250" num="0249">For example, a plurality of textures are selected from a plurality of polka-dotted textures, a plurality of striped textures, a plurality of checkered textures, a plurality of star-shaped textures, and a plurality of heart-shaped textures based on the principle (good or evil) selected based on input information input by the player. When a plurality of polka-dotted textures (polka-dotted texture group) have been selected, one texture is randomly selected from a large polka-dotted texture, a medium polka-dotted texture, and a small polka-dotted texture. The selected texture (e.g., small polka-dotted texture) is determined to be a texture to be mapped onto the primary area of a corresponding part object among a plurality of part objects belonging to one of the categories forming the main parts of the model object. Likewise, a texture randomly selected from a plurality of striped textures is determined to be the texture to be mapped onto the primary area of each of a plurality of part objects belonging to one of the categories forming the secondary parts of the model object, and a texture randomly selected from a plurality of a heart-shaped textures is determined to be the texture to be mapped onto the secondary area of each of a plurality of part objects belonging to one of the categories forming the main parts and the secondary parts of the model object.</p>
<p id="p-0251" num="0250">Although only some embodiments of this invention have been described in detail above, those skilled in the art will readily appreciate that many modifications are possible in the embodiments without materially departing from the novel teachings and advantages of this invention. Accordingly, all such modifications are intended to be included within the scope of the invention.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A program that is stored in a computer-readable information storage medium and generates an image of a model object formed by a plurality of part objects, the program causing a computer to function as:
<claim-text>a storage section that stores the plurality of part objects, each of the plurality of part objects being classified into one of a plurality of categories;</claim-text>
<claim-text>a setting section that selects a part object from the plurality of part objects for each of the plurality of categories, and sets the selected part object as an element of the model object;</claim-text>
<claim-text>a color range determination section that selects a first color range from a plurality of color ranges based on input information; and</claim-text>
<claim-text>a color determination section that determines a color within the first color range to be a color of one of part objects corresponding to categories that form main parts of the model object under a predetermined condition.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The program as defined in <claim-ref idref="CLM-00001">claim 1</claim-ref>,
<claim-text>wherein the color determination section randomly selects a color within the first color range and determines the selected color to be a color of one of the part objects.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The program as defined in <claim-ref idref="CLM-00001">claim 1</claim-ref>,
<claim-text>wherein the color range determination section selects a second color range that differs from the first color range from the plurality of color ranges; and</claim-text>
<claim-text>wherein the color determination section determines a color within the second color range to be a color of one of part objects corresponding to categories that form secondary parts of the model object under a predetermined condition.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The program as defined in <claim-ref idref="CLM-00003">claim 3</claim-ref>,
<claim-text>wherein the color determination section randomly selects a color within the second color range and determines the selected color to be a color of one of the part objects.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The program as defined in <claim-ref idref="CLM-00003">claim 3</claim-ref>,
<claim-text>wherein the color range determination section selects a third color range that differs from the first color range and the second color range from among the plurality of color ranges;</claim-text>
<claim-text>wherein the color determination section determines a color within the first color range to be a color of a first area of one of the part objects corresponding to categories that form the main parts of the model object under a predetermined condition, and determines a color within the third color range to be a color of a second area of one of the part objects under a predetermined condition, the second area being smaller than the first area; and</claim-text>
<claim-text>wherein the color determination section determines a color within the second color range to be a color of a third area of one of the part objects corresponding to categories that form the secondary parts of the model object under a predetermined condition, and determines a color within the third color range to be a color of a fourth area of one of the part objects under a predetermined condition, the fourth area being smaller than the third area.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. A program that is stored in a computer-readable information storage medium and generates an image of a model object formed by a plurality of part objects, the program causing a computer to function as:
<claim-text>a storage section that stores the plurality of part objects, each of the plurality of part objects being classified into one of a plurality of categories;</claim-text>
<claim-text>a setting section that selects a part object from the plurality of part objects for each of the plurality of categories, and sets the selected part object as an element of the model object;</claim-text>
<claim-text>a selectable design information determination section that determines a plurality of pieces of selectable design information based on input information; and</claim-text>
<claim-text>a design information determination section that selects one piece of design information from the plurality of pieces of selectable design information under a predetermined condition, and determines the selected piece of design information to be design information of one of part objects corresponding to categories that form main parts of the model object.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. A program that is stored in a computer-readable information storage medium and generates an image of a model object formed by a plurality of part objects, the program causing a computer to function as:
<claim-text>a storage section that stores the plurality of part objects, each of the plurality of part objects being classified into one of a plurality of categories;</claim-text>
<claim-text>a setting section that selects a part object from the plurality of part objects for each of the plurality of categories, and sets the selected part object as an element of the model object;</claim-text>
<claim-text>a selectable texture determination section that determines a plurality of selectable textures based on input information; and</claim-text>
<claim-text>a texture determination section that selects one texture from the plurality of selectable textures under a predetermined condition, and determines the selected texture to be a texture to be mapped onto one of part objects corresponding to categories that form main parts of the model object.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. An image generation device that generates an image of a model object formed by a plurality of part objects, the image generation device comprising:
<claim-text>a storage section that stores the plurality of part objects, each of the plurality of part objects being classified into one of a plurality of categories;</claim-text>
<claim-text>a setting section that selects a part object from the plurality of part objects for each of the plurality of categories, and sets the selected part object as an element of the model object;</claim-text>
<claim-text>a color range determination section that selects a first color range from a plurality of color ranges based on input information; and</claim-text>
<claim-text>a color determination section that determines a color within the first color range to be a color of one of part objects corresponding to categories that form main parts of the model object under a predetermined condition.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. An image generation device that generates an image of a model object formed by a plurality of part objects, the image generation device comprising:
<claim-text>a storage section that stores the plurality of part objects, each of the plurality of part objects being classified into one of a plurality of categories;</claim-text>
<claim-text>a setting section that selects a part object from the plurality of part objects for each of the plurality of categories, and sets the selected part object as an element of the model object;</claim-text>
<claim-text>a selectable design information determination section that determines a plurality of pieces of selectable design information based on input information; and</claim-text>
<claim-text>a design information determination section that selects one piece of design information from the plurality of pieces of selectable design information under a predetermined condition, and determines the selected piece of design information to be design information of one of part objects corresponding to categories that form main parts of the model object.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. An image generation device that generates an image of a model object formed by a plurality of part objects, the image generation device comprising:
<claim-text>a storage section that stores the plurality of part objects, each of the plurality of part objects being classified into one of a plurality of categories;</claim-text>
<claim-text>a setting section that selects a part object from the plurality of part objects for each of the plurality of categories, and sets the selected part object as an element of the model object;</claim-text>
<claim-text>a selectable texture determination section that determines a plurality of selectable textures based on input information; and</claim-text>
<claim-text>a texture determination section that selects one texture from the plurality of selectable textures under a predetermined condition, and determines the selected texture to be a texture to be mapped onto one of part objects corresponding to categories that form main parts of the model object.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. An image generation method of generating an image of a model object formed by a plurality of part objects, the image generation method, performed by an image generation device having a processor, comprising:
<claim-text>storing the plurality of part objects in a storage section, each of the plurality of part objects being classified into one of a plurality of categories;</claim-text>
<claim-text>selecting a part object from the plurality of part objects for each of the plurality of categories, and sets the selected part object as an element of the model object;</claim-text>
<claim-text>selecting a first color range from a plurality of color ranges based on input information; and</claim-text>
<claim-text>determining a color within the first color range to be a color of one of part objects corresponding to categories that form main parts of the model object under a predetermined condition.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. An image generation method of generating an image of a model object formed by a plurality of part objects, the image generation method, performed by an image generation device having a processor, comprising:
<claim-text>storing the plurality of part objects in a storage section, each of the plurality of part objects being classified into one of a plurality of categories;</claim-text>
<claim-text>selecting a part object from the plurality of part objects for each of the plurality of categories, and sets the selected part object as an element of the model object;</claim-text>
<claim-text>determining a plurality of pieces of selectable design information based on input information; and</claim-text>
<claim-text>selecting one piece of design information from the plurality of pieces of selectable design information under a predetermined condition, and determining the selected piece of design information to be design information of one of part objects corresponding to categories that form main parts of the model object.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. An image generation method of generating an image of a model object formed by a plurality of part objects, the image generation method, performed by an image generation device having a processor, comprising:
<claim-text>storing the plurality of part objects in a storage section, each of the plurality of part objects being classified into one of a plurality of categories;</claim-text>
<claim-text>selecting a part object from the plurality of part objects for each of the plurality of categories, and sets the selected part object as an element of the model object;</claim-text>
<claim-text>determining a plurality of selectable textures based on input information; and</claim-text>
<claim-text>selecting one texture from the plurality of selectable textures under a predetermined condition, and determining the selected texture to be a texture to be mapped onto one of part objects corresponding to categories that form main parts of the model object. </claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
