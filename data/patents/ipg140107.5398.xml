<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626498-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626498</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12711943</doc-number>
<date>20100224</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<us-term-of-grant>
<us-term-extension>926</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20130101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>10</class>
<subclass>L</subclass>
<main-group>25</main-group>
<subgroup>93</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>704210</main-classification>
<further-classification>704215</further-classification>
</classification-national>
<invention-title id="d2e53">Voice activity detection based on plural voice activity detectors</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>6339706</doc-number>
<kind>B1</kind>
<name>Tillgren et al.</name>
<date>20020100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>455419</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6453285</doc-number>
<kind>B1</kind>
<name>Anderson et al.</name>
<date>20020900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6618701</doc-number>
<kind>B2</kind>
<name>Piket et al.</name>
<date>20030900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704233</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>7146315</doc-number>
<kind>B2</kind>
<name>Balan et al.</name>
<date>20061200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>7162248</doc-number>
<kind>B2</kind>
<name>Nagato et al.</name>
<date>20070100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>7174022</doc-number>
<kind>B1</kind>
<name>Zhang et al.</name>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>7203643</doc-number>
<kind>B2</kind>
<name>Garudadri</name>
<date>20070400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>7925510</doc-number>
<kind>B2</kind>
<name>Creamer et al.</name>
<date>20110400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>7042701</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>8244528</doc-number>
<kind>B2</kind>
<name>Niemisto et al.</name>
<date>20120800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704233</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2002/0116186</doc-number>
<kind>A1</kind>
<name>Strauss et al.</name>
<date>20020800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704233</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2003/0179888</doc-number>
<kind>A1</kind>
<name>Burnett et al.</name>
<date>20030900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2003/0228023</doc-number>
<kind>A1</kind>
<name>Burnett et al.</name>
<date>20031200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2004/0122667</doc-number>
<kind>A1</kind>
<name>Lee et al.</name>
<date>20040600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2004/0234067</doc-number>
<kind>A1</kind>
<name>Allen et al.</name>
<date>20041100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2005/0033571</doc-number>
<kind>A1</kind>
<name>Huang et al.</name>
<date>20050200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2005/0102134</doc-number>
<kind>A1</kind>
<name>Manabe et al.</name>
<date>20050500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>2005/0246166</doc-number>
<kind>A1</kind>
<name>Creamer et al.</name>
<date>20051100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>2006/0120537</doc-number>
<kind>A1</kind>
<name>Burnett et al.</name>
<date>20060600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>2007/0192094</doc-number>
<kind>A1</kind>
<name>Garudadri</name>
<date>20070800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704231</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>2008/0249771</doc-number>
<kind>A1</kind>
<name>Wahab</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>2008/0317259</doc-number>
<kind>A1</kind>
<name>Zhang et al.</name>
<date>20081200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>2009/0017879</doc-number>
<kind>A1</kind>
<name>Tsfaty et al.</name>
<date>20090100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>455574</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>2009/0125305</doc-number>
<kind>A1</kind>
<name>Cho</name>
<date>20090500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704233</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>2009/0222264</doc-number>
<kind>A1</kind>
<name>Pilati et al.</name>
<date>20090900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704233</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>2010/0332236</doc-number>
<kind>A1</kind>
<name>Tan</name>
<date>20101200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704275</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>2011/0264449</doc-number>
<kind>A1</kind>
<name>Sehlstedt</name>
<date>20111000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704226</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>CN</country>
<doc-number>1591568</doc-number>
<kind>A</kind>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>CN</country>
<doc-number>1601604</doc-number>
<kind>A</kind>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>CN</country>
<doc-number>101548313</doc-number>
<kind>A</kind>
<date>20090900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>EP</country>
<doc-number>1503368</doc-number>
<kind>A1</kind>
<date>20050200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>GB</country>
<doc-number>2430129</doc-number>
<kind>A</kind>
<date>20070300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-cpc-text>G10L 11/02</classification-cpc-text>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>WO</country>
<doc-number>WO2008058842</doc-number>
<kind>A1</kind>
<date>20080500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00033">
<othercit>Mettala, &#x201c;Bluetooth Protocol Architecture Version 1.0&#x201d; Bluetooth White Paper, Document No. 1.C.120/1.0, Aug. 25, 1999.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00034">
<othercit>International Search Report and Written Opinion&#x2014;PCT/US2010/060363, ISA/EPO&#x2014;May 16, 2011.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>31</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>None</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>6</number-of-drawing-sheets>
<number-of-figures>7</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20110208520</doc-number>
<kind>A1</kind>
<date>20110825</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Lee</last-name>
<first-name>Te-Won</first-name>
<address>
<city>San Diego</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Lee</last-name>
<first-name>Te-Won</first-name>
<address>
<city>San Diego</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<last-name>Diaz Hidalgo</last-name>
<first-name>Espartaco</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>QUALCOMM Incorporated</orgname>
<role>02</role>
<address>
<city>San Diego</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Albertalli</last-name>
<first-name>Brian</first-name>
<department>2657</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A voice activity detection (VAD) system includes a first voice activity detector, a second voice activity detector and control logic. The first voice activity detector is included in a device and produces a first VAD signal. The second voice activity detector is located externally to the device and produces a second VAD signal. The control logic combines the first and second VAD signals into a VAD output signal. Voice activity may be detected based on the VAD output signal. The second VAD signal can be represented as a flag included in a packet containing digitized audio. The packet can be transmitted to the device from the externally located VAD over a wireless link.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="186.94mm" wi="254.93mm" file="US08626498-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="254.59mm" wi="186.35mm" orientation="landscape" file="US08626498-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="241.64mm" wi="187.03mm" file="US08626498-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="257.47mm" wi="186.01mm" file="US08626498-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="163.41mm" wi="161.80mm" file="US08626498-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="253.92mm" wi="185.93mm" orientation="landscape" file="US08626498-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="154.09mm" wi="177.55mm" file="US08626498-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND</heading>
<p id="p-0002" num="0001">1. Field</p>
<p id="p-0003" num="0002">The present disclosure pertains generally to speech processing, and more specifically, to voice activity detection.</p>
<p id="p-0004" num="0003">2. Background</p>
<p id="p-0005" num="0004">Voice activity detection (VAD) is a technique used in speech processing wherein the presence or absence of human speech (voice) is detected in portions of an audio signal, which may also contain music, noise, or other sounds. The main uses of VAD are in voice coding and speech recognition. VAD can facilitate speech processing, and can also be used to deactivate some processes during non-speech segments: it can avoid unnecessary coding/transmission of silence, saving on computation and network bandwidth.</p>
<p id="p-0006" num="0005">VAD is an important enabling technology for a variety of speech-based applications. Customarily, VAD information is usually estimated locally in a single device, such as a communications handset, from an input audio signal.</p>
<p id="p-0007" num="0006">VAD in a voice communications system should be able to detect voice in the presence of very diverse types of acoustic background noise. One difficulty in the detection of voice in noisy environments is the very low signal-to-noise ratios (SNRs) that are sometimes encountered. In these situations, it is often difficult to distinguish between voice and noise or other sounds using known VAD techniques.</p>
<heading id="h-0002" level="1">SUMMARY</heading>
<p id="p-0008" num="0007">The techniques disclosed herein improve VAD in order to enhance speech processing, such as voice coding. The disclosed VAD techniques improve the accuracy and reliability of voice detection, and thus, improve functions that depend on VAD, such as noise reduction, echo cancellation, rate coding and the like. The VAD improvement is achieved by using VAD information that may be provided from one or more separate devices. The VAD information may be generated using multiple microphones or other sensor modalities that provide a more accurate VAD. The VAD information comes from multiple devices that may be connected to each other.</p>
<p id="p-0009" num="0008">According to one aspect, a method of voice activity detection (VAD) includes receiving a first VAD signal from a first voice activity detector included in a device; receiving a second VAD signal from a second voice activity detector not included in the device; combining the first and second VAD signals into a VAD output signal; and detecting voice activity based on the VAD output signal.</p>
<p id="p-0010" num="0009">According to another aspect, a system includes a first voice activity detector included in a device, configured to produce a first VAD signal; a second voice activity detector not included in the device, configured to produce a second VAD signal; and control logic, in communication with the first and second voice activity detectors, configured to combine the first and second VAD signals into a VAD output signal.</p>
<p id="p-0011" num="0010">According to another aspect, a system includes first means for detecting voice activity at a first location; second means for detecting voice activity at a second location; and means for combining output from the first and second means into a VAD signal.</p>
<p id="p-0012" num="0011">According to a further aspect, a computer-readable medium, embodying a set of instructions executable by one or more processors, includes code for receiving a first VAD signal from a first voice activity detector included in a device; code for receiving a second VAD signal from a second voice activity detector not included in the device; and code for combining the first and second VAD signals into a VAD output signal.</p>
<p id="p-0013" num="0012">Other aspects, features, and advantages will be or will become apparent to one with skill in the art upon examination of the following figures and detailed description. It is intended that all such additional features, aspects, and advantages be included within this description and be protected by the accompanying claims.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0014" num="0013">It is to be understood that the drawings are solely for purpose of illustration. Furthermore, the components in the figures are not necessarily to scale, emphasis instead being placed upon illustrating the principles of the techniques described herein. In the figures, like reference numerals designate corresponding parts throughout the different views.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 1</figref> is a diagram of an exemplary voice activity detection (VAD) system.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 2</figref> is a flowchart illustrating a method of detecting voice activity using the system of <figref idref="DRAWINGS">FIG. 1</figref></p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 3</figref> is an exemplary graph showing VAD signal weighting factors as a function of SNR at the external VAD shown in <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 4</figref> is an exemplary graph showing VAD signal weighting factors as a function of SNR at the internal VAD shown in <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 5</figref> is a diagram showing an exemplary headset/handset combination including a VAD system.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 6</figref> is a block diagram showing certain components included in the headset and handset of <figref idref="DRAWINGS">FIG. 5</figref>.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 7</figref> is a block diagram showing certain components of the handset processor shown in <figref idref="DRAWINGS">FIG. 6</figref>.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0022" num="0021">The following detailed description, which references to and incorporates the drawings, describes and illustrates one or more specific embodiments. These embodiments, offered not to limit but only to exemplify and teach, are shown and described in sufficient detail to enable those skilled in the art to practice what is claimed. Thus, for the sake of brevity, the description may omit certain information known to those of skill in the art.</p>
<p id="p-0023" num="0022">The word &#x201c;exemplary&#x201d; is used throughout this disclosure to mean &#x201c;serving as an example, instance, or illustration.&#x201d; Anything described herein as &#x201c;exemplary&#x201d; is not necessarily to be construed as preferred or advantageous over other approaches or features.</p>
<p id="p-0024" num="0023">In conventional speech processing system, voice activity detection (VAD) is typically estimated from an audio input signal such as a microphone signal, e.g., a microphone signal of a cell phone. VAD is an important function in many speech processing devices, such as vocoders and speech recognition devices.</p>
<p id="p-0025" num="0024">As disclosed herein, a voice activity detector is located in a separate device that may be connected to a primary device (e.g., computer, cell phone, other handheld device or the like). Within the primary device, the VAD information from the separate device may be further processed and speech processing takes place.</p>
<p id="p-0026" num="0025">For example, a Bluetooth headset may be connected to a cell phone. A vocoder in the cell phone may include a VAD algorithm that normally uses the cell phone's microphone input signal. When the Bluetooth headset is actively connected to the cell phone, the microphone signal of the Bluetooth headset is used by the VAD algorithm, instead of or in combination with the cell phone's microphone signal. If the Bluetooth headset uses additional information, such as multiple microphones, bone conduction or skin vibration microphones, or electro-magnetic (EM) Doppler radar signals to accurately estimate the VAD of a user (target), then this external VAD information is also used in the cell phone's vocoder to improve the performance of the vocoder. The external VAD information can be used to control vocoder functions, such as noise estimation update, echo cancellation (EC), rate-control, and the like. The external VAD signal can be a 1-bit signal from the headset to the handset and can be either encoded into an audio signal transmitted to the handset or it can be embedded into a Bluetooth packet as header information. The receiving handset is configured to decode this external VAD signal and then use it in the vocoder.</p>
<p id="p-0027" num="0026">With bone conduction and skin vibration microphones, when a user talks, the user's skin and skull bones vibrate, and the microphone converts the skin vibration into analog electrical signal. Bone conduction and skin vibration microphones provide advantage in noisy environments because the voice signal is not passed through the air from mouth to the headset, as in other headsets using conventional microphones. Thus, ambient noise is effectively eliminated from the audio signal passed to the handset.</p>
<p id="p-0028" num="0027">For voice activity detection using an acoustic Doppler radar device, a sensor is used to detect the dynamic status of a speaker's mouth. At the frequencies of operation, background noises are largely attenuated, rendering the device robust to external acoustic noises in most operating conditions. Unlike the other non-acoustic sensors, e.g., bone conduction and skin vibration sensors, the radar device need not be taped or attached to the speaker, making it more acceptable in most situations.</p>
<p id="p-0029" num="0028">Where the external VAD signal is a 1-bit flag of a Bluetooth (BT) packet, the 1-bit flag can be included in the trailer of the access code or the type field in each Bluetooth packet header. Alternatively, the 1-bit VAD flag can be included in a designated location of the payload section of the Bluetooth packet. In either case, the VAD signal is a single bit flag included in each BT packet. When the flag is set, it indicates that the Bluetooth packet includes voice, detected by the external VAD. When the VAD flag is not set, voice is not present in the audio payload of the Bluetooth packet. Sending just one 1-bit flag embedded in a BT header provides a discrete signal (1 bit per block or BT packet). A flag having more bits or multiple flags representing the external VAD signal may alternatively be used.</p>
<p id="p-0030" num="0029">The external VAD reduces speech processing errors that are often experienced in traditional VAD, particularly in low signal-to-noise-ratio (SNR) scenarios, in non-stationary noise and competing voices cases, and other cases where voice may be present. In addition, a target voice can be identified and the external VAD is able to provide a reliable estimation of target voice activity. A more reliable and accurate VAD can be used to improve the following speech processing functions: noise reduction (NR), i.e., with more reliable VAD, higher NR may be performed in non-voice segments; voice and non-voiced segment estimation; echo cancellation (EC), improved double detection schemes; and rate coding improvements which allow more aggressive rate coding schemes (lower rate for non-voice segments).</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 1</figref> is a diagram of an exemplary voice activity detection system <b>10</b>. The system <b>10</b> includes a device <b>12</b>, and an external voice activity detector (VAD) <b>14</b> connected to an acoustic sensor, such as one or more microphones <b>16</b>. The acoustic sensor associated with the external VAD <b>14</b> can alternatively be or additionally include a one or more bone conduction or skin vibration microphones, or electro-magnetic (EM) Doppler radar devices, or any suitable combination of such sensors and/or microphones.</p>
<p id="p-0032" num="0031">The device <b>12</b> includes an internal voice activity detector (VAD) <b>18</b>, control logic <b>20</b>, a speech processor <b>22</b>, such as a vocoder, one or more microphones <b>24</b>, and a sensor <b>26</b>. The device <b>12</b> may be any suitable electronic device configured to perform the functions disclosed herein, such as a computer, a laptop, a communications device, such as a telephone, cellular phone, personal digital assistant (PDA), a gaming device or the like.</p>
<p id="p-0033" num="0032">The internal VAD <b>18</b> may be any suitable device that implements a VAD algorithm, and may be integrated as part of the speech processor <b>22</b>. The control logic <b>20</b> is responsive to VAD signals from the external VAD <b>14</b>, the internal VAD <b>18</b> and the sensor <b>26</b>.</p>
<p id="p-0034" num="0033">The sensor <b>26</b> senses environmental operating conditions and provides input to the control logic <b>20</b>, based on such conditions, that is used to determine the VAD output signal generated by the control logic <b>20</b>. The sensor <b>26</b> may output control inputs that are based on one or more environmental operating conditions, such as ambient noise level, signal-to-noise ratios (SNRs) measured, for example, at the device <b>12</b> and/or proximate to or at the external VAD <b>14</b>. The sensor <b>26</b> may include one or both of the microphones <b>16</b>, <b>24</b>.</p>
<p id="p-0035" num="0034">The external VAD <b>14</b> is located externally to the device <b>12</b> and produces an external VAD signal, which is received by the control logic <b>20</b>. The external VAD <b>14</b> may be any suitable device that implements a VAD algorithm. The external VAD <b>14</b> may be included in a separate device, such as a headset, speakerphone, car-kit, or the like.</p>
<p id="p-0036" num="0035">The external VAD <b>14</b> and device <b>12</b> may communicate with each other using any suitable communication medium and protocol. The connection between the external VAD <b>14</b> and device <b>12</b> can be a wired connection or a wireless connection, such as a radio frequency (RF) or infrared (IR) link, e.g., a Bluetooth link, as defined by the Bluetooth specification, available at www.bluetooth.com. The external VAD signal can be encoded in audio data transferred to the device <b>12</b>, or it can be a flag included in an audio packet, such as Bluetooth packet, as described above.</p>
<p id="p-0037" num="0036">The control logic <b>20</b> may combine the external and internal VAD signals into a VAD output signal. The control logic <b>20</b> can combine the input VAD signals by weighting each of the VAD signals using weighting factors that are based on the environmental inputs from the sensor <b>26</b>. Some examples of weighting factors and methods that may be employed are described below in connection with <figref idref="DRAWINGS">FIGS. 3 and 4</figref>. Voice activity can be detected based on the VAD output signal. In the example shown in <figref idref="DRAWINGS">FIG. 1</figref>, the VAD output signal is provided to the speech processor <b>22</b>, which compares the VAD output signal to a threshold to determine whether voice is present in the audio signal being processed by the speech processor <b>22</b>.</p>
<p id="p-0038" num="0037">The speech processor <b>22</b> can be any type of speech processing component that relies on voice activity detection, such as a vocoder. For example, the speech processor <b>22</b> can be an enhanced variable rate codec (EVRC), such as the EVRC specified in &#x201c;Enhanced Variable Rate Codec, Speech Service Option 3 for Wideband Spread Spectrum Digital Systems&#x201d;, or the 3GPP2, No. 3GPP2 C.S0014-A, dated April, 2004.</p>
<p id="p-0039" num="0038">The VAD algorithm(s) used by the internal and external VADs <b>18</b>, <b>14</b> can be, for example, any suitable VAD algorithm currently known to those skilled in the art. For example, an energy-based VAD algorithm may be used. This type of VAD algorithm computes signal energy and compares the signal energy level to a threshold to determine voice activity. A zero-crossing count type VAD algorithm may also be use. This type of VAD algorithm determines the presence of voice by counting the number of zero crossings per frame as an input audio signal fluctuates from positives to negatives and vice versa. A certain threshold of zero-crossings may be used to indicate voice activity. Also, pitch estimation and detection algorithms can be used to detect voice activity, as well as VAD algorithms that compute formants and/or cepstral coefficient to indicate the presence of voice. Other VAD algorithms or any suitable combination of the above VAD algorithms may alternatively/additionally be employed by the internal and external VADs <b>18</b>, <b>14</b>.</p>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. 2</figref> is a flowchart <b>100</b> illustrating a method of detecting voice activity using the system <b>10</b> of <figref idref="DRAWINGS">FIG. 1</figref>. In decision block <b>102</b>, a check is made to determine whether an external VAD, e.g., external VAD <b>14</b>, is available. If not, the method proceeds to block <b>110</b>, where voice is detected based on the VAD signal output from an internal VAD, e.g., the internal VAD <b>18</b>.</p>
<p id="p-0041" num="0040">If an external VAD is available, the method proceeds to block <b>104</b>. In block <b>104</b>, the function of the external VAD is determined. The function of the external VAD is based on the type of acoustic sensor employed by the external VAD, for example, a bone conduction microphone, an audio microphone, a skin vibration sensor, an array of microphones, a Doppler radar device, or any suitable combination of the foregoing.</p>
<p id="p-0042" num="0041">In block <b>106</b>, the environmental operating conditions are determined. The conditions may include environmental conditions in the vicinity of or at the external VAD or the device. For example, the operating conditions may include measured background noise at the location of the external VAD and/or the device. The operating condition may also include the signal-to-noise ratio (SNR) measured at the external VAD, the device or both locations.</p>
<p id="p-0043" num="0042">Based on the environmental operating conditions, the control logic may determine that only the VAD signal from the external VAD is used (block <b>108</b>), only the VAD signal from the internal VAD is used (block <b>110</b>), or that both the external and internal VAD signals are used (blocks <b>112</b>-<b>116</b>) in determining a VAD output signal.</p>
<p id="p-0044" num="0043">If only the external VAD signal is used, then the voice signal is detected based on the external VAD signal only (block <b>108</b>). If only the internal VAD signal is used, then the voice signal is detected based on the internal VAD signal only (block <b>110</b>).</p>
<p id="p-0045" num="0044">If the operating condition warrant use of both internal and external VAD signals, for example, in cases where there is relatively large amounts of ambient background noise at the internal VAD location, then the confidence of the external VAD signal is estimated (block <b>112</b>) and the confidence of the internal VAD signal is also estimated (block <b>114</b>). The confidence levels can be calculated, for example, by determining a weighting factor (e.g., probability value) for each VAD signal as a function of the measured SNR or another environmental condition at each VAD location, respectively. The probability values can then be applied to the respective VAD signals as weighting values, e.g., by multiplying the VAD signals by the probability values, respectively, to obtain a corresponding confidence level. Each probability value may be a value between zero and one. <figref idref="DRAWINGS">FIGS. 3-4</figref> show graphs depicting exemplary relationships between the probability values and the SNRs measured at each location. The weighting factors may also be based on environmental conditions other than SNRs.</p>
<p id="p-0046" num="0045">In block <b>116</b>, voice activity is detected by the control logic based on combined external and internal VAD signals. The combined VAD signals may be the sum of the weighted external and internal VAD signals, for example:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>Y=P</i><sub>1</sub><i>*V</i><sub>1</sub><i>+P</i><sub>2</sub><i>*V</i><sub>2</sub>,&#x2003;&#x2003;Eq. 1<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where Y=a VAD output signal, P<sub>1</sub>=an external probability value, V<sub>1</sub>=the external VAD signal, P<sub>2</sub>=an internal probability value, and V<sub>2</sub>=the internal VAD signal. Each term P<sub>1</sub>*V<sub>1 </sub>and P<sub>2</sub>*V<sub>2 </sub>in Eq. 1 represents a confidence level. In some circumstances, the external and internal probability values P<sub>1</sub>, P<sub>2 </sub>are each within the range of 0 to 1, and additionally, the sum of probability values may be required to be the value of one. The VAD output signal is compared to a threshold value to determine whether voice activity is present in the audio signal. If the VAD output signal exceeds, for example, the threshold value, then voice is present in the audio signal. Conversely, if the VAD output signal is less than or equal to the threshold value, by way of example, then voice is not present in the audio signal. Other threshold comparisons may be used. Another exemplary weighting formula that may be used is expressed as:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>Y=P*V</i><sub>1</sub>+(1&#x2212;<i>P</i>)*<i>V</i><sub>2</sub>,&#x2003;&#x2003;Eq. 2<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where P is either P<sub>1 </sub>or P<sub>2</sub>. By assigning a value to P, the value of (1&#x2212;P) is obtained as the remaining weighting factor for V<sub>2</sub>, to compute Y.
</p>
<p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. 3</figref> is a graph <b>200</b> showing an exemplary relationship between an example external VAD signal weighting factor, P<sub>1</sub>, and an environmental operating condition, namely, the SNR, n, measured at the external VAD <b>14</b> shown in <figref idref="DRAWINGS">FIG. 1</figref>. The measured SNR is represented on the vertical axis, and the probability values are represented on the horizontal axis. Generally, in this example, the SNR has a direct relationship with the external VAD signal weighting factor, i.e., as the SNR increases, the weighting factor generally increases, and conversely, as the SNR decreases, so does the weighting factor.</p>
<p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. 4</figref> is a graph <b>300</b> showing an exemplary relationship between an example internal VAD signal weighting factor, P<sub>2</sub>, and an environmental operating condition, namely, the SNR, n, measured at the internal VAD <b>18</b> shown in <figref idref="DRAWINGS">FIG. 1</figref>. The measured SNR is represented on the vertical axis, and the probability values are represented on the horizontal axis. Generally, in this example, the SNR has a direct relationship with the internal VAD signal weighting factor, i.e., as the SNR increases, the weighting factor generally increases, and conversely, as the SNR decreases, so does the weighting factor.</p>
<p id="p-0049" num="0048">The graphs <b>200</b>, <b>300</b> show only one set of example relationships. Different probability functions can be employed for either the external or internal VAD. Although <figref idref="DRAWINGS">FIGS. 3-4</figref> illustrate generally sigmoidal relationships between the weighting factors and the measured environmental operating conditions (e.g., the SNRs), other relationships, such as a linear relationship, may be used to derive the weighting factor(s) from the measured environmental condition(s).</p>
<p id="p-0050" num="0049">In situations where the external and internal VAD weighting factors are related, such as given in Equation 2 above, one graph can be used to illustrate the relationship between the environmental operating condition and the weighting factor, and value of the other weight factor can be directly computed. For example, using Eq. 2, the second weighting factor can be computed from 1-P.</p>
<p id="p-0051" num="0050">Generally, the relationship between P<sub>1 </sub>and P<sub>2 </sub>reflects an estimation of which VAD is more reliably determining voice activity, either the internal VAD or external VAD. This depends mostly on the characteristics of the VADs. For example, for an internal VAD that may depends upon microphone input signals, the reliability of the internal VAD signal is highly dependent on the measure SNR at the device, and the graph of <figref idref="DRAWINGS">FIG. 4</figref> may apply. However, at an external device, e.g., a wireless headset, a bone conduction microphone may be used. When a bone conduction microphone is used, the reliability of the external VAD signal, for example, does not depend necessarily on the SNR, but instead on how accurately the bone conduction sensor touches the skin area of the user and accurately detects the vibrations and bone conduction. In this case, the external weighting factor P<sub>1 </sub>would not necessarily be a function of SNR, as shown in <figref idref="DRAWINGS">FIG. 3</figref>, but rather the level of the bone conduction sensor contact to the user's skin. The more the sensor touches the user's skin, the greater the value of P<sub>1</sub>.</p>
<p id="p-0052" num="0051">In systems combining bone conduction sensors, located for example in an external device, such as a headset, and audio microphones, located for example in the primary device, such as a handset, the P<sub>1 </sub>may be related to environmental operating conditions such that P<sub>1 </sub>(for the external bone conduction sensor) depends on usability and wear of the external device, where the sensor touches or in some use cases does not touch the user's skin. This condition may be estimated based on historical data and/or statistics based on the operation of the internal and or external VADs. P<b>2</b> for the internal VAD signal may be based on the measured SNR.</p>
<p id="p-0053" num="0052">The weighting factors and probability values described above, including those illustrated in the graphs <b>200</b>, <b>300</b> can be stored in a look-up table.</p>
<p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. 5</figref> is a diagram showing an exemplary headset/handset combination <b>400</b> including a headset <b>402</b> and handset <b>404</b> that incorporates the functionality of the VAD system <b>10</b>. The system <b>10</b> of <figref idref="DRAWINGS">FIG. 1</figref> can be employed in at least several different operational scenarios. In the example shown in <figref idref="DRAWINGS">FIG. 5</figref>, the functions of VAD system <b>10</b> are incorporated in <b>400</b> headset/handset combination, as described in greater detail herein below. In this environment, external VAD information is measured in the headset <b>402</b>. This measurement can be from an additional microphone or microphones, a jaw vibration microphone/sensor, or an electro-magnetic (EM), e.g., Doppler radar sensor, any of which are included in the headset <b>402</b>. This external VAD information is then sent to the handset <b>404</b> in either binary or continuous signal form as an external VAD signal. The external VAD information can be either encoded into the audio data stream or embedded into the header of the packet sent. The VAD information is then decoded in the handset <b>404</b> and used for further processing in particular to improve the performance of a vocoder, such as an EVRC.</p>
<p id="p-0055" num="0054">A Bluetooth wireless link is preferably used between the headset <b>402</b> and handset <b>404</b>. In configurations where the external VAD signal is included in the packet headers, the external VAD signal is a 1-bit flag of a Bluetooth (BT) packet, the 1-bit flag can be included in the trailer of the access code or the type field in each Bluetooth packet header. Alternatively, the 1-bit VAD flag can be included in a designated location of the payload section of the Bluetooth packet. In either case, the VAD signal is a single bit flag included in each BT packet. When the flag is set, it indicates that the Bluetooth packet includes voice, detected by the external VAD. When the VAD flag is not set, voice is not present in the audio payload of the Bluetooth packet. Sending just one 1-bit flag embedded in a BT header provides a discrete signal (1 bit per block or BT packet). A flag having more bits or multiple flags representing the external VAD signal may alternatively be used.</p>
<p id="p-0056" num="0055">A continuous VAD signal may be encoded into the audio stream using any suitable audio watermarking technique. Using audio watermarking, the VAD signal is modulated onto the audio data in an inaudible range, e.g., modulated into a very low frequency VAD signal or into high frequency VAD signal. The audio watermarking can be implemented by adding audio watermarking pre-processing in the external device, e.g., the headset, which encodes the continuous VAD signal; and also adding audio watermarking post-processing in the primary device, e.g., the handset, which decodes the audio data to extract the continuous VAD signal from the audio data.</p>
<p id="p-0057" num="0056">The handset <b>404</b> may be a portable wireless communication device, such as a cellular phone, gaming device, or PDA, including a secondary wireless communication interface, preferably a Bluetooth interface.</p>
<p id="p-0058" num="0057">The headset <b>402</b> is a wireless headset, preferably a Bluetooth headset. The headset <b>402</b> and handset <b>404</b> communicate with one another over a short-range wireless link, e.g., Bluetooth. Digitized audio may be transferred between the headset <b>402</b> and handset <b>404</b> using conventional Bluetooth profiles (e.g., the HSP) and protocols, as defined by the Bluetooth specification, where the Bluetooth packet headers may be modified to include the external VAD flag in some configurations.</p>
<p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. 6</figref> is a block diagram showing certain components included in the headset <b>402</b> and handset <b>404</b> of <figref idref="DRAWINGS">FIG. 5</figref>.</p>
<p id="p-0060" num="0059">The headset <b>402</b> includes one or more microphones <b>406</b>, a microphone preprocessor <b>408</b>, an external VAD <b>410</b>, and a wireless interface <b>412</b>. The wireless interface <b>412</b> includes a transceiver <b>416</b>. The microphone preprocessor <b>408</b> is configured to process electronic signals received from the microphone <b>406</b>. The microphone preprocessor <b>408</b> may include an analog-to-digital converter (ADC) and other analog and digital processing circuitry. The ADC converts analog signals from the microphone <b>406</b> into digital signals. These digital signals may then be processed by the wireless interface <b>412</b>. The microphone preprocessor <b>408</b> may be implemented using commercially-available hardware, software, firmware, or any suitable combination thereof.</p>
<p id="p-0061" num="0060">The headset <b>402</b> may also or alternatively include one or more jaw or skin vibration sensors, and/or electro-magnetic (EM), e.g., Doppler radar sensors for detecting voice activity. The output(s) of these sensors are provided to the external VAD <b>410</b> in lieu of or in combination with the microphone signal (mic2 signal).</p>
<p id="p-0062" num="0061">The wireless interface <b>412</b> provides two-way wireless communications with the handset <b>404</b> and other devices, if needed. Preferably, the wireless interface <b>412</b> includes a commercially-available Bluetooth module that provides at least a Bluetooth core system consisting of a Bluetooth RF transceiver, baseband processor, protocol stack, as well as hardware and software interfaces for connecting the module to a controller, such as the processor <b>414</b>, in the headset <b>402</b>. Although any suitable wireless technology can be employed with the headset <b>402</b>, the transceiver <b>416</b> is preferably a Bluetooth transceiver. The wireless interface <b>412</b> may be controlled by the headset controller (e.g., the processor <b>414</b>).</p>
<p id="p-0063" num="0062">The external VAD <b>410</b> can be implemented by the processor <b>414</b> executing software code. The external VAD <b>410</b> may be any suitable device that implements a VAD algorithm, including any of the VAD algorithms described herein. The external VAD <b>410</b> outputs an external VAD signal based on the inputs from the microphones <b>406</b> or other sensors. The external VAD signal is then embedded into a Bluetooth audio packet header as a single bit flag, as described above, by the processor <b>414</b>. In alternative configurations of the headset/handset system, the processor <b>414</b> encodes the VAD signal on the digitized mic2 signal using an audio watermarking algorithm.</p>
<p id="p-0064" num="0063">The wireless interface <b>412</b> transfers the digitized mic2 signal and external VAD signal in Bluetooth audio packets to the wireless interface <b>428</b> of the handset <b>404</b> over the Bluetooth wireless link.</p>
<p id="p-0065" num="0064">The processor <b>414</b> can be any suitable computing device, such as a microprocessor, e.g., an ARM7, a digital signal processor (DSP), one or more application specific integrated circuits (ASICs), field programmable gate arrays (FPGAs), complex programmable logic devices (CPLDs), discrete logic, or any suitable combination thereof.</p>
<p id="p-0066" num="0065">The handset <b>404</b> includes one or more microphones <b>418</b>, a microphone preprocessor <b>420</b>, an internal VAD <b>422</b>, control logic <b>424</b>, vocoder <b>426</b>, and a wireless interface <b>428</b>. The wireless interface <b>428</b> includes a transceiver <b>432</b>.</p>
<p id="p-0067" num="0066">The wireless interface <b>428</b> provides two-way wireless communications with the headset <b>402</b> and other devices, if needed. Preferably, the wireless interface <b>428</b> includes a commercially-available Bluetooth module that provides at least a Bluetooth core system consisting of a Bluetooth RF transceiver, baseband processor, protocol stack, as well as hardware and software interfaces for connecting the module to a controller, such as the processor <b>430</b>, in the handset <b>404</b>. Although any suitable wireless technology can be employed with the handset <b>404</b>, the transceiver <b>432</b> is preferably a Bluetooth transceiver. The wireless interface <b>428</b> may be controlled by a handset controller (e.g., the processor <b>430</b>).</p>
<p id="p-0068" num="0067">The internal VAD <b>422</b>, control logic <b>424</b>, and vocoder <b>426</b> can be implemented by the processor <b>430</b> executing software code. The processor <b>430</b> can be any suitable computing device, such as a microprocessor, e.g., an ARM7, a digital signal processor (DSP), one or more application specific integrated circuits (ASICs), field programmable gate arrays (FPGAs), complex programmable logic devices (CPLDs), discrete logic, or any suitable combination thereof.</p>
<p id="p-0069" num="0068">The control logic <b>424</b> is responsive to VAD signals from the external VAD <b>410</b>, and the internal VAD <b>422</b> and the digitized microphone signals from the headset microphone <b>406</b> (mic 2 signal) and handset microphone <b>418</b> (mic 1 signal). The control logic <b>424</b> outputs a VAD output signal, which is provided to the vocoder <b>426</b>. The control logic <b>424</b> may combine the external and internal VAD signals by weighting them to produce the VAD output signal. Weighting of the VAD signals may be performed as described herein above, and the weighting factors applied to each VAD signal may be based on environmental operating conditions measured by one or more sensor (not shown) included in either the handset <b>404</b> or headset <b>402</b>, as described herein above.</p>
<p id="p-0070" num="0069">The vocoder <b>426</b> detects voice activity based on the VAD output signal. Voice activity may be determined for each audio packet on a packet-by-packet basis. The VAD output signal is provided to the vocoder <b>426</b>, which compares the VAD output signal to a threshold to determine whether voice is present in the audio signal (packet) being processed by the vocoder <b>426</b>.</p>
<p id="p-0071" num="0070">The control logic <b>424</b> also provides the digitized audio signals (mic 1 and mic 2 signals) from the microphones <b>406</b>, <b>418</b> to the vocoder <b>426</b> for processing and encoding. The vocoder <b>426</b> can select which microphone signal to process, depending on which microphone <b>406</b>, <b>418</b> is currently being used to receive speech. An encoded speech (voice) signal is output by the vocoder <b>426</b>. The vocoder <b>426</b> can implement any suitable voice coding algorithm, including but not limited to the EVRC specified by the 3GPP2. The encoded speech can then be transmitted to the WWAN using the WWAN interface <b>630</b>.</p>
<p id="p-0072" num="0071">The handset <b>404</b> also includes a wireless wide area network (WWAN) interface <b>630</b> that comprises the entire physical interface necessary to communicate with a WWAN, such as a cellular network. The WWAN interface <b>630</b> includes a wireless transceiver configured to exchange wireless signals with base stations in a WWAN. The WWAN interface <b>630</b> exchanges wireless signals with the WWAN to facilitate voice calls and data transfers over the WWAN to a connected device. The connected device may be another WWAN terminal, a landline telephone, or network service entity such as a voice mail server, Internet server or the like. Examples of suitable wireless communications networks include, but are not limited to, code-division multiple access (CDMA) based networks, WCDMA, GSM, UTMS, AMPS, PHS networks or the like.</p>
<p id="p-0073" num="0072"><figref idref="DRAWINGS">FIG. 7</figref> is a block diagram showing certain components of the handset processor <b>430</b> shown in <figref idref="DRAWINGS">FIG. 6</figref>. The processor <b>430</b> includes a microprocessor (uP) <b>500</b> connected to a memory <b>502</b>. The memory <b>502</b> stores a control logic program <b>504</b>, a vocoder program <b>506</b> and an internal VAD program <b>508</b>. The control logic program <b>504</b> includes software/firmware code that when executed by the uP <b>500</b> provides the functionality of the control logic <b>424</b>. The vocoder program <b>506</b> includes software/firmware code that when executed by the uP <b>500</b> provides the functionality of the vocoder <b>426</b>. The internal VAD program <b>508</b> includes software/firmware code that when executed by the uP <b>500</b> provides the functionality of the internal VAD <b>422</b>. Although illustrated as being separate programs, the control logic, vocoder and internal VAD programs <b>504</b>, <b>506</b>, <b>508</b> can be combined as one or more programs.</p>
<p id="p-0074" num="0073">The memory <b>502</b> and microprocessor <b>500</b> can be coupled together and communicate on a common bus. The memory <b>502</b> and microprocessor <b>500</b> may be integrated onto a single chip, or they may be separate components or any suitable combination of integrated and discrete components. In addition, other processor-memory architectures may alternatively be used, such as a multiprocessor and/or multi memory arrangement.</p>
<p id="p-0075" num="0074">The microprocessor <b>500</b> can be any suitable processor or controller, such as an ARM7, DSP, one or more application specific integrated circuits (ASICs), field programmable gate arrays (FPGAs), complex programmable logic devices (CPLDs), discrete logic, or any suitable combination thereof.</p>
<p id="p-0076" num="0075">Alternatively, a multi-processor architecture having a plurality of processors, such as a microprocessor-DSP combination, may be used to implement the processor <b>430</b> in the handset <b>404</b>. In an exemplary multi-processor architecture, a DSP can be programmed to provide at least some of the audio processing, such as the internal VAD <b>422</b>, control logic <b>424</b> and vocoder <b>426</b> functions, and a microprocessor can be programmed to control overall operating of the handset <b>404</b>.</p>
<p id="p-0077" num="0076">The memory <b>502</b> may be any suitable memory device for storing programming code and/or data contents, such as a flash memory, RAM, ROM, PROM or the like.</p>
<p id="p-0078" num="0077">The VAD system <b>10</b> may also be employed in other systems, for example, in a handset-carkit. In this scenario, the multiple microphones used in the carkit allow for source localization and directionality information to be accurately estimated. This information can be used to suppress noises or unwanted signals. It can be also used to estimate an external VAD signal. This external VAD signal can be sent to the handset that then uses the additional VAD information to enhance the handset's vocoder performance.</p>
<p id="p-0079" num="0078">Another operational scenario in which the VAD system <b>10</b> can be employed is with a conference call speakerphone-handset combination. In this case, the external VAD device is included in a speakerphone device that is either wired or wirelessly connected to the handset. The speakerphone device can use multiple microphones to estimate the VAD of the voice source of interest. The source VAD signal can then be sent to the handset, which then uses the additional VAD information to enhance the handset's vocoder performance.</p>
<p id="p-0080" num="0079">The functionality of the systems, devices, headsets, handsets and their respective components, as well as the method steps and blocks described herein may be implemented in hardware, software, firmware, or any suitable combination thereof. The software/firmware may be a program having sets of instructions (e.g., code segments) executable by one or more digital circuits, such as microprocessors, DSPs, embedded controllers, or intellectual property (IP) cores. If implemented in software/firmware, the functions may be stored on or transmitted over as instructions or code on one or more computer-readable media. Computer-readable medium includes both computer storage medium and communication medium, including any medium that facilitates transfer of a computer program from one place to another. A storage medium may be any available medium that can be accessed by a computer. By way of example, and not limitation, such computer-readable medium can comprise RAM, ROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage or other magnetic storage devices, or any other medium that can be used to carry or store desired program code in the form of instructions or data structures and that can be accessed by a computer. Also, any connection is properly termed a computer-readable medium. For example, if the software is transmitted from a website, server, or other remote source using a coaxial cable, fiber optic cable, twisted pair, digital subscriber line (DSL), or wireless technologies such as infrared, radio, and microwave, then the coaxial cable, fiber optic cable, twisted pair, DSL, or wireless technologies such as infrared, radio, and microwave are included in the definition of medium. Disk and disc, as used herein, includes compact disc (CD), laser disc, optical disc, digital versatile disc (DVD), floppy disk and blu-ray disc where disks usually reproduce data magnetically, while discs reproduce data optically with lasers. Combinations of the above should also be included within the scope of computer-readable medium.</p>
<p id="p-0081" num="0080">Certain embodiments have been described. However, various modifications to these embodiments are possible, and the principles presented herein may be applied to other embodiments as well. For example, the principles disclosed herein may be applied to other devices, such as wireless devices including personal digital assistants (PDAs), personal computers, stereo systems, video games and the like. Also, the principles disclosed herein may be applied to wired headsets, where the communications link between the headset and another device is a wire, rather than a wireless link. In addition, the various components and/or method steps/blocks may be implemented in arrangements other than those specifically disclosed without departing from the scope of the claims.</p>
<p id="p-0082" num="0081">Other embodiments and modifications will occur readily to those of ordinary skill in the art in view of these teachings. Therefore, the following claims are intended to cover all such embodiments and modifications when viewed in conjunction with the above specification and accompanying drawings.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method of voice activity detection (VAD), comprising:
<claim-text>receiving a first VAD signal from a first voice activity detector included in a device;</claim-text>
<claim-text>receiving a second VAD signal from a second voice activity detector not included in the device;</claim-text>
<claim-text>combining the first and second VAD signals into a VAD output signal; and</claim-text>
<claim-text>detecting voice activity based on the VAD output signal.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>weighting the first VAD signal based on environmental conditions.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the environmental conditions include a signal-to-noise ratio (SNR) measured at the device.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>weighting the second VAD signal based on environmental conditions.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the environmental conditions include a signal-to-noise ratio (SNR) measured at an external device including the second voice activity detector.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>determining a function of the second voice activity detector.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the function of the second voice activity detector is based on a bone conduction microphone, an audio microphone, a skin vibration sensor, an array of microphone, or a radar signal.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>transmitting the second VAD signal over a wireless link.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the wireless link is a Bluetooth wireless link.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. A method of voice activity detection (VAD), comprising:
<claim-text>providing a first device and a second device, each device configured to communicate with one another by way of a wireless link;</claim-text>
<claim-text>determining a VAD signal in the second device;</claim-text>
<claim-text>at the second device, setting a flag based on the VAD signal, the flag being included in a packet containing digitized audio, wherein the flag is a one-bit value included in a Bluetooth packet header;</claim-text>
<claim-text>transmitting the packet from second device to the first device by way of the wireless link; and</claim-text>
<claim-text>detecting voice activity at the first device based on the flag included in the packet.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. A system, comprising:
<claim-text>a first voice activity detector included in a device, configured to produce a first voice activity detection (VAD) signal;</claim-text>
<claim-text>a second voice activity detector not included in the device, configured to produce a second voice activity detection (VAD) signal; and</claim-text>
<claim-text>control logic, in communication with the first and second voice activity detectors, configured to combine the first and second VAD signals into a VAD output signal.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising:
<claim-text>a processor receiving the VAD output signal.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the processor includes a vocoder.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the device is a wireless handset.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the second voice activity detector is included in a headset in communication with the device.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the headset is a wireless headset.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the second VAD signal is transmitted to the control logic as a single bit value included in a Bluetooth header.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the control logic is included in the device.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. A system, comprising:
<claim-text>first means for detecting voice activity at a first location;</claim-text>
<claim-text>second means for detecting voice activity at a second location; and</claim-text>
<claim-text>means for combining output from the first and second means into a voice activity detection (VAD) output signal.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The system of <claim-ref idref="CLM-00019">claim 19</claim-ref>, further comprising:
<claim-text>processor means for receiving the VAD output signal.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The system of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the first means is included in a wireless handset.</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The system of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the second means is included in a headset in communication with a device.</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The system of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein the headset is a wireless headset.</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. The system of <claim-ref idref="CLM-00019">claim 19</claim-ref>, further comprising means for transmitting a VAD signal from the first or second means to the means for combining as a single bit value included in a Bluetooth header.</claim-text>
</claim>
<claim id="CLM-00025" num="00025">
<claim-text>25. The system of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the means for combining is included at the first location.</claim-text>
</claim>
<claim id="CLM-00026" num="00026">
<claim-text>26. A non-transitory computer-readable medium embodying a set of instructions executable by one or more processors, comprising:
<claim-text>code for receiving a first VAD signal from a first voice activity detector included in a device;</claim-text>
<claim-text>code for receiving a second VAD signal from a second voice activity detector not included in the device; and</claim-text>
<claim-text>code for combining the first and second VAD signals into a VAD output signal.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00027" num="00027">
<claim-text>27. The computer-readable medium of <claim-ref idref="CLM-00026">claim 26</claim-ref>, further comprising:
<claim-text>code for detecting voice activity based on the VAD output signal.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00028" num="00028">
<claim-text>28. The computer-readable medium of <claim-ref idref="CLM-00026">claim 26</claim-ref>, further comprising:
<claim-text>code for weighting the first VAD signal based on environmental conditions.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00029" num="00029">
<claim-text>29. The computer-readable medium of <claim-ref idref="CLM-00028">claim 28</claim-ref>, wherein the environmental conditions include a signal-to-noise ratio (SNR) measured at the device.</claim-text>
</claim>
<claim id="CLM-00030" num="00030">
<claim-text>30. The computer-readable medium of <claim-ref idref="CLM-00026">claim 26</claim-ref>, further comprising:
<claim-text>code for weighting the second VAD signal based on environmental conditions.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00031" num="00031">
<claim-text>31. The computer-readable medium of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein the environmental conditions include a signal-to-noise ratio (SNR) measured at an external device including the second voice activity detector. </claim-text>
</claim>
</claims>
</us-patent-grant>
