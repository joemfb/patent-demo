<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08627146-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08627146</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13554456</doc-number>
<date>20120720</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="regional">
<country>EP</country>
<doc-number>09155558</doc-number>
<date>20090319</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<disclaimer>
<text>This patent is subject to a terminal disclaimer.</text>
</disclaimer>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>11</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>714 33</main-classification>
</classification-national>
<invention-title id="d2e69">Model-based testing of an application program under test</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>6021261</doc-number>
<kind>A</kind>
<name>Barrett, Jr. et al.</name>
<date>20000200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>7398514</doc-number>
<kind>B2</kind>
<name>Ulrich et al.</name>
<date>20080700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>2003/0014734</doc-number>
<kind>A1</kind>
<name>Hartman et al.</name>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>717125</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>2003/0046609</doc-number>
<kind>A1</kind>
<name>Farchi et al.</name>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>2005/0102596</doc-number>
<kind>A1</kind>
<name>Hekmatpour</name>
<date>20050500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2005/0128471</doc-number>
<kind>A1</kind>
<name>Wilsher et al.</name>
<date>20050600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2007/0168971</doc-number>
<kind>A1</kind>
<name>Royzen et al.</name>
<date>20070700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2007/0282556</doc-number>
<kind>A1</kind>
<name>Achkar et al.</name>
<date>20071200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2010/0241904</doc-number>
<kind>A1</kind>
<name>Bailey et al.</name>
<date>20100900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00010">
<othercit>&#x201c;TTCN-3, Qtronic and SIP&#x201d;, Conformiq Software Ltd. http://www.conformiq.com/downloads/sip-white-paper.pdf 2006 , 8 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00011">
<othercit>&#x201c;U.S. Appl. No. 12/632,892 Final Office Action&#x201d;, Feb. 3, 2012 , 10 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00012">
<othercit>&#x201c;U.S. Appl. No. 12/632,892 Office Action&#x201d;, Jun. 24, 2011 , 11 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00013">
<othercit>Prenninger, Wolfgang et al., &#x201c;Abstractions for Model-Based Testing&#x201d;, Electronic Notes in Theoretical Computer Science (ENTCS) Jan. 2005 , 12 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00014">
<othercit>Utting, Mark et al., &#x201c;A Taxonomy of Model-Based Testing&#x201d;, Working Paper Series. University of Waikato, Department of Computer Science http://www.cs.waikato.ac.nz/pubs/wp/2006/uow-cs-wp-2006-04.pdf Apr. 2006 , 18 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>24</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>714 33</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>5</number-of-drawing-sheets>
<number-of-figures>5</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>12632892</doc-number>
<date>20091208</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>8245080</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13554456</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120284567</doc-number>
<kind>A1</kind>
<date>20121108</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Bailey</last-name>
<first-name>Thomas J. G.</first-name>
<address>
<city>Hatfield</city>
<country>GB</country>
</address>
</addressbook>
<residence>
<country>GB</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Duffell</last-name>
<first-name>John W.</first-name>
<address>
<city>Winchester</city>
<country>GB</country>
</address>
</addressbook>
<residence>
<country>GB</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Taylor</last-name>
<first-name>Mark S.</first-name>
<address>
<city>Salisbury</city>
<country>GB</country>
</address>
</addressbook>
<residence>
<country>GB</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Bailey</last-name>
<first-name>Thomas J. G.</first-name>
<address>
<city>Hatfield</city>
<country>GB</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Duffell</last-name>
<first-name>John W.</first-name>
<address>
<city>Winchester</city>
<country>GB</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Taylor</last-name>
<first-name>Mark S.</first-name>
<address>
<city>Salisbury</city>
<country>GB</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>DeLizio Gilliam, PLLC</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>International Business Machines Corporation</orgname>
<role>02</role>
<address>
<city>Armonk</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Baderman</last-name>
<first-name>Scott</first-name>
<department>2114</department>
</primary-examiner>
<assistant-examiner>
<last-name>Patel</last-name>
<first-name>Jigar</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A method includes receiving a first processing request for an application program under test. The method includes generating a second processing request for a model of the application program, wherein the second processing request is equivalent to said first processing request. The method includes communicating said first and second requests to said application program under test and said model of the application program respectively. The method includes receiving a first response data set from the application program under test and a second response data set from the model of the application program. The method includes comparing said first and second response data sets and generating a success indication if said comparing said first and second response data sets does not identify a difference. The method includes generating an error indication if said comparing said first and second response data sets identifies a difference between the first and second data sets.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="180.42mm" wi="241.22mm" file="US08627146-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="200.58mm" wi="167.89mm" orientation="landscape" file="US08627146-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="242.23mm" wi="195.41mm" orientation="landscape" file="US08627146-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="204.39mm" wi="156.38mm" file="US08627146-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="230.04mm" wi="175.60mm" file="US08627146-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="244.77mm" wi="153.84mm" file="US08627146-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">RELATED MATTERS</heading>
<p id="p-0002" num="0001">This application claims the priority benefit of U.S. patent application Ser. No. 12/632,892 filed on Dec. 8, 2009, which claims priority to European Patent Application Number 09155558.1 filed Mar. 19, 2009, both of which are incorporated herein in their entirety.</p>
<heading id="h-0002" level="1">BACKGROUND</heading>
<p id="p-0003" num="0002">Part of the process of building and maintenance of software systems is the testing of the system against defined requirements. Model-based testing systems utilize an instance of a software system under test in combination with a model of the system. The model is arranged to perform in accordance with the expected behavior of the software system under test. A set of tests is provided in combination with a set of checking rules for testing the behavior of both the software system under test and its model. One problem with such model-based testing systems is that their effective use involves detailed understanding of the rules and corresponding model.</p>
<heading id="h-0003" level="1">SUMMARY</heading>
<p id="p-0004" num="0003">In some example embodiments, a method includes receiving a first processing request for an application program under test. The method includes generating a second processing request for a model of the application program, wherein the second processing request is equivalent to said first processing request. The method includes communicating said first and second requests to said application program under test and said model of the application program respectively. The method includes receiving a first response data set from the application program under test and a second response data set from the model of the application program. The method includes comparing said first and second response data sets. The method includes generating a success indication if said comparing said first and second response data sets does not identify a difference. The method includes generating an error indication if said comparing said first and second response data sets identifies a difference between the first and second data sets.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0005" num="0004">The present embodiments may be better understood, and numerous objects, features, and advantages made apparent to those skilled in the art by referencing the accompanying drawings.</p>
<p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. 1</figref> is a schematic illustration of a software testing system;</p>
<p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. 2</figref> is a schematic illustration of functional elements of the software testing system of <figref idref="DRAWINGS">FIG. 1</figref>;</p>
<p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. 3</figref> is a table illustrating a set of checking code rules used in the software testing system of <figref idref="DRAWINGS">FIG. 2</figref>;</p>
<p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. 4</figref> is a table illustrating an exception produced in the software testing system of <figref idref="DRAWINGS">FIG. 2</figref>; and</p>
<p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. 5</figref> is a flow chart illustrating processing performed by the software testing system of <figref idref="DRAWINGS">FIG. 2</figref>.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DESCRIPTION OF EMBODIMENT(S)</heading>
<p id="p-0011" num="0010">With reference to <figref idref="DRAWINGS">FIG. 1</figref>, a software testing system <b>101</b> comprises a computer <b>102</b> loaded with an operating system <b>103</b>. The operating system <b>103</b> provides a platform for a model based testing application program <b>104</b>, which utilizes an instance of an application program under test <b>105</b> in combination with a model <b>106</b> of the application program under test. The model <b>106</b> is arranged to model the expected behavior of the application program under test <b>105</b> and also to provide an identical interface to the application program under test <b>105</b>.</p>
<p id="p-0012" num="0011">The test application program <b>104</b> is also arranged to provide an identical interface to the application program under test <b>105</b>. Thus, the test application program <b>104</b> is arranged to accept processing requests, in the form of test scenarios <b>107</b>, that comprise the expected operational user interaction with the application program under test <b>105</b>. The test application program <b>104</b> is arranged to pass such processing requests to both the application program under test <b>105</b> and its model <b>106</b>, analyze the response as described below and provide an indication of whether the processing request was a success or whether an error occurred. The test application program <b>104</b> indicates that the processing of a request was a success if the results received from both the application program under test <b>105</b> and its model <b>106</b> are equivalent. If the results received are different then the test application program <b>104</b> indicates that there was an error in the processing of the request by the application program under test <b>105</b>.</p>
<p id="p-0013" num="0012">With reference to <figref idref="DRAWINGS">FIG. 2</figref>, the test application program <b>104</b> comprises a test interface <b>201</b> and a verification interface <b>202</b>. The verification interface <b>202</b> comprises checking code <b>203</b>. The test interface <b>201</b> provides the test application program <b>104</b> a user interface that is identical to that provided by the application program under test <b>105</b>, so as to enable a user to interact with the test application program <b>104</b> as if interacting directly with the application program under test <b>105</b>. In addition, the test interface <b>201</b> provides facilities for observing and storing the results of the testing of the application program under test <b>105</b>. In response to a processing request for the application program under test (SUT) <b>105</b> made via the test interface <b>201</b>, the test interface <b>201</b> is arranged to duplicate the request and pass the original request to the application program under test (SUT) <b>105</b> and the duplicated request to the model <b>106</b>. In the present embodiment, the application programs <b>105</b> and model <b>106</b> are implemented using object oriented programming techniques. Thus, the original and duplicate processing requests are passed to their respective targets in the form of respective sets of SUT and model request parameters <b>204</b>, <b>205</b>. As will be understood by those skilled in the art, such sets of parameters may comprise any suitable parameter types acceptable to the SUT <b>105</b> and model <b>106</b>. The results of the processing of the requests <b>204</b>, <b>205</b> by the SUT <b>105</b> and the model <b>106</b>, respectively, are returned to the verification interface <b>202</b>. The results comprise respective response data sets in the form, for example, of respective result objects <b>206</b>, <b>207</b>.</p>
<p id="p-0014" num="0013">The verification interface <b>202</b> is arranged to compare the response data sets, in the form of the result objects <b>206</b>, <b>207</b>, using the checking code <b>203</b>. As will be understood by those skilled in the art, checking code is generally specific to a given process or method. Therefore, since the test interface <b>201</b> comprises a plurality of such processes or methods, a corresponding plurality of subsets of checking code are correspondingly provided. With reference to <figref idref="DRAWINGS">FIG. 3</figref>, the checking code <b>203</b> for a given method utilizes a set of checking code rules <b>301</b> to compare features of each of the result objects <b>206</b>, <b>207</b> so as to identify any differences. In the example of <figref idref="DRAWINGS">FIG. 3</figref>, the checking code rules <b>301</b> compare the object name, version and data content. If the checking code determines that all of the relevant attributes of the result objects <b>206</b>, <b>207</b> are identical then the processing of the SUT <b>105</b> is deemed to be equivalent to the model <b>106</b> in respect of the original processing request. In response to such successful processing, the verification interface <b>203</b> is arranged to encapsulate both the result objects <b>206</b>, <b>207</b> in a verification object <b>208</b> and pass this object <b>208</b> to the test interface <b>201</b>. The verification object <b>208</b> is arranged to enable the test interface <b>201</b> to present the response from the SUT <b>105</b>. In addition, if a further processing request is issued based on the received response, the verification object <b>208</b> preserves the state of the processing with both SUT <b>105</b> and the model <b>106</b> so as to provide the necessary continuity for such further processing requests. In addition, the verification object <b>208</b> carries sufficient data for the logging of the processing requests for a given testing scenario.</p>
<p id="p-0015" num="0014">If the checking code <b>203</b> determines that one or more of the attributes of the result objects <b>206</b>, <b>207</b> are different, this indicates a departure of the behavior of the SUT <b>105</b> from that of its model <b>106</b>. In this case, the verification interface <b>202</b> is arranged to return an error to the test interface <b>201</b> in the form of an exception. In the present embodiment, with reference to <figref idref="DRAWINGS">FIG. 4</figref>, an exception is defined in an exception object <b>401</b>. Each exception object <b>401</b>, in common with a verification object <b>208</b>, is associated with the relevant result objects. In addition, the exception object provides an indication of the property of the result objects that was identified as differing along with an indication of the value of that property that was expected. The expected property value is determined from the result object <b>207</b> received from the model <b>106</b>. In the example of an exception object <b>401</b> in <figref idref="DRAWINGS">FIG. 4</figref>, the difference was identified in the &#x201c;Property:Version&#x201d; attribute of the SUT result object <b>206</b>. The expected value for the &#x201c;Property:Version&#x201d; attribute identified from the model result object <b>207</b> was &#x201c;2.2&#x201d;. In response to the receipt of an exception object <b>401</b>, the test interface is arranged to log the exception and the user is provided with an option to continue with further processing requests if appropriate.</p>
<p id="p-0016" num="0015">The processing performed by the test application program <b>104</b> when handling a processing request will now be described further with reference to the flow chart of <figref idref="DRAWINGS">FIG. 5</figref>. At step <b>501</b>, processing is initiated in response to the start-up of the test application program <b>104</b>, the SUT <b>105</b> and model <b>106</b> are identified and processing moves to step <b>502</b>. At step <b>502</b>, processing awaits the input of a processing request via the test interface <b>201</b> and, once a processing request is received moves to step <b>503</b>. At step <b>503</b>, in accordance with the functions of the duplicated SUT interface, an appropriate SUT request <b>204</b> is created along with an equivalent model request <b>205</b> and processing moves to step <b>504</b>. At step <b>504</b>, the SUT and model requests <b>204</b>, <b>205</b> are passed to the SUT <b>105</b> and model <b>106</b> respectively and processing moves to step <b>505</b>. At step <b>505</b>, the result of the processing of the requests <b>204</b>, <b>205</b> represented by the SUT and model objects <b>204</b>, <b>205</b> is awaited in the form of the result objects <b>206</b>, <b>207</b>. Once the result objects <b>206</b>, <b>207</b> are received processing moves to step <b>506</b>. At step <b>506</b>, the result objects <b>206</b>, <b>207</b> are compared by the checking code <b>203</b> in accordance with the checking code rules <b>301</b> and processing moves to step <b>507</b>.</p>
<p id="p-0017" num="0016">At step <b>507</b>, if the result objects <b>206</b>, <b>207</b> are equivalent, that is, the checking code <b>203</b> identified no significant differences, then processing moves to step <b>508</b>. At step <b>508</b>, the result objects <b>206</b>, <b>207</b> are wrapped in a verification object <b>208</b> and processing moves to step <b>509</b>. At step <b>509</b>, the verification object <b>208</b>, indicating the success of the processing request, is passed to the test interface <b>201</b> for processing as a normal results return. Processing then returns to step <b>502</b>. At step <b>507</b>, if the result objects <b>206</b>, <b>207</b> comprise one or more significant differences, then processing moves to step <b>510</b>. At step <b>510</b>, the result objects <b>206</b>, <b>207</b> are wrapped in an exception object and processing moves to step <b>511</b>. At step <b>511</b>, an error message, indicating the identified differences and the expected values based on the model result object <b>207</b>, is inserted in the exception object and processing moves to step <b>512</b>. At step <b>512</b>, the exception object is logged and processing moves to step <b>502</b> and proceeds as described above.</p>
<p id="p-0018" num="0017">Some processing requests received by the test interface <b>201</b> may be based on previously received result objects <b>206</b>, <b>207</b> wrapped in a verification object <b>208</b>. In such cases, at step <b>503</b>, the appropriate result objects <b>206</b>, <b>207</b> are extracted from the verification object <b>208</b> so as to form the appropriate part of the relevant set of request parameters for the SUT <b>105</b> and model <b>106</b> respectively.</p>
<p id="p-0019" num="0018">In another embodiment, the test application program is provided as the working front-end for the SUT enabling the SUT to be deployed while still providing a facility for monitoring discrepancies between the SUT and the model while the SUT is operational. The test interface may be arranged to avoid notifying a normal user of any received error notifications but instead to divert such error notifications to another predetermined user such as an administrator via a duplicate or other suitable interface.</p>
<p id="p-0020" num="0019">In a further embodiment, the indication of the error or discrepancy is communicated to the test interface separately from the result objects or data sets. Identifiers may be provided in the error message or the result data sets to provide cross-referencing.</p>
<p id="p-0021" num="0020">In another embodiment, where whole trees or graphs of SUT and model objects are returned from the SUT and model respectively, each object within the graph is wrapped up in a verification object so as to provide a graph of verification objects returned from the test interface.</p>
<p id="p-0022" num="0021">The checking rules may be devised so as to check for any set of one or more identifiable discrepancies between the output of the SUT and the model.</p>
<p id="p-0023" num="0022">While the embodiments described above may be implemented using object-orientated programming techniques, embodiments may also be proved using procedural or other programming techniques. Thus the objects described above that are used for communicating data between the test interface, verification interface, SUT and model may be replaced with equivalent communications means such as appropriate data sets or messages.</p>
<p id="p-0024" num="0023">It will be understood by those skilled in the art that the apparatus that embodies a part or all of the present invention may be a general purpose device having software arranged to provide a part or all of an embodiment of the invention. The device could be a single device or a group of devices and the software could be a single program or a set of programs. Furthermore, any or all of the software used to implement the invention can be communicated via any suitable transmission or storage means so that the software can be loaded onto one or more devices.</p>
<p id="p-0025" num="0024">While the present invention has been illustrated by the description of the embodiments thereof, and while the embodiments have been described in considerable detail, it is not the intention of the applicant to restrict or in any way limit the scope of the appended claims to such detail. Additional advantages and modifications will readily appear to those skilled in the art. Therefore, the invention in its broader aspects is not limited to the specific details representative apparatus and method, and illustrative examples shown and described. Accordingly, departures may be made from such details without departure from the spirit or scope of applicant's general inventive concept.</p>
<p id="p-0026" num="0025">Plural instances may be provided for components, operations or structures described herein as a single instance. Finally, boundaries between various components, operations and data stores are somewhat arbitrary, and particular operations are illustrated in the context of specific illustrative configurations. Other allocations of functionality are envisioned and may fall within the scope of the inventive subject matter. In general, structures and functionality presented as separate components in the exemplary configurations may be implemented as a combined structure or component. Similarly, structures and functionality presented as a single component may be implemented as separate components. These and other variations, modifications, additions, and improvements may fall within the scope of the inventive subject matter.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method comprising:
<claim-text>receiving a first processing request for an application program under test;</claim-text>
<claim-text>generating a second processing request for a model of the application program, wherein the model of the application program is configured to model an expected behavior of the application program under test and configured to provide a same interface as an interface for the application program under test, wherein the second processing request is equivalent to said first processing request;</claim-text>
<claim-text>communicating said first and second requests to said application program under test and said model of the application program respectively;</claim-text>
<claim-text>receiving a first response data set from the application program under test and a second response data set from the model of the application program;</claim-text>
<claim-text>comparing said first and second response data sets;</claim-text>
<claim-text>generating a success indication if said comparing said first and second response data sets does not identify a difference;</claim-text>
<claim-text>generating an error indication if said comparing said first and second response data sets identifies a difference between the first and second data sets;</claim-text>
<claim-text>receiving a third processing request, which is based on the first response data set, for the application program under test;</claim-text>
<claim-text>extracting a first parameter of the first response data set;</claim-text>
<claim-text>communicating the third processing request with the first parameter to the application program under test;</claim-text>
<claim-text>extracting a second parameter of the second response data set;</claim-text>
<claim-text>generating fourth processing request equivalent to the third processing request; and</claim-text>
<claim-text>communicating the fourth processing request with the second parameter to the model of the application program.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said error indication comprises data indicating an attribute of the first and second response data sets that corresponds to the difference and indicating an expected value of the attribute.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said generating the error indication comprises generating an exception.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said error indication references said first and second response data sets.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said comparing said first and second response data sets comprises executing a checking code on the first and second response data sets, wherein the checking code utilizes checking code rules.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the checking code rules indicate an object name, an object version, and data content to be compared.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>receiving a third response data set from the application program under test and a fourth response data set from the model of the application program;</claim-text>
<claim-text>comparing said third and fourth response data sets;</claim-text>
<claim-text>generating a success indication if said comparing said third and fourth response data sets does not identify a difference; and</claim-text>
<claim-text>generating an error indication if said comparing said third and fourth response data sets identifies a difference between the third and fourth response data sets.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein said generating the success indication for comparison of the first response data set and the second response data set comprises generating a verification object that encapsulates the first and second response data sets, wherein extracting the first parameter of the first response data set comprises extracting the first parameter of the first response data set from the verification object, wherein extracting the second parameter of the second response data set comprises extracting the second parameter of the second response data set from the verification object.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. An apparatus comprising:
<claim-text>at least one processor; and</claim-text>
<claim-text>a test interface executable on the at least one processor, the test interface configured to be identical to a user interface of an application program under test, the test interface configured to,
<claim-text>receive a first processing request for the application program under test,</claim-text>
<claim-text>generate a second processing request equivalent to the first processing request,</claim-text>
<claim-text>communicate the first processing request to the application program under test and the second processing request to a model of the application program, wherein the model of the application program is configured to model an expected behavior of the application program under test and configured to provide a same interface as an interface for the application program under test,</claim-text>
<claim-text>receive a third processing request, which is based on the first response data set, for the application program under test;</claim-text>
<claim-text>extract a first parameter of the first response data set;</claim-text>
<claim-text>communicate the third processing request with the first parameter to the application program under test;</claim-text>
<claim-text>extract a second parameter of the second response data set;</claim-text>
<claim-text>generate fourth processing request equivalent to the third processing request; and</claim-text>
<claim-text>communicate the fourth processing request with the second parameter to the model of the application program;</claim-text>
</claim-text>
<claim-text>a verification interface executable on the at least one processor, the verification interface configured to,
<claim-text>compare a first response data set from the application program under test and a second response data set from the model of the application program,</claim-text>
<claim-text>generate an error indication if a difference is determined from comparison of the first response data set and the second response data set and communicate the error indication to the test interface,</claim-text>
<claim-text>generate a success indication if a difference is not determined from comparison of the first response data set and the second response data set and communicate the success indication to the test interface.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The apparatus of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein said error indication comprises data that indicates an attribute of the first and second response data sets that corresponds to the difference and indicating an expected value of the attribute.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The apparatus of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the verification interface is configured to generate the error indication comprises the verification unit operable to generate an exception.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The apparatus of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein said error indication references said first and second response data sets.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The apparatus of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the verification interface configured to compare said first and second response data sets comprises the verification interface configured to execute a checking code on the first and second response data sets, wherein the checking code utilizes checking code rules.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The apparatus of <claim-ref idref="CLM-00013">claim 13</claim-ref> wherein the checking code rules indicate an object name, an object version, and data content to be compared.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The apparatus of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the verification interface is configured to,
<claim-text>receive a third response data set from the application program under test and a fourth response data set from the model of the application program;</claim-text>
<claim-text>compare said third and fourth response data sets;</claim-text>
<claim-text>generate a success indication if said comparing said third and fourth response data sets does not identify a difference; and</claim-text>
<claim-text>generate an error indication if said comparing said third and fourth response data sets identifies a difference between the third and fourth response data sets.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The apparatus of <claim-ref idref="CLM-00015">claim 15</claim-ref>,
<claim-text>wherein generation of the success indication for comparison of the first response data set and the second response data set comprises generation of a verification object that encapsulates the first and second response data sets,</claim-text>
<claim-text>wherein extraction of the first parameter of the first response data set comprises extraction of the first parameter of the first response data set from the verification object, and</claim-text>
<claim-text>wherein extraction the second parameter of the second response data set comprises extraction of the second parameter of the second response data set from the verification object.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. A computer program product for model-based testing of an application program, the computer program product comprising:
<claim-text>a non-transitory computer readable storage medium having computer usable program code embodied therewith, the computer usable program code comprising a computer usable program code configured to:
<claim-text>receive a first processing request for the application program under test;</claim-text>
<claim-text>generate a second processing request for a model of the application program, wherein the model of the application program is configured to model an expected behavior of the application program under test and configured to provide a same interface as an interface for the application program under test, wherein the second processing request is equivalent to said first processing request;</claim-text>
<claim-text>communicate said first and second requests to said application program under test and said model of the application program respectively;</claim-text>
<claim-text>receive a first response data set from the application program under test and a second response data set from the model of the application program;</claim-text>
<claim-text>compare said first and second response data sets;</claim-text>
<claim-text>generate a success indication if said comparing said first and second response data sets does not identify a difference;</claim-text>
<claim-text>generate an error indication if said comparing said first and second response data sets identifies a difference between the first and second data sets;</claim-text>
<claim-text>receive a third processing request, which is based on the first response data set, for the application program under test;</claim-text>
<claim-text>extract a first parameter of the first response data set;</claim-text>
<claim-text>communicate the third processing request with the first parameter to the application program under test;</claim-text>
<claim-text>extract a second parameter of the second response data set;</claim-text>
<claim-text>generate fourth processing request equivalent to the third processing request; and</claim-text>
<claim-text>communicate the fourth processing request with the second parameter to the model of the application program.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The computer program product of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein said error indication comprises data indicating an attribute of the first and second response data sets that corresponds to the difference and indicating an expected value of the attribute.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The computer program product of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the computer usable program code configured to generate the error indication comprises computer usable program code configured to generate an exception.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The computer program product of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein said error indication references said first and second response data sets.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The computer program product of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the computer usable program code configured to compare said first and second response data sets comprises computer usable program code configured to execute a checking code on the first and second response data sets, wherein the checking code utilizes checking code rules.</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The computer program product of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the checking code rules indicate an object name, an object version, and data content to be compared.</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The computer program product of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the
<claim-text>computer usable program code configured to is configured to,</claim-text>
<claim-text>receive a third response data set from the application program under test and a fourth response data set from the model of the application program;</claim-text>
<claim-text>compare said third and fourth response data sets;</claim-text>
<claim-text>generate a success indication if said comparing said third and fourth response data sets does not identify a difference; and</claim-text>
<claim-text>generate an error indication if said comparing said third and fourth response data sets identifies a difference between the third and fourth response data sets.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. The apparatus of <claim-ref idref="CLM-00023">claim 23</claim-ref>,
<claim-text>wherein generation of the success indication for comparison of the first response data set and the second response data set comprises generation of a verification object that encapsulates the first and second response data sets,</claim-text>
<claim-text>wherein extraction of the first parameter of the first response data set comprises extraction of the first parameter of the first response data set from the verification object, and</claim-text>
<claim-text>wherein extraction the second parameter of the second response data set comprises extraction of the second parameter of the second response data set from the verification object.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
