<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625858-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625858</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12242470</doc-number>
<date>20080930</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>JP</country>
<doc-number>2007-259059</doc-number>
<date>20071002</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>868</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>62</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382118</main-classification>
<further-classification>382172</further-classification>
<further-classification>382216</further-classification>
<further-classification>382228</further-classification>
</classification-national>
<invention-title id="d2e71">Method, apparatus, and computer-readable storage medium for pattern recognition</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>6940545</doc-number>
<kind>B1</kind>
<name>Ray et al.</name>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482221</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>2005/0220336</doc-number>
<kind>A1</kind>
<name>Sabe et al.</name>
<date>20051000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382159</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>2005/0264658</doc-number>
<kind>A1</kind>
<name>Ray et al.</name>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>2006/0020597</doc-number>
<kind>A1</kind>
<name>Keating et al.</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>707  6</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>2007/0122010</doc-number>
<kind>A1</kind>
<name>Kitamura et al.</name>
<date>20070500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382118</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2007/0288419</doc-number>
<kind>A1</kind>
<name>Strassner</name>
<date>20071200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>706 55</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>JP</country>
<doc-number>11-073510</doc-number>
<kind>A</kind>
<date>19990300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>JP</country>
<doc-number>2001309225</doc-number>
<date>20000200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>JP</country>
<doc-number>2001-309225</doc-number>
<kind>A</kind>
<date>20011100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00010">
<othercit>Yusuke Mitarai, Katsuhiko Mori, and Masakazu Matsugu, Robust Face Detection System Based on Convolutional Neural Networks Using Selective Activation of Modules, Second Forum on Information Technology, FIT2003, Information Processing Society of Japan, Tokyo, Japan, Sep. 10-12, 2003.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00011">
<othercit>Henry A. Rowley, Shumeet Baluja, and Takeo Kanade, Rotation Invariant Neural Network-Based Face Detection, Proceedings of the 1998 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 38-44, IEEE Piscataway NJ, Jun. 23-25, 1998.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>19</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>None</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>16</number-of-drawing-sheets>
<number-of-figures>16</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20090087040</doc-number>
<kind>A1</kind>
<date>20090402</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Torii</last-name>
<first-name>Kan</first-name>
<address>
<city>Pittsburgh</city>
<state>PA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Mitarai</last-name>
<first-name>Yusuke</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Matsugu</last-name>
<first-name>Masakazu</first-name>
<address>
<city>Yokohama</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
<us-applicant sequence="004" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Kato</last-name>
<first-name>Masami</first-name>
<address>
<city>Sagamihara</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
<us-applicant sequence="005" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Mori</last-name>
<first-name>Katsuhiko</first-name>
<address>
<city>Kawasaki</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
<us-applicant sequence="006" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Sato</last-name>
<first-name>Hiroshi</first-name>
<address>
<city>Kawasaki</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Torii</last-name>
<first-name>Kan</first-name>
<address>
<city>Pittsburgh</city>
<state>PA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Mitarai</last-name>
<first-name>Yusuke</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Matsugu</last-name>
<first-name>Masakazu</first-name>
<address>
<city>Yokohama</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
<inventor sequence="004" designation="us-only">
<addressbook>
<last-name>Kato</last-name>
<first-name>Masami</first-name>
<address>
<city>Sagamihara</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
<inventor sequence="005" designation="us-only">
<addressbook>
<last-name>Mori</last-name>
<first-name>Katsuhiko</first-name>
<address>
<city>Kawasaki</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
<inventor sequence="006" designation="us-only">
<addressbook>
<last-name>Sato</last-name>
<first-name>Hiroshi</first-name>
<address>
<city>Kawasaki</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Canon USA Inc IP Division</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Canon Kabushiki Kaisha</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Mehta</last-name>
<first-name>Bhavesh</first-name>
<department>2665</department>
</primary-examiner>
<assistant-examiner>
<last-name>Dunphy</last-name>
<first-name>David F</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A pattern recognition method, applicable to input information including a plurality of regions, includes obtaining a certainty at which each region of the input information includes a pattern, selecting one or more regions having a relatively high-level certainty among the plurality of regions, and performing pattern detection processing on the selected region or regions.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="235.03mm" wi="153.84mm" file="US08625858-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="110.49mm" wi="133.10mm" file="US08625858-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="120.14mm" wi="130.22mm" file="US08625858-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="206.25mm" wi="133.86mm" file="US08625858-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="239.10mm" wi="153.59mm" file="US08625858-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="239.44mm" wi="168.15mm" file="US08625858-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="117.86mm" wi="85.85mm" file="US08625858-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="216.07mm" wi="156.38mm" orientation="landscape" file="US08625858-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="191.01mm" wi="141.14mm" file="US08625858-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="144.02mm" wi="142.32mm" file="US08625858-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="243.92mm" wi="167.64mm" file="US08625858-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="109.39mm" wi="154.09mm" file="US08625858-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="128.86mm" wi="144.61mm" file="US08625858-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="197.53mm" wi="153.92mm" orientation="landscape" file="US08625858-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="214.38mm" wi="145.20mm" orientation="landscape" file="US08625858-20140107-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="208.70mm" wi="163.41mm" file="US08625858-20140107-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00016" num="00016">
<img id="EMI-D00016" he="246.72mm" wi="167.05mm" file="US08625858-20140107-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0002" num="0001">1. Field of the Invention</p>
<p id="p-0003" num="0002">The present invention relates to a pattern recognition method, a pattern recognition apparatus, and a computer-readable storage medium storing a program enabling a computer to execute a pattern recognition method. The pattern recognition generally includes image recognition and speech recognition.</p>
<p id="p-0004" num="0003">2. Description of the Related Art</p>
<p id="p-0005" num="0004">A conventional pattern recognition method directed to pattern recognition (e.g., image recognition and speech recognition) tends to decrease the processing speed if higher identification accuracy (recognition accuracy) is necessary and tends to deteriorate the identification accuracy if a higher processing speed is required.</p>
<p id="p-0006" num="0005">To satisfy requirements in processing speed and identification accuracy, a conventional pattern recognition method includes connecting a first identifier improved in processing speed and a second identifier enhanced in identification accuracy (See &#x201c;Robust Face Detection System Based on Convolutional Neural Networks Using Selective Activation of Modules&#x201d; by Yusuke Mitarai, Katsuhiko Mori, and Masakazu Matsugu, Second Forum on Information Technology, 2003) According to such a conventional pattern recognition method, the first identifier speedily detects candidate regions and the second identifier accurately evaluates the candidate regions.</p>
<p id="p-0007" num="0006">However, many of identifiers, which are usable as the above-described first or second identifier, generate a multi-valued output referred to as &#x201c;certainty&#x201d;, as an identification result. For example, Japanese Patent Application Laid-Open No. 2001-309225 discusses a conventional method includes binarizing a multi-valued output referred to as &#x201c;certainty&#x201d; with a threshold and determining the presence of any pattern.</p>
<p id="p-0008" num="0007">Two or more identifiers (discriminant functions) are commonly used to classify input information into a plurality of groups and identify a group (identifier) having a highest output value. For example, an identifier referred to as &#x201c;Perceptron&#x201d; selects a linear function that maximizes a linear sum of input information and obtains a classification corresponding to the selected linear function as an identification result. As discussed in &#x201c;Rotation Invariant Neural Network-Based Face Detection&#x201d; by Henry A. Rowley, Shumeet Baluja, and Takeo Kanade, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 1998, by calculating a linear sum using a weighting factor corresponding to an output value of each identifier, a unique classification can be obtained.</p>
<p id="p-0009" num="0008">According to a conventional method using a plurality of identifiers and binarizing a certainty (output) of each identifier with a fixed threshold, selecting an appropriate threshold may be difficult because an output range of each identifier is variable depending on acquisition conditions of input information. For example, when input information is image data, if shooting conditions of an image relating to the input image data are inappropriate, each identifier may not be able to detect a face and may generate a weak output.</p>
<p id="p-0010" num="0009">If a threshold used in such conditions is excessively high, the identifier cannot identify a face involved in an image as a candidate due to a weak output value. On the contrary, if the threshold is excessively low, the second identifier detects so many candidates that may decrease the processing speed. Namely, using a fixed threshold may fail to identify a face in an image in various shooting conditions.</p>
<p id="p-0011" num="0010">According to a method for selecting only one candidate (which has a maximum output value) among outputs of a plurality of identifiers, the first identifier having lower identification accuracy may not be able to detect a correct candidate. If the identification accuracy of the first identifier is lower, the output of an identifier corresponding to a correct candidate may not always take a maximum value. Furthermore, the method cannot identify two or more correct candidates simultaneously, because the method leaves only one candidate.</p>
<p id="p-0012" num="0011">Moreover, a complex identifier includes a plurality of identifiers. If the complex identifier employs serial and parallel arrangements to connect the identifiers, it is desired to constitute a plurality of groups each including serially connected identifiers and connect the groups in parallel to reduce a required memory capacity. However, according to such an arrangement, a rear-stage identifier constituting a serial identifier group tends to perform useless processing. The processing time tends to be longer and the identification tends to end unsuccessfully, because each serial identifier group performs processing independently. In fact, execution of a rear-stage identifier in one identifier group is dependent on the execution of a rear-stage identifier in another identifier group.</p>
<heading id="h-0002" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0013" num="0012">Exemplary embodiments of the present invention are directed to a pattern recognition method applicable to input information, which can easily select a threshold and can satisfy requirements in processing speed and recognition accuracy.</p>
<p id="p-0014" num="0013">According to an aspect of the present invention, a pattern recognition method applicable to input information including a plurality of regions includes obtaining a certainty at which each region of the input information includes a pattern, selecting one or more regions having a relatively high-level certainty among the plurality of regions, and performing pattern detection processing on the selected region or regions.</p>
<p id="p-0015" num="0014">According to another aspect of the present invention, a pattern recognition method applicable to input information includes obtaining a certainty at which the input information includes a respective one of patterns of first to n-th (n being a natural number equal to or greater than 2) classifications, selecting one or more classifications having a relatively high-level certainty among the obtained certainties, and performing pattern detection processing on the input information based on the selected classification or classifications.</p>
<p id="p-0016" num="0015">According to still another aspect of the present invention, a pattern recognition method applicable to input information including a plurality of regions includes obtaining a certainty at which each region of the input information includes a respective one of patterns of first to n-th (n being a natural number equal to or greater than 2) classifications, selecting one or more combinations of a region and a classification having a relatively high-level certainty among the obtained certainties, and performing pattern detection processing on the input information according to the selected combination or combinations.</p>
<p id="p-0017" num="0016">Further features and aspects of the present invention will become apparent from the following detailed description of exemplary embodiments with reference to the attached drawings.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0018" num="0017">The accompanying drawings, which are incorporated in and constitute a part of the specification, illustrate exemplary embodiments and features of the invention and, together with the description, serve to explain at least some of the principles of the invention.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram illustrating an example hardware configuration of a pattern recognition apparatus according to a first exemplary embodiment of the present invention.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram illustrating an example functional configuration of the pattern recognition apparatus according to the first exemplary embodiment of the present invention.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 3</figref> is a flowchart illustrating an example procedure of processing performed by the pattern recognition apparatus according to the first exemplary embodiment of the present invention.</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 4</figref> illustrates an example procedure of face detection processing performed in step S<b>304</b> illustrated in <figref idref="DRAWINGS">FIG. 3</figref>.</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 5</figref> illustrates an example format for first certainty information illustrated in <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 6</figref> illustrates an example format for counting result information of the first certainty information illustrated in <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 7</figref> illustrates an example detailed procedure of processing performed by a first identifier illustrated in <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 8</figref> is a flowchart illustrating an example detailed procedure of processing performed by a threshold determination processing unit illustrated in <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 9</figref> illustrates an example detailed procedure of processing performed by a second identifier illustrated in <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 10</figref> is a flowchart illustrating an example detailed procedure of processing performed in step S<b>304</b> illustrated in <figref idref="DRAWINGS">FIG. 3</figref>.</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 11</figref> is a block diagram illustrating an example hardware configuration of a pattern recognition apparatus according to a second exemplary embodiment of the present invention.</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 12</figref> illustrates an example display screen of a display device illustrated in <figref idref="DRAWINGS">FIG. 11</figref>.</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 13</figref> is a block diagram illustrating an example functional configuration of the pattern recognition apparatus according to the second exemplary embodiment of the present invention.</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 14</figref> illustrates an example format for Fourier transformation result information illustrated in <figref idref="DRAWINGS">FIG. 13</figref>.</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 15</figref> is a flowchart illustrating an example detailed procedure of processing performed by a high-level frequency selection processing unit illustrated in <figref idref="DRAWINGS">FIG. 13</figref>.</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 16</figref> is a flowchart illustrating an example detailed procedure of processing performed by a comparison calculation processing unit illustrated in <figref idref="DRAWINGS">FIG. 13</figref>.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION OF THE EMBODIMENTS</heading>
<p id="p-0035" num="0034">The following description of exemplary embodiments is illustrative in nature and is in no way intended to limit the invention, its application, or uses. Processes, techniques, apparatus, and systems as known by one of ordinary skill in the art are intended to be part of the enabling description where appropriate. It is noted that throughout the specification, similar reference numerals and letters refer to similar items in the following figures, and thus once an item is described in one figure, it may not be discussed for following figures. Exemplary embodiments will be described in detail below with reference to the drawings.</p>
<heading id="h-0005" level="1">First Exemplary Embodiment</heading>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram illustrating an example hardware configuration of a pattern recognition apparatus <b>100</b> according to a first exemplary embodiment of the present invention. The pattern recognition apparatus <b>100</b> according to the first exemplary embodiment is, for example, an information processing apparatus. In an exemplary embodiment, input information including a plurality of regions is image data and a pattern recognized from the image data is a face.</p>
<p id="p-0037" num="0036">As illustrated in <figref idref="DRAWINGS">FIG. 1</figref>, the pattern recognition apparatus <b>100</b> according to the first exemplary embodiment includes a central processing unit (CPU) <b>101</b>, a program memory <b>102</b>, a random access memory (RAM) <b>103</b>, a hard disk (image database) <b>104</b>, a flash memory <b>105</b>, and a control bus/data bus <b>110</b>. The CPU <b>101</b> integrally controls various operations performed by the pattern recognition apparatus <b>100</b>. For example, the CPU <b>101</b> executes a program to realize an image processing method according to an exemplary embodiment.</p>
<p id="p-0038" num="0037">The CPU <b>101</b> can execute program(s) stored in the program memory <b>102</b>. The RAM <b>103</b> temporarily stores various information and various data while the CPU <b>101</b> executes the program(s).</p>
<p id="p-0039" num="0038">The hard disk <b>104</b> stores various data including image data (image files), which is input from an external apparatus. The CPU <b>101</b> can read or write various information and various data from or to the flash memory <b>105</b>. The flash memory <b>105</b> is attachable to or detachable from the pattern recognition apparatus <b>100</b>, so that a user can carry various information and various data stored in the flash memory <b>105</b>.</p>
<p id="p-0040" num="0039">The control bus/data bus <b>110</b> connects the CPU <b>101</b> to the above-described devices <b>102</b> to <b>105</b>. The pattern recognition apparatus <b>100</b> further includes an input device (e.g., a keyboard or a pointing device) and a display device.</p>
<p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram illustrating an example functional configuration of the pattern recognition apparatus <b>100</b> according to the first exemplary embodiment of the present invention. The CPU <b>101</b> executes a program stored in the program memory <b>102</b> to realize functional units <b>201</b> to <b>203</b> illustrated in <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0042" num="0041">A hard disk <b>104</b> illustrated in <figref idref="DRAWINGS">FIG. 2</figref>, which is similar to the hard disk <b>104</b> illustrated in <figref idref="DRAWINGS">FIG. 1</figref>, stores image data (image files). The image reading unit <b>201</b> reads out image data from the hard disk <b>104</b> and stores the read data in the RAM <b>103</b>.</p>
<p id="p-0043" num="0042">The face detection unit <b>202</b> determines whether the image data written in the RAM <b>103</b> includes face data. The image writing unit <b>203</b> writes, into the flash memory <b>105</b>, the image data loaded in the RAM <b>103</b>. A flash memory <b>105</b> is similar to the flash memory illustrated in <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. 3</figref> is a flowchart illustrating an example procedure of processing performed by the pattern recognition apparatus according to the first exemplary embodiment of the present invention. More specifically, <figref idref="DRAWINGS">FIG. 3</figref> is a flowchart illustrating an example processing procedure for the functional configuration illustrated in <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0045" num="0044">In step S<b>301</b>, the image reading unit <b>201</b> initializes an image pointer so that the image pointer indicates initial image data. In this exemplary embodiment, the image pointer is a variable successively designating image data (image files) stored in the hard disk <b>104</b>.</p>
<p id="p-0046" num="0045">In step S<b>302</b>, the image reading unit <b>201</b> determines whether image data to be processed is present in the hard disk <b>104</b>. If the image reading unit <b>201</b> determines that there is no image data to be processed (NO in step S<b>302</b>), namely, when the processing has been completed for all image data, the image reading unit <b>201</b> terminates the processing of this routine.</p>
<p id="p-0047" num="0046">On the other hand, if the image reading unit <b>201</b> determines that there is image data to be processed (YES in step S<b>302</b>), the processing proceeds to step S<b>303</b>. In step S<b>303</b>, the image reading unit <b>201</b> reads out the image data currently indicated by the image pointer from the hard disk <b>104</b> to load it into the RAM <b>103</b>. In step S<b>304</b>, the face detection unit <b>202</b> determines whether the image data loaded into the RAM <b>103</b> in step S<b>303</b> includes face data. As described below, <figref idref="DRAWINGS">FIG. 4</figref> illustrates an example method for the determination processing in step S<b>304</b>.</p>
<p id="p-0048" num="0047">If the face detection unit <b>202</b> determines that the image data includes face data (YES in step S<b>304</b>), the processing proceeds to step S<b>305</b>. In step S<b>305</b>, the image writing unit <b>203</b> writes, into the flash memory <b>105</b>, the image data loaded to the RAM <b>103</b>.</p>
<p id="p-0049" num="0048">After the processing of step S<b>305</b> is completed, or when the face detection unit <b>202</b> determines that there is no face data in the image data (NO in step S<b>304</b>), the processing proceeds to step S<b>306</b>. In step S<b>306</b>, the image reading unit <b>201</b> shifts the image pointer to the next image data (image files) stored in the hard disk <b>104</b>. Then, the processing returns to step S<b>302</b>.</p>
<p id="p-0050" num="0049">Through the above-described processing of steps S<b>301</b> to S<b>306</b>, the face detection unit <b>202</b> can accomplish the face detection processing for all image data stored in the hard disk <b>104</b>.</p>
<p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. 4</figref> illustrates an example procedure of the face detection processing performed in step S<b>304</b> illustrated in <figref idref="DRAWINGS">FIG. 3</figref>. The face detection unit <b>202</b>, performing the face detection processing in step S<b>304</b> illustrated in <figref idref="DRAWINGS">FIG. 3</figref>, includes a first identifier <b>402</b>, a distribution processing unit <b>404</b>, a threshold determination processing unit <b>406</b>, a binarization processing unit <b>407</b>, a second identifier <b>409</b>, and a threshold determination processing unit <b>411</b>, which are illustrated as circular blocks in <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0052" num="0051">In <figref idref="DRAWINGS">FIG. 4</figref>, image data <b>401</b> is the image data loaded into the RAM <b>103</b> by the image reading unit <b>201</b> in step S<b>303</b>. The first identifier <b>402</b> obtains a certainty at which each image region of the image data <b>401</b> includes a respective one of first to n-th (n being a natural number equal to or greater than 2) classification face patterns. As described below, <figref idref="DRAWINGS">FIG. 7</figref> illustrates details of the first identifier <b>402</b>.</p>
<p id="p-0053" num="0052">The first identifier <b>402</b> generates first certainty information <b>403</b>, which includes two or more pieces of information disposed in a two-dimensional pattern. For example, the RAM <b>103</b> stores the first certainty information <b>403</b>. As described below, <figref idref="DRAWINGS">FIG. 5</figref> illustrates an example format for the first certainty information <b>403</b>.</p>
<p id="p-0054" num="0053">The distribution processing unit <b>404</b> performs processing for distributing certainties to a plurality of bins according to their values, to reduce the processing time of the first certainty information <b>403</b>. The distribution processing unit <b>404</b> generates counting result information <b>405</b> of the first certainty information. For example, the counting result information <b>405</b> of the first certainty information is a distribution of certainties obtained by the first identifier <b>402</b>.</p>
<p id="p-0055" num="0054">The counting result information <b>405</b> of the first certainty information is information indicating a processing result generated by the distribution processing unit <b>404</b>. For example, the RAM <b>103</b> stores the counting result information <b>405</b> of the first certainty information. As described below, <figref idref="DRAWINGS">FIG. 6</figref> illustrates an example format for the counting result information <b>405</b> of the first certainty information.</p>
<p id="p-0056" num="0055">The threshold determination processing unit <b>406</b> determines a threshold to be applied to the first certainty information <b>403</b> based on the counting result information <b>405</b> of the first certainty information. As described below, <figref idref="DRAWINGS">FIG. 8</figref> illustrates example processing performed by the threshold determination processing unit <b>406</b>.</p>
<p id="p-0057" num="0056">The binarization processing unit <b>407</b> performs binarization processing on the first certainty information <b>403</b> with a threshold determined by the threshold determination processing unit <b>406</b>. The binarization processing unit <b>407</b> selects one or more image regions having a relatively high-level certainty (the relatively high-level certainty being established as a threshold certainty) from among the certainties obtained by the first identifier <b>402</b>, or one or more classifications (inclination and size of a face according to an exemplary embodiment), or one or more combinations of the image region and the classification.</p>
<p id="p-0058" num="0057">The binarization processing unit <b>407</b> outputs first detection result information <b>408</b>, which is resultant from the binarization processing. The first detection result information <b>408</b> includes an array of candidates to be processed by the second identifier <b>409</b>. More specifically, the first detection result information <b>408</b> indicates a rotational angle and a reduction rate in the processing of each image region included in the image data <b>401</b>. For example, the RAM <b>103</b> stores the first detection result information <b>408</b>. An example format for the first detection result information <b>408</b> is similar to the format for the first certainty information <b>403</b>, although elements disposed in a two-dimensional pattern are binary data.</p>
<p id="p-0059" num="0058">The second identifier <b>409</b> performs face detection processing on the image data <b>401</b> based on the first detection result information <b>408</b>. The second identifier <b>409</b> outputs second certainty information <b>410</b>, which is resultant from the processing. For example, the RAM <b>103</b> stores the second certainty information <b>410</b>.</p>
<p id="p-0060" num="0059">As the second certainty information <b>410</b> is multi-valued information, the threshold determination processing unit <b>411</b> scans and determines whether any value exceeding a threshold is present. The threshold determination processing unit <b>411</b> outputs face detection result information <b>412</b>, which is resultant from the determination processing. The face detection result information <b>412</b> indicates the presence of a face involved in the image data <b>401</b>, when the threshold determination processing unit <b>411</b> determines that there is a value exceeding the threshold. For example, the RAM <b>103</b> stores the face detection result information <b>412</b>. The face detection result information <b>412</b> is used in the face detection processing performed in step S<b>304</b> illustrated in <figref idref="DRAWINGS">FIG. 3</figref>.</p>
<p id="p-0061" num="0060">The second identifier <b>409</b> and the threshold determination processing unit <b>411</b> perform face detection processing on the image data <b>401</b>, according to the image region selected by the binarization processing unit <b>407</b>, the classification, or a combination of the image region and the classification.</p>
<p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. 5</figref> illustrates an example format for the first certainty information <b>403</b> illustrated in <figref idref="DRAWINGS">FIG. 4</figref>. In <figref idref="DRAWINGS">FIG. 5</figref>, the first identifier <b>402</b> can identify 0 degrees, 90 degrees, 180 degrees, and 270 degrees in the inclination of face and can identify one time, two times, and four times in the size of face. In an example embodiment, the face size of one time indicates a minimum face size (e.g., 30 pixels in face width). The face size of two times has a face width of 60 pixels. The description of the embodiment uses magnifications representing face sizes instead of using numerical values, because the numerical values are not relevant to the gist of the invention.</p>
<p id="p-0063" num="0062">Image data <b>401</b> illustrated in <figref idref="DRAWINGS">FIG. 5</figref> is similar to the image data <b>401</b> illustrated in <figref idref="DRAWINGS">FIG. 4</figref>. <figref idref="DRAWINGS">FIG. 5</figref> illustrates twelve pieces of certainty information <b>500</b> to <b>503</b>, <b>510</b> to <b>513</b>, and <b>520</b> to <b>523</b>, as detailed contents of the first certainty information <b>403</b>.</p>
<p id="p-0064" num="0063">The 0-degrees/one-time certainty information <b>500</b> is certainty information relating to a face of 0 degrees in inclination and one time in size, i.e., certainty information indicating that 0-degrees/one-time face is present in a corresponding region of the image data <b>401</b> (an element of the two-dimensional pattern).</p>
<p id="p-0065" num="0064">Similarly, the 90-degrees/one-time certainty information <b>501</b> is certainty information relating to a face of 90 degrees in inclination and one time in size. The 180-degrees/one-time certainty information <b>502</b> is certainty information relating to a face of 180 degrees in inclination and one time in size. The 270-degrees/one-time certainty information <b>503</b> is certainty information relating to a face of 270 degrees in inclination and one time in size.</p>
<p id="p-0066" num="0065">The 0-degrees/two-times certainty information <b>510</b> is certainty information relating to a face of 0 degrees in inclination and two times in size. The 90-degrees/two-times certainty information <b>511</b> is certainty information relating to a face of 90 degrees in inclination and two times in size. The 180-degrees/two-times certainty information <b>512</b> is certainty information relating to a face of 180 degrees in inclination and two times in size. The 270-degrees/two-times certainty information <b>513</b> is certainty information relating to a face of 270 degrees in inclination and two times in size.</p>
<p id="p-0067" num="0066">The 0-degrees/four-times certainty information <b>520</b> is certainty information relating to a face of 0 degrees in inclination and four times in size. The 90-degrees/four-times certainty information <b>521</b> is certainty information relating to a face of 90 degrees in inclination and four times in size. The 180-degrees/four-times certainty information <b>522</b> is certainty information relating to a face of 180 degrees in inclination and four times in size. The 270-degrees/four-times certainty information <b>523</b> is certainty information relating to a face of 270 degrees in inclination and four times in size.</p>
<p id="p-0068" num="0067">More specifically, the first certainty information <b>403</b> illustrated in <figref idref="DRAWINGS">FIG. 4</figref>, when it has accomplished the classification of face in inclination and size, has a plurality pieces of certainty information (i.e., from certainty information relating to a first classification face to certainty information relating to an n-th classification face) A first image pattern includes a first classification face. An n-th image pattern includes an n-th classification face. The example illustrated in <figref idref="DRAWINGS">FIG. 5</figref> includes certainty information relating to the first classification face to certainty information relating to the twelfth classification face (i.e., twelve pieces of certainty information <b>500</b> to <b>503</b>, <b>510</b> to <b>513</b>, and <b>520</b> to <b>523</b>).</p>
<p id="p-0069" num="0068">The example illustrated in <figref idref="DRAWINGS">FIG. 5</figref> includes twelve pieces of certainty information classified into four types (0 degrees, 90 degrees, 180 degrees, and 270 degrees) in face inclination, as an example of a plurality of inclinations (first inclination to n-th inclination (n is a natural number equal to or greater than 2)).</p>
<p id="p-0070" num="0069">Furthermore, the example illustrated in <figref idref="DRAWINGS">FIG. 5</figref> includes twelve pieces of certainty information classified into three types (one time, two times, and four times) in face size, as an example of a plurality of sizes (first size to n-th size (n is a natural number equal to or greater than 2).</p>
<p id="p-0071" num="0070">In <figref idref="DRAWINGS">FIG. 5</figref>, dimensions of the certainty information relating to a face having a two-times or four-times size (<b>510</b> to <b>513</b> or <b>520</b> to <b>523</b>) is illustrated as &#xbd; or &#xbc; times the dimensions of the certainty information relating to a face having a one-time size (<b>500</b> to <b>503</b>). This is because the identifier can identify a face having an enlarged size (two times or four times) by appropriately reducing the size of the image data <b>401</b>. According to an exemplary embodiment, when input image data has a reduced size, the identifier outputs small certainty information correspondingly.</p>
<p id="p-0072" num="0071">The RAM <b>103</b> illustrated in <figref idref="DRAWINGS">FIG. 1</figref> rasterizes each one (two-dimensional information) of the twelve pieces of certainty information <b>500</b> to <b>503</b>, <b>510</b> to <b>513</b>, and <b>520</b> to <b>523</b> illustrated in <figref idref="DRAWINGS">FIG. 5</figref>. A point on two-dimensional certainty information corresponds to a region on the image data <b>401</b> and indicates a certainty at which the region includes a face having a predetermined inclination and a predetermined size.</p>
<p id="p-0073" num="0072">For example, a point (x, y) on the 0-degrees/one-time certainty information <b>500</b> indicates a certainty at which a rectangular region having a diagonal connecting two points (x&#x2212;w/2, y&#x2212;h/2) and (x+w/2, y+h/2) on the image data <b>401</b> includes a 0-degrees/one-time face. In this case, &#x201c;w&#x201d; represents a minimum face width and &#x201c;h&#x201d; represents a minimum face height.</p>
<p id="p-0074" num="0073">Similarly, a point (x, y) on the 90-degrees/n-times certainty information indicates a certainty at which a rectangular region having a diagonal connecting two points (n(y&#x2212;h/2), H&#x2212;n(x+w/2)) and (n(y+h/2, H&#x2212;n(x&#x2212;w/2)) on the image data <b>401</b> includes a 90-degrees/n-times face. In this case, &#x201c;W&#x201d; represents the width of the image data <b>401</b> and &#x201c;H&#x201d; represents the height of the image data <b>401</b>. A point (x,y) on the 180-degrees/n-times certainty information indicates a certainty at which a rectangular region having a diagonal connecting two points (W&#x2212;n(x+w/2), H&#x2212;n(y+h/2)) and (W&#x2212;n(x&#x2212;w/2), H&#x2212;n(y&#x2212;h/2)) on the image data <b>401</b> includes a 180-degrees/n-times face. A similar definition is applicable to the 270 degrees/n-times certainty information.</p>
<p id="p-0075" num="0074">The second certainty information <b>410</b> illustrated in <figref idref="DRAWINGS">FIG. 4</figref> has a format similar to that of the first certainty information <b>403</b>. As described below, the number of the second identifier <b>409</b> can be greater than the number of the first identifier <b>402</b>. In this case, the number of the second certainty information <b>410</b> disposed in a two-dimensional pattern is greater than the number of the first certainty information <b>403</b> disposed in a two-dimensional pattern.</p>
<p id="p-0076" num="0075"><figref idref="DRAWINGS">FIG. 6</figref> illustrates an example format for the counting result information <b>405</b> of the first certainty information illustrated in <figref idref="DRAWINGS">FIG. 4</figref>. For example, the RAM <b>103</b> can rasterize the format for the counting result information <b>405</b> of the first certainty information illustrated in <figref idref="DRAWINGS">FIG. 6</figref>. According to the example illustrated in <figref idref="DRAWINGS">FIG. 6</figref>, the maximum value of the certainty of the first certainty information <b>403</b> is 1.0. <figref idref="DRAWINGS">FIG. 6</figref> illustrates a bin number allocated to each bin.</p>
<p id="p-0077" num="0076">Bin <b>10</b> stores the number of two-dimensional data constituting the first certainty information <b>403</b> whose certainty is greater than 0.9 and equal to or less than 1.0. Bin <b>9</b> stores the number of two-dimensional data constituting the first certainty information <b>403</b> whose certainty is greater than 0.8 and equal to or less than 0.9. Bin <b>8</b> stores the number of two-dimensional data constituting the first certainty information <b>403</b> whose certainty is greater than 0.7 and equal to or less than 0.8. Bin <b>7</b> stores the number of two-dimensional data constituting the first certainty information <b>403</b> whose certainty is greater than 0.6 and equal to or less than 0.7. Bin <b>6</b> stores the number of two-dimensional data constituting the first certainty information <b>403</b> whose certainty is greater than 0.5 and equal to or less than 0.6. Each of Bins <b>1</b> to <b>5</b> has a storage configuration similar to the above-described configuration.</p>
<p id="p-0078" num="0077">In this manner, the distribution processing unit <b>404</b> generates a certainty distribution of certainty values relating to the two-dimensional information stored in the first certainty information <b>403</b>, by classifying the certainty values into respective bins according to the magnitude of each certainty value, as two-dimensional data constituting the first certainty information <b>403</b>. For example, the distribution processing unit <b>404</b> generates a histogram representing a distribution of certainty. Numerical values, including the number of bins, illustrated in <figref idref="DRAWINGS">FIG. 6</figref> are mere examples and are arbitrarily changeable.</p>
<p id="p-0079" num="0078"><figref idref="DRAWINGS">FIG. 7</figref> illustrates an example detailed procedure of processing performed by the first identifier <b>402</b> illustrated in <figref idref="DRAWINGS">FIG. 4</figref>. The first identifier <b>402</b> includes a flesh-color extraction processing unit <b>701</b>, an affine transformation processing unit <b>703</b>, and a first face detector <b>706</b>, which are illustrated as circular blocks in <figref idref="DRAWINGS">FIG. 7</figref>.</p>
<p id="p-0080" num="0079">Image data <b>401</b> illustrated in <figref idref="DRAWINGS">FIG. 7</figref> is similar to the image data <b>401</b> illustrated in <figref idref="DRAWINGS">FIG. 4</figref>. The flesh-color extraction processing unit <b>701</b> performs flesh-color pixel extraction processing on the image data <b>401</b>. More specifically, the flesh-color extraction processing unit <b>701</b> writes &#x201c;1&#x201d; to a portion corresponding to a flesh-color pixel of the image data <b>401</b> and &#x201c;0&#x201d; to a portion corresponding to a non-flesh-color pixel, and generates a flesh-color mask <b>702</b>. For example, the RAM <b>103</b> stores the generated flesh-color mask <b>702</b>.</p>
<p id="p-0081" num="0080">The affine transformation processing unit <b>703</b> performs affine transformation processing for rotating and reducing the image data <b>401</b> and the flesh-color mask <b>702</b>. More specifically, when the first identifier <b>402</b> obtains certainties including the first classification image pattern to the n-th classification image pattern, the affine transformation processing unit <b>703</b> uses a total of n methods for converting the image data <b>401</b>.</p>
<p id="p-0082" num="0081">For example, the RAM <b>103</b> stores affine transformed image data <b>704</b> as a result of the image data <b>401</b> that is rotated and reduced by the affine transformation processing unit <b>703</b>. For example, the RAM <b>103</b> stores an affine transformed flesh-color mask <b>705</b> as a result of the flesh-color mask <b>702</b> that is rotated and reduced by the affine transformation processing unit <b>703</b>.</p>
<p id="p-0083" num="0082">In this case, a rectangular layout is convenient for calculation of some rotational angles. The affine transformed image data <b>704</b> and the affine transformed flesh-color mask <b>705</b> may include ineffective pixels that do not correspond to the image data <b>401</b> and the flesh-color mask <b>702</b>. Such ineffective pixels are negligible in subsequent processing.</p>
<p id="p-0084" num="0083">The first face detector <b>706</b> obtains a certainty at which a face is present at a position on the affine transformed image data <b>704</b> corresponding to the position to which the affine transformed flesh-color mask <b>705</b> writes &#x201c;1.&#x201d; Although not illustrated in <figref idref="DRAWINGS">FIG. 7</figref>, the affine transformation processing unit <b>703</b> and the first face detector <b>706</b> repetitively perform the loop processing to obtain a required amount of certainty information.</p>
<p id="p-0085" num="0084">For example, to identify the face inclination classified into one of four groups (0 degrees, 90 degrees, 180 degrees, and 270 degrees) and the face size classified into one of three groups (one time, two times, and four times), the affine transformation processing unit <b>703</b> and the first face detector <b>706</b> repetitively perform the loop processing twelve times in total. As a result, the exemplary embodiment can prepare twelve pieces of certainty information as the first certainty information <b>403</b>.</p>
<p id="p-0086" num="0085">As described below, <figref idref="DRAWINGS">FIG. 10</figref> illustrates an example loop execution method. For example, the first face detector <b>706</b> can use an object recognition method discussed in Japanese Patent No. 3078166. Furthermore, the first face detector <b>706</b> can use the neural network discussed in Japanese Patent Application Laid-Open No. 2002-8032 or Japanese Patent Application Laid-Open No. 2002-8033, or other pattern recognition method.</p>
<p id="p-0087" num="0086"><figref idref="DRAWINGS">FIG. 8</figref> is a flowchart illustrating an example detailed procedure of processing performed by the threshold determination processing unit <b>406</b> illustrated in <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0088" num="0087">In step S<b>801</b>, the threshold determination processing unit <b>406</b> initializes a sum variable &#x201c;s&#x201d; to 0. The sum variable &#x201c;s&#x201d; is a variable representing a sum.</p>
<p id="p-0089" num="0088">In step S<b>802</b>, the threshold determination processing unit <b>406</b> sets a bin number &#x201c;i&#x201d; to a maximum value (=10). The bin number &#x201c;i&#x201d; is a variable representing a number allocated to a target bin. Namely, the threshold determination processing unit <b>406</b> sets, as a target bin number, bin <b>10</b> illustrated in <figref idref="DRAWINGS">FIG. 6</figref>.</p>
<p id="p-0090" num="0089">In step S<b>803</b>, the threshold determination processing unit <b>406</b> adds, to the sum variable &#x201c;s&#x201d;, a numerical value representing the number of certainty values stored in the target bin (bin number &#x201c;i&#x201d;).</p>
<p id="p-0091" num="0090">In step S<b>804</b>, the threshold determination processing unit <b>406</b> determines whether the sum variable &#x201c;s&#x201d; is equal to or greater than 5% of the total number of effective pixels constituting the first certainty information <b>403</b>. The effective pixels are pixels other than ineffective pixels of the affine transformed image data <b>704</b>.</p>
<p id="p-0092" num="0091">If the threshold determination processing unit <b>406</b> determines that the sum variable &#x201c;s&#x201d; is smaller than 5% of the total number of effective pixels constituting the first certainty information <b>403</b> (NO in step S<b>804</b>), the processing proceeds to step S<b>805</b>.</p>
<p id="p-0093" num="0092">In step S<b>805</b>, the threshold determination processing unit <b>406</b> decrements the bin number &#x201c;i&#x201d; by 1 to change the target bin. The processing returns to step S<b>803</b>. Then, the threshold determination processing unit <b>406</b> repetitively executes the loop processing of steps S<b>803</b> and S<b>805</b> until the sum variable &#x201c;s&#x201d; becomes equal to or greater than 5% of the total number of effective pixels constituting the first certainty information <b>403</b> in step S<b>804</b>.</p>
<p id="p-0094" num="0093">If the threshold determination processing unit <b>406</b> determines that the sum variable &#x201c;s&#x201d; equal to or greater than 5% of the total number of effective pixels constituting the first certainty information <b>403</b> (YES in step S<b>804</b>), the processing proceeds to step S<b>806</b>. In step S<b>806</b>, the threshold determination processing unit <b>406</b> determines a threshold. More specifically, the threshold determination processing unit <b>406</b> sets a threshold to be equal to (i&#x2212;1)/10.</p>
<p id="p-0095" num="0094">Through the above-described processing of steps S<b>801</b> to S<b>806</b>, the threshold determination processing unit <b>406</b> can accomplish the processing for determining a threshold based on the counting result information <b>405</b> of the first certainty information illustrated in <figref idref="DRAWINGS">FIG. 6</figref>.</p>
<p id="p-0096" num="0095">The numerical values (5% and 10) used in steps S<b>804</b> and S<b>802</b> are mere example values and appropriately changeable.</p>
<p id="p-0097" num="0096"><figref idref="DRAWINGS">FIG. 9</figref> illustrates an example detailed procedure of processing performed by the second identifier <b>409</b> illustrated in <figref idref="DRAWINGS">FIG. 4</figref>. The second identifier <b>409</b> includes a segmenting affine transformation processing unit <b>901</b> and a second face detector <b>903</b>, which are illustrated as circular blocks in <figref idref="DRAWINGS">FIG. 9</figref>.</p>
<p id="p-0098" num="0097">Image data <b>401</b> illustrated in <figref idref="DRAWINGS">FIG. 9</figref> is similar to the image data <b>401</b> illustrated in <figref idref="DRAWINGS">FIG. 4</figref>. First detection result information <b>408</b> illustrated in <figref idref="DRAWINGS">FIG. 9</figref> is similar to the first detection result information <b>408</b> illustrated in <figref idref="DRAWINGS">FIG. 4</figref>. The segmenting affine transformation processing unit <b>901</b> performs affine transformation processing for segmenting a corresponding region from the image data <b>401</b> and rotating/reducing the segmented region according to the content of the first detection result information <b>408</b>. For example, the RAM <b>103</b> stores, as segmenting affine transformed image data <b>902</b>, a result of the affine transformation processing performed by the segmenting affine transformation processing unit <b>901</b>.</p>
<p id="p-0099" num="0098">The second face detector <b>903</b> detects a face included in the segmenting affine transformed image data <b>902</b>. For example, the RAM <b>103</b> stores the second certainty information <b>410</b> indicating a certainty at which a face is present. Second certainty information <b>410</b> illustrated in <figref idref="DRAWINGS">FIG. 9</figref> is similar to the second certainty information <b>410</b> illustrated in <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0100" num="0099">The second face detector <b>903</b>, capable of performing face detection processing, can have a configuration similar or dissimilar to that of the first face detector <b>706</b>. When the second face detector <b>903</b> is similar to the first face detector <b>706</b>, the second face detector <b>903</b> desirably has parameters different from those of the first face detector <b>706</b>. More specifically, in view of processing speed, it is effective that the first face detector <b>706</b> has higher robustness compared to the second face detector <b>903</b>.</p>
<p id="p-0101" num="0100">For example, the first face detector <b>706</b> can be configured to detect a face inclination in the range of &#xb1;45 degrees. The second face detector <b>903</b> can be configured to detect a face inclination in the range of &#xb1;15 degrees. In this case, a loop including the processing by the affine transformation processing unit <b>703</b> and the processing by the first face detector <b>706</b> is responsive to respective rotations of 0 degrees, 90 degrees, 180 degrees, and 270 degrees.</p>
<p id="p-0102" num="0101">The segmenting affine transformation processing unit <b>901</b> performs processing corresponding to respective rotations of 0 degrees, 30 degrees, 60 degrees, 90 degrees, 120 degrees, 150 degrees, 180 degrees, 210 degrees, 240 degrees, 270 degrees, 300 degrees, and 330 degrees. Thus, the segmenting affine transformation processing unit <b>901</b> can accurately perform face detection processing.</p>
<p id="p-0103" num="0102">When no reduction is taken into consideration, in the first detection result information <b>408</b>, for example, if a face candidate is present at the angle of 90 degrees, the segmenting affine transformation processing unit <b>901</b> performs processing at three angles of 60 degrees, 90 degrees, and 120 degrees. Similar processing is performed for other angles. A similar method is applicable to the reduction rate.</p>
<p id="p-0104" num="0103"><figref idref="DRAWINGS">FIG. 10</figref> is a flowchart illustrating an example detailed procedure of processing performed in step S<b>304</b> illustrated in <figref idref="DRAWINGS">FIG. 3</figref>. The flowchart of <figref idref="DRAWINGS">FIG. 10</figref> includes the processing procedures described in <figref idref="DRAWINGS">FIGS. 4</figref>, <b>7</b>, and <b>9</b>.</p>
<p id="p-0105" num="0104">In step S<b>1001</b>, the flesh-color extraction processing unit <b>701</b> of the first identifier <b>402</b> illustrated in <figref idref="DRAWINGS">FIG. 7</figref> performs flesh-color pixel extraction processing on the image data <b>401</b>. More specifically, as described above, the flesh-color extraction processing unit <b>701</b> writes &#x201c;1&#x201d; to a position of the image data <b>401</b> corresponding to a flesh-color pixel and &#x201c;0&#x201d; to a position corresponding to a non-flesh-color pixel. Then, the flesh-color extraction processing unit <b>701</b> generates the flesh-color mask <b>702</b>. For example, the RAM <b>103</b> stores the generated flesh-color mask <b>702</b>.</p>
<p id="p-0106" num="0105">In step S<b>1002</b>, the affine transformation processing unit <b>703</b> of the first identifier <b>402</b> performs affine transformation processing on the image data <b>401</b> to rotate and reduce the image. For example, the RAM <b>103</b> stores a result of the affine transformation processing, as the affine transformed image data <b>704</b>.</p>
<p id="p-0107" num="0106">In step S<b>1003</b>, the affine transformation processing unit <b>703</b> performs affine transformation processing on the flesh-color mask <b>702</b> to rotate and reduce the mask. For example, the RAM <b>103</b> stores a result of the affine transformation processing, as the affine transformed flesh-color mask <b>705</b>.</p>
<p id="p-0108" num="0107">In step S<b>1004</b>, the first identifier <b>402</b> determines whether the certainty at which a face is present in each region of the corresponding affine transformed image data <b>704</b> is obtained for all candidate points on the affine transformed flesh-color mask <b>705</b>.</p>
<p id="p-0109" num="0108">If the first identifier <b>402</b> determines that the processing for obtaining a certainty at which a face is present in each region of the corresponding affine transformed image data <b>704</b> has not been accomplished for all candidate points on the affine transformed flesh-color mask <b>705</b> (NO in step S<b>1004</b>), the processing proceeds to step S<b>1005</b>.</p>
<p id="p-0110" num="0109">In step S<b>1005</b>, the first face detector <b>706</b> of the first identifier <b>402</b> performs first face detection processing for detecting a face in a region of the affine transformed image data <b>704</b> corresponding to an unprocessed candidate point on the affine transformed flesh-color mask <b>705</b>.</p>
<p id="p-0111" num="0110">More specifically, the first face detector <b>706</b> obtains a certainty at which a face is present in a region of the affine transformed image data <b>704</b>. Then, the first face detector <b>706</b> stores the obtained certainty in a corresponding portion of the first certainty information <b>403</b>. For example, the RAM <b>103</b> stores the first certainty information <b>403</b>.</p>
<p id="p-0112" num="0111">For example, when the first face detector <b>706</b> uses a neural network to perform the processing of step S<b>1005</b>, the loop processing of steps S<b>1004</b> and S<b>1005</b> can be integrally realized by packaging the neural network. This is because the neural network can improve the calculation efficiency by using a common calculation result for candidate points positioned closely.</p>
<p id="p-0113" num="0112">If the first identifier <b>402</b> determines that the processing for obtaining a certainty at which a face is present in each region of the corresponding affine transformed image data <b>704</b> has been accomplished for all candidate points on the affine transformed flesh-color mask <b>705</b> (YES in step S<b>1004</b>), the processing proceeds to step S<b>1006</b>. In step S<b>1006</b>, the first identifier <b>402</b> determines whether the processing of steps S<b>1002</b> to S<b>1005</b> has been completed for all parameters representing rotational angles and reduction rates. If the first identifier <b>402</b> determines that the processing of steps S<b>1002</b> to S<b>1005</b> has not been completed for all rotation and reduction parameters (NO in step S<b>1006</b>), the processing returns to step S<b>1002</b>.</p>
<p id="p-0114" num="0113">If the first identifier <b>402</b> determines that the processing of steps S<b>1002</b> to S<b>1005</b> has been completed for all rotation and reduction parameters (YES in step S<b>1006</b>), the processing proceeds to step S<b>1007</b>.</p>
<p id="p-0115" num="0114">In step S<b>1007</b>, the distribution processing unit <b>404</b> illustrated in <figref idref="DRAWINGS">FIG. 4</figref> counts the content of the first certainty information <b>403</b> to determine a threshold used in the next step S<b>1008</b>. More specifically, the distribution processing unit <b>404</b> performs processing for distributing each certainty of the first certainty information <b>403</b> to one of the bins illustrated in <figref idref="DRAWINGS">FIG. 6</figref> according to its value, as illustrated in <figref idref="DRAWINGS">FIG. 6</figref>. For example, the RAM <b>103</b> stores a result of the distribution processing performed by the distribution processing unit <b>404</b>, as the counting result information <b>405</b> of the first certainty information.</p>
<p id="p-0116" num="0115">In step S<b>1008</b>, the threshold determination processing unit <b>406</b> determines a threshold to be applied to the first certainty information <b>403</b> based on the counting result information <b>405</b> of the first certainty information. The determined threshold is usable for the binarization processing applied to the first certainty information <b>403</b>. An example determination method is the above-described method described with reference to the flowchart of <figref idref="DRAWINGS">FIG. 8</figref>.</p>
<p id="p-0117" num="0116">In step S<b>1009</b>, the binarization processing unit <b>407</b> converts (separates) the first certainty information <b>403</b> into binary data (&#x201c;0&#x201d; or &#x201c;1&#x201d;) using the threshold determined in step S<b>1008</b>. For example, the RAM <b>103</b> stores the obtained binary data as the first detection result information <b>408</b>. Thus, the binarization processing unit <b>407</b> can generate (select) a plurality sets of face candidate regions of the image data <b>401</b> associated with rotational angle and size data. The processing of steps S<b>1010</b> to S<b>1014</b> is executed based on the selection result.</p>
<p id="p-0118" num="0117">In step S<b>1010</b>, the second identifier <b>409</b> determines whether the processing of steps S<b>1011</b> to S<b>1013</b> has been completed for all candidate points of the first detection result information <b>408</b>.</p>
<p id="p-0119" num="0118">If the second identifier <b>409</b> determines the processing of steps S<b>1011</b> to S<b>1013</b> has not been completed for all candidate points of the first detection result information <b>408</b> (NO in step S<b>1010</b>), the processing proceeds to step S<b>1011</b>.</p>
<p id="p-0120" num="0119">In step S<b>1011</b>, the segmenting affine transformation processing unit <b>901</b> of the second identifier <b>409</b> illustrated in <figref idref="DRAWINGS">FIG. 9</figref> performs segmenting affine transformation processing on the image data <b>401</b> according to the content of the first detection result information <b>408</b>. More specifically, the segmenting affine transformation processing unit <b>901</b> performs affine transformation processing on each segmented portion of the image data <b>401</b> according to the rotational angle and the reduction rate corresponding to the candidate region of the first detection result information <b>408</b>. For example, the RAM <b>103</b> stores a result of the segmenting affine transformation processing performed by the segmenting affine transformation processing unit <b>901</b>, as the segmenting affine transformed image data <b>902</b>.</p>
<p id="p-0121" num="0120">In step S<b>1012</b>, the second face detector <b>903</b> of the second identifier <b>409</b> illustrated in <figref idref="DRAWINGS">FIG. 9</figref> performs second face detection processing for detecting a face in the segmenting affine transformed image data <b>902</b>. More specifically, the second face detector <b>903</b> obtains a certainty at which a face is present in the segmenting affine transformed image data <b>902</b>. Then, the second face detector <b>903</b> stores the obtained certainty in a corresponding portion of the second certainty information <b>410</b>. For example, the RAM <b>103</b> stores the second certainty information <b>410</b>.</p>
<p id="p-0122" num="0121">In step S<b>1013</b>, the second identifier <b>409</b> determines whether rotation and reduction processing has been completed for a target candidate point of the first detection result information <b>408</b>. If the second identifier <b>409</b> determines that the rotation and reduction processing for the target candidate point of the first detection result information <b>408</b> has not been completed (NO in step S<b>1013</b>), the processing returns to step S<b>1011</b>. The second identifier <b>409</b> repeats the loop processing of steps S<b>1011</b> to S<b>1013</b>.</p>
<p id="p-0123" num="0122">If the second identifier <b>409</b> determines that the rotation and reduction processing for the target candidate point of the first detection result information <b>408</b> has been completed (YES in step S<b>1013</b>), the processing returns to step S<b>1010</b>. If the second identifier <b>409</b> determines that the processing of steps S<b>1011</b> to S<b>1013</b> has been completed for all candidate points of the first detection result information <b>408</b> (YES in step S<b>1010</b>), the processing proceeds to step S<b>1014</b>.</p>
<p id="p-0124" num="0123">In step S<b>1014</b>, the threshold determination processing unit <b>411</b> selects a face candidate portion from the second certainty information <b>410</b>. For example, the RAM <b>103</b> stores the selected portion as the face detection result information <b>412</b>.</p>
<p id="p-0125" num="0124">Through the processing of steps S<b>1001</b> to S<b>1014</b>, the face detection unit <b>202</b> accomplishes the face detection processing of step S<b>304</b> illustrated in <figref idref="DRAWINGS">FIG. 3</figref>. As described above, when a plurality of detectors (identifiers) cooperatively constitute the first identifier <b>402</b>, it is desirable to execute the loop processing effectively. For example, if the processing of both the first identifier <b>402</b> and the second identifier <b>409</b> is included in the same loop and the threshold determination processing unit <b>406</b> executes the processing for each loop, the second identifier <b>409</b> may perform the processing unnecessarily.</p>
<p id="p-0126" num="0125">For example, in the face detection processing, two or more faces existing in the image data <b>401</b> tend to have similar inclinations. If many faces have inclinations equal or close to 0 degree, very few faces have inclinations equal or close to 180 degrees. Nevertheless, if the threshold determination processing unit <b>406</b> executes the processing in each loop, the threshold determination processing unit <b>406</b> determines a threshold as described above (see step S<b>804</b>) even when there is no face whose inclination is 180 degrees. In other words, the threshold may be set to a lower value unnecessarily. In this case, the second identifier <b>409</b> executes the processing unnecessarily for the region where there is no face having an inclination of 180 degrees.</p>
<p id="p-0127" num="0126">In view of the foregoing problems, in an exemplary embodiment, the first face detector <b>706</b> repetitively executes the first face detection processing (step S<b>1005</b>) for all candidates. After the first face detector <b>706</b> has completed the detection processing, the threshold determination processing unit <b>406</b> executes the threshold determination processing (step S<b>1008</b>). Subsequently, the second face detector <b>903</b> executes the second face detection processing (step S<b>1012</b>) based on the first detection result information <b>408</b> obtained using the threshold.</p>
<p id="p-0128" num="0127">As described above, an exemplary embodiment provides the threshold determination processing unit <b>406</b> configured to determine a threshold between the first identifier <b>402</b> and the second identifier <b>409</b>. Thus, the threshold determination processing unit <b>406</b> can automatically adjust a threshold in the threshold processing.</p>
<p id="p-0129" num="0128">An exemplary embodiment dynamically selects a high-level output value (threshold) instead of using a fixed threshold. Furthermore, an exemplary embodiment can perform threshold processing in an appropriate range obtained by effectively executing the loop processing, and can satisfy requirements in processing speed and identification accuracy.</p>
<p id="p-0130" num="0129">More specifically, an exemplary embodiment extracts a plurality of relatively high-level values among all output values obtained by the first identifier <b>402</b>. The second identifier <b>409</b> performs the processing on candidate points corresponding to the extracted output values. To select a plurality of high-level output values, an exemplary embodiment checks a distribution of output values and determines a threshold with reference to the distribution. Therefore, the exemplary embodiment does not perform the processing for sorting output values, which takes a significantly long time to complete.</p>
<heading id="h-0006" level="1">Second Exemplary Embodiment</heading>
<p id="p-0131" num="0130">The present invention is applied to a tone signal detection apparatus as a pattern recognition apparatus according to an exemplary embodiment of the present invention, which is configured to input a Pulse Code Modulation (PCM) signal and recognize a pattern of a tone signal.</p>
<p id="p-0132" num="0131">A pattern recognition apparatus (tone signal detection apparatus) according to the second exemplary embodiment is connected to a public telephone line and configured to detect if a user presses a push-button on a telephone. In this exemplary embodiment, the user's telephone can transmit tone signals regulated according to ITU-T Recommendation Q.24. The following table 1 illustrates example tone signals.</p>
<p id="p-0133" num="0132">
<tables id="TABLE-US-00001" num="00001">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="84pt" align="left"/>
<colspec colname="1" colwidth="133pt" align="center"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="1" rowsep="1">TABLE 1</entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
<row>
<entry/>
<entry>HIGHER GROUP/Hz</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="5">
<colspec colname="1" colwidth="84pt" align="center"/>
<colspec colname="2" colwidth="21pt" align="center"/>
<colspec colname="3" colwidth="42pt" align="center"/>
<colspec colname="4" colwidth="21pt" align="center"/>
<colspec colname="5" colwidth="49pt" align="center"/>
<tbody valign="top">
<row>
<entry>LOWER GROUP/Hz</entry>
<entry>1209</entry>
<entry>1336</entry>
<entry>1477</entry>
<entry>1633</entry>
</row>
<row>
<entry namest="1" nameend="5" align="center" rowsep="1"/>
</row>
<row>
<entry>697</entry>
<entry>&#x201c;1&#x201d;</entry>
<entry>&#x201c;2&#x201d;</entry>
<entry>&#x201c;3&#x201d;</entry>
<entry>&#x201c;A&#x201d;</entry>
</row>
<row>
<entry>770</entry>
<entry>&#x201c;4&#x201d;</entry>
<entry>&#x201c;5&#x201d;</entry>
<entry>&#x201c;6&#x201d;</entry>
<entry>&#x201c;B&#x201d;</entry>
</row>
<row>
<entry>582</entry>
<entry>&#x201c;7&#x201d;</entry>
<entry>&#x201c;8&#x201d;</entry>
<entry>&#x201c;9&#x201d;</entry>
<entry>&#x201c;C&#x201d;</entry>
</row>
<row>
<entry>941</entry>
<entry>&#x201c;*&#x201d;</entry>
<entry>&#x201c;0&#x201d;</entry>
<entry>&#x201c;#&#x201d;</entry>
<entry>&#x201c;D&#x201d;</entry>
</row>
<row>
<entry namest="1" nameend="5" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0134" num="0133">Table 1 illustrates two classifications (first classification and second classification) of a lower-group frequency region and a higher-group frequency region. As illustrated in table 1, if a user presses a push-button &#x201c;1&#x201d;, two signals of 697 Hz and 1209 Hz are transmitted simultaneously. If a user presses a push-button &#x201c;2&#x201d;, two signals of 697 Hz and 1336 Hz are transmitted simultaneously. Similarly, if a user presses another push-button, two signals belonging to lower and higher groups are transmitted simultaneously according to the table 1.</p>
<p id="p-0135" num="0134"><figref idref="DRAWINGS">FIG. 11</figref> is a block diagram illustrating an example hardware configuration of a pattern recognition apparatus <b>1100</b> according to the second exemplary embodiment of the present invention. As illustrated in <figref idref="DRAWINGS">FIG. 11</figref>, the pattern recognition apparatus <b>1100</b> includes a central processing unit (CPU) <b>1101</b>, a program memory <b>1102</b>, a random access memory (RAM) <b>1103</b>, a telephone terminal <b>1104</b>, an analog-digital (A/D) converter <b>1105</b>, a display device <b>1106</b>, and a control bus/data bus <b>1110</b>.</p>
<p id="p-0136" num="0135">The CPU <b>1101</b> integrally controls various operations performed by the pattern recognition apparatus <b>1100</b>. For example, the CPU <b>1101</b> executes a program to realize a tone signal detection method according to an exemplary embodiment. The CPU <b>1101</b> can execute program(s) stored in the program memory <b>1102</b>. The RAM <b>1103</b> temporarily stores various information and various data while the CPU <b>1101</b> executes the program(s).</p>
<p id="p-0137" num="0136">The telephone terminal <b>1104</b> serves as an interface that connects the pattern recognition apparatus <b>1100</b> to a public telephone line. The A/D converter <b>1105</b> converts an analog signal entered from the telephone terminal <b>1104</b> into a digital signal. The display device <b>1106</b> presents, to a user, a detection result of the tone signal detection processing performed by the pattern recognition apparatus <b>1100</b>, according to a command from the CPU <b>1101</b>.</p>
<p id="p-0138" num="0137">The control bus/data bus <b>1110</b> connects the CPU <b>1101</b> to the above-described devices <b>1101</b> to <b>1106</b>.</p>
<p id="p-0139" num="0138"><figref idref="DRAWINGS">FIG. 12</figref> illustrates an example display screen of the display device <b>1106</b> illustrated in <figref idref="DRAWINGS">FIG. 11</figref>. A display screen <b>1201</b> of the display device <b>1106</b> includes sixteen push-buttons <b>1202</b> of a user's telephone, which are numerical buttons &#x201c;0&#x201d; to &#x201c;9&#x201d;, alphabetical buttons &#x201c;A&#x201d; to &#x201c;D&#x201d;, and symbol buttons &#x201c;*&#x201d; and &#x201c;#&#x201d; disposed in a matrix pattern. An ordinary telephone does not include the alphabetical buttons &#x201c;A&#x201d; to &#x201c;D.&#x201d; A circle <b>1203</b> momentarily appears when a user presses a button. According to the example illustrated in <figref idref="DRAWINGS">FIG. 12</figref>, the push-button pressed by a user is a numerical button &#x201c;1.&#x201d; If a user presses a symbol button &#x201c;#&#x201d;, a new circle <b>1203</b> appears around the symbol &#x201c;#&#x201d; while the circle disappears from the number &#x201c;1.&#x201d; If a user does not press any button, the circle <b>1203</b> does not appear on the display screen <b>1201</b>.</p>
<p id="p-0140" num="0139"><figref idref="DRAWINGS">FIG. 13</figref> is a block diagram illustrating an example functional configuration of the pattern recognition apparatus according to the second exemplary embodiment of the present invention. The CPU <b>1101</b> executes a program stored in the program memory <b>1102</b> to realize functional blocks <b>1301</b>, <b>1303</b>, <b>1305</b>, and <b>1307</b> illustrated in <figref idref="DRAWINGS">FIG. 13</figref>.</p>
<p id="p-0141" num="0140">The A/D converter <b>1105</b> illustrated in <figref idref="DRAWINGS">FIG. 13</figref> is similar to the A/D converter <b>1105</b> illustrated in <figref idref="DRAWINGS">FIG. 11</figref>. The A/D converter <b>1105</b> is an external input unit for the CPU <b>1101</b>. The discrete Fourier transformation processing unit <b>1301</b> acquires, as input information, a signal entered via a public telephone line and converted into a digital format by the A/D converter <b>1105</b>. More specifically, the input information acquired in this case is a PCM signal, which can be obtained by sampling a speech signal at predetermined time intervals. The discrete Fourier transformation processing unit <b>1301</b> converts a PCM signal of past T msec obtained by the A/D converter <b>1105</b> into a frequency signal at intervals of P msec. For example, the RAM <b>1103</b> stores the converted signal as Fourier transformation result information <b>1302</b>. As described below, <figref idref="DRAWINGS">FIG. 14</figref> illustrates an example format for the Fourier transformation result information <b>1302</b>.</p>
<p id="p-0142" num="0141">The high-level frequency selection processing unit <b>1303</b> selects a plurality of frequencies having higher energy in the Fourier transformation result information <b>1302</b>. As described below, <figref idref="DRAWINGS">FIG. 15</figref> illustrates detailed processing performed by the high-level frequency selection processing unit <b>1303</b>. A high-level frequency list <b>1304</b> includes frequency information selected by the high-level frequency selection processing unit <b>1303</b>. For example, the RAM <b>1103</b> stores the high-level frequency list <b>1304</b>.</p>
<p id="p-0143" num="0142">The comparison calculation processing unit <b>1305</b> performs comparison calculation processing on the Fourier transformation result information <b>1302</b> and the pattern including frequencies in the high-level frequency list <b>1304</b>. For example, the RAM <b>1103</b> stores the output of comparison calculation processing unit <b>1305</b> (i.e., information of a likely pattern), as detection result information <b>1306</b>. As described below, <figref idref="DRAWINGS">FIG. 16</figref> illustrates detailed processing performed by the comparison calculation processing unit <b>1305</b>.</p>
<p id="p-0144" num="0143">The detection result display processing unit <b>1307</b> causes the display device <b>1106</b> to display the content of a detected signal as illustrated in <figref idref="DRAWINGS">FIG. 12</figref>, based on the detection result information <b>1306</b>. The display device <b>1106</b> illustrated in <figref idref="DRAWINGS">FIG. 13</figref> is similar to the display device <b>1106</b> illustrated in <figref idref="DRAWINGS">FIG. 11</figref>.</p>
<p id="p-0145" num="0144"><figref idref="DRAWINGS">FIG. 14</figref> illustrates an example format for the Fourier transformation result information <b>1302</b> illustrated in <figref idref="DRAWINGS">FIG. 13</figref>. A table <b>1401</b> illustrated in <figref idref="DRAWINGS">FIG. 14</figref> represents two-dimensional data stored in the Fourier transformation result information <b>1302</b>. A curve <b>1402</b> illustrated in <figref idref="DRAWINGS">FIG. 14</figref> represents the Fourier transformation result information <b>1302</b> generated by the discrete Fourier transformation processing unit <b>1301</b>. The magnitude of the curve <b>1402</b> indicates Fourier coefficient values corresponding to respective frequencies. In the second exemplary embodiment, the magnitude of the Fourier coefficient is used as a certainty indicating reception of a tone signal including the corresponding frequency.</p>
<p id="p-0146" num="0145">More specifically, the discrete Fourier transformation processing unit <b>1301</b> obtains a certainty (magnitude of the Fourier coefficient) that a frequency region converted from a PCM signal includes tone signals of respective classifications. Furthermore, as illustrated in <figref idref="DRAWINGS">FIG. 14</figref>, the discrete Fourier transformation processing unit <b>1301</b> generates a certainty distribution. The high-level frequency selection processing unit <b>1303</b> determines a threshold based on the certainty distribution and selects one or more frequency regions having a relatively high-level certainty among the certainties obtained by the discrete Fourier transformation processing unit <b>1301</b>. Furthermore, the comparison calculation processing unit <b>1305</b> performs detection processing on a tone signal.</p>
<p id="p-0147" num="0146">The Fourier transformation result information <b>1302</b> is information relating to a result of the discrete Fourier transformation. The number of frequencies is finite. Each dotted line <b>1403</b> in <figref idref="DRAWINGS">FIG. 14</figref> indicates a mutual relationship between a frequency component and each element of the two-dimensional data <b>1401</b>. More specifically, the two-dimensional data <b>1401</b> correspond to the frequency in its horizontal direction and correspond to the time (Fourier transformation timing) in its vertical direction. For example, the Fourier coefficient illustrated in <figref idref="DRAWINGS">FIG. 14</figref> is 0.0 at the maximum frequency. The element of the two-dimensional data <b>1401</b> corresponding to the maximum frequency is 0.0. The Fourier coefficient illustrated in <figref idref="DRAWINGS">FIG. 14</figref> is 0.92 at the minimum frequency. The element of the two-dimensional data <b>1401</b> corresponding to the minimum frequency is 0.92.</p>
<p id="p-0148" num="0147">The Fourier transformation result information <b>1302</b> is the two-dimensional data <b>1401</b> illustrated in <figref idref="DRAWINGS">FIG. 14</figref>, which are historical data (results) of the past Fourier transformation. More specifically, a row of time &#x201c;t&#x201d; indicates a result of the latest Fourier transformation. A row of time &#x201c;t&#x2212;P&#x201d; indicates a result of the second latest Fourier transformation. A row of time &#x201c;t&#x2212;2P&#x201d; indicates a result of the third latest Fourier transformation. To save the memory (e.g., the RAM <b>1103</b>), the row indicating the oldest result of the Fourier transformation disappears every time a new row is added to the table (two-dimensional information) <b>1401</b>.</p>
<p id="p-0149" num="0148"><figref idref="DRAWINGS">FIG. 15</figref> is a flowchart illustrating example detailed procedure of processing performed by the high-level frequency selection processing unit <b>1303</b> illustrated in <figref idref="DRAWINGS">FIG. 13</figref>. When &#x201c;I&#x201d; represents the two-dimensional information <b>1401</b> illustrated in <figref idref="DRAWINGS">FIG. 14</figref>, I[t] (freq) indicates the magnitude of the Fourier coefficient corresponding to a frequency &#x201c;freq&#x201d; at the time t. I[t] (freq) is equal to or greater than 0.0. In the flowchart illustrated in <figref idref="DRAWINGS">FIG. 15</figref>, S[freq] represents I[t](freq)&#x2212;I[t&#x2212;kP](freq) and the time interval &#x201c;kP&#x201d; is an arbitrary value. As an exception, S[<b>0</b>] is regarded as &#x2212;2.0 (S[<b>0</b>]=&#x2212;2.0). The absolute value of S[<b>0</b>] is greater than any element of I.</p>
<p id="p-0150" num="0149">In step S<b>1501</b>, the high-level frequency selection processing unit <b>1303</b> initializes variables freq<b>0</b> and freq<b>1</b>, each representing the frequency, to 0. In step S<b>1502</b>, the high-level frequency selection processing unit <b>1303</b> initializes the subscript &#x201c;freq&#x201d; to the maximum frequency to start scanning the two-dimensional information I.</p>
<p id="p-0151" num="0150">In step S<b>1503</b>, the high-level frequency selection processing unit <b>1303</b> compares S[freq] with S[freq<b>0</b>] and determines whether S[freq] is greater than S[freq<b>0</b>]. If the high-level frequency selection processing unit <b>1303</b> determines that S[freq] is not greater than S[freq<b>0</b>], i.e., S[freq]&#x2266;S[freq<b>0</b>] (NO in step S<b>1503</b>), the processing proceeds to step S<b>1504</b>. In step S<b>1504</b>, the high-level frequency selection processing unit <b>1303</b> compares S[freq] with S[freq<b>1</b>] and determines whether S[freq] is greater than S[freq<b>1</b>].</p>
<p id="p-0152" num="0151">If the high-level frequency selection processing unit <b>1303</b> determines that S[freq] is not greater than S[freq<b>1</b>], i.e., S[freq]&#x2266;S[freq<b>1</b>] (NO in step S<b>1504</b>), the processing proceeds to step S<b>1505</b>. In step S<b>1505</b>, the high-level frequency selection processing unit <b>1303</b> determines whether the &#x201c;freq&#x201d; is the minimum frequency.</p>
<p id="p-0153" num="0152">If the high-level frequency selection processing unit <b>1303</b> determines that the &#x201c;freq&#x201d; is not the minimum frequency (NO in step S<b>1505</b>), the processing proceeds to step S<b>1506</b>. In step S<b>1506</b>, the high-level frequency selection processing unit <b>1303</b> changes the subscript &#x201c;freq&#x201d; to indicate the next largest frequency. Then, the processing returns to step S<b>1503</b>. If the high-level frequency selection processing unit <b>1303</b> determines that S[freq] is greater than S[freq<b>0</b>] (YES in step S<b>1503</b>), the processing proceeds to step S<b>1507</b>. In step S<b>1507</b>, the high-level frequency selection processing unit <b>1303</b> compares S[freq<b>0</b>] with [freq<b>1</b>] and determines whether S[freq<b>0</b>] is greater than S[freq<b>1</b>].</p>
<p id="p-0154" num="0153">If the high-level frequency selection processing unit <b>1303</b> determines that S[freq<b>0</b>] is greater than [freq<b>1</b>] (YES in step S<b>1507</b>), or if the high-level frequency selection processing unit <b>1303</b> determines that S[freq] is greater than S[freq<b>1</b>] (YES in step S<b>1504</b>), the processing proceeds to step S<b>1508</b>. In step S<b>1508</b>, the high-level frequency selection processing unit <b>1303</b> replaces the value of &#x201c;freq<b>1</b>&#x201d; with the value of &#x201c;freq.&#x201d;</p>
<p id="p-0155" num="0154">If the high-level frequency selection processing unit <b>1303</b> determines that S[freq<b>0</b>] is not greater than [freq<b>1</b>], i.e., S[freq<b>0</b>]&#x2266;S[freq<b>1</b>] (NO in step S<b>1507</b>), the processing proceeds to step S<b>1509</b>. In S<b>1509</b>, the high-level frequency selection processing unit <b>1303</b> replaces the value of &#x201c;freq<b>0</b>&#x201d; with the value of &#x201c;freq.&#x201d;</p>
<p id="p-0156" num="0155">After completing the processing of step S<b>1508</b> or step S<b>1509</b>, the processing proceeds to step S<b>1505</b>. If the high-level frequency selection processing unit <b>1303</b> determines that the &#x201c;freq&#x201d; is the minimum frequency (YES in step S<b>1505</b>), the processing proceeds to step S<b>1510</b>. In step S<b>1510</b>, the high-level frequency selection processing unit <b>1303</b> selects &#x201c;freq<b>0</b>&#x201d; and &#x201c;freq<b>1</b>&#x201d; as high-level frequencies. For example, the RAM <b>1103</b> stores the selected frequencies in the high-level frequency list <b>1304</b> illustrated in <figref idref="DRAWINGS">FIG. 13</figref>.</p>
<p id="p-0157" num="0156">Through the above-described processing of steps S<b>1501</b> to S<b>1510</b>, the high-level frequency selection processing unit <b>1303</b> can select two frequencies, which are larger in the increment of output compared to the past data of the two-dimensional information I obtained kP msec before.</p>
<p id="p-0158" num="0157"><figref idref="DRAWINGS">FIG. 16</figref> is a flowchart illustrating an example detailed procedure of processing performed by the comparison calculation processing unit <b>1305</b> illustrated in <figref idref="DRAWINGS">FIG. 13</figref>.</p>
<p id="p-0159" num="0158">In step S<b>1601</b>, the comparison calculation processing unit <b>1305</b> initializes an array P[f] to 0, i.e., P[f]=0, for all &#x201c;f.&#x201d; The subscript of the array P[f] corresponds to each frequency in the above-described discrete Fourier transformation.</p>
<p id="p-0160" num="0159">In step S<b>1602</b>, the comparison calculation processing unit <b>1305</b> inputs, into variable f, one of the frequencies (two frequencies according to the present exemplary embodiment) selected by the high-level frequency selection processing unit <b>1303</b> illustrated in <figref idref="DRAWINGS">FIG. 13</figref>. Then, the comparison calculation processing unit <b>1305</b> repeats the loop processing of steps S<b>1602</b> to S<b>1615</b> for all frequencies selected by the high-level frequency selection processing unit <b>1303</b> illustrated in <figref idref="DRAWINGS">FIG. 13</figref>.</p>
<p id="p-0161" num="0160">In step S<b>1603</b>, the comparison calculation processing unit <b>1305</b> compares S[f] with a constant C and determines whether S[f] is greater than the constant C. If the comparison calculation processing unit <b>1305</b> determines that S[f] is greater than the constant C (YES in step S<b>1603</b>), the processing proceeds to step S<b>1604</b>. In step S<b>1604</b>, the comparison calculation processing unit <b>1305</b> determines whether the frequency f is a higher-group frequency.</p>
<p id="p-0162" num="0161">If the comparison calculation processing unit <b>1305</b> determines that the frequency f is not a higher-group frequency, i.e., when the frequency f is a lower-group frequency (NO in step S<b>1604</b>), the processing proceeds to step S<b>1605</b>. In step S<b>1605</b>, the comparison calculation processing unit <b>1305</b> searches a row of the table 1 that accords with the (lower-group) frequency f. Subsequently, the comparison calculation processing unit <b>1305</b> obtains S[f]+S[g] for each higher-group frequency g in the loop processing of steps S<b>1606</b> to S<b>1608</b>.</p>
<p id="p-0163" num="0162">If the comparison calculation processing unit <b>1305</b> determines that the frequency f is a higher-group frequency (YES in step S<b>1604</b>), the processing proceeds to step S<b>1609</b>. In step S<b>1609</b>, the comparison calculation processing unit <b>1305</b> searches a column of the table 1 that accords with the (higher-group) frequency f. Subsequently, the comparison calculation processing unit <b>1305</b> obtains S[f]+S[g] for each lower-group frequency g in the loop processing of steps S<b>1610</b> to S<b>1612</b>.</p>
<p id="p-0164" num="0163">After completing the processing of step S<b>1608</b> or step S<b>1612</b>, the processing proceeds to step S<b>1613</b>. In step S<b>1613</b>, the comparison calculation processing unit <b>1305</b> obtains a combination of S[f]+S[g] having a maximum value, as P[f], among the combinations obtained through the processing of steps S<b>1606</b> to S<b>1608</b> or steps S<b>1610</b> to S<b>1612</b>. For example, the RAM <b>1103</b> stores the obtained P[f].</p>
<p id="p-0165" num="0164">If the comparison calculation processing unit <b>1305</b> determines that S[f] is not greater than the constant C, i.e., when S[f] is equal to or greater than the constant C (NO in step S<b>1603</b>), the processing proceeds to step S<b>1614</b>. In step S<b>1614</b>, the comparison calculation processing unit <b>1305</b> sets P[f] to 0, which indicates no detection of the set frequency f. For example, P[f] is set to 0 when a user of a transmission terminal does not press any button. The RAM <b>1103</b> stores the value of P[f].</p>
<p id="p-0166" num="0165">After completing the loop processing of steps S<b>1602</b> to S<b>1615</b>, the processing proceeds to step S<b>1616</b>. In step S<b>1616</b>, the comparison calculation processing unit <b>1305</b> determines whether both of S[f] and S[g] of the stored P[f] (i.e., the combination of S[f]+S[g] having the maximum value) are greater than the constant C. Namely, in step S<b>1616</b>, the comparison calculation processing unit <b>1305</b> determines whether a smaller one of S[f] and S[g] is greater than the constant C. The combination of S[f]+S[g] having the maximum value can be obtained by scanning P[f].</p>
<p id="p-0167" num="0166">If the comparison calculation processing unit <b>1305</b> determines that both of S[f] and S[g] are greater than the constant C (YES in step S<b>1616</b>), the processing proceeds to step S<b>1617</b>. In step S<b>1617</b>, the comparison calculation processing unit <b>1305</b> stores information relating to a button corresponding to the maximum value of S[f]+S[g], as the detection result information <b>1306</b> illustrated in <figref idref="DRAWINGS">FIG. 13</figref>, in the RAM <b>1103</b>. Then, based on the detection result information <b>1306</b>, the detection result display processing unit <b>1307</b> illustrated in <figref idref="DRAWINGS">FIG. 13</figref> causes the display device <b>1106</b> to display the circle <b>1203</b> illustrated in <figref idref="DRAWINGS">FIG. 12</figref>.</p>
<p id="p-0168" num="0167">If the comparison calculation processing unit <b>1305</b> determines that at least one of S[f] and S[g] is not greater (equal to or smaller) than the constant C (NO in step S<b>1616</b>), the processing proceeds to step S<b>1618</b>. In step S<b>1618</b>, the comparison calculation processing unit <b>1305</b> does not store any information as the detection result information <b>1306</b> illustrated in <figref idref="DRAWINGS">FIG. 13</figref>. For example, when a user does not press any button, the comparison calculation processing unit <b>1305</b> does not store any information or stores information indicating that no button is pressed.</p>
<p id="p-0169" num="0168">Through the processing of steps S<b>1601</b> to S<b>1618</b>, the comparison calculation processing unit <b>1305</b> can store a result of the comparison calculation processing performed on the Fourier transformation result information <b>1302</b>, as the detection result information <b>1306</b>.</p>
<p id="p-0170" num="0169">As described above, according to an exemplary embodiment, the high-level frequency selection processing unit <b>1303</b> performs high-level frequency selection processing. Therefore, the exemplary embodiment does not execute the determination processing (whether a button is pressed) for all combinations of the higher-group frequency and the lower-group frequency. Accordingly, the exemplary embodiment can detect a tone signal with a small amount of processing.</p>
<p id="p-0171" num="0170">The high-level frequency selection processing unit <b>1303</b> illustrated in <figref idref="DRAWINGS">FIG. 13</figref> selects a plurality of frequencies. Therefore, the exemplary embodiment can detect a tone signal even in an environment including noises, for example, when a user uses a portable phone outdoors.</p>
<p id="p-0172" num="0171">In an exemplary embodiment, to simplify the description, a user's telephone transmits a small number of tone signals regulated according to ITU-T Recommendation Q.24. In this case, an exemplary embodiment can enhance processing load reduction effects when transmitting many frequency signals.</p>
<p id="p-0173" num="0172">The above-described exemplary embodiments can easily select a threshold for the pattern recognition performed on input information and can satisfy requirements in processing speed and recognition accuracy. The CPU executes a program stored in the program memory <b>102</b> illustrated in <figref idref="DRAWINGS">FIG. 1</figref> or the program memory <b>1102</b> illustrated in <figref idref="DRAWINGS">FIG. 11</figref> to realize the example configurations illustrated in <figref idref="DRAWINGS">FIGS. 2</figref>, <b>4</b>, <b>7</b>, <b>9</b>, and <b>13</b> constituting the pattern recognition apparatus according to the above-described exemplary embodiments.</p>
<p id="p-0174" num="0173">Furthermore, the CPU executes a program stored in the program memory <b>102</b> illustrated in <figref idref="DRAWINGS">FIG. 1</figref> or the program memory <b>1102</b> illustrated in <figref idref="DRAWINGS">FIG. 11</figref> to realize the example steps illustrated in <figref idref="DRAWINGS">FIGS. 3</figref>, <b>8</b>, <b>10</b>, <b>15</b>, and <b>16</b> constituting the pattern recognition method for the pattern recognition apparatus according to the above-described exemplary embodiments. The present invention encompasses the program and a computer-readable recording medium storing the program.</p>
<p id="p-0175" num="0174">Furthermore, software program code for realizing the functions of the above-described exemplary embodiments is installable to a system or an apparatus including various devices. A computer (or CPU or micro-processing unit (MPU)) in the system or the apparatus can execute the program to operate the devices to realize the functions of the above-described exemplary embodiments. Accordingly, the present invention encompasses the program code installable on a computer when the computer can realize the functions or processes of the exemplary embodiments.</p>
<p id="p-0176" num="0175">In this case, the program code itself can realize the functions of the exemplary embodiments. The equivalents of programs are usable if they possess comparable functions. Furthermore, the present invention encompasses supplying program code to a computer with a storage (or recording) medium storing the program code. In this case, the type of program can be any one of object code, interpreter program, and OS script data. A storage medium supplying the program can be selected from any one of a floppy disk, a hard disk, an optical disk, a magneto-optical (MO) disk, a compact disk-ROM (CD-ROM), a CD-recordable (CD-R), a CD-rewritable (CD-RW), a magnetic tape, a nonvolatile memory card, a ROM, and a DVD (DVD-ROM, DVD-R).</p>
<p id="p-0177" num="0176">A computer network using a carrier wave carrying program information, such as a local area network (LAN), a wide area network (WAN) represented by the internet, or a wireless communication network is usable to transmit the program. The communication medium includes both a wired line (e.g., an optical fiber) and a wireless line.</p>
<p id="p-0178" num="0177">Moreover, an operating system (OS) or other application software running on a computer can execute part or all of actual processing based on instructions of the programs. Additionally, the program code read out of a storage medium can be written into a memory of a function expansion board equipped in a computer or into a memory of a function expansion unit connected to the computer. In this case, based on an instruction of the program, a CPU provided on the function expansion board or the function expansion unit can execute part or all of the processing to realize the functions of the above-described exemplary embodiments.</p>
<p id="p-0179" num="0178">While the present invention has been described with reference to exemplary embodiments, it is to be understood that the invention is not limited to the disclosed exemplary embodiments. The scope of the following claims is to be accorded the broadest interpretation so as to encompass all modifications, equivalent structures, and functions.</p>
<p id="p-0180" num="0179">This application claims priority from Japanese Patent Application No. 2007-259059 filed Oct. 2, 2007, which is hereby incorporated by reference herein in its entirety.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A pattern recognition method applicable to a single image file including a plurality of regions, the method comprising:
<claim-text>obtaining certainties in which each region of the single image file includes a respective one of a plural of human face patterns by a first face detector;</claim-text>
<claim-text>determining a certainty distribution of the certainties obtained by the first face detector;</claim-text>
<claim-text>determining dynamically a threshold certainty based on the determined certainty distribution;</claim-text>
<claim-text>selecting at least one combination of the region and the human face pattern, the selected combination corresponding to the certainties which are equal to or greater than the determined threshold certainty; and</claim-text>
<claim-text>detecting at least one human face image based on the selected combination by a second face detector.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising obtaining certainties by converting the single image file using n types of methods.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the single image file is image data, and
<claim-text>wherein the method further comprises obtaining certainties by designating a first image pattern as belonging to the first classification and an n-th image pattern as belonging to the n-th classification.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, further comprising obtaining certainties by designating a first inclination image pattern as the first image pattern and an n-th inclination image pattern as the n-th image pattern.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. A non-transitory computer-readable storage medium storing a program for causing a computer to perform the method as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. A pattern recognition method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the single image file is an image file.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. A pattern recognition method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first detector obtains a certainty of each classification in each region, and each classification is an affine transformation.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. A pattern recognition method according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein
<claim-text>each classification of which the certainty is obtained by the first detector, corresponds to an orientation of one of the patterns with a first order of precision;</claim-text>
<claim-text>the pattern detection processing by the second detector obtains the orientation of one of the patterns with a second order of precision; and</claim-text>
<claim-text>the second order of precision is more precise than the first order of precision.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. A pattern recognition method according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein
<claim-text>each classification of which the certainty is obtained by the first detector, corresponds to a parameter of one of the patterns with a first order of precision;</claim-text>
<claim-text>the pattern detection processing by the second detector obtains the parameter of one of the patterns with a second order of precision; and</claim-text>
<claim-text>the second order of precision is more precise than the first order of precision.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. A pattern recognition method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first detector obtains the certainties using a ratio of pixels having a predetermined color in the each region of the single image file.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. A pattern recognition method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the second face detector performs a detection which is different from the first face detector, at least with respect to accuracy.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. A non-transitory computer-readable storage medium that stores a program for causing a computer to execute the method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. A pattern recognition apparatus configured to identify a pattern in a single image file including a plurality of regions, the apparatus comprising:
<claim-text>a first detection unit configured to obtain certainties in which each region of the single image file includes a respective one of a plural of human face patterns of first to n-th, where n is a natural number equal to or greater than 2, classifications by a first detector;</claim-text>
<claim-text>a determination unit configured to determine a certainty distribution of the certainties obtained by the first detector;</claim-text>
<claim-text>a second determination unit configured to dynamically determine a threshold certainty based on the determined certainty distribution;</claim-text>
<claim-text>a selection unit configured to select at least one combination of the region and the human face pattern, the selected combination corresponding to the certainties which are equal to or greater than the determined threshold certainty; and</claim-text>
<claim-text>a second detection unit configured to detecting at least one human face image based on the one or more combinations selected by the selection unit.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. A method comprising:
<claim-text>obtaining certainties in which each region of an image file includes at least one pattern by a first detector;</claim-text>
<claim-text>determining a certainty distribution of the certainties obtained by the first detector;</claim-text>
<claim-text>determining dynamically a threshold certainty based on the determined certainty distribution;</claim-text>
<claim-text>selecting at least one region from the plurality of the regions, the selected region corresponding to the certainty which is equal to or greater than the determined threshold certainty; and</claim-text>
<claim-text>detecting at least one pattern based on the selected region by a second detector.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. A non-transitory computer-readable storage medium that stores a program for causing a computer to execute the method according to <claim-ref idref="CLM-00014">claim 14</claim-ref>.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. A method comprising:
<claim-text>obtaining certainties in which each region of an image file includes a respective one of patterns of first to n-th, where n is a natural number equal to or greater than 2, classifications by a first detector;</claim-text>
<claim-text>determining a certainty distribution of the certainties obtained by the first detector;</claim-text>
<claim-text>determining dynamically a threshold certainty based on the determined certainty distribution;</claim-text>
<claim-text>selecting at least one classification from the plurality of classifications, the selected classification corresponds to the certainty which is equal to or greater than the determined threshold certainty; and</claim-text>
<claim-text>detecting at least one pattern based on the selected classification by a second detector.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. A non-transitory computer-readable storage medium that stores a program for causing a computer to execute the method according to <claim-ref idref="CLM-00016">claim 16</claim-ref>.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. An apparatus comprising:
<claim-text>an obtaining unit configured to obtain certainties in which each region of an image file includes at least one pattern by a first detector;</claim-text>
<claim-text>a first determining unit configured to determine a certainty distribution of the certainties obtained by the first detector;</claim-text>
<claim-text>a second determining unit configured to determine dynamically a threshold certainty based on the determined certainty distribution;</claim-text>
<claim-text>a selecting unit configured to select at least one region from the plurality of the regions, the selected region corresponding to the certainty which is equal to or greater than the determined threshold certainty; and</claim-text>
<claim-text>a detecting unit configured to detect at least one pattern based on the selected region by a second detector.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. An apparatus comprising:
<claim-text>an obtaining unit configured to obtain certainties in which each region of an image file includes a respective one of patterns of first to n-th, where n is a natural number equal to or greater than 2, classifications by a first face detector;</claim-text>
<claim-text>a first determining unit configured to determine a certainty distribution of the certainties obtained by the first detector;</claim-text>
<claim-text>a second determining unit configured to determine dynamically a threshold certainty based on the determined certainty distribution;</claim-text>
<claim-text>a selecting unit configured to select at least one classification from the plurality of classifications, the selected classification corresponding to the certainty which is equal to or greater than the determined threshold certainty; and</claim-text>
<claim-text>a detecting unit configured to detect at least one pattern based on the selected classification by a second detector.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
