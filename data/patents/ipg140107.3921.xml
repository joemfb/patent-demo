<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08624989-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08624989</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12217021</doc-number>
<date>20080701</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<us-term-of-grant>
<us-term-extension>443</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>5</main-group>
<subgroup>228</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>5</main-group>
<subgroup>262</subgroup>
<symbol-position>L</symbol-position>
<classification-value>N</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>40</subgroup>
<symbol-position>L</symbol-position>
<classification-value>N</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>3482221</main-classification>
<further-classification>348239</further-classification>
<further-classification>382254</further-classification>
</classification-national>
<invention-title id="d2e53">System and method for remotely performing image processing operations with a network server device</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>6011547</doc-number>
<kind>A</kind>
<name>Shiota et al.</name>
<date>20000100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6628899</doc-number>
<kind>B1</kind>
<name>Kito</name>
<date>20030900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>396 56</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6937135</doc-number>
<kind>B2</kind>
<name>Kitson et al.</name>
<date>20050800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6977743</doc-number>
<kind>B2</kind>
<name>Carlton</name>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>358  115</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>7206019</doc-number>
<kind>B2</kind>
<name>Suga et al.</name>
<date>20070400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>34821111</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>7768532</doc-number>
<kind>B2</kind>
<name>Eto et al.</name>
<date>20100800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345629</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2001/0017656</doc-number>
<kind>A1</kind>
<name>Araki et al.</name>
<date>20010800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348211</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2002/0051074</doc-number>
<kind>A1</kind>
<name>Kawaoka et al.</name>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348376</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2002/0102966</doc-number>
<kind>A1</kind>
<name>Lev et al.</name>
<date>20020800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2003/0108252</doc-number>
<kind>A1</kind>
<name>Carrig</name>
<date>20030600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2004/0117427</doc-number>
<kind>A1</kind>
<name>Allen et al.</name>
<date>20040600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709200</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2004/0119851</doc-number>
<kind>A1</kind>
<name>Kaku</name>
<date>20040600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348239</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2004/0174434</doc-number>
<kind>A1</kind>
<name>Walker et al.</name>
<date>20040900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482113</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2004/0218834</doc-number>
<kind>A1</kind>
<name>Bishop et al.</name>
<date>20041100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2004/0263629</doc-number>
<kind>A1</kind>
<name>Ambiru et al.</name>
<date>20041200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482071</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2006/0136559</doc-number>
<kind>A1</kind>
<name>Morris</name>
<date>20060600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>2006/0174206</doc-number>
<kind>A1</kind>
<name>Jung et al.</name>
<date>20060800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715751</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>2008/0046502</doc-number>
<kind>A1</kind>
<name>Kamei et al.</name>
<date>20080200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709201</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>2009/0268060</doc-number>
<kind>A1</kind>
<name>Georgis et al.</name>
<date>20091000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482401</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>2011/0080487</doc-number>
<kind>A1</kind>
<name>Venkataraman et al.</name>
<date>20110400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482181</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>CN</country>
<doc-number>1409918</doc-number>
<kind>A</kind>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>JP</country>
<doc-number>2008-067316</doc-number>
<date>20080300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-cpc-text>H04N 7/173</classification-cpc-text>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>WO</country>
<doc-number>WO 01/45388</doc-number>
<kind>A2</kind>
<date>20010600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>WO</country>
<doc-number>WO03001435</doc-number>
<kind>A1</kind>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00025">
<othercit>Daniel Wagner, Dieter Schmalstieg, First Steps Towards Handheld Augmented Reality, Proceedings of the Seventh Ieee International Symposium on Wearable Computers, Oct. 21-23, 2003. Piscataway, NJ, USA, IEEE, pp. 127-135, XP010673786, ISBN: 978-0-7695-2034-6.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00026">
<othercit>Choong S. Boon, Onur G. Guleryuz, Toshiro Kawahara, and Yoshinori Suzuki, Sparse Super-Resolution Reconstructions of Video From Mobile Devices in Digital TV Broadcast Applications&#x2014;PS-2006-0117, http://www.docomolabs-usa.com/pdf/PS-2006-0117.pdf, Jan. 17, 2006, pp. 1-12.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00027">
<othercit>Juergen Gausemeier and Beat Bruederlin, Development of a Real Time Image Based Object Recognition Method for Mobile AR-Devices, XP-002488965, 2003, The Association for Computing Machinery, Inc. XP002488965, ISBN: 978-58113-643-2, pp. 133-139.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00028">
<othercit>Marc Directo, Shahram Shirani, David Capson, Wireless Camera Network for Image Superresolutiori, Electrical and Computer Engineering, Ont., Canada, May 2-5, 2004, Piscataway, NJ, USA,IEEE,US,pp. 681-684, vol. 2, XP010733538, D0I:10.1109/CCECE.2004.1345204,ISBN:978-0-7803-8253-4.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00029">
<othercit>Min Kyu Park et al, Super-Resolution Image Reconstruction: A Technical Overview, IEEE Signal Processing Magazine, IEEE Service Center, Piscataway, NJ, US, vol. 20, No. 3, May 1, 2003, pp. 21-36, XP011097476.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>19</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>3482221</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3482231</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3482071</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>34821199</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3482111</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3482112</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348239</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3482181</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>34820899</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3482084</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3482291</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382232</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382254</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>12</number-of-drawing-sheets>
<number-of-figures>13</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20100002102</doc-number>
<kind>A1</kind>
<date>20100107</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Carpio</last-name>
<first-name>Fredrik</first-name>
<address>
<city>San Diego</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Georgis</last-name>
<first-name>Nikolaos</first-name>
<address>
<city>San Diego</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Zhou</last-name>
<first-name>Yi</first-name>
<address>
<city>San Diego</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Carpio</last-name>
<first-name>Fredrik</first-name>
<address>
<city>San Diego</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Georgis</last-name>
<first-name>Nikolaos</first-name>
<address>
<city>San Diego</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Zhou</last-name>
<first-name>Yi</first-name>
<address>
<city>San Diego</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<last-name>Corporation</last-name>
<first-name>Sony</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Sony Corporation</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
<assignee>
<addressbook>
<orgname>Sony Electronics Inc.</orgname>
<role>02</role>
<address>
<city>Parkridge</city>
<state>NJ</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Jerabek</last-name>
<first-name>Kelly L</first-name>
<department>2662</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A system and method for efficiently performing image processing operations includes a camera device that is configured to automatically capture an image sequence of related offset images that correspond to a particular selected photographic target. The camera device then transmits the captured image sequence to an image processing server through an electronic network. The image processing server perform one or more processing-intensive operations upon the received image sequence to thereby produce an enhanced image that may then be transmitted to any appropriate image destination.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="72.39mm" wi="83.14mm" file="US08624989-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="160.87mm" wi="130.73mm" file="US08624989-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="197.53mm" wi="154.94mm" orientation="landscape" file="US08624989-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="249.68mm" wi="154.94mm" orientation="landscape" file="US08624989-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="242.06mm" wi="163.66mm" orientation="landscape" file="US08624989-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="233.26mm" wi="173.74mm" file="US08624989-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="138.01mm" wi="161.46mm" orientation="landscape" file="US08624989-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="220.13mm" wi="147.32mm" file="US08624989-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="220.81mm" wi="155.79mm" orientation="landscape" file="US08624989-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="216.32mm" wi="184.07mm" file="US08624989-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="204.47mm" wi="113.45mm" file="US08624989-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="205.66mm" wi="102.19mm" file="US08624989-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="223.86mm" wi="145.54mm" file="US08624989-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND SECTION</heading>
<p id="p-0002" num="0001">1. Field of the Invention</p>
<p id="p-0003" num="0002">This invention relates generally to techniques for handling image data, and relates more particularly to a system and method for efficiently performing image processing operations.</p>
<p id="p-0004" num="0003">2. Description of the Background Art</p>
<p id="p-0005" num="0004">Implementing effective methods for handling image data is a significant consideration for designers and manufacturers of contemporary electronic devices. However, effectively handling image data with electronic devices may create substantial challenges for system designers. For example, enhanced demands for increased device functionality and performance may require more system processing power and require additional hardware resources. An increase in processing or hardware requirements may also result in a corresponding detrimental economic impact due to increased production costs and operational inefficiencies.</p>
<p id="p-0006" num="0005">Furthermore, enhanced device capability to perform various advanced operations may provide additional benefits to a system user, but may also place increased demands on the control and management of various device components. For example, an enhanced electronic device that effectively captures digital image data may benefit from an effective implementation because of the large amount and complexity of the digital data involved.</p>
<p id="p-0007" num="0006">Due to growing demands on system resources and substantially increasing data magnitudes, it is apparent that developing new techniques for handling image data is a matter of concern for related electronic technologies. Therefore, for all the foregoing reasons, developing effective systems for handling image data remains a significant consideration for designers, manufacturers, and users of contemporary electronic devices.</p>
<heading id="h-0002" level="1">SUMMARY</heading>
<p id="p-0008" num="0007">In accordance with the present invention, a system and method are disclosed for efficiently performing image processing operations. In one embodiment, a camera device initially launches a camera application program to begin camera functions in an enhanced image capture mode. A camera user frames a selected photographic target by using a viewfinder of the camera. The camera user then activates an image capture button mounted on the exterior of the camera. The camera automatically captures an image sequence in response to the activation of the image capture button. The image sequence may include any desired number of similar, but slightly offset, images. The camera then transmits the captured image sequence to an image processing server through an electronic network.</p>
<p id="p-0009" num="0008">The image processing server locally stores the received image sequence, and then advantageously performs appropriate image processing operations on the received image sequence to produce a corresponding enhanced image. For example, in certain embodiments, the image processing operations may include a super-resolution processing procedure that requires a substantial amount of processing resources from the image processing server. The image processing server may then send the enhanced image to any appropriate image destination. For example, the image processing server may send the enhanced image back to the camera device, or may alternately send the enhanced image to a remote computer destination in the electronic network.</p>
<p id="p-0010" num="0009">The image data captured by the camera device may require image processing that consumes a substantial amount of processing resources. In accordance with the present invention, instead of locally performing various resource-intensive processing operations, the camera device provides the captured image data to the image processing server that is configured to possess a significant amount of processing and memory resources. The image processing server may thus perform image processing functions on behalf of the camera device. The camera device may therefore be implemented in a more economical manner, and may also perform important tasks other than the foregoing image processing functions. The present invention therefore provides an improved system and method for efficiently performing image processing operations.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 1A</figref> is a block diagram for one embodiment of an image processing system, in accordance with the present invention;</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 1B</figref> is a block diagram for one embodiment of the camera of <figref idref="DRAWINGS">FIG. 1A</figref>, in accordance with the present invention;</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram for one embodiment of the capture subsystem of <figref idref="DRAWINGS">FIG. 1B</figref>, in accordance with the present invention;</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 3</figref> is a block diagram for one embodiment of the control module of <figref idref="DRAWINGS">FIG. 1B</figref>, in accordance with the present invention;</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 4</figref> is a block diagram for one embodiment of the memory of <figref idref="DRAWINGS">FIG. 3</figref>, in accordance with the present invention;</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIGS. 5A and 5B</figref> are diagrams illustrating an image processing procedure, in accordance with one embodiment of the present invention;</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 6</figref> is a diagram illustrating an image sequence, in accordance with one embodiment of the present invention;</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 7</figref> is a block diagram of the server of <figref idref="DRAWINGS">FIG. 1A</figref>, in accordance with one embodiment of the present invention;</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 8</figref> is a diagram of the server memory of <figref idref="DRAWINGS">FIG. 7</figref>, in accordance with one embodiment of the present invention;</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIGS. 9A-9B</figref> are a flowchart of method steps for capturing an image sequence, in accordance with a one embodiment of the present invention; and</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 10</figref> is a flowchart of method steps for efficiently performing an image processing procedure, in accordance with a one embodiment of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0022" num="0021">The present invention relates to an improvement in image data handling techniques. The following description is presented to enable one of ordinary skill in the art to make and use the invention and is provided in the context of a patent application and its requirements. Various modifications to the disclosed embodiments will be readily apparent to those skilled in the art, and the generic principles herein may be applied to other embodiments. Thus, the present invention is not intended to be limited to the embodiments shown, but is to be accorded the widest scope consistent with the principles and features described herein.</p>
<p id="p-0023" num="0022">The present invention comprises a system and method for efficiently performing image processing operations, and includes a camera device that is configured to automatically capture an image sequence of related offset images that correspond to a particular selected photographic target. The camera device then transmits the captured image sequence to an image processing server through an electronic network. The image processing server perform one or more processing-intensive operations upon the received image sequence to thereby produce an enhanced image that may then be transmitted to any appropriate image destination.</p>
<p id="p-0024" num="0023">Referring now to <figref idref="DRAWINGS">FIG. 1A</figref>, a block diagram of an image processing system <b>100</b> is shown, in accordance with one embodiment of the present invention. In the <figref idref="DRAWINGS">FIG. 1A</figref> embodiment, image processing system <b>100</b> may include, but is not limited to, a camera <b>110</b>, a server <b>122</b>, and a network <b>124</b>.</p>
<p id="p-0025" num="0024">In alternate embodiments, image processing system <b>100</b> may be implemented by utilizing components and configurations in addition to, or instead of, certain of those components and configurations discussed in conjunction with the <figref idref="DRAWINGS">FIG. 1A</figref> embodiment. For example, for purposes of illustration, a single camera <b>110</b> and server <b>122</b> are shown in <figref idref="DRAWINGS">FIG. 1A</figref>. However, in actual practice, principles of the disclosed invention may readily be performed by any number of different cameras and/or servers.</p>
<p id="p-0026" num="0025">In the <figref idref="DRAWINGS">FIG. 1A</figref> embodiment, camera <b>110</b> may be implemented in any effective manner for capturing image data. In alternate embodiments, camera <b>110</b> may be implemented as any other type of appropriate electronic device. For example, in certain embodiments, camera <b>110</b> may be alternately implemented as a cellular telephone, a digital still camera, a video camcorder, a personal computer, a personal digital assistant (PDA), a scanner, or any type of stationary or portable consumer-electronics device.</p>
<p id="p-0027" num="0026">In certain embodiments, image data captured by camera <b>110</b> may require image processing that consumes a substantial amount of processing resources. In accordance with the present invention, instead of locally performing various resource-intensive processing operations, camera <b>110</b> may provide the captured image data to server <b>122</b> through network <b>124</b>. In certain embodiments, server <b>122</b> may be configured to include a significant amount of processing and memory resources. Server <b>122</b> may thus perform various image processing function on the received image data on behalf of camera <b>110</b>.</p>
<p id="p-0028" num="0027">Camera <b>110</b> may therefore be implemented in a more economical manner, and may also perform important tasks other than the foregoing image processing functions. In the <figref idref="DRAWINGS">FIG. 1A</figref> embodiment, network <b>124</b> may be implemented to include any desired types of wired or wireless electronic networks including, but not limited to, the Internet. After server <b>122</b> completes any required image processing, the image data may be provided to a designated data destination including, but not limited to, camera <b>110</b> or another computer entity in network <b>124</b>. Further details regarding the implementation and utilization of the <figref idref="DRAWINGS">FIG. 1A</figref> image processing system <b>100</b> are discussed below in conjunction with <figref idref="DRAWINGS">FIG. 2-10</figref>.</p>
<p id="p-0029" num="0028">Referring now to <figref idref="DRAWINGS">FIG. 1B</figref>, a block diagram for one embodiment of the <figref idref="DRAWINGS">FIG. 1A</figref> camera <b>110</b> is shown, in accordance with the present invention. In the <figref idref="DRAWINGS">FIG. 1B</figref> embodiment, camera <b>110</b> may include, but is not limited to, a capture subsystem <b>114</b>, a system bus <b>116</b>, and a control module <b>118</b>. In the <figref idref="DRAWINGS">FIG. 1B</figref> embodiment, capture subsystem <b>114</b> may be optically coupled to a photographic target <b>112</b>, and may also be electrically coupled via system bus <b>116</b> to control module <b>118</b>.</p>
<p id="p-0030" num="0029">In alternate embodiments, camera device <b>110</b> may include other components in addition to, or instead of, certain of those components discussed in conjunction with the <figref idref="DRAWINGS">FIG. 1B</figref> embodiment. In addition, in certain embodiments, the present invention may alternately be embodied in any appropriate type of electronic device other than the camera device <b>110</b> of <figref idref="DRAWINGS">FIG. 1B</figref>. For example, camera device <b>110</b> may alternately be implemented as an imaging device, a cellular telephone, a scanner, a video camcorder, a computer device, or a consumer electronics device.</p>
<p id="p-0031" num="0030">In the <figref idref="DRAWINGS">FIG. 1B</figref> embodiment, once a system user has focused capture subsystem <b>114</b> on target <b>112</b> and requested camera <b>110</b> to capture image data corresponding to target <b>112</b>, then control module <b>118</b> may instruct capture subsystem <b>114</b> via system bus <b>116</b> to capture image data representing target <b>112</b>. The captured image data may then be transferred over system bus <b>116</b> to control module <b>118</b>, which may responsively perform various processes and functions with the image data. System bus <b>116</b> may also bi-directionally pass various status and control signals between capture subsystem <b>114</b> and control module <b>118</b>.</p>
<p id="p-0032" num="0031">Referring now to <figref idref="DRAWINGS">FIG. 2</figref>, a block diagram for one embodiment of the <figref idref="DRAWINGS">FIG. 1B</figref> capture subsystem <b>114</b> is shown, in accordance with the present invention. In the <figref idref="DRAWINGS">FIG. 2</figref> embodiment, capture subsystem <b>114</b> comprises, but is not limited to, a shutter <b>218</b>, a lens unit <b>220</b>, an image sensor <b>224</b>, red, green, and blue (R/G/B) amplifiers <b>228</b>, an analog-to-digital (A/D) converter <b>230</b>, and an interface <b>232</b>. In alternate embodiments, capture subsystem <b>114</b> may readily include other components in addition to, or instead of, certain those components discussed in conjunction with the <figref idref="DRAWINGS">FIG. 2</figref> embodiment.</p>
<p id="p-0033" num="0032">In the <figref idref="DRAWINGS">FIG. 2</figref> embodiment, capture subsystem <b>114</b> captures image data corresponding to target <b>112</b> via reflected light impacting image sensor <b>224</b> along optical path <b>236</b>. Image sensor <b>224</b>, which may include a charged-coupled device (CCD), may responsively generate a set of image data representing the target <b>112</b>. The image data may then be routed through red, green, and blue amplifiers <b>228</b>, A/D converter <b>230</b>, and interface <b>232</b>. From interface <b>232</b>, the image data passes over system bus <b>116</b> to control module <b>118</b> for appropriate processing and storage.</p>
<p id="p-0034" num="0033">Referring now to <figref idref="DRAWINGS">FIG. 3</figref>, a block diagram for one embodiment of the <figref idref="DRAWINGS">FIG. 1B</figref> control module <b>118</b> is shown, in accordance with the present invention. In the <figref idref="DRAWINGS">FIG. 3</figref> embodiment, control module <b>118</b> includes, but is not limited to, a viewfinder <b>308</b>, a central processing unit (CPU) <b>344</b>, a memory <b>346</b>, and one or more input/output interface(s) (I/O) <b>348</b>. Viewfinder <b>308</b>, CPU <b>344</b>, memory <b>346</b>, and I/O <b>348</b> are each coupled to, and communicate, via common system bus <b>116</b> that also communicates with capture subsystem <b>114</b>. In alternate embodiments, control module <b>118</b> may include other components in addition to, or instead of, certain of those components discussed in conjunction with the <figref idref="DRAWINGS">FIG. 3</figref> embodiment.</p>
<p id="p-0035" num="0034">In the <figref idref="DRAWINGS">FIG. 3</figref> embodiment, CPU <b>344</b> may be implemented to include any appropriate microprocessor device. Alternately, CPU <b>344</b> may be implemented using any other appropriate technology. For example, CPU <b>344</b> may be implemented to include certain application-specific integrated circuits (ASICs) or other appropriate electronic devices. Memory <b>346</b> may be implemented as one or more appropriate storage devices, including, but not limited to, read-only memory, random-access memory, and various types of non-volatile memory, such as floppy disc devices, hard disc devices, or flash memory. I/O <b>348</b> may provide one or more effective interfaces for facilitating bi-directional communications between camera device <b>110</b> and any external entity, including a system user or another electronic device. I/O <b>348</b> may be implemented using any appropriate input and/or output devices. For example, I/O <b>348</b> may include any effective means of communicating with the <figref idref="DRAWINGS">FIG. 1A</figref> server <b>122</b> by utilizing wireless or wired techniques. The operation and implementation of control module <b>118</b> are further discussed below in conjunction with <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0036" num="0035">Referring now to <figref idref="DRAWINGS">FIG. 4</figref>, a block diagram for one embodiment of the <figref idref="DRAWINGS">FIG. 3</figref> memory <b>346</b> is shown, in accordance with the present invention. In the <figref idref="DRAWINGS">FIG. 4</figref> embodiment, memory <b>346</b> may include, but is not limited to, a camera application <b>412</b>, an operating system <b>414</b>, a communications manager <b>416</b>, configuration parameters <b>420</b>, image data <b>424</b>, and miscellaneous information. In alternate embodiments, memory <b>346</b> may readily include various other components in addition to, or instead of, those components discussed in conjunction with the <figref idref="DRAWINGS">FIG. 4</figref> embodiment.</p>
<p id="p-0037" num="0036">In the <figref idref="DRAWINGS">FIG. 4</figref> embodiment, camera application <b>412</b> may include program instructions that are executed by CPU <b>344</b> (<figref idref="DRAWINGS">FIG. 3</figref>) to perform various functions and operations for camera device <b>110</b>. The particular nature and functionality of camera application <b>412</b> varies depending upon factors such as the type and particular use of the corresponding camera device <b>110</b>. In the <figref idref="DRAWINGS">FIG. 4</figref> embodiment, operating system <b>414</b> preferably controls and coordinates low-level functionality of camera device <b>110</b>.</p>
<p id="p-0038" num="0037">In the <figref idref="DRAWINGS">FIG. 4</figref> embodiment, camera <b>110</b> may utilize communications manager <b>416</b> to perform bi-directional communications with any external entity. Configuration parameters <b>420</b> may be selectively programmed by a system user to specify any desired functions or operating characteristics for camera <b>110</b>. Image data <b>424</b> may include any appropriate type of information. For example, image data <b>424</b> may include raw captured image data, one or more image sequences, and processed image data. In the <figref idref="DRAWINGS">FIG. 4</figref> embodiment, miscellaneous information <b>428</b> may include any additional types of information, data, or software instructions for camera <b>110</b>. The utilization camera <b>110</b> is further discussed below in conjunction with <figref idref="DRAWINGS">FIG. 9</figref>.</p>
<p id="p-0039" num="0038">Referring now to <figref idref="DRAWINGS">FIGS. 5A and 5B</figref>, diagrams illustrating an image processing procedure are shown, in accordance with one embodiment of the present invention. The <figref idref="DRAWINGS">FIG. 5</figref> diagrams are presented for purposes of illustration, and in alternate embodiments, image processing procedures may include functions and techniques in addition to, or instead of, certain of those functions and techniques shown in the <figref idref="DRAWINGS">FIG. 5</figref> example.</p>
<p id="p-0040" num="0039">In the <figref idref="DRAWINGS">FIG. 5A</figref> diagram, two images (<b>522</b> and <b>524</b>) from an image sequence captured by camera <b>110</b> are shown. For purposes of simplicity, each image <b>522</b> and <b>524</b> is shown with only nine pixels (three horizontal and three vertical pixels) that are each represented as a square. However, any desired number of pixels may be utilized. Even though camera <b>110</b> may be aimed at a given target, due to slight movements by the camera user, images <b>522</b> and <b>524</b> are typically slightly offset with respect to each other.</p>
<p id="p-0041" num="0040">The <figref idref="DRAWINGS">FIG. 5B</figref> diagram illustrates a image processing procedure performed when images <b>522</b> and <b>524</b> are transferred from camera <b>110</b> to server <b>122</b>, as discussed above in conjunction with <figref idref="DRAWINGS">FIG. 1A</figref>. In certain embodiments, the image processing procedure may include, but is not limited to, a super-resolution procedure to create an enhanced image from the image sequence of original offset images <b>522</b> and <b>524</b>.</p>
<p id="p-0042" num="0041">As shown in <figref idref="DRAWINGS">FIG. 5B</figref>, since images <b>522</b> and <b>524</b> are slightly offset (by approximately one-half pixel, both horizontally and vertically), an image processor of server <b>122</b> may obtain additional image detail to create sub-pixels for populating the enhanced image. In the <figref idref="DRAWINGS">FIG. 5B</figref> example, there are four sub-pixels for each original pixel from images <b>522</b> and <b>524</b>. The foregoing image processing procedure may thus create an enhanced image with a substantially greater number of pixels and improved resolution characteristics.</p>
<p id="p-0043" num="0042">Referring now to <figref idref="DRAWINGS">FIG. 6</figref>, a diagram illustrating an image sequence <b>614</b> is shown, in accordance with one embodiment of the present invention. In alternate embodiments, the present invention may utilize image sequences with various other configurations and components to implement image processing procedures.</p>
<p id="p-0044" num="0043">In the <figref idref="DRAWINGS">FIG. 6</figref> example, camera <b>110</b> (<figref idref="DRAWINGS">FIG. 1</figref>) may capture and transfer image sequence <b>614</b> to server <b>122</b> (<figref idref="DRAWINGS">FIG. 1</figref>) for performing image processing procedures, as discussed above in conjunction with <figref idref="DRAWINGS">FIG. 5</figref>. In the <figref idref="DRAWINGS">FIG. 6</figref> embodiment, image sequence <b>614</b> includes a header <b>618</b> that includes any appropriate type of information. For example, header <b>618</b> may include an image sequence source identifier to indicate where image sequence <b>614</b> originated. Header <b>618</b> may also include an enhanced image destination identifier to indicate where server <b>122</b> should send an enhanced image created from image sequence <b>614</b>. The capture and utilization of image sequences are further discussed below in conjunction with <figref idref="DRAWINGS">FIGS. 9-10</figref>.</p>
<p id="p-0045" num="0044">Referring now to <figref idref="DRAWINGS">FIG. 7</figref>, a block diagram for one embodiment of the <figref idref="DRAWINGS">FIG. 1A</figref> server <b>122</b> is shown, in accordance with the present invention. In the <figref idref="DRAWINGS">FIG. 7</figref> embodiment, server <b>122</b> includes, but is not limited to, a server central processing unit (server CPU) <b>712</b>, a server display <b>716</b>, a server memory <b>720</b>, and one or more server input/output interface(s) (server I/O interface(s)) <b>724</b>. The foregoing components of server <b>122</b> may be coupled to, and communicate through, a server bus <b>728</b>. In alternate embodiments, server <b>122</b> may alternately be implemented using components and configurations in addition to, or instead of, certain of those components and configurations discussed in conjunction with the <figref idref="DRAWINGS">FIG. 7</figref> embodiment.</p>
<p id="p-0046" num="0045">In the <figref idref="DRAWINGS">FIG. 7</figref> embodiment, server CPU <b>712</b> may be implemented to include any appropriate and compatible microprocessor device that preferably executes software instructions to thereby control and manage the operation of server <b>118</b>. The <figref idref="DRAWINGS">FIG. 7</figref> server display <b>716</b> may include any effective type of display technology including a cathode-ray-tube monitor or a liquid-crystal display device with an appropriate screen for displaying various information to a server user. In the <figref idref="DRAWINGS">FIG. 7</figref> embodiment, server memory <b>720</b> may be implemented to include any combination of desired storage devices, including, but not limited to, read-only memory (ROM), random-access memory (RAM), and various types of non-volatile memory, such as floppy disks, memory sticks, compact disks, or hard disks. The contents and functionality of server memory <b>720</b> are further discussed below in conjunction with <figref idref="DRAWINGS">FIG. 8</figref>.</p>
<p id="p-0047" num="0046">In the <figref idref="DRAWINGS">FIG. 7</figref> embodiment, server I/O interface(s) <b>724</b> may include one or more input and/or output interfaces to receive and/or transmit any required types of information by server <b>122</b>. Server I/O interface(s) <b>724</b> may include one or more means for allowing a server user to communicate with network <b>124</b>. The implementation and utilization of server <b>122</b> is further discussed below in conjunction with <figref idref="DRAWINGS">FIG. 10</figref>.</p>
<p id="p-0048" num="0047">Referring now to <figref idref="DRAWINGS">FIG. 8</figref>, a block diagram for one embodiment of the <figref idref="DRAWINGS">FIG. 7</figref> server memory <b>720</b> is shown, in accordance with the present invention. In the <figref idref="DRAWINGS">FIG. 8</figref> embodiment, server memory <b>720</b> may include, but is not limited to, a server application <b>812</b>, a communications manager <b>816</b>, image data <b>820</b>, an image processor <b>824</b>, and miscellaneous information <b>828</b>. In alternate embodiments, server memory <b>720</b> may include various other components and functionalities in addition to, or instead of, certain those components and functionalities discussed in conjunction with the <figref idref="DRAWINGS">FIG. 8</figref> embodiment.</p>
<p id="p-0049" num="0048">In the <figref idref="DRAWINGS">FIG. 8</figref> embodiment, server application <b>812</b> may include program instructions that are preferably executed by server CPU <b>712</b> (<figref idref="DRAWINGS">FIG. 7</figref>) to perform various functions and operations for server <b>122</b>. The particular nature and functionality of server application <b>812</b> typically varies depending upon factors such as the specific type and particular functionality of the corresponding server <b>122</b>. In the <figref idref="DRAWINGS">FIG. 8</figref> embodiment, server <b>122</b> may utilize communications manager <b>816</b> to perform bi-directional communications with any external entity. Image data <b>820</b> may include any appropriate type of information. For example, image data <b>820</b> may include raw captured image data, one or more image sequences, and processed/enhanced image data.</p>
<p id="p-0050" num="0049">Server <b>122</b> may utilize image processor <b>824</b> to perform any appropriate types of image processing procedures on image data received from camera <b>110</b> (<figref idref="DRAWINGS">FIG. 1</figref>). For example, image processor <b>824</b> may perform a super-resolution processing procedure as discussed above in conjunction with FIG. <b>5</b>. In addition, image processor <b>824</b> may perform any other image processing functions including, but not limited to, white balancing, red-eye correction, image smoothing, and image format conversions. In the <figref idref="DRAWINGS">FIG. 8</figref> embodiment, miscellaneous information <b>828</b> may include any additional types of information, data, or software instructions for use by server <b>122</b>. Additional details regarding the operation and implementation of server <b>122</b> are further discussed below in conjunction with <figref idref="DRAWINGS">FIG. 10</figref>.</p>
<p id="p-0051" num="0050">Referring now to <figref idref="DRAWINGS">FIGS. 9A-9B</figref>, a flowchart of method steps for capturing an image sequence <b>614</b> is shown, in accordance with one embodiment of the present invention. The <figref idref="DRAWINGS">FIG. 9</figref> embodiment is presented for purposes of illustration, and in alternate embodiments, the present invention may readily utilize various other steps and sequences than those discussed in conjunction with the <figref idref="DRAWINGS">FIG. 9</figref> embodiment.</p>
<p id="p-0052" num="0051">In step <b>918</b> of <figref idref="DRAWINGS">FIG. 9A</figref>, a camera <b>110</b> initially launches a camera application program to begin normal camera functions. In step <b>922</b>, camera <b>110</b> determines whether a enhanced image mode is currently activated. If the enhanced image mode is activated, then in step <b>926</b>, a camera user may frame a selected photographic target by using a viewfinder of camera <b>110</b>. In step <b>930</b>, the camera user then activates an image capture button mounted on the exterior of camera <b>110</b>. The <figref idref="DRAWINGS">FIG. 9A</figref> process then advances to step <b>934</b> of <figref idref="DRAWINGS">FIG. 9B</figref> through connecting letter &#x201c;A.&#x201d;</p>
<p id="p-0053" num="0052">In step <b>934</b>, camera <b>110</b> automatically captures an image sequence <b>614</b> in response to the single activation of the image capture button in foregoing step <b>930</b>. The image sequence <b>614</b> may include any desired number of similar, but slightly offset, images. For example, in certain embodiments, the image sequence <b>614</b> includes three or more images. In step <b>938</b>, camera <b>110</b> determines whether an image processing server <b>122</b> is currently available for performing an image processing procedure on the captured image sequence <b>614</b>.</p>
<p id="p-0054" num="0053">Availability criteria for accessing server <b>122</b> may include, but are not limited to, sufficient network bandwidth, adequate server capacity, and the camera user having a valid processing service subscription. If server <b>122</b> is currently available for performing image processing on the image sequence <b>614</b>, then in step <b>946</b>, camera <b>110</b> transmits the captured image sequence <b>614</b> to server <b>122</b>. The <figref idref="DRAWINGS">FIG. 9B</figref> process may then return to step <b>926</b> to capture and transmit additional image sequences to server <b>122</b>.</p>
<p id="p-0055" num="0054">Referring now to <figref idref="DRAWINGS">FIG. 10</figref>, a flowchart of method steps for efficiently performing an image processing procedure is shown, in accordance with a second embodiment of the present invention. The <figref idref="DRAWINGS">FIG. 10</figref> embodiment is presented for purposes of illustration, and in alternate embodiments, the present invention may readily utilize various other steps and sequences than those discussed in conjunction with the <figref idref="DRAWINGS">FIG. 10</figref> embodiment.</p>
<p id="p-0056" num="0055">In the <figref idref="DRAWINGS">FIG. 10</figref> embodiment, in step <b>1014</b>, an image processing server <b>122</b> initially receives an image sequence <b>614</b> from a camera <b>110</b> through an electronic network <b>124</b>. In step <b>1018</b>, the server <b>122</b> locally stores the received image sequence <b>614</b>. Then, in step <b>1022</b>, server <b>122</b> advantageously performs appropriate image processing operations on the received image sequence <b>614</b> to produce a corresponding enhanced image. In step <b>1026</b>, server <b>122</b> determines an appropriate destination for the enhanced image by utilizing any effective techniques.</p>
<p id="p-0057" num="0056">For example, a destination for the enhanced image may be previously specified in programmable user preferences, or server <b>122</b> may analyze a header <b>618</b> of received image sequence <b>614</b> to determine an appropriate destination for the corresponding enhanced image. In the <figref idref="DRAWINGS">FIG. 10</figref> embodiment, server <b>122</b> may send the enhanced image back to camera <b>110</b> in step <b>1030</b>, or may send the enhanced image to a remote computer destination in electronic network <b>124</b>, as shown in step <b>1034</b>. In alternate embodiments, server <b>124</b> may send the enhanced image to any specified destination. The <figref idref="DRAWINGS">FIG. 10</figref> process may then return to step <b>1014</b> to create additional enhanced images in a similar manner. The present invention therefore provides an improved system and method for efficiently performing image processing operations.</p>
<p id="p-0058" num="0057">The invention has been explained above with reference to certain embodiments. Other embodiments will be apparent to those skilled in the art in light of this disclosure. For example, the present invention may readily be implemented using configurations and techniques other than those described in the embodiments above. Additionally, the present invention may effectively be used in conjunction with systems other than those described above. Therefore, these and other variations upon the discussed embodiments are intended to be covered by the present invention, which is limited only by the appended claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A system for performing an image processing procedure, comprising:
<claim-text>a server comprising one or more processors, said one or more processors being operable to:
<claim-text>receive image data comprising a plurality of images captured by an imaging device, wherein at least one of said plurality of images is offset with respect to a preceding image of said plurality of images due to movement of said imaging device; and</claim-text>
<claim-text>automatically perform an image processing operation to create an enhanced high-resolution image corresponding to said at least one of said plurality of images using sub-pixels of said at least one of said plurality of images and said preceding image of said plurality of images, wherein said enhanced high-resolution image is created by splitting pixels of said at least one of said plurality of images and said preceding image of said plurality of images into said sub-pixels.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said imaging device is one of: a still camera, a video camcorder, or a cellular telephone.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said image processing operation includes a super-resolution process to create said enhanced high-resolution image.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said plurality of images is automatically captured in response to a single activation of an image capture button of said imaging device.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said plurality of images comprises a header that specifies an origination device for said plurality of images and a destination device for said enhanced high-resolution image.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said imaging device uses wireless technology to transfer said image data to said server.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said imaging device transfers said image data to said server through an electronic network.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The system of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein said electronic network is an Internet network.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said server transmits said enhanced high-resolution image back to said imaging device after performing said image processing operation.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said server transmits said enhanced high-resolution image to a remote network device after performing said image processing operation.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said server performs said image processing operation according to a premium service subscription.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said imaging device transfers said image data to said server after determining that said server is currently available to perform said image processing operation.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said image processing operation comprises a white balance processing operation and a red-eye processing operation for said image data.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said image processing operation comprises a smoothing operation and a format conversion operation for said image data.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said image processing operation is a resource-intensive procedure with respect to device bandwidth consumption and device memory requirements.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein said server performs said image processing operation on behalf of said imaging device.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein said imaging device is able to perform other processing tasks because said server performs said image processing operation.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said server creates said enhanced-comprises image without combining said image data with an image template.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. A method for performing an image processing procedure, comprising:
<claim-text>receiving image data comprising a plurality of images captured by an imaging device, wherein at least one of said plurality of images is offset with respect to a preceding image of said plurality of images due to movement of said imaging device; and</claim-text>
<claim-text>automatically performing an image processing operation to create an enhanced high-resolution image corresponding to said at least one of said plurality of images using sub-pixels of said at least one of said plurality of images and said preceding image of said plurality of images, wherein said enhanced high-resolution image is created by splitting pixels of said at least one of said plurality of images and said preceding image of said plurality of images into said sub-pixels. </claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
