<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08624986-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08624986</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13077677</doc-number>
<date>20110331</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<us-term-of-grant>
<us-term-extension>477</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>5</main-group>
<subgroup>228</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>5</main-group>
<subgroup>225</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>34820813</main-classification>
<further-classification>3482201</further-classification>
<further-classification>3482085</further-classification>
</classification-national>
<invention-title id="d2e53">Motion robust depth estimation using convolution and wavelet transforms</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>7957599</doc-number>
<kind>B2</kind>
<name>Berkner et al.</name>
<date>20110600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382232</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>8422827</doc-number>
<kind>B2</kind>
<name>Ishii et al.</name>
<date>20130400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382299</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>2003/0117511</doc-number>
<kind>A1</kind>
<name>Belz et al.</name>
<date>20030600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>34833311</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>2007/0019883</doc-number>
<kind>A1</kind>
<name>Wong</name>
<date>20070100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>2007/0036427</doc-number>
<kind>A1</kind>
<name>Nakamura et al.</name>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382154</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2007/0274570</doc-number>
<kind>A1</kind>
<name>Hamza</name>
<date>20071100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382117</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2008/0013861</doc-number>
<kind>A1</kind>
<name>Li</name>
<date>20080100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2009/0268985</doc-number>
<kind>A1</kind>
<name>Wong</name>
<date>20091000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2010/0039538</doc-number>
<kind>A1</kind>
<name>Ikedo</name>
<date>20100200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348241</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>GB</country>
<doc-number>2379113</doc-number>
<kind>A</kind>
<date>20030200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>WO</country>
<doc-number>2007022329</doc-number>
<kind>A2</kind>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>WO</country>
<doc-number>2007052191</doc-number>
<kind>A2</kind>
<date>20070500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00013">
<othercit>Zhang et al.&#x2014;&#x201c;Multi-Scale Blur Estimation and Edge Type Classification for Scene Analysis&#x201d;&#x2014;Abstract Only, Int. Jour. of Computer Vision, vol. 24, No. 3, Sep. 1997, pp. 1-2.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00014">
<othercit>Bae, S. et al.&#x2014;&#x201c;Defocus Magnification&#x201d;&#x2014;Eurographics 2007, vol. 26, No. 3, pp. 1-9.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>20</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>3482201</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348241</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>34820799</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3482086</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>34820814</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>34820813</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3482082</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3482083</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3482081</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3482085</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382168</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382172</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382254</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382255</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382263</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382264</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382275</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382280</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>12</number-of-drawing-sheets>
<number-of-figures>18</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120249833</doc-number>
<kind>A1</kind>
<date>20121004</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Li</last-name>
<first-name>Pingshan</first-name>
<address>
<city>Sunnyvale</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Li</last-name>
<first-name>Pingshan</first-name>
<address>
<city>Sunnyvale</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<last-name>O'Banion</last-name>
<first-name>John P.</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Sony Corporation</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Ye</last-name>
<first-name>Lin</first-name>
<department>2664</department>
</primary-examiner>
<assistant-examiner>
<last-name>Cowan</last-name>
<first-name>Euel</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Apparatus and method for electronically estimating focusing distance between a camera (still and/or video camera) and a subject. Images at different focal positions of a calibration target are collected to arrive at a focus matching model for a given imaging apparatus. In operation, at least two images are captured and convolutions performed which approximate the modeling of blur change as a point spread function. Wavelet transforms are applied to the images after each convolution and images are compared based on the wavelet variance differences to provide a motion robust blur difference determination. Applying the blur differences to the focus matching model provides an estimate of focusing distance, which can be utilized such as for controlling camera focus.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="103.46mm" wi="259.59mm" file="US08624986-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="261.62mm" wi="187.37mm" orientation="landscape" file="US08624986-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="225.21mm" wi="131.66mm" orientation="landscape" file="US08624986-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="182.46mm" wi="166.03mm" orientation="landscape" file="US08624986-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="226.65mm" wi="167.81mm" orientation="landscape" file="US08624986-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="255.69mm" wi="138.01mm" orientation="landscape" file="US08624986-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="186.69mm" wi="166.37mm" orientation="landscape" file="US08624986-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="196.85mm" wi="164.59mm" orientation="landscape" file="US08624986-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="196.51mm" wi="170.18mm" orientation="landscape" file="US08624986-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="197.19mm" wi="166.71mm" orientation="landscape" file="US08624986-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="261.28mm" wi="129.96mm" orientation="landscape" file="US08624986-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="245.87mm" wi="114.22mm" file="US08624986-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="208.03mm" wi="151.64mm" orientation="landscape" file="US08624986-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">Not Applicable</p>
<heading id="h-0002" level="1">STATEMENT REGARDING FEDERALLY SPONSORED RESEARCH OR DEVELOPMENT</heading>
<p id="p-0003" num="0002">Not Applicable</p>
<heading id="h-0003" level="1">INCORPORATION-BY-REFERENCE OF MATERIAL SUBMITTED ON A COMPACT DISC</heading>
<p id="p-0004" num="0003">Not Applicable</p>
<heading id="h-0004" level="1">NOTICE OF MATERIAL SUBJECT TO COPYRIGHT PROTECTION</heading>
<p id="p-0005" num="0004">A portion of the material in this patent document is subject to copyright protection under the copyright laws of the United States and of other countries. The owner of the copyright rights has no objection to the facsimile reproduction by anyone of the patent document or the patent disclosure, as it appears in the United States Patent and Trademark Office publicly available file or records, but otherwise reserves all copyright rights whatsoever. The copyright owner does not hereby waive any of its rights to have this patent document maintained in secrecy, including without limitation its rights pursuant to 37 C.F.R. &#xa7;114.</p>
<heading id="h-0005" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0006" num="0005">1. Field of the Invention</p>
<p id="p-0007" num="0006">This invention pertains generally to image acquisition and processing, and more particularly to depth estimation.</p>
<p id="p-0008" num="0007">2. Description of Related Art</p>
<p id="p-0009" num="0008">Proper camera focus is a critical metric when capturing an image with an image acquisition device (video or still image). Numerous systems have been developed for estimating or attaining a proper camera focus. As a camera-lens system has a number of related elements and characteristics, a brief discussion follows of these elements and their associated characteristics.</p>
<p id="p-0010" num="0009">Generally speaking, the two main optical parameters of a photographic lens are maximum aperture and focal length. The focal length determines the angle of view, and the size of the image relative to that of the object (subject) for a given distance to the subject (subject-distance). The maximum aperture (f-number, or f-stop) limits the brightness of the image and the fastest shutter speed usable for a given setting (focal length/effective aperture), with a smaller number indicating that more light is provided to the focal plane which typically can be thought of as the face of the image sensor in a simple digital camera.</p>
<p id="p-0011" num="0010">One form of typical simple lens (technically a lens having a single element) is that of having a single focal length (also referred to as a &#x201c;prime lens&#x201d;). In focusing a camera using a single focal length lens, the distance between lens and the focal plane is changed therein altering the focal point of the photographic subject onto that focal plane. So although the single focal length lens has a fixed optical relation and focal length, it is used in the camera to focus on subjects across a focal range span. Consequently, one should not confuse the fixed focal distance of a lens with the range of focal distance obtainable on a camera using that lens, whereby adjusting the position of that lens in relation to the focal plane alters focal distance.</p>
<p id="p-0012" num="0011">In using a single focal length lens one would adjust aperture to select the amount of light with respect to desired shutter speed, and then adjust focus according to the subject-distance, which is also referred to as the focal distance and then capture an image. Often a macro setting is provided with a different focal length selection, on an otherwise single focal length lens, for taking close-up shots. A telephoto lens provides a very narrow angle of view with high magnification for filling the frame with images from distance objects.</p>
<p id="p-0013" num="0012">It will be noted that multi-focal length lenses are usually referred to as &#x201c;zoom&#x201d; lenses, because image magnification can be &#x201c;zoomed&#x201d;, or &#x201c;unzoomed&#x201d; as the case may be. Zoom lenses allow the user to select the amount of magnification of the subject, or put another way, the degree to which the subject fills the frame. It is important to understand that the zoom function of these lenses, or camera-lens systems, is conceptually separate from both the focus control and the aperture control.</p>
<p id="p-0014" num="0013">Irrespective of whether a single-focal length lens or multi-focal length lens is utilized, it is necessary to properly focus the lens for a given subject-distance. An acceptable range of focus for a given focus setting is referred to as &#x201c;depth of field&#x201d; which is a measurement of depth of acceptable sharpness in the object space, or subject space. For example, with a subject distance of fifteen feet, an acceptable range of focus for a high definition camera may be on the order of inches, while optimum focus can require even more precision. It will be appreciated that depth of field increases as the focusing moves from intermediate distances out toward &#x201c;infinity&#x201d; (e.g., capturing images of distant mountains, clouds and so forth), which of course at that range has unlimited depth of field.</p>
<p id="p-0015" num="0014">For a single focal length lens at a given aperture setting there will be a single optimum focus setting for a given distance from camera to the subject (subject-distance). Portions of the subject which are closer or farther than the focal distance of the camera will show up in the captured images subject to some measure of blurring, as depends on many factors that impact depth of field. However, in a multi-focal lens there is an optimum focus point for each lens magnification (lens focal length) obtainable by the lens. To increase practicality, lens makers have significantly reduced the need to refocus in response to zoom settings, however, the necessity for refocusing depends on the specific camera-lens system in use. In addition, the aperture setting can require changing in response to different levels of zoom magnification.</p>
<p id="p-0016" num="0015">In early camera systems, focus could only be determined and corrected in response to operator recognition and a manual focus adjustment. However, due to the critical nature of focus on results, focusing aids were readily adopted. More recently, imaging devices often provide the ability to automatically focus on the subject, a function which is generically referred to today as &#x201c;auto focus&#x201d;. Focus continues to be a point of intense technical development as each of the many existing auto focus mechanisms are subject to shortcomings and tradeoffs.</p>
<p id="p-0017" num="0016">Although numerous focusing mechanisms exist, these can be divided into two general types of auto focus (AF) systems: (1) active auto focus and (2) passive auto focus. In active auto focus, one or more image sensors is utilized to determine distance to the focal point, or otherwise to detect focus external of the image capture lens system. Active AF systems can perform rapid focusing although they will not typically focus through windows, or in other specific applications, since sound waves and infrared light are reflected by the glass and other surfaces. In passive auto focus systems the characteristics of the viewed image are used to detect and set focus.</p>
<p id="p-0018" num="0017">The majority of high-end SLR cameras currently use through-the-lens optical AF sensors, which for example, may also be utilized as light meters. The focusing ability of these modern AF systems can often be of higher precision than that achieved manually through an ordinary viewfinder.</p>
<p id="p-0019" num="0018">In one form of passive AF system, phase detection is utilized, such as by dividing the incoming light through a beam splitter into pairs of images and comparing them on an AF sensor. Two optical prisms capture the light rays coming from the opposite sides of the lens and divert it to the AF sensor, creating a simple rangefinder with a base identical to the diameter of the lens. Focus is determined in response to checking for similar light intensity patterns and phase difference calculated to determine if the object is considered in front of the focus or in back of the proper focus position.</p>
<p id="p-0020" num="0019">In another type of passive AF system, contrast measurements are made within a sensor field through the lens. The system adjusts focus to maximize intensity difference between adjacent pixels which is generally indicative of correct image focus. Thus, focusing is performed until a maximum level of contrast is obtained. This form of focusing is slower than active AF, in particular when operating under dim light, but is a common method utilized in low end imaging devices.</p>
<p id="p-0021" num="0020">Passive systems are notoriously poor at making focal decisions in low contrast conditions, notably on large single-colored surfaces (solid surface, sky, and so forth) or in low-light conditions. Passive systems are dependent on a certain degree of illumination to the subject (whether natural or otherwise), while active systems may focus correctly even in total darkness when necessary.</p>
<p id="p-0022" num="0021">Accordingly, a need exists for improved auto focusing techniques which provide rapid and accurate subject-distance estimations and/or focus control under a wide range of conditions. The present invention fulfills that need as well as others and overcomes shortcomings of previous camera focus techniques.</p>
<heading id="h-0006" level="1">BRIEF SUMMARY OF THE INVENTION</heading>
<p id="p-0023" num="0022">A method of camera depth estimation is described which is based on blur differences and multiple picture matching. This method computes a blur difference between images captured at different focus positions. The present invention utilizes a novel mechanism for determining the difference (blur) between images. Previous systems utilized some form of norm operator applied to the pixel-to-pixel difference between those images to compare the two images after convolution. In the inventive technique, a wavelet transform is applied to the images after convolution, and is followed by a calculation of variance of the wavelet coefficients. A difference of the two variances is then determined, such as preferably an absolute difference. As a consequence not needing to compare each pixel in a first image to a corresponding pixel position in a second image, apparatus and methods according to the invention provide a motion robust form of blur determination which is described herein within a system for estimating subject depth.</p>
<p id="p-0024" num="0023">It should be appreciated that blur difference varies depending on lens focus and position in relation to the target image, which according to the present invention can be approximated using a polynomial model, such as preferably of at least two-dimensions. The polynomial model is calibrated, preferably off-line, such as by using a series of step-edge images, or similarly convenient calibration image mechanism for registering proper focus, and is then utilized for calculating the depth for images for a given image collection apparatus (e.g., camera make or model). Accordingly, the calibration target or subject is utilized in a characterization process in which the blur characteristics (focus characteristics) of the camera and lens system are determined and modeled for use during camera operations.</p>
<p id="p-0025" num="0024">The discussion herein is directed primarily to a camera having a single focal length lens, however, the technique is applicable to multi-focal length lenses (e.g., &#x201c;zoom&#x201d; lenses) as will be discussed near the end of the specification. It will be appreciated that in addition to auto focusing, the depth estimation taught herein has many applications in areas including computer/robotic vision, surveillance, 3D imaging, and similar imaging systems.</p>
<p id="p-0026" num="0025">According to a general description of the invention, matching curves are obtained for calibration images (e.g., step-edge images) at different distances across the whole focusing range, or a desired portion thereof. Then a multi-dimensional (e.g., two-dimensional) model is created, preferably a polynomial model, to represent the matching curves. The two-dimensional polynomial model can then be used for depth estimation when blur differences are computed on general images for the given apparatus.</p>
<p id="p-0027" num="0026">The following terms are generally described in relation to the specification, and are not to be interpreted toward constraining specific recitations of the specification.</p>
<p id="p-0028" num="0027">The term &#x201c;histogram&#x201d; is a statistical term describing a graphical display of tabulated frequencies, and generally shows proportionally the number of cases falling into each of several categories, whether discrete in bars or across a range.</p>
<p id="p-0029" num="0028">The term &#x201c;polynomial&#x201d; as applied for modeling a matching curve is a polynomial function, such as having the general one dimensional form:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>y=a</i><sub>n</sub><i>x</i><sup>n</sup><i>+a</i><sub>n-1</sub><i>x</i><sup>n-1</sup><i>+ . . . +a</i><sub>2</sub><i>x</i><sup>2</sup><i>+a</i><sub>1</sub><i>x</i><sup>1</sup><i>+a</i><sub>0 </sub><?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
in which n is a non-negative integer that defines the degree of the polynomial. It will be noted that a polynomial with a degree of 3 is a cubic, 2 is a quadratic, 1 is a line and 0 is a constant. Polynomial equations can be used for modeling a wide range of empirically determined relationships.
</p>
<p id="p-0030" num="0029">The term &#x201c;convolution&#x201d; as used herein describes a mathematical operation on two functions to produce a third function that is typically viewed as a modified version of one of the original functions. Often the second function is reversed and overlays a portion of the first function, toward more properly modeling a given data set.</p>
<p id="p-0031" num="0030">The term &#x201c;point spread function&#x201d; (PSF) describes the response of an imaging system to a point source or point object, this is often also referred to as an impulse response, such as found across a step edge. In this context, the degree of spreading (blurring) of the point object is a measure for the focal quality of the imaging system.</p>
<p id="p-0032" num="0031">The term &#x201c;outlier&#x201d; is a statistical term indicating that one or more observations in the empirical data set are numerically distinct or separate from the remainder of the data set. Outlier points may indicate systemic shortcomings, faulty data, and so forth, although a small number of outliers are expected in any large sample sets. Attempting to model the data set including the &#x201c;outliers&#x201d; could lead to a misleading model, wherein they are typically discarded once it is assured they do not properly represent the characteristics of the underlying function.</p>
<p id="p-0033" num="0032">The invention is amenable to being embodied in a number of ways, including but not limited to the following descriptions.</p>
<p id="p-0034" num="0033">One embodiment of the invention is an image capture apparatus, comprising: (a) an imaging device (or image source) as a means for obtaining an image; (b) a computer processor coupled to the imaging device (or source); (c) memory coupled to the computer processor configured for retaining programming executable on the computer processor, wherein said computer and memory are a means for processing images; (d) a focus matching model retained in the memory, and (e) programming executable on the computer processor for carrying out the steps of, (e)(i) capturing (or receiving) multiple object images, which is a means for obtaining object images, (e)(ii) performing convolutions to model blur changes as a point spread function between the multiple object images, (e)(iii) determining blur difference within each convolution in response to performing a wavelet transform, obtaining wavelet variance and comparing differences (e.g., absolute differences) of wavelet variance for the multiple object images, and (e)(iv) performing depth estimation in response to the convolutions within the focus matching model.</p>
<p id="p-0035" num="0034">At least one embodiment of the invention utilizes a focus matching model based on imaging calibration targets obtained at different focal lengths. At least one embodiment of the invention is configured to utilize at least one size kernel when performing convolutions. For example, larger kernels may be used first to speed convergence, with progressively smaller kernels utilized to obtain the desired level of accuracy. At least one embodiment of the invention is configured for determining differences of wavelet variance, preferably an absolute difference of wavelet variances. At least one embodiment of the invention is configured for determining wavelet variance in at least one wavelet subband and at least one wavelet transform level. At least one embodiment of the invention is configured for determining wavelet variance in all wavelet subbands in at least one wavelet transform level. At least one embodiment of the invention is configured with the focus matching model utilizing a polynomial function, of any desired degree, to reduce mismatching noise. At least one embodiment of the invention is configured with coefficients of the polynomial function stored in memory.</p>
<p id="p-0036" num="0035">At least one embodiment of the invention is configured for performing optional histogram matching of the object images to reduce noise from outliers between focal positions prior to inputting the blur differences into the focus matching model. Histogram matching need not be used in applications subject to significantly complex motions. At least one embodiment of the invention is configured for performing histogram matching in response to the steps comprising: (a) sequentially shifting pixels from a first histogram to a second histogram to equalize the pixels of closest luminance; and (b) approximating histogram matching utilizing a linear matching function; wherein noise effects are reduced which have been introduced into the focus matching model in response to undesired physical and environmental variations.</p>
<p id="p-0037" num="0036">At least one embodiment of the invention is has a focus matching model generated in response to performing a calibration process on the apparatus in which a series of calibration target images are obtained for registering proper focus, with focus curves being obtained for the series of calibration target images; and a multi-dimensional model generated based on matching the focus curves for the series of calibration target images. At least one embodiment of the invention is configured with the imaging device comprising a still image camera, a video image camera, or a combination still and video image camera. At least one embodiment of the invention further comprises: (a) a focus control element coupled to the imaging device; (b) programming executable on the computer processor for adjusting the focus control element in response to performing depth estimation on object images based on inputting blur differences detected between object images into the focus matching model.</p>
<p id="p-0038" num="0037">One embodiment of the invention is an image capture apparatus, comprising: (a) an imaging device; (b) a computer processor coupled to the imaging device; (c) memory coupled to the computer processor configured for retaining programming executable on the computer processor; (d) a focus matching model based on imaging calibration targets at different focal lengths which is retained in the memory, and (e) programming executable on the computer processor for carrying out the steps of, (e)(i) capturing multiple object images, (e)(ii) performing convolutions by at least one size of convolution kernel to model blur changes as a point spread function between the multiple object images, (e)(iii) determining blur difference within each convolution in response to performing wavelet transform, obtaining wavelet variance and comparing differences of wavelet variance in at least one wavelet subband and at least one wavelet transform level, for the multiple object images, and (e)(iv) performing depth estimation in response to the convolutions within the focus matching model.</p>
<p id="p-0039" num="0038">One embodiment of the invention is a method of automatic estimation of camera-to-object focal depth, comprising: (a) generating a multi-dimensional focus matching model in response to detecting blur differences between multiple images of a calibration subject captured at different focal distances; (b) capturing multiple object images; (c) determining blur differences between the multiple object images in response to convolutions which model blur changes as a point spread function between the multiple object images; (d) determining blur difference within each convolution in response to performing wavelet transform, obtaining wavelet variance and comparing differences of wavelet variance for the multiple object images, and (e) performing depth estimation in response to the convolutions within the focus matching model.</p>
<p id="p-0040" num="0039">One embodiment of the invention is an apparatus and method for determining blur difference between images in response to: (a) performing convolutions by at least one size of convolution kernel to model blur changes as a point spread function between the multiple object images; and (b) determining blur difference within each convolution in response to performing wavelet transform, obtaining wavelet variance, and comparing differences of wavelet variance in at least one wavelet subband and at least one wavelet transform level, for the multiple object images.</p>
<p id="p-0041" num="0040">The present invention provides a number of beneficial elements which can be implemented either separately or in any desired combination without departing from the present teachings.</p>
<p id="p-0042" num="0041">An element of the invention is a method for estimating distance to a subject (subject-distance, or camera focal distance) in response to capturing multiple images (or otherwise obtaining images) and matching multiple images during characterization of the camera-lens system at multiple focal points.</p>
<p id="p-0043" num="0042">Another element of the invention is the use of distance estimation to estimate focus, or to control focus adjustments, within a camera system.</p>
<p id="p-0044" num="0043">Another element of the invention is a subject-distance estimation method which can estimate distance in response to the input of images representing at least two focal settings taken of an image at a given subject-distance.</p>
<p id="p-0045" num="0044">Another element of the invention is the use of an image comparison process which is motion robust, as it does not perform a comparison of pixels in corresponding positions in the images being compared.</p>
<p id="p-0046" num="0045">Another element of the invention is a subject-distance estimation method which determines blur difference based on wavelet transforms applied to the subject images whose variance is compared.</p>
<p id="p-0047" num="0046">Another element of the invention is a subject-distance estimation method which only requires the use of two image inputs for estimating a subject-distance, although additional inputs can be utilized for increasing estimation accuracy as desired, or for successive and/or continuous estimations.</p>
<p id="p-0048" num="0047">Another element of the invention is a subject-distance estimation method in which multiple images with different focal settings are captured of an image having a fixed subject-distance, and blur information from these images is plugged into (processed through or by) the focus matching model which is solved for distance to generate an estimate of the actual subject-distance.</p>
<p id="p-0049" num="0048">Another element of the invention is a subject-distance estimating method or apparatus which adopts a polynomial model to represent the empirical focus matching model.</p>
<p id="p-0050" num="0049">Another element of the invention is a histogram matching method in which pixels are sequentially shifted from one histogram to the other to equalize the pixels of closest luminance in the other histogram to reduce the effects of noise introduced into the model in response to undesired physical and environmental variations, and is approximated by a linear matching function.</p>
<p id="p-0051" num="0050">Another element of the invention is a subject-distance estimation apparatus and method which can be utilized for single focal point lenses, discrete focal point lenses (e.g., normal and macro settings), or continuously variable focal point lenses (e.g., zoom lenses).</p>
<p id="p-0052" num="0051">Another element of the invention is a distance estimation apparatus and method in which a focus matching model can be generated for each discrete magnification setting of a camera, or at incremental positions along a continuously variable magnification (zoom) range.</p>
<p id="p-0053" num="0052">A still further element of the invention is that depth estimation and focus can be determined for a wide range of imaging apparatus (e.g., still and/or video camera devices) configured for capturing images at different focus and zoom settings.</p>
<p id="p-0054" num="0053">Further elements of the invention will be brought out in the following portions of the specification, wherein the detailed description is for the purpose of fully disclosing preferred embodiments of the invention without placing limitations thereon.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0007" level="1">BRIEF DESCRIPTION OF THE SEVERAL VIEWS OF THE DRAWING(S)</heading>
<p id="p-0055" num="0054">The invention will be more fully understood by reference to the following drawings which are for illustrative purposes only:</p>
<p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. 1</figref> is a schematic of capturing multiple images at multiple focal points according to an element of the present invention.</p>
<p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. 2A</figref> and <figref idref="DRAWINGS">FIG. 2B</figref> are schematic comparisons of calibration target (e.g., step edge) images according to an element of the present invention.</p>
<p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. 3</figref> is a schematic of computing blur difference in three iterations according to an element of the present invention.</p>
<p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. 4</figref> is a graph of a matching curve collected according to an element of the present invention and showing the inclusion of outliers and noise.</p>
<p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. 5</figref> is a pyramid representation of a wavelet transform utilized according to an element of the present invention, showing subbands and a three level transform structure.</p>
<p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. 6</figref> is a flowchart of wavelet-based blur difference determination according to an embodiment of the present invention.</p>
<p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. 7</figref> is a histogram of mismatching between successive subject-distances according to an element of the present invention.</p>
<p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. 8</figref> is a magnified histogram showing a portion of the histogram depicted in <figref idref="DRAWINGS">FIG. 7</figref>.</p>
<p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. 9</figref> is a graph of a matching curve showing matching before and after histogram matching according to the present invention.</p>
<p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. 10</figref> is a graph of a matching curve across fifteen different distances showing matching before and after histogram matching according to an element of the present invention.</p>
<p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. 11</figref> is a graph of a matching curve showing the use of bi-quadratic fitting according to an element of the present invention.</p>
<p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. 12</figref> is a graph of a matching curve showing the use of bi-cubic fitting according to an element of the present invention.</p>
<p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. 13</figref> is a flowchart of calibration according to an element of the present invention.</p>
<p id="p-0069" num="0068"><figref idref="DRAWINGS">FIG. 14</figref> is a flowchart of camera depth estimation based on two picture matching according to an element of the present invention.</p>
<p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. 15</figref> is a flowchart of histogram matching according to an element of the present invention.</p>
<p id="p-0071" num="0070"><figref idref="DRAWINGS">FIG. 16</figref> is a flowchart of depth estimation according to an embodiment of the present invention.</p>
<p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. 17</figref> is a block diagram of an image capture apparatus configured for performing depth estimation according to an element of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0008" level="1">DETAILED DESCRIPTION OF THE INVENTION</heading>
<p id="p-0073" num="0072">1. Blur Difference.</p>
<p id="p-0074" num="0073">In considering blur differences between images, it will be recognized that when a subject is in optimal focus, the captured image is the sharpest and accordingly provides the highest contrast in relation to images captured at less than optimal focus. The subject becomes increasingly blurry (less contrast) as the lens moves away from the in-focus position. Generally, when two pictures are captured (taken) of a specific subject at two different focus distances, the image captured closer to the subject distance is sharper than the other. The focus distances at which the pictures are taken and the amount of blur difference between these two pictures can be used in a proper model, calibrated for the specific camera model and/or make, to estimate the actual subject distance, or depth. In the present invention this depth estimation is performed in response to a polynomial model.</p>
<p id="p-0075" num="0074"><figref idref="DRAWINGS">FIG. 1</figref> illustrates an embodiment <b>10</b> in which multiple images are captured of a calibration target (or calibration subject), at different focal positions (subject-distances) when collecting a data set for a given imaging apparatus (e.g., specific embodiment, make or model of camera, or a family of cameras using the same/similar optical imaging elements). Collecting the data set comprises a characterization process for the camera-lens system at a given magnification setting (lens at a fixed focal length, zoom setting). An imaging device (camera) <b>12</b> is shown which can focus from a minimum focal length <b>14</b> on out to infinity <b>16</b>. Minimum focal distance <b>14</b> (e.g., in this case 35 cm) is shown as well as focus at infinity <b>16</b>. According to the invention, the focus converges to first focal position <b>18</b> and then to a second focal position <b>20</b>, upon a calibration target <b>22</b>, such as step-edge image, slate, graticule, or similar target having known optical characteristics, along focal path <b>24</b>.</p>
<p id="p-0076" num="0075">By way of example and not limitation, a Sony DSC-R1 camera was used herein to illustrate the inventive method, although one of ordinary skill in the art will appreciate the method can be utilized with other digital still and/or video cameras. The focusing distance of this camera ranges between the minimal focus distance (e.g., 35 cm for Sony DSC-R1) to infinity.</p>
<p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. 2A</figref> depicts a condition <b>30</b> in which subject <b>32</b> is in focus, wherein the captured image is the sharpest, as represented by the sharp contrast curve <b>34</b>, which is also referred to as the &#x201c;edge profile&#x201d; of the step edge. It will be appreciated that the calibration target, or subject, preferably provides a mechanism for simply determining the sharpness of focus based on contrast. For example in a step-edge target, a clear step-edge delineation is made between at least two colors, shades, luminances, wherein the sharpness of focus can be readily determined from the sharpness of the contrast profile. It will be appreciated by one of ordinary skill in the art that the target can be configured in any of a number of different ways, in a manner similar to the use of different chroma keys and color bar patterns in testing different elements of video capture and output.</p>
<p id="p-0078" num="0077"><figref idref="DRAWINGS">FIG. 2B</figref> depicts the condition <b>36</b> as the image of object <b>38</b> becomes increasingly blurry as the lens moves away from the &#x2018;in-focus&#x2019; position, with a resulting sloped contrast curve <b>40</b> shown. Generally, when two pictures are taken at two different focal distances, the one taken closer to the subject-distance is sharper than the other. The focal distances at which the pictures are taken and the amount of the blur difference between these two pictures can be used to estimate the actual subject distance, or depth.</p>
<p id="p-0079" num="0078">Consider a blur difference determination in which two pictures f<sub>A </sub>and f<sub>B </sub>are taken at positions A and B, with f<sub>A </sub>being sharper than f<sub>B</sub>. The blur change can be modeled by a point spread function P from position A to position B according to
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>f</i><sub>A</sub><i>*P=f</i><sub>B </sub><?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where * denotes the operation of two dimensional convolution. Furthermore, the point spread function P can be approximated by using a series of convolutions by a blur kernel K according to
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>P=K*K* . . . *K.</i>&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0080" num="0079">For example a blur kernel K may be chosen as</p>
<p id="p-0081" num="0080">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mi>K</mi>
        <mo>=</mo>
        <mrow>
          <mfrac>
            <mn>1</mn>
            <mn>64</mn>
          </mfrac>
          <mo>&#x2062;</mo>
          <mrow>
            <mo>(</mo>
            <mtable>
              <mtr>
                <mtd>
                  <mn>1</mn>
                </mtd>
                <mtd>
                  <mn>6</mn>
                </mtd>
                <mtd>
                  <mn>1</mn>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mn>6</mn>
                </mtd>
                <mtd>
                  <mn>36</mn>
                </mtd>
                <mtd>
                  <mn>6</mn>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mn>1</mn>
                </mtd>
                <mtd>
                  <mn>6</mn>
                </mtd>
                <mtd>
                  <mn>1</mn>
                </mtd>
              </mtr>
            </mtable>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>2</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
in which the amount of blur difference between f<sub>A </sub>and f<sub>B </sub>can be evaluated on the basis of how many convolutions are performed in Eq. (1). In actual implementation, the blur difference is more preferably obtained by an iterative process.
</p>
<p id="p-0082" num="0081"><figref idref="DRAWINGS">FIG. 3</figref> illustrates an iteration process, herein exemplified with three iterations performed between picture f<sub>A </sub>(left) and picture f<sub>B </sub>(right).</p>
<p id="p-0083" num="0082"><figref idref="DRAWINGS">FIG. 4</figref> depicts a matching curve obtained for an image of a step-edge placed at a fixed distance (e.g., 100 cm). A first picture of the sequence is taken at the focus distance of infinity, then one picture is taken every time the lens is moved to focus at one depth of field closer, until the focus distance reaches the minimal focus distance. This sequence of pictures is denoted by f<sub>0</sub>, f<sub>1</sub>, . . . , f<sub>N-1</sub>, where N is the length of the sequence. In practice, to ensure the sequence covers the whole focus range, f<sub>0 </sub>preferably starts at the distance slightly further than infinity, and f<sub>N-1 </sub>is slightly closer than the specified minimal focus distance. These results were achieved using the DSC-R1 camera configured with software for controlling camera steps and sequences.</p>
<p id="p-0084" num="0083">For a given focal depth, in order to find the relationship between the iteration number and the focus position, a sequence of pictures is taken for the whole focal range of the camera from which the blur difference between every two pictures can be calculated.</p>
<p id="p-0085" num="0084">It should be appreciated what is meant by the iterations and in particular negative iteration numbers, as will be seen represented in certain figures (e.g., <figref idref="DRAWINGS">FIG. 4</figref>). Positive iteration numbers indicate that f<sub>A </sub>is sharper than f<sub>B</sub>.</p>
<p id="p-0086" num="0085">2. Correcting Motion Problems with Blur Difference Determinations.</p>
<p id="p-0087" num="0086">Instead of relying on the use of the norm operator for blur difference calculations, the present invention applies wavelet transforms after an image convolution stage and compares absolute variances between these transforms.</p>
<p id="p-0088" num="0087">Existing blur matching methods have difficulty with providing accurate results when motion arises between the images being compared when using the blur comparisons based on the norm operator. For example, motion can arise with respect to movement of the subject itself, motion of the camera, or a combination of these motions. Certain forms of motion, such as rotation and shape changing, are particularly troublesome for existing blur matching methods.</p>
<p id="p-0089" num="0088">The present invention provides an improved mechanism for estimating blur differences between images. This comparison between images does not depend on the pixel correspondence between the two pictures, and is thus referred to as being motion robust. Use of the norm operator for calculating differences between two images, generally involves computing a difference between every two pixel values at the corresponding locations in two images. However, the norm operator approach does not provide accurate results if motion arises and the object locations change in the two images.</p>
<p id="p-0090" num="0089">In general terms evaluating the difference between two images f<sub>A </sub>and f<sub>B </sub>can be expressed as a measure G(f<sub>A</sub>,f<sub>B</sub>). This measure G(f<sub>A</sub>,f<sub>B</sub>) was previously determined in response to using a norm operator &#x2225;f<sub>A</sub>&#x2212;f<sub>B</sub>&#x2225;, such as in certain prior applications of the inventor. However, in the present invention G(f<sub>A</sub>,f<sub>B</sub>) is determined in response to performing a wavelet transform on the images and then determining the variance between the wavelet transforms. If the variances s<sub>A</sub><sup>2 </sup>and s<sub>B</sub><sup>2 </sup>are calculated from the wavelet coefficients for f<sub>A </sub>and f<sub>B </sub>respectively, then
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>G</i>(<i>f</i><sub>A</sub><i>,f</i><sub>B</sub>)=|<i>s</i><sup>2</sup><sub>A</sub><i>&#x2212;s</i><sub>B</sub><sup>2</sup>|<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
is the difference measure between two pictures as utilized in the present invention for determining blur difference.
</p>
<p id="p-0091" num="0090">Accordingly, the absolute value of the iteration number can be calculated in the present invention by using the equation:</p>
<p id="p-0092" num="0091">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mrow>
  <msub>
    <mi>I</mi>
    <mi>A_B</mi>
  </msub>
  <mo>=</mo>
  <mrow>
    <munder>
      <mrow>
        <mi>arg</mi>
        <mo>&#x2062;</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mi>min</mi>
        <mo>&#x2062;</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mi>G</mi>
      </mrow>
      <mi>I</mi>
    </munder>
    <mo>&#x2062;</mo>
    <mrow>
      <mo>(</mo>
      <mrow>
        <mrow>
          <msub>
            <mi>f</mi>
            <mi>A</mi>
          </msub>
          <mo>&#x2062;</mo>
          <munder>
            <mrow>
              <msup>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>*</mo>
              </msup>
              <mo>&#x2062;</mo>
              <msup>
                <mi>K</mi>
                <mo>*</mo>
              </msup>
              <mo>&#x2062;</mo>
              <msup>
                <mi>K</mi>
                <mo>*</mo>
              </msup>
              <mo>&#x2062;</mo>
              <msup>
                <mi>&#x2026;</mi>
                <mo>*</mo>
              </msup>
              <mo>&#x2062;</mo>
              <mi>K</mi>
            </mrow>
            <munder>
              <mi>&#xfe38;</mi>
              <mrow>
                <mi>I</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.8em" height="0.8ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mi>convolutions</mi>
              </mrow>
            </munder>
          </munder>
        </mrow>
        <mo>,</mo>
        <msub>
          <mi>f</mi>
          <mi>B</mi>
        </msub>
      </mrow>
      <mo>)</mo>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
<br/>
and convolutions are performed, expressed by operator * with a kernel K.
</p>
<p id="p-0093" num="0092">The wavelet variance is determined for at least one subband in at least one level of the wavelet transform structure. In the present implementation, the variance was determined for all coefficients at a specific level, in this case the first level for the image, which by way of example has a size of 488&#xd7;273 pixels. This means that the variance is calculated for all the coefficients in subbands LH<sub>1</sub>, HL<sub>1</sub>, HH<sub>1 </sub>at this first level in this specific implementation.</p>
<p id="p-0094" num="0093"><figref idref="DRAWINGS">FIG. 5</figref> depicts a pyramid representation of a wavelet transform. It will be noted that each level has three subbands: LH, HL, HH. It will be appreciated that each level of wavelet transform generates four subbands: LL, LH, HL, HH, with the subsequent level wavelet transform is taken on the LL subband of the current level, wherein it transforms this LL subband into the next level of LL, LH, HL, HH subbands. and so forth. In general, LH, HL, HH subbands provide contrast or sharpness information of the image in different scales of resolutions. The figure illustrates a wavelet transform structure having three levels with LH<sub>1</sub>, HL<sub>1</sub>, HH<sub>1 </sub>in a first level, LH<sub>2</sub>, HL<sub>2</sub>, HH<sub>2</sub>, and LH<sub>3</sub>, HL<sub>3</sub>, HH<sub>3 </sub>in a third level.</p>
<p id="p-0095" num="0094">The following first considers each convolution of f<sub>A </sub>and K, after which a wavelet transform is performed on this blurred image. A variance is then determined (e.g., calculated) for the wavelet coefficients. According to one implementation the Haar wavelet is utilized for computational efficiency and the variance is determined for coefficients in all subbands (e.g., subbands LH, HL, and HH in <figref idref="DRAWINGS">FIG. 5</figref>) is determined at a specific wavelet transform level. Implementations of the invention can be practiced, however, in response to determining variance for at least one of the subbands within at least one of the wavelet transform levels (e.g., levels 1, 2 and/or 3 in the example). It will be appreciated that the inventive technique may be applied in response to different types of wavelet computations, consequently implementation is not limited to the use of the Haar transform.</p>
<p id="p-0096" num="0095">In addition, the variance may be computed in different ways, such as with respect to different subband levels, edges and so forth without departing from the teachings of the present invention. In general, the invention can be practiced without the difference measure G(f<sub>A</sub>,f<sub>B</sub>) being a norm operator or difference of the variances of the wavelet coefficients. For instance, it can be a difference of contrast or sharpness measures between the two images. However, performance varies in regard to the use of different forms of G(f<sub>A</sub>,f<sub>B</sub>) computations. One object of the present invention is to provide an accurate and efficient means for determining that blur difference in a motion robust form.</p>
<p id="p-0097" num="0096">In similar manner, a wavelet transform is applied to f<sub>B </sub>and again variance is determined of the wavelet coefficients.</p>
<p id="p-0098" num="0097">After wavelet variances are determined for each image, then the absolute difference is determined between the above two variances for the blurred image of f<sub>A </sub>and f<sub>B</sub>. This difference is considered the picture difference and replaces the norm calculation utilized in previous blur difference calculations. This difference does not depend on the pixel correspondence between the two images and therefore provides motion robust results in response to various type of motions, such as rotation and shape changing.</p>
<p id="p-0099" num="0098"><figref idref="DRAWINGS">FIG. 6</figref> illustrates an example embodiment of using convolutions in combination with wavelet transforms and wavelet variances to obtain blur differences. It will be appreciated that blur difference determination is utilized within a number of applications, such as the depth estimation. Blur change is modeled <b>50</b> as a point spread function which are approximated by performing convolutions <b>52</b> on the respective images. Wavelet transforms are applied <b>54</b> to the images and then a variance is determined <b>56</b> for each. Difference between the variances is determined <b>58</b>, such as an absolute difference, which is used as a measure of the blur difference. Then the convolutions, wavelet transforms, and variances are repeated <b>60</b>, until a desired end condition is met, such as convergence, reaching a maximum number of convolutions, or other desired end condition. It should be appreciated that the above convolutions can be performed in response to a single blur kernel, or in response to decreasing the kernel as convolution progress, as described in another invention by the applicant. In addition, the elements of the present invention can be applied to any application which utilizes the norm operator, or similar function, for determining blur difference after a convolution.</p>
<p id="p-0100" num="0099">It should be appreciated that as the wavelet transform can be noise sensitive, in at least one embodiment of the invention a low-pass filter is applied to both images f<sub>A </sub>and f<sub>B </sub>before blur matching, such as just before block <b>52</b> of <figref idref="DRAWINGS">FIG. 6</figref>. Filtering should be performed just once on f<sub>A </sub>and f<sub>B </sub>before any convolution and wavelet transform.</p>
<p id="p-0101" num="0100">Determination of blur difference according to this element of the invention can be applied to a wide range of image processing systems and devices, such as those related to absolute image focus, comparative image focusing, depth estimations, stereoscopic image processing, adjustment of image focus, and other electronic devices which can benefit from a rapid, accurate and motion robust means of determining blur difference.</p>
<p id="p-0102" num="0101">3. Determining the Sharper Image.</p>
<p id="p-0103" num="0102">It should be noted that when the two pictures are taken, it is unknown a priori which one of f<sub>A </sub>or f<sub>B </sub>is sharper, and thus is closer to the proper focal distance. Accordingly the method is configured to compute both Eqs. 3 and 4 below.</p>
<p id="p-0104" num="0103">
<maths id="MATH-US-00003" num="00003">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <msub>
          <mi>I</mi>
          <mn>1</mn>
        </msub>
        <mo>=</mo>
        <mrow>
          <munder>
            <mrow>
              <mi>arg</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.8em" height="0.8ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mi>min</mi>
            </mrow>
            <mi>I</mi>
          </munder>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mrow>
            <mi>G</mi>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mrow>
                  <msub>
                    <mi>f</mi>
                    <mi>A</mi>
                  </msub>
                  <mo>&#x2062;</mo>
                  <munder>
                    <mrow>
                      <msup>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>*</mo>
                      </msup>
                      <mo>&#x2062;</mo>
                      <msup>
                        <mi>K</mi>
                        <mo>*</mo>
                      </msup>
                      <mo>&#x2062;</mo>
                      <msup>
                        <mi>K</mi>
                        <mo>*</mo>
                      </msup>
                      <mo>&#x2062;</mo>
                      <msup>
                        <mi>&#x2026;</mi>
                        <mo>*</mo>
                      </msup>
                      <mo>&#x2062;</mo>
                      <mi>K</mi>
                    </mrow>
                    <munder>
                      <mi>&#xfe38;</mi>
                      <mrow>
                        <mi>I</mi>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.8em" height="0.8ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <mi>convolutions</mi>
                      </mrow>
                    </munder>
                  </munder>
                </mrow>
                <mo>,</mo>
                <msub>
                  <mi>f</mi>
                  <mi>B</mi>
                </msub>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>3</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <msub>
          <mi>I</mi>
          <mn>2</mn>
        </msub>
        <mo>=</mo>
        <mrow>
          <munder>
            <mrow>
              <mi>arg</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.8em" height="0.8ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mi>min</mi>
            </mrow>
            <mi>I</mi>
          </munder>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mrow>
            <mi>G</mi>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mrow>
                  <msub>
                    <mi>f</mi>
                    <mi>B</mi>
                  </msub>
                  <mo>&#x2062;</mo>
                  <munder>
                    <mrow>
                      <msup>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>*</mo>
                      </msup>
                      <mo>&#x2062;</mo>
                      <msup>
                        <mi>K</mi>
                        <mo>*</mo>
                      </msup>
                      <mo>&#x2062;</mo>
                      <msup>
                        <mi>K</mi>
                        <mo>*</mo>
                      </msup>
                      <mo>&#x2062;</mo>
                      <msup>
                        <mi>&#x2026;</mi>
                        <mo>*</mo>
                      </msup>
                      <mo>&#x2062;</mo>
                      <mi>K</mi>
                    </mrow>
                    <munder>
                      <mi>&#xfe38;</mi>
                      <mrow>
                        <mi>I</mi>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.8em" height="0.8ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <mi>convolutions</mi>
                      </mrow>
                    </munder>
                  </munder>
                </mrow>
                <mo>,</mo>
                <msub>
                  <mi>f</mi>
                  <mi>A</mi>
                </msub>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>4</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0105" num="0104">If I<sub>1 </sub>is larger than I<sub>2</sub>, then f<sub>A </sub>is sharper than f<sub>B</sub>, wherein the value of iteration number (as in <figref idref="DRAWINGS">FIG. 4</figref>) will be I<sub>I</sub>. Otherwise if I<sub>2 </sub>is larger than I<sub>1</sub>, then f<sub>B </sub>is sharper than f<sub>A</sub>, and the value of iteration number (e.g., as in <figref idref="DRAWINGS">FIG. 4</figref>) will be &#x2212;I<sub>2</sub>. If I<sub>1 </sub>and I<sub>2 </sub>are equal, then the errors are compared, such as according to the following:</p>
<p id="p-0106" num="0105">
<maths id="MATH-US-00004" num="00004">
<math overflow="scroll">
  <mrow>
    <mrow>
      <msub>
        <mi>e</mi>
        <mn>1</mn>
      </msub>
      <mo>=</mo>
      <mrow>
        <mi>G</mi>
        <mo>&#x2061;</mo>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mrow>
              <msub>
                <mi>f</mi>
                <mi>A</mi>
              </msub>
              <mo>&#x2062;</mo>
              <munder>
                <mrow>
                  <msup>
                    <mstyle>
                      <mspace width="0.3em" height="0.3ex"/>
                    </mstyle>
                    <mo>*</mo>
                  </msup>
                  <mo>&#x2062;</mo>
                  <msup>
                    <mi>K</mi>
                    <mo>*</mo>
                  </msup>
                  <mo>&#x2062;</mo>
                  <msup>
                    <mi>K</mi>
                    <mo>*</mo>
                  </msup>
                  <mo>&#x2062;</mo>
                  <msup>
                    <mi>&#x2026;</mi>
                    <mo>*</mo>
                  </msup>
                  <mo>&#x2062;</mo>
                  <mi>K</mi>
                </mrow>
                <munder>
                  <mi>&#xfe38;</mi>
                  <mrow>
                    <msub>
                      <mi>I</mi>
                      <mn>1</mn>
                    </msub>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.8em" height="0.8ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mi>convolutions</mi>
                  </mrow>
                </munder>
              </munder>
            </mrow>
            <mo>,</mo>
            <msub>
              <mi>f</mi>
              <mi>B</mi>
            </msub>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mrow>
    <mo>;</mo>
    <mi>and</mi>
  </mrow>
</math>
</maths>
<maths id="MATH-US-00004-2" num="00004.2">
<math overflow="scroll">
  <mrow>
    <msub>
      <mi>e</mi>
      <mn>2</mn>
    </msub>
    <mo>=</mo>
    <mrow>
      <mrow>
        <mi>G</mi>
        <mo>&#x2061;</mo>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mrow>
              <msub>
                <mi>f</mi>
                <mi>B</mi>
              </msub>
              <mo>&#x2062;</mo>
              <munder>
                <mrow>
                  <msup>
                    <mstyle>
                      <mspace width="0.3em" height="0.3ex"/>
                    </mstyle>
                    <mo>*</mo>
                  </msup>
                  <mo>&#x2062;</mo>
                  <msup>
                    <mi>K</mi>
                    <mo>*</mo>
                  </msup>
                  <mo>&#x2062;</mo>
                  <msup>
                    <mi>K</mi>
                    <mo>*</mo>
                  </msup>
                  <mo>&#x2062;</mo>
                  <msup>
                    <mi>&#x2026;</mi>
                    <mo>*</mo>
                  </msup>
                  <mo>&#x2062;</mo>
                  <mi>K</mi>
                </mrow>
                <munder>
                  <mi>&#xfe38;</mi>
                  <mrow>
                    <msub>
                      <mi>I</mi>
                      <mn>2</mn>
                    </msub>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.8em" height="0.8ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mi>convolutions</mi>
                  </mrow>
                </munder>
              </munder>
            </mrow>
            <mo>,</mo>
            <msub>
              <mi>f</mi>
              <mi>A</mi>
            </msub>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mrow>
      <mo>.</mo>
    </mrow>
  </mrow>
</math>
</maths>
</p>
<p id="p-0107" num="0106">If e<sub>1 </sub>is smaller than e<sub>2</sub>, then f<sub>A </sub>is sharper than f<sub>B</sub>; otherwise e<sub>2 </sub>is smaller wherein f<sub>B </sub>is sharper than f<sub>A</sub>. Alternatively, the variance values of the wavelet coefficients of f<sub>A </sub>and f<sub>B </sub>can be simply used to determine which image is sharper. The one with the larger variance value of the wavelet coefficients is the sharper image, which can be performed more readily with similar results than prior approaches.</p>
<p id="p-0108" num="0107">The relationship between iteration number and focal positions for the depth of 100 cm is shown in <figref idref="DRAWINGS">FIG. 4</figref>. The blur difference between every two pictures f<sub>A</sub>, and f<sub>B </sub>for i=0, . . . , N&#x2212;2 is calculated. The &#x201c;picture number&#x201d; axis indicates the image pairs for which the iteration number is calculated. For example, picture number 0 means that the iteration number is calculated between f<sub>0 </sub>and f<sub>1</sub>. It can be seen that the absolute value of the number of iterations increases when the lens focus position moves away from the subject distance. The zero-crossing point is where the subject is in focus.</p>
<p id="p-0109" num="0108"><figref idref="DRAWINGS">FIG. 7</figref> and <figref idref="DRAWINGS">FIG. 8</figref> compares the histograms for pictures <b>138</b> and <b>139</b> from <figref idref="DRAWINGS">FIG. 4</figref>, wherein significant mismatching is noted. It will be appreciated that this mismatching should be removed before subject-distance can be accurately computed based on blur.</p>
<p id="p-0110" num="0109">4. Histogram Matching.</p>
<p id="p-0111" num="0110">To correct for mismatching between images being compared, a matching procedure can be performed by modifying one histogram to match the other one. It will be appreciated that a simple linear matching function can be utilized, although other functions can be utilized. Pixels are sequentially shifted from one histogram to equalize the number of pixels of the closest luminance of the other histogram. In response to the shifting of pixels between two histograms, the matching function is determined, such as using a least squared error solution. Afterwards the histogram matching function is applied to the two pictures before the focusing matching is performed. It should be appreciated that histogram matching does not work well when the motions become complicated, wherein this is an optional step of the process which need not be applied to every application of the instant invention.</p>
<p id="p-0112" num="0111"><figref idref="DRAWINGS">FIG. 9</figref> and <figref idref="DRAWINGS">FIG. 10</figref> depict iteration curves for different subject depths without histogram matching being applied before focus matching as shown in the solid lines, and with histogram matching as shown in the dashed lines. <figref idref="DRAWINGS">FIG. 9</figref> depicts a single example while <figref idref="DRAWINGS">FIG. 10</figref> depicts the iteration curves for fifteen different distances. The plots for <figref idref="DRAWINGS">FIG. 10</figref> were generated by placing a step-edge at distances of infinity, 1000, 500, 300, 200, 150, 125, 100, 80, 70, 60, 50, 45, 40 and 35 cm, respectively. The iteration number I can be written as a function F of focus distance L and subject depth D.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>I=F</i>(<i>L,D</i>),&#x2003;&#x2003;(5)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where L and D are both measured by picture number, which physically means the number of depths of field measured from infinity, or from where picture 0 is defined. Depth estimation is a process to determine D given I and L. On the data shown in <figref idref="DRAWINGS">FIG. 10</figref>, Eq. 5 is used to model depth estimation.
</p>
<p id="p-0113" num="0112">The data shown in <figref idref="DRAWINGS">FIG. 4</figref>, <figref idref="DRAWINGS">FIG. 9</figref> and <figref idref="DRAWINGS">FIG. 10</figref> manifest significant signal noise. For instance, in <figref idref="DRAWINGS">FIG. 4</figref>, <figref idref="DRAWINGS">FIG. 7</figref> and <figref idref="DRAWINGS">FIG. 8</figref>, noticeable outliers are seen at picture number <b>139</b>. The source of these outliers may include changes of lighting conditions and aperture variations during the capturing process, as well as other physical camera and environmental variations.</p>
<p id="p-0114" num="0113">It will be appreciated that in view of the mismatching seen in these figures, the histogram matching technique is applied to the images before the blur difference is calculated. Let h<sub>1 </sub>and h<sub>2 </sub>denote the histograms of two different images f<sub>1 </sub>and f<sub>2</sub>, respectively. Consider h<sub>1 </sub>as the reference histogram and h<sub>2 </sub>as the histogram to be modified to match h<sub>1</sub>, wherein the following steps are performed.</p>
<p id="p-0115" num="0114">(1) A pixel mapping matrix w(i,j) is generated.</p>
<p id="p-0116" num="0115">(2) Setting w(i,j)=0 for every i and j ranging from 0 to the maximum gray level M.</p>
<p id="p-0117" num="0116">(3) Find the smallest i that satisfies h<sub>1</sub>(i)&#x3e;0, and find the smallest j that satisfies h<sub>2</sub>(j)&#x3e;0.</p>
<p id="p-0118" num="0117">(4) If h<sub>2 </sub>(j)&#x2267;h<sub>1</sub>(i), set w(i,j)=h<sub>1</sub>(i), update h<sub>2</sub>(j) by h<sub>2</sub>(j)&#x2190;h<sub>2</sub>(j)&#x2212;h<sub>1</sub>(i), and set h<sub>1</sub>(i)=0.</p>
<p id="p-0119" num="0118">Else If h<sub>2</sub>(j)&#x3c;h<sub>1</sub>(i), set w(i,j)=h<sub>2</sub>(j), update h<sub>1</sub>(i) by h<sub>1</sub>(i)&#x2190;h<sub>1</sub>(i)&#x2212;h<sub>2</sub>(j), and set h<sub>2</sub>(j)=0.</p>
<p id="p-0120" num="0119">Steps 3 and 4 are then repeated until both h<sub>1 </sub>and h<sub>2 </sub>become 0 for all gray levels, which arises in response to the two pictures having the same number of pixels.</p>
<p id="p-0121" num="0120">After the mapping matrix w(i,j) is created, a linear matching function H(x)=ax+b is constructed, such as using a weighted least squares regression method, where a and b are computed as follows:</p>
<p id="p-0122" num="0121">
<maths id="MATH-US-00005" num="00005">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mi>b</mi>
        <mo>=</mo>
        <mfrac>
          <mrow>
            <mrow>
              <munderover>
                <mo>&#x2211;</mo>
                <mrow>
                  <mi>i</mi>
                  <mo>=</mo>
                  <mn>0</mn>
                </mrow>
                <mi>M</mi>
              </munderover>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mrow>
                <munderover>
                  <mo>&#x2211;</mo>
                  <mrow>
                    <mi>j</mi>
                    <mo>=</mo>
                    <mn>0</mn>
                  </mrow>
                  <mi>M</mi>
                </munderover>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mrow>
                  <mrow>
                    <mi>w</mi>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mi>i</mi>
                        <mo>,</mo>
                        <mi>j</mi>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mo>&#x2062;</mo>
                  <mrow>
                    <munderover>
                      <mo>&#x2211;</mo>
                      <mrow>
                        <mi>i</mi>
                        <mo>=</mo>
                        <mn>0</mn>
                      </mrow>
                      <mi>M</mi>
                    </munderover>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.3em" height="0.3ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mrow>
                      <munderover>
                        <mo>&#x2211;</mo>
                        <mrow>
                          <mi>j</mi>
                          <mo>=</mo>
                          <mn>0</mn>
                        </mrow>
                        <mi>M</mi>
                      </munderover>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <mrow>
                        <mo>[</mo>
                        <mrow>
                          <mrow>
                            <mi>w</mi>
                            <mo>&#x2061;</mo>
                            <mrow>
                              <mo>(</mo>
                              <mrow>
                                <mi>i</mi>
                                <mo>,</mo>
                                <mi>j</mi>
                              </mrow>
                              <mo>)</mo>
                            </mrow>
                          </mrow>
                          <mo>&#x2062;</mo>
                          <mi>ij</mi>
                        </mrow>
                        <mo>]</mo>
                      </mrow>
                    </mrow>
                  </mrow>
                </mrow>
              </mrow>
            </mrow>
            <mo>-</mo>
            <mrow>
              <munderover>
                <mo>&#x2211;</mo>
                <mrow>
                  <mi>i</mi>
                  <mo>=</mo>
                  <mn>0</mn>
                </mrow>
                <mi>M</mi>
              </munderover>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mrow>
                <munderover>
                  <mo>&#x2211;</mo>
                  <mrow>
                    <mi>j</mi>
                    <mo>=</mo>
                    <mn>0</mn>
                  </mrow>
                  <mi>M</mi>
                </munderover>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mrow>
                  <mrow>
                    <mo>[</mo>
                    <mrow>
                      <mrow>
                        <mi>w</mi>
                        <mo>&#x2061;</mo>
                        <mrow>
                          <mo>(</mo>
                          <mrow>
                            <mi>i</mi>
                            <mo>,</mo>
                            <mi>j</mi>
                          </mrow>
                          <mo>)</mo>
                        </mrow>
                      </mrow>
                      <mo>&#x2062;</mo>
                      <mi>i</mi>
                    </mrow>
                    <mo>]</mo>
                  </mrow>
                  <mo>&#x2062;</mo>
                  <mrow>
                    <munderover>
                      <mo>&#x2211;</mo>
                      <mrow>
                        <mi>i</mi>
                        <mo>=</mo>
                        <mn>0</mn>
                      </mrow>
                      <mi>M</mi>
                    </munderover>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.3em" height="0.3ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mrow>
                      <munderover>
                        <mo>&#x2211;</mo>
                        <mrow>
                          <mi>j</mi>
                          <mo>=</mo>
                          <mn>0</mn>
                        </mrow>
                        <mi>M</mi>
                      </munderover>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <mrow>
                        <mo>[</mo>
                        <mrow>
                          <mrow>
                            <mi>w</mi>
                            <mo>&#x2061;</mo>
                            <mrow>
                              <mo>(</mo>
                              <mrow>
                                <mi>i</mi>
                                <mo>,</mo>
                                <mi>j</mi>
                              </mrow>
                              <mo>)</mo>
                            </mrow>
                          </mrow>
                          <mo>&#x2062;</mo>
                          <mi>j</mi>
                        </mrow>
                        <mo>]</mo>
                      </mrow>
                    </mrow>
                  </mrow>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
          <mrow>
            <mrow>
              <munderover>
                <mo>&#x2211;</mo>
                <mrow>
                  <mi>i</mi>
                  <mo>=</mo>
                  <mn>0</mn>
                </mrow>
                <mi>M</mi>
              </munderover>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mrow>
                <munderover>
                  <mo>&#x2211;</mo>
                  <mrow>
                    <mi>j</mi>
                    <mo>=</mo>
                    <mn>0</mn>
                  </mrow>
                  <mi>M</mi>
                </munderover>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mrow>
                  <mrow>
                    <mi>w</mi>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mi>i</mi>
                        <mo>,</mo>
                        <mi>j</mi>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mo>&#x2062;</mo>
                  <mrow>
                    <munderover>
                      <mo>&#x2211;</mo>
                      <mrow>
                        <mi>i</mi>
                        <mo>=</mo>
                        <mn>0</mn>
                      </mrow>
                      <mi>M</mi>
                    </munderover>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.3em" height="0.3ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mrow>
                      <munderover>
                        <mo>&#x2211;</mo>
                        <mrow>
                          <mi>j</mi>
                          <mo>=</mo>
                          <mn>0</mn>
                        </mrow>
                        <mi>M</mi>
                      </munderover>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <mrow>
                        <mo>[</mo>
                        <mrow>
                          <mrow>
                            <mi>w</mi>
                            <mo>&#x2061;</mo>
                            <mrow>
                              <mo>(</mo>
                              <mrow>
                                <mi>i</mi>
                                <mo>,</mo>
                                <mi>j</mi>
                              </mrow>
                              <mo>)</mo>
                            </mrow>
                          </mrow>
                          <mo>&#x2062;</mo>
                          <msup>
                            <mi>j</mi>
                            <mn>2</mn>
                          </msup>
                        </mrow>
                        <mo>]</mo>
                      </mrow>
                    </mrow>
                  </mrow>
                </mrow>
              </mrow>
            </mrow>
            <mo>-</mo>
            <msup>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <munderover>
                    <mo>&#x2211;</mo>
                    <mrow>
                      <mi>i</mi>
                      <mo>=</mo>
                      <mn>0</mn>
                    </mrow>
                    <mi>M</mi>
                  </munderover>
                  <mo>&#x2062;</mo>
                  <mstyle>
                    <mspace width="0.3em" height="0.3ex"/>
                  </mstyle>
                  <mo>&#x2062;</mo>
                  <mrow>
                    <munderover>
                      <mo>&#x2211;</mo>
                      <mrow>
                        <mi>j</mi>
                        <mo>=</mo>
                        <mn>0</mn>
                      </mrow>
                      <mi>M</mi>
                    </munderover>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.3em" height="0.3ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mrow>
                      <mrow>
                        <mi>w</mi>
                        <mo>&#x2061;</mo>
                        <mrow>
                          <mo>(</mo>
                          <mrow>
                            <mi>i</mi>
                            <mo>,</mo>
                            <mi>j</mi>
                          </mrow>
                          <mo>)</mo>
                        </mrow>
                      </mrow>
                      <mo>&#x2062;</mo>
                      <mi>j</mi>
                    </mrow>
                  </mrow>
                </mrow>
                <mo>)</mo>
              </mrow>
              <mn>2</mn>
            </msup>
          </mrow>
        </mfrac>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>6</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mi>a</mi>
        <mo>=</mo>
        <mfrac>
          <mrow>
            <mrow>
              <munderover>
                <mo>&#x2211;</mo>
                <mrow>
                  <mi>i</mi>
                  <mo>=</mo>
                  <mn>0</mn>
                </mrow>
                <mi>M</mi>
              </munderover>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mrow>
                <munderover>
                  <mo>&#x2211;</mo>
                  <mrow>
                    <mi>j</mi>
                    <mo>=</mo>
                    <mn>0</mn>
                  </mrow>
                  <mi>M</mi>
                </munderover>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mrow>
                  <mo>[</mo>
                  <mrow>
                    <mrow>
                      <mi>w</mi>
                      <mo>&#x2061;</mo>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <mi>i</mi>
                          <mo>,</mo>
                          <mi>j</mi>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                    <mo>&#x2062;</mo>
                    <mi>i</mi>
                  </mrow>
                  <mo>]</mo>
                </mrow>
              </mrow>
            </mrow>
            <mo>-</mo>
            <mrow>
              <mi>b</mi>
              <mo>&#x2062;</mo>
              <mrow>
                <munderover>
                  <mo>&#x2211;</mo>
                  <mrow>
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>0</mn>
                  </mrow>
                  <mi>M</mi>
                </munderover>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mrow>
                  <munderover>
                    <mo>&#x2211;</mo>
                    <mrow>
                      <mi>j</mi>
                      <mo>=</mo>
                      <mn>0</mn>
                    </mrow>
                    <mi>M</mi>
                  </munderover>
                  <mo>&#x2062;</mo>
                  <mstyle>
                    <mspace width="0.3em" height="0.3ex"/>
                  </mstyle>
                  <mo>&#x2062;</mo>
                  <mrow>
                    <mo>[</mo>
                    <mrow>
                      <mrow>
                        <mi>w</mi>
                        <mo>&#x2061;</mo>
                        <mrow>
                          <mo>(</mo>
                          <mrow>
                            <mi>i</mi>
                            <mo>,</mo>
                            <mi>j</mi>
                          </mrow>
                          <mo>)</mo>
                        </mrow>
                      </mrow>
                      <mo>&#x2062;</mo>
                      <mi>j</mi>
                    </mrow>
                    <mo>]</mo>
                  </mrow>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
          <mrow>
            <munderover>
              <mo>&#x2211;</mo>
              <mrow>
                <mi>i</mi>
                <mo>=</mo>
                <mn>0</mn>
              </mrow>
              <mi>M</mi>
            </munderover>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mrow>
              <munderover>
                <mo>&#x2211;</mo>
                <mrow>
                  <mi>j</mi>
                  <mo>=</mo>
                  <mn>0</mn>
                </mrow>
                <mi>M</mi>
              </munderover>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mrow>
                <mi>w</mi>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>i</mi>
                    <mo>,</mo>
                    <mi>j</mi>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
        </mfrac>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>7</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0123" num="0122">Matrix w(i,j) is generally sparse. In one mode of the method only the non-zero values and their locations are stored to improve memory and computational efficiency.</p>
<p id="p-0124" num="0123">The histogram matching function H(x) is applied to each pixel of f<sub>2 </sub>before performing blur matching of the two images. The results of the histogram matching is shown in <figref idref="DRAWINGS">FIG. 10</figref>.</p>
<p id="p-0125" num="0124">It should be appreciated that the main purpose of histogram matching is to remove outliers. Even after the matching procedure has been performed it can be seen that the matching curves still exhibit significant noise. Accordingly, after matching is performed the curves are modeled according to a polynomial model.</p>
<p id="p-0126" num="0125">5. Two Dimensional Polynomial Model.</p>
<p id="p-0127" num="0126">The matching curves described above can be approximated using a multi-dimensional polynomial function, such as a two-dimensional (2-D) polynomial function, to facilitate calculations while removing a large portion of the mismatching noise seen in <figref idref="DRAWINGS">FIG. 4</figref> and <figref idref="DRAWINGS">FIG. 7</figref> through <figref idref="DRAWINGS">FIG. 10</figref>.</p>
<p id="p-0128" num="0127">In this model, the iteration number is a function of lens position and object distance. The coefficients are determined, for example in response to using a least squared error two-dimensional polynomial fitting algorithm. A two-dimensional polynomial is used to model the blur iteration function of Eq. 5.</p>
<p id="p-0129" num="0128">
<maths id="MATH-US-00006" num="00006">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mi>I</mi>
        <mo>=</mo>
        <mrow>
          <munderover>
            <mo>&#x2211;</mo>
            <mrow>
              <mi>i</mi>
              <mo>=</mo>
              <mn>0</mn>
            </mrow>
            <mi>m</mi>
          </munderover>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mrow>
            <munderover>
              <mo>&#x2211;</mo>
              <mrow>
                <mi>j</mi>
                <mo>=</mo>
                <mn>0</mn>
              </mrow>
              <mi>m</mi>
            </munderover>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mrow>
              <mrow>
                <mi>C</mi>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>i</mi>
                    <mo>,</mo>
                    <mi>j</mi>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mo>&#x2062;</mo>
              <msup>
                <mi>L</mi>
                <mi>i</mi>
              </msup>
              <mo>&#x2062;</mo>
              <msup>
                <mi>D</mi>
                <mi>j</mi>
              </msup>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>8</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0130" num="0129">The coefficients C(i,j) are determined using a least squares multidimensional polynomial fitting method. The degree of the polynomial, m and n, are chosen depending on the use of specific lenses and applications. Examples of bi-quadratic (m=n=2) and bi-cubic (m=n=3) polynomials are shown in the figures.</p>
<p id="p-0131" num="0130">By way of a first example, bi-quadratic function coefficients can be used to approximate the fitting algorithm. By way of example and not limitation, for a bi-quadratic approximation the curves can be represented by a 3&#xd7;3 matrix, such as the following.</p>
<p id="p-0132" num="0131">
<maths id="MATH-US-00007" num="00007">
<math overflow="scroll">
<mrow>
  <mrow>
    <mi>C</mi>
    <mo>&#x2061;</mo>
    <mrow>
      <mo>(</mo>
      <mrow>
        <mi>i</mi>
        <mo>,</mo>
        <mi>j</mi>
      </mrow>
      <mo>)</mo>
    </mrow>
  </mrow>
  <mo>=</mo>
  <mtable>
    <mtr>
      <mtd>
        <mrow>
          <mrow>
            <mo>-</mo>
            <mn>5.268385</mn>
          </mrow>
          <mo>&#x2062;</mo>
          <mi>e</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mtext>+</mtext>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>00</mn>
        </mrow>
      </mtd>
      <mtd>
        <mrow>
          <mn>1.014786</mn>
          <mo>&#x2062;</mo>
          <mi>e</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mtext>+</mtext>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>01</mn>
        </mrow>
      </mtd>
      <mtd>
        <mrow>
          <mrow>
            <mo>-</mo>
            <mn>3.073324</mn>
          </mrow>
          <mo>&#x2062;</mo>
          <mi>e</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mtext>-</mtext>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>02</mn>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd>
        <mrow>
          <mrow>
            <mo>-</mo>
            <mn>9.677197</mn>
          </mrow>
          <mo>&#x2062;</mo>
          <mi>e</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mtext>+</mtext>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>00</mn>
        </mrow>
      </mtd>
      <mtd>
        <mrow>
          <mrow>
            <mo>-</mo>
            <mn>1.522669</mn>
          </mrow>
          <mo>&#x2062;</mo>
          <mi>e</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mtext>-</mtext>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>02</mn>
        </mrow>
      </mtd>
      <mtd>
        <mrow>
          <mn>3.695552</mn>
          <mo>&#x2062;</mo>
          <mi>e</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mtext>-</mtext>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>04</mn>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd>
        <mrow>
          <mn>3.325387</mn>
          <mo>&#x2062;</mo>
          <mi>e</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mtext>-</mtext>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>02</mn>
        </mrow>
      </mtd>
      <mtd>
        <mrow>
          <mrow>
            <mo>-</mo>
            <mn>2.438326</mn>
          </mrow>
          <mo>&#x2062;</mo>
          <mi>e</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mtext>-</mtext>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>04</mn>
        </mrow>
      </mtd>
      <mtd>
        <mrow>
          <mrow>
            <mo>-</mo>
            <mrow>
              <mn>3.8337</mn>
              <mo>&#x2062;</mo>
              <mn>38</mn>
            </mrow>
          </mrow>
          <mo>&#x2062;</mo>
          <mi>e</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mtext>-</mtext>
          </mstyle>
          <mo>&#x2062;</mo>
          <mrow>
            <mn>0</mn>
            <mo>&#x2062;</mo>
            <mn>7</mn>
          </mrow>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</mrow>
</math>
</maths>
</p>
<p id="p-0133" num="0132"><figref idref="DRAWINGS">FIG. 11</figref> is a bi-quadratic fitting curve shown in the dashed lines in comparison with the matching data shown in solid lines. The smooth lines of bi-quadratic curve fitting is in stark contrast to the more jagged lines for the empirically collected matching data. It will seen that the polynomial provides a sufficient match with the matching data shown by the solid lines.</p>
<p id="p-0134" num="0133">By way of a second example, bi-cubic function coefficients can be alternately utilized to approximate the fitting algorithm. By way of example and not limitation, for a bi-cubic approximation the curves can be represented by a 4&#xd7;4 matrix, such as the following.</p>
<p id="p-0135" num="0134">
<maths id="MATH-US-00008" num="00008">
<math overflow="scroll">
<mrow>
  <mrow>
    <mi>C</mi>
    <mo>&#x2061;</mo>
    <mrow>
      <mo>(</mo>
      <mrow>
        <mi>i</mi>
        <mo>,</mo>
        <mi>j</mi>
      </mrow>
      <mo>)</mo>
    </mrow>
  </mrow>
  <mo>=</mo>
  <mtable>
    <mtr>
      <mtd>
        <mrow>
          <mrow>
            <mo>-</mo>
            <mn>2.096603</mn>
          </mrow>
          <mo>&#x2062;</mo>
          <mi>e</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mtext>+</mtext>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>01</mn>
        </mrow>
      </mtd>
      <mtd>
        <mrow>
          <mn>1.414987</mn>
          <mo>&#x2062;</mo>
          <mi>e</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mtext>+</mtext>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>01</mn>
        </mrow>
      </mtd>
      <mtd>
        <mrow>
          <mrow>
            <mo>-</mo>
            <mn>1.356138</mn>
          </mrow>
          <mo>&#x2062;</mo>
          <mi>e</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mtext>-</mtext>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>01</mn>
        </mrow>
      </mtd>
      <mtd>
        <mrow>
          <mn>5.802068</mn>
          <mo>&#x2062;</mo>
          <mi>e</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mtext>-</mtext>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>04</mn>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd>
        <mrow>
          <mrow>
            <mo>-</mo>
            <mn>1.074841</mn>
          </mrow>
          <mo>&#x2062;</mo>
          <mi>e</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mtext>+</mtext>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>01</mn>
        </mrow>
      </mtd>
      <mtd>
        <mrow>
          <mrow>
            <mo>-</mo>
            <mn>1.387527</mn>
          </mrow>
          <mo>&#x2062;</mo>
          <mi>e</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mtext>-</mtext>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>01</mn>
        </mrow>
      </mtd>
      <mtd>
        <mrow>
          <mn>4.771262</mn>
          <mo>&#x2062;</mo>
          <mi>e</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mtext>-</mtext>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>03</mn>
        </mrow>
      </mtd>
      <mtd>
        <mrow>
          <mrow>
            <mo>-</mo>
            <mn>2.600512</mn>
          </mrow>
          <mo>&#x2062;</mo>
          <mi>e</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mtext>-</mtext>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>05</mn>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd>
        <mrow>
          <mn>8.499311</mn>
          <mo>&#x2062;</mo>
          <mi>e</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mtext>-</mtext>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>02</mn>
        </mrow>
      </mtd>
      <mtd>
        <mrow>
          <mrow>
            <mo>-</mo>
            <mn>4.243161</mn>
          </mrow>
          <mo>&#x2062;</mo>
          <mi>e</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mtext>-</mtext>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>04</mn>
        </mrow>
      </mtd>
      <mtd>
        <mrow>
          <mrow>
            <mo>-</mo>
            <mn>3.456327</mn>
          </mrow>
          <mo>&#x2062;</mo>
          <mi>e</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mtext>-</mtext>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>05</mn>
        </mrow>
      </mtd>
      <mtd>
        <mrow>
          <mn>2.485215</mn>
          <mo>&#x2062;</mo>
          <mi>e</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mtext>-</mtext>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>07</mn>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd>
        <mrow>
          <mrow>
            <mo>-</mo>
            <mn>3.199641</mn>
          </mrow>
          <mo>&#x2062;</mo>
          <mi>e</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mtext>-</mtext>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>04</mn>
        </mrow>
      </mtd>
      <mtd>
        <mrow>
          <mn>6.471844</mn>
          <mo>&#x2062;</mo>
          <mi>e</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mtext>-</mtext>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>06</mn>
        </mrow>
      </mtd>
      <mtd>
        <mrow>
          <mn>5.348240</mn>
          <mo>&#x2062;</mo>
          <mi>e</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mtext>-</mtext>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>08</mn>
        </mrow>
      </mtd>
      <mtd>
        <mrow>
          <mrow>
            <mo>-</mo>
            <mn>6.416592</mn>
          </mrow>
          <mo>&#x2062;</mo>
          <mi>e</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mtext>-</mtext>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>10</mn>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</mrow>
</math>
</maths>
</p>
<p id="p-0136" num="0135"><figref idref="DRAWINGS">FIG. 12</figref> depicts a bi-cubic fitting curve shown in the dashed lines in comparison with the matching data shown in solid lines. It will be seen that this bi-cubic polynomial provides a slightly closer match than that the bi-quadratic fit shown in <figref idref="DRAWINGS">FIG. 11</figref>.</p>
<p id="p-0137" num="0136">6. Depth Estimation.</p>
<p id="p-0138" num="0137">Using the model presented by Eq. 5, the depth estimation method is readily implemented. First, two images at different focal positions are captured, with distance between the focus positions being within one depth of field. It will be noted that the subject distance is not known at this moment, as this is what is being estimated. The two pictures used in the process can be captured at any distances as long as the difference between the focus positions of these two pictures is within one depth of field. Optionally, noise processing, such as histogram matching, may be performed on the captured image information prior to determination of blur difference. The blur difference between the captured images as calculated in regards to Eq. 2 through Eq. 5 becomes a single variable polynomial equation. The polynomial equation is solved for D, which results in generating an estimated depth of the object, also referred to as subject-distance. It should be noted that D can be configured in any desired format, such as an integer or floating point number. For auto focus applications, the lens can be moved to focus at the estimated distance D, and estimate the new depth in the same manner. The procedure may be repeated until the iteration number converges to 0, or below some desired threshold. It should be appreciated that this algorithm may be extended to higher dimensional polynomial models for varying focal lengths and apertures.</p>
<p id="p-0139" num="0138">7. General Descriptions of Method and Apparatus.</p>
<p id="p-0140" num="0139"><figref idref="DRAWINGS">FIG. 13</figref> illustrates a calibration embodiment, such as would be performed by the manufacturer of a given imaging device, such as a camera. In block <b>70</b> matching curves are obtained for step-edge images at different focal lengths. A two-dimensional model is then created as per block <b>72</b> to represent the matching curves, by way of example as a multi-dimensional polynomial model. After this calibration process the representation of the model, such as its polynomial coefficients, are stored <b>74</b>, for instance encoded into the non-volatile program memory of the camera device.</p>
<p id="p-0141" num="0140"><figref idref="DRAWINGS">FIG. 14</figref> illustrates an embodiment of using the multi-dimensional polynomial model for depth estimation within a camera device according to the present invention. After the calibration process (<figref idref="DRAWINGS">FIG. 13</figref>), the model is thus available for estimating object depth within the specific camera device. Represented in block <b>80</b>, two images are captured (e.g., pictures taken) at two different focus positions. Histogram matching is preferably performed on the images as per block <b>82</b>. Blur difference is then calculated in block <b>84</b> on each image by performing a wavelet transform, determining wavelet variance, and then comparing the absolute differences between the variances. After this the polynomial model is used in block <b>86</b> to estimate the depth based on the blur difference and the focus positions at which the two images were captured.</p>
<p id="p-0142" num="0141">It should be appreciated that a series of depth estimations may be performed according to the present invention. For example if the method is utilized in concert with camera focus adjustment, then as the camera focus is adjusted, additional image input may be collected and the distance estimation process performed again (or continuously) to provide increasing accuracy as the camera nears proper focus when subject-distance estimates match up with the actual subject-distance.</p>
<p id="p-0143" num="0142">In order to simplify the focus matching model and to smooth the response, it is desirable to eliminate errors arising from changes in physical camera elements (e.g., aperture variation, optical element power and temperature variation, mechanical lens setting fluctuations, and the like) and environmental factors (i.e., lighting, motion, temperature, position and so forth). Although the histogram matching process removed some noise source prior to determining blur difference, there is still a measure of noise which can be eliminated, such as was seen in <figref idref="DRAWINGS">FIG. 9</figref>. Toward removing additional noise, the focus matching model itself determined in response to the calibration process is preferably cast into a sufficiently smooth mathematical representation (function). This can representation comprising, according to an element of the present invention a polynomial function of a desired degree (e.g., 2, 3 or 4 degrees) which can be used for depth estimation. Thus, a function (e.g., polynomial function) is selected to substitute for the model created based on the empirically collected data. It will be appreciated that the substitute function should provide sufficient curve matching (fit) with the empirical data in order that use of the model will render sufficiently accurate distance estimations.</p>
<p id="p-0144" num="0143">For example, given a lens position and an iteration number, a two-dimensional polynomial equation becomes a single variable equation. Elements of the invention describe examples of the single variable equation as a quadratic or cubic equation, which can be solved in one step. It should also be appreciated that the algorithm can be extended to higher dimensional polynomial functions as desired, for example for use with different focal lengths and apertures.</p>
<p id="p-0145" num="0144"><figref idref="DRAWINGS">FIG. 15</figref> illustrates the histogram matching process which is optionally performed, as seen in block <b>82</b> of <figref idref="DRAWINGS">FIG. 14</figref>, to remove noise prior to computing blur difference. It will be noted that in considering images subject to complex motions, the use of histogram matching can actually introduce error, wherein its use depends on the application and the expected motion of the images. As represented in block <b>90</b>, histograms are generated for two pictures obtained at different focus positions. Pixels are sequentially shifted from one histogram to equalize the number of pixels of the closest luminance of the other histogram as per block <b>92</b>. A histogram matching function is determined using a least squared error solution as represented in block <b>94</b>. It should be recognized that a one dimensional linear functions preferably selected for this histogram matching function, by virtue of its simplicity. It should be recognized that the histogram matching function and the focus matching function are different and distinct, the latter being a two dimensional polynomial function.</p>
<p id="p-0146" num="0145"><figref idref="DRAWINGS">FIG. 16</figref> illustrates an example embodiment of using convolutions in combination with wavelet transforms and wavelet variances to obtain blur differences within an image capture system which estimates actual subject distance in response to blur difference within a polynomial. Images are captured at two subject images <b>100</b> and histogram matching, or an alternative process for removing outliers, optionally performed <b>102</b>. Actual subject distances are estimated <b>104</b> and modeling is performed of blur change as a point spread function approximated <b>106</b> by convolutions. After a convolution on a wavelet transform is applied <b>108</b> to the images, wavelet variances are determined. After wavelet transforms are applied to both images f<sub>A </sub>and f<sub>B</sub>, and wavelet variances determined, then the differences are determined <b>110</b>, such as preferably a measure of absolute differences, between these variances as an estimation of the difference between the pictures and thus the blur difference. The preceding convolutions, wavelet transforms, and variance differences are repeated <b>112</b> using one or more convolution kernels until a desired level of convergence is reached or other desired threshold condition is reached. Depth is estimated <b>114</b> based on a polynomial model which has been previously determined for the camera system. In a system providing automatic focusing (as opposed to a system providing focus indicators), the focus is adjusted <b>116</b> on the image capture device (e.g., camera focus control), and the above steps iteratively performed <b>118</b> until the desired focus accuracy is obtained. It should be appreciated that the present invention can be implemented on a variety of devices and systems which are configured to perform any of a number of different forms of image processing and/or image capture. By way of example and not limitation the following describes an embodiment within a camera device.</p>
<p id="p-0147" num="0146"><figref idref="DRAWINGS">FIG. 17</figref> illustrates an example embodiment <b>130</b> of an image capture device (camera) <b>130</b> configured for depth estimation according to the invention. A focus/zoom control <b>134</b> is shown coupled to imaging optics <b>132</b> as controlled by a computer (CPU) <b>136</b>. Computer <b>136</b> performs the depth estimation method in response to instructions executed from memory <b>138</b> and/or auxiliary memory <b>140</b>. Shown by way of example for a camera device (e.g., video or still) are an image display <b>142</b> and touch screen <b>144</b>, and non-touch interface <b>146</b>. However, it should be appreciated that the apparatus and method according to the present invention can be implemented on various image capture devices which are configured with focus control, focus indicators, or combinations thereof. It should be appreciated that the calibration process (e.g., <figref idref="DRAWINGS">FIG. 13</figref>) which generates the model, such as defined by polynomial coefficients, is performed by a computer controlled test setup. The depth estimation and focusing control elements of the invention are preferably implemented in the camera device itself as depicted in <figref idref="DRAWINGS">FIG. 17</figref>, or a similar imaging device.</p>
<p id="p-0148" num="0147">It should be appreciated that the blur difference determinations using convolutions combined with wavelet variance based comparisons as depicted in <figref idref="DRAWINGS">FIG. 6</figref>, histogram matching, polynomial modeling of depth estimation, and autofocus steps would all be preferably performed by computer processor <b>136</b> in combination with memory <b>138</b> and/or auxiliary memory <b>140</b>. Although the computer processor and memory are described above in relation to an image capture device, it will be recognized that the computer and its associated programming may be used to perform the blur difference determination within any electronic device which performs image processing.</p>
<p id="p-0149" num="0148">In regards to the use of a zoom control or other means of changing the lens focal length, also referred to as magnification, it should be appreciated that the camera and/or lens system in use will be preferably characterized according to the present invention across its applicable zoom range. For example, characterization of the camera and/or lens will be performed as described for each discrete focal length of lens setting in a camera having discrete lens selections, or at incremental steps along the zoom range of a camera having a continuously selectable zoom control. In this way the estimation of distance to a subject can be performed for single focal length lenses, as described, or for those having multiple ranges whether continuous ranges (e.g., zoom) or discontinuous which is more typically referred to as discrete ranges (e.g., normal/macro setting or other selectable range settings). In a prior section the extension of the two-dimensional (2D) polynomial model to higher dimensions has been described which provides for various focal lengths (different zoom positions) and apertures. By way of example and not limitation, Eq. (5) can be rewritten as I=F(L,D,Z,A) where Z is focal length, and A is the aperture to provide a four-dimensional polynomial model.</p>
<p id="p-0150" num="0149">The present invention provides methods and apparatus of depth estimation using blur difference determinations based on wavelet transform and absolute variance comparisons which are motion robust. The teachings herein can be applied to a number of systems including cameras and any form of image capture device, in particular those configured for automatically detecting and/or adjusting focus. It should also be appreciated that the teachings can be applied without limitation to systems which process images from a separate camera device, or other image source, and in such systems which require blur difference comparisons, distance determinations, and/or focus control.</p>
<p id="p-0151" num="0150">As can be seen, therefore, the present invention includes the following inventive embodiments among others:</p>
<p id="p-0152" num="0151">1. An image capture apparatus, comprising: (a) an imaging device; (b) a computer processor coupled to the imaging device; (c) memory coupled to said computer processor configured for retaining programming executable on said computer processor; (d) a focus matching model retained in said memory; and (e) programming executable on said computer processor for carrying out the steps comprising, (e)(i) capturing multiple object images, (e)(ii) performing convolutions to model blur changes as a point spread function between said multiple object images, (e)(iii) determining blur difference within each convolution in response to performing a wavelet transform, obtaining wavelet variance and comparing differences of wavelet variance for said multiple object images, and (e)(iv) performing depth estimation in response to said convolutions within said focus matching model.</p>
<p id="p-0153" num="0152">2. The apparatus of embodiment 1, wherein said focus matching model is based on imaging calibration targets obtained at different focal lengths.</p>
<p id="p-0154" num="0153">3. The apparatus of embodiment 1, wherein programming executable on said computer processor is configured for performing said convolutions by at least one size of convolution kernel.</p>
<p id="p-0155" num="0154">4. The apparatus of embodiment 1, wherein programming executable on said computer processor is configured for determining said differences of wavelet variance in response to a determination of absolute wavelet differences.</p>
<p id="p-0156" num="0155">5. The apparatus of embodiment 1, wherein programming executable on said computer processor is configured for determining said wavelet variance in at least one wavelet subband and at least one wavelet transform level.</p>
<p id="p-0157" num="0156">6. The apparatus of embodiment 1, wherein programming executable on said computer processor is configured for determining said wavelet variance in all wavelet subbands in at least one wavelet transform level.</p>
<p id="p-0158" num="0157">7. The apparatus of embodiment 1, wherein said focus matching model utilizes a polynomial function to reduce mismatching noise.</p>
<p id="p-0159" num="0158">8. The apparatus of embodiment 7, wherein coefficients of the polynomial function are stored in said memory.</p>
<p id="p-0160" num="0159">9. The apparatus of embodiment 1, wherein programming executable on said computer processor is further configured for performing histogram matching of the object images to reduce noise from outliers between focal positions prior to inputting the blur differences into the focus matching model.</p>
<p id="p-0161" num="0160">10. The apparatus of embodiment 9, wherein programming executable on said computer processor is configured for performing said histogram matching in response to steps comprising: (a) sequentially shifting pixels from a first histogram to a second histogram to equalize the pixels of closest luminance; and (b) approximating histogram matching utilizing a linear matching function; (c) wherein noise effects are reduced which have been introduced into said focus matching model in response to undesired physical and environmental variations.</p>
<p id="p-0162" num="0161">11. The apparatus of embodiment 1, wherein the focus matching model is generated by performing a calibration process on said apparatus in which a series of calibration target images are obtained for registering proper focus, focus curves are obtained for the series of calibration target images; and a multi-dimensional model generated based on matching the focus curves for the series of calibration target images.</p>
<p id="p-0163" num="0162">12. The apparatus of embodiment 1, wherein the imaging device comprises a still image camera, a video image camera, or a combination still and video image camera.</p>
<p id="p-0164" num="0163">13. The apparatus of embodiment 1, further comprising: (a) a focus control element coupled to the imaging device; (b) programming executable on said computer processor for adjusting said focus control element in response to performing depth estimation on object images based on inputting blur differences detected between object images into said focus matching model.</p>
<p id="p-0165" num="0164">14. An image capture apparatus, comprising: (a) an imaging device; (b) a computer processor coupled to the imaging device; (c) memory coupled to said computer processor configured for retaining programming executable on said computer processor; (d) a focus matching model based on imaging calibration targets at different focal lengths which is retained in said memory, and (e) programming executable on said computer processor for carrying out the steps of, (e)(i) capturing multiple object images, (e)(ii) performing convolutions by at least one size of convolution kernel to model blur changes as a point spread function between said multiple object images, (e)(iii) determining blur difference within each convolution in response to performing wavelet transform, obtaining wavelet variance and comparing differences of wavelet variance in at least one wavelet subband and at least one wavelet transform level, for said multiple object images, and (e)(iv) performing depth estimation in response to said convolutions within said focus matching model.</p>
<p id="p-0166" num="0165">15. The apparatus of embodiment 14, wherein programming executable on said computer processor is configured for determining said differences of wavelet variance in response to a determination of absolute wavelet differences.</p>
<p id="p-0167" num="0166">16. The apparatus of embodiment 14, wherein programming executable on said computer processor is configured for determining said wavelet variance in at least one wavelet subband and at least one wavelet transform level.</p>
<p id="p-0168" num="0167">17. The apparatus of embodiment 14, wherein programming executable on said computer processor is configured for determining said wavelet variance in all wavelet subbands in at least one wavelet transform level.</p>
<p id="p-0169" num="0168">18. The apparatus of embodiment 14, wherein said focus matching model utilizes a polynomial function, whose coefficients are stored in said memory, to reduce mismatching noise.</p>
<p id="p-0170" num="0169">19. The apparatus of embodiment 14, wherein programming executable on said computer processor is further configured for performing histogram matching of the object images to reduce noise from outliers between focal positions prior to inputting the blur differences into the focus matching model.</p>
<p id="p-0171" num="0170">20. A method of automatic estimation of camera-to-object focal depth, comprising: (a) generating a multi-dimensional focus matching model in response to detecting blur differences between multiple images of a calibration subject captured at different focal distances; (b) capturing multiple object images; (c) determining blur differences between the multiple object images in response to convolutions which model blur changes as a point spread function between said multiple object images; (d) determining blur difference within each convolution in response to performing wavelet transform, obtaining wavelet variance and comparing differences of wavelet variance for said multiple object images, and (e) performing depth estimation in response to said convolutions within said focus matching model.</p>
<p id="p-0172" num="0171">Embodiments of the present invention may be described with reference to flowchart illustrations of methods and systems according to embodiments of the invention. These methods and systems can also be implemented as computer program products. In this regard, each block or step of a flowchart, and combinations of blocks (and/or steps) in a flowchart, can be implemented by various means, such as hardware, firmware, and/or software including one or more computer program instructions embodied in computer-readable program code logic. As will be appreciated, any such computer program instructions may be loaded onto a computer, including without limitation a general purpose computer or special purpose computer, or other programmable processing apparatus to produce a machine, such that the computer program instructions which execute on the computer or other programmable processing apparatus create means for implementing the functions specified in the block(s) of the flowchart(s).</p>
<p id="p-0173" num="0172">Accordingly, blocks of the flowcharts support combinations of means for performing the specified functions, combinations of steps for performing the specified functions, and computer program instructions, such as embodied in computer-readable program code logic means, for performing the specified functions. It will also be understood that each block of the flowchart illustrations, and combinations of blocks in the flowchart illustrations, can be implemented by special purpose hardware-based computer systems which perform the specified functions or steps, or combinations of special purpose hardware and computer-readable program code logic means.</p>
<p id="p-0174" num="0173">Furthermore, these computer program instructions, such as embodied in computer-readable program code logic, may also be stored in a computer-readable memory that can direct a computer or other programmable processing apparatus to function in a particular manner, such that the instructions stored in the computer-readable memory produce an article of manufacture including instruction means which implement the function specified in the block(s) of the flowchart(s). The computer program instructions may also be loaded onto a computer or other programmable processing apparatus to cause a series of operational steps to be performed on the computer or other programmable processing apparatus to produce a computer-implemented process such that the instructions which execute on the computer or other programmable processing apparatus provide steps for implementing the functions specified in the block(s) of the flowchart(s).</p>
<p id="p-0175" num="0174">Although the description above contains many details, these should not be construed as limiting the scope of the invention but as merely providing illustrations of some of the presently preferred embodiments of this invention. Therefore, it will be appreciated that the scope of the present invention fully encompasses other embodiments which may become obvious to those skilled in the art, and that the scope of the present invention is accordingly to be limited by nothing other than the appended claims, in which reference to an element in the singular is not intended to mean &#x201c;one and only one&#x201d; unless explicitly so stated, but rather &#x201c;one or more.&#x201d; All structural and functional equivalents to the elements of the above-described preferred embodiment that are known to those of ordinary skill in the art are expressly incorporated herein by reference and are intended to be encompassed by the present claims. Moreover, it is not necessary for a device or method to address each and every problem sought to be solved by the present invention, for it to be encompassed by the present claims. Furthermore, no element, component, or method step in the present disclosure is intended to be dedicated to the public regardless of whether the element, component, or method step is explicitly recited in the claims. No claim element herein is to be construed under the provisions of 35 U.S.C. 112, sixth paragraph, unless the element is expressly recited using the phrase &#x201c;means for.&#x201d;</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08624986-20140107-M00001.NB">
<img id="EMI-M00001" he="11.26mm" wi="76.20mm" file="US08624986-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US08624986-20140107-M00002.NB">
<img id="EMI-M00002" he="8.13mm" wi="76.20mm" file="US08624986-20140107-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00003" nb-file="US08624986-20140107-M00003.NB">
<img id="EMI-M00003" he="17.61mm" wi="76.20mm" file="US08624986-20140107-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00004 MATH-US-00004-2" nb-file="US08624986-20140107-M00004.NB">
<img id="EMI-M00004" he="19.05mm" wi="76.20mm" file="US08624986-20140107-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00005" nb-file="US08624986-20140107-M00005.NB">
<img id="EMI-M00005" he="40.22mm" wi="116.08mm" file="US08624986-20140107-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00006" nb-file="US08624986-20140107-M00006.NB">
<img id="EMI-M00006" he="8.81mm" wi="76.20mm" file="US08624986-20140107-M00006.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00007" nb-file="US08624986-20140107-M00007.NB">
<img id="EMI-M00007" he="13.38mm" wi="76.20mm" file="US08624986-20140107-M00007.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00008" nb-file="US08624986-20140107-M00008.NB">
<img id="EMI-M00008" he="23.62mm" wi="76.20mm" file="US08624986-20140107-M00008.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. An image capture apparatus, comprising:
<claim-text>an imaging device;</claim-text>
<claim-text>a computer processor coupled to the imaging device;</claim-text>
<claim-text>memory coupled to said computer processor configured for retaining programming executable on said computer processor;</claim-text>
<claim-text>a focus matching model retained in said memory, and</claim-text>
<claim-text>programming executable on said computer processor for carrying out steps comprising:
<claim-text>(i) capturing multiple object images;</claim-text>
<claim-text>(ii) performing convolutions to model blur changes as a point spread function between said multiple object images;</claim-text>
<claim-text>(iii) determining blur difference within each convolution in response to performing a wavelet transform, obtaining wavelet variance and comparing differences of wavelet variance for said multiple object images; and</claim-text>
<claim-text>(iv) performing depth estimation in response to said convolutions within said focus matching model.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. An apparatus as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said focus matching model is based on imaging calibration targets obtained at different focal lengths.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. An apparatus as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein programming executable on said computer processor is configured for performing said convolutions by at least one size of convolution kernel.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. An apparatus as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein programming executable on said computer processor is configured for comparing said differences of wavelet variance in response to an absolute difference of wavelet variance.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. An apparatus as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein programming executable on said computer processor is configured for determining said wavelet variance in at least one wavelet subband and at least one wavelet transform level.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. An apparatus as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein programming executable on said computer processor is configured for determining said wavelet variance in all wavelet subbands in at least one wavelet transform level.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. An apparatus as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said focus matching model utilizes a polynomial function to reduce mismatching noise.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. An apparatus as recited in <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein coefficients of the polynomial function are stored in said memory.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. An apparatus as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein programming executable on said computer processor is further configured for performing histogram matching of the object images to reduce noise from outliers between focal positions prior to inputting the blur differences into the focus matching model.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. An apparatus as recited in <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein programming executable on said computer processor is configured for performing said histogram matching in response to steps comprising:
<claim-text>sequentially shifting pixels from a first histogram to a second histogram to equalize the pixels of closest luminance; and</claim-text>
<claim-text>approximating histogram matching utilizing a linear matching function;</claim-text>
<claim-text>wherein noise effects are reduced which have been introduced into said focus matching model in response to undesired physical and environmental variations.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. An apparatus as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the focus matching model is generated in response to a calibration process upon said apparatus in which a series of calibration target images are obtained for registering proper focus, with focus curves obtained for the series of calibration target images; and in which a multi-dimensional model is generated based on matching the focus curves for the series of calibration target images.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. An apparatus as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the imaging device comprises a still image camera, a video image camera, or a combination still and video image camera.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. An apparatus as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>a focus control element coupled to the imaging device; and</claim-text>
<claim-text>programming executable on said computer processor for adjusting said focus control element in response to performing depth estimation on object images based on inputting blur differences detected between object images into said focus matching model.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. An image capture apparatus, comprising:
<claim-text>an imaging device;</claim-text>
<claim-text>a computer processor coupled to the imaging device;</claim-text>
<claim-text>memory coupled to said computer processor configured for retaining programming executable on said computer processor;</claim-text>
<claim-text>a focus matching model based on imaging calibration targets at different focal lengths which is retained in said memory, and</claim-text>
<claim-text>programming executable on said computer processor for carrying out steps comprising:
<claim-text>(i) capturing multiple object images;</claim-text>
<claim-text>(ii) performing convolutions by at least one size of convolution kernel to model blur changes as a point spread function between said multiple object images;</claim-text>
<claim-text>(iii) determining blur difference within each convolution in response to performing wavelet transform, obtaining wavelet variance and comparing differences of wavelet variance in at least one wavelet subband and at least one wavelet transform level, for said multiple object images; and</claim-text>
<claim-text>(iv) performing depth estimation in response to said convolutions within said focus matching model.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. An apparatus as recited in <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein programming executable on said computer processor is configured for comparing said differences of wavelet variance in response to an absolute difference of wavelet variance.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. An apparatus as recited in <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein programming executable on said computer processor is configured for determining said wavelet variance in at least one wavelet subband and at least one wavelet transform level.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. An apparatus as recited in <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein programming executable on said computer processor is configured for determining said wavelet variance in all wavelet subbands in at least one wavelet transform level.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. An apparatus as recited in <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein said focus matching model utilizes a polynomial function, whose coefficients are stored in said memory, to reduce mismatching noise.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. An apparatus as recited in <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein programming executable on said computer processor is further configured for performing histogram matching of the object images to reduce noise from outliers between focal positions prior to inputting the blur differences into the focus matching model.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. A method of automatic estimation of camera-to-object focal depth, comprising:
<claim-text>executing a program stored on a computer processor, the processor,</claim-text>
<claim-text>generating a multi-dimensional focus matching model in response to detecting blur differences between multiple images of a calibration subject captured at different focal distances;</claim-text>
<claim-text>capturing multiple object images;</claim-text>
<claim-text>determining blur differences between the multiple object images in response to convolutions which model blur changes as a point spread function between said multiple object images,</claim-text>
<claim-text>determining blur difference within each convolution in response to performing wavelet transform, obtaining wavelet variance and comparing differences of wavelet variance for said multiple object images, and</claim-text>
<claim-text>performing depth estimation in response to said convolutions within said focus matching model.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
