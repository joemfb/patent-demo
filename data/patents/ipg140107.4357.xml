<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625429-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625429</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13329904</doc-number>
<date>20111219</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<us-term-of-grant>
<us-term-extension>31</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>L</subclass>
<main-group>12</main-group>
<subgroup>26</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>370235</main-classification>
</classification-national>
<invention-title id="d2e53">Scheduling data over multiple network interfaces</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>7185121</doc-number>
<kind>B2</kind>
<name>Fitzsimmons et al.</name>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>710 26</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>7424709</doc-number>
<kind>B2</kind>
<name>Neiger et al.</name>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>718  1</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>7685286</doc-number>
<kind>B2</kind>
<name>Brown et al.</name>
<date>20100300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709226</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>7706273</doc-number>
<kind>B2</kind>
<name>Poletto et al.</name>
<date>20100400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>370235</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>2002/0097724</doc-number>
<kind>A1</kind>
<name>Halme et al.</name>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>370392</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2007/0268903</doc-number>
<kind>A1</kind>
<name>Nakagawa</name>
<date>20071100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>370392</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2008/0288771</doc-number>
<kind>A1</kind>
<name>Kulakowski et al.</name>
<date>20081100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>713150</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2010/0031339</doc-number>
<kind>A1</kind>
<name>Minnen</name>
<date>20100200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>726 12</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2010/0329256</doc-number>
<kind>A1</kind>
<name>Akella et al.</name>
<date>20101200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>370392</main-classification></classification-national>
</us-citation>
</us-references-cited>
<number-of-claims>23</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>370235</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3702301</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>370231</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>5</number-of-drawing-sheets>
<number-of-figures>7</number-of-figures>
</figures>
<us-related-documents>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>61424373</doc-number>
<date>20101217</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120320748</doc-number>
<kind>A1</kind>
<date>20121220</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Schmidtke</last-name>
<first-name>Jakub</first-name>
<address>
<city>Waterloo</city>
<country>CA</country>
</address>
</addressbook>
<residence>
<country>CA</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Robinson</last-name>
<first-name>Robert</first-name>
<address>
<city>Waterloo</city>
<country>CA</country>
</address>
</addressbook>
<residence>
<country>CA</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Schmidtke</last-name>
<first-name>Jakub</first-name>
<address>
<city>Waterloo</city>
<country>CA</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Robinson</last-name>
<first-name>Robert</first-name>
<address>
<city>Waterloo</city>
<country>CA</country>
</address>
</addressbook>
</inventor>
</inventors>
</us-parties>
<examiners>
<primary-examiner>
<last-name>Rinehart</last-name>
<first-name>Mark</first-name>
<department>2463</department>
</primary-examiner>
<assistant-examiner>
<last-name>Anwar</last-name>
<first-name>Mohammad</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A scheduler is configured to schedule packets in a plurality of flows to a corresponding one of a plurality of interfaces. Each of the packets includes a flow identifier for identifying a corresponding one of the plurality of flows. The scheduler include memory having instructions for execution by the processor to implement a scheduling algorithm. The scheduling algorithm person as follows. Sections within a predefined range are assigned to corresponding ones of the plurality of interfaces. For each packet, a hash function is applied to the flow identifier to obtain a hash value that is evenly distributed within the predefined range. Additionally, for each packet, it is determined in which of the sections the hash value falls and the corresponding one of the plurality of interfaces is identified accordingly.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="107.27mm" wi="185.25mm" file="US08625429-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="184.49mm" wi="127.17mm" orientation="landscape" file="US08625429-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="184.83mm" wi="168.40mm" file="US08625429-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="208.87mm" wi="156.46mm" file="US08625429-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="199.98mm" wi="172.30mm" file="US08625429-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="166.03mm" wi="129.12mm" file="US08625429-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<p id="p-0002" num="0001">The present invention relates generally to scheduling data over multiple network interfaces and specifically doing so in an efficient manner to reduce resource overhead.</p>
<heading id="h-0001" level="1">BACKGROUND</heading>
<p id="p-0003" num="0002">Network devices that have more than one outgoing network interface and are processing Internet Protocol (IP) packets need to decide how outgoing packets are going to be assigned to each of the available network interfaces. Due to limitations in standard IP networks, it is not desirable to send a single packet out more than one interface. Therefore, for a given outgoing packet, the network device needs to assign this packet to one of the network interfaces, at which point the network interface will transmit this packet across the network to its destination. The process of assigning packets to network interfaces is referred to as packet scheduling or scheduling.</p>
<p id="p-0004" num="0003">In many cases scheduling is done in a static manner. That is, if Interface A is available, all packets are sent via it. If, however, Interface A is no longer available all packets are sent via Interface B. For example, this is the default behavior of many multi-homed IP routers. This approach is used for a variety of reasons inherent in way that IP networks address and route packets. However, there exist systems where a more advanced scheduling algorithm can yield beneficial outcomes.</p>
<p id="p-0005" num="0004">More advanced scheduling algorithms fall into one of two categories; scheduling on a per-packet basis; and scheduling on a per-flow basis. When scheduling on a per-packet basis, a scheduler decides how to assign a given packet to a network interface based upon how it assigned the previous packet. An example of a simple per-packet scheduling algorithm is a round robin approach. Thus each packet is scheduled over a subsequent interface. These scheduling algorithms work best in environments where the network characteristics of each link are very similar, since network interfaces with different network characteristics can introduce packet reordering into a flow, which may be detrimental to the behavior of many flows.</p>
<p id="p-0006" num="0005">Scheduling on a per-flow bases attempts to avoid this pitfall by operating on packets based on the flow to which they belong, and schedule packets from each flow over a common interface. In a per-flow scheduler, each flow is assigned to a single interface, but all interfaces can be in use at once if multiple flows are present in the system. An example of a simple per-flow scheduling algorithm is a round robin algorithm. However, the round robin algorithm is used to assign entire flows to a corresponding interface, and not individual packets. Once a flow is assigned to an interface, all the packets associated with that flow are routed over the assigned interface.</p>
<p id="p-0007" num="0006">In order to manage multiple flows, a table is created to map each flow to the interface to which it is assigned. This ensures that subsequent packets will be scheduled over the proper interface. However, a table-based approach has several disadvantages. First, it is necessary to expire entries from the table, which is not a trivial exercise for stateless flows like User Data Protocol (UDP) flows. Second, it is necessary to access the table every time a packet arrives, which can be an inefficient operation for large tables or a highly complex implementation if more efficient data structures are used. Third, this approach requires that the amount of data stored be based upon the number of flows, which can grow quite large as clients make large numbers of connections. On a server, having large tables like this for each client may be highly inefficient or prohibitively expensive.</p>
<p id="p-0008" num="0007">Yet further, some network systems require packet scheduling to be performed by two devices working together across a network link. For example, in a multi-interface encapsulation system, it may be necessary for both a client component and a server component to perform packet scheduling when sending packets across the encapsulation system to their destination. In such environments, it is desirable for both the client and server to be making the same scheduling decisions, to avoid the traffic sent in one direction be subject to different network conditions than traffic being sent in the other direction. For example, if a Transmission Control Protocol (TCP) stream is scheduled across a high speed, low latency interface but the acknowledgements are scheduled across a high-latency interface, then the TCP stream will throttle itself unnecessarily because TCP will incorrectly interpret the slow acknowledgments as a slow network. Thus, in order to ensure that both the client and server are scheduling packets across the same network link, the table needs to be maintained by both devices. Further, synchronization data needs to be sent between the devices for each new flow, increasing the bandwidth overhead.</p>
<p id="p-0009" num="0008">Accordingly, it is desirable to implement a scheduling algorithm that reduces the system overhead required for current implementations.</p>
<heading id="h-0002" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0010" num="0009">The scheduling algorithms described herein address scheduling of packets on a per-flow basis in an approach that reduces both the amount of memory and computational power used by each device, and the bandwidth overhead necessary to synchronize two devices across a network.</p>
<p id="p-0011" num="0010">Thus, in accordance with an aspect of the present invention there is provided a scheduler configured to schedule packets in a plurality of flows to a corresponding one of a plurality of interfaces, each of the packets including a flow identifier for identifying a corresponding one of the plurality of flows, the scheduler comprising: a processor; memory having stored thereon instructions, which when executed by the processor implement the steps of: assigning sections within a predefined range to corresponding ones of the plurality of interfaces; for each packet, applying a hash function to the flow identifier to obtain a hash value, the hash function configured to evenly distribute the hash value within the predefined range; for each packet, determining in which of the sections the hash value falls and identifying the corresponding one of the plurality of interfaces accordingly.</p>
<p id="p-0012" num="0011">In accordance with a further aspect of the present invention, there is provided a computer readable medium having stored thereon instructions for scheduling packets in a plurality of flows to a corresponding one of a plurality of interfaces, each of the packets including a flow identifier for identifying a corresponding one of the plurality of flows, the instructions, when executed by a processor, operable to perform the steps of: assigning sections within a predefined range to corresponding ones of the plurality of interfaces; for each packet, applying a hash function to the flow identifier to obtain a hash value, the hash function configured to evenly distribute the hash value within the predefined range; for each packet, determining in which of the sections the hash value falls and identifying the corresponding one of the plurality of interfaces accordingly.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram illustrating a sample multi-interface system (prior art);</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 2</figref> is a graph illustrating the distribution of flow identifiers after being processed by a hash function;</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 3</figref> is a block diagram illustrating an address range divided into sections;</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 4</figref> is a flow chart illustrating operation of a scheduling algorithm;</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 5</figref> is a block diagram of the address range in <figref idref="DRAWINGS">FIG. 3</figref>, further divided into subsections</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 6</figref> is a flow chart illustrating operation of a rebalancing algorithm</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIGS. 7</figref><i>a </i>and <b>7</b><i>b </i>are block diagrams illustrating rebalancing of the section.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading>
<p id="p-0020" num="0019">Referring to <figref idref="DRAWINGS">FIG. 1</figref>, a sample multi-interface system is illustrated generally by numeral <b>100</b>. The multi-interface system comprises a client <b>102</b>, a plurality of access points <b>103</b>, a network <b>104</b>, and a server <b>106</b>. The client includes a scheduler <b>108</b> and a plurality of interfaces <b>110</b>. Similarly, the server <b>106</b> includes the scheduler <b>108</b> and the plurality of interfaces <b>110</b>. The access points <b>103</b> are configured to facilitate transfer of data between a corresponding one of the interfaces <b>110</b> and the network <b>104</b>.</p>
<p id="p-0021" num="0020">The interfaces <b>110</b> can include wireless local area network interfaces, such as Wi-Fi, wireless wide are network interfaces, such as the 3G or 4G mobile telecommunication standards, or hard-wired network interfaces, such as Ethernet.</p>
<p id="p-0022" num="0021">The multi-interface system <b>100</b> is described for illustration purposes only. For example, in a practical application, the plurality of interfaces <b>110</b> will not be physically located at the server <b>106</b>, but likely at an edge server or servers in the network <b>104</b>. Various other methods, known or proprietary, can be implemented without affecting the present invention, as will be appreciated by a person of ordinary skill in the art.</p>
<p id="p-0023" num="0022">Thus, in order to distribute flows across the plurality of interfaces <b>110</b>, a scheduling algorithm implemented by the scheduler <b>108</b> includes a hash function. In the present context, a flow refers to a set of ordered packets that are logically part of the same communication between an application on the client <b>102</b> and the server <b>106</b>.</p>
<p id="p-0024" num="0023">Each of the packets transmitted between the client <b>102</b> and the server <b>106</b> includes a flow identifier that identifies the flow to which it is associated. The hash function is applied to the flow identifier of each packet to determine to which of the interfaces <b>110</b> the packet is associated. Since all packets in a flow will have the same flow identifier, all packets in a flow will be routed over the same interface. In this manner, a table is not required to manage the distribution of the flows. Moreover, less overhead bandwidth is required to synchronize the schedulers <b>108</b> at the client <b>102</b> and the server <b>106</b>. The scheduling algorithm and its implementation are described in more detail below.</p>
<p id="p-0025" num="0024">In Internet Protocol (IP) network systems, there exist transport protocols such as Transmission Control Protocol (TCP), User Datagram Protocol (UDP), Stream Control Transmission Protocol (SCTP), and others that are used by applications to transmit and receive data from a single application between endpoints in the network. In IP networks, endpoints are uniquely represented by an IP address. Similarly, applications are uniquely represented on each endpoint by one or more transport protocol port addresses. When an application attempts to communicate with the server <b>106</b>, it generates a local port that will receive data sent by the server <b>106</b>. This mechanism of generating a port on the client is referred to as generating an ephemeral port, since the port is only valid for the duration of the session. The combination of source address, source port, destination address, destination port, and protocol type, is referred to as a five-tuple.</p>
<p id="p-0026" num="0025">Depending on the transport protocol, the flow may be uniquely identified by just the source and destination IP address plus protocol type. However, when the transport protocol has a port number, such as in the case of TCP, UDP or SCTP, the five-tuple uniquely identifies the flow.</p>
<p id="p-0027" num="0026">Each TCP, SCTP, and UDP packet contains two port numbers; a source port and a destination port. Typically, one of the port numbers is a well-known port number, such as port number <b>80</b> for Hyper Text Transfer Protocol (HTTP). This port number is usually associated with the server <b>106</b> and is not very useful, since every application could use several flows with the same well-known port number. The other port number, however, known as an &#x201c;ephemeral port number&#x201d;, is a short-lived, automatically allocated number. The ephemeral port number is generated for each new flow and is maintained as long as the flow is active. Once the flow terminates, the ephemeral port number can be reassigned. For packets that are sent from the client <b>102</b> to the server <b>106</b>, the ephemeral port number is the source port. For packets that are sent from the server <b>106</b> to the client <b>102</b>, the ephemeral port number is the destination port number.</p>
<p id="p-0028" num="0027">Since it is unknown at implementation which side of the connection is the client <b>102</b> and which side is the server <b>106</b>, the port number ranges can be used to make the determination. Well-known port numbers are typically issued from a central registry&#x2014;traditionally operated by the Internet Assigned Numbers Authority (IANA), and these values are typically assigned in the range from 0 to several thousand. IANA has recommended that ephemeral ports be allocated from 49,152 to 65,535. That is, the well-known port numbers have relatively small values and ephemeral port numbers have relatively large values. Thus, for each packet the flow identifier is determined to be the larger of the port numbers. Since the port numbers are already included in a header of the packets, no additional bandwidth is required.</p>
<p id="p-0029" num="0028">Alternatively, the scheduler <b>108</b> at the client <b>102</b> can be programmed to use the source port as the flow identifier and the scheduler <b>108</b> at the server <b>106</b> can be programmed to use the destination port as the flow identifier.</p>
<p id="p-0030" num="0029">The flows, and consequently the packets, are assigned to different interfaces based on their flow identifier. In the present embodiment, the port number is a 16-bit number, so the address space is 65,536 address. However, because ephemeral port numbers are typically assigned sequentially, it is difficult to assign flow identifiers to interfaces effectively.</p>
<p id="p-0031" num="0030">Accordingly, a hash function is configured to distribute the sequentially generated flow identifiers relatively evenly in the address space. In the present embodiment, the hash function reverses the order of the bits in the 16-bit flow identifier resulting in a hash value. The resulting 16-bit hash value &#x201c;points&#x201d; to a specific spot in the range from 0-65535. This particular hash function has interesting characteristics for sequential values. Referring to Table 1, port numbers, reverse-bit values and resulting hash values are listed. For ease of explanation only 4-bit values are shown. Referring to <figref idref="DRAWINGS">FIG. 2</figref>, a graph illustrating the distribution of the hashed value of the port number is illustrated by numeral <b>200</b>.</p>
<p id="p-0032" num="0031">
<tables id="TABLE-US-00001" num="00001">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="5">
<colspec colname="offset" colwidth="21pt" align="left"/>
<colspec colname="1" colwidth="28pt" align="center"/>
<colspec colname="2" colwidth="70pt" align="center"/>
<colspec colname="3" colwidth="28pt" align="center"/>
<colspec colname="4" colwidth="70pt" align="center"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="4" rowsep="1">TABLE 1</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="4" align="center" rowsep="1"/>
</row>
<row>
<entry/>
<entry>Port</entry>
<entry>Port in</entry>
<entry>Hashed</entry>
<entry>Hashed</entry>
</row>
<row>
<entry/>
<entry>Number</entry>
<entry>Bits</entry>
<entry>Bits</entry>
<entry>Value</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="4" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="5">
<colspec colname="offset" colwidth="21pt" align="left"/>
<colspec colname="1" colwidth="28pt" align="char" char="."/>
<colspec colname="2" colwidth="70pt" align="char" char="."/>
<colspec colname="3" colwidth="28pt" align="char" char="."/>
<colspec colname="4" colwidth="70pt" align="char" char="."/>
<tbody valign="top">
<row>
<entry/>
<entry>0</entry>
<entry>0000</entry>
<entry>0000</entry>
<entry>0</entry>
</row>
<row>
<entry/>
<entry>1</entry>
<entry>0001</entry>
<entry>1000</entry>
<entry>8</entry>
</row>
<row>
<entry/>
<entry>2</entry>
<entry>0010</entry>
<entry>0100</entry>
<entry>4</entry>
</row>
<row>
<entry/>
<entry>3</entry>
<entry>0011</entry>
<entry>1100</entry>
<entry>12</entry>
</row>
<row>
<entry/>
<entry>4</entry>
<entry>0100</entry>
<entry>0010</entry>
<entry>2</entry>
</row>
<row>
<entry/>
<entry>5</entry>
<entry>0101</entry>
<entry>1010</entry>
<entry>10</entry>
</row>
<row>
<entry/>
<entry>6</entry>
<entry>0110</entry>
<entry>0110</entry>
<entry>6</entry>
</row>
<row>
<entry/>
<entry>7</entry>
<entry>0111</entry>
<entry>1110</entry>
<entry>14</entry>
</row>
<row>
<entry/>
<entry>8</entry>
<entry>1000</entry>
<entry>0001</entry>
<entry>1</entry>
</row>
<row>
<entry/>
<entry>9</entry>
<entry>1001</entry>
<entry>1001</entry>
<entry>9</entry>
</row>
<row>
<entry/>
<entry>10</entry>
<entry>1010</entry>
<entry>0101</entry>
<entry>5</entry>
</row>
<row>
<entry/>
<entry>11</entry>
<entry>1011</entry>
<entry>1101</entry>
<entry>13</entry>
</row>
<row>
<entry/>
<entry>12</entry>
<entry>1100</entry>
<entry>0011</entry>
<entry>3</entry>
</row>
<row>
<entry/>
<entry>13</entry>
<entry>1101</entry>
<entry>1011</entry>
<entry>11</entry>
</row>
<row>
<entry/>
<entry>14</entry>
<entry>1110</entry>
<entry>0111</entry>
<entry>7</entry>
</row>
<row>
<entry/>
<entry>15</entry>
<entry>1111</entry>
<entry>1111</entry>
<entry>15</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="4" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0033" num="0032">Considering the example illustrated in Table 1, assume only four flows are generated. Without applying the hash function, the port numbers assigned to the flows are 0, 1, 2 and 3, which are difficult to distribute across different interfaces given their proximity across the range of 16 possible values. However, after applying the hash function the port numbers have corresponding hash values of 0, 8, 4 and 12, respectively. Since these numbers are much more evenly distributed across the range of 16 possible values, it becomes easier to distribute them across the different interfaces, as will be described below.</p>
<p id="p-0034" num="0033">Referring to <figref idref="DRAWINGS">FIG. 3</figref>, a block diagram illustrating the address space is illustrated generally by numeral <b>300</b>. The address space <b>300</b> is divided into sections <b>302</b>. The number of sections <b>302</b> corresponds with the number of interfaces <b>110</b> available. The sections <b>302</b> are defined by thresholds <b>304</b>. Assignment of a threshold value for the thresholds <b>304</b> can be static or dynamic. For example, if there are three interfaces <b>110</b> and all three interfaces <b>110</b> are expected to have the same available bandwidth, the address space can be divided in a mostly equal fashion. Thus, a first threshold <b>304</b><i>a </i>is set at 22,000 and a second threshold is set at 44,000. Packets are routed via a corresponding one of the interfaces by applying the hash function to the flow identifier. If the resulting hash value is from 0 to the first threshold <b>304</b><i>a </i>then the packet is routed via a first one of the interfaces <b>110</b>. If the resulting hash value falls between the first threshold <b>304</b><i>a </i>and the second threshold <b>304</b><i>b </i>then the packet is routed via a second one of the interfaces <b>110</b>. If the resulting hash value is from the second threshold <b>304</b><i>b </i>to <b>65</b>,<b>535</b> then the packet is routed via a third one of the interfaces <b>110</b>.</p>
<p id="p-0035" num="0034">Alternatively, if the different interfaces <b>110</b> have different available bandwidths, then the first and second thresholds <b>304</b><i>a </i>and <b>304</b><i>b </i>are set to weight the address space accordingly. For example, if the first one of the interfaces <b>110</b> accounts for 20% of the overall bandwidth, the second one of the interfaces accounts for 30% of the overall bandwidth and the third one of the interfaces accounts for 50% of the overall bandwidth, then the first threshold <b>304</b><i>a </i>can be set at 13,000 and the second threshold <b>304</b><i>b </i>can be set at 33,000.</p>
<p id="p-0036" num="0035">Further, the thresholds <b>304</b> can be modified dynamically in response to changing conditions at the interfaces <b>110</b>. For example, if the bandwidth of the first one of the interfaces <b>110</b> decreases, then the first threshold <b>304</b><i>a </i>is lowered to compensate. The second threshold <b>304</b><i>b </i>is also adjusted so as not to overload the second one of the interfaces. In addition to the bandwidth, other network conditions can be used to adjust the thresholds <b>304</b>. Scheme used to determine link quality, either known or proprietary, may be used.</p>
<p id="p-0037" num="0036">Accordingly, it will be appreciated that the relatively even distribution of the hash values of the flow identifiers allows flows, and therefore packets, to be assigned to a corresponding interface <b>110</b> simply by segmenting the address space and assigning the hash value of the flow identifier to an interface <b>110</b> accordingly.</p>
<p id="p-0038" num="0037">Referring to <figref idref="DRAWINGS">FIG. 4</figref>, a flow chart illustrating the steps taken by the scheduler <b>108</b> to implement the scheduling algorithm described above is illustrated generally by numeral <b>400</b>. At step <b>402</b>, a data packet is received at the scheduler.</p>
<p id="p-0039" num="0038">At step <b>404</b>, the scheduler extracts the flow identifier from the data packet. As previously described, in the present embodiment the higher of the two port numbers associated with the data packet is the ephemeral port number and, thus, the flow identifier.</p>
<p id="p-0040" num="0039">At step <b>406</b> the scheduler applies the hash function to determine a hash value for the flow identifier. As described above, in the present embodiment the hash function is a reversal of the bits of the flow identifier.</p>
<p id="p-0041" num="0040">At step <b>408</b>, the data packet is assigned to one of the interfaces <b>110</b> based on the determined hash value. As described above, the hash value is compared with a plurality of thresholds <b>304</b> to determine to which of the interfaces the data packet should be assigned. Continuing the previous example of three possible interfaces, if the hash value falls between 0 and the first threshold <b>304</b><i>a</i>, then the data packet is assigned to the first one of the interfaces <b>110</b>. If the hash value falls between the first threshold <b>304</b><i>a </i>and the second threshold <b>304</b><i>b</i>, then the data packet is assigned to the second one of the interfaces <b>110</b>. If the hash value falls between the second threshold <b>304</b><i>b </i>and 65,535, then the data packet is assigned to the third one of the interfaces <b>110</b>. Each data packet within the same flow will have the same flow identifier and, therefore, the same hash value. Thus, each data packet in a flow will be scheduled over the same interface, unless the thresholds <b>304</b> change.</p>
<p id="p-0042" num="0041">In summary, the scheduling algorithm described herein is highly efficient because the amount of data it stores grows only based upon the number of interfaces available to the system instead of the number of flows present. Given that each device is unlikely to have more than a handful of interfaces at any given point in time, a much smaller amount of data needs to be stored on the client and the server, making it easier to scale.</p>
<p id="p-0043" num="0042">Additionally, much less data needs to be synchronized between the client and server. Specifically, all that needs to be synchronized are the values that mark the different sections <b>302</b> assigned to each interface within the 16-bit address space.</p>
<p id="p-0044" num="0043">If the network conditions do not change then no additional data needs to be transmitted. It is only when a link changes enough to require a change in the thresholds <b>304</b> that an update is required.</p>
<p id="p-0045" num="0044">Thus, no complex data structure is required to store this data. Indeed storing the portion of the 16-bit space assigned to each interface requires no more than a single 16-bit entry in an array that is the same size as the number of interfaces <b>110</b> available. Such a solution is less error-prone than traditional solutions and requires less implementation complexity.</p>
<p id="p-0046" num="0045">Finally, it is not required to expire entries from the table, since there is no table. Since information is being stored about the interfaces <b>110</b> rather than the flows, such issues disappear.</p>
<p id="p-0047" num="0046">The scheduling algorithm described above ensures a good distribution of flows across the plurality of interfaces <b>110</b> depending, at least in part, on network characteristics. However, an even distribution of flows is not always sufficient for efficient utilization of the available interfaces. For example, although a client may have established fifty flows, only three of these flows are actually being used to transmit data at any significant rate. If all three of the flows happen to have been assigned to the same one of the interfaces <b>110</b>, the data distribution will be poor even though good flow distribution will have been achieved. To help address this issue, the scheduling algorithm further analyses the bandwidth within each of the sections <b>302</b> of the address range to determine if one or more of the plurality of interfaces <b>110</b> is being underutilized. If it is determined that one or more of the plurality of interfaces <b>110</b> is being underutilized, the thresholds <b>304</b> may be adjusted accordingly, as will be described below.</p>
<p id="p-0048" num="0047">Referring to <figref idref="DRAWINGS">FIG. 5</figref>, a block diagram illustrating the address space is illustrated generally by numeral <b>500</b>. Continuing the previous example, the 16-bit address range from 0-65,535 is segmented into a plurality of sections <b>302</b> based on the number of interfaces <b>110</b> available. The boundaries of the sections are set by threshold <b>304</b> based upon how much traffic it is determined to allocate to each interface, assuming that each flow will generate a relatively equal amount of traffic per flow.</p>
<p id="p-0049" num="0048">To facilitate modifying the thresholds <b>304</b> in cases where a small number of flows are generating a majority of traffic, each section <b>302</b> is divided into a plurality of subsections <b>502</b>. In the present embodiment, each section <b>302</b> is divided into four subsections <b>502</b> as that has been determined to provide sufficient granularity for the analysis, without requiring a significant amount of storage space. However, a different number of subsections <b>502</b> may be used depending on the implementation.</p>
<p id="p-0050" num="0049">For each subsection, the scheduler <b>308</b> records the total number of bytes that have been transmitted by flows that have a hash value associated therein. Thus, it becomes possible to start adjusting the thresholds <b>304</b> based upon the amount of data each flow has sent.</p>
<p id="p-0051" num="0050">For a predefined accumulation time period, the scheduler <b>108</b> examines the number of bytes sent across each one of the interfaces <b>110</b>, and determines if the distribution of traffic across the interfaces matches the desired distribution. If one of the interfaces <b>110</b> is transmitting a higher proportion of traffic than desired then a rebalancing algorithm is executed to bring the traffic distribution closer to the desired distribution. However, it will be appreciated that it will often not be able to distribute the traffic in an ideal fashion. Accordingly, if the distribution of traffic is only off the ideal distribution by a small amount then nothing is done to avoid computational overhead for minimal gain. The value of the small amount can vary depending on the implementation and the requirements placed on the scheduler. Yet further, the rebalancing algorithm also determines whether or not rebalancing the distribution of traffic will improve or worsen the end result.</p>
<p id="p-0052" num="0051">Referring to <figref idref="DRAWINGS">FIG. 6</figref>, the rebalancing algorithm is described in greater detail as follows. As described with reference to <figref idref="DRAWINGS">FIG. 5</figref>, the address range is divided into n (in <figref idref="DRAWINGS">FIG. 5</figref>, n=3) sections <b>302</b> based on the number of available interfaces. Each of the sections if further divided into m (in <figref idref="DRAWINGS">FIG. 5</figref>, m=4) subsections <b>502</b>.</p>
<p id="p-0053" num="0052">At step <b>602</b>, it is determined whether or not the accumulation period has passed. Once the accumulation period has passed the rebalancing algorithm continues at step <b>604</b>. At step <b>604</b>, the rebalance algorithm retrieves the statistics from the scheduler <b>108</b> regarding the number of bytes transmitted via each one of the subsections <b>502</b> and calculates a total transmitted byte count accordingly. At step <b>606</b>, the rebalancing algorithm starts the first one of the subsections by setting a subsection count to 1, a section count to 1 and an accumulated bandwidth to 0.</p>
<p id="p-0054" num="0053">At step <b>608</b>, the rebalancing algorithm retrieves the amount of data transmitted for the subsection identified by the subsection count and adds it to the accumulated byte count.</p>
<p id="p-0055" num="0054">At step <b>610</b>, the rebalancing algorithm calculates a percentage error that would result from: assigning the current subsection to the interface identified by the subsection count; and not assigning the current subsection to the interface identified by the subsection count.</p>
<p id="p-0056" num="0055">In the present embodiment the percentage error from assigning the current subsection to the interface is determined by comparing the ratio of the accumulated byte count to total byte count with the desired distribution for the section. The percentage error from not assigning the current subsection to the interface is determined by comparing the ratio of the accumulated byte count, less the byte count of the current subsection, to total byte count with the desired distribution percentage for the section.</p>
<p id="p-0057" num="0056">In an alternate embodiment, adjusting the thresholds is determined recursively. In this embodiment, the rebalancing algorithm runs itself recursively given both possible decisions to assign or not assign a given subsection to the current section. The rebalancing algorithm selects the result with the lower percentage error.</p>
<p id="p-0058" num="0057">At step <b>612</b>, it is determined which of the percentage errors calculated at step <b>610</b> is greater. If the percentage error from assigning the subsection to the interface identified by the section count is less than the error of not assigning it, then the rebalancing algorithm continues at step <b>614</b>. At step <b>614</b>, the subsection count is incremented and the rebalancing algorithm returns to step <b>610</b>.</p>
<p id="p-0059" num="0058">If the percentage error from assigning the subsection to the interface identified by the section count is greater than the error of not assigning it, then the rebalancing algorithm continues at step <b>616</b>. At step <b>616</b>, the section count is incremented, the accumulated byte count is reset to 0 and the threshold <b>304</b> is adjusted to correspond with the upper address of the last subsection, that is the subsection identified by the subsection count less one.</p>
<p id="p-0060" num="0059">At step <b>618</b>, it is determined if all of the sections <b>302</b> has been resized. In the present embodiment, this is achieved by determining if the section count is equal to the number of sections m. If the section count is less than m then the rebalancing algorithm returns to step <b>610</b>. If the section count is equal to m, then all of the sections <b>302</b> have been resized. At step <b>620</b>, each of the new sections <b>302</b> are divided into four equal subsections <b>502</b> and the rebalancing algorithm starts anew.</p>
<p id="p-0061" num="0060">The rebalancing algorithm can be performed independently on the client <b>102</b> and the server <b>106</b>. However, this may result in a transient period where the thresholds <b>304</b> on the client <b>102</b> and the server <b>106</b> are not perfectly aligned. Since the client <b>102</b> and the server <b>106</b> are each independently calculating the thresholds, the corresponding schedulers <b>108</b> will eventually converge onto the same result. This delay may not be important, depending on the type of Internet traffic generated. If, however, it is important to not have either scheduler diverge in behavior, it is possible for one of the client <b>102</b> or the server <b>106</b> to send an update to the other when the thresholds <b>302</b> are recalculated.</p>
<p id="p-0062" num="0061">Referring to <figref idref="DRAWINGS">FIGS. 7</figref><i>a </i>and <b>7</b><i>b</i>, an example of the affect of the rebalancing algorithm on the distribution of the threshold is an address range is shown. Referring first to <figref idref="DRAWINGS">FIG. 7</figref><i>a</i>, it is initially established that there are two interfaces <b>110</b> and, thus, two sections <b>302</b>. Due to network conditions it is desirable to split the flow distribution by 60% to a first section and 40% to a second section. Accordingly, the threshold is set at 40,000. This number is approximated for illustrative purpose only.</p>
<p id="p-0063" num="0062">The first section is divided into four equal subsections: 0 to 10,000; 10,001 to 20,000; 20,001 to 30,000; and 30,001 to 40,000. Similarly, the second section is divided into four subsections: 40,001 to 46,384; 46,385 to 52,768; 52,769 to 59,152; and 59,153 to 65,535. The byte count for the each of the subsections during the accumulation period is determined to be 10 kB, 100 kB, 50 kB, 100 kB, 0 kB, 10 kB, 0 kB and 0 kB respectively. Thus the total utilized bandwidth is 270 kB.</p>
<p id="p-0064" num="0063">Since it is desirable that the first interface receives 60% of the traffic, ideally 162 kB of data would be transmitted there through. However, a quick inspection of <figref idref="DRAWINGS">FIG. 7</figref><i>a </i>reveals that 260 kB are being transmitted through the first interface, well above the desired amount.</p>
<p id="p-0065" num="0064">Accordingly, the rebalancing algorithm determines where to move the threshold as follows. The byte count of the first subsection is analyzed. Since 10 kB is closer than 0 kB to 162 kB, the first subsection is included in the first section. The byte count of the second subsection is then analyzed. Since 110 kB is closer than 10 kB to 162 kB, the second subsection is included in the first section. The byte count of the third subsection is then analyzed. Since 160 kB is closer than 110 kB to 162 kB, the third subsection is included in the first section. The byte count of the fourth subsection is then analyzed. Since 260 kB is further than 160 kB from 162 kB, the fourth subsection is not included in the first section. Accordingly, the threshold is moved to 30,000.</p>
<p id="p-0066" num="0065">Since there are only two sections, only one threshold needs to be determined. Accordingly, the subsections are recalculated to represent four equal portions of their corresponding sections. Specifically, the first section is divided into four equal subsections: 0 to 7,500; 7,501 to 15,000; 15,001 to 22,500; and 22,501 to 30,000. Similarly, the second section is divided into four subsections: 30,001 to 38,884; 38,885 to 47,767; 47,768 to 56,653; and 56,654 to 65,535.</p>
<p id="p-0067" num="0066">As illustrated above, restructuring the sections and corresponding subsections provides an increased resolution in areas of interest. Consider, for example, the first section. Initially, the threshold was set at 40,000 and the subsections were 10,000 possible hash values in size. However, due to a greater than expected byte count in the first section, the threshold was moved to 30,000. The subsections were resized accordingly to 7,500 possible hash values, thereby increasing resolution in the section that is demonstrating the higher byte count. In contrast, the subsections in the second section increase in size, therefore decreasing in resolution, but in an area with less activity.</p>
<p id="p-0068" num="0067">Thus, the rebalancing algorithm rearranges the thresholds <b>304</b> based on the use of the flows. Accordingly, if the data transmitted by each flow does not change significantly then the rebalancing algorithm will not change the thresholds <b>304</b>.</p>
<p id="p-0069" num="0068">Although the embodiments described above reference a particular hash function and rebalancing algorithm, variations may be implement without affecting the desired result. Yet further, if it is determined that a particular hash function is not yielding a sufficiently even distribution of flows, the scheduler <b>108</b> may fall back to a secondary, or even tertiary, hash function to improve the result.</p>
<p id="p-0070" num="0069">Yet further, several examples are provided herein for illustrative purposes. For ease of explanation only, the percentages may not match up exactly with the specific numbers used. The ability to tolerate such variations will vary depending on the implementation.</p>
<p id="p-0071" num="0070">Using the foregoing specification, the invention may be implemented as a machine, process or article of manufacture by using standard programming and/or engineering techniques to produce programming software, firmware, hardware or any combination thereof.</p>
<p id="p-0072" num="0071">Any resulting program(s), having computer-readable instructions, may be stored within one or more computer-usable media such as memory devices or transmitting devices, thereby making a computer program product or article of manufacture according to the invention. As such, the terms &#x201c;software&#x201d; and &#x201c;application&#x201d; as used herein are intended to encompass a computer program existent as instructions on any computer-readable medium such as on any memory device or in any transmitting device, that are to be executed by a processor.</p>
<p id="p-0073" num="0072">Examples of memory devices include, hard disk drives, diskettes, optical disks, magnetic tape, semiconductor memories such as FLASH, RAM, ROM, PROMS, and the like. Examples of networks include, but are not limited to, the Internet, intranets, telephone/modem-based network communication, hard-wired/cabled communication network, cellular communication, radio wave communication, satellite communication, and other stationary or mobile network systems/communication links.</p>
<p id="p-0074" num="0073">A machine embodying the invention may involve one or more processing systems including, for example, CPU, memory/storage devices, communication links, communication/transmitting devices, servers, I/O devices, or any subcomponents or individual parts of one or more processing systems, including software, firmware, hardware, or any combination or subcombination thereof, which embody the invention as set forth in the claims.</p>
<p id="p-0075" num="0074">Using the description provided herein, those skilled in the art will be readily able to combine software created as described with appropriate general purpose or special purpose computer hardware to create a computer system and/or computer subcomponents embodying the invention, and to create a computer system and/or computer subcomponents for carrying out the method of the invention.</p>
<p id="p-0076" num="0075">Although preferred embodiments of the invention have been described herein, it will be understood by those skilled in the art that variations and combinations may be made thereto without departing from the scope of the appended claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A scheduler configured to schedule packets in a plurality of flows to a corresponding one of a plurality of interfaces, each of the packets including a flow identifier for identifying a corresponding one of the plurality of flows, the scheduler comprising:
<claim-text>a processor;</claim-text>
<claim-text>memory having stored thereon instructions, which when executed by the processor implement the steps of:
<claim-text>assigning sections within a predefined range to corresponding ones of the plurality of interfaces;</claim-text>
<claim-text>selecting an ephemeral port number associated with the flow to be used as the flow identifier by comparing port numbers for each packet and selecting a larger one of the port numbers, wherein an ephemeral port range is higher than a predefined known port range;</claim-text>
<claim-text>for each packet, applying a hash function to the flow identifier to obtain a hash value, the hash function configured to evenly distribute the hash value within the predefined range; and</claim-text>
<claim-text>for each packet, determining in which of the sections the hash value falls and identifying the corresponding one of the plurality of interfaces accordingly.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The scheduler of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the ephemeral port number is:
<claim-text>a source port for packets transmitted from a client to a server; or</claim-text>
<claim-text>a destination port for packets transmitted from the server to the client.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The scheduler of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the hash function reverses the order of bits in the flow identifier.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The scheduler of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each of the sections within the predefined range is substantially equal.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The scheduler of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each of the sections within the predefined range is assigned based on anticipated performance of the corresponding one of the plurality of interfaces.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The scheduler of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each of the sections within the predefined range is dynamically modified based on performance metrics of the corresponding one of the plurality of interfaces.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The scheduler of <claim-ref idref="CLM-00001">claim 1</claim-ref> comprising further instructions for changing the hash function if it is determined that the hash values are not evenly distributed within the predefined range.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. A scheduler configured to schedule packets in a plurality of flows to a corresponding one of a plurality of interfaces, each of the packets including a flow identifier for identifying a corresponding one of the plurality of flows, the scheduler comprising:
<claim-text>a processor;</claim-text>
<claim-text>memory having stored thereon instructions, which when executed by the processor implement the steps of:
<claim-text>assigning sections within a predefined range to corresponding ones of the plurality of interfaces;</claim-text>
<claim-text>for each packet, applying a hash function to the flow identifier to obtain a hash value, the hash function configured to evenly distribute the hash value within the predefined range;</claim-text>
<claim-text>for each packet, determining in which of the sections the hash value falls and identifying the corresponding one of the plurality of interfaces accordingly;</claim-text>
<claim-text>determining performance for each of the sections over a predefined accumulation period;</claim-text>
<claim-text>comparing the determined performance with a corresponding anticipated performance and adjusting the size of each of the sections accordingly.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The scheduler of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein performance for each of the sections is determined by dividing each of the sections into a plurality of subsections and maintaining a byte count of data transmitted by flows having a hash value associated with each of the subsections.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The scheduler of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the determined performance is compared with the corresponding anticipated performance by determining the byte count of the section and comparing it with an anticipated byte count of the section.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The scheduler of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the size of each of the sections is adjusted by assigning one or more contiguous subsections to an adjacent section to more closely match the determined byte count of the section with the anticipated byte count of the section.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The scheduler of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein adjusting the size of each of the sections improves accuracy of the scheduler due to an increased resolution when determining the performance for each of the sections as a result of a decrease in subsection size.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. A non-transitory computer readable medium having stored thereon instructions for scheduling packets in a plurality of flows to a corresponding one of a plurality of interfaces, each of the packets including a flow identifier for identifying a corresponding one of the plurality of flows, the instructions, when executed by a processor, operable to perform the steps of:
<claim-text>assigning sections within a predefined range to corresponding ones of the plurality of interfaces;</claim-text>
<claim-text>selecting an ephemeral port number associated with the flow to be used as the flow identifier by comparing port numbers for each packet and selecting a larger one of the port numbers, wherein an ephemeral port range is higher than a predefined known port range;</claim-text>
<claim-text>for each packet, applying a hash function to the flow identifier to obtain a hash value, the hash function configured to evenly distribute the hash value within the predefined range;</claim-text>
<claim-text>for each packet, determining in which of the sections the hash value falls and identifying the corresponding one of the plurality of interfaces accordingly.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The non-transitory computer readable medium of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the hash function reverses the order of bits in the flow identifier.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The non-transitory computer readable medium of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein each of the sections within the predefined range is substantially equal.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The non-transitory computer readable medium of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein each of the sections within the predefined range is assigned based on anticipated performance of the corresponding one of the plurality of interfaces.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The non-transitory computer readable medium of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein each of the sections within the predefined range is dynamically modified based on performance metrics of the corresponding one of the plurality of interfaces.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The non-transitory computer readable medium of <claim-ref idref="CLM-00013">claim 13</claim-ref> comprising further instructions for changing the hash function if it is determined that the hash values are not evenly distributed within the predefined range.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. A non-transitory computer readable medium having stored thereon instructions for scheduling packets in a plurality of flows to a corresponding one of a plurality of interfaces, each of the packets including a flow identifier for identifying a corresponding one of the plurality of flows, the instructions, when executed by a processor, operable to perform the steps of:
<claim-text>assigning sections within a predefined range to corresponding ones of the plurality of interfaces;</claim-text>
<claim-text>for each packet, applying a hash function to the flow identifier to obtain a hash value, the hash function configured to evenly distribute the hash value within the predefined range;</claim-text>
<claim-text>for each packet, determining in which of the sections the hash value falls and identifying the corresponding one of the plurality of interfaces accordingly;</claim-text>
<claim-text>determining performance for each of the sections over a predefined accumulation period; and</claim-text>
<claim-text>comparing the determined performance with a corresponding anticipated performance and adjusting the size of each of the sections accordingly.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The non-transitory computer readable medium of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein performance for each of the sections is determined by dividing each of the sections into a plurality of subsections and maintaining a byte count of data transmitted by flows having a hash value associated with each of the subsections.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The non-transitory computer readable medium of <claim-ref idref="CLM-00020">claim 20</claim-ref>, wherein the determined performance is compared with the corresponding anticipated performance by determining the byte count of the section and comparing it with an anticipated byte count of the section.</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The non-transitory computer readable medium of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the size of each of the sections is adjusted by assigning one or more contiguous subsections to an adjacent section to more closely match the determined byte count of the section with the anticipated byte count of the section.</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The non-transitory computer readable medium of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein adjusting the size of each of the sections improves accuracy of the scheduler due to an increased resolution when determining the performance for each of the sections as a result of a decrease in subsection size. </claim-text>
</claim>
</claims>
</us-patent-grant>
