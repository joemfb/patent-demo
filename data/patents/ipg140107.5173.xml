<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626272-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626272</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12685397</doc-number>
<date>20100111</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>IL</country>
<doc-number>160067</doc-number>
<date>20020726</date>
</priority-claim>
<priority-claim sequence="02" kind="national">
<country>IL</country>
<doc-number>155046</doc-number>
<date>20030323</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>1033</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>A</section>
<class>61</class>
<subclass>B</subclass>
<main-group>6</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>A</section>
<class>61</class>
<subclass>B</subclass>
<main-group>1</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classifications-cpc>
<main-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>A</section>
<class>61</class>
<subclass>B</subclass>
<main-group>1</main-group>
<subgroup>00009</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
</main-cpc>
</classifications-cpc>
<classification-national>
<country>US</country>
<main-classification>600476</main-classification>
</classification-national>
<invention-title id="d2e90">Apparatus and method for light control in an in-vivo imaging device</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>4471228</doc-number>
<kind>A</kind>
<name>Nishizawa et al.</name>
<date>19840900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348298</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>4868645</doc-number>
<kind>A</kind>
<name>Kobayashi</name>
<date>19890900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5108407</doc-number>
<kind>A</kind>
<name>Geremia et al.</name>
<date>19920400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5730702</doc-number>
<kind>A</kind>
<name>Tanaka et al.</name>
<date>19980300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5782771</doc-number>
<kind>A</kind>
<name>Hussman</name>
<date>19980700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>5833603</doc-number>
<kind>A</kind>
<name>Kovacs et al.</name>
<date>19981100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6219091</doc-number>
<kind>B1</kind>
<name>Yamanaka et al.</name>
<date>20010400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6254531</doc-number>
<kind>B1</kind>
<name>Higuchi et al.</name>
<date>20010700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6285897</doc-number>
<kind>B1</kind>
<name>Kilcoyne et al.</name>
<date>20010900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6364829</doc-number>
<kind>B1</kind>
<name>Fulghum</name>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6458074</doc-number>
<kind>B1</kind>
<name>Matsui et al.</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6462770</doc-number>
<kind>B1</kind>
<name>Cline et al.</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>6527753</doc-number>
<kind>B2</kind>
<name>Sekine et al.</name>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>6535764</doc-number>
<kind>B2</kind>
<name>Imran et al.</name>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>6667765</doc-number>
<kind>B1</kind>
<name>Tanaka</name>
<date>20031200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>6831689</doc-number>
<kind>B2</kind>
<name>Yadid-Pecht</name>
<date>20041200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>7053954</doc-number>
<kind>B1</kind>
<name>Canini</name>
<date>20060500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>7099056</doc-number>
<kind>B1</kind>
<name>Kindt</name>
<date>20060800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>7160258</doc-number>
<kind>B2</kind>
<name>Imran et al.</name>
<date>20070100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>7511733</doc-number>
<kind>B2</kind>
<name>Takizawa</name>
<date>20090300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>2001/0049497</doc-number>
<kind>A1</kind>
<name>Kalloo et al.</name>
<date>20011200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>2002/0042562</doc-number>
<kind>A1</kind>
<name>Meron et al.</name>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>2002/0138009</doc-number>
<kind>A1</kind>
<name>Brockway et al.</name>
<date>20020900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>2002/0156347</doc-number>
<kind>A1</kind>
<name>Kim et al.</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>2003/0117491</doc-number>
<kind>A1</kind>
<name>Avni et al.</name>
<date>20030600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>2003/0210334</doc-number>
<kind>A1</kind>
<name>Sarwari</name>
<date>20031100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>2003/0210439</doc-number>
<kind>A1</kind>
<name>Sarwari</name>
<date>20031100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>2003/0211405</doc-number>
<kind>A1</kind>
<name>Venkataraman</name>
<date>20031100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>EP</country>
<doc-number>0667 115</doc-number>
<kind>A1</kind>
<date>19950800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-cpc-text>A61B 1/00</classification-cpc-text>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>JP</country>
<doc-number>58-019233</doc-number>
<date>19830200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>JP</country>
<doc-number>64-086933</doc-number>
<date>19890300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>JP</country>
<doc-number>04-176443</doc-number>
<date>19920600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00033">
<document-id>
<country>JP</country>
<doc-number>04-347138</doc-number>
<date>19921200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00034">
<document-id>
<country>JP</country>
<doc-number>06-063030</doc-number>
<date>19940300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00035">
<document-id>
<country>JP</country>
<doc-number>06-142081</doc-number>
<date>19940500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00036">
<document-id>
<country>JP</country>
<doc-number>8-313823</doc-number>
<date>19961100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00037">
<document-id>
<country>JP</country>
<doc-number>H10-112118</doc-number>
<date>19980400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00038">
<document-id>
<country>JP</country>
<doc-number>11-111795</doc-number>
<date>19990400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00039">
<document-id>
<country>JP</country>
<doc-number>11-305144</doc-number>
<date>19991100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00040">
<document-id>
<country>JP</country>
<doc-number>2001-203910</doc-number>
<date>20010700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00041">
<document-id>
<country>JP</country>
<doc-number>2003-038425</doc-number>
<date>20030200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00042">
<document-id>
<country>JP</country>
<doc-number>2003-038424</doc-number>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00043">
<document-id>
<country>JP</country>
<doc-number>2003-093328</doc-number>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00044">
<document-id>
<country>WO</country>
<doc-number>WO 2004/082472</doc-number>
<date>20040900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00045">
<othercit>Extended European Search Report, issued Feb. 2, 2010, in connection with European Patent Application No. 10151811.6.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00046">
<othercit>Final Office Action, issued Dec. 23, 2009, in connection with U.S. Appl. No. 10/551,053.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00047">
<othercit>Bo-Rong Lin et al., &#x201c;A High Dynamic Range CMOS Image Sensor Design Based on Two-Frame Composition,&#x201d; Electrical Engineering Department, National Tsing-Hua University, Sep. 2003, pp. 389-392.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00048">
<othercit>Orly Yadid-Pecht et al., &#x201c;Wide Intrascene Dynamic Range CMOS APS Using Dual Sampling,&#x201d; IEEE Transactions on Electron Devices, vol. 33, No. 10, Oct. 1997.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00049">
<othercit>David Stoppa et al., &#x201c;Novel CMOS Image Sensor With a 132-dB Dynamic Range,&#x201d; IEEE Journal of Solid-State Circuits, vol. 37, No. 12, Dec. 2002.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00050">
<othercit>Final Office Action, issued Sep. 28, 2010, for U.S. Appl. No. 10/551,053.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00051">
<othercit>Office Action, issued Apr. 13, 2010, for U.S. Appl. No. 10/551,053.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>20</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>None</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>12</number-of-drawing-sheets>
<number-of-figures>20</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>11295690</doc-number>
<date>20051207</date>
</document-id>
<parent-status>ABANDONED</parent-status>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>12685397</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>10551053</doc-number>
</document-id>
<parent-status>PENDING</parent-status>
<parent-pct-document>
<document-id>
<country>WO</country>
<doc-number>PCT/IL2004/000265</doc-number>
<date>20040323</date>
</document-id>
</parent-pct-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>11295690</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation-in-part>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>10202608</doc-number>
<date>20020725</date>
</document-id>
<parent-status>ABANDONED</parent-status>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>11295690</doc-number>
</document-id>
</child-doc>
</relation>
</continuation-in-part>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60307603</doc-number>
<date>20010726</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20100110168</doc-number>
<kind>A1</kind>
<date>20100506</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Avni</last-name>
<first-name>Dov</first-name>
<address>
<city>Haifa</city>
<country>IL</country>
</address>
</addressbook>
<residence>
<country>IL</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Glukhovsky</last-name>
<first-name>Arkady</first-name>
<address>
<city>Santa Clarita</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Avni</last-name>
<first-name>Dov</first-name>
<address>
<city>Haifa</city>
<country>IL</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Glukhovsky</last-name>
<first-name>Arkady</first-name>
<address>
<city>Santa Clarita</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Pearl Cohen Zedek Latzer Baratz LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Given Imaging Ltd.</orgname>
<role>03</role>
<address>
<city>Yoqneam</city>
<country>IL</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Diep</last-name>
<first-name>Nhon</first-name>
<department>2487</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A device and method for example operating an in vivo imaging device wherein the illumination is operated at a certain rate or range of rates, and images are transmitted from the device.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="57.07mm" wi="219.71mm" file="US08626272-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="189.31mm" wi="144.61mm" orientation="landscape" file="US08626272-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="217.93mm" wi="149.18mm" orientation="landscape" file="US08626272-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="205.40mm" wi="151.55mm" orientation="landscape" file="US08626272-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="213.02mm" wi="112.69mm" file="US08626272-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="189.65mm" wi="147.24mm" orientation="landscape" file="US08626272-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="220.22mm" wi="140.46mm" file="US08626272-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="185.67mm" wi="152.15mm" orientation="landscape" file="US08626272-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="140.38mm" wi="148.25mm" orientation="landscape" file="US08626272-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="226.14mm" wi="142.66mm" orientation="landscape" file="US08626272-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="125.22mm" wi="167.64mm" file="US08626272-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="228.43mm" wi="172.89mm" file="US08626272-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="130.47mm" wi="151.89mm" file="US08626272-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">RELATED APPLICATION DATA</heading>
<p id="p-0002" num="0001">The present application is a continuation of U.S. application Ser. No. 11/295,690, entitled &#x201c;Apparatus And Method For Light Control In An In-Vivo Imaging Device&#x201d; filed on Dec. 29, 2005, which is a continuation of U.S. application Ser. No. 10/551,053, filed Sep. 23, 2005, entitled &#x201c;Apparatus and Method for Light Control in an In-Vivo Imaging Device&#x201d;, which is a national phase of International App. PCT/IL2004/000265, entitled &#x201c;Apparatus and Method for Light Control in an In-Vivo Imaging Device&#x201d; filed on Mar. 23, 2004, which in turn claims priority from Israel patent application 155046, filed on Mar. 23, 2003; in addition U.S. application Ser. No. 11/295,690 is a continuation-in-part of U.S. patent application Ser. No. 10/202,608, filed Jul. 25, 2002, which in turn claims priority from U.S. Provisional Application 60/307,603, filed Jul. 26, 2001, all of which are incorporated by reference in their entirety.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">Devices and methods for performing in-vivo imaging of passages or cavities within a body are known in the art. Such devices may include, inter alia, various endoscopic imaging systems and devices for performing imaging in various internal body cavities.</p>
<p id="p-0004" num="0003">Reference is now made to <figref idref="DRAWINGS">FIG. 1</figref> which is a schematic diagram illustrating an embodiment of an autonomous in-vivo imaging device. The device <b>10</b>A typically includes an optical window <b>21</b> and an imaging system for obtaining images from inside a body cavity or lumen, such as the GI tract. The imaging system includes an illumination unit <b>23</b>. The illumination unit <b>23</b> may include one or more discrete light sources <b>23</b>A, or may include only one light source <b>23</b>A. The one or more light sources <b>23</b>A may be a white light emitting diode (LED), or any other suitable light source, known in the art. The device <b>10</b>A includes a CMOS imaging sensor <b>24</b>, which acquires the images and an optical system <b>22</b> which focuses the images onto the CMOS imaging sensor <b>24</b>. The illumination unit <b>23</b> illuminates the inner portions of the body lumen through an optical window <b>21</b>. Device <b>10</b>A further includes a transmitter <b>26</b> and an antenna <b>27</b> for transmitting the video signal of the CMOS imaging sensor <b>24</b>, and one or more power sources <b>25</b>. The power source(s) <b>25</b> may be any suitable power sources such as but not limited to silver oxide batteries, lithium batteries, or other electrochemical cells having a high energy density, or the like. The power source(s) <b>25</b> may provide power to the electrical elements of the device <b>10</b>A.</p>
<p id="p-0005" num="0004">Typically, in the gastrointestinal application, as the device <b>10</b>A is transported through the gastrointestinal (GI) tract, the imager, such as but not limited to the multi-pixel CMOS sensor <b>24</b> of the device <b>10</b>A acquires images (frames) which are processed and transmitted to an external receiver/recorder (not shown) worn by the patient for recording and storage. The recorded data may then be downloaded from the receiver/recorder to a computer or workstation (not shown) for display and analysis. Other systems and methods may also be suitable.</p>
<p id="p-0006" num="0005">During the movement of the device <b>10</b>A through the GI tract, the imager may acquire frames at a fixed or at a variable frame acquisition rate. For example, the imager (such as, but not limited to the CMOS sensor <b>24</b> of <figref idref="DRAWINGS">FIG. 1</figref>) may acquire images at a fixed rate of two frames per second (2 Hz). However, other different frame rates may also be used, depending, inter alia, on the type and characteristics of the specific imager or camera or sensor array implementation that is used, and on the available transmission bandwidth of the transmitter <b>26</b>. The downloaded images may be displayed by the workstation by replaying them at a desired frame rate. According to this implementation, the expert or physician examining the data may be provided with a movie-like video playback, which may enable the physician to review the passage of the device through the GI tract.</p>
<p id="p-0007" num="0006">One of the limitations of electronic imaging sensors is that they may have a limited dynamic range. The dynamic range of most existing electronic imaging sensors is significantly lower than the dynamic range of the human eye. Thus, when the imaged field of view includes both dark and bright parts or imaged objects, the limited dynamic range of the imaging sensor may result in underexposure of the dark parts of the field of view, or overexposure of the bright parts of the field of view, or both.</p>
<p id="p-0008" num="0007">Various methods may be used for increasing the dynamic range of an imager. Such methods may include changing the amount of light reaching the imaging sensor, such as for example by changing the diameter of an iris or diaphragm included in the imaging device to increase or decrease the amount of light reaching the imaging sensor, methods for changing the exposure time, methods for changing the gain of the imager or methods for changing the intensity of the illumination. For example, in still cameras, the intensity of the flash unit may be changed during the exposure of the film.</p>
<p id="p-0009" num="0008">When a series of consecutive frames is imaged such as in video cameras, the intensity of illumination of the imaged field of view within the currently imaged frame may be modified based on the results of measurement of light intensity performed in one or more previous frames. This method is based on the assumption that the illumination conditions do not change abruptly from one frame to the consecutive frame.</p>
<p id="p-0010" num="0009">However, in an in vivo imaging device, for example, for imaging the GI tract, which may operate at low frame rates and which is moved through a body lumen (e.g., propelled by the peristaltic movements of the intestinal walls), the illumination conditions may vary significantly from one frame to the next frame. Therefore, methods of controlling the illumination based on analysis of data or measurement results of previous frames may not be always feasible, particularly at low frame rates.</p>
<p id="p-0011" num="0010">Therefore there is a need for an imaging device that provides more accurate illumination, possibly tailored to particular in-vivo illumination requirements or environmental conditions.</p>
<heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0012" num="0011">Some embodiments of the present invention include a device and method for operating an in vivo imaging device wherein the illumination produced by the device may be varied in intensity and/or duration according to, for example, the amount of illumination produced by the device, which is reflected back to the device. In such a manner, the illumination can be controlled and made more efficient.</p>
<p id="p-0013" num="0012">According to some embodiments of the present invention, a method for implementing light control in an in vivo device is provided. Accordingly, the parameters such as exposure time and/or the gain factor, or other parameters, for transmitting the recorded light may be altered. For example, the gain factor may be altered as a function of a light saturation level measured at least one interval within the frame exposure period. In such a manner the in vivo device can prevent cases of over and under exposure, in addition to helping to ensure that exposure ceases after full exposure is attained.</p>
<p id="p-0014" num="0013">According to some embodiments of the present invention, a method is provided for detecting problematic pixels in an imaging device. This method may enable defining and/or excluding problematic or non-functional pixels, for example based on an initial short exposure that enables a threshold saturation level to be reached only for problematic pixels.</p>
<p id="p-0015" num="0014">According to some embodiments of the present invention, a method is provided for determining when an in vivo imaging device has entered a particular part of a body. Accordingly, environmental measurement devices may be used to detect environmental parameters, such as pH levels and temperature levels etc. Results recorded from these measurement devices may be used to define areas, regions, organs etc. wherein the in vivo device may be or may have been located. The device mode may be changed in accordance with the resulting definition.</p>
<p id="p-0016" num="0015">According to some embodiments of the present invention, a method is provided for determining when an in vivo imaging device has entered a body, using dark frames. For example, when dark frames require substantial gain factor to attain full exposure, the device may be defined as being inside a body (a dark environment). The device mode may be changed in accordance with the resulting definition.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0017" num="0016">The present invention is herein described, by way of example only, with reference to the accompanying drawings, in which like components are designated by like reference numerals, it being understood that these drawings are given for illustrative purposes only and are not meant to be limiting, wherein:</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 1</figref> is a schematic diagram illustrating an embodiment of a prior art autonomous in-vivo imaging device;</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 2</figref> is a schematic block diagram illustrating part of an in-vivo imaging device having an automatic illumination control system, in accordance with an embodiment of the present invention;</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 3</figref> is a schematic cross-sectional view of part of an in-vivo imaging device having an automatic illumination control system and four light sources, in accordance with an embodiment of the present invention;</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 4</figref> is a schematic front view of the device illustrated in <figref idref="DRAWINGS">FIG. 3</figref>;</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 5</figref> is a schematic diagram illustrating a method of timing of the illumination and image acquisition in an in vivo imaging device having a fixed illumination duration, according to an embodiment of the invention;</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 6</figref> is a schematic diagram illustrating one possible configuration for an illumination control unit coupled to a light sensing photodiode and to a light emitting diode, in accordance with an embodiment of the present invention;</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 7</figref> is a schematic diagram illustrating the illumination control unit of <figref idref="DRAWINGS">FIG. 6</figref> in detail, in accordance with an embodiment of the present invention;</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 8</figref> is a schematic diagram useful for understanding a method of timing of the illumination and image acquisition in an in vivo imaging device having a variable controlled illumination duration, according to an embodiment of the invention;</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 9</figref> is a schematic diagram useful for understanding a method of timing of the illumination and image acquisition in an in vivo imaging device having a variable frame rate and a variable controlled illumination duration according to an embodiment of the invention;</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 10A</figref> is a timing diagram schematically illustrating an imaging cycle of an in vivo imaging device using an automatic illumination control method, in accordance with another embodiment of the present invention;</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 10B</figref> is a schematic exemplary graph representing the light intensity as a function of time, possible when using a method of automatic illumination control, according to an embodiment of the invention, for example as illustrated in <figref idref="DRAWINGS">FIG. 10A</figref>;</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 10C</figref> is another exemplary schematic graph representing another example of the light intensity as a function of time, possible when using a method of automatic illumination control, according to an embodiment of the invention, illustrated in <figref idref="DRAWINGS">FIG. 10A</figref>;</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 11</figref> is a schematic diagram illustrating an illumination control unit including a plurality of light sensing units for controlling a plurality of light sources, in accordance with an embodiment of the present invention;</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 12</figref> is a schematic diagram illustrating a front view of an autonomous imaging device having four light sensing units and four light sources, in accordance with an embodiment of the present invention;</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 13</figref> is a schematic top view illustrating the arrangement of pixels on the surface of a CMOS imager usable for illumination control, in accordance with an embodiment of the present invention;</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 14</figref> is a schematic top view of the pixels of a CMOS imager illustrating an exemplary distribution of control pixel groups suitable for being used in local illumination control in an imaging device, according to an embodiment of the invention;</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 15</figref> is a schematic exemplary graph representing the light saturation as a function of pixel output and time, possibly when implementing light control, according to an embodiment of the invention;</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 16A</figref> depicts a series of steps of a method according to an embodiment of the present invention;</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 16B</figref> depicts a series of steps of a method according to an alternate embodiment of the present invention; and</p>
<p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. 16C</figref> depicts a series of steps of a method according to an additional embodiment of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION OF THE INVENTION</heading>
<p id="p-0038" num="0037">Various aspects of the present invention are described herein. For purposes of explanation, specific configurations and details are set forth in order to provide a thorough understanding of the present invention. However, it will also be apparent to one skilled in the art that the present invention may be practiced without the specific details presented herein. Furthermore, well known features may be omitted or simplified in order not to obscure the present invention.</p>
<p id="p-0039" num="0038">Some embodiments of the present invention are based, inter alia, on controlling the illumination provided by the in-vivo imaging device based on light measurement which is performed within the duration of a single frame acquisition time or a part thereof.</p>
<p id="p-0040" num="0039">It is noted that while the embodiments of the invention shown hereinbelow are adapted for imaging of the gastrointestinal (GI) tract, the devices and methods disclosed herein may be adapted for imaging other body cavities or spaces.</p>
<p id="p-0041" num="0040">Reference is now made to <figref idref="DRAWINGS">FIG. 2</figref> which is a schematic block diagram illustrating part of an in-vivo imaging device having an automatic illumination control system, in accordance with an embodiment of the present invention. The device <b>30</b> may be constructed as a swallowable video capsule as disclosed for the device <b>10</b>A of <figref idref="DRAWINGS">FIG. 1</figref> or in U.S. Pat. No. 5,604,531 to Iddan et al., or in Co-pending PCT Patent Application, Publication no. WO 01/65995 to Glukhovsky et al, both hereby incorporated by reference in their entirety. However, the system and method of the present invention may be used in conjunction with other in-vivo imaging devices.</p>
<p id="p-0042" num="0041">The device <b>30</b> may include an imaging unit <b>32</b> adapted for imaging the GI tract. The imaging unit <b>32</b> may include an imaging sensor (not shown in detail), such as but not limited to the CMOS imaging sensor <b>24</b> of <figref idref="DRAWINGS">FIG. 1</figref>. However, the imaging unit <b>32</b> may include any other suitable type of imaging sensor known in the art. The imaging unit <b>32</b> may also include an optical unit <b>32</b>A including one or more optical elements (not shown), such as one or more lenses (not shown), one or more composite lens assemblies (not shown), one or more suitable optical filters (not shown), or any other suitable optical elements (not shown) adapted for focusing an image of the GI tract on the imaging sensor as is known in the art and disclosed hereinabove with respect to the optical unit <b>22</b> of <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0043" num="0042">The optical unit <b>32</b>A may include one or more optical elements (not shown) which are integrated with the imaging unit <b>32</b>A, such as for example, a lens (not shown) which is attached to, or mounted on, or fabricated on or adjacent to the imager light sensitive pixels (not shown) as is known in the art.</p>
<p id="p-0044" num="0043">The device <b>30</b> may also include a telemetry unit <b>34</b> suitably connected to the imaging unit <b>32</b> for telemetrically transmitting the images acquired by the imaging unit <b>32</b> to an external receiving device (not shown), such as but not limited to the receiver/recorder device disclosed in U.S. Pat. No. 5,604,531 to Iddan et al., or in Co-pending PCT Patent Application, Publication No. WO 01/65995 to Glukhovsky et al.</p>
<p id="p-0045" num="0044">The device <b>30</b> may also include a controller/processor unit <b>36</b> suitably connected to the imaging unit <b>32</b> for controlling the operation of the imaging unit <b>32</b>. The controller/processor unit <b>36</b> may comprise any suitable type of controller, such as but not limited to, an analog controller, a digital controller such as, for example, a data processor, a microprocessor, a micro-controller, or a digital signal processor (DSP). The controller/processor unit <b>36</b> may also comprise hybrid analog/digital circuits as are known in the art. The controller/processor unit <b>36</b> may be suitably connected to the telemetry unit <b>34</b> for controlling the transmission of image frames by the telemetry unit <b>34</b>.</p>
<p id="p-0046" num="0045">The controller/processor unit <b>36</b> may be (optionally) suitably connected to the imaging unit <b>32</b> for sending control signals thereto. The controller/processor unit <b>36</b> may thus (optionally) control the transmission of image data from the imaging unit <b>32</b> to the telemetry unit <b>34</b>.</p>
<p id="p-0047" num="0046">The device <b>30</b> may include an illuminating unit <b>38</b> for illuminating the GI tract. The illuminating unit <b>38</b> may include one or more discrete light sources <b>38</b>A, <b>38</b>B, to <b>38</b>N or may include only one light source; such light source(s) may be, for example, but are not limited to, the light sources <b>23</b>A of <figref idref="DRAWINGS">FIG. 1</figref>. The light source(s) <b>38</b>A, <b>38</b>B, to <b>38</b>N of the illuminating unit <b>38</b> may be white light emitting diodes, such as the light sources disclosed in co-pending PCT Patent Application, Publication No. WO 01/65995 to Glukhovsky et al. However, the light source(s) <b>38</b>A, <b>38</b>B, <b>38</b>N of the illuminating unit <b>38</b> may also be any other suitable light source, known in the art, such as but not limited to incandescent lamp(s), flash lamp(s) or gas discharge lamp(s), or any other suitable light source(s).</p>
<p id="p-0048" num="0047">It is noted that, in accordance with another embodiment of the present invention, the in vivo imaging device may include a single light source (not shown).</p>
<p id="p-0049" num="0048">The device <b>30</b> may also include an illumination control unit <b>40</b> suitably connected to the light sources <b>38</b>A, <b>38</b>B, to <b>38</b>N of the illuminating unit <b>38</b> for controlling the energizing of the light sources <b>38</b>A, <b>38</b>B, to <b>38</b>N of the illuminating unit <b>38</b>. The illumination control unit <b>40</b> may be used for switching one or more of the light sources <b>38</b>A, <b>38</b>B, to <b>38</b>N on or off, and/or for controlling the intensity of the light produced by one or more of the light sources <b>38</b>A, <b>38</b>B, to <b>38</b>N, as is disclosed in detail hereinafter.</p>
<p id="p-0050" num="0049">The controller/processor unit <b>36</b> may be suitably connected to the illumination control unit <b>40</b> for (optionally) sending control signals thereto. Such control signals may be used for synchronizing or timing the energizing of the light sources <b>38</b>A, <b>38</b>B, <b>38</b>N within the illuminating unit <b>38</b>, relative to the imaging cycle or period of the imaging unit <b>32</b>. The illumination control unit <b>40</b> may be (optionally) integrated within the controller/processor unit <b>36</b>, or may be a separate controller. In some embodiments, illumination control unit <b>40</b> and/or controller/processor unit <b>36</b> may be part of telemetry unit <b>34</b>.</p>
<p id="p-0051" num="0050">The device <b>30</b> may further include a light sensing unit(s) <b>42</b> for sensing the light produced by the illuminating unit <b>38</b> and reflected from the walls of the GI tract. The light sensing unit(s) <b>42</b> may comprise a single light sensitive device or light sensor, or a plurality of discrete light sensitive device(s) or light sensor(s), such as but not limited to, a photodiode, a phototransistor, or the like. Other types of light sensors known in the art and having suitable characteristics may also be used for implementing the light sensing unit or units of embodiments of the present invention.</p>
<p id="p-0052" num="0051">The light sensing unit(s) <b>42</b> may be suitably connected to the illumination control unit <b>40</b> for providing the illumination control unit <b>40</b> with a signal representative of the intensity of the light reflected from the walls of the gastrointestinal tract (or any other object within the field of view of the imaging unit <b>32</b>). In operation, the illumination control unit <b>40</b> may process the signal received from the light sensing unit(s) <b>42</b> and, based on the processed signal, may control the operation of the light source(s) <b>38</b>A, <b>38</b>B, to <b>38</b>N as is disclosed in detail hereinabove and hereinafter.</p>
<p id="p-0053" num="0052">The device <b>30</b> may also include a power source <b>44</b> for providing power to the various components of the device <b>30</b>. It is noted that for the sake of clarity of illustration, the connections between the power source <b>44</b> and the circuits or components of the device <b>30</b> which receive power therefrom, are not shown in detail. The power source <b>44</b> may be, for example, an internal power source similar to the power source(s) <b>25</b> of the device <b>10</b>A, e.g., a battery or other power source. However, if the device <b>30</b> is configured as an insertable device (such as, for example, an endoscope-like device or a catheter-like device, or any other type of in vivo imaging device known in the art), the power source <b>44</b> may also be an external power source which may be placed outside the device <b>30</b> (such an external configuration is not shown in <figref idref="DRAWINGS">FIG. 2</figref> for the sake of clarity of illustration). In such an embodiment having an external power source (not shown), the external power source (not shown) may be connected to the various power requiring components of the imaging device through suitable electrical conductors (not shown), such as insulated wires or the like.</p>
<p id="p-0054" num="0053">It is noted that while for autonomous or swallowable in-vivo imaging device such as the device <b>10</b>A the power source(s) <b>25</b> are preferably (but not necessarily) compact power sources for providing direct current (DC), external power sources may be any suitable power sources known in the art, including but not limited to power sources providing alternating current (AC) or direct current or may be power supplies couples to the mains as is known in the art.</p>
<p id="p-0055" num="0054">The various functions and processes implemented by the swallowable in-vivo imaging device may be executed by, for example, a processor unit (e.g., unit <b>36</b> in <figref idref="DRAWINGS">FIG. 2</figref>). These functions and processes may be implemented by the processor unit <b>36</b> alone, and/or by alternative units, such as illumination control unit <b>40</b>, telemetry unit <b>34</b>, light sensing units <b>42</b>, imaging unit <b>32</b> etc., or any combination of units. The various units may optionally be integrated within the processor unit <b>36</b>, such that the processor unit can be said to implement any of the functions and processes herein described. The methods and processes described may be also embodied in other sensing devices having other structures and other components.</p>
<p id="p-0056" num="0055">Reference is now made to <figref idref="DRAWINGS">FIGS. 3 and 4</figref>. <figref idref="DRAWINGS">FIG. 3</figref> is a schematic cross-sectional view of part of an in-vivo imaging device having an automatic illumination control system and four light sources, in accordance with an embodiment of the present invention. <figref idref="DRAWINGS">FIG. 4</figref> is a schematic front view of the device illustrated in <figref idref="DRAWINGS">FIG. 3</figref>.</p>
<p id="p-0057" num="0056">The device <b>60</b> (only part of which is shown in <figref idref="DRAWINGS">FIG. 3</figref>) includes an imaging unit <b>64</b>. The imaging unit <b>64</b> may be similar to the imaging unit <b>32</b> of <figref idref="DRAWINGS">FIG. 2</figref> or to the imaging unit <b>24</b> of <figref idref="DRAWINGS">FIG. 1</figref>. Preferably, the imaging unit <b>64</b> may be a CMOS imaging unit, but other different types of imaging units may be also used. The imaging unit <b>64</b> may include CMOS imager circuitry, as is known in the art, but may also include other types of support and or control circuitry therein, as is known in the art and disclosed, for example, in U.S. Pat. No. 5,604,531 to Iddan et al., or in Co-pending PCT Patent Application, Publication No. WO 01/65995 to Glukhovsky et al. The device <b>60</b> also includes an optical unit <b>62</b> which may comprise a lens or a plurality of optical elements as disclosed hereinabove for optical unit <b>22</b> of <figref idref="DRAWINGS">FIG. 1</figref> and the optical unit <b>32</b>A of <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0058" num="0057">The device <b>60</b> may include an illuminating unit <b>63</b>, which may include four light sources <b>63</b>A, <b>63</b>B, <b>63</b>C and <b>63</b>D which may be disposed within the device <b>60</b> as shown in <figref idref="DRAWINGS">FIG. 4</figref>. The light sources <b>63</b>A, <b>63</b>B, <b>63</b>C and <b>63</b>D may be the white LED light sources or as disclosed, for example, in Co-pending US patent application, PCT patent application, Publication No. WO 01/65995 to Glukhovsky et al., but may also be any other suitable type of light sources, including but not limited to, infrared light sources, monochromatic light sources, band limited light sources known in the art or disclosed hereinabove.</p>
<p id="p-0059" num="0058">It is noted that while in accordance with one embodiment of the present invention the light sources <b>63</b>A, <b>63</b>B, <b>63</b>C and <b>63</b>D are shown to be identical, other embodiments of the invention may be implemented with multiple light sources which may not be identical. Some of the light sources may have a spectral distribution, which is different than the spectral distribution of the other light sources. For example, of the light sources within the same device, one of the light sources may be a red LED, another light source may be a blue LED and another light source may be a yellow LED. Other configurations of light sources are also possible.</p>
<p id="p-0060" num="0059">The device <b>60</b> may also include a baffle <b>70</b>, which may be conically shaped or which may have any other suitable shape. The baffle <b>70</b> may have an aperture <b>70</b>A therein. The baffle <b>70</b> may be interposed between the light sources <b>63</b>A, <b>63</b>B, <b>63</b>C and <b>63</b>D and the optical unit <b>62</b> and may reduce the amount of light coming directly from the light sources <b>63</b>A, <b>63</b>B, <b>63</b>C and <b>63</b>D to enter the aperture <b>70</b>A. The device <b>60</b> may include a transparent optical dome <b>61</b> similar to the optical dome <b>21</b> of <figref idref="DRAWINGS">FIG. 1</figref>. The optical dome <b>61</b> may be made from a suitable transparent plastic material or glass or from any other suitable material which is sufficiently transparent to at least some of the wavelengths of light produced by the light sources <b>63</b>A, <b>63</b>B, <b>63</b>C and <b>63</b>D to allow for adequate imaging.</p>
<p id="p-0061" num="0060">The device <b>60</b> may further include at least one light sensing unit <b>67</b> for sensing light, which is reflected from or diffused by the intestinal wall <b>76</b>. The light sensing unit may be attached to the baffle <b>70</b> such that its light sensitive part <b>67</b>A faces the optical dome <b>61</b>. Preferably, but not necessarily, the light sensing unit <b>67</b> may be positioned on the surface of baffle <b>70</b> at a position which allows the light sensing unit <b>67</b> to sense an amount of light which is representative or proportional to the amount of light entering the aperture <b>70</b>A of the baffle <b>70</b>. This may be true when the illuminated object is semi-diffusive (as the intestinal surface may be), and when the size of the light sensing unit <b>67</b> and its distance from the imaging sensor axis <b>75</b> are small compared to the diameter D of the capsule like device <b>60</b>.</p>
<p id="p-0062" num="0061">The device <b>60</b> (<figref idref="DRAWINGS">FIG. 3</figref>) is illustrated as being adjacent to the intestinal wall <b>76</b>. In operation, light rays <b>72</b> which are generated by the light sources <b>63</b>A, <b>63</b>B, <b>63</b>C and <b>63</b>D may penetrate the optical dome <b>61</b> and may be reflected from the intestinal wall <b>76</b>. Some of the reflected light rays <b>74</b> may reach the light-sensing unit <b>67</b>. Other reflected light rays (not shown) may reach the aperture <b>70</b>A and pass the optical unit <b>62</b> to be focused on the imaging unit <b>64</b>.</p>
<p id="p-0063" num="0062">The amount of light measured by the light-sensing unit <b>67</b> may be proportional to the amount of light entering the aperture <b>70</b>A. Thus, the measurement of the light intensity reaching the light sensing unit <b>67</b> may be used to determine the light output of the light sources <b>63</b>A, <b>63</b>B, <b>63</b>C and <b>63</b>D as is disclosed in detail hereinafter.</p>
<p id="p-0064" num="0063">The device <b>60</b> also includes an illumination control unit <b>40</b>A. The illumination control unit <b>40</b>A is suitably coupled to the light sensing unit <b>67</b> and to the illuminating unit <b>63</b>. The illumination control unit <b>40</b>A may process the signal received from the light sensing unit <b>67</b> to control the light sources <b>63</b>A, <b>63</b>B, <b>63</b>C and <b>63</b>D as is disclosed in detail hereinafter.</p>
<p id="p-0065" num="0064">The device <b>60</b> may also include a wireless transmitter unit (not shown in <figref idref="DRAWINGS">FIG. 3</figref>) and an antenna (not shown in <figref idref="DRAWINGS">FIG. 3</figref>), such as but not limited to the transmitter <b>26</b> and the antenna <b>27</b> of <figref idref="DRAWINGS">FIG. 1</figref> or may include any suitable telemetry unit (such as, but not limited to the telemetry unit <b>34</b> of <figref idref="DRAWINGS">FIG. 2</figref>). The telemetry unit may be a transmitter or a transceiver, for wirelessly transmitting (and optionally also receiving) data and control signals to (and optionally from) an external receiver/recorder (not shown in <figref idref="DRAWINGS">FIG. 3</figref>) as disclosed in detail hereinabove. The device <b>60</b> may also include one or more power sources such as, for example, the power sources <b>25</b> of <figref idref="DRAWINGS">FIG. 1</figref>, or any other suitable power sources, known in the art.</p>
<p id="p-0066" num="0065">Reference is now made to <figref idref="DRAWINGS">FIG. 5</figref> which is a schematic diagram illustrating a method of timing of the illumination and image acquisition in an in vivo imaging device having a fixed illumination duration. The timing method may be characteristic for imaging devices having CMOS imagers but may also be used in devices having other types of imagers.</p>
<p id="p-0067" num="0066">An image acquisition cycle or period starts at the time T. The first image acquisition cycle ends at time T<b>1</b> and has a duration &#x394;T<b>1</b>. The second image acquisition cycle starts at time T<b>1</b>, ends at time T<b>2</b> and has a duration &#x394;T<b>1</b>. Each imaging cycle or period may comprise two parts, an illumination period <b>90</b> having a duration &#x394;T<b>2</b>, and a dark period <b>92</b> having a duration &#x394;T<b>3</b>. The illumination periods <b>90</b> are represented by the hashed bars of <figref idref="DRAWINGS">FIG. 5</figref>. During the illumination period <b>90</b> of each imaging cycle, the illumination unit (such as but not limited to the illuminating unit <b>38</b> of <figref idref="DRAWINGS">FIG. 2</figref>, or the illuminating unit <b>63</b> of <figref idref="DRAWINGS">FIG. 3</figref>) is turned on and provides light for illuminating the intestinal wall. During the dark period <b>92</b> of each imaging cycle, the illuminating unit (such as but not limited to the illuminating unit <b>38</b> of <figref idref="DRAWINGS">FIG. 2</figref>, or the illuminating unit <b>63</b> of <figref idref="DRAWINGS">FIG. 3</figref>) is switched off and does not provide light.</p>
<p id="p-0068" num="0067">The dark period <b>92</b>, or a part thereof, may be used for, for example, to acquiring an image from the imager by, for example, scanning the pixels of the imager and for processing the imager output signals and for transmitting the output signals or the processed output signals to an external receiver or receiver/recorder device, as disclosed hereinabove.</p>
<p id="p-0069" num="0068">It is noted that while for the sake of simplicity, the diagram of <figref idref="DRAWINGS">FIG. 5</figref> illustrates a case in which the image acquisition cycle duration is fixed, and imaging is performed at a fixed frame rate, this is not mandatory. Thus, the frame rate and therefore the image acquisition cycle duration may vary during imaging in accordance with a measured parameter such as, for example the velocity of the imaging device within the gastrointestinal tract.</p>
<p id="p-0070" num="0069">Generally, different types of light control methods may be used for ensuring adequate image acquisition.</p>
<p id="p-0071" num="0070">In a first method, the amount of light impinging on the light sensing unit <b>67</b> may be continuously measured and recorded during the illumination of the target tissue by the illuminating unit <b>63</b> to provide a cumulative value representative of the total cumulative number of photons detected by the light sensing unit <b>67</b>. When this cumulative value reaches a certain value, the illuminating unit <b>63</b> may be shut off by switching off the light sources <b>63</b>A, <b>63</b>B, <b>63</b>C, and <b>63</b>D included in the illuminating unit <b>63</b>. In this way the device <b>60</b> may ensure that when the quantity of measured light is sufficient to result in an adequately exposed frame (on the average), the illuminating unit <b>63</b> is turned off.</p>
<p id="p-0072" num="0071">One advantage of the first method is that if the light sources (such as the light sources <b>63</b>A, <b>63</b>B, <b>63</b>C, and <b>63</b>D) are operated at their maximal or nearly maximal light output capacity, the switching off may save energy when compared to the energy expenditure in a fixed duration illumination period (such as the illumination period <b>90</b> of <figref idref="DRAWINGS">FIG. 5</figref>).</p>
<p id="p-0073" num="0072">Another advantage of the first method is that it enables the shortening of the duration of the illumination period within the imaging cycle in comparison with using a fixed illumination period. In a moving imaging device, such as the device <b>60</b>, ideally, it may be desirable to have the illumination period as short as practically possible, since this prevents or reduces image smearing due to the movement of the device <b>60</b> within the GI tract. Thus, typically, in a moving imaging device, the shorter the illumination period, the sharper will the resulting image be (assuming that enough light is generated by the illuminating unit to ensure adequate imager exposure).</p>
<p id="p-0074" num="0073">This may be somewhat similar to the increasing of the shutter speed in a regular shutter operated camera in order to decrease the duration of exposure to light to prevent smearing of the image of a moving object or image, except that in embodiments of the present method there is typically no shutter and the illumination period is being shortened controllably to reduce image smearing due to device movements in the GI tract.</p>
<p id="p-0075" num="0074">Reference is now made to <figref idref="DRAWINGS">FIGS. 6 and 7</figref>. <figref idref="DRAWINGS">FIG. 6</figref> is a schematic diagram illustrating one possible configuration for an illumination control unit coupled to a light sensing photodiode and to a light emitting diode, in accordance with an embodiment of the present invention. <figref idref="DRAWINGS">FIG. 7</figref> is a schematic diagram illustrating the illumination control unit of <figref idref="DRAWINGS">FIG. 6</figref> in detail, in accordance with an embodiment of the present invention.</p>
<p id="p-0076" num="0075">The illumination control unit <b>40</b>B of <figref idref="DRAWINGS">FIG. 6</figref> may be suitably connected to a photodiode <b>67</b>B, which may be operated as a light sensing unit. Any other suitable sensing unit or light sensor may be used. The illumination control unit <b>40</b>B may be suitably connected to a light emitting diode (LED) <b>63</b>E. The LED <b>63</b>E may be a white LED as disclosed hereinabove or may be any other type of LED suitable for illuminating the imaged target (such as the gastrointestinal wall). The illumination control unit <b>40</b>B may receive a current signal from the photodiode <b>67</b>B. The received signal may be proportional to the intensity of light (represented schematically by the arrows <b>81</b>) impinging the photodiode <b>67</b>B. The illumination control <b>40</b>B may process the received signal to determine the amount of light that illuminated the photodiode <b>67</b>B within the duration of a light measuring time period. The illumination control <b>40</b>B may control the energizing of the LED <b>63</b>E based on the amount of light that illuminated the photodiode <b>67</b>B within the duration of the light measuring time period. Examples of the type of processing and control of energizing are disclosed in detail hereinafter. The illumination control unit <b>40</b>B may also receive control signals from other circuitry components included in the in vivo imaging device. For example, the control signals may include timing and/or synchronization signals, on/off switching signals, reset signals, or the like.</p>
<p id="p-0077" num="0076">The light sensing unit(s) and light producing unit(s) may be any suitable light producing or sensing units other than diodes.</p>
<p id="p-0078" num="0077"><figref idref="DRAWINGS">FIG. 7</figref> illustrates one possible embodiment of the illumination control unit <b>40</b>B. The illumination control unit <b>40</b>B may include, for example, an integrator unit <b>80</b>, a comparator unit <b>82</b> and a LED driver unit <b>84</b>. The integrator unit <b>80</b> is coupled to the photodiode <b>67</b>B to receive therefrom a signal indicative of the intensity of the light impinging on the photodiode <b>67</b>B, and to record and integrate the amount of light impinging on the photodiode <b>67</b>B. The integrator unit <b>80</b> may be suitably connected to the comparator unit <b>82</b>.</p>
<p id="p-0079" num="0078">The integrator unit <b>80</b> may record and integrate the amount of light impinging on the photodiode <b>67</b>B, integrating the received signal, and output an integrated signal to the comparator unit <b>82</b>. The integrated signal may be proportional to or indicative of the cumulative number of photons hitting the photodiode <b>67</b>B over the integration time period. The comparator unit <b>80</b> may be suitably connected to the LED driver unit <b>84</b>. The comparator unit <b>80</b> may continuously compare the value of the integrated signal to a preset threshold value. When the value of the integrated signal is equal to the threshold value, the comparator unit <b>82</b> may control the LED driver unit <b>84</b> to switch off the power to the LED <b>63</b>E and thus cease the operation of the LED <b>63</b>E.</p>
<p id="p-0080" num="0079">Thus, the illumination control unit <b>40</b>A may be constructed and operated similar to the illumination control unit <b>40</b>B of <figref idref="DRAWINGS">FIGS. 7 and 8</figref>.</p>
<p id="p-0081" num="0080">It is noted that while the circuits illustrated in <figref idref="DRAWINGS">FIG. 7</figref> may be implemented as analog circuits, digital circuits and/or hybrid analog/digital circuits may be used in implementing the illumination control unit, as is disclosed in detail hereinafter (with respect to <figref idref="DRAWINGS">FIG. 11</figref>).</p>
<p id="p-0082" num="0081">Reference is now made to <figref idref="DRAWINGS">FIG. 8</figref>, which is a schematic diagram useful for understanding a method of timing of the illumination and image acquisition in an in vivo imaging device having a variable controlled illumination duration, according to one embodiment.</p>
<p id="p-0083" num="0082">An image acquisition cycle or period starts at the time T. The first image acquisition cycle ends at time T<b>1</b> and has a duration &#x394;T<b>1</b>. The second image acquisition cycle starts at time T<b>1</b>, ends at time T<b>2</b> and has a duration &#x394;T<b>1</b>. In each imaging cycle, the time period having a duration &#x394;T<b>4</b> defines the maximal allowable illumination period. The maximal allowable illumination period &#x394;T<b>4</b> may typically be a time period which is short enough as to enable imaging without excessive image smearing or blurring due to the movement of the device <b>60</b> within the GI tract. The time T<sub>M </sub>is the time of the end of the maximal allowable illumination period &#x394;T<b>4</b> relative to the beginning time of the first imaging cycle.</p>
<p id="p-0084" num="0083">The maximal allowable illumination period &#x394;T<b>4</b> may be factory preset taking into account, inter alia, the typical or average (or maximal) velocity reached by the imaging device within the GI tract, (as may be determined empirically in a plurality of devices used in different patients), the type of the imaging sensor (such as, for example, the CMOS sensor <b>64</b> of the device <b>50</b>) and its scanning time requirements, and other manufacturing and timing considerations. In accordance with one implementation of the invention, when imaging at 2 frames per second &#x394;T<b>1</b>=0.5 second, the duration of &#x394;T<b>4</b> may be set to have a value in the range of 20-30 milliseconds. However, this duration is given by way of example only, and &#x394;T<b>4</b> may have other different values. Typically, the use of a maximal allowable illumination period &#x394;T<b>4</b> of less than 30 milliseconds may result in acceptable image quality of most of the acquired image frames without excessive degradation due to blurring of the image resulting from movement of the imaging device within the GI tract.</p>
<p id="p-0085" num="0084">The time period &#x394;T<b>5</b> is defined as the difference between the entire imaging cycle duration &#x394;T<b>1</b> and the maximal allowable illumination period &#x394;T<b>4</b> (&#x394;T<b>5</b>=&#x394;T<b>1</b>&#x2212;&#x394;T<b>4</b>).</p>
<p id="p-0086" num="0085">At the time of beginning T of the first imaging cycle, the illumination unit (such as but not limited to the illuminating unit <b>63</b> of <figref idref="DRAWINGS">FIG. 3</figref>) is turned on and provides light for illuminating the intestinal wall. The light sensing unit <b>67</b> senses the light reflected and/or diffused from the intestinal wall <b>76</b> and provides a signal to the illumination control unit <b>40</b>A of the device <b>60</b>. The signal may be proportional to the average amount of light entering the aperture <b>70</b>A. The signal provided by the light sensing unit <b>67</b> may be integrated by the illumination control unit <b>40</b>A as is disclosed in detail hereinabove with respect to the illumination control unit <b>40</b>B of <figref idref="DRAWINGS">FIGS. 7 and 8</figref>.</p>
<p id="p-0087" num="0086">The integrated signal may be compared to a preset threshold value (for example by a comparator such as the comparator unit <b>82</b> of <figref idref="DRAWINGS">FIG. 7</figref>). When the integrated signal is equal to the threshold value, the illumination control unit <b>40</b>A ceases the operation of the light sources <b>63</b>A, <b>63</b>B, <b>63</b>C and <b>63</b>D of the illuminating unit <b>63</b>. The time TE<b>1</b> is the time at which the illuminating control unit turns off the light sources <b>63</b>A, <b>63</b>B, <b>63</b>C and <b>63</b>D within the first imaging cycle. The time interval beginning at time T and ending at time TE<b>1</b> is the illumination period <b>94</b> (represented by the hashed bar labeled <b>94</b>) for the first imaging cycle. The illumination period <b>94</b> has a duration of &#x394;T<b>6</b>. It may be seen that for the first imaging cycle &#x394;T<b>6</b>&#x3c;&#x394;T<b>4</b>.</p>
<p id="p-0088" num="0087">After the time TE<b>1</b> the scanning of the pixels CMOS sensor <b>64</b> may begin and the pixel data (and possibly other data) may be transmitted by the transmitter (not shown in <figref idref="DRAWINGS">FIG. 3</figref>) or telemetry unit of the device <b>60</b>.</p>
<p id="p-0089" num="0088">Preferably, the scanning (read out) of the pixels of the CMOS sensor <b>64</b> may begin as early as the time TE<b>1</b> of the termination of the illumination. For example the illumination control unit <b>40</b>A may send a control signal to the CMOS sensor at time TE<b>1</b> to initiate the scanning of the pixels of the CMOS sensor <b>64</b>. However, the scanning of the pixels may also begin at a preset time after the time T<sub>M </sub>which is the ending time of the maximal allowable illumination period &#x394;T<b>4</b>, provided that sufficient time is available for pixel scanning and data transmission operations. According to one embodiment, keeping the start of readout time fixed, for example at T<sub>M</sub>, may enable simpler implementation of the receiving unit.</p>
<p id="p-0090" num="0089">At the time of beginning T<b>1</b> of the second imaging cycle, the illuminating unit <b>63</b> is turned on again. The light sensing unit <b>67</b> senses the light reflected and/or diffused from the intestinal wall <b>76</b> and provides a signal to the illumination control unit <b>40</b>A of the device <b>60</b>. The signal may be proportional to the average amount of light entering the aperture <b>70</b>A.</p>
<p id="p-0091" num="0090">The signal provided by the light sensing unit <b>67</b> may be integrated and compared to the threshold value as disclosed hereinabove for the first imaging cycle. When the integrated signal is equal to the threshold value, the illumination control unit <b>40</b>A turns off the light sources <b>63</b>A, <b>63</b>B, <b>63</b>C and <b>63</b>D of the illuminating unit <b>63</b>. However, in the particular schematic example illustrated in <figref idref="DRAWINGS">FIG. 8</figref>, the intensity of light reaching the light sensing unit <b>67</b> in the second imaging cycle is lower than the intensity of light reaching the light sensing unit <b>67</b> in the first imaging cycle.</p>
<p id="p-0092" num="0091">This difference of the illumination intensity or intensity versus time profile between different imaging cycle may be due to, inter alia, movement of the device <b>60</b> away from the intestinal wall <b>76</b>, or a change of the position or orientation of the device <b>60</b> with respect to the intestinal wall <b>76</b>, or a change in the light absorption or light reflecting or light diffusion properties of the part of the intestinal wall <b>76</b> which is within the field of view of the device <b>60</b>.</p>
<p id="p-0093" num="0092">Therefore it takes longer for the integrated signal output of the integrator unit to reach the threshold value. Therefore, the illumination control unit <b>40</b>A turns the illuminating unit <b>63</b> off at a time TE<b>2</b> (it is noted that TE<b>2</b>&#x3e;TE<b>1</b>).</p>
<p id="p-0094" num="0093">The time interval beginning at time T<b>1</b> and ending at time TE<b>2</b> is the illumination period <b>96</b> for the second imaging cycle. The illumination period <b>96</b> (represented by the hashed bar labeled <b>96</b>) has a duration &#x394;T<b>7</b>. It may be seen that for the second imaging cycle &#x394;T<b>7</b>&#x3c;&#x394;T<b>4</b>.</p>
<p id="p-0095" num="0094">Thus, the duration of the illumination period within different imaging cycles may vary and may depend, inter alia, on the intensity of light reaching the light sensing unit <b>67</b>.</p>
<p id="p-0096" num="0095">After the time TE<b>2</b> the scanning of the pixels CMOS sensor <b>64</b> may begin and the pixel data (and possibly other data) may be transmitted as disclosed in detail hereinabove for the first imaging cycle of <figref idref="DRAWINGS">FIG. 8</figref>.</p>
<p id="p-0097" num="0096">It is noted that while for the sake of simplicity, the diagram of <figref idref="DRAWINGS">FIG. 8</figref> illustrates a case in which the image acquisition cycle duration &#x394;T<b>1</b> is fixed, and imaging is performed at a fixed frame rate, this is not mandatory. Thus, the frame rate and therefore the image acquisition cycle duration &#x394;T<b>1</b> may vary during imaging in accordance with a measured parameter such as, for example the velocity of the imaging device within the gastrointestinal tract. In such cases, the duration of the imaging cycle may be shortened or increased in response to the measured velocity of the device <b>60</b> in order to increase or decrease the frame rate, respectively.</p>
<p id="p-0098" num="0097">For example, co-pending U.S. patent application Ser. No. 09/571,326, filed May 15, 2000, co-assigned to the assignee of the present application, incorporated herein by reference in its entirety for all purposes, discloses, inter alia, a device and method for controlling the frame rate of an in-vivo imaging device.</p>
<p id="p-0099" num="0098">The automatic illumination control methods disclosed hereinabove may be adapted for use in device having variable frame rate. Such adaptation may take into account the varying duration of the imaging cycle and the implementation may depend, inter alia, on the amount of time required to complete the pixel scanning and the data transmission, the available amount of power available to the device <b>60</b>, and other considerations.</p>
<p id="p-0100" num="0099">A simple way of adapting the method may be to limit the maximal frame rate of the imaging device, such that even when the maximal frame rate is being used, there will be enough time left for pixel scanning and data transmission within the time period.</p>
<p id="p-0101" num="0100">Reference is now made to <figref idref="DRAWINGS">FIG. 9</figref>, which is a schematic diagram useful for understanding a method of timing of the illumination and image acquisition in an in vivo imaging device having a variable frame rate and a variable controlled illumination duration.</p>
<p id="p-0102" num="0101">The first imaging cycle of <figref idref="DRAWINGS">FIG. 9</figref> is similar to the first imaging cycle of <figref idref="DRAWINGS">FIG. 8</figref> except that the duration of the illumination period <b>98</b> of <figref idref="DRAWINGS">FIG. 9</figref> (represented by the hashed bar labeled <b>98</b>) is longer than the duration of the illumination period <b>94</b> of <figref idref="DRAWINGS">FIG. 8</figref>. The first imaging cycle of <figref idref="DRAWINGS">FIG. 9</figref> starts at time T, ends at time T<b>1</b>, and has a duration &#x394;T<b>1</b>. The time T<sub>M </sub>represents the end of the maximal allowable illumination period &#x394;T<b>4</b>. The second imaging cycle of <figref idref="DRAWINGS">FIG. 9</figref> begins at time T<b>1</b> and ends at time T<b>3</b>. The duration of the second imaging cycle &#x394;T<b>8</b> is shorter than the duration of the first imaging cycle &#x394;T<b>1</b> (&#x394;T<b>8</b>&#x3c;&#x394;T<b>1</b>). The duration of the second imaging cycle &#x394;T<b>8</b> corresponds to the highest frame rate usable in the imaging device. The illumination period <b>100</b> of the second imaging cycle (represented by the hashed bar labeled <b>100</b> of <figref idref="DRAWINGS">FIG. 9</figref>) is timed by the illumination control unit depending on the light intensity as disclosed in detail hereinabove. The time period <b>102</b> (represented by the dotted bar labeled <b>102</b>) represents the amount of time &#x394;T<b>9</b> required for scanning the pixels of the imager and transmitting the scanned frame data. T<sub>M </sub>represents the time of ending of the maximal allowable illumination period relative to the beginning time of each imaging cycle. Thus, if the frame rate is increased, even at the highest possible frame rate there is enough time to scan the pixels and transmit the data.</p>
<p id="p-0103" num="0102">It is noted that typically, in an exemplary in vivo imaging device having a fixed frame rate, the time requited for scanning the pixels of a CMOS sensor having approximately 66,000 pixels (such as but not limited to a CMOS sensor arranged in a 256&#xd7;256 pixel array), and for transmitting the digital (serial) data signals to an external receiver recorder may be approximately 0.4 seconds (assuming a scanning and data transmission time of approximately 6 microseconds per pixel). Thus, assuming a maximal illumination period of approximately 20-30 milliseconds, the frame rate may not be extended much higher than 2 frames per second. Alternate frame rates may be used for example, for implementing different readout rates.</p>
<p id="p-0104" num="0103">It may however be possible to substantially shorten the time required for scanning the pixels and for transmitting the data. For example, by increasing the clock rate of the CMOS pixel array, it may be possible to reduce the time required to scan an individual frame to 3 microseconds or even less. Additionally, it may be possible to increase the data transmission rate of the transmitter <b>26</b> to even further shorten the overall time required for scanning the array pixels for transmitting the pixel data to the external receiver/recorder.</p>
<p id="p-0105" num="0104">Therefore, variable frame rate in vivo imaging devices, as well as fixed frame rate devices, may be implemented which may be capable of frame rates of approximately 4-8 frames per second, and even higher.</p>
<p id="p-0106" num="0105">When the method disclosed hereinabove for turning off the illuminating unit when the integrated output of the light sensing unit reaches a threshold value adapted to ensure a good average image quality is implemented, the tendency of the designer may be to operate the illuminating unit (such as, for example the illuminating unit <b>63</b> of <figref idref="DRAWINGS">FIG. 3</figref>) close to the maximal available light output capacity. This may be advantageous because of the shortened illumination period duration achievable which may improve image clarity by reducing movement induced image blurring.</p>
<p id="p-0107" num="0106">It may not always be possible or desired to operate the illuminating unit close to the maximal possible light output capacity. Therefore, it may be desired to start the operation of the illuminating unit <b>63</b> at a given light output which is lower than the maximal light output of illuminating unit <b>63</b>.</p>
<p id="p-0108" num="0107">In a second illumination control method, the illuminating unit <b>63</b> of <figref idref="DRAWINGS">FIG. 3</figref> may be initially operated at a first light output level at the beginning of each of the imaging cycles. The light sensing unit <b>67</b> may be used to measure the amount of light during a short illumination sampling period.</p>
<p id="p-0109" num="0108">Reference is now made to <figref idref="DRAWINGS">FIGS. 10A</figref>, <b>10</b>B and <b>10</b>C. <figref idref="DRAWINGS">FIG. 10A</figref> is a timing diagram schematically illustrating an imaging cycle of an in vivo imaging device using an automatic illumination control method in accordance with another embodiment of the present invention. <figref idref="DRAWINGS">FIG. 10B</figref> is an exemplary schematic graph representing an example of the light intensity as a function of time, possible when using the method of automatic illumination control illustrated in <figref idref="DRAWINGS">FIG. 10A</figref>. <figref idref="DRAWINGS">FIG. 10C</figref> is a schematic graph representing another example of the light intensity as a function of time, possible when using the method of automatic illumination control illustrated in <figref idref="DRAWINGS">FIG. 10A</figref>.</p>
<p id="p-0110" num="0109">In <figref idref="DRAWINGS">FIGS. 10A</figref>, <b>10</b>B and <b>10</b>C, the horizontal axes of the graphs represents time in arbitrary units. In <figref idref="DRAWINGS">FIGS. 10B and 10C</figref>, the vertical axis represents the intensity I of the light output by the illuminating unit <b>63</b> (<figref idref="DRAWINGS">FIG. 3</figref>).</p>
<p id="p-0111" num="0110">The automatic illumination control method illustrated in <figref idref="DRAWINGS">FIG. 10A</figref> operates by using an illumination sampling period <b>104</b> included in a total illumination period <b>108</b>. An imaging cycle <b>110</b> includes the total illumination period <b>108</b> and a dark period <b>112</b>. The illuminating unit <b>63</b> may illuminate the intestinal wall <b>76</b> within the duration total illumination period <b>108</b>. The dark period <b>112</b> may be used for scanning the pixels of the CMOS imager <b>64</b> and for processing and transmitting the image data as disclosed in detail hereinabove.</p>
<p id="p-0112" num="0111">The total illumination period of the imaging cycle starts at time T and ends at time T<sub>M</sub>. The time T<sub>M </sub>is fixed with respect to the beginning time T of the imaging cycle <b>110</b>, and represents the maximal allowable illumination time. Practically, the time T<sub>M </sub>may be selected to reduce the possibility of image blurring as explained hereinabove. For example, the time T<sub>M </sub>may be selected as 20 milliseconds from the time of beginning T of the imaging cycle <b>110</b> (in other words, the duration of the total illumination period <b>108</b> may be set at 30 milliseconds), but other larger or smaller values of the time T<sub>M </sub>and of the total illumination period <b>108</b> may also be used.</p>
<p id="p-0113" num="0112">The total illumination period <b>108</b> may include an illumination sampling period <b>104</b> and a main illumination period <b>106</b>. The illumination sampling period <b>104</b> starts at time T and ends at time T<sub>S</sub>. The main illumination period <b>106</b> starts at time T<sub>S </sub>and ends at time T<sub>M</sub>.</p>
<p id="p-0114" num="0113">In an exemplary embodiment of the method, the duration of the illumination sampling period <b>104</b> may be set at approximately 2-5 milliseconds, but other larger or shorter duration values may be used depending, inter alia, on the type and characteristics of the light sensing unit <b>67</b>, its sensitivity to light, its signal to noise ratio (S/N), the intensity I<sub>1 </sub>at which the illuminating unit <b>63</b> is operated during the illumination sampling period <b>104</b>, and other implementation and manufacturing considerations.</p>
<p id="p-0115" num="0114">Turning to <figref idref="DRAWINGS">FIGS. 10B and 10C</figref>, during the illumination sampling period <b>104</b>, the illuminating unit <b>63</b> is operated such that the intensity of light is I<sub>1</sub>. The light sensing <b>67</b> may sense the light reflected from and diffused by the intestinal wall <b>76</b>. The illumination control unit <b>40</b>A may integrate the intensity signal to determine the quantity Q of light reaching the light sensing unit <b>67</b> within the duration of the illumination sampling period <b>104</b>. The illumination control unit <b>40</b>A may then compute from the value Q and from the known duration of the main illumination period <b>106</b>, the intensity of light I<sub>N </sub>at which the illuminating unit <b>63</b> needs to be operated for the duration of the main illumination period <b>106</b> in order to provide adequate average exposure of the CMOS sensor <b>64</b>. In one embodiment an estimated total amount of light received is kept substantially constant across a set of imaging cycles, or is kept within a certain target range. The computation may be performed, for example, by subtracting from a fixed light quantity which is desired to be received or applied the amount of light recorded during the sampling period <b>104</b> and dividing the result by a fixed time period which corresponds to the main illumination period <b>106</b>. One possible way to perform the computation would be using equation 1 as follows:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>I</i><sub>N</sub>=(<i>Q</i><sub>T</sub><i>&#x2212;Q</i>)/&#x394;<i>T</i><sub>MAIN</sub>&#x2003;&#x2003;equation 1<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0116" num="0115">Wherein,</p>
<p id="p-0117" num="0116">&#x394;T<sub>MAIN </sub>is the duration of the main illumination period <b>106</b>, Q<sub>T </sub>is the total quantity of light that needs to reach the light sensing unit <b>67</b> within an imaging cycle to ensure adequate average exposure of the CMOS sensor <b>64</b>, and Q is the quantity of light reaching the light sensing unit <b>67</b> within the duration of an illumination sampling period <b>104</b> of an imaging cycle.</p>
<p id="p-0118" num="0117">It is noted that the value of Q<sub>T </sub>may be empirically determined.</p>
<p id="p-0119" num="0118"><figref idref="DRAWINGS">FIG. 10B</figref> schematically illustrates a graph showing the intensity of light produced by the illuminating unit <b>63</b> as a function of time for an exemplary imaging cycle. During the illumination sampling period <b>104</b> the light intensity has a value I<sub>1</sub>. After the end of the illumination sampling period <b>104</b>, the light intensity I<sub>N</sub>=I<sub>2 </sub>may be computed as disclosed in equation 1 hereinabove, or by using any other suitable type of analog or digital computation.</p>
<p id="p-0120" num="0119">For example, if the computation is digitally performed by the controller/processor <b>36</b> of <figref idref="DRAWINGS">FIG. 2</figref>, the value of I<sub>N </sub>may be computed within a very short time (such as for example less than a microsecond) compared to the duration of the main illumination period <b>106</b>.</p>
<p id="p-0121" num="0120">If the computation of I<sub>N </sub>is performed by an analog circuit (not shown) which may be included in the illumination control unit <b>40</b> of <figref idref="DRAWINGS">FIG. 2</figref>, or in the illumination control unit <b>40</b>B of <figref idref="DRAWINGS">FIG. 6</figref>, or in the illumination control unit <b>40</b>A of FIG. <b>3</b>, the computation time may also be short compared to the duration of the main illumination period <b>106</b>.</p>
<p id="p-0122" num="0121">After the computation of I<sub>2 </sub>for the imaging cycle represented in <figref idref="DRAWINGS">FIG. 10B</figref> is completed, the illumination control unit <b>40</b>A may change the intensity of the light output of the illuminating unit of the imaging device to I<sub>2</sub>. This may be achieved, for example, by increasing the amount of current output from the LED driver unit <b>84</b> of <figref idref="DRAWINGS">FIG. 7</figref>, or by increasing the amount of current output from one or more LED driver units (not shown in detail) which may be included in the illumination control unit <b>40</b>A to supply current to the light sources <b>63</b>A, <b>63</b>B, <b>63</b>C, and <b>63</b>D. At the end of the main illumination period <b>108</b> (at time T<sub>M</sub>), the illumination control unit <b>40</b>A may switch the illuminating unit <b>63</b> off until time T<b>1</b> which is the beginning of a new imaging cycle (not shown). At the beginning of the new imaging cycle, the light intensity is switched again to the value I<sub>1 </sub>and a new illumination sampling period begins.</p>
<p id="p-0123" num="0122"><figref idref="DRAWINGS">FIG. 10C</figref> schematically illustrates a graph showing the intensity of light produced by the illuminating unit <b>63</b> as a function of time for another different exemplary imaging cycle. The illumination intensity I<sub>1 </sub>is used throughout the illumination sampling period <b>104</b> as disclosed hereinabove. In this imaging cycle, however, the value of Q measured for the illumination sampling period <b>104</b> is higher than the value of Q measured for the illumination sampling period of <figref idref="DRAWINGS">FIG. 10B</figref>. This may happen, for example, due to movement of the position of the imaging device <b>60</b> relative to the intestinal wall <b>76</b>. Therefore the computed value of I<sub>3 </sub>is lower than the value of I<sub>2 </sub>of the imaging cycle illustrated in <figref idref="DRAWINGS">FIG. 10B</figref>. The value of I<sub>3 </sub>is also lower than the value of I<sub>1</sub>. Thus, the intensity of light emitted by the illuminating unit <b>63</b> during the main illuminating period <b>106</b> illustrated in <figref idref="DRAWINGS">FIG. 10C</figref> is lower than the intensity of light emitted by the illuminating unit <b>63</b> during the illumination sampling period <b>104</b> of <figref idref="DRAWINGS">FIG. 10C</figref>.</p>
<p id="p-0124" num="0123">It is noted that if the computed value of I<sub>3 </sub>is equal to the value of I<sub>1 </sub>(case not shown in <figref idref="DRAWINGS">FIGS. 10B-10C</figref>) the illumination intensity may be maintained at the initial value of I<sub>1 </sub>for the duration of the total illumination period <b>108</b>, and no modification of the illumination intensity is performed at time T<sub>M</sub>.</p>
<p id="p-0125" num="0124">An advantage of the second illumination control method disclosed hereinabove may be that it may at least initially avoid the operating of the illuminating unit <b>63</b> at its maximal light output intensity. This may be useful for improving the performance of the power sources, such as, for example, the power source(s) <b>25</b> of <figref idref="DRAWINGS">FIG. 1</figref>, and may extend the useful operational life thereof. It is known in the art that many batteries and electrochemical cells do not perform optimally when they are operated near their maximal current output. When using the second illumination method, the light sources (such as the light sources <b>63</b>A, <b>63</b>B, <b>63</b>C, and <b>63</b>D of <figref idref="DRAWINGS">FIG. 3</figref>) are initially operated at a light intensity I<sub>1 </sub>which may be a fraction of their maximal output light intensity. Thus, in cases where it is determined that the maximal light output intensity is not required for the current frame acquisition, the light sources may be operated at a second light intensity level (such as, for example the light intensity level I<sub>3 </sub>which is lower than the light intensity level I<sub>1</sub>). Thus, the second illumination control method may reduce the current required for operating the illuminating unit <b>63</b> drawn from the batteries or other power sources of the imaging device which may extend the useful operational life of the batteries or of other power sources used in the imaging device. According to one embodiment a combination of both methods (variable duration and variable intensity) is possible.</p>
<p id="p-0126" num="0125">It will be appreciated by those skilled in the art that the embodiments of the present invention are not limited to the use of a single light sensing element and/or a single light source. Additionally, it will be appreciated that the light sensing elements may include photo detectors that are separate from an imager, or are part of an imager.</p>
<p id="p-0127" num="0126">Reference is now made to <figref idref="DRAWINGS">FIG. 11</figref> which is a schematic diagram illustrating an illumination control unit for controlling a plurality of light sources, in accordance with an embodiment of the present invention.</p>
<p id="p-0128" num="0127">The illumination control unit <b>120</b> includes a plurality of light sensing units <b>122</b>A, <b>122</b>B, . . . <b>122</b>N, suitably interfaced with a plurality of analog to digital (A/D) converting units <b>124</b>A, <b>124</b>B, . . . <b>124</b>N, respectively. The A/D converting units are suitably connected to a processing unit <b>126</b>. The processing unit <b>126</b> is suitably connected to a plurality of LED drivers <b>128</b>A, <b>128</b>B, . . . <b>128</b>N which are suitably connected to a plurality of LED light sources <b>130</b>A, <b>130</b>B, . . . <b>130</b>N.</p>
<p id="p-0129" num="0128">Signals representing the intensity of light sensed by the light sensing units <b>122</b>A, <b>122</b>B, . . . <b>122</b>N are fed to the A/D converting units <b>124</b>A, <b>124</b>B, . . . <b>124</b>N, respectively, which output digitized signals. The digitized signals may be received by the processing unit <b>126</b> which may process the signals. For example the processing unit <b>136</b> may perform integration of the signals to compute the quantity of light sensed by one or more of the light sensing units <b>122</b>A, <b>122</b>B, . . . <b>122</b>N. The computed quantity of light may be the total combined quantity of light sensed by all the light sensing units <b>122</b>A, <b>122</b>B, . . . <b>122</b>N taken together, or may be the individual quantities of light separately computed for each individual light sensing unit of the light sensing units <b>122</b>A, <b>122</b>B, . . . <b>122</b>N.</p>
<p id="p-0130" num="0129">The processing unit <b>136</b> may further process the computed light quantity or light quantities, to provide control signals to the LED drivers <b>128</b>A, <b>128</b>B, . . . <b>128</b>N which in turn may provide, individually or in combination, suitable currents to the LED light sources <b>130</b>A, <b>130</b>B, . . . <b>130</b>N. According to one embodiment of the present invention, each sensor may be directly related to one or more illumination sources.</p>
<p id="p-0131" num="0130">According to some embodiments of the present invention, individual control of illumination sources may be enabled by using special control pixels. These control pixels may be adapted for fast read-out, which is well known in the art. A fast read-out procedure may not reset the pixel values.</p>
<p id="p-0132" num="0131">It is noted that the illumination control unit <b>120</b> of <figref idref="DRAWINGS">FIG. 11</figref> may be operated using different processing and control methods.</p>
<p id="p-0133" num="0132">In accordance with one embodiment of the present invention, all the light sensing units <b>122</b>A, <b>122</b>B, . . . <b>122</b>N may be used as a single light sensing element and the computation is performed using the combined total quantity of light to simultaneously control the operation of all the LED light sources <b>130</b>A, <b>130</b>B, . . . <b>130</b>N together. In this embodiment, the illumination control unit <b>120</b> may be implemented using, for example, the first illumination control method as disclosed hereinabove and illustrated in <figref idref="DRAWINGS">FIGS. 5</figref>, <b>8</b>, and <b>9</b>, which uses a fixed illumination intensity and computes the termination time of the illumination. According to other embodiments multiple A/D units (e.g., <b>124</b>) are not included, rather analog processing is performed.</p>
<p id="p-0134" num="0133">Alternatively, in accordance with another embodiment of the present invention, the illumination control unit <b>120</b> may be implemented using the second illumination control method, for example, as disclosed hereinabove and illustrated in <figref idref="DRAWINGS">FIGS. 10A-10C</figref> which uses a first illumination intensity I<sub>1 </sub>in an illumination sampling period and computes a second light intensity I<sub>N </sub>for use in a main illumination period as disclosed in detail hereinabove. In such a case, the illumination intensity I<sub>1 </sub>used throughout the illumination sampling period <b>104</b> (see <figref idref="DRAWINGS">FIGS. 10A-10C</figref>) may be identical for all the LED light sources <b>130</b>A, <b>130</b>B, . . . <b>130</b>N, and the illumination intensity I<sub>N </sub>used throughout the main illumination period <b>106</b> (<figref idref="DRAWINGS">FIGS. 10A-10C</figref>) may be identical for all the LED light sources <b>130</b>A, <b>130</b>B, . . . <b>130</b>N.</p>
<p id="p-0135" num="0134">In accordance with another embodiment of the present invention, each of the light sensing units <b>122</b>A, <b>122</b>B, . . . <b>122</b>N may be used as a separate light sensing unit and the computation may be performed using the individual quantities of light sensed by each of the light sensing units <b>122</b>A, <b>122</b>B, . . . <b>122</b>N to differentially control the operation of at least one of the LED light sources <b>130</b>A, <b>130</b>B, . . . <b>130</b>N respectively or in any combination. In this embodiment, the illumination control unit <b>120</b> may be implemented using the first illumination control method as disclosed hereinabove and illustrated in <figref idref="DRAWINGS">FIGS. 5</figref>, <b>8</b>, and <b>9</b>, which uses a fixed illumination intensity for each of the LED light sources <b>130</b>A, <b>130</b>B, . . . <b>130</b>N and may separately compute the termination time of the illumination for each of the LED light sources <b>130</b>A, <b>130</b>B, . . . <b>130</b>N. In such a manner, sets of light sources <b>130</b>A, <b>130</b>B, . . . <b>130</b>N (where a set may include one) may be paired with sets of sensors <b>122</b>A, <b>122</b>B, . . . <b>122</b>N.</p>
<p id="p-0136" num="0135">Alternatively, in accordance with another embodiment of the present invention, the illumination control unit <b>120</b> may be implemented using the second illumination control method as disclosed hereinabove and illustrated in <figref idref="DRAWINGS">FIGS. 10A-10C</figref> which uses a first illumination intensity I<sub>1 </sub>in an illumination sampling period and computes a second light intensity I<sub>N </sub>for use in a main illumination period as disclosed in detail hereinabove. In such a case, the illumination intensity I<sub>1 </sub>may be identical for all the LED light sources <b>130</b>A, <b>130</b>B, . . . <b>130</b>N, and the illumination intensity I<sub>N </sub>may be identical for all the LED light sources <b>130</b>A, <b>130</b>B, . . . <b>130</b>N.</p>
<p id="p-0137" num="0136">Typically, this embodiment may be used in cases in which the positioning of the light sources <b>130</b>A, <b>130</b>B, . . . <b>130</b>N and the light sensing units <b>122</b>A, <b>122</b>B, . . . <b>122</b>N in the imaging device is configured to ensure that a reasonably efficient &#x201c;local control&#x201d; of illumination is enabled and that the cross-talk between different light sources is at a sufficiently low level to allow reasonable local control of the illumination intensity produced by a one or more of the light sources <b>130</b>A, <b>130</b>B, . . . <b>130</b>N by processing the signals from one or more light sensing unit which are associated in a control loop with the one or more light sources.</p>
<p id="p-0138" num="0137">Reference is now made to <figref idref="DRAWINGS">FIG. 12</figref> which is a schematic diagram illustrating a front view of an autonomous imaging device having four light sensing units and four light sources, in accordance with an embodiment of the present invention.</p>
<p id="p-0139" num="0138">The device <b>150</b> includes four light sources <b>163</b>A, <b>163</b>B, <b>163</b>C and <b>163</b>D and four light sensing units <b>167</b>A, <b>167</b>B, <b>167</b>C and <b>167</b>D. The light sources <b>163</b>A, <b>163</b>B, <b>163</b>C and <b>163</b>D may be the white LED sources as disclosed hereinabove, or may be other suitable light sources. The light sensing units <b>167</b>A, <b>167</b>B, <b>167</b>C and <b>167</b>D are attached on the surface of the baffle <b>70</b>, surrounding the aperture <b>62</b>. The front part of the device <b>150</b> may include four quadrants <b>170</b>A, <b>170</b>B, <b>170</b>C and <b>170</b>D. The device <b>150</b> may include an illumination control unit (not shown in the front view of <figref idref="DRAWINGS">FIG. 12</figref>), and all the optical components, imaging components, electrical circuitry, and power source(s) for image processing and transmitting as disclosed in detail hereinabove and illustrated in the drawing Figures (See <figref idref="DRAWINGS">FIGS. 1</figref>, <b>2</b>).</p>
<p id="p-0140" num="0139">The quadrants are schematically represented by the areas <b>170</b>A, <b>170</b>B, <b>170</b>C and <b>170</b>D between the dashed lines. In accordance with an embodiment of the invention, the device <b>150</b> may include four independent local control loops. For example, the light source <b>163</b>A and the light sensing unit <b>167</b>A which are positioned within the quadrant <b>170</b>A may be suitably coupled to the illumination control unit (not shown) in a way similar to the coupling of the light sources <b>38</b>A-<b>38</b>N and the light sensing unit(s) <b>42</b> to the illumination control unit <b>40</b> of <figref idref="DRAWINGS">FIG. 2</figref>. The signal from the light sensing unit <b>167</b>A may be used to control the illumination parameters of the light source <b>163</b>A using any of the illumination control methods disclosed hereinabove, forming a local control loop for the quadrant <b>170</b>A.</p>
<p id="p-0141" num="0140">Similarly, the signal from the light sensing unit <b>167</b>B may be used to control the illumination parameters of the light source <b>163</b>B using any of the illumination control methods disclosed hereinabove, forming a local control loop for the quadrant <b>170</b>B, the signal from the light sensing unit <b>167</b>C may be used to control the illumination parameters of the light source <b>163</b>C using any of the illumination control methods disclosed hereinabove, forming a local control loop for the quadrant <b>170</b>C, and the signal from the light sensing unit <b>167</b>D may be used to control the illumination parameters of the light source <b>163</b>D using any of the illumination control methods disclosed hereinabove, forming a local control loop for the quadrant <b>170</b>D.</p>
<p id="p-0142" num="0141">It is noted that there may be some cross-talk or interdependency between the different local control loops, since practically, some of the light produced by the light source <b>163</b>A may be reflected from or diffused by the intestinal wall and may reach the light sensing units <b>167</b>B, <b>167</b>C, and <b>167</b>D which form part of the other local control loops for the other quadrants <b>170</b>B, <b>170</b>C, and <b>170</b>D, respectively.</p>
<p id="p-0143" num="0142">The arrangement of the positions light sensing units <b>167</b>A, <b>167</b>B, <b>167</b>C and <b>167</b>D and the light sources <b>163</b>A, <b>163</b>B, <b>163</b>C and <b>163</b>D within the device <b>150</b> may be designed to reduce such cross-talk.</p>
<p id="p-0144" num="0143">In other embodiments of the invention it may be possible to use processing methods such as &#x201c;fuzzy logic&#x201d; methods or neural network implementations to link the operation of the different local control loops together. In such implementations, the different local control loops may be coupled together such that information from one of the light sensing unit may influence the control of illumination intensity of light sources in other local control loops.</p>
<p id="p-0145" num="0144">It is noted that, while the imaging device <b>150</b> illustrated in <figref idref="DRAWINGS">FIG. 12</figref> includes four light sources and four light sensing units. The number of light sources may vary and the imaging device of embodiments of the present invention may be constructed with a different number (higher or lower than four) of light sources. Similarly, the number of the light sensing units may also vary, and any suitable or practical number of light sensing units may be used. Additionally, it is noted that the number of light sensing units in a device need not be identical to the number of light sources included in the device. Thus, for example, a device may be constructed having three light sensing units and six light sources. Or in another example, a device may be constructed having ten light sensing units and nine light sources.</p>
<p id="p-0146" num="0145">The factors determining the number of light sources and the number of light sensing units may include, inter alia, the geometrical (two dimensional and three dimensional) arrangement of the light sources and the light sensing units within the device and their arrangement relative to each other, the size and available power of the light sources, the size and sensitivity of the light sensing units, manufacturing and wiring considerations.</p>
<p id="p-0147" num="0146">The number of local control loops may also be determined, inter alia, by the degree of uniformity of illumination desired, the degree of cross-talk between the different local control loops, the processing power of the illumination control unit available, and other manufacturing considerations.</p>
<p id="p-0148" num="0147">The inventors of the present invention have noticed that it is also possible to achieve illumination control using one or more of the light sensitive pixels of the imager itself, instead of or in addition to using dedicated light sensing unit(s) which are not part of the imager. In addition, special light sensing elements may be integrated into the pixel array on the surface of the CMOS imager IC.</p>
<p id="p-0149" num="0148">For example, in CMOS type imagers, some of the pixels of the CMOS imager may be used for controlling the illumination, or alternatively, specially manufactured light sensitive elements (such as, analog photodiodes, or the like) may be formed within the pixel array of the imager.</p>
<p id="p-0150" num="0149">Reference is now made to <figref idref="DRAWINGS">FIG. 13</figref> which is a top view schematically illustrating the arrangement of pixels on the surface of a CMOS imager usable for illumination control, in accordance with an embodiment of the present invention. It is noted that the pixel arrangement in <figref idref="DRAWINGS">FIG. 13</figref> is only schematically illustrated and the actual physical arrangement of the circuitry on the imager is not shown.</p>
<p id="p-0151" num="0150">The surface of the CMOS imager <b>160</b> is schematically represented by a 12&#xd7;12 array comprising 144 square pixels. The regular pixels <b>160</b>P are schematically represented by the white squares. The CMOS imager also includes sixteen control pixels <b>160</b>C, which are schematically represented by the hashed squares.</p>
<p id="p-0152" num="0151">It is noted that while the number of the pixels in the CMOS imager <b>160</b> was arbitrarily chosen as <b>144</b> for the sake of simplicity and clarity of illustration only, the number of pixels may be larger or smaller if desired. Typically, a larger numbers of pixels may be used to provide adequate image resolution. For example a 256&#xd7;256 pixel array may be suitable for GI tract imaging.</p>
<p id="p-0153" num="0152">In accordance with an embodiment of the present invention, the control pixels <b>160</b>C may be regular CMOS imager pixels which are assigned to be operated as control pixels. In accordance with this embodiment, the control pixels <b>160</b>C may be scanned at a different time than the regular imaging pixels <b>160</b>P. This embodiment has the advantage that it may be implemented with a regular CMOS pixel array imager.</p>
<p id="p-0154" num="0153">Turning back to <figref idref="DRAWINGS">FIG. 10A</figref>, the timing diagram of <figref idref="DRAWINGS">FIG. 10A</figref> may also be used, according to one embodiment, to illustrate the automatic illumination control method using control pixels. The method may operate by using a fast scan of the control pixels <b>160</b>C at the beginning of each imaging cycle <b>110</b>. The illuminating unit (not shown) may be turned on at the beginning of the imaging cycle <b>110</b> (at time T). The scanning of the control pixels <b>160</b>C may be performed similar to the scanning of the regular pixels <b>160</b>P, except that the scanning of all of the control pixels <b>160</b>C occurs within the illumination sampling period <b>104</b>. The control pixels <b>160</b>C may be serially scanned within the duration of the illumination sampling period <b>104</b>. This is possible due to the ability to randomly scan any desired pixel in a CMOS pixel array, by suitably addressing the pixel readout lines (not shown) as is known in the art.</p>
<p id="p-0155" num="0154">It is noted that since the control pixels <b>160</b>C are scanned serially (one after the other), the control pixel which is scanned first has been exposed to light for a shorter time period than the control pixels which are scanned next. Thus, each control pixel is scanned after it has been exposed to light for a different exposure time period.</p>
<p id="p-0156" num="0155">If one assumes that the intensity of light reflected from the intestinal wall does not change significantly within the duration of the illumination sampling period <b>104</b>, it may be possible to compensate for this incrementally increasing pixel exposure time by computationally correcting the average measured light intensity for all the control pixels <b>160</b>C, or the computed average quantity of light reaching all the control pixels <b>160</b>C. For example, a weighted average of the pixel intensities may be computed.</p>
<p id="p-0157" num="0156">Alternatively, in accordance with another embodiment of the present invention, the illuminating unit <b>63</b> may be turned off after the end on the illumination sampling period <b>104</b> (the turning off is not shown in <figref idref="DRAWINGS">FIG. 10A</figref>). This turning off may enable the scanning of the control pixels <b>160</b>C while the pixels <b>160</b>C are not exposed to light and may thus prevent the above described incremental light exposure of the control pixels.</p>
<p id="p-0158" num="0157">After the scanning (readout) of all the control pixels <b>160</b>C is completed and the scanned control pixel signal values are processed (by analog or by digital computation or processing), the value of the required illumination intensity in the main illumination period may be computed, for example, by the illumination control unit <b>40</b>A (or, for example, by the illumination control unit <b>40</b> of <figref idref="DRAWINGS">FIG. 2</figref>).</p>
<p id="p-0159" num="0158">The computation of the required illumination intensity or of the current required from the LED driver unit <b>84</b> may be performed as disclosed hereinabove, using the known value of I<sub>1 </sub>(see <figref idref="DRAWINGS">FIG. 10B</figref>) and may or may not take into account the duration of the period in which the illuminating unit <b>63</b> was turned off (this duration may be approximately known from the known time required to scan the control pixels <b>160</b>C and from the approximate time required for the data processing and/or computations). The illumination unit <b>63</b> may then be turned on (the turning on is not shown in <figref idref="DRAWINGS">FIG. 10A</figref> for the sake of clarity of illustration) using the computed current value to generate the required illumination intensity value I<sub>2 </sub>(see <figref idref="DRAWINGS">FIG. 10B</figref>) till the end of the main illumination period <b>106</b> at time T<sub>M</sub>.</p>
<p id="p-0160" num="0159">It is noted that if the number of control pixels <b>160</b>C is small the time required for scanning the control pixels <b>160</b>C may be short in comparison to the total duration of the total illumination period <b>108</b>. For example, if the scan time for scanning a single control pixel is approximately 6 microseconds, the scanning of 16 control pixels may require about 96 microseconds. Since the time required for computing the required light intensity may also be small (a few microseconds or tens of microseconds may be required), the period of time during which the illumination unit <b>63</b> is turned of at the end of the illumination sampling period <b>104</b> may comprise a small fraction of the main illumination period <b>108</b> which may typically be 20-30 milliseconds.</p>
<p id="p-0161" num="0160">It may also be possible to compute a weighted average in which the intensity read for each pixel may be differently weighted according to the position of the particular control pixel within the entire pixel array <b>160</b>. Such weighting methods may be used for obtaining center biased intensity weighting, as is known in the art, or any other type of biased measurement known in the art, including but not limited to edge (or periphery) biased weighting, or any other suitable type of weighting known in the art. Such compensating or weighting computations may be performed by an illumination control unit (not shown) included in the imaging device, or by any suitable processing unit (not shown), or controller unit (not shown) included in the imaging device in which the CMOS imager <b>160</b> illustrated in <figref idref="DRAWINGS">FIG. 13</figref> is included.</p>
<p id="p-0162" num="0161">Thus, if an averaging or weighting computation is used, after the readout of the control pixels and any type of compensation or weighting computation is finished, the illumination control unit (not shown) may compute the value of the weighted (and/or compensated) quantity of light sensed by the control pixels <b>160</b>C and use this value for computing the value of I<sub>2</sub>.</p>
<p id="p-0163" num="0162">It is noted that the ratio of the number of the control pixels <b>160</b>C to the regular pixels <b>160</b>P should be a small number. The ratio of 16/144 which is illustrated is given by example only (for the sake of clarity of illustration). In other implementations the ratio may be different depending, inter alia, on the total number of pixels in the CMOS array of the imager and on the number of control pixels used. For example in a typical 256&#xd7;256 CMOS pixel array it may be practical to use 16-128 pixels as illumination control pixels for illumination control purposes. The number of control pixels in the 256&#xd7;256 CMOS pixel array may however also be smaller than 16 control pixels or larger than 128 control pixels.</p>
<p id="p-0164" num="0163">Generally, the number of control pixels and the ratio of control pixels to regular pixels may depend, inter alia, on the total number of pixels available on the imager pixel array, on the pixel scanning speed of the particular imager, on the number of control pixels which may be practically scanned in the time allocated for scanning, and on the duration of the illumination sampling period.</p>
<p id="p-0165" num="0164">An advantage of the embodiments using automatic illumination control methods in which some of the pixels of the CMOS imager pixel array (such as for example the example illustrated in <figref idref="DRAWINGS">FIG. 13</figref>) is that in contrast to light sensitive sensors which may be disposed externally to the surface of the imager (such as for example, the light sensing unit <b>67</b> of <figref idref="DRAWINGS">FIG. 3</figref>), the control pixels <b>160</b>C actually sense the amount of light reaching the imager's surface since they are also imaging pixels disposed on the surface of the imager. This may be advantageous due to, inter alia, higher accuracy of light sensing, and may also eliminate the need for accurately disposing or the light sensing unit at an optimal place in the optical system, additionally, the control pixels may have signal to noise characteristics and temperature dependence properties similar to the other (non-control) pixels of the imager.</p>
<p id="p-0166" num="0165">Another advantage of using control pixels is that no external light sensing units are needed which may reduce the cost and simplify the assembly of the imaging device.</p>
<p id="p-0167" num="0166">It is noted that, according to one embodiment, in a CMOS imager such as imager <b>160</b>, the scanning of the control pixels <b>160</b>C after the illumination sampling period <b>104</b> does not reset the pixels. Thus, the control pixels <b>160</b>C continue to sense and integrate the light during the main illumination period <b>106</b>, and are scanned after the time T<sub>M </sub>together with all the other regular pixels <b>160</b>P of the imager <b>160</b>. Thus, the acquired image includes the full pixel information since the control pixels <b>160</b>C and the regular pixels <b>160</b>P have been exposed to light for the same duration. The image quality or resolution is thus not significantly affected by the use of the control pixels <b>160</b>C for controlling the illumination.</p>
<p id="p-0168" num="0167">It is also noted that while the arrangement of the control pixels <b>160</b>C on the imager <b>160</b> is symmetrical with respect to the center of the imager, any other suitable arrangement of the pixels may be used. The number and the distribution of the control pixels on the imager <b>160</b> may be changed or adapted in accordance with the type of averaging used and/or for example, with the type of acquired images.</p>
<p id="p-0169" num="0168">Furthermore, the control pixels may be grouped into groups that may be processed separately to allow local illumination control in imagers using a plurality of separately controllable light sources.</p>
<p id="p-0170" num="0169">Reference is now made to <figref idref="DRAWINGS">FIG. 14</figref>, which is a schematic top view of the pixels of a CMOS imager illustrating an exemplary distribution of control pixel groups suitable for being used in local illumination control in an imaging device, in accordance with an embodiment of the present invention.</p>
<p id="p-0171" num="0170">The illustrated imager <b>170</b> is a 20&#xd7;20 pixel array having 400 pixels. The control pixels are schematically represented by the hashed squares <b>170</b>A, <b>170</b>B, <b>170</b>C and <b>170</b>C and the remaining imager pixels are schematically represented by the non-hashed squares <b>170</b>P. Four groups of control pixels are illustrated on the imager <b>170</b>.</p>
<p id="p-0172" num="0171">The first pixel group includes four control pixels <b>170</b>A arranged within the top left quadrant of the surface of the imager <b>170</b>. The second pixel group includes four control pixels <b>170</b>B arranged within the top right quadrant of the surface of the imager <b>170</b>. The third pixel group includes four control pixels <b>170</b>C arranged within the bottom right quadrant of the surface of the imager <b>170</b>. The fourth pixel group includes four control pixels <b>170</b>D arranged within the top left bottom quadrant of the surface of the imager <b>170</b>.</p>
<p id="p-0173" num="0172">If the imager <b>170</b> is disposed in an autonomous imaging device having a plurality of light sources (such as, but not limited to the device <b>150</b> of <figref idref="DRAWINGS">FIG. 12</figref>), each of the four groups of control pixels <b>170</b>A, <b>170</b>B, <b>170</b>C and <b>170</b>D may be scanned and processed as disclosed hereinabove to provide data for locally controlling the illumination level reaching each of the respective four quadrants of the imager <b>170</b>. The scanned data for each of the pixels within each of the four groups may be processed to compute a desired value of illumination intensity for the respective imager quadrant. The methods for controlling the illumination using separate local control loops may be similar to any of the methods disclosed hereinabove with respect to the device <b>150</b> of <figref idref="DRAWINGS">FIG. 12</figref>, except that in the device <b>150</b> the light sensing units are units external to the imager and in the device <b>170</b>, the control pixels used for sensing are imager pixels which are integral parts of the imager <b>170</b>.</p>
<p id="p-0174" num="0173">The illumination control methods using control pixels may implemented using the closed-loop method of terminating the illumination when the integrated sensor signal reaches a threshold level as disclosed hereinabove or may be implemented by using an initial illumination intensity in a sampling illumination period and adapting or modifying the illumination intensity (if necessary) in accordance with a value computed or determined from the control pixel scanning as disclosed hereinabove.</p>
<p id="p-0175" num="0174">The signals or data of (representing the pixel charge) the pixel groups may be processed using averaging or weighted averaging methods to perform center biased or periphery biased averages or according to any other averaging or processing method known in the art. The results of the processing may be used as disclosed hereinabove to control the light sources (such as for example four light sources disposed within the imaging device in an arrangement similar to the arrangement of the four light sources <b>163</b>A, <b>163</b>B, <b>163</b>C, and <b>163</b>D of <figref idref="DRAWINGS">FIG. 12</figref>).</p>
<p id="p-0176" num="0175">It will be appreciated by those skilled in the art that the number of control pixels and the distribution of the control pixels on the surface of the imager may be varied, inter alia, in accordance with the desired type of averaging, the required number of local illumination control groups, the number and position of the light sources available in the imaging device, the computational power available to the processing unit available, the speed of the illumination control unit, and other design considerations.</p>
<p id="p-0177" num="0176">In accordance with another embodiment of the present invention, the control pixels <b>160</b>C of <figref idref="DRAWINGS">FIG. 13</figref> may be specially fabricated pixels, which are constructed differently than the regular pixels <b>160</b>P. In accordance with this embodiment, the control pixels <b>160</b>C may be fabricated as analog photodiodes with appropriate readout or sampling circuitry (not shown) as is known in the art. This implementation may use a specially fabricated custom CMOS imager in which the analog photodiodes serving as the control pixels <b>160</b>C may be read simultaneously which may be advantageous since the readout or scanning time may be shorter than the time required to sequentially scan the same number of control pixels implemented in a regular CMOS pixel array having uniform pixel construction.</p>
<p id="p-0178" num="0177">It is noted that when analog photodiodes or other known types of dedicated sensors are integrated into the CMOS pixel array of the imaging device, the acquired image will have &#x201c;missing&#x201d; image pixels, since the area in which the analog photodiode is disposed is not scanned together with the regular CMOS array pixels. The image data will therefore have &#x201c;missing pixels&#x201d;. If, however, a small number of analog photodiodes or other dedicated control pixels is included in the CMOS pixel array, the missing pixels may not cause a significant degradation of image quality. Additionally, such dedicated analog photodiodes or other control pixels may be distributed within the pixel array and may be sufficiently spaced apart from each other, so that image quality may be only slightly affected by the missing image pixels.</p>
<p id="p-0179" num="0178">It is noted that while the illumination control methods are disclosed for use in an autonomous imaging device such as the device <b>10</b>A of <figref idref="DRAWINGS">FIG. 1</figref>, these illumination control methods may also be used with or without adaptations in other in-vivo imaging devices having an imager and an illumination unit, such as in endoscopes or catheter-like devices having imaging sensor arrays, or in devices for performing in-vivo imaging which are insertable through a working channel of an endoscope, or the like.</p>
<p id="p-0180" num="0179">Additionally, the illumination control methods disclosed herein may be used in still cameras and in video cameras which include a suitable imager, such as a CMOS imager, and which include or are operatively connected to an illumination source.</p>
<p id="p-0181" num="0180">Additionally, the use of control pixels implemented in a CMOS pixel array imagers, using selected regular pixels as control pixels or using specially fabricated control pixels such as the analog photodiodes or the like, may be applied for controlling the illumination of a flash unit or another illumination unit which may be integrated within the camera or may be external to the camera and operatively connected thereto.</p>
<p id="p-0182" num="0181">The advantages of using control pixels which are part of the CMOS imager of the camera may include, inter alia, simplicity of construction and operation, the ability to implement and use a plurality of controllably interchangeable averaging methods including weighted averaging methods and biasing methods, as disclosed in detail hereinabove, increased accuracy of illumination control.</p>
<p id="p-0183" num="0182">Additionally, in specialty cameras operating under conditions in which the light source included in the camera or operatively connected thereto is the only source of available illumination (such as, for example, in camera's operated at the bottom of the ocean, or in cameras which are designed to perform surveillance or monitoring in difficult to access areas which are normally dark), the use of illumination control methods disclosed hereinabove may allow to use shutterless cameras, which may advantageously increase the reliability of such devices, reduce their cost, and simplify their construction and operation.</p>
<p id="p-0184" num="0183">It is noted that, while in the embodiments of the invention disclosed hereinabove the number and the arrangement of the control pixels are fixed, in accordance with another different embodiment of the present invention, the number and/or the geometrical configuration (arrangement) of the control pixels may be dynamically changed or controlled. For example, briefly turning to <figref idref="DRAWINGS">FIG. 2</figref>, the light sensing unit(s) <b>42</b> may represent one or more control pixels of a CMOS pixel array, and the illumination control unit <b>40</b>, and/or the controller/processor unit <b>36</b> may be configured for changing the number of the control pixels used in an imaging acquisition cycle and/or for changing the arrangement of the control pixels on the pixel array of the imaging unit <b>32</b>.</p>
<p id="p-0185" num="0184">Such changing of control pixel number and/or arrangement may be performed, in a non-limiting example, by changing number and/or arrangement of the pixels selected to be scanned as control pixels during the illumination sampling period <b>104</b> (<figref idref="DRAWINGS">FIG. 10A</figref>). Such a changing may allow the use of different averaging arrangements and methods and may allow changing of different biasing methods for different imaging cycles.</p>
<p id="p-0186" num="0185">Additionally, using dynamically controllable control pixel configuration, it may be possible to implement two or more illumination sampling periods within a single imaging cycle and to use a different pixel number or configuration for each of these two or more illumination sampling periods.</p>
<p id="p-0187" num="0186">It may also be possible to remotely control the number and/or configuration of the control pixels, by instructions which are wirelessly transmitted to the telemetry unit, for example, telemetry unit <b>34</b> (<figref idref="DRAWINGS">FIG. 2</figref>), in which case the telemetry unit may be configured as a transceiver unit capable of transmitting data and of receiving control data transmitted to it by an external transmitter unit (not shown in <figref idref="DRAWINGS">FIG. 2</figref>).</p>
<p id="p-0188" num="0187">It is noted that, while the embodiments disclosed hereinabove were based on modifying the light output from the illumination unit(s) (such as, for example the illumination unit <b>63</b> of <figref idref="DRAWINGS">FIG. 3</figref>) based on measurement and processing of the amount of light reaching the light sensing elements (such as, for example the light sensing unit <b>67</b> of <figref idref="DRAWINGS">FIG. 3</figref>, or the light sensing units <b>42</b> of <figref idref="DRAWINGS">FIG. 2</figref>, or the control pixels <b>160</b>C of <figref idref="DRAWINGS">FIG. 13</figref>), another approach may be used.</p>
<p id="p-0189" num="0188">According to some embodiments of the present invention, the gain of the pixel amplifiers of the imager (for example, imaging unit <b>32</b> of <figref idref="DRAWINGS">FIG. 2</figref>) and/or other parameters may be changed. The parameter change decisions may be based on, for example, the results of the measurement of the amount of light reaching the light sensing unit or units (such as, for example, the light sensing unit <b>42</b> of <figref idref="DRAWINGS">FIG. 2</figref>, the control pixels <b>160</b>C of <figref idref="DRAWINGS">FIG. 13</figref>, or the like). In such an embodiment, the illumination unit of the imaging device(s) (such as, for example, the illumination unit <b>63</b>A of <figref idref="DRAWINGS">FIG. 3</figref>, or the illumination unit <b>38</b> of <figref idref="DRAWINGS">FIG. 2</figref> etc.) may be operated at a fixed or variable illumination intensity, for a fixed or variable time period. The light that reaches the light sensing unit(s) or the control pixels of the imaging device during a sampling period (e.g., a portion of the exposure period within a frame) may be measured at a sampling instance or point. A sampling instance or point may be a discrete point in time or may extend over a certain amount of time. Parameters such as the gain level or sensitivity of the imager pixel amplifiers, the light intensity, duration of illumination or other parameters may be changed individually or in any combination in relation to, for example, measurements of relative light saturation of selected pixels, to achieve proper or appropriate imaging.</p>
<p id="p-0190" num="0189">For example, if the amount of light reaching the light sensing unit(s) during an illumination sampling period, as measured at least one selected sampling instance (period), is approximately sufficient to ensure proper image exposure (relative to, for example, an expected threshold for a determined number of pixels), the exposure may be stopped. In this case full exposure has already been achieved, and no further exposure is necessary. In this case, no change is the current image gain level should be required when transmitting the image. In addition, since the exposure was relatively short, there should be no problem with blurring, which may accompany images recorded with long light exposures.</p>
<p id="p-0191" num="0190">In the case where the measurement at a sampling instance determines that too little saturation has been attained (relative to a threshold value) during an illumination sampling period, the exposure should be continued to enable sufficient lighting for an image. However, too much exposure may cause blurring, so the imager may be commanded to lower the saturation threshold so as to have a shorter exposure. In addition to lowering the saturation threshold, in order to compensate for the short exposure, the imager may be commanded to provide a higher gain level for the image transmission, to enable enough exposure in spite of the short exposure time. For example, if the saturation threshold was halved to shorten the exposure sufficiently, the gain level will correspondingly need to be doubled to enable adequate exposure.</p>
<p id="p-0192" num="0191">If too much light, possibly relative to an expected threshold, reaches the light sensing unit(s) during an illumination sampling period, the exposure may be stopped, and the pixel amplifier gain (or other parameters) may be decreased to prevent overexposure. Additional sampling periods may be instituted at chosen instances, to enable further fine-tuning of variables such as image gain and exposure time.</p>
<p id="p-0193" num="0192">In addition to changes in the analog gain, which may be based on continuously scanning the analog output of a selected number of pixels during the early phase of the exposure period, the exposure may be stopped at any stage where full (e.g., adequate) saturation has been reached. In this way, for example, full image exposure may be provided in many cases of low exposure, by, for example, adding gain levels to images. In addition, over exposure can be prevented in many cases where there is high exposure, by, for example stopping exposure when saturation is attained. These changes may result in increased image quality, energy saving and/or other benefits.</p>
<p id="p-0194" num="0193">Various embodiments may utilize various time, saturation, and voltage levels, and are not limited to the following defined levels. According to a particular application of the present embodiment, required time resolution, which defines the maximum read out time required to achieve saturation for all pre-selected pixels may be, for example, 0.25 s. Other values or ranges may be used.</p>
<p id="p-0195" num="0194">According to some embodiments of the present invention, a total exposure time (e.g., an expected time required for adequate and/or correct exposure) may be defined (T<b>1</b>), within which an exposure measurement time (sampling time, such as T<b>1</b>/<sub>4 </sub>or other times which are a portion of T<b>1</b>) may be defined. The discrete time instances, where changes of reference levels may occur and gain decisions may be taken, may be determined by T<b>1</b>. This value may be used indirectly to set time intervals such as T<b>1</b>/<sub>2 </sub>and T<b>1</b>/<sub>4</sub>, or other intervals, which may be sample time intervals for measuring pixel saturation. A maximum exposure time may also be defined (T<sub>M</sub>). Typically, T<b>1</b> and T<sub>M </sub>are both programmable. Typically, T<sub>M </sub>does not impact on the calculations, other than to set a maximal exposure time at which point exposure may be stopped, regardless of whether exposure saturation thresholds have been crossed. T<b>1</b>, on the other hand, may be used as the target exposure time. For example, T<b>1</b> may refer to the expected exposure for adequate or complete saturation etc. At intervals T<b>1</b>/<sub>4 </sub>and T<b>1</b>/<sub>2</sub>, for example, the current system or method according to some embodiments sets the saturation threshold levels expected to be crossed before T<b>1</b>.</p>
<p id="p-0196" num="0195">Typically, the device <b>30</b> transmits image information in discrete portions. Each portion typically corresponds to an image or frame. Other transmission methods are possible. For example, the device <b>30</b> may capture an image once every half second, and, after capturing such an image, transmit the image to the receiving antenna. Other capture rates are possible. Typically, the image data recorded and transmitted is digital color image data, although in alternate embodiments other image formats (e.g., black and white image data) may be used. In one embodiment, each frame of image data includes 256 rows of 256 pixels each, each pixel including data for color and brightness, according to known methods. For example, in each pixel, color may be represented by a mosaic of four sub-pixels, each sub-pixel corresponding to primaries such as red, green, or blue (where one primary is represented twice). The brightness of the overall pixel may be recorded by, for example, a one byte (i.e., 0-255) brightness value. Other data formats may be used.</p>
<p id="p-0197" num="0196">According to one embodiment, a reliable exposure measurement may require inclusion of every n<sup>th </sup>(e.g., 4<sup>th</sup>) pixel (which may be, for example, every second RED pixel, since there are typically more red pixels according to one embodiment, in every m lines (e.g. 10 out of 256 lines, in a typical approximately 66,000 pixel frame (e.g., 256&#xd7;256 pixels)). This is equivalent to approximately 640 pixels in a typical frame. In one embodiment a reliable exposure measurement may require approximately 1.5% (for example, 11 pixels out of 640) of the selected pixels to be saturated in order to pass a saturation threshold, according to which gain decisions may be taken. Other frame sizes, percentages, and sample rates may be used, as appropriate. For example, 9, 11, 15, 24, and any other number of pixels can be used per frame or per sampled subset to determine a saturation threshold. Other individual pixels, e.g., non-red pixels, may be sampled, and sampling need not be based on color.</p>
<p id="p-0198" num="0197">According to one embodiment, exposure time may be determined in, for example, 8 steps, from 5 ms to 40 ms. Other numbers of steps and intervals may be used, and intervals need not be used&#x2014;e.g., exposure time may be determined on a continual basis. Furthermore, T<b>1</b> may be digitally programmable, for example, in 8 steps with a, for example, logarithmic scale from, for example, 1 ms to 100 ms. According to one embodiment, the accuracy of detection levels may be defined, for example, as less than 5%. Other accuracy levels may be used, as appropriate.</p>
<p id="p-0199" num="0198">The measurement of the exposure, according to one embodiment, may be performed on a subset of the pixels in the sensor array or imager. A determination as to whether a gain exposure change may be in order may be based on, for example, a percentage of the selected pixels that are saturated with light, relative to, for example, a saturation threshold for one or more pixels. The gain or other parameter setting decision may be based on one or more discrete time intervals within which the outputs from a nominal number of the selected number of pixels (e.g., 11 according to one embodiment) have reached a certain saturation level (e.g., reference level), or threshold. In this way, the exposure may continue until it is determined that the pixel output from the nominal number of pixels has reached a new saturation level, at which time the gain (or other parameter) level may be changed. According to one embodiment, exposure may continue until, for example, full saturation or maximum exposure time (T<sub>m</sub>) is reached. It should be noted that determination of full saturation may differ according to the various gain (or other parameter) levels. For example, expected saturation at gain <b>1</b> may be V<b>1</b>, and expected saturation at gain <b>4</b> may be V<b>1</b>/<sub>4</sub>. The reference voltage (Vref), or threshold voltage, for determining whether gain and/or exposure need to be changed may be defined at any discreet time interval, relative to the proportion of time to T<b>1</b>. For example, Vref may be equal to Vfs/<sub>4 </sub>at time T<b>1</b>/<sub>4</sub>, when the light reflection may initially be measured. Similarly, Vref may be equal to Vfs/<sub>2 </sub>at time T<b>1</b>/<sub>2</sub>, when the light reflection may subsequently be measured.</p>
<p id="p-0200" num="0199">According to some embodiments of the present invention, an aggregate gray scale for the selected pixels (<b>640</b> in the current example, etc.) may be measured at one or more intervals. The average may be compared to a saturation threshold, and gain decisions that are taken may be related to the average gray scale measurement and the relevant saturation threshold.</p>
<p id="p-0201" num="0200">According to some embodiments of the present invention, the signal saturation level may be defined as Voltage Saturation, or Vsat, which represents a low pixel voltage, and may be defined as the pixel saturation referred to ground. Pixel reset level may be defined as Vrst, which represents the highest pixel voltage. Finally, Vfs may be defined as the A/D full-scale voltage, which represents the actual saturation or voltage level at a determined interval. Accordingly, Vsat=Vrst-Vsat=Vfs. In other words, the pixel signal level may be defined as the difference between the pixel reset level and the instantaneous pixel voltage, and may be a positive voltage, increasing from 0, during exposure. This &#x201c;delta-voltage&#x201d; may be compared with the comparator reference voltage, such as, for example Vfs/<sub>4</sub>, at T<b>1</b>/<sub>4</sub>.</p>
<p id="p-0202" num="0201">Reference is now made to <figref idref="DRAWINGS">FIG. 15</figref>, which illustrates, according one embodiment of the present invention, gain setting decisions that may be made, during an exposure period, as a function of pixel output (light saturation) vs. time. As can be seen in <figref idref="DRAWINGS">FIG. 15</figref>, for example, three gain settings that may be used are gain <b>1</b>, gain <b>2</b> and gain <b>4</b>. The thicker lines <b>161</b>, <b>163</b> and <b>165</b> represent the gain limits or thresholds. The settings of <figref idref="DRAWINGS">FIG. 15</figref> are provided as an example only, and are not meant to be limiting. Other numbers of gain settings may be used.</p>
<p id="p-0203" num="0202">At an interval such as T<b>1</b>/<sub>4</sub>, the saturation threshold (level) for selected pixels may be measured. The saturation threshold may be at, below, or above an expected threshold <b>161</b>. In case &#x201c;b&#x201d;, where the pixel output (average saturation of the selected pixels) is above the expected threshold <b>161</b>, it is to be expected that full exposure will be completed within, for example, T<b>1</b>. Therefore no increase in gain, or sensitivity, is necessary, and exposure continues until full saturation (Vfs) is reached, at which time exposure is stopped.</p>
<p id="p-0204" num="0203">In case &#x201c;c&#x201d;, where the pixel output is below the expected threshold <b>161</b>, but above a middle threshold <b>163</b>, it is to be expected that exposure will not be completed within T<b>1</b>. Even though saturation may eventually be attained, the exposure will inevitably be longer than T<b>1</b>, which may cause a blurring effect. Therefore the imager may be commanded to increase the gain level from gain <b>1</b> to gain <b>2</b>. Accordingly, the saturation threshold may be decreased by, for example, half, to Vfs/<sub>2</sub>, which together with, for example, gain <b>2</b> amplification may provide full image exposure. The exposure may then continue until saturation level Vfs/<sub>2 </sub>is reached, at which time exposure may be stopped. Vfs/<sub>2 </sub>represents full saturation in this case since gain level <b>2</b> was applied. Other gain levels may be used.</p>
<p id="p-0205" num="0204">In case &#x201c;d&#x201d; where the pixel output is below the middle threshold <b>163</b>, but above a lower threshold <b>165</b>, it may be expected that exposure will not be completed within T<b>1</b>. Therefore exposure continues and, in addition, a more significant increase in gain level may be necessary. The imager may be commanded to increase the gain level to gain <b>4</b>, for example, and exposure may subsequently continue until corresponding saturation level Vfs/<sub>4 </sub>is reached, at which time exposure may be stopped.</p>
<p id="p-0206" num="0205">In case &#x201c;a&#x201d; where the pixel output is below the lower threshold <b>164</b>, it may be expected that exposure will not be completed within T<b>1</b>, or even within T<sub>M</sub>. Therefore an increase in gain level to gain <b>4</b>, for example, may be necessary, and exposure may continue until T<b>1</b>. In this example, since full exposure has not been achieved even at T<b>1</b>, exposure may continue until T<sub>M</sub>, to increase the possibility of sufficient exposure. T<sub>M </sub>may be the maximum exposure time for all cases.</p>
<p id="p-0207" num="0206">As can been in the above example, and in relation to <figref idref="DRAWINGS">FIG. 15</figref>, full exposure has been achieved in 3 out of the 4 frames, and exposure time has likewise been reduced in 3 out of the four frames, possibly leading to significant increases in exposure clarity and decreases in usage of power resources. In alternate embodiments, other levels of alteration or improvement may result.</p>
<p id="p-0208" num="0207">Additional measurement instances may also be provided, such as, for example, T<b>1</b>/<b>2</b> and T<b>1</b>/<sub>3 </sub>etc. The pixel exposures in the above scenarios may likewise be measured at this second interval, to establish whether further gain level changes are necessary.</p>
<p id="p-0209" num="0208">In one embodiment, a first scan may be utilized to search for &#x201c;white spots&#x201d;, or &#x201c;hot spots&#x201d;, which are problematic (poorly or non-functioning) image receivers that may not be counted in the group of saturated pixels. This first scan may therefore be designed to detect, define and discard the problematic or non-functional pixels from the selected pixel group. Such a defective/non-functioning scan need not be used.</p>
<p id="p-0210" num="0209">Following is an example of a non-limiting description of &#x201c;pseudo-code&#x201d;, which may be used to implement an embodiment of the present invention. Other embodiments of the present invention may be implemented without coding, such as by using circuit design. Various embodiments of the present invention may be implemented using different code sequences and programming or logic design techniques:</p>
<p id="p-0211" num="0210">
<tables id="TABLE-US-00001" num="00001">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="21pt" align="left"/>
<colspec colname="1" colwidth="196pt" align="left"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry>Vref=Vfs/<sub>4</sub></entry>
</row>
<row>
<entry/>
<entry>Gain=4</entry>
</row>
<row>
<entry/>
<entry>Exposure=&#x2019;on&#x2019;</entry>
</row>
<row>
<entry/>
<entry>White_spots=Scan result</entry>
</row>
<row>
<entry/>
<entry>While Exposure=&#x2019;on&#x2019;</entry>
</row>
<row>
<entry/>
<entry>&#x2003;Pix_above_ref=Scan result %%%%For each scan</entry>
</row>
<row>
<entry/>
<entry>&#x2003;If(t&#x3e;=Tm)</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;Exposure=&#x2019;off&#x2019;</entry>
</row>
<row>
<entry/>
<entry>&#x2003;Else %(t&#x3c;Tm)</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;If (Pix_above_ref-White_spots&#x3e;=&#x2019;table_value&#x2019;)</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;If (Gain=4)</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;If (t&#x3c;T1/<sub>4</sub>)</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;Vref=Vfs;</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;Gain=1;</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;Else %(t&#x3e;=T1/<sub>4</sub>)</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;If (t&#x3c;T1/<sub>2</sub>)</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;Vref=Vfs/<sub>2</sub>;</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;Gain=2</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;Else %(t&#x3e;=T1/<sub>2</sub>)</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;exposure=&#x2019;off&#x2019;</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;End %(t&#x3c;T1/<sub>2</sub>)</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;End %(t&#x3c;T1/4)</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;Else %(Gain !=4)</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;&#x2003;Exposure=&#x2019;off&#x2019;</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;&#x2003;End %(Gain=4)</entry>
</row>
<row>
<entry/>
<entry>&#x2003;&#x2003;End % (Pix_above_ref-White_spots&#x3e;=&#x2019;table_value&#x2019;)</entry>
</row>
<row>
<entry/>
<entry>&#x2003;End %(t&#x3c;Tm)</entry>
</row>
<row>
<entry/>
<entry>End %( While Exposure=&#x2019;on&#x2019;)</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0212" num="0211">It is noted that such automatic gain control may result, under certain conditions in changes in the signal to noise ratio (S/N) of the imager in some cases. For example, increasing the pixel amplifier gain in CMOS pixel array imagers may result in higher S/N ratios, while increasing the exposure time (Tm) may increase the image &#x201c;blur&#x201d;.</p>
<p id="p-0213" num="0212">According to some embodiments of the present invention, the above described principles and related <figref idref="DRAWINGS">FIG. 15</figref> may likewise be used to describe implementations of the present invention for other parameters such as illumination intensity and duty cycle etc., or in combination with such other parameters. For example, a method may be followed for determining light saturation in relation to light intensity and duty cycle et, according to threshold values, at selected instances. These calculations of saturation level may determine whether such parameters are at, below or above a given threshold. Such a determination may enable decision making such as to change light illumination, duty cycle etc. to reflect the exposure needs of the relevant frames.</p>
<p id="p-0214" num="0213">For example, the controller or any other element may measure light saturation levels in one or more light measuring elements, and in response to resulting measurements, may simultaneously at least one of control illumination duration, illumination intensity and/or image gain level. According to one embodiment of the present invention, illumination (exposure) may be increased eight-fold, for example, using the following or other possibilities:</p>
<p id="p-0215" num="0214"><img id="CUSTOM-CHARACTER-00001" he="2.79mm" wi="3.13mm" file="US08626272-20140107-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/> duration&#xd7;2, intensity&#xd7;2, gain&#xd7;2</p>
<p id="p-0216" num="0215"><img id="CUSTOM-CHARACTER-00002" he="2.79mm" wi="2.79mm" file="US08626272-20140107-P00002.TIF" alt="custom character" img-content="character" img-format="tif"/> duration&#xd7;8, intensity&#xd7;1, gain&#xd7;1</p>
<p id="p-0217" num="0216"><img id="CUSTOM-CHARACTER-00003" he="2.79mm" wi="2.12mm" file="US08626272-20140107-P00003.TIF" alt="custom character" img-content="character" img-format="tif"/> duration&#xd7;1, intensity&#xd7;8, gain&#xd7;1</p>
<p id="p-0218" num="0217"><img id="CUSTOM-CHARACTER-00004" he="2.79mm" wi="2.46mm" file="US08626272-20140107-P00004.TIF" alt="custom character" img-content="character" img-format="tif"/> duration&#xd7;1, intensity&#xd7;1, gain&#xd7;8</p>
<p id="p-0219" num="0218"><img id="CUSTOM-CHARACTER-00005" he="2.79mm" wi="2.79mm" file="US08626272-20140107-P00005.TIF" alt="custom character" img-content="character" img-format="tif"/> duration&#xd7;1.5, intensity&#xd7;1.5, gain&#xd7;3.56 etc.</p>
<p id="p-0220" num="0219">Any other combinations of parameters that may be required to implement the above or alternate illumination targets may be utilized. Additional, any other suitable parameters may be factored in to the above described embodiments, individually or in any combination, to enable an in-vivo imaging device to provide more accurate exposure.</p>
<p id="p-0221" num="0220">According to some embodiments of the present invention, a method is provided for determining the location or position of an in vivo device, such as an in vivo imaging capsule. For example, a method is provided for determining when an in vivo device enters into a body or enters into a particular area of the body, etc. This determination may be used for decision making, such as, for example, a decision whether an in vivo capsule should enter or exit an operational mode such as &#x201c;fast mode&#x201d;, &#x201c;slow mode&#x201d;, &#x201c;standard mode&#x201d; etc. A fast mode, for example, may enable the imaging device to attain an increased frame rate, which may be particularly useful for enabling rapid imaging of the esophagus after swallowing an imaging capsule. Such rapid imaging is not required, however, when the device is in, for example, the small intestine, therefore the device may be programmed to switch to a &#x201c;standard mode&#x201d; after a time period following the internalization of the device. Other operational mode changes may be effected. Thus, for example, the imaging device may send compressed data in &#x201c;fast mode&#x201d; when traveling down the esophagus, and then operate in regular uncompressed mode at a lower frame rate thereafter, when such a fast frame rate is not required.</p>
<p id="p-0222" num="0221">In one embodiment of the invention, this may be accomplished by setting a mode, such as a fast mode, to end when a significant change has been determined in the environment surrounding the device. This may be accomplished by providing an environmental monitoring tool, such as a pH indicator, temperature gauge or light level indicator etc. in, on or outside of the imaging device to measure or otherwise determine environmental data. The monitoring or measurement tool may compare the measured data with previously measured environmental data to determine environmental changes. When, for example, the environment of the capsule has fallen below a certain pH level, this change in pH level may indicate that the esophagus has been traversed and that the imaging device is in the stomach. Various environments or environmental changes may be determined, depending on the measurement tools being used, such as outside the body, inside the body, in the mouth, in the throat, in the esophagus, in the stomach, in the small intestine etc.</p>
<p id="p-0223" num="0222">In other embodiments, a mode, such as a fast mode, may be set to end a fixed amount of time, e.g. five minutes, after a change is detected. A change may be, for example, the capsule entering a dark environment such as the mouth. For example, a controller may configure a light source to provide a &#x201c;dark frame&#x201d; at determined frame intervals, such as, for example, at 1 frame out of every 256. During a dark frame, LEDs or other illumination sources may not be lit or may be lit for a brief instant, substantially inadequate to provide viable exposure for an image. For example, an in vivo imaging device may require a 25 ms exposure at a fixed light intensity to adequately light an internal lumen, yet purposefully provides an inadequate exposure of, for example, 5 ms. The device may periodically process the &#x201c;dark&#x201d; frame, which may be analyzed to determine the presence of ambient light in the environment of the device. If the ambient light is above a threshold level, indicating that there is a substantial amount of surrounding light and that a non-substantial amount of additional light is required for the image to attain saturation, it may be assumed that the capsule has not yet entered the body and that the fast mode should continue. If the ambient light during the dark frame is below a threshold level, indicating that the image requires a substantially significant amount of additional light to attain saturation, it may be assumed that the capsule has entered a darker environment, such as the body. In this case it may be assumed that the fast mode will no longer be necessary after a predetermined period of time, e.g., five minutes, by which time it may be assumed that the capsule has passed through the esophagus.</p>
<p id="p-0224" num="0223">According to some embodiments, the exposure of the dark frame may be measured in relation to a light saturation threshold for the dark frames, to determine whether and/or by how much a change in an imager gain level is required. If the dark frame requires a substantially non-maximal gain factor, indicating that the image has a relatively adequate amount of light and only a relatively small gain level may be required to reach full exposure, the device may be defined as being outside a body. When the dark frame requires a large or maximal gain factor, indicating that a substantial gain is required for possibly reaching full saturation, the device may be defined as being inside a body.</p>
<p id="p-0225" num="0224"><figref idref="DRAWINGS">FIG. 16A</figref> depicts a series of steps of a method for determine a necessary image gain level during an exposure period, according to an embodiment of the present invention. In alternate embodiments, other steps, and other series of steps, may be used.</p>
<p id="p-0226" num="0225">In step <b>500</b>, a device, such as an in-vivo imaging device, turns on a light source.</p>
<p id="p-0227" num="0226">In step <b>510</b>, which may be a short sampling period, the device records (and possibly integrates) the amount of light received to at least one light measuring element. This may be, for example, to a sensor on the device, or possibly to an external sensor.</p>
<p id="p-0228" num="0227">In step <b>520</b>, the device determines the amount of light recorded.</p>
<p id="p-0229" num="0228">In step <b>530</b>, if the amount of light recorded, for example, by a portion of the frame's pixels is less than a certain pre-determined value (saturation threshold).</p>
<p id="p-0230" num="0229">In step <b>540</b>, the image gain level may be increased, and the device may continue exposure (or other parameters, such as light level etc.) until saturation is attained <b>560</b>. In this case, step <b>520</b> may be repeated at a subsequent time interval.</p>
<p id="p-0231" num="0230">In step <b>550</b>, where the amount of light recorded is more than a certain value (threshold), the image gain level may be decreased. Step <b>520</b> may be repeated at a subsequent time. At full saturation exposure may be stopped <b>560</b>.</p>
<p id="p-0232" num="0231">If the amount of light recorded is substantially equivalent to a determined saturation threshold (close in value to such a threshold such that the amount of light exposed can be assumed to be sufficient), a current gain level or light level etc., may be maintained and the exposure may be stopped when full saturation occurs, or until a maximum exposure time (T<sub>M</sub>) is reached, whichever is first to occur. The process may then be repeated <b>570</b> for subsequent frames.</p>
<p id="p-0233" num="0232">In step <b>570</b>, the above process is repeated from step <b>500</b>, as, the device may operate across a series of imaging periods. However, the method need not be repeated.</p>
<p id="p-0234" num="0233">According to one embodiment steps <b>500</b> to <b>520</b> may be complemented or replaced by a step wherein data from an environmental monitoring tool is analyzed, to determine if one or more particular data measurements, such as pH level, temperature level etc., are above, below or equal to a threshold for the particular measurement(s), or to one or more previous measurements. The results of such a comparison may be used to determine whether an environmental change has occurred, and whether an appropriate gain level change is in order. For example, an in vivo capsule may be adapted to carry multiple measurement tools for measuring different aspects in the environment, including a light level indicator and a pH indicator. In frame A the capsule may have measured levels i and ii using the two indicators listed above. In a second frame B it may be determined, for example, that both parameters i and ii have changed substantially from their measurements in frame <b>1</b>. In the case where the light level indicator reflects a darkening of the environment, and the pH indicator indicate a higher acidity, it may be determined that the capsule has both entered the body (a darker environment) and entered the stomach (increased acidity).</p>
<p id="p-0235" num="0234"><figref idref="DRAWINGS">FIG. 16B</figref> depicts a series of steps of a method for determining when an in vivo device has entered an area with different illumination, according to an embodiment of the present invention. In alternate embodiments, other steps, and other series of steps, may be used.</p>
<p id="p-0236" num="0235">In step <b>600</b>, a device, such as an in-vivo imaging device, turns on (operates) a light source.</p>
<p id="p-0237" num="0236">In step <b>610</b>, the device records (and possibly integrates) the amount of light received to a light measurement element. This may be, for example, part of the imager, a sensor on the device, or possibly to an external sensor.</p>
<p id="p-0238" num="0237">In step <b>620</b>, the device determines the amount of light recorded. Furthermore, the device may calculate this amount of light recorded in relation to a saturation threshold or any other threshold, to determine a possible location of the device based on the amount of light recorded. For example, if the light recorded in a first frame is above a saturation threshold of, for example, 15 pixels out of the frame, this indicates that saturation has easily been attained, and the device is assumed to be outside the body (in a light environment). If the light recorded in a second frame is below the same saturation threshold of, for example, 15 pixels out of the frame, this indicates that saturation has not been attained, and the device is assumed to be inside the body (in a dark environment).</p>
<p id="p-0239" num="0238">In step <b>630</b>, a decision may be taken to change operation mode of the device, depending on the amount of light recorded relative to a threshold value.</p>
<p id="p-0240" num="0239">In step <b>640</b>, for example, if the amount of light recorded is less than a certain value (threshold), indicating that the device is located in a darker area, the device may change the mode of operation <b>640</b> to reflect this darker environment. For example, the device may be configured to start operating in a fast-mode for a period of 10 minutes after entering the body, to enable fast imaging of the esophagus area. After determining that the device has entered the body, the timer may be initiated, so that after 10 minutes the device will change into a slower mode for the remainder of the procedure.</p>
<p id="p-0241" num="0240">In step <b>650</b>, for example, if the amount of light recorded is more than a certain value (threshold), indicating that the device is located in a lighter area, the device may change the mode of operation <b>650</b> to reflect this lighter environment.</p>
<p id="p-0242" num="0241"><figref idref="DRAWINGS">FIG. 16C</figref> depicts a series of steps of a method for determining an in vivo device's location, according to an embodiment of the present invention. In alternate embodiments, other steps, and other series of steps, may be used.</p>
<p id="p-0243" num="0242">In step <b>700</b>, a device, such as an in-vivo imaging device, operates at least one environmental measuring device, such as, for example, a pH level sensor and a light detection meter.</p>
<p id="p-0244" num="0243">In step <b>710</b>, the device records a measurement, such as pH level for example, received to the measurement device. This measurement device may be, for example, a sensor on or in the device and/or an external sensor.</p>
<p id="p-0245" num="0244">In step <b>720</b>, the device determines the quantity and/or quality of the measurement recorded.</p>
<p id="p-0246" num="0245">In step <b>730</b>, the device may determine a location in a body based on the measurement data recorded, as compared with a threshold value or previous measurements etc. For example, if the pH level in a first frame is above a saturation threshold of, for example 7 on the pH scale, this indicates that the device is in a non-acidic environment, and may be assumed to be in the throat area (in an acidic-neutral environment). If the pH level recorded in a second frame is below the same threshold of, for example, 7 on the pH scale, this indicates that the device is in a more acidic environment, and may be assumed to be in the stomach or intestine area, depending on the pH level recorded.</p>
<p id="p-0247" num="0246">In step <b>740</b>, a decision may be taken to change operation mode of the device, depending on the measurement data recorded relative to a threshold or alternative value.</p>
<p id="p-0248" num="0247">In step <b>750</b>, for example, if the amount of measurement data quantity and/or quality is more or less than a certain value (threshold or other value), indicating (or verifying) that the device is located in a different area, the device may change the mode of operation to reflect this new environment.</p>
<p id="p-0249" num="0248">The results of the above processes may be used to determine whether environmental conditions have substantially changed, based on results from various optional monitoring and/or measuring tools. The change required to be defined as &#x201c;substantial&#x201d; or &#x201c;significant&#x201d; may be determined for each case or by manufacture.</p>
<p id="p-0250" num="0249">Typically, the various embodiments discussed herein may be implemented in a device such as device <b>30</b> (<figref idref="DRAWINGS">FIG. 2</figref>); however, such embodiments may be implemented in a variety of imaging or sensing devices, varying in structure. The various functions and processes described above, including but not limited to processing, monitoring, measuring, analyzing, defining, tracking, comparing, computing, commanding, stopping exposure, increasing gain, decreasing gain, increasing exposure, decreasing exposure, changing mode etc. may be executed by, for example, a processor unit (e.g., <b>36</b> in <figref idref="DRAWINGS">FIG. 2</figref>). These functions or processes may additionally and/or alternatively be implemented by the processor unit <b>36</b> alone, by alternative units, such as illumination control unit <b>40</b>, telemetry unit <b>34</b>, light sensing units <b>42</b>, imaging unit <b>32</b> etc., or any combination of units. The methods and processes described may also be embodied in other sensing devices having other structures and other components. Alternately, part or all of the analysis or control involved in the various methods presented may be performed by an external workstation or processing unit.</p>
<p id="p-0251" num="0250">It will be appreciated by those skilled in the art that while the invention has been described with respect to a limited number of embodiments, many variations, modifications, combinations and other applications of the invention may be made which are within the scope and spirit of the invention.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method of operating a swallowable in-vivo imaging capsule, the capsule comprising a light source; an imager comprising a plurality of pixels, the plurality of pixels comprising one or more control pixels which are a subset of the plurality of pixels, the method comprising across a plurality of imaging periods, each imaging period including an illumination period and a dark period, each illumination period including a control period:
<claim-text>operating the light source during the control period;</claim-text>
<claim-text>recording via the control pixels the amount of light reflected to the imager during the control period; and</claim-text>
<claim-text>based on the recorded amount of light, computing a duration of illumination required for the illumination period; and</claim-text>
<claim-text>providing an illumination period according to the computed duration.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the imager is a CMOS imager.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> comprising scanning the imager to acquire an image.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref> wherein the recording is performed at a different time than the scanning.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref> wherein the recording does not reset the one or more control pixels.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein when the imager is scanned the one or more control pixels are read to acquire a portion of the image.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more control pixels are arranged symmetrically with respect to the center of the imager.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more control pixels comprise analog photodiodes.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the amount of light recorded by the control pixels is computationally corrected to compensate for different control pixel exposure periods.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, comprising recording, via each of at least two subsets of one or more control pixels, an amount of light reflected to a portion of the imager.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. A swallowable in vivo imaging capsule comprising:
<claim-text>a light source;</claim-text>
<claim-text>an imager comprising a plurality of pixels; and</claim-text>
<claim-text>a controller to, across a plurality of imaging periods, each imaging period including an illumination period and a dark period, each illumination period including a control period:
<claim-text>operate the light source during the control period;</claim-text>
<claim-text>record, via one or more control pixels being a subset of the plurality of pixels, the amount of light reflected to the imager during the control period;</claim-text>
<claim-text>based on the recorded amount of light, compute a duration of illumination required for the illumination period; and</claim-text>
<claim-text>provide an illumination period according to the computed duration.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The capsule of <claim-ref idref="CLM-00011">claim 11</claim-ref> wherein the imager is a CMOS imager.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The capsule of <claim-ref idref="CLM-00011">claim 11</claim-ref> wherein the controller is to scan the imager to acquire an image.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The capsule of <claim-ref idref="CLM-00013">claim 13</claim-ref> wherein the recording the amount of light via the one or more control pixels is performed at a different time than the scanning to acquire an image.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The capsule of <claim-ref idref="CLM-00011">claim 11</claim-ref> wherein recording of the amount of light does not reset the one or more control pixels.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The capsule of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein when the imager is scanned to acquire an image the one or more control pixels are read to acquire a portion of the image.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The capsule of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the one or more control pixels of the plurality of pixels are arranged symmetrically with respect to the center of the imager.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The capsule of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the one or more control pixels comprise analog photodiodes.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The capsule of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the amount of light recorded by the control pixels is computationally corrected to compensate for different control pixel exposure periods.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The capsule of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the controller is to record, via each of at least two subsets of or more control pixels, an amount of light reflected to a portion of the imager.</claim-text>
</claim>
</claims>
</us-patent-grant>
