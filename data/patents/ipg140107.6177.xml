<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08627301-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08627301</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>11750441</doc-number>
<date>20070518</date>
</document-id>
</application-reference>
<us-application-series-code>11</us-application-series-code>
<us-term-of-grant>
<us-term-extension>1968</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>9</main-group>
<subgroup>44</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>9</main-group>
<subgroup>45</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>717158</main-classification>
<further-classification>717132</further-classification>
<further-classification>717128</further-classification>
<further-classification>717130</further-classification>
<further-classification>717131</further-classification>
<further-classification>717149</further-classification>
<further-classification>717156</further-classification>
</classification-national>
<invention-title id="d2e53">Concurrent management of adaptive programs</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>6009452</doc-number>
<kind>A</kind>
<name>Horvitz</name>
<date>19991200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>718102</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>2007/0234326</doc-number>
<kind>A1</kind>
<name>Kejariwal et al.</name>
<date>20071000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>717151</main-classification></classification-national>
</us-citation>
<us-citation>
<nplcit num="00003">
<othercit>Acar et al, &#x201c;Adaptive Functional Programming&#x201d;, 2002, ACM.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00004">
<othercit>R. Kramer, &#x201c;The Combining DAG: A Technique for Parallel Data Flow Analysis&#x201d;, 1994, IEEE Transactions on parallel and distributed systems, vol. 5.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00005">
<othercit>Bender, et al., &#x201c;On-the-Fly Maintenance of Series-Parallel Relationships in Fork-Join Multithreaded Programs&#x201d;, Proceedings of the sixteenth annual ACM symposium on Parallelism in algorithms and architectures (SPAA'04), Barcelona, Spain, Jun. 27-30, 2004, 12 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00006">
<othercit>Hammer, et al., &#x201c;CEAL: A C-Based Language for Self-Adjusting Computation&#x201d;, Technical Report&#x2014;Toyota Technological Institute Chicago (TTIC-TR-2009-2), 2009, 36 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00007">
<othercit>Acar, et al., &#x201c;Adaptive Functional Programming&#x201d;, ACM Transactions on Programming Languages and Systems, vol. 28, No. 6, Nov. 2006, pp. 990-1034.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>11</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>None</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>2</number-of-drawing-sheets>
<number-of-figures>2</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20080288950</doc-number>
<kind>A1</kind>
<date>20081120</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Hammer</last-name>
<first-name>Matthew</first-name>
<address>
<city>Stoughton</city>
<state>WI</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Rajagopalan</last-name>
<first-name>Mohan</first-name>
<address>
<city>Mountain View</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Ghuloum</last-name>
<first-name>Anwar</first-name>
<address>
<city>Menlo Park</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Hammer</last-name>
<first-name>Matthew</first-name>
<address>
<city>Stoughton</city>
<state>WI</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Rajagopalan</last-name>
<first-name>Mohan</first-name>
<address>
<city>Mountain View</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Ghuloum</last-name>
<first-name>Anwar</first-name>
<address>
<city>Menlo Park</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Grossman, Tucker, Perreault &#x26; Pfleger, PLLC</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Intel Corporation</orgname>
<role>02</role>
<address>
<city>Santa Clara</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Bullock, Jr.</last-name>
<first-name>Lewis A</first-name>
<department>2199</department>
</primary-examiner>
<assistant-examiner>
<last-name>Gooray</last-name>
<first-name>Mark</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A method for concurrent management of adaptive programs is disclosed wherein changes in a set of modifiable references are initially identified. A list of uses of the changed references is next computed using records made in structures of the references. The list is next inserted into an elimination queue. Comparison is next made of each of the uses to the other uses to determine independence or dependence thereon. Determined dependent uses are eliminated and the preceding steps are repeated for all determined independent uses until all dependencies have been eliminated.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="212.09mm" wi="159.85mm" file="US08627301-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="213.11mm" wi="162.39mm" file="US08627301-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="139.45mm" wi="150.79mm" file="US08627301-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">FIELD</heading>
<p id="p-0002" num="0001">The present disclosure relates to adaptive computer programs and the management thereof. In particular, the present disclosure teaches processes for the management of adaptive programs in a parallel environment such as a multi-core processing system.</p>
<heading id="h-0002" level="1">BACKGROUND</heading>
<p id="p-0003" num="0002">In the most general terms, an adaptive program is a program in which changes in input are automatically propagated to the output. That is to say, the output reflects changes in input values without having to rerun the whole program, and only those parts affected by the changes are re-evaluated. The advent of multi-core processing technologies enables parallel processing of different kinds of applications, many of which often require a &#x201c;divide and conquer&#x201d; approach.</p>
<p id="p-0004" num="0003">For applications such as video, vision, graphics, audio, physical simulation, gaming, and mining, such parallelism allows the program to meet application speed requirements and take advantage of faster multi-core technologies. Adaptive programming is especially useful in such multi-core processor systems and has been shown to significantly improve the running time of such &#x201c;divide and conquer&#x201d; style methods. Current approaches to adaptive programs have mainly been explored in the context of functional languages and the approaches have been tailored to uni-processor based sequential execution methods. Efforts to date to manage adaptive programming processes have been exclusively serial in nature and have lacked advances in concurrent or parallel adaptive programming.</p>
<p id="p-0005" num="0004">This disclosure presents primitives that enable a parallelizable approach to adaptive programming in imperative (non-declarative) languages. This disclosure also presents methods and mechanisms that allow one to efficiently perform change propagation in a parallel process, as in multi-core processors, enabling effective optimization of data parallelism for multi-core architecture, which will further enable and promote parallel software development.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading>
<p id="p-0006" num="0005">Features and advantages of the claimed subject matter will be apparent from the following detailed description of embodiments consistent therewith, which description should be considered with reference to the accompanying drawings, wherein:</p>
<p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. 1</figref> is a concurrent change propagation method according to the disclosure; and</p>
<p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. 2</figref>: is a schematic diagram of a recovering partial order maintenance scheme according to the disclosure, using a combination of two orders.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<p id="p-0009" num="0008">Although the following detailed description will proceed with reference being made to illustrative embodiments, many alternatives, modifications, and variations thereof will be apparent to those skilled in the art. Accordingly, it is intended that the claimed subject matter be viewed broadly.</p>
<heading id="h-0004" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0010" num="0009">A key aspect of an adaptive program is the automatic tracking and efficient propagation of changes in the system state. To implement this functionality, each &#x201c;step&#x201d; in the program is tracked, essentially recording how all pieces of data are produced and consumed, and inter-leaving (control-flow or data-flow) constraints are tracked. One way of obtaining this information is by exhaustively recording the state in a program's execution trace. The steps in the trace essentially describe the different operations that can be performed and how changes need to be propagated through the system.</p>
<p id="p-0011" num="0010">Each independent unit of data is typically abstracted opaquely by a modifiable reference (modref). Modrefs are immutable, in the sense that they may be written to only once. In order to change the value associated with a modref, the original is invalidated and a new modref is created to replace the original. The modref's application programming interface (API) essentially provides three primitive operations: read, write and create. Create creates a new modref cell whereas write updates the value associated with a modref. Read allows the program to use the value of a modref, that has been previous written into, and records within the structure of the given modref where and when the value has been used. Moreover, if the data is not available, the program must perform some evaluation to produce the data and populate the modref before execution continues. Read operations have traditionally been performed with eager evaluation methods and this imposes a sequential evaluation order.</p>
<p id="p-0012" num="0011">Traditionally, modrefs have been processed with eager evaluation methods in a serial execution setting; each modref always holds data before it is read, and is empty before it is written. Eager evaluation generally refers to a mode of operation in which tasks are executed (evaluated) as soon as they are created. On the other end of the spectrum, lazy evaluation refers to a mode of operation in which tasks are executed only when their results are required. A first feature of the disclosed system is a new primitive, called a &#x201c;lazy modref&#x201d;. Lazy or lenient evaluation refers to a mode of operation in which tasks need not be executed as soon as they are created, however, idle resources (processors, hardware threads, etc.) are free to process created tasks as they desire.</p>
<p id="p-0013" num="0012">In general, if the number of resources is large, then there is a high probability that the tasks would have been processed before their results are required.</p>
<p id="p-0014" num="0013">Lazy modrefs improve on traditional modrefs by allowing such lenient or lazy evaluation methods. These methods are the basis for parallelizing adaptive programs. In the disclosed setting, a lenient execution model is used where each lazy-modref can be empty, typically after creation. Alternatively, the lazy-modref may contain a ready-to-read value or a continuation that will work to populate the value once executed. This continuation may also populate the lazy-modref with yet another continuation. This continues in loop fashion until the lazy-modref is populated with a data value. This is guaranteed to eventually occur under correct usage of the API and framework. When the lazy-modref holds such a continuation, the programmer has setup a process to eventually write to the lazy-modref when executed. Since each process can be executed leniently, the programmer may not have written to the lazy-modref yet. The lazy modref's API provides the same three primitive operations as traditional modrefs&#x2014;create, write and read&#x2014;with similar methods. However, a fundamental difference is that lazy-modrefs' signatures are based on destination-passing-style methods and that the caller manage the memory used to store the result. Decoupling the store in this way is essential for enabling lenient evaluation methods and forms the basis for parallelizing adaptive programs.</p>
<p id="p-0015" num="0014">A set of interconnected modrefs tracks changes to the system state. The execution trace is abstracted out as a binary tree of modrefs that describes nodes where data is created and consumed, and where the different control and data-flow dependencies leading to these nodes are created and consumed. The dependency constraints are exposed through the seq-split primitives. This information may be used to identify nodes where the computation can be parallelized.</p>
<p id="p-0016" num="0015">A second feature of the disclosed system is a mechanism for parallelizing change propagation in an adaptive program. Change propagation describes the process of updating system state in response to some change in the input set. The parallel change propagation method is similar to the serial change propagation method except that in order to preserve correctness in the presence of parallelism we need to rely on recording extra information about uses of modrefs. This information, stored in the modrefs themselves, is used at runtime to identify nodes that need to be recomputed to make appropriate adjustments to the output values. Any given modref may have multiple read locations that may be dependent on each other. A separate mechanism maintains and facilitates querying for these relationships which is called the &#x201c;dependence structure&#x201d;.</p>
<p id="p-0017" num="0016">The method <b>100</b> works as shown in <figref idref="DRAWINGS">FIG. 1</figref>. A set of changed modrefs <b>102</b> is first identified, which correspond to the parts of the program's input that have changed. Next is computed a list of uses (&#x201c;reads&#x201d;) <b>104</b> of these modrefs using the records made in their structures each time they're used. This list is called the &#x201c;invalidated uses&#x201d;. Next is computed each invalidated use <b>106</b> and inserted into an elimination queue, which is similar to a conventional queue except that its methods for &#x201c;insert&#x201d; differ: when an element is to be inserted, it is compared <b>108</b> against every element in the queue for dependence. If any element in the queue is dependent (the new element precedes this element in question according to our dependence structure) the dependent element is dropped from the queue. Likewise, if the element being inserted is found to be dependent on any element already in the queue, the insertion process stops and the new element is dropped.</p>
<p id="p-0018" num="0017">Once each invalidated use has been processed with the elimination queue, the elements left in the queue are independent of one another as well as independent of all elements originally in the set of invalidated uses. Modrefs are next examined <b>110</b> for changes in value, upon the occurrence of which re-execution proceeds for each use (which is itself a continuation) in parallel. Upon re-execution some other modrefs may change value, upon which the change propagation method for each of these (or each subset of these) is re-instated.</p>
<p id="p-0019" num="0018">Unlike prior art approaches, there are several opportunities for parallelization in the disclosed process. First, each execution of the program itself has independent components which may be executed in parallel. Next, since these components' independence is stored precisely by the dependence structure, false dependencies causing needless re-execution are circumvented and the ability to re-execute these components in parallel is gained, should they require re-execution.</p>
<p id="p-0020" num="0019">This is the first and only system that is capable of parallel change propagation. This is the only work that supports imperative language features. This is the first approach to parallelize adaptive programs.</p>
<p id="p-0021" num="0020">Next, order maintenance in adaptive programs is addressed. Order maintenance involves three operations, insert, remove and compare, which modify and query a total ordering. Insert is the operation that adds a new element to the ordering immediately after an existing element. Remove removes a given element. Given two elements in the ordering, compare computes whether they have a less-than, an equal-to, or a greater-than relationship. In the order maintenance routine of the disclosure, the problems traditionally associated with order maintenance are solved for the three operations with amortized constant-time on a machine where arithmetic can be done in constant time. This disclosure solves traditional order maintenance problems for concurrent adaptive or incremental computation for fine-grained dependent task graphs in constant time. Additionally, this disclosure enables the use of a truly lock-free data structure for concurrent updates.</p>
<p id="p-0022" num="0021">In existing adaptive programs, a total ordering is used to efficiently represent and later query the execution trace. This total order is used when querying dependency information and forms the basis for doing insert, read and compare operations in constant time. However, this representation fails to support parallel execution methods. Since each step of the method has a distinct position in the trace of the method, only serial updates may be realized. To elaborate, once they are included in the total-ordering, steps that are conceptually independent may appear to be dependent. The presently disclosed representation solves this problem by efficiently encoding the dependency information while eliminating such false dependencies. This provides a means for identifying and scheduling parallel tasks while retaining the constant time bounds.</p>
<p id="p-0023" num="0022">One can check for dependency by comparing the rank in the two total orders (TO), #<b>1</b> and #<b>2</b>. In general, approximation of a partial order is made through a combination of total orders. Specifically, a combination of two total orders may be used for a large class of programs to recover the original partial order, which conveys dependency information, exactly. <figref idref="DRAWINGS">FIG. 2</figref> illustrates the gist of this approach. Intuitively, the approach corresponds to choosing two complementary topological sorts of the partial order. This method may select the two total orders correctly. In one embodiment, no more than constant overhead is added for each operation when compared to the original scheme.</p>
<p id="p-0024" num="0023">All of the dependency information contained within the trace of the adaptive program may be represented as a binary tree. A language may be developed that can be used to verify this property. While there may be arbitrary dependencies in the task graph, the reduction to a binary tree is performed by annotating the graph using three special nodes: read nodes, split nodes and sequence nodes. Read nodes represent a data dependency on some cell of memory and may have a single child which conceptually encloses the &#x201c;use&#x201d; of this data value. Split and sequence nodes represent control flow and each has two children. Sequence node children are ordered and are referred to as &#x201c;first&#x201d; and &#x201c;second&#x201d; children. Split nodes introduce independence and their children are referred to as &#x201c;left&#x201d; and &#x201c;right&#x201d; children. After annotating with these nodes the original task-graph reduces into a binary tree. The two total orderings can be thought of as two topological sorts of this binary tree, though they are created on-line, as the tree is created. The first total ordering is a depth-first walk of the tree, where &#x201c;first&#x201d; is visited before its sibling &#x201c;second&#x201d; in sequence nodes and &#x201c;left&#x201d; is visited before its sibling &#x201c;right&#x201d; in split nodes. This is generally called &#x201c;English&#x201d; ordering. The second ordering is the same, except that children of split nodes are visited in reverse order (&#x201c;right&#x201d; and then &#x201c;left&#x201d;), which is generally called &#x201c;Hebrew&#x201d; ordering.</p>
<p id="p-0025" num="0024">Referring to <figref idref="DRAWINGS">FIG. 2</figref>, the herein disclosed order maintenance scheme <b>200</b> is depicted in schematic form. In the diagram, ranking of the orders is in accordance with the following guide;</p>
<p id="p-0026" num="0025">
<tables id="TABLE-US-00001" num="00001">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="6">
<colspec colname="offset" colwidth="70pt" align="left"/>
<colspec colname="1" colwidth="35pt" align="center"/>
<colspec colname="2" colwidth="28pt" align="center"/>
<colspec colname="3" colwidth="28pt" align="center"/>
<colspec colname="4" colwidth="28pt" align="center"/>
<colspec colname="5" colwidth="28pt" align="center"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="5" align="center" rowsep="1"/>
</row>
<row>
<entry/>
<entry>A(202)</entry>
<entry>B(204)</entry>
<entry>C(206)</entry>
<entry>D(208)</entry>
<entry>E(210)</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="5" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="6">
<colspec colname="1" colwidth="70pt" align="left"/>
<colspec colname="2" colwidth="35pt" align="center"/>
<colspec colname="3" colwidth="28pt" align="center"/>
<colspec colname="4" colwidth="28pt" align="center"/>
<colspec colname="5" colwidth="28pt" align="center"/>
<colspec colname="6" colwidth="28pt" align="center"/>
<tbody valign="top">
<row>
<entry>Rank in total order #1</entry>
<entry>1</entry>
<entry>2</entry>
<entry>3</entry>
<entry>4</entry>
<entry>5</entry>
</row>
<row>
<entry>(English):</entry>
</row>
<row>
<entry>Rank in total order #2</entry>
<entry>1</entry>
<entry>5</entry>
<entry>2</entry>
<entry>3</entry>
<entry>4</entry>
</row>
<row>
<entry>(Hebrew):</entry>
</row>
<row>
<entry namest="1" nameend="6" align="center" rowsep="1"/>
</row>
<row>
<entry namest="1" nameend="6" align="left" id="FOO-00001">B depends on A iff: Rank(B) &#x3e; Rank(A) in both total orders.</entry>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0027" num="0026">The disclosed scheme maintains these two total orderings in a manner that is similar to the way a single total order is traditionally maintained except that any two dependent read nodes, A <b>202</b> and B <b>204</b>, will appear in both orderings with A before B, whereas two independent read nodes, C <b>206</b> and D <b>208</b>, will appear in differing orders in the two total orderings. One will report that D precedes C and the other will report that C precedes D. The particular order that reports D first versus C first is of no consequence, so long as both dependence and independence with constant overhead may be detected using these differing outcomes.</p>
<p id="p-0028" num="0027">Two instances of the original total ordering data structure are used. Each total ordering is a doubly-linked list of labeled elements, where the label is a 32-bit or 64-bit word that represents its relative position in the ordering. Each read node introduces two elements into each total order to mark its beginning and ending points. Each split node and sequence node introduces a single element into each total order to mark its &#x201c;middle&#x201d;, which is an element that separates the node's two children and each of its children's successors. The methods of mapping threads uses this &#x201c;middle&#x201d; element abstraction to distribute work to a worker thread. New threads look for work using the middle elements that correspond to split nodes. Note that this ensures that all worker threads work on logically independent regions of the data structure. Thus another feature of the scheme is the lack of reliance on locks for accessing and updating the total ordering data structure concurrently. This is the first work aimed at developing a concurrent order-maintenance data structure, yet these methods have the same complexity bounds as the state of the art for sequential versions.</p>
<p id="p-0029" num="0028">Various features, aspects, and embodiments have been described herein. The features, aspects, and numerous embodiments described herein are susceptible to combination with one another as well as to variation and modification, as will be understood by those having skill in the art. The present disclosure should, therefore, be considered to encompass such combinations, variations, and modifications. The terms and expressions which have been employed herein are used as terms of description and not of limitation, and there is no intention, in the use of such terms and expressions, of excluding any equivalents of the features shown and described (or portions thereof), and it is recognized that various modifications are possible within the scope of the claims. Other modifications, variations, and alternatives are also possible. Accordingly, the claims are intended to cover all such equivalents.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method for an adaptive computer program comprising:
<claim-text>populating one or more modifiable references with task information;</claim-text>
<claim-text>employing idle resources to cooperate with said one or more modifiable references and process assigned tasks in a background mode using said task information;</claim-text>
<claim-text>providing task results on an as-needed basis;</claim-text>
<claim-text>tracking system state changes by said one or more modifiable references;</claim-text>
<claim-text>generating an execution trace as a binary tree of said one or more modifiable references, said trace describing nodes where data is created and consumed, and where different control and data-flow dependencies leading to said nodes are created and consumed;</claim-text>
<claim-text>exposing said dependencies through sequence and split primitives in said binary tree;</claim-text>
<claim-text>generating two total orderings based on the binary tree, wherein the two total orderings include a first total ordering comprising a depth-first ordering and a second total Hebrew ordering;</claim-text>
<claim-text>sequentially comparing the order of the nodes of the two total orderings to determine at least one exposed dependency, wherein an exposed dependency is when a first node precedes a second node in both total; and</claim-text>
<claim-text>using the exposed dependencies to identify computations of nodes to parallelize.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The evaluation method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein said one or more modifiable references are unpopulated or contain only ready-to-read values or values resulting from one or more previously assigned tasks immediately prior to said populating.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The evaluation method of <claim-ref idref="CLM-00002">claim 2</claim-ref> further comprising;
<claim-text>re-populating one or more of said one or more modifiable references with continuing task information; and</claim-text>
<claim-text>providing continuing task results on an as-needed basis.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The evaluation method of <claim-ref idref="CLM-00003">claim 3</claim-ref> wherein said idle resources are taken from the group including processors and hardware threads.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The evaluation method of <claim-ref idref="CLM-00004">claim 4</claim-ref> wherein said one or more modifiable references comprise signatures based on destination-passing-style methods.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The evaluation method of <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising:
<claim-text>parallelizing said computations.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. An article comprising an adaptive computer program stored on non-transitory storage medium that if executed enables a system to:
<claim-text>populating one or more modifiable references with task information;</claim-text>
<claim-text>employing idle resources to cooperate with said one or more modifiable references and process assigned tasks in a background mode using said task information;</claim-text>
<claim-text>providing task results on an as-needed basis;</claim-text>
<claim-text>tracking system state changes by said one or more modifiable references;</claim-text>
<claim-text>generating an execution trace as a binary tree of said one or more modifiable references, said trace describing nodes where data is created and consumed, and where different control and data-flow dependencies leading to said nodes are created and consumed;</claim-text>
<claim-text>exposing said dependencies through sequence and split primitives in said binary tree;</claim-text>
<claim-text>generating two total orderings based on the binary tree, wherein the two total orderings include a first total ordering comprising a depth-first ordering and a second total Hebrew ordering;</claim-text>
<claim-text>sequentially comparing the order of the nodes of the two total orderings to determine at least one exposed dependency, wherein an exposed dependency is when a first node precedes a second node in both total; and</claim-text>
<claim-text>using the exposed dependencies to identify computations of nodes to parallelize.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The article of <claim-ref idref="CLM-00007">claim 7</claim-ref> wherein said one or more modifiable references are unpopulated or contain only ready-to-read values or values resulting from one or more previously assigned tasks immediately prior to said populating.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The article of <claim-ref idref="CLM-00008">claim 8</claim-ref> wherein said computer program further enables said system to;
<claim-text>re-populate one or more of said one or more modifiable references with continuing task information; and</claim-text>
<claim-text>provide continuing task results on an as-needed basis.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The article of <claim-ref idref="CLM-00009">claim 9</claim-ref> wherein said idle resources are taken from the group including processors and hardware threads.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The article of <claim-ref idref="CLM-00010">claim 10</claim-ref> wherein said one or more modifiable references comprise signatures based on destination-passing-style methods. </claim-text>
</claim>
</claims>
</us-patent-grant>
