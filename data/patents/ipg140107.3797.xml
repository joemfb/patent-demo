<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08624863-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08624863</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13605351</doc-number>
<date>20120906</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<us-term-of-grant>
<disclaimer>
<text>This patent is subject to a terminal disclaimer.</text>
</disclaimer>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>3</main-group>
<subgroup>041</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>345173</main-classification>
<further-classification>345179</further-classification>
<further-classification>178 1801</further-classification>
</classification-national>
<invention-title id="d2e51">Touch driven method and apparatus to integrate and display multiple image layers forming alternate depictions of same subject matter</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>3478220</doc-number>
<kind>A</kind>
<name>Milroy</name>
<date>19691100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>3673327</doc-number>
<kind>A</kind>
<name>Johnson</name>
<date>19720600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>3764813</doc-number>
<kind>A</kind>
<name>Clement</name>
<date>19731000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>3775560</doc-number>
<kind>A</kind>
<name>Ebeling</name>
<date>19731100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>3860754</doc-number>
<kind>A</kind>
<name>Johnson</name>
<date>19750100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>4144449</doc-number>
<kind>A</kind>
<name>Funk</name>
<date>19790300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>4245634</doc-number>
<kind>A</kind>
<name>Albisser</name>
<date>19810100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>4247767</doc-number>
<kind>A</kind>
<name>O'Brien</name>
<date>19810100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>4463380</doc-number>
<kind>A</kind>
<name>Hooks, Jr.</name>
<date>19840700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>4507557</doc-number>
<kind>A</kind>
<name>Tsikos</name>
<date>19850300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>4517559</doc-number>
<kind>A</kind>
<name>Deitch</name>
<date>19850500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>4527240</doc-number>
<kind>A</kind>
<name>Kvitash</name>
<date>19850700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>4722053</doc-number>
<kind>A</kind>
<name>Dubno</name>
<date>19880100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>4742221</doc-number>
<kind>A</kind>
<name>Sasaki</name>
<date>19880500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>4746770</doc-number>
<kind>A</kind>
<name>McAvinney</name>
<date>19880500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>4782328</doc-number>
<kind>A</kind>
<name>Denlinger</name>
<date>19881100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>5105186</doc-number>
<kind>A</kind>
<name>May</name>
<date>19920400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>5239373</doc-number>
<kind>A</kind>
<name>Tang et al.</name>
<date>19930800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>5379238</doc-number>
<kind>A</kind>
<name>Stark</name>
<date>19950100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>5436639</doc-number>
<kind>A</kind>
<name>Arai et al.</name>
<date>19950700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>5448263</doc-number>
<kind>A</kind>
<name>Martin</name>
<date>19950900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>5483261</doc-number>
<kind>A</kind>
<name>Yasutake</name>
<date>19960100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>5512826</doc-number>
<kind>A</kind>
<name>Hardy et al.</name>
<date>19960400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>5528263</doc-number>
<kind>A</kind>
<name>Platzker</name>
<date>19960600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>5971922</doc-number>
<kind>A</kind>
<name>Arita et al.</name>
<date>19991000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>5982352</doc-number>
<kind>A</kind>
<name>Pryor</name>
<date>19991100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>6008798</doc-number>
<kind>A</kind>
<name>Mato, Jr.</name>
<date>19991200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>6057845</doc-number>
<kind>A</kind>
<name>Dupouy</name>
<date>20000500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>6141000</doc-number>
<kind>A</kind>
<name>Martin</name>
<date>20001000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>6215477</doc-number>
<kind>B1</kind>
<name>Morrison</name>
<date>20010400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>US</country>
<doc-number>6232957</doc-number>
<kind>B1</kind>
<name>Hinckley</name>
<date>20010500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>US</country>
<doc-number>6240306</doc-number>
<kind>B1</kind>
<name>Rohrscheib et al.</name>
<date>20010500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00033">
<document-id>
<country>US</country>
<doc-number>6280381</doc-number>
<kind>B1</kind>
<name>Malin et al.</name>
<date>20010800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00034">
<document-id>
<country>US</country>
<doc-number>6297838</doc-number>
<kind>B1</kind>
<name>Chang et al.</name>
<date>20011000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00035">
<document-id>
<country>US</country>
<doc-number>6309884</doc-number>
<kind>B1</kind>
<name>Cooper et al.</name>
<date>20011000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00036">
<document-id>
<country>US</country>
<doc-number>6333753</doc-number>
<kind>B1</kind>
<name>Hinckley</name>
<date>20011200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00037">
<document-id>
<country>US</country>
<doc-number>6335722</doc-number>
<kind>B1</kind>
<name>Tani et al.</name>
<date>20020100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00038">
<document-id>
<country>US</country>
<doc-number>6335724</doc-number>
<kind>B1</kind>
<name>Takekawa</name>
<date>20020100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00039">
<document-id>
<country>US</country>
<doc-number>6337681</doc-number>
<kind>B1</kind>
<name>Martin</name>
<date>20020100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00040">
<document-id>
<country>US</country>
<doc-number>6352351</doc-number>
<kind>B1</kind>
<name>Ogasahara</name>
<date>20020300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00041">
<document-id>
<country>US</country>
<doc-number>6379301</doc-number>
<kind>B1</kind>
<name>Worthington et al.</name>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00042">
<document-id>
<country>US</country>
<doc-number>6384809</doc-number>
<kind>B1</kind>
<name>Smith</name>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00043">
<document-id>
<country>US</country>
<doc-number>6414671</doc-number>
<kind>B1</kind>
<name>Gillespie</name>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00044">
<document-id>
<country>US</country>
<doc-number>6415167</doc-number>
<kind>B1</kind>
<name>Blank et al.</name>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00045">
<document-id>
<country>US</country>
<doc-number>6421042</doc-number>
<kind>B1</kind>
<name>Omura</name>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00046">
<document-id>
<country>US</country>
<doc-number>6429856</doc-number>
<kind>B1</kind>
<name>Omura</name>
<date>20020800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00047">
<document-id>
<country>US</country>
<doc-number>6442578</doc-number>
<kind>B1</kind>
<name>Forcier</name>
<date>20020800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00048">
<document-id>
<country>US</country>
<doc-number>6487429</doc-number>
<kind>B2</kind>
<name>Hockersmith et al.</name>
<date>20021100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00049">
<document-id>
<country>US</country>
<doc-number>6504532</doc-number>
<kind>B1</kind>
<name>Ogasahara</name>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00050">
<document-id>
<country>US</country>
<doc-number>6512936</doc-number>
<kind>B1</kind>
<name>Monfre et al.</name>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00051">
<document-id>
<country>US</country>
<doc-number>6518959</doc-number>
<kind>B1</kind>
<name>Ito</name>
<date>20030200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00052">
<document-id>
<country>US</country>
<doc-number>6528809</doc-number>
<kind>B1</kind>
<name>Thomas et al.</name>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00053">
<document-id>
<country>US</country>
<doc-number>6531999</doc-number>
<kind>B1</kind>
<name>Trajkovic</name>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00054">
<document-id>
<country>US</country>
<doc-number>6532006</doc-number>
<kind>B1</kind>
<name>Takekawa</name>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00055">
<document-id>
<country>US</country>
<doc-number>6563491</doc-number>
<kind>B1</kind>
<name>Omura</name>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00056">
<document-id>
<country>US</country>
<doc-number>6594023</doc-number>
<kind>B1</kind>
<name>Omura</name>
<date>20030700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00057">
<document-id>
<country>US</country>
<doc-number>6608619</doc-number>
<kind>B2</kind>
<name>Omura et al.</name>
<date>20030800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00058">
<document-id>
<country>US</country>
<doc-number>6636635</doc-number>
<kind>B2</kind>
<name>Matsugu</name>
<date>20031000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00059">
<document-id>
<country>US</country>
<doc-number>6651061</doc-number>
<kind>B2</kind>
<name>Hara et al.</name>
<date>20031100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00060">
<document-id>
<country>US</country>
<doc-number>6654007</doc-number>
<kind>B2</kind>
<name>Ito</name>
<date>20031100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00061">
<document-id>
<country>US</country>
<doc-number>6654620</doc-number>
<kind>B2</kind>
<name>Wu et al.</name>
<date>20031100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00062">
<document-id>
<country>US</country>
<doc-number>6675030</doc-number>
<kind>B2</kind>
<name>Ciurczak et al.</name>
<date>20040100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00063">
<document-id>
<country>US</country>
<doc-number>6723929</doc-number>
<kind>B2</kind>
<name>Kent</name>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00064">
<document-id>
<country>US</country>
<doc-number>6747636</doc-number>
<kind>B2</kind>
<name>Martin</name>
<date>20040600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00065">
<document-id>
<country>US</country>
<doc-number>6764185</doc-number>
<kind>B1</kind>
<name>Beardsley</name>
<date>20040700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00066">
<document-id>
<country>US</country>
<doc-number>6765558</doc-number>
<kind>B1</kind>
<name>Dotson</name>
<date>20040700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00067">
<document-id>
<country>US</country>
<doc-number>6788297</doc-number>
<kind>B2</kind>
<name>Itoh et al.</name>
<date>20040900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00068">
<document-id>
<country>US</country>
<doc-number>6791700</doc-number>
<kind>B2</kind>
<name>Omura</name>
<date>20040900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00069">
<document-id>
<country>US</country>
<doc-number>6803906</doc-number>
<kind>B1</kind>
<name>Morrison</name>
<date>20041000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00070">
<document-id>
<country>US</country>
<doc-number>6810351</doc-number>
<kind>B2</kind>
<name>Katsurahira</name>
<date>20041000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00071">
<document-id>
<country>US</country>
<doc-number>6825890</doc-number>
<kind>B2</kind>
<name>Matsufusa</name>
<date>20041100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00072">
<document-id>
<country>US</country>
<doc-number>6828959</doc-number>
<kind>B2</kind>
<name>Takekawa</name>
<date>20041200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00073">
<document-id>
<country>US</country>
<doc-number>6885883</doc-number>
<kind>B2</kind>
<name>Parris et al.</name>
<date>20050400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00074">
<document-id>
<country>US</country>
<doc-number>6888536</doc-number>
<kind>B2</kind>
<name>Westerman</name>
<date>20050500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00075">
<document-id>
<country>US</country>
<doc-number>6922642</doc-number>
<kind>B2</kind>
<name>Sullivan</name>
<date>20050700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00076">
<document-id>
<country>US</country>
<doc-number>6998247</doc-number>
<kind>B2</kind>
<name>Monfre et al.</name>
<date>20060200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00077">
<document-id>
<country>US</country>
<doc-number>6999061</doc-number>
<kind>B2</kind>
<name>Hara et al.</name>
<date>20060200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00078">
<document-id>
<country>US</country>
<doc-number>7339580</doc-number>
<kind>B2</kind>
<name>Westerman et al.</name>
<date>20080300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00079">
<document-id>
<country>US</country>
<doc-number>7342574</doc-number>
<kind>B1</kind>
<name>Fujioka</name>
<date>20080300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00080">
<document-id>
<country>US</country>
<doc-number>7411575</doc-number>
<kind>B2</kind>
<name>Hill et al.</name>
<date>20080800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00081">
<document-id>
<country>US</country>
<doc-number>7474296</doc-number>
<kind>B2</kind>
<name>Obermeyer et al.</name>
<date>20090100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00082">
<document-id>
<country>US</country>
<doc-number>7519223</doc-number>
<kind>B2</kind>
<name>Dehlin et al.</name>
<date>20090400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00083">
<document-id>
<country>US</country>
<doc-number>7719523</doc-number>
<kind>B2</kind>
<name>Hillis</name>
<date>20100500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00084">
<document-id>
<country>US</country>
<doc-number>7724242</doc-number>
<kind>B2</kind>
<name>Hillis</name>
<date>20100500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00085">
<document-id>
<country>US</country>
<doc-number>7728821</doc-number>
<kind>B2</kind>
<name>Hillis et al.</name>
<date>20100600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00086">
<document-id>
<country>US</country>
<doc-number>7743348</doc-number>
<kind>B2</kind>
<name>Robbins et al.</name>
<date>20100600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00087">
<document-id>
<country>US</country>
<doc-number>8072439</doc-number>
<kind>B2</kind>
<name>Hillis et al.</name>
<date>20111200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00088">
<document-id>
<country>US</country>
<doc-number>8139043</doc-number>
<kind>B2</kind>
<name>Hillis</name>
<date>20120300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00089">
<document-id>
<country>US</country>
<doc-number>8188985</doc-number>
<kind>B2</kind>
<name>Hillis et al.</name>
<date>20120500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00090">
<document-id>
<country>US</country>
<doc-number>8269739</doc-number>
<kind>B2</kind>
<name>Hillis et al.</name>
<date>20120900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345173</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00091">
<document-id>
<country>US</country>
<doc-number>2001/0016682</doc-number>
<kind>A1</kind>
<name>Berner et al.</name>
<date>20010800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00092">
<document-id>
<country>US</country>
<doc-number>2001/0019325</doc-number>
<kind>A1</kind>
<name>Takekawa</name>
<date>20010900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00093">
<document-id>
<country>US</country>
<doc-number>2001/0022579</doc-number>
<kind>A1</kind>
<name>Hirabayashi</name>
<date>20010900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00094">
<document-id>
<country>US</country>
<doc-number>2001/0026268</doc-number>
<kind>A1</kind>
<name>Ito</name>
<date>20011000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00095">
<document-id>
<country>US</country>
<doc-number>2002/0019022</doc-number>
<kind>A1</kind>
<name>Dunn et al.</name>
<date>20020200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00096">
<document-id>
<country>US</country>
<doc-number>2002/0036617</doc-number>
<kind>A1</kind>
<name>Pryor</name>
<date>20020300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00097">
<document-id>
<country>US</country>
<doc-number>2002/0132279</doc-number>
<kind>A1</kind>
<name>Hockersmith</name>
<date>20020900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00098">
<document-id>
<country>US</country>
<doc-number>2002/0185981</doc-number>
<kind>A1</kind>
<name>Dietz et al.</name>
<date>20021200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00099">
<document-id>
<country>US</country>
<doc-number>2003/0001825</doc-number>
<kind>A1</kind>
<name>Omura et al.</name>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00100">
<document-id>
<country>US</country>
<doc-number>2003/0063775</doc-number>
<kind>A1</kind>
<name>Rafii et al.</name>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00101">
<document-id>
<country>US</country>
<doc-number>2003/0137494</doc-number>
<kind>A1</kind>
<name>Tulbert</name>
<date>20030700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00102">
<document-id>
<country>US</country>
<doc-number>2003/0214481</doc-number>
<kind>A1</kind>
<name>Xiong</name>
<date>20031100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00103">
<document-id>
<country>US</country>
<doc-number>2003/0231167</doc-number>
<kind>A1</kind>
<name>Leung</name>
<date>20031200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00104">
<document-id>
<country>US</country>
<doc-number>2004/0033618</doc-number>
<kind>A1</kind>
<name>Haass et al.</name>
<date>20040200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00105">
<document-id>
<country>US</country>
<doc-number>2004/0046744</doc-number>
<kind>A1</kind>
<name>Rafii et al.</name>
<date>20040300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00106">
<document-id>
<country>US</country>
<doc-number>2004/0106163</doc-number>
<kind>A1</kind>
<name>Workman et al.</name>
<date>20040600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00107">
<document-id>
<country>US</country>
<doc-number>2005/0038674</doc-number>
<kind>A1</kind>
<name>Braig et al.</name>
<date>20050200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00108">
<document-id>
<country>US</country>
<doc-number>2005/0052427</doc-number>
<kind>A1</kind>
<name>Wu et al.</name>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00109">
<document-id>
<country>US</country>
<doc-number>2005/0106651</doc-number>
<kind>A1</kind>
<name>Chaiken et al.</name>
<date>20050500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00110">
<document-id>
<country>US</country>
<doc-number>2006/0010400</doc-number>
<kind>A1</kind>
<name>Dehlin et al.</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00111">
<document-id>
<country>US</country>
<doc-number>2006/0022955</doc-number>
<kind>A1</kind>
<name>Kennedy</name>
<date>20060200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00112">
<document-id>
<country>US</country>
<doc-number>2006/0026521</doc-number>
<kind>A1</kind>
<name>Hotelling</name>
<date>20060200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00113">
<document-id>
<country>US</country>
<doc-number>2006/0026536</doc-number>
<kind>A1</kind>
<name>Hotelling et al.</name>
<date>20060200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00114">
<document-id>
<country>US</country>
<doc-number>2006/0031786</doc-number>
<kind>A1</kind>
<name>Hillis</name>
<date>20060200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00115">
<document-id>
<country>US</country>
<doc-number>2006/0063218</doc-number>
<kind>A1</kind>
<name>Bartkowiak et al.</name>
<date>20060300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00116">
<document-id>
<country>US</country>
<doc-number>2007/0252821</doc-number>
<kind>A1</kind>
<name>Hollemans et al.</name>
<date>20071100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00117">
<document-id>
<country>US</country>
<doc-number>2007/0268273</doc-number>
<kind>A1</kind>
<name>Westerman et al.</name>
<date>20071100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00118">
<document-id>
<country>US</country>
<doc-number>2008/0211785</doc-number>
<kind>A1</kind>
<name>Hotelling et al.</name>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00119">
<document-id>
<country>US</country>
<doc-number>2010/0234638</doc-number>
<kind>A1</kind>
<name>Fitzpatrick</name>
<date>20100900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00120">
<document-id>
<country>US</country>
<doc-number>2012/0206393</doc-number>
<kind>A1</kind>
<name>Hillis et al.</name>
<date>20120800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00121">
<document-id>
<country>US</country>
<doc-number>2012/0223971</doc-number>
<kind>A1</kind>
<name>Hillis</name>
<date>20120900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00122">
<document-id>
<country>US</country>
<doc-number>2013/0082967</doc-number>
<kind>A1</kind>
<name>Hillis et al.</name>
<date>20130400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00123">
<document-id>
<country>EP</country>
<doc-number>0881592</doc-number>
<kind>B2</kind>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00124">
<document-id>
<country>EP</country>
<doc-number>0881591</doc-number>
<kind>B1</kind>
<date>20030900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00125">
<document-id>
<country>JP</country>
<doc-number>2001175807</doc-number>
<date>20010600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00126">
<document-id>
<country>WO</country>
<doc-number>0216905</doc-number>
<kind>A2</kind>
<date>20020200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00127">
<othercit>&#x201c;Smart Board for Flat Panel Displays (interactive Overlay)&#x201d;, Smart Technologies Inc., Nov. 2003, 2 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00128">
<othercit>Essenther, et al., &#x201c;Diamond Touch Applications&#x201d;, NCSA (Nat'l Center for Supercomputing Applications), Johnson and Curtis, Univ. of Illinois, Diamond Touch for Multi-User Multi-Touch Applications&#x2014;ACM Conference on Computer Supported Cooperative Work, Nov. 2002 (CSCW 2002, TR2002-048).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00129">
<othercit>Johnston, Douglas M. et al., &#x201c;GM Slab Function&#x201d;, NCSA (National Center for Supercomputing Applications), Univ. of Illinois, Jul. 10, 2003, 5 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00130">
<othercit>Paradiso, Joe et al., &#x201c;The Laser Wall&#x201d;, MIT Media Lab, Retrived from website: http://web.media.mit.edu/&#x2dc;joep/SpectrumWeb/captions/Laser.html, Sep. 1997, 3 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00131">
<othercit>Rekimoto, Jun , &#x201c;SmartSkin: An Infrastructure for Freehand Manipulation on Interactive Surfaces&#x201d;, Interaction Laboratory Sony Computer Science Laboratories, Inc., CHI 2002; Retrieved from website: http://www.csl.sony.co.jp/person/rekimoto/smartskin/, Apr. 2002, 1-11.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00132">
<othercit>Ryall, K et al., &#x201c;Diamond Touch Applications&#x201d;, Mitsubishi Electric Research Laboratories, Aug. 2, 2004, 2 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00133">
<othercit>Small, Charles H. , &#x201c;Touchscreens Provide a Robust and Intuitive User Interface&#x201d;, TechOnline, Retrived from website: www.techonline.com/showArticle.jhtml?articleID=192200401&#x26;queryText=touch+screen, May 24, 2002, pp. 1-6.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00134">
<othercit>Wu, et al., &#x201c;Gesture Registration, Relaxation, and Reuse for Multi-Point Direct-Touch Surfaces&#x201d;, Proceedings of IEEE International Workshop on Horizontal Interactive Human-Computer Systems, Adelaide, South Australia, Jan. 2006, 8 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00135">
<othercit>Wu, Mike et al., &#x201c;Multi-Finger and Whole Hand Gestural Interaction Techniques for Multi-User Tabletop Displays&#x201d;, ACM, CHI Letters, vol. 5 No. 2; ACM UIST 2003 Symposium on User interface Software &#x26; Technology; Vancouver, Canada;, Nov. 2003, pp. 193-202.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00136">
<othercit>Bennion, N. et al., &#x201c;Alternate Site Glucose Testing: A Crossover Design,&#x201d; Diabetes Technology &#x26; Therapeutics, 2002, vol. 4 (1), pp. 25-33.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00137">
<othercit>Blank T.B., et al., &#x201c;Clinical Results from a Noninvasive Blood Glucose Monitor,&#x201d; Proceedings of SPIE, Optical Diagnostics and Sensing of Biological Fluids and Glucose and Cholesterol Monitoring II, May 23, 2002, vol. 4624. pp. 1-10.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00138">
<othercit>&#x201c;Diabetes Statistics,&#x201d; Nov. 1997, Publication No. 98-3926, National Institutes of Health, Bethesda.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00139">
<othercit>Esenther, et al., &#x201c;Diamond Touch SDK: Support for Multi-User, Multi-Touch Applications,&#x201d; Mitsubishi Electric Research Laboratories, TR2002-048, Nov. 2002, 5 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00140">
<othercit>Fischer J.S., et al., &#x201c;Comparisons of Capillary Blood Glucose Concentrations from the Fingertips and the Volar Aspects of the Left and Right Forearm&#x201d;, Instrumentations Metrics, Inc., American Diabetes Association, 62 Annual Meeting, Jun. 14, 2002.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00141">
<othercit>&#x201c;Functionality: Introduction, Functionality: Console Display, NCSA-GMSlab Team,&#x201d; NCSA(National Center for Supercomputing Applications), Mississippi River Web Museum Consortium, University of Illinois, 2003.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00142">
<othercit>Hazen K.H., et al., &#x201c;Glucose Determination in Biological Matrices Using Near&#x2014;Infrared Spectroscopy,&#x201d; 1995, Doctoral Dissertation, University of Iowa.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00143">
<othercit>Jungheim K., et al., &#x201c;Glucose Monitoring at the Arm,&#x201d; Diabetes Care, Jun. 2002, vol. 25 (6), pp. 956-960.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00144">
<othercit>Jungheim K., et al., &#x201c;Risky Delay of Hypoglycemia Detection by Glucose Monitoring at the Arm,&#x201d; Diabetes Care, Jul. 2001, vol. 24 (7), pp. 1303-1304.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00145">
<othercit>Khalil 0.S., &#x201c;Spectroscopic and Clinical Aspects of Noninvasive Glucose Measurements,&#x201d; Clinical Chemistry, 1999, vol. 45 (2), pp. 165-177.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00146">
<othercit>Klonoff D.C., &#x201c;Noninvasive Blood Glucose Monitoring,&#x201d; Mar 1997, Diabetes Care, vol. 20 (3), pp. 433-437.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00147">
<othercit>Lee D.M., et al., &#x201c;A Study of Forearm Versus Finger Stick Glucose Monitoring,&#x201d; Diabetes Technology &#x26; Therapeutics, 2001, vol. 4 (1), pp. 13-23.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00148">
<othercit>Malin S.F., et al., &#x201c;Noninvasive Prediction of Glucose by Near Infrared Spectroscopy&#x201d;, Clinical Chemistry, 1999, vol. 45 (9), pp. 1651-1658.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00149">
<othercit>Matt W., et al., &#x201c;Alternative Site: Fingertip vs. Forearm&#x201d;, Instrumentation Metrics, Inc., Internal Report, Dec. 12, 2001.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00150">
<othercit>McGARRAUGH G., et al., &#x201c;Glucose Measurements Using Blood Extracted from the Forearm and Finger,&#x201d; TheraSense, Inc., ART010022 Rev. C, 2001.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00151">
<othercit>McGARRAUGH G., et al., &#x201c;Physiological Influences on Off-Finger Glucose Testing,&#x201d; Diabetes Technology &#x26; Therapeutics, 2001, vol. 3 (3), pp. 367-376.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00152">
<othercit>McGARRAUGH, G., &#x201c;Response to Jungheim and Koschinsky,&#x201d; Comments and Responses, Diabetes Care, Jul. 2001, vol. 24 (7), pp. 1304-1306.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00153">
<othercit>Monfre S.L., et al., &#x201c;Physiologic Differences between Volar and Dorsal Capillary Forearm Glucose Concentrations and Finger Stick Concentrations in Diabetics&#x201d;, Instrumentations Metrics, Inc., American Diabetes Association, 62 Annual Meeting, Jun. 14, 2002.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00154">
<othercit>National Center for Supercomputing Applications (NCSA) Overview; Mississippi RiverWeb Museum Consortium; Jun. 13, 2005; Johnson and Curtis Univ. Of Illinois.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00155">
<othercit>NCSA (National Center for Supercomputing Applications); GM Slab Console; Museum Consortium; Jul. 10, 2003, Jhonson and Curtis University of Illinois.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00156">
<othercit>NCSA (National Center for Supercomputing Applications); GM Slab Function; Museum Consortium; Jul. 10, 2003, Jhonson and Curtis University of Illinois.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00157">
<othercit>Peled N., et al., &#x201c;Comparison of Glucose Levels in Capillary Blood Samples from a Variety of Body Sites&#x201d;, Diabetes Technology &#x26; Therapeutics, 2002, vol. 4 (1), pp. 35-44.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00158">
<othercit>Peripheral Circulation, Johnson P.C. Ed., New York, 1978, pp. 198.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00159">
<othercit>Ryan T.J., &#x201c;A Study of the Epidermal Capillary Unit in Psoriasis&#x201d;, Dermatologica, 1969, vol. 138, pp. 459-472.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00160">
<othercit>Small C., &#x201c;Touchscreens Provide a Robust and Intuitive User Interface,&#x201d; TechOnline, 1996- 2005.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00161">
<othercit>Sparks H.V., &#x201c;Skin and Muscle&#x201d;, Peripheral Circulation, New York, 1978, pp. 193-230.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00162">
<othercit>Summary Minutes of the Clinical Chemistry and Clinical Toxicology Devices Meeting, Oct. 29, 2001.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00163">
<othercit>Szuts, Ete Z., et al., &#x201c;Blood Glucose Concentrations of Arm and Finger During Dynamic Glucose Conditions&#x201d;, Diabetes Technology &#x26; Therapeutics, 2002, vol. 4 (1), pp. 3-11.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00164">
<othercit>Tamada J.A., et al., &#x201c;Noninvasive Glucose Monitoring Comprehensive Clinical Results&#x201d;, Journal of the American Medical Association, 1999, vol. 282 (19), pp. 1839-1844.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00165">
<othercit>The Diabetes Control and Complications Trial Research Group, &#x201c;The Effect of Intensive Treatment of Diabetes on the Development and Progression of Long-Term Complication in Insulin-Dependent Diabetes Mellitus&#x201d;, The New England Journal of Medicine, 1993, vol. 329 (14), pp. 997-986.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00166">
<othercit>U.K. Prospective Diabetes Study (UKPDS) Group, &#x201c;Intensive Blood-Glucose Control with Sulphonylureas or Insulin Compared with Conventional Treatment and Risk of Complications in Patients with Type 2 Ddiabetes (UKPDS 33),&#x201d; The Lancet, vol. 352, Sep. 12, 1998, pp. 837-853.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00167">
<othercit>Wu M., et al., &#x201c;Gesture Registration, Relation, and Reuse for Multi-Point Direct-Touch Surfaces,&#x201d; in Proceedings of IEEE Tabletop 2006 Conference on Horizontal Interactive Human- Computer Systems, Adelaide, South Australia, 2003, 8 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00168">
<othercit>Yasuo O., et al., &#x201c;Intensive Insulin Therapy Prevents the Progression of Diabetic Microvascular Complications in Japanese Patients with Non-Insulin-Dependent Diabetes Mellitus: A Randomized Prospective 6-year Study,&#x201d; Diabetes Research and Clinical Practice, 1995, vol. 28, pp. 103-117.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00169">
<othercit>Zheng P., et al., &#x201c;Noninvasive Glucose Determination by Oscillating Thermal Gradient Spectrometry,&#x201d; Diabetes Technology &#x26; Therapeutics, 2000, vol. 2 (1), pp. 17-25.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00170">
<othercit>Zlatko T., et al., &#x201c;Open-Flow Microperfusion of Subcutaneous Adipose Tissue for On-Line Continuous Ex Vivo Measurement of Glucose Concentration,&#x201d; Diabetes Care, Jul. 1997, vol. 20 (7), pp. 1114-1120.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00171">
<othercit>Zlatko T., et al., &#x201c;Portable Device for Continuous Fractionated Blood Sampling and Continuous Ex Vivo Blood Glucose Monitoring&#x201d;, Biosensors &#x26; Bioelectronics, 1996, vol. 11 (5), pp. 479-487.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>17</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>345156-158</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345173-179</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>178 1801- 1804</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>178 1901- 1903</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>715863</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>12</number-of-drawing-sheets>
<number-of-figures>14</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>12582611</doc-number>
<date>20091020</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>8269739</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13605351</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>11286232</doc-number>
<date>20051123</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>7724242</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>12582611</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation-in-part>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>10913105</doc-number>
<date>20040806</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>7728821</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>11286232</doc-number>
</document-id>
</child-doc>
</relation>
</continuation-in-part>
<continuation-in-part>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>11188186</doc-number>
<date>20050722</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>7907124</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>10913105</doc-number>
</document-id>
</child-doc>
</relation>
</continuation-in-part>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60701892</doc-number>
<date>20050722</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120331415</doc-number>
<kind>A1</kind>
<date>20121227</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Hillis</last-name>
<first-name>W. Daniel</first-name>
<address>
<city>Encino</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Ferren</last-name>
<first-name>Bran</first-name>
<address>
<city>Beverly Hills</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Hillis</last-name>
<first-name>W. Daniel</first-name>
<address>
<city>Encino</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Ferren</last-name>
<first-name>Bran</first-name>
<address>
<city>Beverly Hills</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<last-name>Johnson</last-name>
<first-name>Michael</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>QUALCOMM Incorporated</orgname>
<role>02</role>
<address>
<city>San Diego</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Nguyen</last-name>
<first-name>Kimnhung</first-name>
<department>2693</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">An interactive display system, including a touch sensitive display, establishes a first image and at least one secondary image, each image representing various spatial coordinates, the spatial coordinates overlapping at least in part such that each image comprises an alternate depiction of subject matter common to all of the images. The first image is presented upon the display. Responsive to user input including contact with the display, imagery presented by the display is updated to integrate a region of at least one of the secondary images into the display. Each integrated region has substantially identical represented coordinates as a counterpart region of the first image. Further, each integrated region is presented in same scale and display location as the counterpart region of the first image.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="144.27mm" wi="177.97mm" file="US08624863-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="235.54mm" wi="185.17mm" file="US08624863-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="173.82mm" wi="177.38mm" file="US08624863-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="177.63mm" wi="173.65mm" file="US08624863-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="218.52mm" wi="163.07mm" file="US08624863-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="230.63mm" wi="199.64mm" file="US08624863-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="178.90mm" wi="148.67mm" file="US08624863-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="236.22mm" wi="170.35mm" file="US08624863-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="122.00mm" wi="132.25mm" file="US08624863-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="239.86mm" wi="187.79mm" file="US08624863-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="126.75mm" wi="147.91mm" file="US08624863-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="229.02mm" wi="175.60mm" file="US08624863-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="119.46mm" wi="128.69mm" file="US08624863-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application is a continuation of the following application, and claims the benefit thereof in accordance with 35 USC 120: U.S. application Ser. No. 12/582,611, filed on Oct. 20, 2009 now U.S. Pat. No. 8,269,739, which is a continuation of U.S. application Ser. No. 11/286,232, filed on Nov. 23, 2005 now U.S. Pat. No. 7,724,242. The '232 application is a continuation-in-part of the following earlier filed, applications and claims the benefit thereof in accordance with 35 USC 120: U.S. patent application Ser. No. 10/913,105, filed on Aug. 6, 2004 now U.S. Pat. No. 7,728,821; U.S. patent application Ser. No. 11/188,186, filed on Jul. 22, 2005 now U.S. Pat. No. 7,907,124. The '232 application also claimed the benefit under 35 USC 120 of the following application: U.S. Provisional Application No. 60/701,892, filed on Jul. 22, 2005. The entirety of each aforementioned application is hereby incorporated herein by this reference thereto.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">1. Field of the Invention</p>
<p id="p-0004" num="0003">The present invention relates to interactive display systems whose presentation is controlled through user performed touch. More particularly, the invention concerns various embodiments of method, apparatus, signal-bearing medium, and logic circuitry used in implementing an interactive display system that responds to user touch to selectively integrate different layers of imagery comprising alternate depictions of same subject matter.</p>
<p id="p-0005" num="0004">2. Description of the Related Art</p>
<p id="p-0006" num="0005">In many cases, a situation arises calling for user review of several alternate depictions of the same subject matter. For example, a city planner may seek to review a satellite photograph in conjunction with a graphical map depicting the same region. In another example, an architect may be interested in reviewing and correlating different floor plans of the same section of building. Similarly, a circuit designer may be motivated to gain understanding into the interrelationship between different layers of a multi-layer integrated circuit. Although the focus in each case is the same subject matter, there differing depictions that contrast by camera angle, time of view, level of a multi-layer structure, or other parameter.</p>
<p id="p-0007" num="0006">Traditionally, people have reviewed such data in physical form, such as photographs, blueprints, diagrams, and the like. In this case, the reviewer must mentally assimilate alternate depictions of the common subject matter by aligning the depictions side-by-side, shuffling through them, etc. Another traditional vehicle for reviewing such data is the computer. With a computer, the reviewer can change from document to document (shuffling review), or view documents in separate windows (side-by-side review).</p>
<p id="p-0008" num="0007">Although the foregoing approaches will always enjoy some popularity, the present inventors have sought ways to improve the interface between humans and computers.</p>
<heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0009" num="0008">An interactive display system, including a touch sensitive display, establishes a first image and at least one secondary images, each image representing various spatial coordinates, the spatial coordinates overlapping at least in part such that each image comprises an alternate depiction of subject matter common to all of the images. The first image is presented upon the display. Responsive to user input including contact with the display, imagery presented by the display is updated to integrate a region of at least one of the secondary images into the display. Each integrated region has substantially identical represented coordinates as a counterpart region of the first image. Further, each integrated region is presented in same scale and display location as the counterpart region of the first image.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. 1A</figref> is a block diagram of the hardware components and interconnections of an interactive multi-user touch sensitive interactive display system.</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 1B</figref> is a plan view showing several users operating an interactive, touch detecting display.</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 1C</figref> shows a side view of an interactive, touch detecting, tabletop projection display.</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 1D</figref> is a block diagram of a digital data processing machine.</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 1E</figref> shows an exemplary signal-bearing medium.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 1F</figref> shows exemplary logic circuitry.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 2A</figref> is a flowchart of a generalized sequence for operating a multi-user touch sensitive interactive display system.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 2B</figref> is a flowchart of a sequence for operating an interactive touch display system to integrate different layers of imagery comprising alternate depictions of same subject matter.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 3A</figref> shows a flowchart of exemplary operations to operate an interactive touch display system to effectuate a multi-layer fade mode.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 3B</figref> is a diagram showing an example of user participation in a fade mode.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 4A</figref> is a flowchart of exemplary operations to operate an interactive touch display system to effectuate a swipe mode.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 4B</figref> is a diagram showing an example of user participation in a swipe mode.</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 5A</figref> is a flowchart of exemplary operations to operate an interactive touch display system to effectuate a slider mode.</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 5B</figref> is a diagram showing an example of user participation in a slider mode.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0024" num="0023">The nature, objectives, and advantages of the invention will become more apparent to those skilled in the art after considering the following detailed description in connection with the accompanying drawings.</p>
<heading id="h-0006" level="1">Hardware Components &#x26; Interconnections</heading>
<p id="h-0007" num="0000">Overall Structure</p>
<p id="p-0025" num="0024">One aspect of the present disclosure concerns an interactive touch detecting display system, which may be embodied by various hardware components and interconnections, with one example being described in <figref idref="DRAWINGS">FIG. 1A</figref>. The system <b>120</b> includes a table <b>122</b> with a display surface <b>124</b>, computer <b>126</b>, and projector <b>128</b>. The projector <b>128</b> projects imagery upon the display surface <b>124</b> under direction of the computer <b>126</b>. As one example, the system <b>120</b> may be implemented by a touch detecting interactive display as disclosed in U.S. patent application Ser. No. 10/913,105, the entirety of which is incorporated by reference.</p>
<p id="p-0026" num="0025">The table <b>122</b> detects touch input from human users as applied to the display surface <b>124</b>, and provides a representative output to the computer <b>126</b>, indicating the position, size, timing, and other characteristics of the user's touch. Optionally, the table <b>122</b> may also detect applied force. Based upon this information, the computer <b>126</b> identifies one or more user gestures from a predefined set of defined gestures, and further identifies an action associated with each identified gesture. In this respect, the computer <b>126</b> includes a gesture dictionary <b>126</b><i>a</i>, listing of actions <b>126</b><i>b</i>, and mapping <b>126</b><i>c </i>between gestures and actions. The computer <b>126</b> interprets the table <b>122</b>'s output by utilizing the dictionary <b>126</b><i>a </i>to identify the gesture performed by the user. The computer <b>126</b> then carries out appropriate action <b>126</b><i>c </i>corresponding to the user-performed gesture. The actions <b>126</b><i>c </i>comprise, for example, predetermined machine executable operations for updating imagery presented by the display.</p>
<p id="p-0027" num="0026">The presently described embodiment of the system <b>120</b> facilitates user manipulation of the projected imagery as a whole, for example, through operations such as panning, zooming, rotating, and the like. This contrasts with personal computer applications, which utilize numerous separately movable icons. Still, the system <b>120</b> may utilize one or more peripheral menus or other control interfaces to support user manipulation of the subject imagery. Accordingly, the system <b>120</b> is particularly well suited to hands-on, intuitive, collaborative, multi-user study and manipulation of a large unitary item of imagery such as a photograph or map, presented upon the display <b>124</b>.</p>
<p id="p-0028" num="0027">In this respect, <figref idref="DRAWINGS">FIG. 1B</figref> shows several users operating an interactive, touch detecting display <b>11</b>. The users <b>10</b> surround the display <b>11</b>, such that each user can view the display surface <b>12</b>, which shows imagery of interest to the users. For example, the display may present Geographic Information System (GIS) imagery characterized by geographic <b>13</b>, economic <b>14</b>, political <b>15</b>, and other features, organized into one or more imagery layers. Because the users can comfortably surround and view the display, group discussion and interaction with the display is readily facilitated. In the example of <figref idref="DRAWINGS">FIG. 1B</figref>, a user <b>16</b> has gestured by placing his fingertips on the display surface and moving them in an outwardly separating manner. As discussed in greater detail below, this particular gesture <b>17</b> is associated with a zoom-in command. When the computer <b>126</b> performs a zoom-in command, it directs the projector to provide <b>128</b> a closer, more detailed view of the displayed imagery.</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 1C</figref> shows a side view of the components <b>124</b>, <b>128</b>. The display surface is a horizontally oriented, planar projection surface <b>21</b> supported by a table-like structure <b>22</b>. The structure in this example supports the projection surface at waist level for adult users, allowing the users to view and touch the entirety of the projection surface comfortably. The displayed imagery is generated by a projector <b>23</b> located above and projecting <b>24</b> downward onto the projection surface.</p>
<p id="p-0030" num="0029">While projection from above onto a horizontally oriented display is illustrated, this disclosure also contemplates other display surface orientations, projector configurations, and display technologies. For example, a horizontally oriented rear-projection surface may be used as the display surface, with the projector mounted below the display surface and projecting upward. This approach offers the advantage of eliminating the shadows generated in those configurations where a user may position his body between the projector and the projection surface. The display may also be mounted in a vertical orientation and affixed to a wall or other supporting structure. In this nonprojection case, thin profile display technologies may be most appropriate, such as LCDs, OLEDs, or plasma displays, although those skilled in the art will appreciate that many display technologies may be used.</p>
<p id="p-0031" num="0030">A possible consequence of the horizontal orientation of the display surface is a natural inclination of users to rest a hand on the projection surface for support, especially when leaning forward to point to objects near the center of the projection surface. To avoid erroneously interpreting such contact with the display as a gesture, the projection surface may be surrounded by a small railing (not shown). The railing provides a visual cue that discourages users from leaning onto the display, and also provides structural support should the user wish to lean forward towards the center of the display.</p>
<p id="p-0032" num="0031">Referring to <figref idref="DRAWINGS">FIG. 1A</figref>, the table <b>122</b> may employ various approaches to detect of when and where a user touches the display surface. In one embodiment, a set of infrared emitters and receivers (not shown) is arrayed around the perimeter of the display surface <b>124</b>, oriented such that each emitter emits light in a plane a short distance above the display surface. The table <b>122</b> determines the location where the user is touching the projection surface by considering which emitters are and are not occluded as viewed from each of the receivers. A configuration incorporating a substantially continuous set of emitters around the perimeter and three receivers, each positioned in a corner of the projection surface, is particularly effective in resolving multiple locations of contact.</p>
<p id="p-0033" num="0032">As an alternative, the table <b>122</b> may employ a resistive touch pad, such as those commonly used in laptop computers, placed beneath the display surface <b>124</b>, which is flexible. The resistive touch pad comprises two layers of plastic that are separated by a compressible insulator such as air, and a voltage differential is maintained across the separated layers. When the upper layer is touched with sufficient pressure, it is deflected until it contacts the lower layer, changing the resistive characteristics of the upper to lower layer current pathway. By considering these changes in resistive characteristics, the computer <b>126</b> can determine the location of contact.</p>
<p id="p-0034" num="0033">In yet another embodiment, the table <b>122</b> employs a thin layer of liquid crystal film or other material that changes optical properties in response to pressure. The thin layer is placed beneath the display surface <b>124</b>, which is flexible. One or more video cameras trained on the underside of the material capture the changes in optical properties that occur when a user touches the projection surface and therefore applies pressure to the thin layer. The location of contact is then determined by using the computer <b>126</b> to analyze the video camera images.</p>
<p id="p-0035" num="0034">In still another embodiment, the table <b>122</b> employs ultrasound to detect contact information. Capacitive touch pads may also be used, with one example being the Synaptics TouchPad&#x2122; product. A variety of capacitive touch pads are available commercially, and described in various publications. As another example, the display surface <b>124</b> may employ another scheme such as ultrasound, or a combination of any of the foregoing. Furthermore, the table <b>122</b> may employ a combination of some of the foregoing schemes, such as IR together with ultrasound.</p>
<p id="p-0036" num="0035">In any case, the detection scheme employed by the table <b>122</b> periodically provides a machine readable location output signal to the computer <b>126</b>, which in turn analyzes the location information to identify user gestures. Depending upon the implementation, the table output may comprise a raw signal corresponding to the physics of the detection mechanism, or a more refined signal indicative of actual contact position. Thus, the computer <b>126</b> may serve to interpret the table's output to develop a Cartesian or other representation of touch position.</p>
<p id="p-0037" num="0036">As an optional enhancement, the display surface <b>124</b> may be mounted on load cells or other devices that sense force of the user contact on the display surface <b>124</b>. As described in greater detail below, the computer <b>126</b> may employ the detected force to supplement the identification of gestures. One example, illustrated below in greater detail, permits the user to apply force to slow imagery that has been set in motion using simulated inertia. Similarly, the computer <b>126</b> may also use force intensity to determine the gain or attenuation applied to the velocity used to carry out the identified gestures.</p>
<p id="h-0008" num="0000">Exemplary Digital Data Processing Apparatus</p>
<p id="p-0038" num="0037">Data processing entities such as the computer <b>126</b> may be implemented in various forms. One example is a digital data processing apparatus, as exemplified by the hardware components and interconnections of the digital data processing apparatus <b>100</b> of <figref idref="DRAWINGS">FIG. 1D</figref>.</p>
<p id="p-0039" num="0038">The apparatus <b>100</b> includes a processor <b>102</b>, such as a microprocessor, personal computer, workstation, controller, microcontroller, state machine, or other processing machine, coupled to storage <b>104</b>. In the present example, the storage <b>104</b> includes a fast-access storage <b>106</b>, as well as nonvolatile storage <b>108</b>. The fast-access storage <b>106</b> may comprise random access memory (&#x201c;RAM&#x201d;), and may be used to store the programming instructions executed by the processor <b>102</b>. The nonvolatile storage <b>108</b> may comprise, for example, battery backup RAM, EEPROM, flash PROM, one or more magnetic data storage disks such as a hard drive, a tape drive, or any other suitable storage device. The apparatus <b>100</b> also includes an input/output <b>110</b>, such as a line, bus, cable, electromagnetic link, or other means for the processor <b>102</b> to exchange data with other hardware external to the apparatus <b>100</b>.</p>
<p id="p-0040" num="0039">Despite the specific foregoing description, ordinarily skilled artisans (having the benefit of this disclosure) will recognize that the apparatus discussed above may be implemented in a machine of different construction, without departing from the scope of the invention. As a specific example, one of the components <b>106</b>, <b>108</b> may be eliminated; furthermore, the storage <b>104</b>, <b>106</b>, and/or <b>108</b> may be provided on-board the processor <b>102</b>, or even provided externally to the apparatus <b>100</b>.</p>
<p id="h-0009" num="0000">Signal-Bearing Media</p>
<p id="p-0041" num="0040">In contrast to the digital data processing apparatus described above, a different aspect of this disclosure concerns one or more signal-bearing media tangibly embodying a program of machine-readable instructions executable by such a digital processing apparatus. In one example, the machine-readable instructions are executable to carry out various functions related to this disclosure, such as the operations described in greater detail below. In another example, the instructions upon execution serve to install a software program upon a computer, where such software program is independently executable to perform other functions related to this disclosure, such as the operations described below.</p>
<p id="p-0042" num="0041">In any case, the signal-bearing media may take various forms. In the context of <figref idref="DRAWINGS">FIG. 1D</figref>, such a signal-bearing media may comprise, for example, the storage <b>104</b> or another signal-bearing media, such as a magnetic data storage diskette <b>130</b> (<figref idref="DRAWINGS">FIG. 1E</figref>), directly or indirectly accessible by a processor <b>102</b>. Whether contained in the storage <b>106</b>, diskette <b>130</b>, or elsewhere, the instructions may be stored on a variety of machine-readable data storage media. Some examples include direct access storage (e.g., a conventional &#x201c;hard drive&#x201d;, redundant array of inexpensive disks (&#x201c;RAID&#x201d;), or another direct access storage device (&#x201c;DASD&#x201d;)), serial-access storage such as magnetic or optical tape, electronic non-volatile memory (e.g., ROM, EPROM, flash PROM, or EEPROM), battery backup RAM, optical storage (e.g., CD-ROM, WORM, DVD, digital optical tape), or other suitable signal-bearing media including analog or digital transmission media and analog and communication links and wireless communications. In one embodiment, the machine-readable instructions may comprise software object code, compiled from a language such as assembly language, C, etc.</p>
<p id="h-0010" num="0000">Logic Circuitry</p>
<p id="p-0043" num="0042">In contrast to the signal-bearing media and digital data processing apparatus discussed above, a different embodiment of this disclosure uses logic circuitry instead of computer-executed instructions to implement processing entities of the system <b>120</b>.</p>
<p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. 1F</figref> shows exemplary logic circuitry <b>140</b>. Depending upon the particular requirements of the application in the areas of speed, expense, tooling costs, and the like, this logic may be implemented by constructing an application-specific integrated circuit (ASIC) having thousands of tiny integrated transistors. Such an ASIC may be implemented with CMOS, TTL, VLSI, or another suitable construction. Other alternatives include a digital signal processing chip (DSP), discrete circuitry (such as resistors, capacitors, diodes, inductors, and transistors), field programmable gate array (FPGA), programmable logic array (PLA), programmable logic device (PLD), and the like.</p>
<heading id="h-0011" level="1">Operation</heading>
<p id="p-0045" num="0044">Having described the structural features of the present disclosure, the operational aspect of the disclosure will now be described. One operational aspect of the disclosure involves the identification of particular touch-based user gestures from points of contact, velocity, and/or applied force, and implementing of predetermined actions associated with the gestures. A different aspect concerns the operation of an interactive display system that responds to user touch to selectively integrate different layers of imagery comprising alternate depictions of same subject matter.</p>
<p id="p-0046" num="0045">Although the present invention has broad applicability to touch based computing systems, the explanation that follows will emphasize the application of <figref idref="DRAWINGS">FIGS. 1A-1C</figref> in order to tangibly explain a useful example, without any intended limitation.</p>
<p id="h-0012" num="0000">Gesture Recognition &#x26; Execution</p>
<p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. 2A</figref> shows a sequence <b>200</b> to detect and analyze contact points, history, velocity, and/or applied force to recognize user application of predefined touch-based user gestures, and thereafter implement predetermined actions pre-associated with the recognized gestures. As described in further detail below, optional features such as inertia, touch initiated object slowing, friction, and others may be implemented. For ease of explanation, but without any intended limitation, the example of <figref idref="DRAWINGS">FIG. 2A</figref> is described in the context of the interactive touch input system of <figref idref="DRAWINGS">FIGS. 1A-1C</figref>.</p>
<p id="p-0048" num="0047">Broadly, the steps <b>202</b>, <b>204</b>, <b>206</b> run continuously to process user contact with the display surface <b>124</b> as it occurs. Steps <b>202</b>, <b>204</b>, <b>206</b> therefore serve to analyze contact occurring when the user contacts the surface <b>124</b> at one or more contact regions utilizing one or more fingers, hands, arms, etc. As explained in greater detail below, step <b>208</b> analyzes the history of position, velocity, force, and other touch characteristics to recognize when the user has performed a recognized &#x201c;gesture.&#x201d;</p>
<p id="p-0049" num="0048">The sequence <b>200</b> is now described in greater detail. As an example, the sequence <b>200</b> may be initiated upon boot up, reconfiguration, initialization, or other startup of the system <b>120</b>. In step <b>201</b>, the user initiates (and the display/computer detects) the user's physical contact with the display surface <b>124</b>. Without any intended limitation, the illustrated embodiment of the sequence <b>200</b> performs one instance of the (repeating) steps <b>202</b>-<b>204</b> for each such contact initiated. The contact of step <b>201</b> is referred to as the &#x201c;current&#x201d; contact. In one gesture recognition scheme, the computer <b>126</b> tracks a predetermined number of distinct contact locations (such as two). If the computer identifies another contact location (such as a third), the computer <b>126</b> ignores it until the user releases a sufficient number of the existing contact locations.</p>
<p id="p-0050" num="0049">In step <b>202</b>, the table <b>122</b> detects and monitors the position, size, shape, and timing of the current contact region. Namely, the table <b>122</b> provides a machine readable output to the computer <b>126</b>, which is representative of the position, size, shape, and timing of each contact region, or contains information from which this information can be calculated or derived. The timing output may be satisfied, for example, by the table <b>122</b> providing its output in real time. Also in step <b>202</b>, the computer <b>126</b> stores a position history for each contact region. The position history provides a record of how each contact region moves or and/or changes shape over time.</p>
<p id="p-0051" num="0050">In step <b>204</b>, the computer <b>126</b> computes and monitors the velocity (if any) of the subject contact that is occurring by analyzing the contact's position history. The computed velocity may comprise an instantaneous velocity, average velocity over some or all of the past, moving average, or other suitable computation.</p>
<p id="p-0052" num="0051">In step <b>206</b>, the table <b>122</b> detects and monitors the force by which the current user contact is being applied. As a specific example, this may occur by the table <b>122</b> detecting applied pressure of the current contact (utilizing a mechanism such as load cells, solid state force sensors, or other devices), or by assuming that applied force increases or decreases proportionally to the size of the contact region. To provide some examples, step <b>206</b> may be performed concurrently with step <b>202</b>, in series (as shown), or omitted entirely. Also in step <b>206</b>, the table <b>122</b> provides a machine-readable output to the computer <b>126</b>, this signal representing the detected force or containing information by which force can be derived or computed.</p>
<p id="p-0053" num="0052">In step <b>208</b>, the computer <b>126</b> determines whether activity of the current contact matches a predetermined pattern, and therefore constitutes a &#x201c;gesture.&#x201d; Step <b>208</b> repeats continually, utilizing some or all of the position, position history (movement), velocity, and force information from steps <b>202</b>, <b>204</b>, <b>206</b>. More particularly, in step <b>208</b> the computer <b>126</b> compares the history of contact position, size, movement, velocity, and/or force to the dictionary <b>126</b><i>a </i>of predetermined gestures to determine if the user has performed any of these gestures.</p>
<p id="p-0054" num="0053">As long as the current contact continues, but no gesture has been detected, step <b>208</b> repeats (via <b>208</b><i>a</i>). If the current contact ends but no gesture is detected (<b>208</b><i>b</i>), then the computer <b>126</b> may optionally provide feedback to the user that an attempted gesture was not recognized (step <b>209</b>). Feedback may be provided, for example, by audible alert, visual alert, error log, etc. In contrast, if step <b>208</b> detects that the user has initiated a gesture (<b>208</b><i>c</i>), the computer in step <b>214</b> utilizes the mapping <b>126</b><i>c </i>to identify the action <b>126</b><i>b </i>associated with the gesture that was identified in step <b>208</b>. As mentioned above, the predefined actions include various machine implemented operations for updating the presentation of imagery by the display. In one embodiment, gestures are both identified (<b>208</b>) and associated (<b>214</b>) with display control commands via a single procedure.</p>
<p id="p-0055" num="0054">After step <b>214</b>, the computer <b>126</b> initiates performance of the identified action (step <b>216</b>). As described in greater detail below, some examples of actions <b>126</b><i>b </i>include panning, zooming, rotating, and the like. Thus, step <b>216</b> starts the requested pan, zoom, rotate, or other operation.</p>
<p id="p-0056" num="0055">In step <b>218</b>, the computer/display detects that the current gesture has ended because the user terminated contact with the display. In a simple embodiment, the computer <b>126</b> may respond to termination of the current gesture by ending the associated action (step <b>220</b>). However, by simulating physical properties, such as inertia and friction, the system <b>120</b> can more closely approximate the look and feel of manipulating a physical object. An important consequence of these properties is that motion of the displayed imagery can continue, and subsequently cease, after the initiating points of contact are removed. Therefore, in step <b>218</b> the computer <b>126</b> considers whether the gesture terminated with a non-zero velocity. In other words, step <b>218</b> determines whether, at the moment the user ended the current gesture by terminating contact with the display surface, the contact region was moving. Step <b>218</b> may conclude that the gesture ended with motion if there was any motion whatsoever, or step <b>218</b> may apply a predetermined threshold (e.g., one inch per second), above which the contact region is considered to be moving.</p>
<p id="p-0057" num="0056">If the current gesture ended with a zero velocity (or a nonzero velocity that did not meet the threshold), then step <b>218</b> progresses (via <b>218</b><i>a</i>) to step <b>220</b>, where the computer <b>126</b> terminates the action being performed for the subject gesture. In contrast, if the current gesture ended with a nonzero velocity, step <b>218</b> advances (via <b>218</b><i>b</i>) to step <b>222</b>, which executes the action in a manner that imparts inertia to the action.</p>
<p id="p-0058" num="0057">For example, if the action identified in step <b>214</b> was &#x201c;rotate,&#x201d; then the computer <b>126</b> in step <b>222</b> directs the projector <b>128</b> to additionally continue the requested rotation after the gesture terminates. In one embodiment, the imparted inertia may be proportional to the nonzero velocity at gesture termination (computed at <b>204</b>), which may serve to simulate continuation of the motion that was occurring when the gesture terminated.</p>
<p id="p-0059" num="0058">Another example is where the computer <b>126</b> detects (<figref idref="DRAWINGS">FIG. 2</figref>, step <b>208</b>) that the user has initiated a pan gesture by drawing a finger across the display surface at a particular velocity, and lifted his/her finger from the surface while still moving (<figref idref="DRAWINGS">FIG. 2</figref>, step <b>218</b><i>b</i>). With the optional inertia feature enabled, the computer <b>126</b> continues (<figref idref="DRAWINGS">FIG. 2</figref>, step <b>222</b>) to pan the imagery in the initiated direction at the velocity implied by the gesture at the time the finger was lifted until a stopping or slowing naturally occurs (step <b>224</b>). If the velocity when the finger was lifted is low, the computer <b>126</b> pans the display at a correspondingly slow rate, useful for slowly panning across imagery. Alternatively, if the computer <b>126</b> detects a panning gesture terminated at a rapid velocity, the computer <b>126</b> quickly translates the imagery in the desired direction, without the need for repeated panning gestures to continue movement. The computer <b>126</b> similarly recognizes user termination of other gestures with residual velocity, such as rotation and zoom, with inertia continuing the appropriate motion until stopped.</p>
<p id="p-0060" num="0059">With various techniques, the routine <b>200</b> may slow the imparted inertia as illustrated by step <b>224</b>. For example, without user contact, the computer <b>126</b> may slow the inertia at a predetermined rate to simulate friction. As another example, upon new user contact after terminating the gesture with inertia, the computer <b>126</b> may (1) slow the inertia in proportion to force exerted by the user, the size of the contact area, or other properties, (2) abruptly terminate the inertia, thus bringing the motion of the imagery to an immediate stop, (3) terminate the inertia and immediately impart a motion correlating with the new contact, or (4) perform another action.</p>
<p id="p-0061" num="0060">One example of a slowing gesture (step <b>224</b>) comprises placing the finger or hand on the display surface, as if stopping a spinning globe. In response to this gesture, the computer <b>126</b> may slow movement at a rate that is proportional to the force with which the gesture is applied or to the area of contact. For example, responsive to the user lightly touching a finger, the computer <b>126</b> will cause &#x201c;drag&#x201d; and gradually slow the motion. Likewise, responsive to a firmer touch or wider area of contact (such as a whole hand), the computer <b>126</b> more briskly slows the motion, or immediately stops entirely. This graduated response is useful when, for example, the imagery is panning at high speed and the desired location is approaching. Thus, the user can gently slow down the display with a light touch then press firmly when the location is reached. In an alternative embodiment, the computer <b>126</b> ceases motion at the first tap or other touch.</p>
<p id="p-0062" num="0061">In one embodiment, the computer <b>126</b> is responsive to user input to enable, disable, and/or adjust the above described inertia, friction, and such properties. For example, a simulated friction coefficient governs the degree to which the imagery motion slows over time. With the friction coefficient is set to zero or inactive, the computer <b>126</b> utilizes a simulated friction of zero, and continues motion at the initiated velocity until stopped by the user through a stopping gesture. In contrast, with the friction coefficient set to a nonzero value, the computer <b>126</b> slows the motion of the imagery at the given rate. The computer <b>126</b> may also recognize an adjustable threshold for determining motion (<b>218</b>) or no motion (<b>218</b><i>b</i>).</p>
<p id="h-0013" num="0000">Integrating Multiple Image Layers</p>
<p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. 2B</figref> shows a sequence <b>230</b> to illustrate one example of a multi-layer application of this disclosure. For ease of explanation, but without any intended limitation, the operations of <figref idref="DRAWINGS">FIG. 2B</figref> are described in the hardware context of the interactive touch input system of <figref idref="DRAWINGS">FIGS. 1A-1C</figref>.</p>
<p id="p-0064" num="0063">Although the following sequence <b>230</b> may be implemented as part of the gesture recognition and implementation sequence <b>200</b> (or it may incorporate the sequence <b>200</b>), the sequence <b>230</b> may also be implemented independent of the sequence <b>200</b>. For ease of discussion, sequence <b>230</b> has been described as a standalone product, although certain steps of the sequence <b>230</b> utilize operations similar to those of sequence <b>200</b> (such as steps <b>201</b>-<b>206</b>). In this case, details such as inertia (<b>218</b>-<b>224</b>), sensing velocity and force (<b>204</b>, <b>206</b>), and other such details may be adopted or left out of the implementation of sequence <b>230</b> as appropriate to the particular application.</p>
<p id="p-0065" num="0064">Broadly, the sequence <b>230</b> functions as follows. The system <b>120</b> receives a first image and at least one secondary images. Each image represents various spatial coordinates, which overlap at least in part; thus, each image is an alternate depiction of subject matter common to all of the images. The sequence <b>230</b> presents the first image on the display <b>124</b>. Responsive to user input including contact with the display, the sequence <b>230</b> updates the displayed imagery to integrate a region of one (or more) of the secondary images into the display. Each integrated region has substantially identical represented coordinates as a counterpart region of the first image. Further, each integrated region is presented in same scale and display location as the counterpart region of the first image.</p>
<p id="p-0066" num="0065">The following is a more particular discussion of the sequence <b>230</b>, with particular reference to <figref idref="DRAWINGS">FIG. 2A</figref>. In step <b>232</b>, the display system <b>120</b> receives, defines, creates, modifies, formats, or otherwise establishes multiple images. Each image represents various spatial coordinates, which overlap at least in part; thus, each image is an alternate depiction of subject matter common to all of the images. Each image may be referred to as an image &#x201c;layer&#x201d;, since the images comprise alternate depictions of the same subject matter. Some examples of depicted subject matter include a scene, a physical object, a building, a section of earth, city, area of earth topography, machine, or virtually any other subject matter capable of representation by visual images.</p>
<p id="p-0067" num="0066">In one example, the images depict the subject matter at different times. In another example, the images depict different levels of a subject matter with multiple levels, such as planes of circuitry, floor plans of a multi-story building, strata of earth, etc. In another example, the images differ in that some form an actual depiction of subject matter (such as a photograph) and others provide a logical, artistic, computer graphic, or man-made representation of the subject matter (such as a road map). Images in various combinations of the foregoing may also be used.</p>
<p id="p-0068" num="0067">Each image represents various spatial coordinates, and all images' spatial coordinates include at least some common coordinates. For example, all images may represent the same extent of latitude and longitude. As a different example, the images may represent different extents of latitude/longitude, with these extents nevertheless sharing some portions in common.</p>
<p id="p-0069" num="0068">In step <b>234</b>, the system <b>120</b> receives one or more mappings that define how the images interrelate. As mentioned above, each image represents various spatial coordinates. The mappings state the relationship between each image and its represented spatial coordinates. Some exemplary spatial coordinates include latitude/longitude, polar coordinates, Cartesian coordinates, mathematical translation between points or lines or borders in an image to spatial coordinates, or virtually any other technique for correlating an image with the content that is being represented. In one embodiment, the mappings are embodied in lookup tables, linked lists, databases, files, registers, or another data structure. In another embodiment, each image layer's mappings are incorporated into that image, for example, by displayed latitude/longitude values, hidden coordinate values, document metadata, or another system. Simply stated, the mappings provide a scale-free, content independent translation between each image layer and the represented spatial coordinates. In the event the mappings are incorporated into the images themselves, then step <b>234</b> is carried out when step <b>232</b> is performed.</p>
<p id="p-0070" num="0069">In step <b>236</b>, the system <b>120</b> presents a first one of the images on the display surface <b>124</b>. The first image may be a first one of the images if they are ordered, an arbitrary one of the images, a default image according to system or user-supplied settings, etc. The remaining, un-displayed images are referred to as secondary images.</p>
<p id="p-0071" num="0070">In step <b>238</b>, the system determines whether it has detected user input including an integrate command. The integrate command may be provided via on-screen menu entry, mouse click, off-screen input, on-screen gesture, voice command, foot pedal, or any other user input mechanism, device, or method. In a simple example, the user supplies the integrate command by touching the display surface <b>124</b>.</p>
<p id="p-0072" num="0071">If step <b>238</b> did not receive the integrate command, the system <b>120</b> performs various other processing in step <b>242</b>. For example, in step <b>242</b> the system may determine (<b>208</b><i>a</i>) whether other input than the integrate command has been received, and if so, process such input accordingly.</p>
<p id="p-0073" num="0072">On the other hand, if step <b>238</b> detected an integrate command, then step <b>240</b> integrates the images in a certain way. More specifically, the system <b>120</b> updates imagery presented by the display, namely the first image per step <b>236</b>, to integrate one or more of the other (not currently displayed) images into the display. More particularly, step <b>240</b> updates the displayed imagery to integrate a region of at least one of the secondary images into the display. Each integrated region has substantially identical represented coordinates as a counterpart region of the first image. For example, if the integrated region corresponds to Colorado, then the counterpart region of the first image is also Colorado. Stated in another way, the part of the second image that is being integrated into the display corresponds to a given part of the first image pursuant to the spatial coordinates. Moreover, each integrated region is presented in same scale and display location as the counterpart region of the first image. The integrated region of the second image (and counterpart region of the first image) may comprise the entire images or subparts of the respective images.</p>
<p id="p-0074" num="0073">As described in greater below, the integration of step <b>240</b> may be carried out in various ways. In one embodiment (<figref idref="DRAWINGS">FIG. 3A</figref>), the system <b>120</b> responds to the integrate gesture by performing a fade-out of the first image and a fade-in of one or more secondary images, as if the secondary images were initially hidden beneath the first image (in perfect alignment, and the same scale). In another embodiment (<figref idref="DRAWINGS">FIG. 4A</figref>), the system <b>120</b> responds to the integrate gesture by opening a user defined, virtual window in the first image, through which the corresponding portion of one or more secondary images is viewed. In another embodiment (<figref idref="DRAWINGS">FIG. 5A</figref>), the system <b>120</b> responds to the integrate gesture by interpolating or &#x201c;morphing&#x201d; between the first image and one or more secondary images in proportion to user movement of a &#x201c;slider&#x201d; tool depicted on the display surface <b>124</b>. The embodiments of <figref idref="DRAWINGS">FIGS. 3A</figref>, <b>4</b>A, and <b>5</b>B are discussed in detail below.</p>
<p id="h-0014" num="0000">Introduction to <figref idref="DRAWINGS">FIGS. 3A</figref>, <b>4</b>A, <b>5</b>A</p>
<p id="p-0075" num="0074"><figref idref="DRAWINGS">FIGS. 3A</figref>, <b>4</b>A, <b>5</b>A shows respective sequences <b>300</b>, <b>400</b>, <b>500</b> to illustrate various multi-layer embodiments. Although each sequence may be implemented as part of the gesture recognition and implementation sequence <b>200</b>, or vice versa, each sequence may also be implemented independent of the sequence <b>200</b>. For ease of discussion, each sequence <b>300</b>, <b>400</b>, <b>500</b> is described as a standalone product, although certain steps of each sequence may utilize operations similar to those of sequence <b>200</b> (such as steps <b>201</b>-<b>206</b>). For ease of explanation, but without any intended limitation, the aforementioned sequences are described in the hardware context of the interactive touch input system of <figref idref="DRAWINGS">FIGS. 1A-1C</figref>.</p>
<p id="h-0015" num="0000">Fade Application</p>
<p id="p-0076" num="0075">Broadly, the fade mode sequence <b>300</b> serves to &#x201c;fade&#x201d; from an initially displayed image to one or more initially images that are not initially displayed; this is performed in response to user touch applied to the display screen. The images are displayed in situ, so that each point on the display screen continues to show the same spatial coordinates regardless of which image is being shown.</p>
<p id="p-0077" num="0076">Steps <b>332</b>, <b>334</b>, <b>336</b> receive image layers, receive mapping, and present the first image in the same manner as steps <b>232</b>, <b>234</b>, <b>236</b> described above (<figref idref="DRAWINGS">FIG. 2B</figref>). In order to present the first image in step <b>336</b>, an image-display mapping is developed between the first image and the display surface <b>124</b> to present the image with the desired scale. Such image-display mappings are well known components of computer graphics software and require no further description here, mention being made merely to aid in discussion of the fade window below.</p>
<p id="p-0078" num="0077">Step <b>338</b> detects whether the user has supplied a predetermined fade command. The fade command may be input by any type of user-to-computer input device or methodology, such as activating a predetermined selection of a GUI menu or other functional interface followed by manually contacting the display surface <b>124</b>. In the present embodiment, the fade command is detected whenever the user performs a predefined fade gesture comprising application of a predetermined threshold force to the display surface <b>124</b> with a sufficiently small velocity (e.g., less than a prescribed threshold velocity).</p>
<p id="p-0079" num="0078">If the system <b>120</b> detects other input than the predefined fade command, various other processing <b>342</b> is performed, such as waiting for performance of a gesture, providing an error message to the user, or other operations such as those discussed above in context of steps <b>208</b><i>a</i>, <b>209</b>, etc. of <figref idref="DRAWINGS">FIG. 2B</figref>.</p>
<p id="p-0080" num="0079">On the other hand, when the system <b>120</b> detects the fade command, step <b>340</b> performs an act of integrating <b>340</b> the images. This involves defining a fade window (<b>340</b><i>a</i>), fading-out the first image (step <b>340</b><i>b</i>), and fading-in a second image (step <b>340</b><i>c</i>).</p>
<p id="p-0081" num="0080">The fade window may be defined (<b>340</b><i>a</i>) in various ways, with some examples of fade window including: (1) the entire display surface, (2) a predefined size and shape of window established by user selection or default settings, (3) a shape and size of window surrounding the point of user contact and proportional to the size, shape, and/or force of user contact with the display surface <b>124</b>, or (4) another arrangement entirely.</p>
<p id="p-0082" num="0081">As mentioned above, steps <b>340</b><i>b</i>-<b>340</b><i>c </i>involve reducing visibility of the first image inside the fade window (<b>340</b><i>b</i>), and increasing visibility of a second image within the fade window (<b>340</b><i>c</i>). In other words, steps <b>340</b><i>b</i>-<b>340</b><i>c </i>reduce visibility of a region of the first image corresponding to the fade window, and increase visibility of the second image through the fade window as if the second image were residing beneath the first image (and aligned therewith according to the applicable mappings). In making the transition of steps <b>340</b><i>b</i>-<b>340</b><i>c</i>, one implementation is to utilize the image-display mapping to determine which spatial coordinates of the first image are bounded by the fade-window (such as by latitude/longitude), and then to use the mapping of step <b>334</b> to identify the same region in the second image. Inside the fade window, the identified region of the first image is subject to fade-out, and the identified region of the second image is subject to fade-in.</p>
<p id="p-0083" num="0082">In an exemplary embodiment, steps <b>340</b><i>b</i>-<b>340</b><i>c </i>may be performed by the computer <b>126</b> reacting to user input by specifying appropriate transparency values of each image layer, and inputting them into an appropriate computer graphics program, mapping program, rendering program, or other such program for generating an output on the display <b>124</b>. For instance, each image is initially (e.g., step <b>336</b>) assigned a transparency value indicative of the image's level of fade-in or fade-out. For instance, in step <b>336</b>, the first image has a transparency level of 0% and all other images have a transparency level of 100%. The system <b>120</b> displays each image according to its present transparency level. In this embodiment, steps <b>340</b><i>b</i>-<b>340</b><i>c </i>are performed by simultaneously increasing the transparency value for the first image layer and decreasing the transparency level for the second image.</p>
<p id="p-0084" num="0083">In performing the fade-out (<b>340</b><i>b</i>) and fade-in (<b>340</b><i>c</i>), the system <b>120</b> may operate to provide a rate of change in transparency that is proportional to the force applied (e.g., more force, more fade). Furthermore, the system <b>120</b> may automatically return to the presentation of the original image when the user force is removed or falls below a given threshold.</p>
<p id="p-0085" num="0084">The subject matter depicted by the second image displayed in the window matches the subject matter formerly depicted by the first image in the window (albeit, removed when the fade occurred). Therefore, steps <b>340</b><i>b</i>-<b>340</b><i>c </i>have the presentation effect of fading from the first image to the second image. Optionally, fade-in and fade-out may be largely or precisely inversely proportional to provide a smooth transition. Fading may occur across the entire display surface, or within a limited window smaller than the display <b>124</b>. Despite the use of the term &#x201c;fade,&#x201d; transition from one image to the next may be abrupt, or may involve gradually lessening the presentation of one image while gradually increasing the next in proportion to default settings, user-defined settings, size or force of user contact applied, etc. Furthermore, the system <b>100</b> may regulate the degree of fade in proportion to a parameter such as the amount of user force applied at the contact site.</p>
<p id="p-0086" num="0085">In the event there are greater than two images, steps <b>340</b><i>b</i>-<b>340</b><i>c </i>involves a fade from first image to second, third, and further images that represent successive slices of the depicted subject matter in a particular order (such as higher layers to lower layers, etc.). In this embodiment, the images may therefore compose a series of images having a defined order. In this embodiment, the image layers' transparency is determined by the amount of user force applied to the display surface <b>124</b>. For example, steps <b>340</b><i>b</i>-<b>340</b><i>c </i>may favor higher layers with lesser applied force, etc. In one embodiment, transition between layers occurs smoothly, by interpolating between layers during transition between them; in a different embodiment, transition is abrupt, where only one layer has 100% transparency at a given time and all other layers have 0% transparency.</p>
<p id="p-0087" num="0086">In step <b>344</b>, the fade operation of step <b>340</b> is reversed. In one example, the fade is partially or completely reversed in response to lessening or removal of user contact with the display surface. In other examples, fade may be reversed upon passage of a default or user-defined time, upon user performance of another prescribed gesture, user selection of a GUI menu entry, etc. Broadly, step <b>340</b> involves increasing visibility of the first image in the virtual window, and reducing visibility of the second image in the window.</p>
<p id="p-0088" num="0087"><figref idref="DRAWINGS">FIG. 3B</figref> depicts an example of the foregoing fade operation. In this example, the user's hand <b>381</b> is applying force to an area <b>382</b> of the display surface <b>124</b>. The circular rings of <b>382</b> are shown to illustrate the contact between the user's hand <b>381</b> and the display surface, however, these rings are not visible in practice. The fade window in this example is the entire display surface <b>124</b>. Thus, the first image (an aerial photograph) is being faded-out and a second image (a roadmap of the same area) is being faded-in across the entire display <b>124</b>.</p>
<p id="h-0016" num="0000">Swipe Mode Application</p>
<p id="p-0089" num="0088">Broadly, the swipe mode sequence <b>400</b> presents a first image, and response to user definition of a swipe window on the display screen, presents a second image within the window instead of the first image. The second image is introduced in situ, so that each point on the display screen continues to show the same spatial coordinates, whether inside or outside the swipe window. Optionally, the swipe window may be resizable in response to user input.</p>
<p id="p-0090" num="0089">More specifically, steps <b>432</b>, <b>434</b>, <b>436</b> receive image layers, receive mapping, and present the first image in the same manner as steps <b>232</b>, <b>234</b>, <b>236</b> described above (<figref idref="DRAWINGS">FIG. 2B</figref>). As in <figref idref="DRAWINGS">FIG. 3A</figref>, in order to present the first image in step <b>436</b>, an image-display mapping is developed between the first image and the display surface <b>124</b> to present the image with the desired scale.</p>
<p id="p-0091" num="0090">Particular layers may be designated to be displayed in swipe mode by a user selection through a configuration menu, by default, or by another technique. As an example, the system <b>120</b> may keep track of layer attributes in a lookup table or other file containing data about layers, such that when slider mode is selected (see below) the system only displays those layers designated for that mode. All other layers not so designated are not displayed in swipe mode.</p>
<p id="p-0092" num="0091">Step <b>438</b> recognizes when the user activates swipe mode. In one embodiment, this occurs by the user selecting swipe mode from a menu or other control interface. Alternatively, the system <b>120</b> may detect a predefined, user-applied swipe mode activation gesture distinguished by particular characteristics of touch position, velocity, and/or force.</p>
<p id="p-0093" num="0092">If the system <b>120</b> does not detect activation of swipe mode, various other processing <b>442</b> is performed, such as waiting for performance of a this or a different gesture, providing an error message to the user, or other operations as discussed above in the context of steps <b>208</b><i>a</i>, <b>209</b>, etc. of <figref idref="DRAWINGS">FIG. 2B</figref>.</p>
<p id="p-0094" num="0093">When step <b>438</b> does detect swipe mode activation, step <b>440</b> integrates the first image (initially displayed) and second image (not initially displayed) according to the swipe mode, as follows. The second image is that image layer designated by the operations discussed above. First, the system <b>120</b> defines a swipe area (step <b>440</b><i>a</i>). In discussing the swipe area, it is helpful to refer to vertical (<b>469</b>) and lateral (<b>468</b>) directions of the display <b>124</b>, as illustrated in <figref idref="DRAWINGS">FIG. 4B</figref>. To define the swipe area, the system <b>120</b> waits for occurrence of a first point of user contact on the display surface, then occurrence of a second point of contact away from said first location. As an alternative, the user may define the second point of contact in a different way&#x2014;not by a new contact with the display <b>124</b>, but by dragging the first point of contact to a new location.</p>
<p id="p-0095" num="0094">The example of <figref idref="DRAWINGS">FIG. 4B</figref> depicts a user <b>463</b> touching the display surface <b>124</b> at a second point <b>466</b> of contact. Step <b>440</b><i>a </i>defines the swipe area as a vertical band <b>467</b> of the display surface <b>124</b> laterally bounded by the vertical lines <b>460</b>-<b>461</b> passing through first (not shown) and second <b>466</b> contact points. In the example of <figref idref="DRAWINGS">FIG. 4B</figref>, the vertical band <b>467</b> (swipe area) has the lateral boundaries <b>460</b>-<b>461</b>, as defined by first (not shown) and second (<b>466</b>) contact points.</p>
<p id="p-0096" num="0095">Optionally, upon sensing the first and second points of contact, the system <b>120</b> may update the display <b>124</b> to actually show the vertical lines <b>460</b>-<b>461</b> intersecting the respective contact points. Alternatively, display of these boundaries may be implied by the difference in appearance between the layers of imagery inside (<b>462</b>) and outside (<b>465</b>) the swipe area, as discussed below. Alternatively, the system <b>120</b> may recognize more than two borders to denote any desired rectangular area within the display, a circular area, a polygonal area, an irregular area designated by dragging a point of contact in a closed path on the display, etc.</p>
<p id="p-0097" num="0096">Referring to <figref idref="DRAWINGS">FIGS. 4A-4B</figref>, after defining the swipe area <b>467</b>, the system <b>120</b> in step <b>440</b><i>b </i>ceases display of the first image <b>465</b> within the swipe area <b>467</b> and instead displays the corresponding portion <b>462</b> of the second image. Consequently, from the user's perspective, the swipe area <b>467</b> has the effect of presenting a window through the first image into the second image's depiction of the same subject matter. Step <b>440</b><i>c </i>maintains display of the first image outside the swipe area <b>467</b>.</p>
<p id="p-0098" num="0097">In making the transition of steps <b>440</b><i>b</i>-<b>440</b><i>c</i>, one implementation is to utilize the image-display mapping to determine which spatial coordinates of the first image are bounded by the swipe window (such as by latitude/longitude), and then to apply these spatial coordinates to identify the same region in the second image. Inside the swipe window, the first image is replaced by the identified region of the second image.</p>
<p id="p-0099" num="0098">In the example of <figref idref="DRAWINGS">FIG. 4B</figref>, the first image <b>465</b> (maintained outside the swipe area <b>467</b>) is a satellite photo, and the second image <b>462</b> is a roadmap. The portion <b>462</b> of the roadmap revealed by the swipe area <b>467</b> and the portion of the satellite photo <b>465</b> that is cut-out by the swipe area <b>467</b> corresponding to the same area of represented subject matter. Thus, despite performance of swipe mode, each point on the display continues to depict imagery corresponding to the same spatial coordinates regardless of which image is being shown.</p>
<p id="p-0100" num="0099">In one implementation, steps <b>440</b><i>a</i>-<b>440</b><i>c </i>may be performed as follows. The second image is displayed and a screen shot is taken and stored, for example in bitmap format. The transparency of the second image is then set to 100% so that it disappears from view, quickly enough that this may be imperceptible to the user, and the first image is displayed (i.e. its transparency is set to 0%, or completely opaque). Thereafter, the screen shot is utilized as the second image in the swipe area. If the boundary <b>461</b> is subsequently redefined <b>440</b><i>a </i>(discussed below), this is performed by varying a size of the earlier screen shot being presented. To display (<b>440</b><i>b</i>) the second image in the swipe area <b>461</b> and display (<b>440</b><i>c</i>) the first image outside the swipe area <b>461</b>, the following implementation may be used, as one example. Namely, the system specifies a transparency value for the second image within the swipe window as 0% (fully opaque) and a transparency value for the first image outside the swipe window as 100% (fully transparent), and inputs these desired transparency values into an appropriate computer graphics program, mapping program, rendering program, or other such program for generating an output on the display <b>124</b>. After performing steps <b>440</b><i>b</i>-<b>440</b><i>c</i>, step <b>440</b> may optionally redefine the boundary <b>461</b> by repeating step <b>440</b><i>a</i>, responsive to user touch and drag movements seeking to alter boundaries of the swipe area. For example, the swipe area may be redefined (step <b>440</b><i>a</i>) whenever the user, after making the second point of contact, drags the contact point as illustrated by <b>464</b>. Further, step <b>440</b><i>a </i>may detect a new point of contact occurring at one of the lateral boundaries <b>460</b>-<b>461</b> (or within a given threshold of distance), and in response, step <b>440</b><i>a </i>may modify the extent of the swipe area by moving that boundary responsive to initiating the new point of contact.</p>
<p id="p-0101" num="0100">In addition to redefining the swipe area, step <b>440</b><i>a </i>may also redefine contents of the area. For instance, step <b>440</b> may respond to user operation of a GUI menu or other functional interface to designate a different image for display in the swipe area. In this case, steps <b>440</b><i>b</i>-<b>440</b><i>c </i>are repeated in order to identify and thereafter present the relevant portion of the third, fourth, or other selected image in substitution of the image currently present in the swipe window.</p>
<p id="p-0102" num="0101">In step <b>444</b>, the swipe operation of step <b>440</b> may be discontinued. In one example, the system <b>120</b> removes the swipe area <b>467</b> and restores the first image <b>465</b> across the display surface <b>124</b>. This may be performed, for example, in response to removal of user contact with the display surface, or other events such as: passage of a default or user-defined time, user performance of another prescribed gesture, user selection of a menu entry, etc.</p>
<p id="h-0017" num="0000">Slider Mode Application</p>
<p id="p-0103" num="0102">Broadly, the slider mode sequence <b>500</b> interpolates between different image layers according to position of a user-positioned slider tool. Each image is introduced in situ, so that each point on the display screen continues to show the same spatial coordinates, regardless of which image layer(s) are being shown.</p>
<p id="p-0104" num="0103">Steps <b>532</b>, <b>534</b>, <b>536</b> receive image layers, receive mapping, and present a first image in the same manner as steps <b>232</b>, <b>234</b>, <b>236</b> described above (<figref idref="DRAWINGS">FIG. 2B</figref>). As with <figref idref="DRAWINGS">FIGS. 3A</figref>, <b>4</b>A, in order to present the first image in step <b>536</b>, an image-display mapping is developed between the first image and the display surface <b>124</b> to present the image with the desired scale.</p>
<p id="p-0105" num="0104">In the illustrated example, the images of step <b>532</b> have prescribed positions in a given image sequence. For example, each image may represent (1) an image of certain subject matter as of a different time or date, (2) an image of a different floor plan of a multi-story building or ship, (3) an image of a different cross-sectional plan view of a multi-level circuit or machine, etc.</p>
<p id="p-0106" num="0105">Step <b>537</b> recognizes when the user has activated the slider mode. In one embodiment, this occurs when the user selects a slider mode from a menu or other control interface. Alternatively, the system <b>120</b> may detect a predefined slider mode activation gesture by analyzing touch position, velocity, and/or force.</p>
<p id="p-0107" num="0106">If the system <b>120</b> does not detect the slider mode movement in step <b>537</b>, various other processing <b>542</b> is performed, such as waiting for performance of a this or a different gesture, providing an error message to the user, or other operations as discussed above in context of steps <b>208</b><i>a</i>, <b>209</b>, etc. of <figref idref="DRAWINGS">FIG. 2B</figref>.</p>
<p id="p-0108" num="0107">When slider mode is activated (step <b>537</b>), the display <b>124</b> presents a slider tool in step <b>538</b>. Broadly, the slider tool includes a bar, knob, button, dial, or other suitable GUI component. The presently described embodiment utilizes a linearly movable slider bar <b>560</b> illustrated in <figref idref="DRAWINGS">FIG. 5B</figref>. In this example, each designated linear position of the slider bar corresponds to a different image layer of step <b>532</b>. In other words, the slider bar is set up so that different positions of the slider bar correspond to different positions in the prescribed sequence of images. The slider bar observes an appropriately convenient scale.</p>
<p id="p-0109" num="0108">Next, in step <b>539</b> the system <b>120</b> analyzes user touch characteristics such as position, velocity, and/or force to determine whether the user has touched the slider bar, and dragged his/her finger (or stylus, or pen, etc.) in order to effectively push, pull, drag, or otherwise move the slider bar.</p>
<p id="p-0110" num="0109">When step <b>539</b> detects user movement of the slider bar, then step <b>540</b> computes and displays the appropriate image. Whenever the slider bar occupies a position corresponding in the sequence to a single one of the images, step <b>540</b> presents that single image upon the display. For example, if there are ten images and ten slider positions, when the slider bar rests in position one, step <b>540</b> always presents image one.</p>
<p id="p-0111" num="0110">Whenever the slider bar occupies a position without a corresponding single image, other tasks are performed. Namely, step <b>540</b> selects multiple images (each having a position in the given sequence with a predetermined relationship to slider tool position), and interpolates among the selected images. In one example, if the slider bar is between designated slider bar positions, step <b>540</b> selects the nearest two images to the slider bar position for interpolation. For example, if the slider bar rests between positions two and three, step <b>540</b> selects images two and three for interpolation. In one embodiment, a fixed weighting is always used (such as a 50% weighting of two images). In another embodiment, interpolation weighting is conducted in proportion to position of the slider bar. In this embodiment, the degree of interpolation varies in proportion to the position of the slider bar between established slider bar positions. For example, if the slider bar is 30% of the way between positions five and six, and there is one image per position, then step <b>540</b> interpolates between images five and six with a 30% weighting to image six and a 70% weighting to image five.</p>
<p id="p-0112" num="0111">In one embodiment, interpolation may be implemented by the computer <b>126</b> acting in response to user input to specify a transparency value for each image layer between 0% (fully opaque) and 100% (fully transparent), and inputting the image layers desired transparency values into a computer graphics or mapping program. Optionally, step <b>540</b> may perform higher order interpolations. For example, the interpolation formula may consider the contribution of layers adjacent to the slider button (as described previously) along with the contribution of one or more nonadjacent layers as well. Alternatively, the interpolation formula may consider non-adjacent layers only. As another optional feature, in performing slider mode interpolation the system <b>120</b> may ignore specified &#x201c;hidden&#x201d; layers in accordance with user-specified settings. As an example, the system <b>120</b> may keep track of layer attributes in a lookup table or other file containing data about layers, such that when slider mode is selected the system only displays those layers designated for use in slider mode. When interpolating between layers, the &#x201c;hidden&#x201d; layers are ignored. This function may be implemented via menu, separate program, or other technique. Alternatively, instead of affirmatively specifying &#x201c;hidden&#x201d; layers, all layers may be assumed hidden unless the user affirmatively specifies them as visible for use in the slider mode.</p>
<p id="p-0113" num="0112">Having computed the new (interpolated image), step <b>540</b> ultimately displays the resultant image. Thus, the effect of step <b>540</b> is for the system <b>120</b> to &#x201c;morph&#x201d; between the image layers in response to slider bar position. After step <b>540</b> presents the interpolated image, the routine <b>500</b> returns to step <b>538</b> to determine whether the user has moved the slider bar, and if so, to re-interpolate images and present the resultant image as appropriate. One particular utility of the technique <b>500</b> is to graphically demonstrate changes in an aerial or other scene over time. In this embodiment, there are multiple images, appropriate in number to the desired level of detail and accuracy. In this embodiment, where the slider bar <b>560</b> is movable along a line, the slider bar is accompanied by a stationary time bar <b>562</b> indicating various image dates and/or times. In the illustrated example, when the slider bar rests on a month for which an image is available, the system <b>120</b> presents the corresponding image <b>565</b> on the display <b>124</b>. When the slider bar rests on a month for which an image is not available, or rests between months, then the system <b>120</b> mathematically interpolates between the nearest available images in proportion to the placement of the slider bar <b>560</b>. For example, if the slider bar rests on June, and the nearest images correspond to May and July, then the system <b>120</b> interpolates between the May and July with a 50% weighting to each constituent image.</p>
<p id="p-0114" num="0113">If the images correspond to evenly spaced times or dates, then the slider time bar represents the various layers with corresponding, evenly spaced, points along the length of the slider time bar. As depicted in the example of <figref idref="DRAWINGS">FIG. 5B</figref>, the system <b>120</b> recognizes leftward slider movements to present older imagery, and rightward slider bar movements to present newer imagery. Optionally, in performing the slider mode the system <b>120</b> may act to simulate various physical properties, such as inertia and friction, to more closely approximate the look and feel of manipulating a physical object. Simulation of inertia, friction, and the like are discussed in greater detail above. As one example, when the user moves the slider bar and terminates the gesture with nonzero velocity, the system <b>120</b> may simulate inertia to keep the slider in motion, blending between layers until the last layer is reached. Alternatively, once the last layer in the sequence is reached, the system <b>120</b> may perform a wraparound by resetting to the first image layer and continuing its motion. The system <b>120</b> may also simulate friction to slow the motion of the slider bar, once released. Responsive to a stop/slow command, such as touching the display, the system <b>120</b> halts or slows the motion.</p>
<heading id="h-0018" level="1">Other Embodiments</heading>
<p id="p-0115" num="0114">While the foregoing disclosure shows a number of illustrative embodiments, it will be apparent to those skilled in the art that various changes and modifications can be made herein without departing from the scope of the invention as defined by the appended claims. Furthermore, although elements of the invention may be described or claimed in the singular, the plural is contemplated unless limitation to the singular is explicitly stated. Additionally, ordinarily skilled artisans will recognize that operational sequences must be set forth in some specific order for the purpose of explanation and claiming, but the present invention contemplates various changes beyond such specific order.</p>
<p id="p-0116" num="0115">In addition, those of ordinary skill in the relevant art will understand that information and signals may be represented using a variety of different technologies and techniques. For example, any data, instructions, commands, information, signals, bits, symbols, and chips referenced herein may be represented by voltages, currents, electromagnetic waves, magnetic fields or particles, optical fields or particles, other items, or a combination of the foregoing.</p>
<p id="p-0117" num="0116">Moreover, ordinarily skilled artisans will appreciate that any illustrative logical blocks, modules, circuits, and process steps described herein may be implemented as electronic hardware, computer software, or combinations of both. To clearly illustrate this interchangeability of hardware and software, various illustrative components, blocks, modules, circuits, and steps have been described above generally in terms of their functionality. Whether such functionality is implemented as hardware or software depends upon the particular application and design constraints imposed on the overall system. Skilled artisans may implement the described functionality in varying ways for each particular application, but such implementation decisions should not be interpreted as causing a departure from the scope of the present invention.</p>
<p id="p-0118" num="0117">The various illustrative logical blocks, modules, and circuits described in connection with the embodiments disclosed herein may be implemented or performed with a general purpose processor, a digital signal processor (DSP), an application specific integrated circuit (ASIC), a field programmable gate array (FPGA) or other programmable logic device, discrete gate or transistor logic, discrete hardware components, or any combination thereof designed to perform the functions described herein. A general purpose processor may be a microprocessor, but in the alternative, the processor may be any conventional processor, controller, microcontroller, or state machine. A processor may also be implemented as a combination of computing devices, e.g., a combination of a DSP and a microprocessor, a plurality of microprocessors, one or more microprocessors in conjunction with a DSP core, or any other such configuration.</p>
<p id="p-0119" num="0118">The steps of a method or algorithm described in connection with the embodiments disclosed herein may be embodied directly in hardware, in a software module executed by a processor, or in a combination of the two. A software module may reside in RAM memory, flash memory, ROM memory, EPROM memory, EEPROM memory, registers, hard disk, a removable disk, a CD-ROM, or any other form of storage medium known in the art. An exemplary storage medium is coupled to the processor such the processor can read information from, and write information to, the storage medium. In the alternative, the storage medium may be integral to the processor. The processor and the storage medium may reside in an ASIC.</p>
<p id="p-0120" num="0119">The previous description of the disclosed embodiments is provided to enable any person skilled in the art to make or use the present invention. Various modifications to these embodiments will be readily apparent to those skilled in the art, and the generic principles defined herein may be applied to other embodiments without departing from the spirit or scope of the invention. Thus, the present invention is not intended to be limited to the embodiments shown herein but is to be accorded the widest scope consistent with the principles and novel features disclosed herein.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. At least one digital data storage media tangibly embodying a program of machine-readable instructions executable by a digital data processing apparatus to perform operations to manage an interactive display system including a touch-sensitive display, the operations comprising:
<claim-text>establishing a first image comprising subject matter having predefined spatial coordinates;</claim-text>
<claim-text>presenting the first image upon the touch-sensitive display; and</claim-text>
<claim-text>responsive to prescribed user input including contact with the touch-sensitive display, integrating at least one secondary image comprising subject matter having predefined spatial coordinates into the display of the first image;</claim-text>
<claim-text>wherein each point on the touch-sensitive display depicts subject matter having substantially identical spatial coordinates regardless of which of the first and said secondary images is presented at the point.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The media of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first image comprises a depiction of specific subject matter, and wherein the at least one secondary image includes an alternate depiction of the specific subject matter.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The media of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the at least one secondary image comprises a depiction of the specific subject matter at a different time than the first image.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The media of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the at least one secondary image comprises a plan view of a different cross-sectional level of the specific subject matter than the first image.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The media of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the at least one secondary image comprises a diagram limited to a selected aspect of the specific subject matter.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The media of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising operations of receiving one or more mappings defining correspondence between the first and the at least one secondary image and their respective spatial coordinates.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. At least one digital data storage media tangibly embodying a program of machine-readable instructions executable by a digital data processing apparatus to perform operations to manage an interactive display system including a touch-sensitive display, the display presenting a first one of multiple images, each of the images comprising subject matter having predefined spatial coordinates, the operations comprising, responsive to prescribed user input including contact with the touch-sensitive display:
<claim-text>modifying presentation of the first image upon the touch-sensitive display by integrating a region of at least one other of the multiple images into the display;</claim-text>
<claim-text>wherein each integrated region depicts subject matter of substantially identical spatial coordinates as a counterpart region of the first image within which the integration occurs; and</claim-text>
<claim-text>wherein each point on the touch-sensitive display that depicts any one or a combination of the multiple images always depicts subject matter of unchanging spatial coordinates regardless of which of the multiple images is currently being presented.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. At least one digital data storage media tangibly embodying a program of machine-readable instructions executable by a digital data processing apparatus to perform operations to manage an interactive display system including a touch-sensitive display, the operations comprising:
<claim-text>establishing a first image and at least one secondary image, each of the images comprising subject matter having predefined spatial coordinates;</claim-text>
<claim-text>presenting the first image upon the touch-sensitive display;</claim-text>
<claim-text>responsive to prescribed user contact upon the touch-sensitive display, performing operations comprising:</claim-text>
<claim-text>reducing visibility of a fade-out region of the first image;</claim-text>
<claim-text>substantially simultaneously increasing visibility of a fade-in region of the at least one secondary image;</claim-text>
<claim-text>wherein the fade-out region and fade-in regions represent substantially identical spatial coordinates.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. At least one digital data storage media tangibly embodying a program of machine-readable instructions executable by a digital data processing apparatus to perform operations to manage an interactive display system including a touch-sensitive display, the operations comprising:
<claim-text>establishing a sequence of images, each image in the sequence comprising subject matter having predefined spatial coordinates;</claim-text>
<claim-text>responsive to prescribed user contact upon the touch-sensitive display, performing operations comprising:</claim-text>
<claim-text>defining a window upon the touch-sensitive display, the window representing an extent of represented spatial coordinates; and</claim-text>
<claim-text>within the window, presenting a region of an image whose order in the sequence is proportional to force of user contact upon the touch-sensitive display, the region corresponding to the represented spatial coordinates.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. At least one digital data storage media tangibly embodying a program of machine-readable instructions executable by a digital data processing apparatus to perform operations to manage an interactive display system including a touch-sensitive display, the operations comprising:
<claim-text>establishing a first image and at least one secondary image, each of the images comprising subject matter having predefined spatial coordinates;</claim-text>
<claim-text>presenting the first image upon the touch-sensitive display;</claim-text>
<claim-text>responsive to detecting occurrence of two or more contact sites on the touch-sensitive display, defining a swipe area bounded by the contact sites;</claim-text>
<claim-text>within the swipe area, ceasing presentation of the first image and presenting a region of the at least one secondary image,</claim-text>
<claim-text>wherein the region of the at least one secondary image comprises subject matter with spatial coordinates substantially matching the spatial coordinates of the subject matter of the first image within the swipe area.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. At least one digital data storage media tangibly embodying a program of machine-readable instructions executable by a digital data processing apparatus to perform operations to manage an interactive display system including a touch-sensitive display, the operations comprising:
<claim-text>establishing a plurality of images, each of the images comprising subject matter having predefined spatial coordinates;</claim-text>
<claim-text>responsive to prescribed user input, presenting a slider tool upon the touch-sensitive display; and</claim-text>
<claim-text>combining two or more of the images and presenting the combined image on the touch-sensitive display,</claim-text>
<claim-text>wherein the images are combined such that each point on the touch-sensitive display presenting the combined image depicts subject matter of substantially identical spatial coordinates regardless of which one or more of the images is presented at the point, and</claim-text>
<claim-text>wherein the combining operation weights the images in proportion to position of the slider tool.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. A computer driven interactive display apparatus, comprising:
<claim-text>a touch-sensitive display; and</claim-text>
<claim-text>a digital data processing machine programmed to manage said touch-sensitive display by performing operations comprising:</claim-text>
<claim-text>establishing a first image comprising subject matter having predefined spatial coordinates;</claim-text>
<claim-text>presenting the first image upon the touch-sensitive display; and</claim-text>
<claim-text>responsive to prescribed user input including contact with the touch-sensitive display, integrating at least one secondary image comprising subject matter having predefined spatial coordinates into the display of the first image;</claim-text>
<claim-text>wherein each point on the touch-sensitive display depicts subject matter having substantially identical spatial coordinates regardless of which of the first and said secondary images is presented at the point.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The apparatus of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the first image comprises a depiction of specific subject matter, and wherein the at least one secondary image includes an alternate depiction of the specific subject matter.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The apparatus of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the at least one secondary image comprises a depiction of the specific subject matter at a different time than the first image.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The apparatus of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the at least one secondary image comprises a plan view of a different cross-sectional level of the specific subject matter than the first image.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The apparatus of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the at least one secondary image comprises a diagram limited to a selected aspect of the specific subject matter.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The apparatus of <claim-ref idref="CLM-00012">claim 12</claim-ref>, further comprising said digital data processing machine programmed for performing operations of receiving one or more mappings defining correspondence between the first and the at least one secondary image and their respective spatial coordinates. </claim-text>
</claim>
</claims>
</us-patent-grant>
