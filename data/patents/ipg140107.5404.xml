<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626504-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626504</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13599992</doc-number>
<date>20120830</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20130101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>10</class>
<subclass>L</subclass>
<main-group>21</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>11</main-group>
<subgroup>02</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>704230</main-classification>
<further-classification>37524015</further-classification>
<further-classification>37524025</further-classification>
<further-classification>704219</further-classification>
</classification-national>
<invention-title id="d2e43">Extracting features of audio signal content to provide reliable identification of the signals</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5530483</doc-number>
<kind>A</kind>
<name>Cooper et al.</name>
<date>19960600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5550594</doc-number>
<kind>A</kind>
<name>Cooper et al.</name>
<date>19960800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5751368</doc-number>
<kind>A</kind>
<name>Cooper</name>
<date>19980500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5920842</doc-number>
<kind>A</kind>
<name>Cooper et al.</name>
<date>19990700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5946049</doc-number>
<kind>A</kind>
<name>Cooper et al.</name>
<date>19990800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6098046</doc-number>
<kind>A</kind>
<name>Cooper et al.</name>
<date>20000800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6141057</doc-number>
<kind>A</kind>
<name>Cooper et al.</name>
<date>20001000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6392707</doc-number>
<kind>B1</kind>
<name>Cooper et al.</name>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6421636</doc-number>
<kind>B1</kind>
<name>Cooper et al.</name>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6469741</doc-number>
<kind>B2</kind>
<name>Cooper et al.</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6901209</doc-number>
<kind>B1</kind>
<name>Cooper et al.</name>
<date>20050500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6973431</doc-number>
<kind>B2</kind>
<name>Cooper et al.</name>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>6989869</doc-number>
<kind>B2</kind>
<name>Cooper et al.</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>7212651</doc-number>
<kind>B2</kind>
<name>Viola et al.</name>
<date>20070500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>8020180</doc-number>
<kind>B2</kind>
<name>Deng</name>
<date>20110900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>8023773</doc-number>
<kind>B2</kind>
<name>Brunk et al.</name>
<date>20110900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>2007/0250311</doc-number>
<kind>A1</kind>
<name>Shires</name>
<date>20071000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>2009/0080784</doc-number>
<kind>A1</kind>
<name>Luh</name>
<date>20090300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>2010/0254568</doc-number>
<kind>A1</kind>
<name>Chen</name>
<date>20101000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>2011/0216937</doc-number>
<kind>A1</kind>
<name>Radhakrishnan</name>
<date>20110900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>2011/0261257</doc-number>
<kind>A1</kind>
<name>Terry</name>
<date>20111000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>2011/0268315</doc-number>
<kind>A1</kind>
<name>Bauer</name>
<date>20111100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>2012/0054194</doc-number>
<kind>A1</kind>
<name>Gao</name>
<date>20120300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>2012/0078894</doc-number>
<kind>A1</kind>
<name>Jiang</name>
<date>20120300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>JP</country>
<doc-number>H08-500471</doc-number>
<date>19960100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>JP</country>
<doc-number>2004-519015</doc-number>
<date>20040600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>JP</country>
<doc-number>2005-517211</doc-number>
<date>20050600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>JP</country>
<doc-number>2005-531183</doc-number>
<date>20051000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>JP</country>
<doc-number>2009-516023</doc-number>
<date>20090400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>TW</country>
<doc-number>200637310</doc-number>
<date>20061000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>WO</country>
<doc-number>2012/106261</doc-number>
<date>20120800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00032">
<othercit>Fridrich, J. et al., &#x201c;Robust Hash Functions for Digital Watermarking,&#x201d; Proc. ICIT: Coding and Computing, XX, XX, No. PRO0540, (2000), pp. 178-183, XP002474318.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00033">
<othercit>Mihcak, M.K. et al., &#x201c;A Perceptual Audio Hashing Algorithm: A Tool for Robust Audio Identification and Information Hiding,&#x201d; (2001) pp. 51-65, XP002356506.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00034">
<othercit>Ozer, H. et al., &#x201c;Perceptual Audio Hashing Functions,&#x201d; Eurasip Journal on Applied Signal Processing, (2005) No. 12, pp. 1780-1793, XP002487282.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00035">
<othercit>Toguro, M. et al., &#x201c;Video Stream Retrieval Based on Temporal Feature of Frame Difference,&#x201d; ICASSP (2005), pp. II-445-II-448, XP010790672.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00036">
<othercit>Zhou, X., et al., &#x201c;Perceptual Hashing of Video Content Based on Differential Block Similarity,&#x201d; CIS (2005) Part II, LNAI 3802, pp. 80-85, XP002479786.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00037">
<othercit>Radhakrishnan, R. et al. &#x201c;Audio Signature Extraction Based on Projections of Spectograms&#x201d; Multimedia and Expo IEEE International Conference, Jul. 2, 2007, pp. 2110-2113.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00038">
<othercit>Supangkat, S.H. et al. &#x201c;A Public Key Signature for Authentication in Telephone&#x201d; Circuits and Systems, Oct. 28-31, 2002, pp. 495-498.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>15</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>37524001- 29</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>703230</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>6</number-of-drawing-sheets>
<number-of-figures>15</number-of-figures>
</figures>
<us-related-documents>
<division>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>12312840</doc-number>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>8259806</doc-number>
</document-id>
</parent-grant-document>
<parent-pct-document>
<document-id>
<country>WO</country>
<doc-number>PCT/US2007/024744</doc-number>
<date>20071129</date>
</document-id>
</parent-pct-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13599992</doc-number>
</document-id>
</child-doc>
</relation>
</division>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60872090</doc-number>
<date>20061130</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20130064416</doc-number>
<kind>A1</kind>
<date>20130314</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Radhakrishnan</last-name>
<first-name>Regunathan</first-name>
<address>
<city>San Bruno</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Bauer</last-name>
<first-name>Claus</first-name>
<address>
<city>Beijing</city>
<country>CN</country>
</address>
</addressbook>
<residence>
<country>CN</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Terry</last-name>
<first-name>Kent Bennett</first-name>
<address>
<city>Millbrae</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="004" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Link</last-name>
<first-name>Brian David</first-name>
<address>
<city>San Jose</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="005" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Kim</last-name>
<first-name>Hyung-Suk</first-name>
<address>
<city>Valencia</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="006" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Gsell</last-name>
<first-name>Eric</first-name>
<address>
<city>Valencia</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Radhakrishnan</last-name>
<first-name>Regunathan</first-name>
<address>
<city>San Bruno</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Bauer</last-name>
<first-name>Claus</first-name>
<address>
<city>Beijing</city>
<country>CN</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Terry</last-name>
<first-name>Kent Bennett</first-name>
<address>
<city>Millbrae</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="004" designation="us-only">
<addressbook>
<last-name>Link</last-name>
<first-name>Brian David</first-name>
<address>
<city>San Jose</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="005" designation="us-only">
<addressbook>
<last-name>Kim</last-name>
<first-name>Hyung-Suk</first-name>
<address>
<city>Valencia</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="006" designation="us-only">
<addressbook>
<last-name>Gsell</last-name>
<first-name>Eric</first-name>
<address>
<city>Valencia</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Dolby Laboratories Licensing Corporation</orgname>
<role>02</role>
<address>
<city>San Francisco</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Czekaj</last-name>
<first-name>David</first-name>
<department>2487</department>
</primary-examiner>
<assistant-examiner>
<last-name>Beck</last-name>
<first-name>Leron</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Signatures that can be used to identify video and audio content are generated from the content by generating measures of dissimilarity between features of corresponding groups of pixels in frames of video content and by generating low-resolution time-frequency representations of audio segments. The signatures are generated by applying a hash function to intermediate values derived from the measures of dissimilarity and to the low-resolution time-frequency representations. The generated signatures may be used in a variety of applications such as restoring synchronization between video and audio content streams and identifying copies of original video and audio content. The generated signatures can provide reliable identifications despite intentional and unintentional modifications to the content.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="49.28mm" wi="168.40mm" file="US08626504-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="232.24mm" wi="179.75mm" file="US08626504-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="247.48mm" wi="172.47mm" file="US08626504-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="238.25mm" wi="166.12mm" file="US08626504-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="246.89mm" wi="165.27mm" file="US08626504-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="221.49mm" wi="158.58mm" file="US08626504-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="246.21mm" wi="183.05mm" file="US08626504-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">TECHNICAL FIELD</heading>
<p id="p-0002" num="0001">The present invention pertains generally to the processing of video and audio signals and pertains more specifically to processes that extract features from video signals and audio signals to identify the signals. Throughout this disclosure, the term &#x201c;video signals&#x201d; and &#x201c;video content&#x201d; refer to signals and content that represent images intended for visual perception and the term &#x201c;audio signals&#x201d; and &#x201c;audio content&#x201d; refer to signals and content that represent sounds intended for aural perception.</p>
<heading id="h-0002" level="1">BACKGROUND ART</heading>
<p id="p-0003" num="0002">Applications such as those that attempt to detect pirated content conveyed by video and audio signals or that attempt to resynchronize disassociated video and audio signals often rely on processes that examine signal content to identify the signals. For many of these applications, it is important to obtain a reliable identification of signals even when the content of those signals has been modified either unintentionally or intentionally such that the modified content can still be recognized by a human observer as being substantially the same as the original content. If the perceived difference between the content of an original signal and a modified signal is small, then preferably the identification process can extract identifying features from the original and modified signals that are very similar to one another.</p>
<p id="p-0004" num="0003">Examples of unintentional modifications to signal content include the insertion or addition of noise to signals in transmission channels and on storage media. Examples of intentional modifications to video signals include luminance and color modifications such as contrast/brightness adjustments, gamma correction, luminance histogram equalization, color saturation adjustments and color correction for white balancing, include geometric modifications such as image cropping and resizing, image rotation and flipping, stretching, speck removal, blurring, sharpening and edge enhancement, and include coding techniques such as lossy compression. Examples of intentional modifications to audio signals include amplification, equalization, dynamic range modification, channel up-mixing, time-scale modification, spectral shaping and lossy data compression.</p>
<heading id="h-0003" level="1">DISCLOSURE OF INVENTION</heading>
<p id="p-0005" num="0004">It is an object of the present invention to provide identification processes that can be used to obtain a reliable identification of video and audio signals even if the content of the signals has been modified by mechanisms such as those mentioned above.</p>
<p id="p-0006" num="0005">This object is achieved by the present invention that is described below.</p>
<p id="p-0007" num="0006">The various features of the present invention and its preferred embodiments may be better understood by referring to the following discussion and the accompanying drawings in which like reference numerals refer to like elements in the several figures. The contents of the following discussion and the drawings are set forth as examples only and should not be understood to represent limitations upon the scope of the present invention.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading>
<p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. 1</figref> is a schematic block diagram of a system that may be used to obtain a reliable identification of video and audio signals.</p>
<p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. 2</figref> is a schematic block diagram of a system that may be used to obtain a reliable identification of video signals.</p>
<p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. 3</figref> is a schematic block diagram of a system that may be used to obtain a reliable identification of audio signals.</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIGS. 4A-4C</figref> are schematic block diagrams of processes that may be used to generate dissimilarity measures representing differences between two frames of video content.</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIGS. 5A-5B</figref> are schematic block diagrams of a set of intermediate values in a low-resolution image.</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 6</figref> is a schematic block diagram of a process that may be used to generate a time-frequency representation of a segment of audio content.</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 7</figref> is a schematic block diagram of a set of intermediate values in a low-resolution time-frequency representation.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 8</figref> is a schematic block diagram of a device that captures reference signatures and alignment information for synchronized video/audio streams.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 9</figref> is a schematic block diagram of a device that restores synchronization to video/audio streams.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 10</figref> is a schematic block diagram illustrating timing delays in video/audio streams.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 11</figref> is a schematic block diagram of a device that manages a signature database for detection of copies of video or audio content.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 12</figref> is a schematic block diagram of a device that may be used to implement various aspects of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">MODES FOR CARRYING OUT THE INVENTION</heading>
<heading id="h-0006" level="1">A. Introduction</heading>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 1</figref> is a schematic block diagram of an exemplary system <b>300</b> that examines the contents of segments <b>3</b><i>a </i>to <b>3</b><i>d </i>of a video/audio signal <b>3</b> to obtain a reliable identification of that signal. The video signature generator <b>100</b> obtains a set of video signatures (SV) <b>199</b><i>b </i>to <b>199</b><i>d </i>that identify video content and the audio signal generator <b>200</b> obtains a set of audio signatures (SA) <b>299</b><i>a </i>to <b>299</b><i>d </i>that identify audio content. In the example shown in the figure, individual signatures of video and audio content correspond to segments of the video/audio signal. This particular example is discussed further in the next few paragraphs, where each segment of the video/audio signal conveys a frame of video content and a segment of audio content. This particular correspondence between video/audio segments, video frames, audio segments and signatures is presented as only an example. Other arrangements are possible.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 2</figref> is a schematic block diagram of the video signature generator <b>100</b>. The dissimilarity measure processor <b>120</b> examines the content of two video frames <b>1</b><i>a</i>, <b>1</b><i>b </i>within a sequence of video frames and generates intermediate values that represent one or more measures of dissimilarity between all or a portion of the two frames. If the content of each video frame is represented by an array of values expressing the intensity of discrete picture elements or pixels, for example, the intermediate values may be an array of differences between the average or standard deviation of intensities for groups of pixels. The video signature processor <b>170</b> applies a hash function to the intermediate values to generate a video signature (SV) <b>199</b><i>b </i>that identifies the content of the video frames.</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 3</figref> is a schematic block diagram of the audio signature generator <b>200</b>. The time-frequency representation processor <b>210</b> examines the content of an audio segment <b>2</b><i>b </i>within a sequence of segments and generates spectral values representing all or a portion of the spectral components of the audio content in the segment. If the audio content of the segment is represented by values expressing the amplitude of discrete samples, for example, the spectral values may be a set of coefficients within a time-frequency representation generated by a block time-domain to frequency-domain transform. The intermediate values processor <b>250</b> examines groups of the spectral values and derives an intermediate value from the intensities of the spectral values in each group. The audio signature processor <b>270</b> applies a hash function to the intermediate values to generate an audio signature (SA) <b>299</b><i>b </i>that identifies the content of the audio segment.</p>
<heading id="h-0007" level="1">B. Video Signature Generator</heading>
<p id="p-0023" num="0022">The components of the video signature generator <b>100</b> may be implemented in a variety of ways. Preferred implementations generate a signature that is relatively insensitive to modifications of video content that have little or no perceptual effect. If modifications to video content have no significant effect on the perceived image, then preferably these modifications also have no significant effect on the generated signature. Some measure of difference between two video signatures is commensurate with a measure of difference between the two contents from which the signatures are generated. A few exemplary implementations are discussed below.</p>
<p id="p-0024" num="0023">The implementations discussed below calculate intermediate values representing one or more measures of dissimilarity between video frames because the use of dissimilarity measures increases the sensitivity of the generated signature to motion and other changes in original content but eliminates or reduces sensitivity to modifications from subsequent processes such as those that change brightness or contrast, that convert between different color spaces or that apply color correction.</p>
<p id="p-0025" num="0024">The intermediate values may be calculated for any two frames <b>1</b><i>a</i>, <b>1</b><i>b </i>of video content. The two frames may be adjacent video frames within a sequence of frames or they may be separated from one another by one or more intervening frames. If the two frames are separated by a specified interval of time rather than by a specified number of intervening frames, the intermediate values calculated for these two frames will generally be more resistant to modifications caused by coding process that change the video frame rate.</p>
<heading id="h-0008" level="1">1. Dissimilarity Measure Processor</heading>
<p id="p-0026" num="0025">A few exemplary implementations of the dissimilarity measure processor <b>120</b> are illustrated in <figref idref="DRAWINGS">FIGS. 4A to 4C</figref>. Referring to <figref idref="DRAWINGS">FIG. 4A</figref>, the component <b>122</b><i>a </i>forms one or more groups of pixels from the video frame <b>1</b><i>a </i>and the component <b>124</b><i>a </i>extracts one or more features from each of these groups of pixels and calculates a value R representing each feature. The component <b>122</b><i>b </i>forms one or more groups of pixels from the video frame <b>1</b><i>b </i>and the component <b>124</b><i>b </i>extracts one or more features from each of these groups of pixels and calculates a value R representing each feature. The component <b>126</b> calculates intermediate values Q that represent the dissimilarity measures between the values R for corresponding features and corresponding groups of pixels in the two video frames <b>1</b><i>a</i>, <b>1</b><i>b. </i></p>
<heading id="h-0009" level="1">a) Pixel Group Formation</heading>
<p id="p-0027" num="0026">The components <b>122</b><i>a </i>and <b>122</b><i>b </i>may form pixel groups in essentially any way that may be desired. A few alternatives are discussed below. If desired, the information in the video frame that is used to generate the video signature may be limited to only a portion of the total image to avoid changes created by any processes that add letterboxes or graphics to edges or corners of the image. This may be achieved in a variety of ways such as by cropping the image prior to feature extraction, by cropping the array of values R that represent the extracted features after they have been calculated, or by cropping the array of dissimilarity values calculated from the values R. Preferably, this is achieved by cropping the image prior to feature extraction.</p>
<p id="p-0028" num="0027">For video applications such as television, a suitable cropping selects a central portion of the image so that any logos or other graphical objects inserted into the video content near the edges of the image do not affect the extracted features. Cropping may also eliminate modifications to the image due to conversion between progressive-scan and interlaced-scan formats and between high-definition (HD) and standard definition (SD) formats. Cropping for one particular HD to SD format conversion is discussed in the following paragraphs.</p>
<p id="p-0029" num="0028">If original video content in HD format with a resolution of 1080&#xd7;1920 pixels is converted into SD format with a resolution of 480&#xd7;640 pixels, for example, the original image can be cropped to select the central portion of the original image that remains in the converted image. An appropriate cropping removes 240 pixels from the left-hand edge and removes 240 pixels from right-hand edge of the original HD-format image to obtain an image with a resolution of 1080&#xd7;1440 pixels having the same aspect ratio as the SD-format image. The cropped area may be adjusted to remove additional areas of the image that may be modified with logos or graphical objects as mentioned above.</p>
<p id="p-0030" num="0029">The array of pixels may also be down-sampled to reduce sensitivity to modifications that can occur when frames of video are converted between different formats. In television applications, for example, the images may be down sampled to a resolution of 120&#xd7;160 pixels, which is a convenient choice for HD and SD formats, and for progressive-scan an interlaced-scan formats. This downsampling may be implemented by examining parameters or other metadata conveyed with the video content to determine the horizontal and vertical resolution of the images conveyed in the video frames, selecting a factor in response to these resolutions, and downsampling the images by an amount equal to the factor. For the examples discussed here, a factor equal to nine is selected for the cropped HD-format image and a factor equal to four is selected for the SD-format image.</p>
<p id="p-0031" num="0030">For example, suppose the content of an original video signal is in HD format with a resolution of 1080&#xd7;1920 pixels. This content can be cropped to an image having a resolution of 1080&#xd7;1440 pixels as described above and then down sampled by a factor of nine to a resolution of 120&#xd7;160 pixels. Features can be extracted from this low-resolution image. Suppose further that the original video signal is converted to SD format with a resolution of 480&#xd7;640 pixels. This converted image can be down sampled by a factor of four to a resolution of 120&#xd7;160 pixels, allowing essentially the same features to be extracted from the converted signal as was done for the original signal. The same down sampling can be used to accommodate conversions from SD to HD formats and between progressive-scan and interlaced-scan formats. If appropriate down sampling is used, the feature extraction process and the subsequent signature generation process are insensitive to modifications that occur from conversions between formats.</p>
<p id="p-0032" num="0031">If a video signal conveys content in an interlaced-scan format in which frames of video are arranged in two fields, it may be converted to a progressive-scan format before extracting features. Alternatively, greater independence from the choice of scan format can be achieved by extracting features from only one of the fields in an interlaced-scan frame. For example, features can be extracted from only the first field in a frame or from only the second field in the frame. Video content in the other field would be ignored. This process avoids the need to convert to a progressive-scan format before extracting features.</p>
<p id="p-0033" num="0032">In one implementation, pixel groups are formed in a down-sampled image having a resolution of 120&#xd7;160 pixels. Referring to <figref idref="DRAWINGS">FIG. 5A</figref>, for example, the pixel groups are uniform in size and are GX pixels wide and GY pixels high. The horizontal size GX of the groups is chosen such that K&#xb7;GX=RH and the vertical size GY of the groups is chosen such that L&#xb7;GY=RV where RH and RV are the horizontal and vertical dimensions of the image, respectively, in each video frame. One suitable choice of values is GX=8, GY=8, K=15 and L=20. This describes a 15&#xd7;20 array of groups, each group having a size of 8&#xd7;8 pixels in the down-sampled image.</p>
<p id="p-0034" num="0033">A similar result can be obtained by forming pixel groups in the original image with a size that is adjusted in response to the format of the image in the video frame. Continuing the examples described above, HD-format images are cropped to a size of 1080&#xd7;1440 pixels and pixel groups are formed in the cropped image having a size of 72&#xd7;72 pixels. This yields a 15&#xd7;20 array of pixel groups. For images in SD format, pixel groups are formed in the original image having a size of 32&#xd7;32 pixels, which yields a 15&#xd7;20 array of pixel groups.</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 5B</figref> illustrates groups of pixels that are not uniform in size. A 6&#xd7;4 array of smaller pixel groups constitutes the central portion of the image. A set of larger pixel groups surrounds the groups in the central portion. This type of arrangement can be used advantageously with video frame information that has content in the central portion of each image that is perceptually more significant.</p>
<p id="p-0036" num="0035">Pixel groups may be of essentially any size or shape. For example, the central portion of the image shown in <figref idref="DRAWINGS">FIG. 5B</figref>, which is surrounded by a rectangle drawn with wider lines, could constitute a single pixel group and the remaining portion of the image could constitute another pixel group.</p>
<p id="p-0037" num="0036">Preferably, the pixels are low-pass filtered to reduce sensitivity to changes caused by any variations in pixel group alignment that may occur as the result of video content modification. The filtering may be performed one or more times during the pixel group formation process. For example, the pixels may be low-pass filtered before the down-sampling operations discussed above, immediately after the down-sampling operations and/or immediately after the formation of pixel groups. The size of the filter should be chosen to balance a trade off between resistance to changes in alignment on one hand and sensitivity to changes in video content on the other hand. A larger filter increases resistance to changes in alignment. A smaller filter increases the sensitivity to changes in video content. If the low-pass filter is applied to the 120&#xd7;160 down-sampled image discussed above, empirical studies have shown good results can be obtained by using a 3&#xd7;3 two-dimensional filter with all filter tap coefficients equal to one.</p>
<p id="p-0038" num="0037">The following discussion of feature extraction refers to the exemplary grouping shown in <figref idref="DRAWINGS">FIG. 5A</figref>.</p>
<heading id="h-0010" level="1">b) Feature Extraction</heading>
<p id="p-0039" num="0038">The components <b>124</b><i>a </i>and <b>124</b><i>b </i>extract one or more features from each pixel group and calculate a value R that represents each feature.</p>
<p id="p-0040" num="0039">If each video frame conveys a monochromatic image, the features may be extracted from data e that represents the intensities of individual pixels. If each video frame conveys a color image comprising pixels represented by red, green and blue (RGB) values, for example, separate features may be extracted from the data e that represents each of the red, green, and blue pixel components. Alternatively, features may be extracted from data e that represents pixel luminance or brightness derived from the data that represents the red, green, and blue components.</p>
<p id="p-0041" num="0040">One feature that may be extracted is average pixel intensity. A value R<sub>AVE </sub>representing this feature may be obtained from the following expression:</p>
<p id="p-0042" num="0041">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mrow>
            <msub>
              <mi>R</mi>
              <mi>AVE</mi>
            </msub>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mi>k</mi>
                <mo>,</mo>
                <mi>l</mi>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>=</mo>
          <mrow>
            <mfrac>
              <mn>1</mn>
              <mrow>
                <mi>GX</mi>
                <mo>&#xb7;</mo>
                <mi>GY</mi>
              </mrow>
            </mfrac>
            <mo>&#x2062;</mo>
            <mrow>
              <munderover>
                <mo>&#x2211;</mo>
                <mrow>
                  <mi>i</mi>
                  <mo>=</mo>
                  <mrow>
                    <mi>k</mi>
                    <mo>&#xb7;</mo>
                    <mi>GX</mi>
                  </mrow>
                </mrow>
                <mrow>
                  <mrow>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mi>k</mi>
                        <mo>+</mo>
                        <mn>1</mn>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                    <mo>&#xb7;</mo>
                    <mi>GX</mi>
                  </mrow>
                  <mo>-</mo>
                  <mn>1</mn>
                </mrow>
              </munderover>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mrow>
                <munderover>
                  <mo>&#x2211;</mo>
                  <mrow>
                    <mi>j</mi>
                    <mo>=</mo>
                    <mrow>
                      <mi>l</mi>
                      <mo>&#xb7;</mo>
                      <mi>GY</mi>
                    </mrow>
                  </mrow>
                  <mrow>
                    <mrow>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <mi>l</mi>
                          <mo>+</mo>
                          <mn>1</mn>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                      <mo>&#xb7;</mo>
                      <mi>GY</mi>
                    </mrow>
                    <mo>-</mo>
                    <mn>1</mn>
                  </mrow>
                </munderover>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mrow>
                  <mi>e</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mi>i</mi>
                      <mo>,</mo>
                      <mi>j</mi>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
        <mo>&#x2062;</mo>
        <mstyle>
          <mtext>
</mtext>
        </mstyle>
        <mo>&#x2062;</mo>
        <mrow>
          <mrow>
            <mrow>
              <mi>for</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.8em" height="0.8ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mn>0</mn>
            </mrow>
            <mo>&#x2264;</mo>
            <mi>k</mi>
            <mo>&#x3c;</mo>
            <mi>K</mi>
          </mrow>
          <mo>;</mo>
          <mrow>
            <mn>0</mn>
            <mo>&#x2264;</mo>
            <mi>l</mi>
            <mo>&#x3c;</mo>
            <mi>L</mi>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>1</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where R<sub>AVE</sub>(k, l)=average intensity of pixels in a group of pixels (k, l);
</p>
<p id="p-0043" num="0042">e(i, j)=intensity of pixel (i, j) within the group;</p>
<p id="p-0044" num="0043">GX=width of pixel groups expressed in numbers of pixels;</p>
<p id="p-0045" num="0044">GY=height of pixel groups expressed in numbers of pixels;</p>
<p id="p-0046" num="0045">K=horizontal resolution of the image, expressed in numbers of groups; and</p>
<p id="p-0047" num="0046">L=vertical resolution of the image, expressed in numbers of groups.</p>
<p id="p-0048" num="0047">Another feature that may be extracted is the standard deviation of pixel intensity. Alternatively, the variance or square of the standard devation may be used. A value R<sub>SD </sub>representing standard deviation may be obtained from the following expression:</p>
<p id="p-0049" num="0048">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mrow>
            <msub>
              <mi>R</mi>
              <mi>SD</mi>
            </msub>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mi>k</mi>
                <mo>,</mo>
                <mi>l</mi>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>=</mo>
          <msqrt>
            <mrow>
              <mfrac>
                <mn>1</mn>
                <mrow>
                  <mi>GX</mi>
                  <mo>&#xb7;</mo>
                  <mi>GY</mi>
                </mrow>
              </mfrac>
              <mo>&#x2062;</mo>
              <mrow>
                <munderover>
                  <mo>&#x2211;</mo>
                  <mrow>
                    <mi>i</mi>
                    <mo>=</mo>
                    <mrow>
                      <mi>k</mi>
                      <mo>&#xb7;</mo>
                      <mi>GX</mi>
                    </mrow>
                  </mrow>
                  <mrow>
                    <mrow>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <mi>k</mi>
                          <mo>+</mo>
                          <mn>1</mn>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                      <mo>&#xb7;</mo>
                      <mi>GX</mi>
                    </mrow>
                    <mo>-</mo>
                    <mn>1</mn>
                  </mrow>
                </munderover>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mrow>
                  <munderover>
                    <mo>&#x2211;</mo>
                    <mrow>
                      <mi>j</mi>
                      <mo>=</mo>
                      <mrow>
                        <mi>l</mi>
                        <mo>&#xb7;</mo>
                        <mi>GY</mi>
                      </mrow>
                    </mrow>
                    <mrow>
                      <mrow>
                        <mrow>
                          <mo>(</mo>
                          <mrow>
                            <mi>l</mi>
                            <mo>+</mo>
                            <mn>1</mn>
                          </mrow>
                          <mo>)</mo>
                        </mrow>
                        <mo>&#xb7;</mo>
                        <mi>GY</mi>
                      </mrow>
                      <mo>-</mo>
                      <mn>1</mn>
                    </mrow>
                  </munderover>
                  <mo>&#x2062;</mo>
                  <mstyle>
                    <mspace width="0.3em" height="0.3ex"/>
                  </mstyle>
                  <mo>&#x2062;</mo>
                  <msup>
                    <mrow>
                      <mo>[</mo>
                      <mrow>
                        <mrow>
                          <mi>e</mi>
                          <mo>&#x2061;</mo>
                          <mrow>
                            <mo>(</mo>
                            <mrow>
                              <mi>i</mi>
                              <mo>,</mo>
                              <mi>j</mi>
                            </mrow>
                            <mo>)</mo>
                          </mrow>
                        </mrow>
                        <mo>-</mo>
                        <mrow>
                          <msub>
                            <mi>R</mi>
                            <mi>AVE</mi>
                          </msub>
                          <mo>&#x2061;</mo>
                          <mrow>
                            <mo>(</mo>
                            <mrow>
                              <mi>k</mi>
                              <mo>,</mo>
                              <mi>l</mi>
                            </mrow>
                            <mo>)</mo>
                          </mrow>
                        </mrow>
                      </mrow>
                      <mo>]</mo>
                    </mrow>
                    <mn>2</mn>
                  </msup>
                </mrow>
              </mrow>
            </mrow>
          </msqrt>
        </mrow>
        <mo>&#x2062;</mo>
        <mstyle>
          <mtext>
</mtext>
        </mstyle>
        <mo>&#x2062;</mo>
        <mstyle>
          <mspace width="1.1em" height="1.1ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mrow>
          <mrow>
            <mrow>
              <mi>for</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.8em" height="0.8ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mn>0</mn>
            </mrow>
            <mo>&#x2264;</mo>
            <mi>k</mi>
            <mo>&#x3c;</mo>
            <mi>K</mi>
          </mrow>
          <mo>;</mo>
          <mrow>
            <mn>0</mn>
            <mo>&#x2264;</mo>
            <mi>l</mi>
            <mo>&#x3c;</mo>
            <mi>L</mi>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>2</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where R<sub>SD</sub>(k, l)=standard deviation of pixel intensities in a group of pixels (k, l).
</p>
<p id="p-0050" num="0049">Another feature that may be extracted is a histogram of pixel intensities. A set of values R<sub>HIST </sub>that represents this feature may be obtained by counting the number of pixels that have a particular intensity for each intensity within the range of possible intensities.</p>
<p id="p-0051" num="0050">Yet another feature is the amplitude and/or phase of the spectrum. A set of values R<sub>SPECTRUM </sub>representing a spectrum can be obtained by applying a two-dimensional Fourier transform to the group of pixel intensities.</p>
<p id="p-0052" num="0051">No particular feature is critical to the present invention; however, empirical results have shown that averages and standard deviations of pixel intensities are good choices for many applications.</p>
<p id="p-0053" num="0052">If desired, the values R representing extracted features may be arranged in groups for subsequent processing. For example, spectral features represented by the set of values R<sub>SPECTRUM </sub>may be organized into groups according to frequency or phase.</p>
<p id="p-0054" num="0053">Furthermore, features may be extracted from the calculated values R. For example, the standard deviation of average intensity R<sub>AVE </sub>or of spectral values R<sub>SPECTRUM </sub>may be calculated.</p>
<heading id="h-0011" level="1">c) Dissimilarity Measure Calculation</heading>
<p id="p-0055" num="0054">The component <b>126</b> may calculate the intermediate values Q representing measures of dissimilarity E in a variety of ways. The choice of the measure is not critical in principle to the present invention but some measures may perform better depending upon the features that are extracted by the components <b>124</b><i>a </i>and <b>124</b><i>b</i>. Empirical studies may be needed to make an appropriate choice; however, the two measures described below have been found to give good results in a wide range of applications.</p>
<p id="p-0056" num="0055">One measure of dissimilarity is the absolute value of the difference between values R representing corresponding features for corresponding groups of pixels in two different frames. This measure may be calculated from the following expression:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>E</i>(<i>k,l,f</i><sub>1</sub><i>,f</i><sub>2</sub>)=|<i>R</i>(<i>k,l,f</i><sub>1</sub>)&#x2212;<i>R</i>(<i>k,l,f</i><sub>2</sub>)| for 0&#x2266;<i>k&#x3c;K; </i>0&#x2266;<i>l&#x3c;L</i>&#x2003;&#x2003;(3<i>a</i>)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where E(k,l, f<sub>1</sub>, f<sub>2</sub>)=dissimilarity between frames f<sub>1 </sub>and f<sub>2 </sub>in pixel group (k, l); and
</p>
<p id="p-0057" num="0056">R(k, l, x)=value representing extracted feature of pixel group (k, l) in frame x.</p>
<p id="p-0058" num="0057">If the extracted feature is represented by a value having two or more elements such as, for example, amplitudes in R<sub>SPECTRUM </sub>representing a spectral feature, a measure of dissimilarity may be calculated from the sum of the absolute values of differences between elements in the values R representing corresponding features for corresponding groups of pixels in two different frames. This measure may be calculated from the following expression:</p>
<p id="p-0059" num="0058">
<maths id="MATH-US-00003" num="00003">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mrow>
            <mi>E</mi>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mi>k</mi>
                <mo>,</mo>
                <mi>l</mi>
                <mo>,</mo>
                <msub>
                  <mi>f</mi>
                  <mn>1</mn>
                </msub>
                <mo>,</mo>
                <msub>
                  <mi>f</mi>
                  <mn>2</mn>
                </msub>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>=</mo>
          <mrow>
            <munder>
              <mo>&#x2211;</mo>
              <mi>z</mi>
            </munder>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mrow>
              <mo>&#xf603;</mo>
              <mrow>
                <mrow>
                  <mi>R</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mi>k</mi>
                      <mo>,</mo>
                      <mi>l</mi>
                      <mo>,</mo>
                      <mi>z</mi>
                      <mo>,</mo>
                      <msub>
                        <mi>f</mi>
                        <mn>1</mn>
                      </msub>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>-</mo>
                <mrow>
                  <mi>R</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mi>k</mi>
                      <mo>,</mo>
                      <mi>l</mi>
                      <mo>,</mo>
                      <mi>z</mi>
                      <mo>,</mo>
                      <msub>
                        <mi>f</mi>
                        <mn>2</mn>
                      </msub>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
              <mo>&#xf604;</mo>
            </mrow>
          </mrow>
        </mrow>
        <mo>&#x2062;</mo>
        <mstyle>
          <mtext>
</mtext>
        </mstyle>
        <mo>&#x2062;</mo>
        <mrow>
          <mrow>
            <mrow>
              <mi>for</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.8em" height="0.8ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mn>0</mn>
            </mrow>
            <mo>&#x2264;</mo>
            <mi>k</mi>
            <mo>&#x3c;</mo>
            <mi>K</mi>
          </mrow>
          <mo>;</mo>
          <mrow>
            <mn>0</mn>
            <mo>&#x2264;</mo>
            <mi>l</mi>
            <mo>&#x3c;</mo>
            <mi>L</mi>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mn>3</mn>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mi>b</mi>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where R(k, l, z, x)=element z in the value R for pixel group (k, l) in frame x.
</p>
<p id="p-0060" num="0059">If desired, a composite measure of dissimilarity for two or more groups of pixels in the frames may be calculated from a similar expression as follows:</p>
<p id="p-0061" num="0060">
<maths id="MATH-US-00004" num="00004">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>E</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <msub>
                <mi>f</mi>
                <mn>1</mn>
              </msub>
              <mo>,</mo>
              <msub>
                <mi>f</mi>
                <mn>2</mn>
              </msub>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <munder>
            <mo>&#x2211;</mo>
            <mi>k</mi>
          </munder>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mrow>
            <munder>
              <mo>&#x2211;</mo>
              <mi>l</mi>
            </munder>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mrow>
              <munder>
                <mo>&#x2211;</mo>
                <mi>z</mi>
              </munder>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mrow>
                <mo>&#xf603;</mo>
                <mrow>
                  <mrow>
                    <mi>R</mi>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mi>k</mi>
                        <mo>,</mo>
                        <mi>l</mi>
                        <mo>,</mo>
                        <mi>z</mi>
                        <mo>,</mo>
                        <msub>
                          <mi>f</mi>
                          <mn>1</mn>
                        </msub>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mo>-</mo>
                  <mrow>
                    <mi>R</mi>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mi>k</mi>
                        <mo>,</mo>
                        <mi>l</mi>
                        <mo>,</mo>
                        <mi>z</mi>
                        <mo>,</mo>
                        <msub>
                          <mi>f</mi>
                          <mn>2</mn>
                        </msub>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                </mrow>
                <mo>&#xf604;</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mn>3</mn>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mi>c</mi>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where E(f<sub>1</sub>, f<sub>2</sub>)=composite measure dissimilarity between frames f<sub>1 </sub>and f<sub>2</sub>; and
</p>
<p id="p-0062" num="0061">the limits of summation for k and l are chosen to include the desired groups. This particular example assumes the values R have more than one element. If the values have only one element, the summation over z is omitted.</p>
<p id="p-0063" num="0062">Another measure of dissimilarity is the square of the difference between values R representing corresponding features for corresponding groups of pixels in two different frames. This measure may be calculated from the following expression:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>E</i>(<i>k,l,f</i><sub>1</sub><i>,f</i><sub>2</sub>)=(<i>R</i>(<i>k,l,f</i><sub>1</sub>)&#x2212;<i>R</i>(<i>k,l,f</i><sub>2</sub>))<sup>2 </sup>for 0&#x2266;<i>k&#x3c;K; </i>0&#x2266;<i>l&#x3c;L</i>&#x2003;&#x2003;(4<i>a</i>)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0064" num="0063">If the extracted feature is represented by a value having two or more elements, a measure of dissimilarity may be calculated from the sum of the squares of differences between elements in the values R representing corresponding features for corresponding groups of pixels in two different frames. This measure may be calculated from the following expression:</p>
<p id="p-0065" num="0064">
<maths id="MATH-US-00005" num="00005">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mrow>
            <mi>E</mi>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mi>k</mi>
                <mo>,</mo>
                <mi>l</mi>
                <mo>,</mo>
                <msub>
                  <mi>f</mi>
                  <mn>1</mn>
                </msub>
                <mo>,</mo>
                <msub>
                  <mi>f</mi>
                  <mn>2</mn>
                </msub>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>=</mo>
          <mrow>
            <munder>
              <mo>&#x2211;</mo>
              <mi>z</mi>
            </munder>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <msup>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mrow>
                    <mi>R</mi>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mi>k</mi>
                        <mo>,</mo>
                        <mi>l</mi>
                        <mo>,</mo>
                        <mi>z</mi>
                        <mo>,</mo>
                        <msub>
                          <mi>f</mi>
                          <mn>1</mn>
                        </msub>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mo>-</mo>
                  <mrow>
                    <mi>R</mi>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mi>k</mi>
                        <mo>,</mo>
                        <mi>l</mi>
                        <mo>,</mo>
                        <mi>z</mi>
                        <mo>,</mo>
                        <msub>
                          <mi>f</mi>
                          <mn>2</mn>
                        </msub>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                </mrow>
                <mo>)</mo>
              </mrow>
              <mn>2</mn>
            </msup>
          </mrow>
        </mrow>
        <mo>&#x2062;</mo>
        <mstyle>
          <mtext>
</mtext>
        </mstyle>
        <mo>&#x2062;</mo>
        <mrow>
          <mrow>
            <mrow>
              <mi>for</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.8em" height="0.8ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mn>0</mn>
            </mrow>
            <mo>&#x2264;</mo>
            <mi>k</mi>
            <mo>&#x3c;</mo>
            <mi>K</mi>
          </mrow>
          <mo>;</mo>
          <mrow>
            <mn>0</mn>
            <mo>&#x2264;</mo>
            <mi>l</mi>
            <mo>&#x3c;</mo>
            <mi>L</mi>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mn>4</mn>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mi>b</mi>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0066" num="0065">If desired, a composite measure of dissimilarity for two or more groups of pixels in the frames may be calculated from the following expression:</p>
<p id="p-0067" num="0066">
<maths id="MATH-US-00006" num="00006">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>E</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <msub>
                <mi>f</mi>
                <mn>1</mn>
              </msub>
              <mo>,</mo>
              <msub>
                <mi>f</mi>
                <mn>2</mn>
              </msub>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <munder>
            <mo>&#x2211;</mo>
            <mi>k</mi>
          </munder>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mrow>
            <munder>
              <mo>&#x2211;</mo>
              <mi>l</mi>
            </munder>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mrow>
              <munder>
                <mo>&#x2211;</mo>
                <mi>z</mi>
              </munder>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <msup>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mrow>
                      <mi>R</mi>
                      <mo>&#x2061;</mo>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <mi>k</mi>
                          <mo>,</mo>
                          <mi>l</mi>
                          <mo>,</mo>
                          <mi>z</mi>
                          <mo>,</mo>
                          <msub>
                            <mi>f</mi>
                            <mn>1</mn>
                          </msub>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                    <mo>-</mo>
                    <mrow>
                      <mi>R</mi>
                      <mo>&#x2061;</mo>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <mi>k</mi>
                          <mo>,</mo>
                          <mi>l</mi>
                          <mo>,</mo>
                          <mi>z</mi>
                          <mo>,</mo>
                          <msub>
                            <mi>f</mi>
                            <mn>2</mn>
                          </msub>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                  </mrow>
                  <mo>)</mo>
                </mrow>
                <mn>2</mn>
              </msup>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mn>4</mn>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mi>c</mi>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0068" num="0067">where the limits of summation for k and l are chosen to include the desired groups. This particular example assumes the values R have more than one element. If the values have only one element, the summation over z is omitted.</p>
<p id="p-0069" num="0068">In one implementation, the intermediate values Q are set equal to the calculated measures of dissimilarity E. An alternative is discussed below.</p>
<heading id="h-0012" level="1">d) Alternative Implementation</heading>
<p id="p-0070" num="0069">If a difference between average pixel intensities is the only dissimilarity measure that is used for signature generation, the dissimilarity measure processor <b>120</b> may be implemented as shown in <figref idref="DRAWINGS">FIGS. 4B and 4C</figref>. In these implementations, pixel intensities or average intensities are extracted from the video frames <b>1</b><i>a </i>and <b>1</b><i>b</i>, measures of dissimilarity between the extracted features are calculated and the dissimilarity measures are formed into groups for subsequent signature generation.</p>
<p id="p-0071" num="0070">In the exemplary implementations shown in <figref idref="DRAWINGS">FIGS. 4B and 4C</figref>, frames of video content are represented by an array of discrete pixels, the dissimilarity measure processor <b>120</b> obtains difference images each comprising an array of differential elements &#x394; by calculating the difference between corresponding pixels in two video frames. If each video frame conveys a color image comprising pixels represented by red, green and blue (RGB) values, for example, the differential elements may be calculated from the differences between respective red, green, and blue values for corresponding pixels. Preferably, the differential elements are calculated from the absolute differences between a luminance or brightness of corresponding pixels that is derived from the red, green, and blue values. If each video frame conveys a monochromatic image, the differential elements may be calculated from the difference between the intensities of corresponding pixels.</p>
<p id="p-0072" num="0071">If desired, differential elements may be limited to only a portion of the total image to avoid changes created by any processes that add letterboxes or graphics to edges or corners of the image. This may be achieved by cropping the image prior to calculating the differential elements or by cropping the array of differential elements after they have been calculated.</p>
<p id="p-0073" num="0072">The resolution of the difference image may also be changed as described above for pixel group formation. This may be done by modifying data in the video frames prior to calculating the differential elements or by modifying the differential elements after they have been calculated.</p>
<p id="p-0074" num="0073">Referring to the implementation illustrated in <figref idref="DRAWINGS">FIG. 4C</figref>, the component <b>123</b> calculates the difference between values of corresponding pixels in a video frame <b>1</b><i>a </i>and a video frame <b>1</b><i>b </i>and the component <b>125</b> obtains a set of the differential elements &#x394; from the absolute values of the pixel differences. The component <b>127</b> performs cropping and down-sampling. The cropping operation retains only a central part of the difference image by removing differential elements near the top, bottom, right-hand and left-hand edges of the difference image. The down-sampling operation down samples the cropped difference image to generate an array of differential elements that has a specified size of 120&#xd7;160 pixels regardless the format of the input video frames. This particular size is only exemplary. The component <b>128</b> forms the differential elements &#x394; into groups and calculates the average value for the differential elements in each group. If desired, the down-sampling and group-formation operations could be combined as explained above.</p>
<p id="p-0075" num="0074">The operations that calculate differences, down-sample, crop and form groups may be performed in other orders. For example, the differential elements &#x394; may be calculated by first down sampling the contents of two video frames, cropping the two down-sampled images, forming groups of pixels in the cropped images, calculating an average intensity for the pixels in each group, and then calculating differences between corresponding average intensities in the two images.</p>
<p id="p-0076" num="0075">Referring to the group formation operation <b>128</b> shown in <figref idref="DRAWINGS">FIG. 4C</figref>, the differential elements &#x394; are grouped into regions of a difference image where each region is GX elements wide and GY elements high. Intermediate values Q are derived from the intensities of the differential elements &#x394; by calculating the average intensity of the elements in each region. These intermediate values constitute a low-resolution representation of the difference image that has a resolution of K&#xd7;L intermediate values. This is analogous to the formation of pixel groups discussed above and illustrated in <figref idref="DRAWINGS">FIGS. 5A and 5B</figref>. The exemplary implementation described in the following paragraphs uses a low-resolution image with elements that are arranged in the same way as the pixel groups shown in <figref idref="DRAWINGS">FIG. 5A</figref>.</p>
<p id="p-0077" num="0076">The intermediate values may be obtained from the following expression:</p>
<p id="p-0078" num="0077">
<maths id="MATH-US-00007" num="00007">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mrow>
            <mi>Q</mi>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mi>k</mi>
                <mo>,</mo>
                <mi>l</mi>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>=</mo>
          <mrow>
            <mfrac>
              <mn>1</mn>
              <mrow>
                <mi>GX</mi>
                <mo>&#xb7;</mo>
                <mi>GY</mi>
              </mrow>
            </mfrac>
            <mo>&#x2062;</mo>
            <mrow>
              <munderover>
                <mo>&#x2211;</mo>
                <mrow>
                  <mi>i</mi>
                  <mo>=</mo>
                  <mrow>
                    <mi>k</mi>
                    <mo>&#xb7;</mo>
                    <mi>GX</mi>
                  </mrow>
                </mrow>
                <mrow>
                  <mrow>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mi>k</mi>
                        <mo>+</mo>
                        <mn>1</mn>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                    <mo>&#xb7;</mo>
                    <mi>GX</mi>
                  </mrow>
                  <mo>-</mo>
                  <mn>1</mn>
                </mrow>
              </munderover>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mrow>
                <munderover>
                  <mo>&#x2211;</mo>
                  <mrow>
                    <mi>j</mi>
                    <mo>=</mo>
                    <mrow>
                      <mi>l</mi>
                      <mo>&#xb7;</mo>
                      <mi>GY</mi>
                    </mrow>
                  </mrow>
                  <mrow>
                    <mrow>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <mi>l</mi>
                          <mo>+</mo>
                          <mn>1</mn>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                      <mo>&#xb7;</mo>
                      <mi>GY</mi>
                    </mrow>
                    <mo>-</mo>
                    <mn>1</mn>
                  </mrow>
                </munderover>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mrow>
                  <mi>&#x394;</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mi>i</mi>
                      <mo>,</mo>
                      <mi>j</mi>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
        <mo>&#x2062;</mo>
        <mstyle>
          <mtext>
</mtext>
        </mstyle>
        <mo>&#x2062;</mo>
        <mrow>
          <mrow>
            <mrow>
              <mi>for</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.8em" height="0.8ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mn>0</mn>
            </mrow>
            <mo>&#x2264;</mo>
            <mi>k</mi>
            <mo>&#x3c;</mo>
            <mi>K</mi>
          </mrow>
          <mo>;</mo>
          <mrow>
            <mn>0</mn>
            <mo>&#x2264;</mo>
            <mi>l</mi>
            <mo>&#x3c;</mo>
            <mi>L</mi>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>5</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where Q(k ,l)=intermediate value in the low-resolution image;
</p>
<p id="p-0079" num="0078">GX=width of differential element groups expressed in numbers of elements;</p>
<p id="p-0080" num="0079">GY=height of differential element groups expressed in numbers of elements;</p>
<p id="p-0081" num="0080">K=horizontal resolution of the low-resolution image;</p>
<p id="p-0082" num="0081">L=vertical resolution of the low-resolution image; and</p>
<p id="p-0083" num="0082">&#x394;(i, j)=differential element.</p>
<p id="p-0084" num="0083">The horizontal size GX of the groups is chosen such that K&#xb7;GX=RH and the vertical size GY of the groups is chosen such that L&#xb7;GY=RV where RH and RV are the horizontal and vertical resolutions of the difference image, respectively. For the exemplary implementation discussed above that generates elements in a down-sampled difference image with a resolution of 120&#xd7;160, one suitable size for the groups is 8&#xd7;8 pixels, which provides a low-resolution image with a resolution of 120/8&#xd7;160/8=15&#xd7;20. By using the lower-resolution intermediate values Q to generate the video signature rather than the higher-resolution differential elements, the generated video signature is less sensitive to processes that change details of video signal content but preserve average intensity.</p>
<heading id="h-0013" level="1">2. Video Signature Processor</heading>
<p id="p-0085" num="0084">The implementation of the video signature processor <b>170</b> that is described in the following paragraphs generates a video signature from a K&#xd7;L array of intermediate values Q obtained from either the array of values R shown in <figref idref="DRAWINGS">FIG. 5A</figref> or from a K&#xd7;L array of differential elements &#x394; as described above in connection with the processor illustrated in <figref idref="DRAWINGS">FIG. 4C</figref>.</p>
<p id="p-0086" num="0085">The video signature processor <b>170</b> applies a hash function to a K&#xd7;L array of the intermediate values Q to generate a set of N hash bits. These hash bits constitute the video signature (SV) that identifies the content of the video frames. Preferably, the hash function is relatively insensitive to changes in the intermediate values but may be sensitive to changes in any hash key that may be used. Unlike a typical cryptographic hash function whose output changes significantly with a change to even a single bit of its input, a preferred hash function for this application provides an output that undergoes only small changes for small changes in the input intermediate values. This allows the generated video signature to change only slightly with small changes to video signal content.</p>
<p id="p-0087" num="0086">One suitable hash function uses a set of N base matrices to generate a set of N hash bits. The base matrices P<sub>1 </sub>to P<sub>N </sub>are K&#xd7;L arrays of random-valued matrix elements. The matrix elements p<sub>n</sub>(k, l) of each base matrix P<sub>n </sub>may be generated from the following expression:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>p</i><sub>n</sub>(<i>k,l</i>)=<i>RNG&#x2212; <o ostyle="single">p</o></i><sub>n </sub>for 1&#x2266;<i>n&#x2266;N, </i>0&#x2266;<i>k&#x3c;K, </i>0&#x2266;<i>l&#x3c;L</i>&#x2003;&#x2003;(6)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where p<sub>n</sub>(k, l)=matrix element (k, l) of base matrix P<sub>n</sub>;
</p>
<p id="p-0088" num="0087">RNG=output of a random-number generator; and</p>
<p id="p-0089" num="0088"> <o ostyle="single">p</o><sub>n</sub>=average value of the numbers generated by RNG for each interim matrix. The generator RNG generates random or pseudo-random values that are uniformly distributed in the range [0,1]. The initial state of the generator may be initialized by a hash key, thereby allowing the hash function and the generated video signature to be more cryptographically secure.</p>
<p id="p-0090" num="0089">The set of N hash bits is obtained by first projecting the interim values Q onto each of the N base matrices, which may be expressed as:</p>
<p id="p-0091" num="0090">
<maths id="MATH-US-00008" num="00008">
<math overflow="scroll">
<mrow>
  <msub>
    <mi>H</mi>
    <mi>n</mi>
  </msub>
  <mo>=</mo>
  <mrow>
    <mrow>
      <munderover>
        <mo>&#x2211;</mo>
        <mrow>
          <mi>k</mi>
          <mo>=</mo>
          <mn>0</mn>
        </mrow>
        <mrow>
          <mi>K</mi>
          <mo>-</mo>
          <mn>1</mn>
        </mrow>
      </munderover>
      <mo>&#x2062;</mo>
      <mstyle>
        <mspace width="0.3em" height="0.3ex"/>
      </mstyle>
      <mo>&#x2062;</mo>
      <mrow>
        <munderover>
          <mo>&#x2211;</mo>
          <mrow>
            <mi>l</mi>
            <mo>=</mo>
            <mn>0</mn>
          </mrow>
          <mrow>
            <mi>L</mi>
            <mo>-</mo>
            <mn>1</mn>
          </mrow>
        </munderover>
        <mo>&#x2062;</mo>
        <mstyle>
          <mspace width="0.3em" height="0.3ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mrow>
          <mrow>
            <mrow>
              <mi>Q</mi>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>k</mi>
                  <mo>,</mo>
                  <mi>l</mi>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>&#xb7;</mo>
            <mrow>
              <msub>
                <mi>p</mi>
                <mi>n</mi>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>k</mi>
                  <mo>,</mo>
                  <mi>l</mi>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mi>for</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>1</mn>
        </mrow>
      </mrow>
    </mrow>
    <mo>&#x2264;</mo>
    <mi>n</mi>
    <mo>&#x2264;</mo>
    <mi>N</mi>
  </mrow>
</mrow>
</math>
</maths>
<br/>
where H<sub>n</sub>=the projection of the interim values onto the base matrix P<sub>n</sub>.
</p>
<p id="p-0092" num="0091">The hash bits are then obtained by comparing each projection to the median value of all projections and setting the hash bit to a first value if the projection is equal to or exceeds the threshold and setting the hash bit to a second value if the projection is less than the threshold. This may be expressed as:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>B</i><sub>n</sub><i>=sgn</i>(<i>H</i><sub>n</sub><i>&#x2212; <o ostyle="single">H</o></i>)&#x2003;&#x2003;(7)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where
</p>
<p id="p-0093" num="0092">
<maths id="MATH-US-00009" num="00009">
<math overflow="scroll">
<mrow>
  <mrow>
    <mi>sgn</mi>
    <mo>&#x2061;</mo>
    <mrow>
      <mo>(</mo>
      <mi>x</mi>
      <mo>)</mo>
    </mrow>
  </mrow>
  <mo>=</mo>
  <mrow>
    <mo>[</mo>
    <mtable>
      <mtr>
        <mtd>
          <mn>0</mn>
        </mtd>
        <mtd>
          <mrow>
            <mrow>
              <mi>for</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.8em" height="0.8ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mi>x</mi>
            </mrow>
            <mo>&#x3c;</mo>
            <mn>0</mn>
          </mrow>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mn>1</mn>
        </mtd>
        <mtd>
          <mrow>
            <mrow>
              <mi>for</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.8em" height="0.8ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mi>x</mi>
            </mrow>
            <mo>&#x2265;</mo>
            <mn>0</mn>
          </mrow>
        </mtd>
      </mtr>
    </mtable>
    <mo>]</mo>
  </mrow>
</mrow>
</math>
</maths>
<br/>
and
</p>
<p id="p-0094" num="0093"> <o ostyle="single">H</o>=median value of all projections H<sub>n</sub>.</p>
<heading id="h-0014" level="1">C. Audio Signature Generator</heading>
<p id="p-0095" num="0094">The components of the audio signature generator <b>200</b> may be implemented in a variety of ways. Preferred implementations generate a signature that is relatively insensitive to modifications of audio content that have little or no perceptual effect. If modifications to audio content have no significant effect on the perceived sound, then preferably these modifications also have no significant effect on the generated signature. Some measure of difference between two audio signatures is commensurate with a measure of difference between the two contents from which the signatures are generated. A few suitable implementations are discussed below.</p>
<heading id="h-0015" level="1">1. Time-Frequency Representation Processor</heading>
<p id="p-0096" num="0095">In an exemplary implementation in which segments of an audio signal are represented by values expressing the amplitude of discrete samples, the time-frequency representation processor <b>210</b> obtains a set of spectral values from transform coefficients generated by applying a time-domain to frequency-domain transform to a sequence of overlapping blocks of audio samples within each segment. If desired, the spectral values may be limited to only a portion of the total bandwidth of the audio content to avoid changes created by any processes that alter the spectral shape of the audio content. For example, a limited representation may be obtained by excluding those transform coefficients generated by the transform that represent the lowest-frequency and highest-frequency spectral components or by bandpass filtering the audio content prior to application of the transform.</p>
<p id="p-0097" num="0096">The operations performed by an exemplary implementation of the time-frequency representation processor <b>210</b> are illustrated schematically in <figref idref="DRAWINGS">FIG. 6</figref>. In this implementation, a segment of audio content <b>2</b><i>a </i>is divided into a sequence of T overlapping blocks BLOCK-<b>1</b> to BLOCK-T. The length of each segment is LS samples and the length of each block is LB samples. The offset between the start of adjacent blocks is a number of samples ZB referred to as the block step size. A block transform is applied to each block of samples to generate a set of spectral values. <figref idref="DRAWINGS">FIG. 6</figref> illustrates a transformation of one block BLOCK-<b>2</b> by a transform <b>20</b> into a set of spectral values <b>25</b><i>a</i>. In this particular implementation, processing continues with the blocks in the next segment <b>2</b><i>b </i>after all T blocks in the segment <b>2</b><i>a </i>have been transformed into respective sets of spectral values. The offset between the start of adjacent segments is a number of samples ZS that is referred to as the segment step size.</p>
<p id="p-0098" num="0097">The time resolution of the time-frequency representation is a function of the segment length, block length and block step size, which may be expressed as follows:</p>
<p id="p-0099" num="0098">
<maths id="MATH-US-00010" num="00010">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mi>T</mi>
        <mo>=</mo>
        <mfrac>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mi>LS</mi>
              <mo>-</mo>
              <mi>LB</mi>
            </mrow>
            <mo>)</mo>
          </mrow>
          <mi>ZB</mi>
        </mfrac>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>8</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where T=time resolution or number of blocks in each segment;
</p>
<p id="p-0100" num="0099">LS=length of each segment in samples;</p>
<p id="p-0101" num="0100">LS=length of each block in samples; and</p>
<p id="p-0102" num="0101">ZB=block step size.</p>
<p id="h-0016" num="0000">The frequency resolution is generally determined by the length of the block or the length of the transform that is used to generate the spectral values.</p>
<p id="p-0103" num="0102">In one application discussed below that synchronizes video and audio content, audio content is divided into segments that are equal to the length of three video frames. For some television applications, the time interval spanned by three video frames is approximately 100 msec. If the audio sample rate is 48 kHz, then the audio segment length is 4,800 samples. The block length is chosen to be 256 samples and the block step size is chosen to be 32 samples. For this implementation, each audio segment has T=142 blocks; therefore, the time resolution of the time-frequency representation is equal to 142. A 256-point Fast Fourier Transform (FFT) is applied to each block of samples to generate 129 spectral values; therefore, the frequency resolution of the time-frequency representation is equal to 129. The segment step size is chosen to be 512 samples or approximately 10.7 msec.</p>
<heading id="h-0017" level="1">2. Intermediate Values Processor</heading>
<p id="p-0104" num="0103">The intermediate values processor <b>250</b> examines groups of the spectral values and derives an intermediate value from the intensities of the spectral values in each group.</p>
<p id="p-0105" num="0104">In an exemplary implementation, the spectral values S are grouped into time-frequency regions where each region is GF spectral values wide and GT blocks long. Intermediate values Q are derived from the intensities of the spectral values by calculating the average intensity of the spectral values in each region. These intermediate values constitute a low-resolution time-frequency representation that has a resolution of K&#xd7;L intermediate values. This is illustrated schematically in <figref idref="DRAWINGS">FIG. 7</figref>. The intermediate values may be obtained from the following expression:</p>
<p id="p-0106" num="0105">
<maths id="MATH-US-00011" num="00011">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mrow>
            <mi>Q</mi>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mi>k</mi>
                <mo>,</mo>
                <mi>l</mi>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>=</mo>
          <mrow>
            <mfrac>
              <mn>1</mn>
              <mrow>
                <mi>GF</mi>
                <mo>&#xb7;</mo>
                <mi>GT</mi>
              </mrow>
            </mfrac>
            <mo>&#x2062;</mo>
            <mrow>
              <munderover>
                <mo>&#x2211;</mo>
                <mrow>
                  <mi>i</mi>
                  <mo>=</mo>
                  <mrow>
                    <mi>k</mi>
                    <mo>&#xb7;</mo>
                    <mi>GF</mi>
                  </mrow>
                </mrow>
                <mrow>
                  <mrow>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mi>k</mi>
                        <mo>+</mo>
                        <mn>1</mn>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                    <mo>&#xb7;</mo>
                    <mi>GF</mi>
                  </mrow>
                  <mo>-</mo>
                  <mn>1</mn>
                </mrow>
              </munderover>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mrow>
                <munderover>
                  <mo>&#x2211;</mo>
                  <mrow>
                    <mi>j</mi>
                    <mo>=</mo>
                    <mrow>
                      <mi>l</mi>
                      <mo>&#xb7;</mo>
                      <mi>GT</mi>
                    </mrow>
                  </mrow>
                  <mrow>
                    <mrow>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <mi>l</mi>
                          <mo>+</mo>
                          <mn>1</mn>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                      <mo>&#xb7;</mo>
                      <mi>GT</mi>
                    </mrow>
                    <mo>-</mo>
                    <mn>1</mn>
                  </mrow>
                </munderover>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mrow>
                  <mi>S</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mi>i</mi>
                      <mo>,</mo>
                      <mi>j</mi>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
        <mo>&#x2062;</mo>
        <mstyle>
          <mtext>
</mtext>
        </mstyle>
        <mo>&#x2062;</mo>
        <mrow>
          <mrow>
            <mrow>
              <mi>for</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.8em" height="0.8ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mn>0</mn>
            </mrow>
            <mo>&#x2264;</mo>
            <mi>k</mi>
            <mo>&#x3c;</mo>
            <mi>K</mi>
          </mrow>
          <mo>;</mo>
          <mrow>
            <mn>0</mn>
            <mo>&#x2264;</mo>
            <mi>l</mi>
            <mo>&#x3c;</mo>
            <mi>L</mi>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>9</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where Q(k, l)=intermediate value in the low-resolution representation;
</p>
<p id="p-0107" num="0106">GF=width of spectral value groups expressed in numbers of values;</p>
<p id="p-0108" num="0107">GT=length of spectral value groups expressed in numbers of blocks;</p>
<p id="p-0109" num="0108">K=frequency resolution of the low-resolution representation;</p>
<p id="p-0110" num="0109">L=time resolution of the low-resolution representation; and</p>
<p id="p-0111" num="0110">S(i, j)=spectral values.</p>
<p id="p-0112" num="0111">The size GF of the groups is chosen such that K&#xb7;GF=RT and the size GT of the groups is chosen such that L&#xb7;GT=RT where RF and RT are the frequency and time resolutions of the low-resolution representation, respectively. For the exemplary implementation discussed above and below, one suitable size for the groups is GF=6 and GT=14, which provides a low-resolution representation of 129/6&#xd7;142/14&#x2248;20&#xd7;10 intermediate values. By using the lower-resolution intermediate values Q to generate the audio signature rather than the higher-resolution time-frequency representation, the generated audio signature is less sensitive to processes that change details of spectral content but preserve average spectral levels.</p>
<p id="p-0113" num="0112">The operations that calculate the time-frequency representation and the intermediate values may be performed in other ways. For example, a set of spectral values with a lower-frequency resolution may be obtained by decreasing the block length and transform length and increasing the length GT of the spectral value groups to obtain the same time resolution. If desired, the width GF of the groups can be varied across the spectrum. If high-frequency spectral components are deemed to be less significant than lower-frequency components for the generated signature, this relative significance can be realized by increasing the width of the groups for the higher frequencies.</p>
<heading id="h-0018" level="1">3. Audio Signature Processor</heading>
<p id="p-0114" num="0113">The audio signature processor <b>270</b> applies a hash function to a K&#xd7;L array of the intermediate values Q to generate a set of N hash bits. These hash bits constitute the audio signature (SA) that identifies the content of the audio segments. This may be done in the same way that is described above for the video signature.</p>
<heading id="h-0019" level="1">D. Applications</heading>
<p id="p-0115" num="0114">The video and audio signature generators that are discussed above may be used in a variety of applications including an application that recovers lost synchronization between streams of video and audio content and applications that detect copies of video and audio content. Exemplary implementations for these applications are described below.</p>
<heading id="h-0020" level="1">1. Video/Audio Synchronization</heading>
<p id="p-0116" num="0115">Streams of video and audio content are often synchronized with one another when they are recorded or created but synchronization may be lost during subsequent processing. In a television broadcasting system, for example, synchronized video and audio streams are often separated into two different paths for signal processing before they are assembled together for transmission. Different processing delays in the two paths can cause a loss of synchronization. The streams can be re-synchronized manually but this is tedious and prone to human error. The signature generators described above can be used to restore synchronization automatically. In a broadcast system, for example, synchronization can be restored at any point in the system including at the transmitter just prior to broadcast or at a receiver just prior to listening.</p>
<p id="p-0117" num="0116">Techniques like those described above are used to generate signatures from streams of video and audio content when the video/audio streams are known to be synchronized. Alignment information that specifies the alignment between the video and audio content underlying these signatures is also captured. These video and audio signatures and the alignment information are provided to a &#x201c;resync device&#x201d; that is responsible for restoring synchronization between the two streams. The resync device receives streams of the video and audio content after these streams have been processed and have lost synchronization with one another, generates new signatures along with current alignment information, correlates the newly generated signatures and current alignment information with the original signatures and alignment information, and adjusts the current alignment until proper synchronization is achieved. One way in which this may be done is described below in more detail.</p>
<heading id="h-0021" level="1">a) Exemplary Implementation Overview</heading>
<p id="p-0118" num="0117"><figref idref="DRAWINGS">FIG. 8</figref> is a schematic block diagram of a capture device <b>350</b> that generates video and audio signatures and alignment information from synchronized video and audio streams. The synchronized video and audio content and streams are referred to herein as reference content and reference streams. The signatures and alignment information that are obtained from the reference streams are referred to herein as reference signatures and reference alignment information, respectively. The video signature is obtained by the video signature generator <b>100</b> and is passed along the path <b>190</b> for subsequent use by a resync device. The audio signature is obtained by the audio signature generator <b>200</b> and is passed along the path <b>290</b> for subsequent use by a resync device. Techniques that may be used to generate video and audio signatures are described above.</p>
<p id="p-0119" num="0118">The capture device <b>350</b> also captures alignment information that may be represented in a variety of ways. For example, the alignment information may be implied by associating a particular video signature with an audio signature where the two signatures are generated from frames of video content and segments of audio content that begin at substantially the same time. In this context, the starting times of video and audio content are considered to be substantially the same if a human observer has difficulty telling which content precedes the other. As another example, the alignment information may be represented by time stamps or time offsets that are associated with the video and audio signatures and specify the relative timing of the underlying video content and audio content. If explicit alignment information is provided, it is passed along the path <b>390</b> for subsequent use by a resync device. No particular type of alignment information is critical. The examples discussed below are consistent with the assumption that the alignment information is implicit. These examples can be changed easily to account for any explicit time stamps or time offsets that may accompany the signatures.</p>
<p id="p-0120" num="0119"><figref idref="DRAWINGS">FIG. 9</figref> is a schematic block diagram of a resync device <b>400</b> that may be used to restore the synchronization between streams of video and audio content. The resync device <b>400</b> includes a video signature generator <b>100</b> and an audio signature generator <b>200</b> that generate video and audio signatures from streams of video and audio content received from the path <b>33</b>. The content of these streams may have been modified intentionally and unintentionally by a variety of processes and may not by properly synchronized with one another. These video and audio content and streams are referred to herein as the current content and current streams. The signatures generated from the current streams are referred to herein as the current signatures.</p>
<p id="p-0121" num="0120">The Content Delay Calculator <b>410</b> compares the current video signatures against the reference video signatures and estimates the relative timing difference between the underlying frames of the reference video content and the current video content that are deemed to be corresponding frames. The Content Delay Calculator <b>420</b> compares the current audio signatures against the reference audio signatures and estimates the relative timing difference between the underlying segments of the reference audio content and the current audio content that are deemed to be corresponding segments. For applications like broadcasting, the reference signatures and alignment information should be delivered to the content delay calculators before the current streams arrive so that sufficient information is available to restore synchronization in real time.</p>
<p id="p-0122" num="0121">The Relative Delay Calculator <b>430</b> uses these relative timing differences to calculate an amount of delay that is needed to adjust either or both current streams to achieve proper synchronization. Information representing this delay is passed along the path <b>490</b> for use by other equipment to implement the delay. For example, suppose the relative video timing difference indicates the reference video frame precedes the corresponding current video frame by four seconds and the relative audio timing difference indicates the reference audio segment precedes the corresponding current audio segment by five seconds. The Relative Delay Calculator <b>430</b> can calculate an amount of delay equal to one second for the current video stream to achieve proper synchronization.</p>
<heading id="h-0022" level="1">b) Content Delay Calculator</heading>
<p id="p-0123" num="0122">The two content delay calculators mentioned above compare sequences of reference and current signatures and estimate the relative timing difference between the underlying video/audio content. These calculators may be implemented in a variety of ways. One implementation using a Hamming distance function is described in the following paragraphs.</p>
<p id="p-0124" num="0123">Suppose the current video stream is obtained from the reference video stream through a signal processing path that imposes a processing delay denoted by the symbol &#x3b4;<sub>v</sub>. Suppose further that the current audio stream is obtained from the reference audio stream through a signal processing path that imposes a processing delay denoted by the symbol &#x3b4;<sub>A</sub>. The Content Delay Calculator <b>410</b> compares the reference video signatures SV<sub>REF </sub>and the current video signatures SV<sub>CURR </sub>and generates an estimate &#x3b5;<sub>v </sub>of the video processing delay. The relative timing difference between the reference and current video streams can be obtained from the estimated delay &#x3b5;<sub>v</sub>. The Content Delay Calculator <b>420</b> compares the reference audio signatures SA<sub>REF </sub>and the current audio signatures SA<sub>CURR </sub>and generates an estimate &#x3b5;<sub>A </sub>of the audio processing delay. The relative timing difference between the reference and current audio streams can be obtained from the estimated delay &#x3b5;<sub>A</sub>.</p>
<p id="p-0125" num="0124">One technique that may be used to estimate the processing delay is described in the following paragraphs. No distinction is made between video and audio because the video processing delay estimate &#x3b5;<sub>v </sub>and the audio processing delay estimate &#x3b5;<sub>A </sub>can be calculated in the same manner.</p>
<p id="p-0126" num="0125">The content delay calculator receives two sets of signatures. One set is a sequence of reference signatures denoted {S<sub>REF</sub>(i)} and the second set is a sequence of current signatures denoted {S<sub>CURR</sub>(i)}. The correlator searches over ranges of the two sequences to find a window in which the sets of signatures have the highest degree of correlation. This may be done by first computing the following scores:</p>
<p id="p-0127" num="0126">
<maths id="MATH-US-00012" num="00012">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mrow>
            <mi>D</mi>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mi>m</mi>
                <mo>,</mo>
                <mi>i</mi>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>=</mo>
          <mrow>
            <munderover>
              <mo>&#x2211;</mo>
              <mrow>
                <mi>j</mi>
                <mo>=</mo>
                <mn>0</mn>
              </mrow>
              <mrow>
                <mi>W</mi>
                <mo>-</mo>
                <mn>1</mn>
              </mrow>
            </munderover>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mrow>
              <mi>HD</mi>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>[</mo>
                <mrow>
                  <mrow>
                    <msub>
                      <mi>S</mi>
                      <mi>REF</mi>
                    </msub>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mi>i</mi>
                        <mo>+</mo>
                        <mi>j</mi>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mo>,</mo>
                  <mrow>
                    <msub>
                      <mi>S</mi>
                      <mi>CURR</mi>
                    </msub>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mi>m</mi>
                        <mo>+</mo>
                        <mi>j</mi>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                </mrow>
                <mo>]</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
        <mo>&#x2062;</mo>
        <mstyle>
          <mtext>
</mtext>
        </mstyle>
        <mo>&#x2062;</mo>
        <mrow>
          <mrow>
            <mrow>
              <mi>for</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.8em" height="0.8ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mn>1</mn>
            </mrow>
            <mo>&#x2264;</mo>
            <mi>i</mi>
            <mo>&#x2264;</mo>
            <mi>F</mi>
          </mrow>
          <mo>,</mo>
          <mrow>
            <mrow>
              <mi>i</mi>
              <mo>-</mo>
              <mi>U</mi>
            </mrow>
            <mo>&#x2264;</mo>
            <mi>m</mi>
            <mo>&#x2264;</mo>
            <mrow>
              <mi>i</mi>
              <mo>+</mo>
              <mi>U</mi>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>10</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where D(m, i)=calculated score for a particular alignment between streams;
</p>
<p id="p-0128" num="0127">HD[r,c]=Hamming distance between signatures r and c;</p>
<p id="p-0129" num="0128">F=number of signatures in the set of reference signatures {S<sub>REF</sub>(i)};</p>
<p id="p-0130" num="0129">U=the search range for the correlator; and</p>
<p id="p-0131" num="0130">W=length of correlation window expressed as a number of signatures. The Hamming distance is equal to the number of bit positions in which two signatures differ.</p>
<p id="p-0132" num="0131">The estimated processing delay may be derived from the value of m where the closest match between reference and current signatures within the correlation window is found. This can be expressed as:</p>
<p id="p-0133" num="0132">
<maths id="MATH-US-00013" num="00013">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <msub>
          <mi>&#x25b;</mi>
          <mi>i</mi>
        </msub>
        <mo>=</mo>
        <mrow>
          <mrow>
            <mrow>
              <mi>arg</mi>
              <mo>&#x2062;</mo>
              <mrow>
                <munder>
                  <mi>min</mi>
                  <mi>m</mi>
                </munder>
                <mo>&#x2062;</mo>
                <mrow>
                  <mrow>
                    <mo>[</mo>
                    <mrow>
                      <mi>D</mi>
                      <mo>&#x2061;</mo>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <mi>m</mi>
                          <mo>,</mo>
                          <mi>i</mi>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                    <mo>]</mo>
                  </mrow>
                  <mo>&#x2062;</mo>
                  <mstyle>
                    <mspace width="0.8em" height="0.8ex"/>
                  </mstyle>
                  <mo>&#x2062;</mo>
                  <mi>for</mi>
                  <mo>&#x2062;</mo>
                  <mstyle>
                    <mspace width="0.8em" height="0.8ex"/>
                  </mstyle>
                  <mo>&#x2062;</mo>
                  <mi>i</mi>
                </mrow>
              </mrow>
            </mrow>
            <mo>-</mo>
            <mi>U</mi>
          </mrow>
          <mo>&#x2264;</mo>
          <mi>m</mi>
          <mo>&#x2264;</mo>
          <mrow>
            <mi>i</mi>
            <mo>+</mo>
            <mi>U</mi>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>11</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
If there is no processing delay, then &#x3b5;<sub>i</sub>=i. If the estimated delay is three frames or three segments, then &#x3b5;<sub>i</sub>=i+3. The relative timing difference for the frame or segment that corresponds to the signature S<sub>REF</sub>(i) is the offset between &#x3b5;<sub>i </sub>and i. The relative timing difference for video frame i is denoted herein as dv(i) and the relative timing difference for audio segment i is denoted herein as da(i).
</p>
<p id="p-0134" num="0133">In one implementation for television, the values W=10 and U=45 are used by the Content Delay Calculator <b>410</b> for video frames and the values W=23 and U=47 are used by the Content Delay Calculator <b>420</b> for audio segments. The values for F used in the video and audio delay calculators are the numbers of video frames and audio segments, respectively, in the reference streams.</p>
<p id="p-0135" num="0134">The Relative Delay Calculator <b>430</b> uses the relative timing differences for the video and audio streams to calculate an amount of delay for either or both current streams to achieve proper synchronization. This is illustrated schematically in <figref idref="DRAWINGS">FIG. 10</figref>. The reference video frame <b>1</b><i>b </i>in a reference video stream <b>1</b> and the reference audio segment <b>2</b><i>b </i>in a reference audio stream <b>2</b> are shown in synchronization. Signal processing <b>81</b> for the video stream <b>1</b> and signal processing <b>82</b> for the audio stream <b>2</b> introduce different delays into the two streams. As a result, the current video frame <b>1</b><i>b </i>in the current video stream <b>31</b> and the current audio segment <b>2</b><i>b </i>in the current audio stream <b>32</b> are no longer in synchronization. The current video frame <b>1</b><i>b </i>has been delayed by dv and the current audio segment <b>2</b><i>b </i>has been delayed by da. The adjustment to delay adj that is needed to restore synchronization is equal to (dv&#x2212;da). If dv is greater than da as shown in the figure, then synchronization may be restored by delaying the audio segment by the adjustment adj. If dv is less than da then synchronization may be restored by delaying the video frame by the adjustment adj. The streams can be delayed in essentially any way that may be desired but one way is to store and retrieve stream content in a first-in first-out (FIFO) buffer that has sufficient capacity to provide the adjustment to the delay.</p>
<heading id="h-0023" level="1">2. Detection of Copies</heading>
<p id="p-0136" num="0135">The signatures that are generated from video and audio content can be used to identify the content even when that content has been modified by a variety of processes including those mentioned above. The ability to determine reliably whether specified video content or audio content is a copy of a reference video content or audio content, even when modified, can be used in a variety of applications. A few examples are described briefly in the following list:
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0136">Detection of unauthorized copies: Networks of peer-to-peer servers can facilitate the distribution of content but they can also increase the difficulty of detecting unauthorized or pirated copies of proprietary content because many copies of the content can exist among the peer-to-peer servers. A facility can automatically determine if any unauthorized copies exist in the network by generating signatures for all the content available from the network and checking these signatures against a database of reference signatures.</li>
        <li id="ul0002-0002" num="0137">Confirmation of broadcast: Businesses that contract with broadcast networks to distribute specified video and audio content can confirm the terms of the contract were met by generating signatures from signals received by a broadcast receiver and comparing these signatures to reference signatures for the specified content.</li>
        <li id="ul0002-0003" num="0138">Identification of reception: Businesses that provide ratings for broadcast networks can identify content that is received by a receiver by generating signatures from the received signals and comparing those signatures against reference signatures.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0137" num="0139"><figref idref="DRAWINGS">FIG. 11</figref> is a schematic block diagram of a system that may be used to implement a variety of applications such as those mentioned in the preceding list. The video signature generator <b>100</b> and the audio signature generator <b>200</b> generate reference video signatures and reference audio signatures from reference video/audio streams of content received from the path <b>31</b>. The generated reference video signatures are stored in the video-signature database (VSIG DB) <b>180</b> and the generated reference audio signatures are stored in the audio-signature database (ASIG DB) <b>280</b>. The reference signatures may be stored with other information that may facilitate implementation of the application. For example, the reference signatures may be stored with the underlying content itself or with data that identifies information about the content such as the content owner, content licensing terms, title of the content or a textual description of the content. Each reference signature has a database search key. This key may be derived in any manner that may be desired. Preferably, the key is based on or derived from the reference signature itself.</p>
<p id="p-0138" num="0140">The identity of any specified video content or audio content may be checked against reference content represented by information stored in the video and audio databases. The content whose identity is to be checked is referred to herein as the test content. The identity of the test video content may be checked by having the video signature generator <b>101</b> generate test video signatures from the test video content received from the path <b>33</b> and passing the test video signatures to the video search engine <b>185</b>. The video search engine <b>185</b> attempts to find reference video signatures in the video-signature database <b>180</b> that are exact or close match to the test video signatures. The identity of the test audio content may be checked by having the audio signature generator <b>201</b> generate test audio signatures from the test audio content received from the path <b>33</b> and passing the test audio signatures to the audio search engine <b>285</b>. The audio search engine <b>285</b> attempts to find reference audio signatures in the audio-signature database <b>280</b> that are exact or close match to the test audio signatures.</p>
<p id="p-0139" num="0141">In one implementation, the search engines calculate the Hamming distances between the test signatures and the reference signatures stored in the databases and searches for a sequence of reference signatures that are closest to a sequence of the test video signatures. The calculations shown above in expressions 10 and 11 or some variation of them may be used to conduct the search. If the distance between two sequences of signatures is less than some threshold, the test content associated with the sequence of test signatures is deemed to be an exact or modified copy of the reference content that is associated with the sequence of matching reference signatures. Empirical results suggest that good results can be obtained for a variety of video and audio content using sequences of signatures that represent about two seconds of content.</p>
<heading id="h-0024" level="1">E. Implementation</heading>
<p id="p-0140" num="0142">Devices that incorporate various aspects of the present invention may be implemented in a variety of ways including software for execution by a computer or some other device that includes more specialized components such as digital signal processor (DSP) circuitry coupled to components similar to those found in a general-purpose computer. <figref idref="DRAWINGS">FIG. 12</figref> is a schematic block diagram of a device <b>70</b> that may be used to implement aspects of the present invention. The processor <b>72</b> provides computing resources. RAM <b>73</b> is system random access memory (RAM) used by the processor <b>72</b> for processing. ROM <b>74</b> represents some form of persistent storage such as read only memory (ROM) for storing programs needed to operate the device <b>70</b> and possibly for carrying out various aspects of the present invention. I/O control <b>75</b> represents interface circuitry to receive and transmit signals by way of the communication channels <b>76</b>, <b>77</b>. In the embodiment shown, all major system components connect to the bus <b>71</b>, which may represent more than one physical or logical bus; however, a bus architecture is not required to implement the present invention.</p>
<p id="p-0141" num="0143">In embodiments implemented by a general-purpose computer system, additional components may be included for interfacing to devices such as a keyboard or mouse and a display, and for controlling a storage device <b>78</b> having a storage medium such as magnetic tape or disk, or an optical medium. The storage medium may be used to record programs of instructions for operating systems, utilities and applications, and may include programs that implement various aspects of the present invention.</p>
<p id="p-0142" num="0144">The functions required to practice various aspects of the present invention can be performed by components that are implemented in a wide variety of ways including discrete logic components, integrated circuits, one or more ASICs and/or program-controlled processors. The manner in which these components are implemented is not important to the present invention.</p>
<p id="p-0143" num="0145">Software implementations of the present invention may be conveyed by a variety of machine readable media such as baseband or modulated communication paths throughout the spectrum including from supersonic to ultraviolet frequencies, or storage media that convey information using essentially any recording technology including magnetic tape, cards or disk, optical cards or disc, and detectable markings on media including paper.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08626504-20140107-M00001.NB">
<img id="EMI-M00001" he="13.80mm" wi="76.20mm" file="US08626504-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US08626504-20140107-M00002.NB">
<img id="EMI-M00002" he="15.49mm" wi="76.20mm" file="US08626504-20140107-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00003" nb-file="US08626504-20140107-M00003.NB">
<img id="EMI-M00003" he="10.92mm" wi="76.20mm" file="US08626504-20140107-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00004" nb-file="US08626504-20140107-M00004.NB">
<img id="EMI-M00004" he="7.03mm" wi="76.20mm" file="US08626504-20140107-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00005" nb-file="US08626504-20140107-M00005.NB">
<img id="EMI-M00005" he="10.92mm" wi="76.20mm" file="US08626504-20140107-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00006" nb-file="US08626504-20140107-M00006.NB">
<img id="EMI-M00006" he="7.03mm" wi="76.20mm" file="US08626504-20140107-M00006.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00007" nb-file="US08626504-20140107-M00007.NB">
<img id="EMI-M00007" he="13.80mm" wi="76.20mm" file="US08626504-20140107-M00007.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00008" nb-file="US08626504-20140107-M00008.NB">
<img id="EMI-M00008" he="8.81mm" wi="76.20mm" file="US08626504-20140107-M00008.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00009" nb-file="US08626504-20140107-M00009.NB">
<img id="EMI-M00009" he="7.45mm" wi="76.20mm" file="US08626504-20140107-M00009.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00010" nb-file="US08626504-20140107-M00010.NB">
<img id="EMI-M00010" he="6.35mm" wi="76.20mm" file="US08626504-20140107-M00010.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00011" nb-file="US08626504-20140107-M00011.NB">
<img id="EMI-M00011" he="13.80mm" wi="76.20mm" file="US08626504-20140107-M00011.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00012" nb-file="US08626504-20140107-M00012.NB">
<img id="EMI-M00012" he="13.80mm" wi="76.20mm" file="US08626504-20140107-M00012.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00013" nb-file="US08626504-20140107-M00013.NB">
<img id="EMI-M00013" he="4.91mm" wi="76.20mm" file="US08626504-20140107-M00013.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method for generating a signature that identifies content of an audio signal, wherein the method performed by a device comprises:
<claim-text>obtaining, by a device, a time-frequency representation of a set of blocks within a sequence of blocks of the audio signal, wherein the time-frequency representation comprises sets of spectral values, each set of spectral values representing all spectral components within at least a portion of the bandwidth of the audio signal in a respective block in the set of blocks;</claim-text>
<claim-text>deriving, by a device, intermediate values from intensities of all the spectral values arranged in groups of one or more spectral values within a respective set of spectral values; and</claim-text>
<claim-text>generating, by a device the signature that identifies content of the audio signal by projecting the intermediate values onto a set of random vectors, wherein the signature is represented by bits and each bit of the signature is derived from contributions from all of the intermediate values;</claim-text>
<claim-text>wherein each respective vector in the set of random vectors has vector elements with values that are obtained from a difference between uniformly distributed random variables within a range from zero to one and an average of the uniformly distributed random variables for all vector elements of the respective vector;</claim-text>
<claim-text>the projection of the intermediate values onto a respective random vector is obtained from an inner product of the intermediate values with the vector elements of the respective vector; and</claim-text>
<claim-text>each component of the signature has either a first value when the projection of the intermediate values onto a corresponding random vector is greater than a threshold or has a second value when the projection of the intermediate values onto the corresponding random vector is less than a threshold, wherein the threshold is equal to a median of the projections of intermediate values onto the set of random vectors.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:
<claim-text>the time-frequency representation is obtained by applying a time-to-frequency transform to each block of the audio signal in the set of blocks to obtain a respective set of spectral values; and</claim-text>
<claim-text>a respective intermediate value is derived by calculating an average intensity of the one or more spectral values in a group within the respective set of spectral values.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the groups of spectral values have numbers of spectral values that vary with frequency.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the groups of spectral values for higher frequencies have a greater number of spectral values.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each component of the signature is derived from the projection of the intermediate values onto a respective random vector.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. An apparatus for generating a signature that identifies content of an audio signal, wherein the apparatus comprises:
<claim-text>means for obtaining a time-frequency representation of a set of blocks within a sequence of blocks of the audio signal, wherein the time-frequency representation comprises sets of spectral values, each set of spectral values representing all spectral components within at least a portion of the bandwidth of the audio signal in a respective block in the set of blocks;</claim-text>
<claim-text>means for deriving intermediate values from intensities of all the spectral values arranged in groups of one or more spectral values within a respective set of spectral values; and</claim-text>
<claim-text>means for generating the signature that identifies content of the audio signal by projecting the intermediate values onto a set of random vectors, wherein the signature is represented by bits and each bit of the signature is derived from contributions from all of the intermediate values;</claim-text>
<claim-text>wherein each respective vector in the set of random vectors has vector elements with values that are obtained from a difference between uniformly distributed random variables within a range from zero to one and an average of the uniformly distributed random variables for all vector elements of the respective vector;</claim-text>
<claim-text>the projection of the intermediate values onto a respective random vector is obtained from an inner product of the intermediate values with the vector elements of the respective vector; and</claim-text>
<claim-text>each component of the signature has either a first value when the projection of the intermediate values onto a corresponding random vector is greater than a threshold or has a second value when the projection of the intermediate values onto the corresponding random vector is less than a threshold, wherein the threshold is equal to a median of the projections of intermediate values onto the set of random vectors.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The apparatus of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein:
<claim-text>the time-frequency representation is obtained by applying a time-to-frequency transform to each block of the audio signal in the set of blocks to obtain a respective set of spectral values; and</claim-text>
<claim-text>a respective intermediate value is derived by calculating an average intensity of the one or more spectral values in a group within the respective set of spectral values.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The apparatus of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the groups of spectral values have numbers of spectral values that vary with frequency.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The apparatus of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the groups of spectral values for higher frequencies have a greater number of spectral values.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The apparatus of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein each component of the signature is derived from the projection of the intermediate values onto a respective random vector.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. A non-transitory computer readable storage medium that records a program of instructions that is executable by a device to perform a method for generating a signature that identifies content of an audio signal, wherein the method comprises:
<claim-text>obtaining a time-frequency representation of a set of blocks within a sequence of blocks of the audio signal, wherein the time-frequency representation comprises sets of spectral values, each set of spectral values representing all spectral components within at least a portion of the bandwidth of the audio signal in a respective block in the set of blocks;</claim-text>
<claim-text>deriving intermediate values from intensities of all the spectral values arranged in groups of one or more spectral values within a respective set of spectral values; and</claim-text>
<claim-text>generating the signature that identifies content of the audio signal by projecting the intermediate values onto a set of random vectors, wherein the signature is represented by bits and each bit of the signature is derived from contributions from all of the intermediate values;</claim-text>
<claim-text>wherein each respective vector in the set of random vectors has vector elements with values that are obtained from a difference between uniformly distributed random variables within a range from zero to one and an average of the uniformly distributed random variables for all vector elements of the respective vector;</claim-text>
<claim-text>the projection of the intermediate values onto a respective random vector is obtained from an inner product of the intermediate values with the vector elements of the respective vector; and</claim-text>
<claim-text>each component of the signature has either a first value when the projection of the intermediate values onto a corresponding random vector is greater than a threshold or has a second value when the projection of the intermediate values onto the corresponding random vector is less than a threshold, wherein the threshold is equal to a median of the projections of intermediate values onto the set of random vectors.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The storage non-transitory computer readable storage medium of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein:
<claim-text>the time-frequency representation is obtained by applying a time-to-frequency transform to each block of the audio signal in the set of blocks to obtain a respective set of spectral values; and</claim-text>
<claim-text>a respective intermediate value is derived by calculating an average intensity of the one or more spectral values in a group within the respective set of spectral values.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The non-transitory computer readable storage medium of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the groups of spectral values have numbers of spectral values that vary with frequency.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The non-transitory computer readable medium of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the groups of spectral values for higher frequencies have a greater number of spectral values.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The non-transitory computer readable storage medium of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein each component of the signature is derived from the projection of the intermediate values onto a respective random vector. </claim-text>
</claim>
</claims>
</us-patent-grant>
