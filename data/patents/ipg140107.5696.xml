<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626797-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626797</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13089956</doc-number>
<date>20110419</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>JP</country>
<doc-number>P2010-101042</doc-number>
<date>20100426</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>76</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>17</main-group>
<subgroup>30</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>707796</main-classification>
</classification-national>
<invention-title id="d2e71">Information processing apparatus, text selection method, and program</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>7835859</doc-number>
<kind>B2</kind>
<name>Bill</name>
<date>20101100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>701424</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>8213913</doc-number>
<kind>B2</kind>
<name>Ghai et al.</name>
<date>20120700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>4554142</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>2003/0225509</doc-number>
<kind>A1</kind>
<name>Okamoto</name>
<date>20031200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>701201</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>2009/0312946</doc-number>
<kind>A1</kind>
<name>Yoshioka et al.</name>
<date>20091200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>701208</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>2010/0003658</doc-number>
<kind>A1</kind>
<name>Fadel et al.</name>
<date>20100100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>434322</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2010/0047748</doc-number>
<kind>A1</kind>
<name>Hwang</name>
<date>20100200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>434157</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2010/0195909</doc-number>
<kind>A1</kind>
<name>Wasson et al.</name>
<date>20100800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382176</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2010/0299615</doc-number>
<kind>A1</kind>
<name>Miluzzo et al.</name>
<date>20101100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715752</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>JP</country>
<doc-number>2005-292730</doc-number>
<date>20051000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>14</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>707796</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707812</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>32</number-of-drawing-sheets>
<number-of-figures>33</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20110264691</doc-number>
<kind>A1</kind>
<date>20111027</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Migita</last-name>
<first-name>Takahito</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Kanemoto</last-name>
<first-name>Katsuyoshi</first-name>
<address>
<city>Chiba</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Masuda</last-name>
<first-name>Hiroyuki</first-name>
<address>
<city>Kanagawa</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
<us-applicant sequence="004" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Tsuboi</last-name>
<first-name>Naoto</first-name>
<address>
<city>Kanagawa</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Migita</last-name>
<first-name>Takahito</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Kanemoto</last-name>
<first-name>Katsuyoshi</first-name>
<address>
<city>Chiba</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Masuda</last-name>
<first-name>Hiroyuki</first-name>
<address>
<city>Kanagawa</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
<inventor sequence="004" designation="us-only">
<addressbook>
<last-name>Tsuboi</last-name>
<first-name>Naoto</first-name>
<address>
<city>Kanagawa</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Finnegan, Henderson, Farabow, Garrett &#x26; Dunner, L.L.P.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Sony Corporation</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Trujillo</last-name>
<first-name>James</first-name>
<department>2159</department>
</primary-examiner>
<assistant-examiner>
<last-name>Davanlou</last-name>
<first-name>Soheila</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Disclosed herein is an information processing apparatus including: a sensor information acquisition section configured to acquire sensor information outputted from a sensor for detecting a user motion and sensor information outputted from a sensor for obtaining a user current location; an action pattern detection block configured to analyze sensor information indicative of a user motion to detect an action pattern corresponding to the acquired sensor information from a plurality of action patterns obtained by classifying user's actions that are executed in a comparatively short time; a keyword conversion block configured to convert, on the basis of the sensor information indicative of a current location, the information into at least one keyword associated with the current location; and a text extraction block configured to extract a text for user presentation from a plurality of texts on the basis of the detected action pattern and the generated at least one keyword.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="118.19mm" wi="152.15mm" file="US08626797-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="154.26mm" wi="136.23mm" orientation="landscape" file="US08626797-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="223.35mm" wi="150.28mm" orientation="landscape" file="US08626797-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="130.98mm" wi="156.46mm" file="US08626797-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="160.78mm" wi="143.68mm" file="US08626797-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="226.14mm" wi="162.48mm" file="US08626797-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="159.09mm" wi="136.91mm" file="US08626797-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="194.65mm" wi="142.66mm" file="US08626797-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="223.69mm" wi="170.01mm" file="US08626797-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="201.00mm" wi="160.78mm" file="US08626797-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="152.91mm" wi="137.84mm" file="US08626797-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="220.39mm" wi="144.61mm" orientation="landscape" file="US08626797-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="239.18mm" wi="137.75mm" orientation="landscape" file="US08626797-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="205.23mm" wi="159.77mm" file="US08626797-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="204.30mm" wi="164.08mm" file="US08626797-20140107-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="235.54mm" wi="144.61mm" orientation="landscape" file="US08626797-20140107-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00016" num="00016">
<img id="EMI-D00016" he="234.27mm" wi="149.94mm" orientation="landscape" file="US08626797-20140107-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00017" num="00017">
<img id="EMI-D00017" he="171.03mm" wi="132.76mm" file="US08626797-20140107-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00018" num="00018">
<img id="EMI-D00018" he="238.84mm" wi="143.00mm" orientation="landscape" file="US08626797-20140107-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00019" num="00019">
<img id="EMI-D00019" he="156.80mm" wi="112.01mm" file="US08626797-20140107-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00020" num="00020">
<img id="EMI-D00020" he="235.29mm" wi="145.63mm" orientation="landscape" file="US08626797-20140107-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00021" num="00021">
<img id="EMI-D00021" he="222.08mm" wi="145.63mm" orientation="landscape" file="US08626797-20140107-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00022" num="00022">
<img id="EMI-D00022" he="188.13mm" wi="133.77mm" orientation="landscape" file="US08626797-20140107-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00023" num="00023">
<img id="EMI-D00023" he="189.82mm" wi="133.43mm" orientation="landscape" file="US08626797-20140107-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00024" num="00024">
<img id="EMI-D00024" he="203.28mm" wi="110.07mm" orientation="landscape" file="US08626797-20140107-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00025" num="00025">
<img id="EMI-D00025" he="206.93mm" wi="106.09mm" orientation="landscape" file="US08626797-20140107-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00026" num="00026">
<img id="EMI-D00026" he="197.02mm" wi="111.34mm" orientation="landscape" file="US08626797-20140107-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00027" num="00027">
<img id="EMI-D00027" he="198.97mm" wi="106.09mm" orientation="landscape" file="US08626797-20140107-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00028" num="00028">
<img id="EMI-D00028" he="215.82mm" wi="163.41mm" file="US08626797-20140107-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00029" num="00029">
<img id="EMI-D00029" he="112.01mm" wi="126.49mm" file="US08626797-20140107-D00029.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00030" num="00030">
<img id="EMI-D00030" he="198.37mm" wi="123.53mm" file="US08626797-20140107-D00030.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00031" num="00031">
<img id="EMI-D00031" he="197.36mm" wi="131.83mm" orientation="landscape" file="US08626797-20140107-D00031.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00032" num="00032">
<img id="EMI-D00032" he="229.95mm" wi="146.64mm" orientation="landscape" file="US08626797-20140107-D00032.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0002" num="0001">1. Field of the Invention</p>
<p id="p-0003" num="0002">The present invention relates to an information processing apparatus, a text selection method, and a program.</p>
<p id="p-0004" num="0003">2. Description of the Related Art</p>
<p id="p-0005" num="0004">Services realized by the advancement of information processing technologies include so-called e-learning systems in which individual person learns various subjects by use of each person's free time at his or her own pace through networks. One of the subjects that can be learned by such e-learning systems is the learning of foreign languages. On the other hand, portable devices for searching for foreign language sentences which can be used during travels have been developed.</p>
<heading id="h-0002" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0006" num="0005">However, related-art e-learning systems are configured to provided only learning materials suitable for users' foreign language proficiency levels. Therefore, each user must determine where and how to use results of his or her e-learning.</p>
<p id="p-0007" num="0006">Portable devices for searching for foreign language conversational sentences which can be used during travels have also been developed. However, these devices present problems that timely searching for suitable sentences in a specific situation is very troublesome.</p>
<p id="p-0008" num="0007">Therefore, technologies of enabling a user to search for a proper sentence (or text) suitable for user's current location or user's situation and presenting the results of the searching to the user.</p>
<p id="p-0009" num="0008">Meanwhile, devices for identifying user's current location or user's body motion to present suitable music content have been proposed (refer to Japanese Patent Laid-open No. 2005-292730), but any specific methods of applying such technologies to foreign language learning have not been developed.</p>
<p id="p-0010" num="0009">Therefore, the present invention addresses the above-identified and other problems associated with related-art methods and apparatuses and solves the addressed problems by providing an information processing apparatus, a text selection method, and a program that are configured to search for proper sentences (or texts) in accordance with user's current location or user's situation and present the results of the searching to the user.</p>
<p id="p-0011" num="0010">In carrying out the invention and according to one embodiment thereof, there is provided an information processing apparatus including: a sensor information acquisition section configured to acquire sensor information outputted from a sensor for detecting a user motion and sensor information outputted from a sensor for obtaining a user current location; an action pattern detection block configured to analyze sensor information from the sensor for detecting a user motion acquired by the sensor information acquisition section to detect an action pattern corresponding to the acquired sensor information from a plurality of action patterns obtained by classifying user's actions that are executed in a comparatively short time; a keyword conversion block configured to convert, on the basis of the sensor information indicative of a current location acquired by the sensor information acquisition section, the information indicative of a current location into at least one keyword associated with the current location; and a text extraction block configured to extract a text for user presentation from a plurality of texts on the basis of the action pattern detected by the action pattern detection block and the at least one keyword generated by the keyword conversion block.</p>
<p id="p-0012" num="0011">The above-mentioned information processing apparatus preferably includes a text analysis block configured to analyze each of the plurality of texts to assign an attribute to each word making up each analyzed text and assign a context indicative of at least one of motion and state indicated by each of the plurality of texts thereto. The keyword conversion block preferably outputs at least one keyword associated with the current location to the text analysis block, and the text analysis block preferably assigns an attribute to the at least one keyword associated with the current location outputted from the keyword conversion block.</p>
<p id="p-0013" num="0012">The above-mentioned text extraction block preferably refers to the context assigned to each of the plurality of texts, the each word included in each of the plurality of texts, and the attribute assigned to the each word to extract a text that matches at least any one of the action pattern detected by the action pattern detection block, at least one keyword generated by the keyword conversion block, and the attribute assigned to the keyword.</p>
<p id="p-0014" num="0013">The above-mentioned information processing apparatus preferably includes a positional information analysis block configured to generate information associated with a place of user's frequent visit and a place of user's probable visit next to the current location by use of information indicative of the current location acquired by the sensor information acquisition section and a log of the information indicative of a current location. The keyword conversion block preferably further converts the information associated with a place of user's frequent visit and a place of user's probable visit next to the current location into a keyword.</p>
<p id="p-0015" num="0014">In the above-mentioned information processing apparatus, if a text with which the attribute matches but the keyword does not match has been extracted, the text extraction block preferably replaces the word included in the extracted text by the keyword generated by the keyword conversion block.</p>
<p id="p-0016" num="0015">The above-mentioned keyword conversion block preferably converts the information indicative of the current location into a keyword indicative of at least one of an address of the current location, a place name of the current location, and a name of a matter located near the current location.</p>
<p id="p-0017" num="0016">In carrying out the invention and according to another embodiment thereof, there is provided a text selection method including the steps of: acquiring sensor information outputted from a sensor for detecting a user motion and sensor information outputted from a sensor for obtaining a user current location; analyzing sensor information from the sensor for detecting a user motion acquired in the sensor information acquisition step to detect an action pattern corresponding to the acquired sensor information from a plurality of action patterns obtained by classifying user's actions that are executed in a comparatively short time; converting, on the basis of the sensor information indicative of a current location acquired in the sensor information acquisition step, the information indicative of a current location into at least one keyword associated with the current location; and extracting a text for user presentation from a plurality of texts on the basis of the action pattern detected in the action pattern detection step and the at least one keyword generated in the keyword conversion step.</p>
<p id="p-0018" num="0017">In carrying out the invention and according to another embodiment thereof, there is provided a program for making a computer capable of communication with a sensor for detecting a user motion and a sensor for detecting a current location realize functions of: acquiring sensor information outputted from a sensor for detecting a user motion and sensor information outputted from a sensor for obtaining a user current location; analyzing sensor information from the sensor for detecting a user motion acquired by the sensor information acquisition function to detect an action pattern corresponding to the acquired sensor information from a plurality of action patterns obtained by classifying user's actions that are executed in a comparatively short time; converting, on the basis of the sensor information indicative of a current location acquired by the sensor information acquisition function, the information indicative of a current location into at least one keyword associated with the current location; and extracting a text for user presentation from a plurality of texts on the basis of the action pattern detected by the action pattern detection function and the at least one keyword generated by the keyword conversion function.</p>
<p id="p-0019" num="0018">As described and according to the invention, searching for a proper sentence (or text) in accordance with user's current location or user's situation and presenting the results of the searching can be realized.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0020" num="0019">Other features and advantages of the invention will become apparent from the following description of embodiments with reference to the accompanying drawings in which:</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram illustrating an exemplary configuration of an information processing apparatus practiced as a first embodiment of the invention;</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram illustrating an exemplary configuration of a text selection section associated with the first embodiment;</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 3</figref> is a diagram illustrating one example of user's action patterns (context);</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIGS. 4 through 9</figref> are flowcharts indicative of different examples of action patterns;</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 10</figref> is a flowchart indicative of one example of an action pattern detection method;</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 11</figref> is a diagram illustrating one example of user action log information;</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIGS. 12 to 16</figref> are diagrams for explaining different text analysis methods;</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 17</figref> is a diagram illustrating one example of a text database;</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 18</figref> is a flowchart indicative of one example of a text analysis method;</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 19</figref> is a flowchart indicative of one example of a processing flow of a text selection method associated with the first embodiment;</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 20</figref> is a flowchart indicative of one example of a processing flow of another text selection method associated with the first embodiment;</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 21</figref> is a block diagram illustrating an exemplary configuration of an information processing apparatus practiced as a second embodiment of the invention;</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 22</figref> is a block diagram illustrating an exemplary configuration of a questioning tendency setting section associated with the second embodiment;</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 23</figref> is a diagram illustrating one example of a correct answers percentage table associated with the second embodiment;</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 24</figref> is a diagram illustrating one example of a wrong answer matrix associated with the second embodiment;</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 25</figref> is a diagram illustrating one example of a table associated with final answer date and answer count associated with the second embodiment;</p>
<p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. 26</figref> is a diagram illustrating one example of a forgetting percentage table group associated with the second embodiment;</p>
<p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. 27</figref> is a diagram illustrating one example of a forgetting curve;</p>
<p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. 28</figref> is a diagram illustrating one example of a questioning tendency setting method associated with the second embodiment;</p>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. 29</figref> is a diagram illustrating one example of another questioning tendency setting method associated with the second embodiment;</p>
<p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. 30</figref> is a flowchart indicative of a processing flow of a questioning tendency setting method associated with the second embodiment;</p>
<p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. 31</figref> is a block diagram illustrating an exemplary configuration of an information processing apparatus practiced as a third embodiment of the invention; and</p>
<p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. 32</figref> is a block diagram illustrating an exemplary hardware configuration of a computer associated with the embodiments of the invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading>
<p id="p-0044" num="0043">This invention will be described in further detail by way of embodiments thereof with reference to the accompanying drawings. It should be noted that, throughout the present specifications and the drawing accompanied thereto, component blocks having substantially the same function are denoted by the same reference numerals and the description of the duplication will be skipped.</p>
<p id="p-0045" num="0044">The description will be carried out in the following order:
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0045">(1) First embodiment</li>
        <li id="ul0002-0002" num="0046">(1-1) Configuration of information processing apparatus</li>
        <li id="ul0002-0003" num="0047">(1-2) Processing flow of information processing method;</li>
        <li id="ul0002-0004" num="0048">(2) Second embodiment</li>
        <li id="ul0002-0005" num="0049">(2-1) Configuration of information processing apparatus</li>
        <li id="ul0002-0006" num="0050">(2-2) Processing flow of questioning tendency setting method;</li>
        <li id="ul0002-0007" num="0051">(3) Third embodiment</li>
        <li id="ul0002-0008" num="0052">(3-1) Configuration of information processing apparatus; and</li>
        <li id="ul0002-0009" num="0053">(4) Hardware configuration of information processing apparatus (computer) associated with the embodiments of the present invention.</li>
    </ul>
    </li>
</ul>
</p>
<heading id="h-0005" level="1">(1) First Embodiment</heading>
<p id="p-0046" num="0054">First, an information processing apparatus and a text selection method associated with the first embodiment of the invention will be detailed with reference to <figref idref="DRAWINGS">FIG. 1</figref> through <figref idref="DRAWINGS">FIG. 20</figref>.</p>
<p id="p-0047" num="0055">As will be described in detail, an information processing apparatus <b>10</b> practiced as the first embodiment is an apparatus that is configured to analyze a current state, a current position, and so on of a user by use of sensor information output from various sensors, thereby selecting a text that matches the obtained user's current state and position.</p>
<heading id="h-0006" level="1">(1-1) Exemplary Configuration of Information Processing Apparatus</heading>
<p id="p-0048" num="0056">Now, referring to <figref idref="DRAWINGS">FIG. 1</figref>, an exemplary configuration of the information processing apparatus <b>10</b> will be described in detail. <figref idref="DRAWINGS">FIG. 1</figref> is a block diagram illustrating the exemplary configuration of the information processing apparatus <b>10</b>.</p>
<p id="p-0049" num="0057">The information processing apparatus <b>10</b> associated with the first embodiment mainly has a sensor information acquisition section <b>101</b>, a text selection section <b>103</b>, a display control section <b>105</b>, a user answer acquisition section <b>107</b>, a user answer evaluation section <b>109</b>, and a storage section <b>111</b> as shown in <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0050" num="0058">The sensor information acquisition section <b>101</b> is realized by a CPU (Central Processing Unit), a ROM (Read Only Memory), a RAM (Random Access Memory), communication apparatus, and so on, for example. The sensor information acquisition section <b>101</b> acquires sensor information output from various sensors including a sensor for detecting a user motion (hereafter also referred to as a motion sensor) and a sensor for detecting a current position of a user (hereafter also referred to as a position sensor). The motion sensor may include a three-axis acceleration sensor (including an acceleration sensor, a gravidity detection sensor, and a drop detection sensor, for example), a three-axis gyro sensor (including an angular velocity sensor, hand-shake correction sensor, and a geomagnetism sensor, for example). The position sensor may be a GPS (Global Positioning System) sensor for receiving data output from a GPS. It should be noted that the longitude and latitude of a current position can be obtained from the access points of an RFID (Radio Frequency Identification) device and a Wi-Fi (Wireless Fidelity) device and the information output from wireless base stations, for example, so that these detection means may be used as position sensors. The above-mentioned various sensors may be installed in the information processing apparatus <b>10</b> associated with the first embodiment or arranged externally to the information processing apparatus <b>10</b>.</p>
<p id="p-0051" num="0059">When the user moves, an acceleration change and a rotation around the gravity axis are detected by the above-mentioned motion sensor. The motion sensor outputs the information about the detected change and rotation. The sensor information acquisition section <b>101</b> acquires the information about the change and rotation output from the motion sensor as sensor information. At the same time, in response to a user action, the position sensor obtains positional information (longitude and latitude, for example) indicative of a place at which the user is located (the current position). The sensor information acquisition section <b>101</b> outputs the positional information output from the position sensor as sensor information.</p>
<p id="p-0052" num="0060">It should be noted that, if date information is not related with the acquired information in acquiring the information output from various sensors, then the sensor information acquisition section <b>101</b> may relate information indicative of the acquisition date with the acquired information.</p>
<p id="p-0053" num="0061">The sensor information acquisition section <b>101</b> outputs the acquired various types of sensor information to the text selection section <b>103</b>. Also, the sensor information acquisition section <b>101</b> may store acquired various types of information into the storage section <b>111</b> to be described later as log information.</p>
<p id="p-0054" num="0062">The text selection section <b>103</b> is realized by a CPU, a ROM, a RAM, and so on, for example. On the basis of the sensor information output from the sensor information acquisition section <b>101</b>, the text selection section <b>103</b> selects a text to be presented to the user from among two or more texts stored in the storage section <b>111</b> to be described later or the like.</p>
<p id="p-0055" num="0063">Selecting the text to be presented to the user from among two or more texts, the text selection section <b>103</b> outputs information corresponding to the selected text to the display control section <b>105</b> to be described later. Also, if the selected text is like a question sentence for prompting the user for the entry of an answer, for example, then the text selection section <b>103</b> outputs the information about the selected text to the user answer evaluation section <b>109</b> to be described later.</p>
<p id="p-0056" num="0064">It should be noted that the text selection section <b>103</b> may store the information about the selected text into the storage section <b>111</b> to be described later or the like as log information.</p>
<p id="p-0057" num="0065">The following describes a detail configuration of the text selection section <b>103</b> associated with the first embodiment.</p>
<p id="p-0058" num="0066">The display control section <b>105</b> is realized by a CPU, a ROM, a RAM, and so on, for example. The display control section <b>105</b> is a processing block for controlling the display of the contents of display screen to be displayed on a display section (not shown) of the information processing apparatus <b>10</b>. To be more specific, the display control section <b>105</b> refers to the information corresponding to a text output from the text selection section <b>103</b> to display a text (or a sentence) corresponding to that information on the display screen of the display section.</p>
<p id="p-0059" num="0067">If the text selected by the text selection section <b>103</b> is like a question sentence for prompting the user to enter an answer, then the display control section <b>105</b> displays on the display screen an evaluation result (or a correct/wrong evaluation of answer) of a user answer executed by the user answer evaluation section <b>109</b> to be described later.</p>
<p id="p-0060" num="0068">In controlling the display of the display screen, the display control section <b>105</b> can use various objects, such as icons, stored in the storage section <b>111</b> to be described later or the like or refer to various databases stored in the storage section <b>111</b> or the like.</p>
<p id="p-0061" num="0069">The user answer acquisition section <b>107</b> is realized by a CPU, a ROM, a RAM, and an input apparatus, for example. If a text selected by the text selection section <b>103</b> is like a question sentence for prompting the user to enter an answer, then the user answer acquisition section <b>107</b> acquires a user answer for the selected text. The user answer may be directly entered through a keyboard or a touch panel or entered through the selection of an object, such as an icon, corresponding to the answer by operating a mouse for example. The user answer acquisition section <b>107</b> acquires information corresponding to the user answer entered in any of various means and outputs the acquired information to the user answer evaluation section <b>109</b> to be described later.</p>
<p id="p-0062" num="0070">The user answer evaluation section <b>109</b> is realized by a CPU, a ROM, a RAM, and so on, for example. If a text selected by the text selection section <b>103</b> is like a question sentence for prompting the user to enter an answer, then the user answer evaluation section <b>109</b> executes an correct/wrong evaluation on the user answer output from the user answer acquisition section <b>107</b>.</p>
<p id="p-0063" num="0071">To be more specific, when the information about a selected text is supplied, the user answer evaluation section <b>109</b> refers to the information about the acquired text and acquires the information about the correct answer of the selected text (or question) from a database stored in the storage section <b>111</b> or the like. Next, the user answer evaluation section <b>109</b> compares the user answer output from the user answer acquisition section <b>107</b> with the correct answer to determine whether the user answer is correct or wrong.</p>
<p id="p-0064" num="0072">When the correct/wrong evaluation on the user answer is completed, the user answer evaluation section <b>109</b> may output an evaluation result to the display control section <b>105</b>. Displaying the evaluation result on the display screen by the display control section <b>105</b> allows the user of the information processing apparatus <b>10</b> to know whether the user's answer is correct or wrong.</p>
<p id="p-0065" num="0073">Also, when the correct/wrong evaluation on the user answer is completed, the user answer evaluation section <b>109</b> may output information indicative of the completion of the correct/wrong evaluation to the text selection section <b>103</b>. Outputting this information to the text selection section <b>103</b> allows the text selection section <b>103</b> to use the acquisition of this information as a trigger of a new processing operation. This allows the text selection section <b>103</b> to start a new processing operation, such as requesting the display control section <b>105</b> for displaying a newly selected text, for example.</p>
<p id="p-0066" num="0074">The user answer evaluation section <b>109</b> may store a log associated with a user answer evaluation result into the storage section <b>111</b> to be described later or the like.</p>
<p id="p-0067" num="0075">The storage section <b>111</b> is one example of a storage apparatus of the information processing apparatus <b>10</b> associated with the first embodiment. The storage section <b>111</b> stores various databases and various data for use by the text selection section <b>103</b> and the user answer evaluation section <b>109</b> in executing various processing operations.</p>
<p id="p-0068" num="0076">Also, the storage section <b>111</b> may store various kinds of log information. Further, the storage section <b>111</b> may appropriately store various parameters, information about the progression of the processing, and various databases that need to be stored in executing processing by the information processing apparatus <b>10</b> associated with the first embodiment.</p>
<p id="p-0069" num="0077">This storage section <b>111</b> is accessible for read/write operations by the component blocks of the information processing apparatus <b>10</b>.</p>
<p id="h-0007" num="0000">Configuration of Text Selection Section</p>
<p id="p-0070" num="0078">The following describes details of a configuration of the text selection section <b>103</b> associated with the first embodiment with reference to <figref idref="DRAWINGS">FIG. 2</figref>. <figref idref="DRAWINGS">FIG. 2</figref> is a block diagram illustrating an exemplary configuration of the text selection section <b>103</b> associated with the first embodiment.</p>
<p id="p-0071" num="0079">As shown in <figref idref="DRAWINGS">FIG. 2</figref>, the text selection section <b>103</b> associated with the first embodiment has a condition setting block <b>121</b>, an action pattern detection block <b>123</b>, a positional information analysis block <b>125</b>, a text analysis block <b>127</b>, a keyword conversion block <b>129</b>, and a text extraction block <b>131</b>.</p>
<p id="p-0072" num="0080">The condition setting block <b>121</b> is realized by a CPU, a ROM, a RAM, an input apparatus, and so on, for example. The condition setting block <b>121</b> is a processing block for setting, on the basis of a user operation, conditions for selecting a text from among two or more texts by the text extraction block <b>131</b> to be described later. When a text selection condition is entered by the user through the keyboard, mouse, touch panel, or button, for example, the condition setting block <b>121</b> outputs the entered information to the text extraction block <b>131</b> to be described later.</p>
<p id="p-0073" num="0081">Text selection conditions may be set appropriately. However, if the text selection section <b>103</b> selects a question sentence or sample sentence for language learning, for example, from two or more texts, then the following conditions are set:
<ul id="ul0003" list-style="none">
    <li id="ul0003-0001" num="0000">
    <ul id="ul0004" list-style="none">
        <li id="ul0004-0001" num="0082">the type of the language to be leaned;</li>
        <li id="ul0004-0002" num="0083">the linguistic level of the language to be learned;</li>
        <li id="ul0004-0003" num="0084">the user's movement and state (context to be described later);</li>
        <li id="ul0004-0004" num="0085">the type of location (the current position, a place frequently visited, a place to be visited next, and so on); and</li>
        <li id="ul0004-0005" num="0086">others.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0074" num="0087">Setting the above-mentioned conditions allows the user to automatically browse texts (or sentences) suited to a situation desired by the user.</p>
<p id="p-0075" num="0088">The action pattern detection block <b>123</b> is realized by a CPU, a ROM, a RAM, and so on, for example. By use of the sensor information output from the motion sensor, the action pattern detection block <b>123</b> detects a user motion pattern and state pattern. The motion and state patterns that can be detected by the action pattern detection block <b>123</b> include &#x201c;walking,&#x201d; &#x201c;running,&#x201d; &#x201c;still,&#x201d; &#x201c;jumping,&#x201d; &#x201c;train (ride/not ride),&#x201d; &#x201c;elevator (ride/not ride/up/down),&#x201d; and so on, for example. It should be noted that methods of detecting the motion and state patterns by the action pattern detection block <b>123</b> will be detailed later. It should also be noted that the methods of detecting the motion and state patterns are not limited to those described later; it also practicable to use machine learning for example. The motion and state patterns detected by the action pattern detection block <b>123</b> are entered in the text extraction block <b>131</b> to be described later.</p>
<p id="p-0076" num="0089">The following describes the function of the action pattern detection block <b>123</b> in detail with reference to <figref idref="DRAWINGS">FIG. 3</figref> through <figref idref="DRAWINGS">FIG. 10</figref>. <figref idref="DRAWINGS">FIG. 3</figref> through <figref idref="DRAWINGS">FIG. 10</figref> shows the function and operation of the action pattern detection block <b>123</b>.</p>
<p id="h-0008" num="0000">Configuration of Input/Output Data</p>
<p id="p-0077" num="0090">As described above, sensor information output from the motion sensor is entered in the action pattern detection block <b>123</b>. The sensor information to be acquired by the action pattern detection block <b>123</b> includes acceleration waveform data (hereafter referred to as acceleration data) for example. It should be noted that this acceleration data includes acceleration data in x direction (x-acc), acceleration data in y direction (y-acc), and acceleration data in z direction (z-acc). Here, x, y, and z are indicative of orthogonal directions. If a gyro sensor is installed, three-dimensional gyro data (x-gyro, y-gyro, and z-gyro) are entered as sensor information. It is desirable that these sensor data be calibrated because the sensitivity of the sensor changes with temperature, atmospheric pressure, and so on, for example.</p>
<p id="p-0078" num="0091">When sensor information is supplied, the action pattern detection block <b>123</b> detects motion and state patterns of a user on the basis of the supplied sensor information. The motion and state patterns that can be detected by the action pattern detection block <b>123</b> include &#x201c;walking,&#x201d; &#x201c;running,&#x201d; &#x201c;still,&#x201d; &#x201c;temporarily still,&#x201d; &#x201c;jumping,&#x201d; &#x201c;posture change,&#x201d; &#x201c;turn,&#x201d; &#x201c;train (ride/not ride),&#x201d; &#x201c;elevator (up/down),&#x201d; &#x201c;car (ride),&#x201d; &#x201c;bicycle (ride), and so on, for example (refer to <figref idref="DRAWINGS">FIG. 3</figref>).</p>
<p id="p-0079" num="0092">For example, take an algorithm for detecting a walking state. Normally, the frequency of the acceleration data that is detected when a human is walking is around 2 Hz (about two steps in one second). Therefore, the action pattern detection block <b>123</b> analyzes the frequency of the acceleration data to detect a portion with the frequency near 2 Hz. The portion detected by this processing is equivalent to motion and state pattern &#x201c;walking.&#x201d; In addition, the action pattern detection block <b>123</b> can detect the occurrence time and the continuation duration of &#x201c;walking&#x201d; motion and state pattern from the acceleration data. Further, the action pattern detection block <b>123</b> can detect &#x201c;walking&#x201d; intensity from the amplitude of the acceleration data.</p>
<p id="p-0080" num="0093">Thus, on the basis of the data, such as the frequency, strength, and so on, obtained by analyzing the sensor information, a feature quantity (hereafter referred to as motion and state feature quantity) of each motion and state pattern can be extracted. It should be noted that, in the case of &#x201c;walking&#x201d; motion and state pattern, only acceleration data is used; depending on the type of motion and state pattern, gyro data is also used. Obtaining time-depending change in motion and state feature quantity, the action pattern detection block <b>123</b> sequentially determines the motion and state patterns from the motion and state feature quantity, thereby outputting time-dependently changing motion and state patterns.</p>
<p id="p-0081" num="0094">The motion and state patterns thus obtained by the action pattern detection block <b>123</b> are entered in the text extraction block <b>131</b>.</p>
<p id="p-0082" num="0095">It should be noted that the action pattern detection block <b>123</b> can also detect user's action patterns in cooperation with the positional information analysis block <b>125</b> or the keyword conversion block <b>129</b> to be described later. For example, on the basis of action patterns executed by the user in a comparatively short time of several seconds to several minutes, such as &#x201c;walking,&#x201d; &#x201c;running,&#x201d; &#x201c;jumping,&#x201d; &#x201c;still,&#x201d; and so on, and various kinds of information provided from the positional information analysis block <b>125</b> or the keyword conversion block <b>129</b>, the action pattern detection block <b>123</b> can identify an action pattern that is executed in a longer time, such as &#x201c;meal,&#x201d; &#x201c;shopping,&#x201d; and &#x201c;work.&#x201d;</p>
<p id="p-0083" num="0096">For example, the cooperation with the positional information analysis block <b>125</b> or the keyword conversion block <b>129</b> allows identification that the current position of the user is in a restaurant for example. Therefore, if the user current position is moving inside a restaurant, it is determined that the user is walking and being still in the restaurant. So, for such an action pattern, the action pattern detection block <b>123</b> can identify an action pattern indicative of &#x201c;meal.&#x201d; If the user current position is moving in a building owned by a corporation or a so-called business street, then the action pattern detection block <b>123</b> can identify the user action pattern to be &#x201c;work.&#x201d;</p>
<p id="p-0084" num="0097">In addition, by further considering information about date, the action pattern detection block <b>123</b> can consider whether the timing of action pattern detection is a weekday or a holiday, thereby detecting action patterns more correctly.</p>
<p id="p-0085" num="0098">Further, if the personal information of the user (home address, office address, and so on, for example) is stored for use, referencing this personal information allows the more correct detection of action patterns.</p>
<p id="p-0086" num="0099">The detection of long-term action patterns, such as &#x201c;meal,&#x201d; &#x201c;shopping,&#x201d; and &#x201c;work&#x201d; mentioned above is executed before the executing of a detection algorithm to be described below and a detection result is output to the text extraction block <b>131</b>.</p>
<p id="p-0087" num="0100">The following describes more detail detection algorithms of some of the motion and state patterns shown in <figref idref="DRAWINGS">FIG. 3</figref>.</p>
<p id="h-0009" num="0000">Method of Recognizing Pausing/Still State</p>
<p id="p-0088" num="0101">First, a method of recognizing whether the user is pausing or being still will be described with reference to <figref idref="DRAWINGS">FIG. 4</figref>. <figref idref="DRAWINGS">FIG. 4</figref> shows a method of recognizing whether the user is pausing or being still.</p>
<p id="p-0089" num="0102">First, when the user moves, the corresponding sensor information is entered in the action pattern detection block <b>123</b>. Here, the acceleration data (x-acc, y-acc, and z-acc) of three-dimensional directions are entered. When the sensor information is entered, the action pattern detection block <b>123</b> records the sensor data in a FIFO format (S<b>1001</b>). When the data of a predetermined amount has been recorded, the action pattern detection block <b>123</b> computes each of the variances of x-acc, y-acc, and z-acc (S<b>1003</b>). Next, the action pattern detection block <b>123</b> extracts a maximum variance (det) for still-state evaluation that is the greatest variance among these variances (S<b>1005</b>).</p>
<p id="p-0090" num="0103">When maximum variance for still-state evaluation has been detected, the action pattern detection block <b>123</b> determines whether the extracted maximum variance for still-state evaluation is equal to or smaller than still-state recognition value D<sub>1 </sub>indicative of a still state (S<b>1007</b>). If the maximum variance for still-state evaluation is found to be neither equal to nor smaller than D<sub>1</sub>, then the action pattern detection block <b>123</b> determines that the user is not being still. If this determination is made, the user is estimated to be moving. Hence, the action pattern detection block <b>123</b> enters the information indicative that the user is not being still into the text extraction block <b>131</b> (S<b>1009</b>).</p>
<p id="p-0091" num="0104">On the other hand, if the maximum variance for still-state evaluation is found to be smaller than D<sub>1</sub>, then the action pattern detection block <b>123</b> determines whether the state in which the maximum variance is smaller than D<sub>1 </sub>continues longer than still-state recognition time T<b>1</b> (S<b>1011</b>). Here, T<b>1</b> is indicative of a shortest time in which the user is regarded as being still. If the maximum variance continues longer than T<b>1</b>, the action pattern detection block <b>123</b> determines that the user is being still and enters the information indicative of being still into the text extraction block <b>131</b> (S<b>1013</b>). If the maximum variance does not continue longer than T<b>1</b>, the action pattern detection block <b>123</b> determines that the user is pausing and enters the information indicative of a pausing state into the text extraction block <b>131</b> (S<b>1015</b>).</p>
<p id="p-0092" num="0105">As described above, executing the determination processing in accordance with the example shown in <figref idref="DRAWINGS">FIG. 4</figref> allows the determination of a still state, pausing state, and a non-still state.</p>
<p id="h-0010" num="0000">Method of Recognizing Walking/Running</p>
<p id="p-0093" num="0106">The following describes a method of recognizing whether the user is walking or running with reference to <figref idref="DRAWINGS">FIG. 5</figref>. <figref idref="DRAWINGS">FIG. 5</figref> shows a method of recognizing whether the user is walking or running.</p>
<p id="p-0094" num="0107">First, when the user moves, the corresponding sensor information is entered in the action pattern detection block <b>123</b>. Here, the acceleration data (x-acc, y-acc, and z-acc) of three-dimensional directions are entered. When the sensor information is entered, the action pattern detection block <b>123</b> removes the frequency outside the frequency range in which the user is recognized to be walking or running from the acceleration data (x-acc, y-acc, and z-acc) by use of bandpass filter (BPF) (S<b>1101</b>). Next, the action pattern detection block <b>123</b> records the acceleration data (x-acc, y-acc, and z-acc) that has passed the BPF in the FIFO format (S<b>1103</b>).</p>
<p id="p-0095" num="0108">Next, the action pattern detection block <b>123</b> reads the acceleration data (x-acc, y-acc, and z-acc) by a predetermined data amount recorded after the passing of the BPF to compute SACF (Summary Autocorrelation Function) (S<b>1105</b>) for the data that has been read. The time sequence of SACF peak corresponds to a periodical motion of the user that occurs during walking or running. However, SACF includes the harmonic component of the frequency corresponding to walking or running. Therefore, on the basis of a computed SACF, the action pattern detection block <b>123</b> computes an ESACF (Enhanced Summary Autocorrelation Function) (S<b>1107</b>). Next, the action pattern detection block <b>123</b> computes an autocorrelation peak on the basis of the ESACF (S<b>1109</b>) to obtain a walking/running evaluation frequency (freq).</p>
<p id="p-0096" num="0109">In addition, the action pattern detection block <b>123</b> records the acceleration data (x-acc, y-acc, and z-acc) before passing the BPF in step S<b>1101</b> in the FIFO format (S<b>1111</b>). Next, the action pattern detection block <b>123</b> reads the acceleration data (x-acc, y-acc, and z-acc) by a predetermined data amount to compute each variance (S<b>1113</b>). Then, the action pattern detection block <b>123</b> extracts the greatest variance from among the computed variances and outputs the extracted variance as a walking/running evaluation maximum variance (var) (S<b>1115</b>).</p>
<p id="p-0097" num="0110">Next, the action pattern detection block <b>123</b> multiplies the above-mentioned waling/running evaluation frequency (freq) by the above-mentioned walking/running evaluation maximum variance (var) (S<b>1117</b>). The number of steps in a unit time is expressed by freq. The magnitude of motion is expressed by var. Further, on the basis of the number of steps and the magnitude of motion, the action pattern detection block <b>123</b> can determine whether the user is walking or running. Therefore, by determining whether a product between freq and var falls within the range of a predetermined area, the action pattern detection block <b>123</b> can determine whether the user is walking or running. First, in order to enhance the accuracy of this evaluation, the action pattern detection block <b>123</b> removes, from the product between freq and var, the frequency area in which the wrong recognition of walking or running is easy through the lowpass filter (LPF), thereby computing a walking/running evaluation data det (S<b>1119</b>).</p>
<p id="p-0098" num="0111">Next, the action pattern detection block <b>123</b> determines whether the walking/running-state evaluation data is equal to or greater than minimum walking-state recognition value D<sub>2 </sub>that is the lower-limit value in which the user is recognized to be walking and equal to or smaller than maximum walking-state recognition value D<sub>3 </sub>that is the upper-limit value in which the user is recognized to be walking (S<b>1121</b>). If the walking/running-state evaluation data is found to be equal to or greater than D<sub>2 </sub>and equal to or smaller than D<sub>3</sub>, then the action pattern detection block <b>123</b> determines that the user is walking and enters the information indicative of walking into the text extraction block <b>131</b> (S<b>1123</b>). On the other hand, if not D<sub>2</sub>&#x2266;det&#x2266;D<sub>3</sub>, then the action pattern detection block <b>123</b> goes to step S<b>1125</b> to determine whether walking/running-state evaluation data det is equal to or greater than D<sub>3 </sub>(S<b>1125</b>).</p>
<p id="p-0099" num="0112">If the walking/running-state evaluation data is found to be greater than D<sub>3</sub>, then the action pattern detection block <b>123</b> determines that the user is running and enters the information indicative of running into the text extraction block <b>131</b> (S<b>1127</b>). On the other hand, if the walking/running-state evaluation data is found to be smaller than D<sub>2</sub>, then the action pattern detection block <b>123</b> determines that the user is neither walking nor running and enters the information indicative that the walking/running pattern is neither walking nor running into the text extraction block <b>131</b> (S<b>1129</b>). It should be noted that, by integrating freq, the information about the number of steps taken by the user in a time equivalent to an integration time can be obtained. Therefore, the action pattern detection block <b>123</b> computes the information about the number of steps (S<b>1131</b>) and enters the computed information into the text extraction block <b>131</b> (S<b>1133</b>).</p>
<p id="p-0100" num="0113">Thus, by executing the evaluation processing in accordance with the example shown in <figref idref="DRAWINGS">FIG. 5</figref>, the recognition of a walking state, a running state, and a non-walking/non-running state can be achieved.</p>
<p id="h-0011" num="0000">Method of Recognizing Jumping</p>
<p id="p-0101" num="0114">The following describes a method of recognizing whether the user is jumping or not with reference to <figref idref="DRAWINGS">FIG. 6</figref>. <figref idref="DRAWINGS">FIG. 6</figref> shows a method of recognizing whether the user is jumping or not.</p>
<p id="p-0102" num="0115">First, when the user moves, the corresponding sensor information is entered in the action pattern detection block <b>123</b>. Here, the acceleration data (x-acc, y-acc, and z-acc) of three-dimensional directions are entered. When the sensor information is entered, the action pattern detection block <b>123</b> computes a jumping acceleration expressed by the magnitudes of x-acc, y-acc, and z-acc (S<b>1201</b>). Next, the action pattern detection block <b>123</b> removes, through the bandpass filter (BPF), the frequency outside the jumping-state recognition value area in which the user is recognized to be jumping (S<b>1203</b>). Next, the action pattern detection block <b>123</b> computes an absolute value of the value that has passed the BPF and outputs the computed absolute value as a corrected jumping acceleration (S<b>1205</b>). Taking the absolute value as described allows the removal of a noise component caused by the swing or vibration of the housing caused by user's jumping motion, as compared unlike the case of using jumping acceleration.</p>
<p id="p-0103" num="0116">Next, the action pattern detection block <b>123</b> removes, through the lowpass filter (LPF), the frequency area in which jumping is easily recognized erroneously from the corrected jumping acceleration (S<b>1207</b>). Then, the action pattern detection block <b>123</b> computes a jumping-state evaluation value (det) for evaluating whether the user is jumping or not from the data that has passed the LPF. Next, the action pattern detection block <b>123</b> determines whether the jumping-state evaluation value is equal to or greater than minimum jumping-state recognition value D<sub>4 </sub>that is the lower-limit value in which the user is recognized to be jumping (S<b>1209</b>). If the jumping-state evaluation value is found to be equal to or greater than minimum jumping-stage recognition value D<sub>4</sub>, then the action pattern detection block <b>123</b> determines that the user is jumping and enters the information indicative of a jumping-state into the text extraction block <b>131</b> (S<b>1211</b>). On the other hand, if the jumping-state evaluation value is found to be smaller than the minimum jumping-state recognition value D<sub>4</sub>, then the action pattern detection block <b>123</b> determines that the user is not jumping and enters the information indicative that the user is not jumping into the text extraction block <b>131</b> (S<b>1213</b>).</p>
<p id="p-0104" num="0117">As described, executing the evaluation processing in accordance with the example shown in <figref idref="DRAWINGS">FIG. 6</figref> allows the determination of a jumping-state or a non-jumping state.</p>
<p id="h-0012" num="0000">Method of Recognizing Posture Change</p>
<p id="p-0105" num="0118">The following describes a method of recognizing whether the user is sitting or standing with reference to <figref idref="DRAWINGS">FIG. 7</figref>. <figref idref="DRAWINGS">FIG. 7</figref> shows a method of recognizing whether the user is sitting or standing. It should be noted that the recognition of sitting or standing is the recognition that the sitting user stands up or the standing user sits down. Namely, this recognition concerns a posture change of the user.</p>
<p id="p-0106" num="0119">First, when the user moves, the corresponding sensor information is entered in the action pattern detection block <b>123</b>. Here, the acceleration data (x-acc, y-acc, and z-acc) of three-dimensional directions are entered. When the sensor information is entered, the action pattern detection block <b>123</b> removes, through the lowpass filter (LPF), the frequency area in which user posture changing is easily recognized erroneously from the acceleration data (x-acc, y-acc, and z-acc) (S<b>1301</b>). Then, on the basis of the acceleration data (x-acc, y-acc, and z-acc), the action pattern detection block <b>123</b> computes x-grav, y-grav, and z-grav, respectively. The x-gray, y-grav, and z-grav are gravity data indicative of directions in which gravity is applied.</p>
<p id="p-0107" num="0120">Next, the action pattern detection block <b>123</b> computes value &#x3b4;(x-grav) indicative of a change in x-grave, &#x3b4;(y-grav) indicative of a change in y-grav, and &#x3b4;(z-grav) indicative of a change in z-grav (S<b>1303</b>). Then, the action pattern detection block <b>123</b> computes posture changing values indicative of the magnitudes of &#x3b4;(x-grav), &#x3b4;(y-grav), and &#x3b4;(z-grav) (S<b>1305</b>). Next, the action pattern detection block <b>123</b> removes, through the lowpass filter (LPF), the area in which user posture changing is erroneously recognized easily from the computed posture changing value (S<b>1307</b>) to compute a posture changing evaluation value (det) for determining whether a posture changing has occurred or not.</p>
<p id="p-0108" num="0121">Next, the action pattern detection block <b>123</b> determines whether the posture changing evaluation value is equal to or greater than a minimum posture changing recognition value D<sub>5 </sub>that is the lower-limit value in which the user is recognized to the changing in posture (S<b>1309</b>). If the posture changing evaluation value is found to be smaller than D<sub>5</sub>, then the action pattern detection block <b>123</b> determines that there is no change in posture and enters the information indicative of no posture changing into the text extraction block <b>131</b> (S<b>1311</b>). On the other hand, if the posture changing threshold value is found to be equal to or greater than D<sub>5</sub>, then the action pattern detection block <b>123</b> goes to step S<b>1313</b> to determine whether the user is currently standing or sitting (S<b>1313</b>). If the user is found to be already standing, the action pattern detection block <b>123</b> determines that the user has sat down and enters the information indicative of sitting down into the text extraction block <b>131</b> (S<b>1315</b>). On the other hand, if the user is already sitting, the action pattern detection block <b>123</b> determines that the user has stood up and enters the information indicative of the standing up into the text extraction block <b>131</b> (S<b>1317</b>).</p>
<p id="p-0109" num="0122">As described, executing the evaluation processing in accordance with the example shown in <figref idref="DRAWINGS">FIG. 7</figref> allows the determination whether there occurred a change in user posture.</p>
<p id="h-0013" num="0000">Method of Recognizing Going Up/Down by Elevator</p>
<p id="p-0110" num="0123">The following describes a method of recognizing whether the user is riding in an elevator or not with reference to <figref idref="DRAWINGS">FIG. 8</figref>. <figref idref="DRAWINGS">FIG. 8</figref> shows a method of recognizing whether the user is riding in an elevator or not.</p>
<p id="p-0111" num="0124">First, when the user moves, the corresponding sensor information is entered in the action pattern detection block <b>123</b>. Here, the acceleration data (x-acc, y-acc, and z-acc) of three-dimensional directions are entered. When the sensor information is entered, the action pattern detection block <b>123</b> removes, through the lowpass filter (LPF), the frequency area in which a gravity direction acceleration is erroneously recognized easily on the basis of the acceleration data (x-acc, y-acc, and z-acc) (S<b>1401</b>). Next, the action pattern detection block <b>123</b> computes gravity direction acceleration sensor data (acc) on the basis of the acceleration data (x-acc, y-acc, and z-acc) that has passed the LPF (S<b>1403</b>).</p>
<p id="p-0112" num="0125">In addition, in order to make the value of gravity adjustable, the action pattern detection block <b>123</b> computes gravity adjustment data expressed by the magnitude of acceleration data (x-acc, y-acc, and z-acc) and records the computed gravity adjustment data in the FIFO format (S<b>1405</b>, S<b>1407</b>). Next, the action pattern detection block <b>123</b> reads the gravity adjustment data by a predetermined data amount to compute a gravity adjustment variance (var) that is a variance of the gravity adjustment data (S<b>1409</b>). Further, the action pattern detection block <b>123</b> reads the gravity adjustment data by a predetermined data amount to compute gravity adjustment average data that is an average value of the gravity adjustment data (S<b>1409</b>).</p>
<p id="p-0113" num="0126">Next, the action pattern detection block <b>123</b> determines whether the above-mentioned gravity adjustment variance is equal to or smaller than maximum allowable gravity variance V that is a maximum variance permitting gravity adjustment (S<b>1411</b>). If the above-mentioned gravity adjustment variance is found to be greater than V, then the action pattern detection block <b>123</b> does not update the gravity value (S<b>1413</b>). On the other hand, if the above-mentioned gravity adjustment variance is found to be equal to or smaller than maximum allowable gravity adjustment variance V, then the action pattern detection block <b>123</b> determines whether the above-mentioned gravity adjustment average data is equal to or greater than minimum allowable gravity average value A<sub>1 </sub>that is a minimum average value permitting gravity adjustment and equal to or smaller than maximum allowable gravity average value A<sub>2 </sub>that is a maximum average value permitting gravity adjustment (S<b>1415</b>).</p>
<p id="p-0114" num="0127">If the above-mentioned gravity adjustment average data is found to be equal to or greater than A<sub>1 </sub>and equal to or smaller than A<sub>2</sub>, then the action pattern detection block <b>123</b> goes to step S<b>1419</b>. Otherwise, the action pattern detection block <b>123</b> does not update the gravity value (S<b>1417</b>). In step S<b>1419</b>, the action pattern detection block <b>123</b> removes, through the lowpass filter (LPF), the low area that is easily recognizable erroneously as gravity (S<b>1419</b>), thereby computing corrected gravity adjustment average data. Next, the action pattern detection block <b>123</b> computes a difference between the above-mentioned gravity direction acceleration sensor data and the above-mentioned corrected gravity adjustment average data (S<b>1421</b>). Next, from the computed difference, the action pattern detection block <b>123</b> removes a frequency area that is easily recognizable erroneously as that the user is riding in an elevator, thereby computing elevator up/down-state evaluation data (S<b>1423</b>).</p>
<p id="p-0115" num="0128">Next, the action pattern detection block <b>123</b> determines whether the elevator up/down-state evaluation data is equal to or greater than a predetermined value D<sub>6 </sub>(S<b>1425</b>). If the elevator up/down-state evaluation data is found to be equal to or greater than the predetermined value D<sub>6</sub>, then the action pattern detection block <b>123</b> goes to step S<b>1427</b>. On the other hand, if the elevator up/down-state evaluation data is found to be smaller than predetermined value D<sub>6</sub>, then action pattern detection block <b>123</b> goes to step S<b>1433</b>. It should be noted that the predetermined value D<sub>6 </sub>is a lower-limit value in which starting of going up of the user in an elevator can be recognized.</p>
<p id="p-0116" num="0129">In step S<b>1427</b>, the action pattern detection block <b>123</b> determines whether the elevator up/down-state evaluation data has exceeded predetermined value D<sub>6 </sub>for the first time or not (S<b>1427</b>). If the elevator up/down-state evaluation data is found to have exceeded predetermined value D<sub>6 </sub>for the first time, then the action pattern detection block <b>123</b> goes to step S<b>1429</b> to determine that the user is going up in an elevator, thereby entering information indicative of the going up in an elevator into the text extraction block <b>131</b> (S<b>1429</b>). On the other hand, if the elevator up/down-state evaluation data is found to have exceeded the predetermined value D<sub>6 </sub>not for the first time, then action pattern detection block <b>123</b> goes to step S<b>1431</b> to determine that going down in an elevator has ended, thereby entering information indicative of the end of going down in an elevator into the text extraction block <b>131</b> (S<b>1431</b>).</p>
<p id="p-0117" num="0130">In step S<b>1433</b>, the action pattern detection block <b>123</b> determines whether the elevator up/down-state evaluation data is equal to or smaller than a predetermined value D<sub>7 </sub>(S<b>1433</b>). It should be noted that the predetermined value D<sub>7 </sub>is an upper-limit value in which starting of going down of the user in an elevator can be recognized. If the elevator up/down-state evaluation data is found to be equal to or smaller than the predetermined value D<sub>7</sub>, the action pattern detection block <b>123</b> goes to step S<b>1435</b>. On the other hand, if the elevator up/down-state evaluation data is found to be greater than the predetermined value D<sub>7</sub>, then the action pattern detection block <b>123</b> goes to step S<b>1441</b>.</p>
<p id="p-0118" num="0131">In step S<b>1435</b>, the action pattern detection block <b>123</b> determines whether the elevator up/down-state evaluation data has gone below predetermined value D<sub>7 </sub>for the first time or not (S<b>1435</b>). If the elevator up/down-state evaluation data is found to have gone below the predetermined value D<sub>7 </sub>for the first time, the action pattern detection block <b>123</b> goes to step S<b>1437</b> to determine that the user is going down in an elevator, thereby entering information indicative of going down of the user in an elevator into the text extraction block <b>131</b> (S<b>1437</b>). On the other hand, if the elevator up/down-state evaluation data is found to have gone below the predetermined value D<sub>7 </sub>not for the first time, then the action pattern detection block <b>123</b> determines that the going up of the user in an elevator ended, thereby entering information indicative of the end of going up of the user in an elevator into the text extraction block <b>131</b> (S<b>1439</b>).</p>
<p id="p-0119" num="0132">In step S<b>1441</b>, the action pattern detection block <b>123</b> determines whether the user is currently riding in an elevator or not (S<b>1441</b>). If the user is found to be currently riding in an elevator, then the action pattern detection block <b>123</b> goes to step S<b>1443</b> to determine that the elevator is not in an acceleration or deceleration state, thereby entering the information indicative that the elevator is not in an acceleration or deceleration state into the text extraction block <b>131</b> (S<b>1443</b>). On the other hand, if the user is found to be not riding in an elevator, then the action pattern detection block <b>123</b> goes to step S<b>1445</b>, thereby entering the information indicative that the user is not riding in an elevator into the text extraction block <b>131</b> (S<b>1445</b>).</p>
<p id="p-0120" num="0133">As described above, executing the evaluation processing in accordance with the example shown in <figref idref="DRAWINGS">FIG. 8</figref> allows the determination whether the user is riding up or down in an elevator or not.</p>
<p id="h-0014" num="0000">Method of Recognizing Whether the User is Riding in a Train</p>
<p id="p-0121" num="0134">The following describes a method of recognizing whether the user is riding in a train or not with reference to <figref idref="DRAWINGS">FIG. 9</figref>. <figref idref="DRAWINGS">FIG. 9</figref> shows a method of recognizing whether the user is riding in a train or not.</p>
<p id="p-0122" num="0135">First, when the user moves, the corresponding sensor information is entered in the action pattern detection block <b>123</b>. Here, the acceleration data (x-acc, y-acc, and z-acc) of three-dimensional directions are entered. When the sensor information is entered, the action pattern detection block <b>123</b> removes, through the lowpass filter (LPF), a frequency area that is easily recognizable erroneously that the user is riding in a train on the basis of the acceleration data (x-acc, y-acc, and z-acc) (S<b>1501</b>). Next, on the basis of the acceleration data (x-acc, y-acc, and z-acc) with the above-mentioned frequency area removed, the action pattern detection block <b>123</b> computes acceleration data in the horizontal direction and the vertical direction (S<b>1503</b>, S<b>1505</b>). It should be noted that the horizontal direction and the vertical direction denote the directions relative to the ground on which the train is traveling.</p>
<p id="p-0123" num="0136">Next, the action pattern detection block <b>123</b> records the above-mentioned horizontal direction acceleration data and the above-mentioned vertical direction acceleration data by a predetermined data mount each in FIFO format (S<b>1507</b>, S<b>1509</b>). Then, the action pattern detection block <b>123</b> reads the horizontal direction acceleration data by a predetermined data amount to compute a horizontal direction variance (h-var) that is the variance of the horizontal direction acceleration data (S<b>1511</b>). Also, the action pattern detection block <b>123</b> reads the vertical direction acceleration data by a predetermined data amount to compute a vertical direction variance (v-var) that is the variance of the vertical direction acceleration data (S<b>1513</b>). The horizontal direction variance (h-var) is indicative of the degree of horizontal swinging or vibration that is detected when the train is running. The vertical direction variance (v-var) is indicative of the degree of vertical swinging or vibration that is detected when the train is running.</p>
<p id="p-0124" num="0137">Next, the action pattern detection block <b>123</b> determines whether the vertical direction variance (v-var) is equal to or greater than a minimum allowable vertical variance V<sub>1 </sub>that is a minimum allowable vertical direction variance and equal to or smaller than a maximum allowable vertical variance V<sub>2 </sub>that is a maximum allowable vertical direction variance (S<b>1515</b>). If the vertical direction variance (v-var) is found to be smaller than V<sub>1 </sub>or greater than V<sub>2</sub>, then the action pattern detection block <b>123</b> sets train ride evaluation data (det) to 0 (S<b>1517</b>). On the other hand, if the vertical direction variance is found to be equal to or greater than V<sub>1 </sub>and equal to or smaller than V<sub>2</sub>, then the action pattern detection block <b>123</b> goes to step S<b>1519</b>.</p>
<p id="p-0125" num="0138">In step S<b>1519</b>, the action pattern detection block <b>123</b> determines which is smaller, the vertical direction variance or the horizontal direction variance (S<b>1519</b>). If vertical direction variance (v-var) is found to be smaller, then the action pattern detection block <b>123</b> integrates vertical direction variance (v-var) by a predetermined data amount to compute an integration value (S<b>1521</b>). On the other hand, if horizontal direction variance (h-var) is found to be smaller, then the action pattern detection block <b>123</b> integrates horizontal direction variance (h-var) by a predetermined data amount to compute an integration value (S<b>1523</b>). Then, the integration values obtained in steps S<b>1521</b> and S<b>1523</b> are set to train ride evaluation data (det) that is used for determining whether the user is riding in a train or not.</p>
<p id="p-0126" num="0139">Next, the action pattern detection block <b>123</b> determines whether the train ride evaluation data is equal to or greater than a minimum train ride recognition value D<sub>8 </sub>that is the lower-limit value in which the user is recognized to be riding in a train (S<b>1525</b>). If the train ride evaluation data is found to be equal to or greater than D<sub>8</sub>, then the action pattern detection block <b>123</b> determines that the user is riding in a train, thereby entering information indicative that the user is riding in a train into the text extraction block <b>131</b> (S<b>1527</b>). On the other hand, if the train ride evaluation data is found to be smaller than D<sub>8</sub>, then the action pattern detection block <b>123</b> determines that the user is not riding in a train, thereby entering the information indicative that the user is not riding in a train into the text extraction block <b>131</b> (S<b>1529</b>).</p>
<p id="p-0127" num="0140">As described above, executing the evaluation processing in accordance with the example shown in <figref idref="DRAWINGS">FIG. 9</figref> allows the determination whether the user is riding in a train or not. By considering running states of a train, namely, an acceleration state through a deceleration state, the action pattern detection block <b>123</b> can determine that the user is riding in a train that is stopping at a station, the user is riding in a train that comes to a stop, the user gets out of a train that has arrived at a station and the user started walking from the train, and other states. These evaluation results may be supplied to the text extraction block <b>131</b> in configuration.</p>
<p id="h-0015" num="0000">Method of Recognizing Right-Turn/Left-Turn</p>
<p id="p-0128" num="0141">The following describes a method of recognizing whether the user has turned left or right with reference to <figref idref="DRAWINGS">FIG. 10</figref>. <figref idref="DRAWINGS">FIG. 10</figref> shows a method of recognizing whether the user has turned left or right.</p>
<p id="p-0129" num="0142">First, when the user moves, the corresponding sensor information is entered in the action pattern detection block <b>123</b>. Here, the acceleration data (x-acc, y-acc, and z-acc) of three-dimensional directions and the three-dimensional gyro data (x-gyro, y-gyro, and z-gyro) are entered. When the sensor information is entered, the action pattern detection block <b>123</b> removes, through the lowpass filter (LPF), a frequency area easily recognizable erroneously that the user is turning left or right from the entered sensor information (S<b>1601</b>). Next, the action pattern detection block <b>123</b> computes an angular velocity in the gravity direction on the basis of the acceleration data (x-acc, y-acc, and z-acc) of three-dimensional directions and the three-dimensional gyro data (x-gyro, y-gyro, and z-gyro) with the above-mentioned frequency area removed (S<b>1603</b>).</p>
<p id="p-0130" num="0143">Next, the action pattern detection block <b>123</b> removes, through the bandpass filter (BPF), a value outside of a turning recognition area for recognizing that the user is turning left or right from the computed angular velocity, thereby computing a corrected angular velocity (det) (S<b>1605</b>). Then, the action pattern detection block <b>123</b> determines whether the corrected angular velocity is equal to or smaller than a maximum right turn recognition value D<sub>9 </sub>that is the upper-limit value for recognizing that the user is turning right (S<b>1607</b>). If the angular velocity is found to be equal to or smaller than D<sub>9</sub>, then the action pattern detection block <b>123</b> determines that the user is turning right and enters the evaluation result into the text extraction block <b>131</b> (S<b>1609</b>). On the other hand, if the angular velocity is found to be greater than D<sub>9</sub>, then the action pattern detection block <b>123</b> goes to step S<b>1611</b>.</p>
<p id="p-0131" num="0144">In step S<b>1611</b>, the action pattern detection block <b>123</b> determines whether the corrected angular velocity is equal to or greater than a minimum left turn recognition value D<sub>10 </sub>that is the lower-limit value for recognizing that the user is turning left (S<b>1611</b>). If the angular velocity is found to be equal to or greater than D<sub>10</sub>, then the action pattern detection block <b>123</b> determines that the user is turning left and enters the information indicative that the user is turning left into the text extraction block <b>131</b> (S<b>1613</b>). On the other hand, if the angular velocity is found to be smaller than D<sub>10</sub>, then the action pattern detection block <b>123</b> determines that the user is not turning left or right and enters the evaluation result into the text extraction block <b>131</b> (S<b>1615</b>).</p>
<p id="p-0132" num="0145">As described above, executing the evaluation processing in accordance with the example shown in <figref idref="DRAWINGS">FIG. 10</figref> allows the determination whether the user is turning right or left.</p>
<p id="p-0133" num="0146">The details of the functions of the action pattern detection block <b>123</b> have been described. As described above, the motion and state patterns are not indicative of user's specific live behaviors. The motion and state patterns herein denote a state of the user at a point of time (or a relatively short time).</p>
<p id="p-0134" num="0147">Now, returning to <figref idref="DRAWINGS">FIG. 2</figref>, the configuration of the text selection section <b>103</b> associated with the first embodiment will be described. The positional information analysis block <b>125</b> is realized by a CPU, a ROM, a RAM, and so on, for example. The positional information analysis block <b>125</b> outputs positional information entered from the sensor information acquisition section <b>101</b> to the text extraction block <b>131</b> to be described later. Also, the positional information analysis block <b>125</b> analyzes positional information by use of the positional information output from the sensor information acquisition section <b>101</b> and the user action log information <b>133</b> stored in the storage section <b>111</b>.</p>
<p id="p-0135" num="0148">To be more detail, the positional information analysis block <b>125</b> updates the log of positional information that is a type of user action log information by use of the entered positional information. In doing so, the positional information analysis block <b>125</b> relates user-unique identification information (user ID) with the entered positional information to update the log of the positional information. Also, if a combination of longitude and latitude written to the entered positional information is indicative of a new place, then the positional information analysis block <b>125</b> may relate the identification information (place ID) unique to that place information with the corresponding positional information to record these pieces of information. Further, the positional information analysis block <b>125</b> may refer to the information about the date related with the positional information to identify a time zone in which the written time is included, thereby relating the identification information (time zone ID) corresponding to that time zone to record these pieces of information.</p>
<p id="p-0136" num="0149">Further, by use of the entered positional information and the log of the positional information, the positional information analysis block <b>125</b> analyzes a place to be frequently visited or a place to be visited after the current position. The analysis of a place to be frequently visited is executed by computing the frequency of user's visiting to each of places written to the positional information log, thereby determining a score on the basis of the computed frequency, for example. The analysis of a place to be visited next is executed by computing a conditional probability of user's moving from the current position to each of places written to the positional information log, thereby determining a score on the basis of the obtained conditional probability, for example. The larger the values of these scores, the surer the corresponding events.</p>
<p id="p-0137" num="0150">Referring to <figref idref="DRAWINGS">FIG. 11</figref>, there is shown an example of the user action log information <b>133</b> stored in the storage section <b>111</b> associated with the first embodiment of the invention. The user action log information <b>133</b> stores the positional information log and the information generated by various processing operations executed by the positional information analysis block <b>125</b>. Each of the processing blocks of the text selection section <b>103</b> is able to refer to this user action log information <b>133</b> in executing the processing of each processing block.</p>
<p id="p-0138" num="0151">The text analysis block <b>127</b> is realized by a CPU, a ROM, a RAM, a communication apparatus, and so on, for example. The text analysis block <b>127</b> analyzes each type of text stored in a text database (hereafter referred to as a text DB) stored in the storage section <b>111</b>. The analysis by the text analysis block <b>127</b> assigns an attribute to each word included in a text (or a sentence) and, at the same time, a motion and a state (a context) expressed by the text to the text itself. This analysis processing makes it clear when each text is used (a context) and what each word in a text points (a word attribute).</p>
<p id="p-0139" num="0152">First, the text analysis block <b>127</b> obtains each text stored in the text DB <b>135</b> and executes so-called morphological analysis on the obtained text. In executing morphological analysis, the text analysis block <b>127</b> uses various dictionaries included in the text analysis database (hereafter referred to as a text analysis DB) <b>137</b> stored in the storage section <b>111</b>. Consequently, the text is resolved into one or more words that constitute the text. The text selection section <b>103</b> associated with the first embodiment handles these words generated as described above as keywords. In addition, the text analysis block <b>127</b> refers to the dictionaries for use in morphological analysis to assign an attribute to each word. It should be noted that, in addition to morphological analysis, the text analysis block <b>127</b> may execute structural analysis or semantic analysis on texts as required.</p>
<p id="p-0140" num="0153">It should also be noted that, depending on words, various attributes may be assigned to one word. For example, word &#x201c;Ebisu&#x201d; is the name of a place in Tokyo, the name of Japanese gods (one of the Seven Auspicious Gods), and the name of a railroad station. As with this example, the text analysis block <b>127</b> assigns two or more attributes to a word rather than only one attribute when two or more attributes are assignable. Consequently, the text analysis block <b>127</b> is able to grasp one word in a multifaceted manner.</p>
<p id="p-0141" num="0154">When an attribute is assigned to a word constituting a text, the text analysis block <b>127</b> assigns a context (a motion or a state expressed by a text) of a text by use of the assigned attribute. In this case also, the text analysis block <b>127</b> assigns two or more contexts to a word rather than only one context when two or more contexts are assignable. Consequently, the text analysis block <b>127</b> grasps a context of one sentence in a multifaceted manner.</p>
<p id="p-0142" num="0155">When the resolution of a text into words and the subsequent assignment of attribute and context have been completed, the text analysis block <b>127</b> executes the scoring of a combination of each word (namely, a keyword) and attribute and the scoring of context. Consequently, the probability of an attribute for each word included in a text and the probability of a context can be put in numeral forms.</p>
<p id="p-0143" num="0156">The text analysis processing mentioned above executed by the text analysis block <b>127</b> is executed with a given timing. For example, when an unanalyzed text is added to the text DB <b>135</b> or the like, the text analysis block <b>127</b> may execute the text analysis processing described above. Also, the text analysis block <b>127</b> may extract an unanalyzed text at certain intervals (once a day for example) and execute the text analysis processing mentioned above on the extracted unanalyzed text.</p>
<p id="p-0144" num="0157">The following specifically describes the text analysis processing to be executed by the text analysis block <b>127</b> with reference to <figref idref="DRAWINGS">FIG. 12</figref> through <figref idref="DRAWINGS">FIG. 16</figref>. <figref idref="DRAWINGS">FIG. 12</figref> through <figref idref="DRAWINGS">FIG. 16</figref> are diagrams for explaining the text analysis processing to be executed by the text analysis block <b>127</b>.</p>
<p id="p-0145" num="0158">First, referring to <figref idref="DRAWINGS">FIG. 12</figref>, there is shown an outline of the text analysis processing to be executed by the text analysis block <b>127</b>. In the example shown in <figref idref="DRAWINGS">FIG. 12</figref>, the text analysis block <b>127</b> executes text analysis processing on a text (or sentence) &#x201c;In the Hotei station area, there was a beer factory in the past.&#x201d;</p>
<p id="p-0146" num="0159">In this case, the text analysis block <b>127</b> executes morphological analysis on a sentence in attention to divide the sentence into two or more words. In the example shown in <figref idref="DRAWINGS">FIG. 12</figref>, the sentence in attention is divided into nouns &#x201c;Hotei,&#x201d; &#x201c;station,&#x201d; &#x201c;area,&#x201d; &#x201c;past,&#x201d; &#x201c;beer,&#x201d; &#x201c;factory,&#x201d; and a verb &#x201c;be,&#x201d; and a preposition &#x201c;in,&#x201d; and articles &#x201c;a&#x201d; and &#x201c;the,&#x201d; and an adverb &#x201c;there.&#x201d; The text analysis block <b>127</b> assigns identification information (keyword ID) to these keywords and, on the basis of referred dictionaries and so on, attributes to these keywords. In the example shown in <figref idref="DRAWINGS">FIG. 12</figref>, keyword &#x201c;Hotei&#x201d; is assigned with attributes &#x201c;building: railroad: station,&#x201d; &#x201c;place name,&#x201d; &#x201c;proper noun: Japanese gods,&#x201d; and &#x201c;food: beer.&#x201d; As shown in <figref idref="DRAWINGS">FIG. 12</figref>, attributes to be assigned may be related with superordinate concepts, such as &#x201c;railroad&#x201d; and &#x201c;building,&#x201d; in addition to a subordinate concept, such as &#x201c;station.&#x201d; In addition, the text analysis block <b>127</b> computes a score for each combination of keyword and attribute and relates the computed score with a corresponding combination.</p>
<p id="p-0147" num="0160">Further, the text analysis block <b>127</b> assigns identification information (sentence ID) unique to each sentence in attention and, at the same time, assigns a context considered to correspond to the sentence in attention. In the example shown in <figref idref="DRAWINGS">FIG. 12</figref>, contexts, such as &#x201c;statement,&#x201d; &#x201c;moving: walking,&#x201d; and &#x201c;moving: train,&#x201d; are assigned to a sentence in attention and a score is computed for each context. As shown in <figref idref="DRAWINGS">FIG. 12</figref>, a superordinate concept, such as &#x201c;moving,&#x201d; may be related with each context in addition to subordinate concepts, such as &#x201c;walking&#x201d; and &#x201c;train.&#x201d;</p>
<p id="p-0148" num="0161"><figref idref="DRAWINGS">FIG. 13A</figref> and <figref idref="DRAWINGS">FIG. 13B</figref> show methods of estimating attributes of an entire text on the basis of the frequency of the attribute of each word existing in the text to determine the likelihood of the attribute of each word for the words constituting the text.</p>
<p id="p-0149" num="0162">In the example shown in <figref idref="DRAWINGS">FIG. 13A</figref>, the case with attention placed on sentence &#x201c;In the Hotei station area, there was a beer factory in the past&#x201d; is used. The text analysis block <b>127</b> executes morphological analysis on the sentence in attention to divide the sentence into morphemes &#x201c;Hotei,&#x201d; &#x201c;station,&#x201d; &#x201c;area,&#x201d; &#x201c;past,&#x201d; &#x201c;beer,&#x201d; &#x201c;factory,&#x201d; and &#x201c;be.&#x201d; In addition, by referencing dictionaries stored in a text analysis DB <b>137</b>, the text analysis block <b>127</b> assigns four types of attributes &#x201c;station,&#x201d; &#x201c;place name,&#x201d; &#x201c;gods,&#x201d; and &#x201c;drink&#x201d; to &#x201c;Hotei&#x201d; and, at the same time, assigns attributes also to each word in the same manner.</p>
<p id="p-0150" num="0163">With the entire sentence in attention, attributes &#x201c;station&#x201d; and &#x201c;drink&#x201d; exist in two each and the other attributes in one each. Therefore, in the entire sentence, it can be determined that it is highly possible for the sentence in attention to be about &#x201c;station&#x201d; and &#x201c;drink,&#x201d; Therefore, on the basis of this evaluation result, the text analysis block <b>127</b> can compute a score for each of keyword attributes. In the example shown in <figref idref="DRAWINGS">FIG. 13A</figref>, scores &#x201c;station (score: 0.4),&#x201d; &#x201c;drink (score: 0.4),&#x201d; &#x201c;gods (score: 0.1),&#x201d; and &#x201c;place name (score: 0.1)&#x201d; are assigned to keyword &#x201c;Hotei.&#x201d;</p>
<p id="p-0151" num="0164">In the example shown in <figref idref="DRAWINGS">FIG. 13B</figref>, sentence &#x201c;Hotei is the next station&#x201d; is analyzed. With this sentence, the text analysis block <b>127</b> executes analysis in the same manner as with the example shown in <figref idref="DRAWINGS">FIG. 13A</figref>, thereby determining that it is highly possible for the sentence to be about &#x201c;station.&#x201d; Therefore, on the basis of this evaluation result, the text analysis block <b>127</b> can compute a score for each of keyword attributes. In this example shown in <figref idref="DRAWINGS">FIG. 13B</figref>, scores &#x201c;station (score: 0.8),&#x201d; &#x201c;drink (score: 0.66),&#x201d; &#x201c;gods (score: 0.66),&#x201d; and &#x201c;place name (score: 0.66)&#x201d;</p>
<p id="h-0016" num="0000">are assigned to keyword &#x201c;Hotei.&#x201d;</p>
<p id="p-0152" num="0165">As shown in <figref idref="DRAWINGS">FIG. 13A</figref> and <figref idref="DRAWINGS">FIG. 13B</figref>, the scores computed for attributes are different from sentence to sentence in attention.</p>
<p id="p-0153" num="0166">In the example shown in <figref idref="DRAWINGS">FIG. 14</figref>, a collection having a large amount of sentences is analyzed in advance to generate a cluster of words constituting a sentence and a word cluster thus generated is used to assign word attributes. In this case, the text analysis block <b>127</b> determines to which cluster of the word clusters each of two or more words obtained as a result of morphological analysis belongs. For example, word &#x201c;Hotei&#x201d; shown in <figref idref="DRAWINGS">FIG. 14</figref> belongs to cluster &#x201c;gods&#x201d; to which word &#x201c;Fukurokuju&#x201d; belongs, cluster &#x201c;drink&#x201d; to which drink maker name &#x201c;Karin&#x201d; belongs, and cluster &#x201c;station.&#x201d; In such a case, the text analysis block <b>127</b> may regard that the activity in clusters &#x201c;station&#x201d; and &#x201c;drink&#x201d; is high, thereby assigning &#x201c;station&#x201d; and &#x201c;drink&#x201d; as attributes of &#x201c;Hotei.&#x201d;</p>
<p id="p-0154" num="0167">It should be noted that the methods of assigning attributes to words are not limited to those shown above; other methods are also available. Further, if there are sentences before and after a sentence in attention, forming a sequence of sentences associated with each other, the text analysis block <b>127</b> may use the analysis results of associated sentences to assign attributes to words.</p>
<p id="p-0155" num="0168">The following describes methods of assigning a context to a text with reference to <figref idref="DRAWINGS">FIG. 15</figref> and <figref idref="DRAWINGS">FIG. 16</figref>. <figref idref="DRAWINGS">FIG. 15</figref> and <figref idref="DRAWINGS">FIG. 16</figref> show methods of assigning a context to a text associated with the first embodiment of the invention.</p>
<p id="p-0156" num="0169">Referring to <figref idref="DRAWINGS">FIG. 15</figref>, there is shown a method of assigning a context to a text by use of an attribute frequency in a sentence and a dictionary file stored in the text analysis DB <b>137</b>.</p>
<p id="p-0157" num="0170">In the example shown <figref idref="DRAWINGS">FIG. 15</figref>, the case with attention placed on sentence &#x201c;In the Hotei station area, there was a beer factory in the past&#x201d; is used. In this method, if a certain classification category is set to a text in attention, the text analysis block <b>127</b> uses this classification category. In the example shown in <figref idref="DRAWINGS">FIG. 15</figref>, category &#x201c;statement&#x201d; is set to sample sentence &#x201c;In the Hotei station area, there was a beer factory in the past&#x201d; in advance, so that the text analysis block <b>127</b> uses this classification category (or the sample sentence category). It should be noted that category &#x201c;statement&#x201d; is a classification category that is assigned to a text (or a sentence) describing something.</p>
<p id="p-0158" num="0171">In addition, the text analysis block <b>127</b> executes morphological analysis on the text in attention in the method described above, thereby assigning an attribute to a word (or a keyword) existing in the text. On the basis of this assignment, the text analysis block <b>127</b> refers to a dictionary file stored in the text analysis DB <b>137</b> to extract the superordinate concept for the assigned attribute. It should be noted that, if there is no superordinate concept for the assigned attribute, the assigned attribute is used as it is.</p>
<p id="p-0159" num="0172">In the example shown in <figref idref="DRAWINGS">FIG. 15</figref>, analyzing the text assigns attributes &#x201c;station,&#x201d; &#x201c;place name,&#x201d; &#x201c;gods,&#x201d; &#x201c;drink,&#x201d; &#x201c;time,&#x201d; &#x201c;factory,&#x201d; and &#x201c;existence.&#x201d; The text analysis block <b>127</b> changes attributes for which superordinate concept exists by use of assigned attributes and the dictionary file to extract attributes &#x201c;railroad,&#x201d; &#x201c;food,&#x201d; &#x201c;proper noun,&#x201d; &#x201c;time,&#x201d; &#x201c;building,&#x201d; and &#x201c;existence.&#x201d;</p>
<p id="p-0160" num="0173">Subsequently, the text analysis block <b>127</b> identifies a context from the extracted attributes (of superordinate concept) by use of a mapping table that lists the correlation between attribute and context stored in the text analysis DB <b>137</b> or the like. In the example shown in <figref idref="DRAWINGS">FIG. 15</figref>, the attribute of superordinate concept &#x201c;railroad&#x201d; is related with context &#x201c;moving: train&#x201d; by use of the mapping table. Likewise, the attribute of superordinate concept &#x201c;food&#x201d; is related with context &#x201c;taking meal&#x201d; and the attribute of superordinate concept &#x201c;proper noun&#x201d; is related with context &#x201c;statement.&#x201d;</p>
<p id="p-0161" num="0174">The text analysis block <b>127</b> determines the context of a text in attention by use of the context related by the mapping table and a sample sentence category of the text in attention if any. For example, in <figref idref="DRAWINGS">FIG. 15</figref>, the text analysis block <b>127</b> determines, from the comparison of both, that the probability of context &#x201c;moving: train,&#x201d; &#x201c;taking meal,&#x201d; and &#x201c;statement&#x201d; is high.</p>
<p id="p-0162" num="0175">Consequently, the text analysis block <b>127</b> determines that the sentence context of the text in attention is &#x201c;moving: train,&#x201d; &#x201c;taking meal,&#x201d; and &#x201c;statement.&#x201d;</p>
<p id="p-0163" num="0176">On the other hand, in the example shown in <figref idref="DRAWINGS">FIG. 16</figref>, while learning a context set by a user through the condition setting block <b>121</b> or the like, the text analysis block <b>127</b> records a log of sentences used under a preset context condition. In the example shown in <figref idref="DRAWINGS">FIG. 16</figref>, as seen from the log information (or usage log) as shown in the figure, sentence X is often used in context &#x201c;moving: train.&#x201d; Therefore, the text analysis block <b>127</b> determines that it is probable that sentence X in attention is context &#x201c;moving: train.&#x201d; Thus, the example shown in <figref idref="DRAWINGS">FIG. 16</figref> is a method of determining the context of a sentence in attention by machine learning the log of context and feeding back an obtained learning result.</p>
<p id="p-0164" num="0177">It should be noted that the method based on the feedback technique shown above may be used along with other sentence context assigning methods.</p>
<p id="p-0165" num="0178">It should also be noted that a text on which analysis is executed by the text analysis block <b>127</b> is not limited to those recorded to the text DB <b>135</b>. The text analysis block <b>127</b> is able to analyze texts recorded to an externally connected device or a removable recording media connected to the information processing apparatus <b>10</b> or texts stored in various devices connected to the Internet or a home network, for example.</p>
<p id="p-0166" num="0179">If various keywords are entered from the keyword conversion block <b>129</b>, the text analysis block <b>127</b> executes analysis processing on the entered keywords and assigns attributes corresponding to the analyzed keywords. When attribute assignment to keywords is completed, the text analysis block <b>127</b> outputs the information indicative of the attributes assigned to keywords to the keyword conversion block <b>129</b>.</p>
<p id="p-0167" num="0180">The execution of the above-mentioned processing builds the text DB <b>135</b> as shown in <figref idref="DRAWINGS">FIG. 17</figref>. As shown in <figref idref="DRAWINGS">FIG. 17</figref>, the text DB <b>135</b> stores the information associated with stored sentences, the information associated with extracted keywords, the information associated with the correlation between sentence and keyword, and so on.</p>
<p id="p-0168" num="0181">For example, the information associated with sentences contains the information associated with texts stored in the text DB <b>135</b>. This information contains sentence-unique identification information (sentence ID), the information indicative of sentence type, the information indicative of sentence itself, the information associated with the level indicative of the degree of difficulty of sentence, and the identification information (language ID) indicative of language type. Each sentence is related with identification information (related sentence ID) indicative of related sentences.</p>
<p id="p-0169" num="0182">The text extraction block <b>131</b> to be described later is able to correctly extract texts suited to user's current position or action pattern by use of the text DB <b>135</b> described above.</p>
<p id="p-0170" num="0183">The keyword conversion block <b>129</b> is realized by a CPU, a ROM, a RAM, a communication apparatus, and so on, for example. The keyword conversion block <b>129</b> converts positional information output from the sensor information acquisition section <b>101</b> into a keyword associated with a place indicated by this positional information. This keyword conversion can be executed by use of various dictionaries and databases stored in the text analysis DB <b>137</b> or various servers that control network search engines, for example. By executing this keyword conversion processing, the keyword conversion block <b>129</b> can obtain various keywords, such as address, place name, names of nearby buildings, roads, and shops, and so on.</p>
<p id="p-0171" num="0184">In addition, the keyword conversion block <b>129</b> may refer not only to the positional information supplied from the sensor information acquisition section <b>101</b> but also to the user action log information <b>133</b> analyzed and updated by the positional information analysis block <b>125</b> to execute the keyword conversion processing on places frequently visited or places to be visited next, for example. Consequently, keywords associated with places to be visited by the user and associable with the place indicated by the positional information supplied from the sensor information acquisition section <b>101</b> can be obtained.</p>
<p id="p-0172" num="0185">The keyword conversion block <b>129</b> outputs the keywords thus obtained to the text analysis block <b>127</b> to request the text analysis block <b>127</b> for assigning attributes to the obtained keywords. At the same time, if attributes are assigned to the converted keywords, the keyword conversion block <b>129</b> outputs the keywords assigned with attributes to the text extraction block <b>131</b>.</p>
<p id="p-0173" num="0186">The text extraction block <b>131</b> is realized by a CPU, a ROM, a RAM, and so on, for example. On the basis of the context output from the action pattern detection block <b>123</b>, the positional information output from the positional information analysis block <b>125</b>, and the keywords output from the keyword conversion block <b>129</b>, the text extraction block <b>131</b> extracts a proper text from the two or more texts stored in the text DB <b>135</b>. In text extraction, the text extraction block <b>131</b> also considers various conditions set by the condition setting block <b>121</b>.</p>
<p id="p-0174" num="0187">To be more specific, on the basis of the entered context, setting conditions, keywords, attributes, and so on, the text extraction block <b>131</b> executes matching with the texts (and attributes and context assigned to the texts) stored in the text DB <b>135</b>. On the basis of this matching, the text extraction block <b>131</b> presents a text most suited to the entered conditions and so on to the user as a user presentation text. Consequently, a sentence most suited to user's current position or state (context) is presented to the user, enabling the user to refer to the sentence that provides the higher sense of presence.</p>
<p id="p-0175" num="0188">It should be noted that, in text extraction, a sentence may be extracted that matches in attribute but mismatches in keyword, for example. If this happens, the text extraction block <b>131</b> may appropriately replace a keyword in the extracted text by an attribute-matching keyword entered from the keyword conversion block <b>129</b>. This keyword replacing enables the presentation of a sentence having the higher sense of presence to the user.</p>
<p id="p-0176" num="0189">It should also be noted that, in the above description, text extraction processing is executed on the basis of positional information, information derived from positional information, conditions set by the condition setting block <b>121</b>, and user states (or a context) detected by the action pattern detection block <b>123</b>. However, if the text selection section <b>103</b> has no action pattern detection block <b>123</b>, the text extraction processing may be executed on the basis of positional information, information derived from positional information, and conditions set by the condition setting block <b>121</b>.</p>
<p id="p-0177" num="0190">Thus, one example of the functions of the information processing apparatus <b>10</b> associated with the first embodiment of the invention has been described. Each of the above-mentioned component elements may be configured by a general-purpose member for a circuit or a hardware device dedicated to the function of each component element. The function of each component element may all be carried out by a CPU and so on, for example. Therefore, the configuration to be used may be appropriately changed in accordance with technological levels valid at the time of practicing the present embodiment.</p>
<p id="p-0178" num="0191">It is practicable to write a computer program for realizing each of the functions of the information processing apparatus practiced as the first embodiment of the invention and install the written computer program in a personal computer or the like, for example. In addition, a computer-readable recording media storing such computer programs may be provided. This recording media may include a magnetic disk, an optical disk, a magneto-optical disk, and a flush memory, for example. Also, the above-mentioned computer program may be distributed through networks for example, rather than in a recording media.</p>
<heading id="h-0017" level="1">(1-2) Processing Flow of Information Processing Methods</heading>
<p id="p-0179" num="0192">The following briefly describes an exemplary processing flow of an information processing method associated with the present embodiment with reference to <figref idref="DRAWINGS">FIG. 18</figref> through <figref idref="DRAWINGS">FIG. 20</figref>.</p>
<p id="h-0018" num="0000">Processing Flow of a Text Analysis Method</p>
<p id="p-0180" num="0193">First, the processing flow of a text analysis method to be executed by the text analysis block <b>127</b> will be briefly described with reference to <figref idref="DRAWINGS">FIG. 18</figref>. <figref idref="DRAWINGS">FIG. 18</figref> shows a flowchart indicative of the processing flow of the text analysis method associated with the first embodiment of the invention.</p>
<p id="p-0181" num="0194">First, the text analysis block <b>127</b> obtains one unanalyzed sentence from the sample sentence and questions by language stored in the text DB <b>135</b> (S<b>101</b>). Next, the text analysis block <b>127</b> executes morphological analysis on the obtained unanalyzed sentence to determine a keyword attribute to be assigned to the sample sentence and question and a context in the manner described before (S<b>103</b>). Then, the text analysis block <b>127</b> writes the obtained keyword attribute and context to a corresponding location of the text DB <b>135</b> (S<b>105</b>).</p>
<p id="p-0182" num="0195">Subsequently, the text analysis block <b>127</b> determines whether there is any other unanalyzed sentence or not (S<b>107</b>). If an unanalyzed sentence is found, the text analysis block <b>127</b> returns to step S<b>101</b> to repeat the above-mentioned processing. If no unanalyzed sentence is found, the text analysis block <b>127</b> ends the text analysis processing.</p>
<p id="h-0019" num="0000">Processing Flow of a Text Selection Method</p>
<p id="p-0183" num="0196">The following briefly describes a text selection method to be executed by the information processing apparatus <b>10</b> associated with the first embodiment of the invention with reference to <figref idref="DRAWINGS">FIG. 19</figref> and <figref idref="DRAWINGS">FIG. 20</figref>. <figref idref="DRAWINGS">FIG. 19</figref> and <figref idref="DRAWINGS">FIG. 20</figref> show flowcharts indicative of the processing flows of the text selection method associated with the first embodiment.</p>
<p id="p-0184" num="0197">First, with reference to <figref idref="DRAWINGS">FIG. 19</figref>, the sensor information acquisition section <b>101</b> obtains sensor information output from various sensors (S<b>111</b>). The sensor information acquisition section <b>101</b> outputs the obtained sensor information to the action pattern detection block <b>123</b>, the positional information analysis block <b>125</b>, and the keyword conversion block <b>129</b> of the text selection section <b>103</b>.</p>
<p id="p-0185" num="0198">On the basis of the sensor information (the sensor information output from the motion sensor) supplied from the sensor information acquisition section <b>101</b>, the action pattern detection block <b>123</b> detects a user state to determine a user context (S<b>113</b>). When the context is determined, the action pattern detection block <b>123</b> outputs the information about the determined context to the text extraction block <b>131</b>.</p>
<p id="p-0186" num="0199">Also, in the basis of the sensor information (positional information) output from the sensor information acquisition section <b>101</b>, the positional information analysis block <b>125</b> executes various analyses associated with frequently visited places or places to be visited next (S<b>115</b>). Then, the positional information analysis block <b>125</b> reflects the obtained analysis result and positional information onto the user action log information <b>133</b>.</p>
<p id="p-0187" num="0200">By use of various databases and network search engines, the keyword conversion block <b>129</b> converts the positional information output from the sensor information acquisition section <b>101</b> into keywords, such as address and place name and the names of nearby buildings, roads, shops, and so on (S<b>119</b>). Then, the keyword conversion block <b>129</b> outputs the keywords obtained as a result of the conversion to the text analysis block <b>127</b>. The text analysis block <b>127</b> analyzes the keywords supplied from the keyword conversion block <b>129</b> (S<b>121</b>) to assign attributes to the analyzed keywords. When the assignment of attributes is completed, the text analysis block <b>127</b> outputs the information indicative of the attribute assigned to each keyword to the keyword conversion block <b>129</b>. Receiving the information indicative of the attribute assigned to each keyword, the keyword conversion block <b>129</b> outputs the obtained keywords and the attributes assigned thereto to the text extraction block <b>131</b>.</p>
<p id="p-0188" num="0201">On the basis of the context, setting conditions, keywords, attributes, positional information, and so on output from various processing blocks, the text extraction block <b>131</b> extracts a proper sample sentence or question from two or more samples sentences and questions stored in the text DB <b>135</b> (S<b>123</b>). If the extracted sample sentence or question is matching in attribute and context but mismatching in keyword, the extracted sample sentence or question may be edited in accordance with the keywords (S<b>125</b>). Then, the text extraction block <b>131</b> outputs the extracted sample sentence or question to the display control section <b>105</b> (S<b>127</b>). The display control section <b>105</b> displays the sample sentence or question received from the text extraction block <b>131</b> onto a display block, such as a display monitor, of the information processing apparatus <b>10</b>. Consequently, the user of the information processing apparatus <b>10</b> is able to browse sample sentences or questions suited to the user's current location and context selected by the text selection section <b>103</b>.</p>
<p id="p-0189" num="0202">With reference to <figref idref="DRAWINGS">FIG. 19</figref>, an example was described in which sample sentences and questions are extracted in accordance with the user's current location, attribute, keyword, context, and so on. The following describes an example of extracting sample sentences and questions without using user's context with reference to <figref idref="DRAWINGS">FIG. 20</figref>.</p>
<p id="p-0190" num="0203">First, the sensor information acquisition section <b>101</b> of the information processing apparatus <b>10</b> obtains the sensor information output from various sensors (S<b>131</b>). The sensor information acquisition section <b>101</b> outputs the obtained sensor information to the positional information analysis block <b>125</b> and the keyword conversion block <b>129</b> of the text selection section <b>103</b>.</p>
<p id="p-0191" num="0204">On the basis of the sensor information (the positional information) received from the sensor information acquisition section <b>101</b>, the positional information analysis block <b>125</b> executes various analyses associated with frequently visited places or places to be visited next (S<b>133</b>). Then, the positional information analysis block <b>125</b> reflects the obtained analysis result and positional information onto the user action log information <b>133</b> (S<b>135</b>).</p>
<p id="p-0192" num="0205">In addition, by use of various databases and network search engines, the keyword conversion block <b>129</b> converts the positional information received from the sensor information acquisition section <b>101</b> into keywords of address and place name and nearby buildings, roads, shops, and so on (S<b>137</b>). Next, the keyword conversion block <b>129</b> outputs the keywords obtained as a result of the conversion to the text analysis block <b>127</b>. The text analysis block <b>127</b> analyzes the keywords received from the keyword conversion block <b>129</b> (S<b>139</b>) and assigns attributes to the keywords. When the attribute assignment has been completed, the text analysis block <b>127</b> outputs the information indicative of the attribute assigned to each keyword to the keyword conversion block <b>129</b>. Receiving the information indicative of the attribute assigned to each keyword, the keyword conversion block <b>129</b> outputs the information indicative of the obtained keywords and the attributes assigned thereto to the text extraction block <b>131</b>.</p>
<p id="p-0193" num="0206">On the basis of the setting conditions, keywords, attributes, positional information, and so on output from various processing blocks, the text extraction block <b>131</b> extracts a proper sample sentence or question from two or more samples sentences and questions stored in the text DB <b>135</b> (S<b>141</b>). If the extracted sample sentence or question is matching in attribute or the like but mismatching in keyword, the extracted sample sentence or question may be edited in accordance with the keywords (S<b>143</b>). Then, the text extraction block <b>131</b> outputs the extracted sample sentence or question to the display control section <b>105</b> (S<b>145</b>). The display control section <b>105</b> displays the sample sentence or question received from the text extraction block <b>131</b> onto a display block, such as a display monitor, of the information processing apparatus <b>10</b>. Consequently, the user of the information processing apparatus <b>10</b> is able to browse sample sentences or questions suited to the user's current location and context selected by the text selection section <b>103</b>.</p>
<p id="p-0194" num="0207">As described above, the information processing apparatus <b>10</b> associated with the first embodiment of the invention is able to present to the user the sample sentences, questions, and problems that are high in the possibility of being used more practically, being suited to the situations associated with user's current location, frequently visited places, places to be visited next, and user's contexts, for example. Consequently, the information processing apparatus <b>10</b> associated with the first embodiment of the invention allows the user to be interested in learning, thereby maintaining the user's learning motivation at high levels. As a result, the user can have efficient learning.</p>
<p id="p-0195" num="0208">In addition, the information processing apparatus <b>10</b> associated with the first embodiment of the invention allows the automatic selection of sentences in accordance with the user's positional information. Therefore, applying the information processing apparatus <b>10</b> associated with the first embodiment of the invention to language learning or the like for example allows the automatic presentation of necessary sentences to the user while traveling for example. This allows the user to obtain foreign language conversation sentences suited to specific situations without searching two or more sentences.</p>
<p id="p-0196" num="0209">It should be noted that the above description has been made with attention paid especially to positional information. However, it is also practicable to do sentence selection by paying attention to the information associated with time, in place of positional information or in addition to positional information. This configuration allows the user to select sentences in accordance with the time at which the user operates the information processing apparatus <b>10</b>, thereby providing the automatic selection of timely sentences. In addition, obtaining not only time information but also the information associated with the current weather for example obtained from a network search engine for example allows the automatic selection of sentences that reflect the current weather.</p>
<heading id="h-0020" level="1">(2) Second Embodiment</heading>
<p id="p-0197" num="0210">The following describes an information processing apparatus and a questioning tendency setting method practiced as the second embodiment of the invention with reference to <figref idref="DRAWINGS">FIG. 21</figref> through <figref idref="DRAWINGS">FIG. 30</figref>.</p>
<p id="p-0198" num="0211">The information processing apparatus associated with the first embodiment of the invention has a function of automatically selecting texts suited to user's positional information and contexts. An information processing apparatus <b>10</b> associated with the second embodiment of the invention to be described below has a function of automatically setting questioning tendencies of questioning in match with user's learning levels. Use of the information processing apparatus <b>10</b> associated with the second embodiment allows the user to efficiently carry out his learning.</p>
<heading id="h-0021" level="1">(2-1) Exemplary Configuration of Information Processing Apparatus</heading>
<p id="p-0199" num="0212">First, an exemplary configuration of the information processing apparatus <b>10</b> associated with the second embodiment will be described in detail with reference to <figref idref="DRAWINGS">FIG. 21</figref>. <figref idref="DRAWINGS">FIG. 21</figref> shows a block diagram illustrating an exemplary configuration of the information processing apparatus <b>10</b> associated with the second embodiment.</p>
<p id="p-0200" num="0213">As shown in <figref idref="DRAWINGS">FIG. 21</figref>, the information processing apparatus <b>10</b> associated with the second embodiment has a display control section <b>105</b>, a user answer acquisition section <b>107</b>, a user answer evaluation section <b>109</b>, a storage section <b>111</b>, a questioning tendency setting section <b>141</b>, and a question selection section <b>143</b>.</p>
<p id="p-0201" num="0214">It should be noted that the display control section <b>105</b>, the user answer acquisition section <b>107</b>, and the storage section <b>111</b> are substantially the same in configuration of effect as the display control section <b>105</b>, the user answer acquisition section <b>107</b>, and the storage section <b>111</b> of the first embodiment, so that detail description of these functional blocks of the second embodiment will be skipped.</p>
<p id="p-0202" num="0215">The user answer evaluation section <b>109</b> associated with the second embodiment is substantially the same in configuration and effect as the user answer evaluation section <b>109</b> associated with the first embodiment except that the user answer evaluation section <b>109</b> associated with the second embodiment determines a user answer to a question set by the question selection section <b>143</b> and outputs correct/wrong information to the questioning tendency setting section <b>141</b>. Therefore, details description of the user answer evaluation section <b>109</b> associated with the second embodiment will be skipped.</p>
<p id="p-0203" num="0216">The questioning tendency setting section <b>141</b> is realized by a CPU, a ROM, a RAM, and so on for example. In accordance with user's learning levels (or user's degree of proficiency in learning), the questioning tendency setting section <b>141</b> automatically sets a tendency in questioning. The questioning tendency set by the questioning tendency setting section <b>141</b> includes the preferential questioning of questions similar to a given question or the repetitive questioning of questions not answered proficiently, in addition to the difficulty level of questions, for example.</p>
<p id="h-0022" num="0000">Detail Configuration Of Questioning Tendency Setting Section</p>
<p id="p-0204" num="0217">The following describes the configuration of the questioning tendency setting section <b>141</b> in further detail with reference to <figref idref="DRAWINGS">FIG. 22</figref>. <figref idref="DRAWINGS">FIG. 22</figref> is a block diagram illustrating an exemplary configuration of the questioning tendency setting section <b>141</b> associated with the second embodiment of the invention.</p>
<p id="p-0205" num="0218">As shown in <figref idref="DRAWINGS">FIG. 22</figref>, the questioning tendency setting section <b>141</b> associated with the second embodiment further has a user answer analysis block <b>151</b>, a forgetting curve generation block <b>153</b>, and a questioning condition setting block <b>155</b>.</p>
<p id="p-0206" num="0219">The user answer analysis block <b>151</b> is realized by a CPU, a ROM, a RAM, and so on for example. By use of a user answer correct/wrong evaluation result obtained in the user answer evaluation section <b>109</b>, the user answer analysis block <b>151</b> computes a correct-answer percentage and a wrong-answer percentage of answers made by the user. In addition, by use of the computed user answer correct percentage, the user answer analysis block <b>151</b> computes the difficulty level of the question.</p>
<p id="p-0207" num="0220">The following specifically describes the functions of the user answer analysis block <b>151</b>.</p>
<p id="p-0208" num="0221">Upon receiving a user answer correct/wrong evaluation result from the user answer evaluation section <b>109</b>, the user answer analysis block <b>151</b> update a correct-answer percentage table as shown in <figref idref="DRAWINGS">FIG. 23</figref> and computes a correct-answer percentage of questions corresponding to the correct/wrong evaluation result.</p>
<p id="p-0209" num="0222">The correct-answer table lists the number of correct answers and the number of questions for each identification information (questioning ID) unique to questioning for each user as shown in <figref idref="DRAWINGS">FIG. 23</figref>. This correct-answer percentage table is stored in a predetermined area of the storage section <b>111</b>, for example. For example, the number of correct answers for question ID<b>1</b> for user A is 5 and the number of questions is 20. In this case, suppose that user A have again solved a question corresponding to questioning ID<b>1</b>. Then, if the user answer is correct, one is added to the number of correct answers and the number of questions, which become 6 and 21, respectively. The correct-answer percentage becomes 0.29.</p>
<p id="p-0210" num="0223">It should be noted that, as the correct-answer percentage goes down, the user feels that the answer to a question in attention more difficult. Therefore, the user answer analysis block <b>151</b> can use the reciprocal number of the computed correct-answer percentage as a numeral form of the difficulty of the question. For example, with a question having the number of correct answers and the number of questions being 5 and 20, respectively, the correct-answer percentage is 0.25 and the difficulty is 4.00.</p>
<p id="p-0211" num="0224">Further, the user answer analysis block <b>151</b> updates an wrong-answer matrix as shown in <figref idref="DRAWINGS">FIG. 24</figref> by use of a correct/wrong evaluation result. As shown in <figref idref="DRAWINGS">FIG. 24</figref>, the wrong-answer matrix has the number of wrong answers and the number of questions for each questioning ID for each user. This wrong-answer matrix is stored in a predetermined area of the storage section <b>111</b>, for example. From a relation of the number of correct answers+ the number of wrong answers=the number of questions, the user answer analysis block <b>151</b> can easily generate a wrong-answer matrix. In addition, by use of a wrong-answer matrix, the user answer analysis block <b>151</b> computes a wrong-answer percentage.</p>
<p id="p-0212" num="0225">Further, by use of a correct/wrong evaluation result, the user answer analysis block <b>151</b> updates a table associated with a final answer date and the number of answers as shown in <figref idref="DRAWINGS">FIG. 25</figref>. As shown in <figref idref="DRAWINGS">FIG. 25</figref>, this table lists the final answer date and the number of answers for each questioning ID for each user. This table associated with a final answer date and the number of answers is stored in a predetermined area of the storage section <b>111</b>, for example. This table associated with a final answer date and the number of answers is used by the forgetting curve generation block <b>153</b> to be described later for generating forgetting curves.</p>
<p id="p-0213" num="0226">Upon updating of these tables, the user answer analysis block <b>151</b> updates two or more forgetting percentage tables (hereafter referred to as a forgetting percentage table group) as shown in <figref idref="DRAWINGS">FIG. 26</figref>. Each of the forgetting percentage tables is provided for each number of times answers are made and lists the number of correct answers and the number of questions for each elapsed time (for every day for example). The tables shown in <figref idref="DRAWINGS">FIG. 23</figref> through <figref idref="DRAWINGS">FIG. 25</figref> are managed for each user. The forgetting percentage table shown in <figref idref="DRAWINGS">FIG. 26</figref> is generated with reference to the number of times answers are made (without user distinction). The forgetting percentage table shown in <figref idref="DRAWINGS">FIG. 26</figref> is indicative of changes in the number of correct answers for each elapsed time (every day) with the number of times answers are made being q.</p>
<p id="p-0214" num="0227">It should be noted that the forgetting percentage table group generated by the user answer analysis block <b>151</b> are not limited to those generated for each questioning in each number of times answers are made as shown in <figref idref="DRAWINGS">FIG. 26</figref>; those tables which are generated for each block of questions (the English vocabulary of the level of seventh grade for example) are also practicable. Generating these forgetting percentage tables of each question block allows the judgment of user answer tendencies in a wider perspective.</p>
<p id="p-0215" num="0228">When the user answer analysis processing including the updating of various tables comes to an end, the user answer analysis block <b>151</b> notifies the forgetting curve generation block <b>153</b> and the questioning condition setting block <b>155</b> of the information about this end. Upon receiving the information about this end, the forgetting curve generation block <b>153</b> and the questioning condition setting block <b>155</b> start the processing thereof.</p>
<p id="p-0216" num="0229">The forgetting curve generation block <b>153</b> is realized by a CPU, a ROM, a RAM, and so on for example. By use of a forgetting percentage table group updated by the user answer analysis block <b>151</b>, the forgetting curve generation block <b>153</b> generates forgetting curves indicative of time-dependent changes in correct-answer percentage. One example of forgetting curves is shown in <figref idref="DRAWINGS">FIG. 27</figref>. As shown in <figref idref="DRAWINGS">FIG. 27</figref>, the forgetting curves are graphed with the horizontal axis being the time up to which the user forgets (namely, the elapsed time) and the vertical axis being the percentage in which the user remembers a matter in attention (namely, the correct-answer percentage). Here, the correct-answer percentage used for the vertical axis is an average of correct-answer percentages for each question (or a block of questions), for example. Because the forgetting curves are generated by use of the forgetting percentage table group shown in <figref idref="DRAWINGS">FIG. 26</figref>, a curve is generated for each number of times answers are made as shown in <figref idref="DRAWINGS">FIG. 27</figref>.</p>
<p id="p-0217" num="0230">The forgetting curve generation block <b>153</b> stores the generated forgetting curves into a predetermined area of the storage section <b>111</b>, for example. Consequently, the questioning tendency setting section <b>141</b> and the question selection section <b>143</b> associated with the second embodiment are able to use the generated forgetting curves in executing the processing of these blocks.</p>
<p id="p-0218" num="0231">Upon ending of the generation of forgetting curves, the forgetting curve generation block <b>153</b> notifies the questioning condition setting block <b>155</b> of the information indicative of the end of generating the forgetting curves.</p>
<p id="p-0219" num="0232">It should be noted that, if data is running short or there are many noises, the forgetting curve generation block <b>153</b> may make the generated forgetting curves regress to parametric functions.</p>
<p id="p-0220" num="0233">The questioning condition setting block <b>155</b> is realized by a CPU, a ROM, a RAM, and so on, for example. On the basis of a wrong-answer percentage computed by the user answer analysis block <b>151</b>, the questioning condition setting block <b>155</b> computes the similarity between two or more questions and, at the same time, the evaluation values of two or more questions by use of the computed similarity. In addition, the questioning condition setting block <b>155</b> updates the user correct-answer percentage computed by the user answer analysis block <b>151</b> by use of a correct-answer threshold computed by use of the above-mentioned forgetting percentage table group.</p>
<p id="p-0221" num="0234">The following specifically describes the updating of correct-answer percentage.</p>
<p id="p-0222" num="0235">By use of the user correct-answer percentage and the forgetting percentage table group stored in the storage section <b>111</b>, the questioning condition setting block <b>155</b> updates user correct-answer percentage p from equation 101 below if any of the following conditions applies. In equation 101 below, p denotes a user correct-answer percentage and r denotes a correct-answer threshold computed from the forgetting percentage table group. Further, &#x3b7; denotes a coefficient (a learning percentage) indicative of user's degree of learning, which is a parameter that is appropriately determined in advance. It should be noted that, in equation 101 below, the correct-answer percentage after updating is written as p&#x2032; for convenience.
<ul id="ul0005" list-style="none">
    <li id="ul0005-0001" num="0000">
    <ul id="ul0006" list-style="none">
        <li id="ul0006-0001" num="0236">Condition 1: user answer is wrong and p&#x3c;r</li>
        <li id="ul0006-0002" num="0237">Condition 2: user answer is correct and p&#x3e;r
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>p&#x2032;=p</i>+&#x3b7;(<i>r&#x2212;p</i>)&#x2003;&#x2003;(101)<?in-line-formulae description="In-line Formulae" end="tail"?>
</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0223" num="0238">It should be noted that, if the information processing apparatus <b>10</b> associated with the second embodiment of the invention is used for the first time, the questioning condition setting block <b>155</b> assumes that user correct-answer percentage p be a correct-answer percentage of the entire information processing apparatus (namely, an average of correct-answer percentages of all registered users, for example). Further, if the user made answer to a question m times and the m-th answer was made n days after the date of the last answer, then the questioning condition setting block <b>155</b> assumes that correct-answer threshold r be a correct-answer percentage computed from the number of correct answers and the number of questions written to the column after n days in the m-th forgetting percentage table.</p>
<p id="p-0224" num="0239">The user correct-answer percentage thus updated is usable in setting a period up to setting a question again as shown in <figref idref="DRAWINGS">FIG. 28</figref>, for example. To be more specific, in the information processing apparatus <b>10</b> associated with the second embodiment of the invention, questions (namely, the questions with the period shown in <figref idref="DRAWINGS">FIG. 28</figref> passed or passing from the time when the last answer was made) indicated by hatching shown in <figref idref="DRAWINGS">FIG. 28</figref> are automatically selected. Also, as described above, correct-answer percentage p is updated on the basis of equation 101 above if a predetermined condition is satisfied (to be more specific, in the case of a wrong answer, correct-answer percentage p is updated to increase and, in the case of a correct answer, correct-answer percentage p is updated to decrease). Hence, in the information processing apparatus <b>10</b> associated with the second embodiment, the period up to the re-questioning is also changed dynamically. To be more specific, as shown in <figref idref="DRAWINGS">FIG. 29</figref>, as correct-answer percentage p obtains higher ((a) in the figure), the questioning is executed again without interval; as correct-answer percentage p obtains lower ((b) in the figure), questioning is executed again with an interval.</p>
<p id="p-0225" num="0240">The updating of the correct-answer percentage shown in equation 101 above is desirably executed when a question set by the information processing apparatus <b>10</b> associated with the second embodiment is demanded to consider forgetting. However, if a question set by the information processing apparatus <b>10</b> associated with the second embodiment is not demanded to consider forgetting, the above-mentioned updating of correct-answer percentages need not be executed.</p>
<p id="p-0226" num="0241">Also, by use of the wrong-answer matrix updated by the user answer analysis block <b>151</b>, the questioning condition setting block <b>155</b> computes similarity sim(j, k) between question j and question k from equation (102) below. In equation (102) below, M(i, j) is a wrong-answer percentage in question j of user i, which is a value to be computed by use of the wrong-answer matrix (or the correct-answer percentage of question j of user i). In equation (102) below, parameter N denotes the number of registered users.</p>
<p id="p-0227" num="0242">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>sim</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mi>j</mi>
              <mo>,</mo>
              <mi>k</mi>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mfrac>
          <mrow>
            <munderover>
              <mo>&#x2211;</mo>
              <mrow>
                <mi>i</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mi>N</mi>
            </munderover>
            <mo>&#x2062;</mo>
            <mrow>
              <mrow>
                <mi>M</mi>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>i</mi>
                    <mo>,</mo>
                    <mi>j</mi>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mo>&#xb7;</mo>
              <mrow>
                <mi>M</mi>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>i</mi>
                    <mo>,</mo>
                    <mi>k</mi>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
          <mrow>
            <msqrt>
              <mrow>
                <munderover>
                  <mo>&#x2211;</mo>
                  <mrow>
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                  </mrow>
                  <mi>N</mi>
                </munderover>
                <mo>&#x2062;</mo>
                <msup>
                  <mrow>
                    <mi>M</mi>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mi>i</mi>
                        <mo>,</mo>
                        <mi>j</mi>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mn>2</mn>
                </msup>
              </mrow>
            </msqrt>
            <mo>&#xb7;</mo>
            <msqrt>
              <mrow>
                <munderover>
                  <mo>&#x2211;</mo>
                  <mrow>
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                  </mrow>
                  <mi>N</mi>
                </munderover>
                <mo>&#x2062;</mo>
                <msup>
                  <mrow>
                    <mi>M</mi>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mi>i</mi>
                        <mo>,</mo>
                        <mi>k</mi>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mn>2</mn>
                </msup>
              </mrow>
            </msqrt>
          </mrow>
        </mfrac>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>102</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0228" num="0243">Thus, the questioning condition setting block <b>155</b> is able to grasp the similar questions in a numeric form by the cognitive filtering (CF) technique for computing the degree of getting wrong answers to both question j and question k (a wrong-answer cooccurrence score).</p>
<p id="p-0229" num="0244">Next, by use of computed similarity sim(j, k) and the wrong-answer matrix, the questioning condition setting block <b>155</b> computes a score for each question by equation (103) below. In equation (103) below, P denotes a parameter indicative of a total number of questions.</p>
<p id="p-0230" num="0245">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <msub>
            <mi>S</mi>
            <mi>CF</mi>
          </msub>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mi>k</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <munderover>
            <mo>&#x2211;</mo>
            <mrow>
              <mi>j</mi>
              <mo>=</mo>
              <mn>1</mn>
            </mrow>
            <mi>P</mi>
          </munderover>
          <mo>&#x2062;</mo>
          <mrow>
            <mrow>
              <mi>sim</mi>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>j</mi>
                  <mo>,</mo>
                  <mi>k</mi>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>&#xb7;</mo>
            <mrow>
              <mi>M</mi>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>i</mi>
                  <mo>,</mo>
                  <mi>j</mi>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>103</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0231" num="0246">If it a question to be set is one that must consider forgetting, the questioning condition setting block <b>155</b> may correct an evaluation value as follows when any of the following conditions applies. In the conditions shown below, p denotes a user correct-answer percentage and r denotes a correct-answer percentage threshold that is computed from the forgetting percentage table group. It should be noted that correct-answer percentage threshold r is, if the user made answer to a question k m times and the m-th answer was made n days after the date of the last answer, a correct-answer percentage computed from the number of correct answers and the number of questions written to the column after n days in the m-th forgetting percentage table.
<ul id="ul0007" list-style="none">
    <li id="ul0007-0001" num="0000">
    <ul id="ul0008" list-style="none">
        <li id="ul0008-0001" num="0247">Condition a: if r&#x3e;p, then evaluation value S<sub>CF</sub>(k)=0</li>
        <li id="ul0008-0002" num="0248">Condition b: if r&#x2266;p, then evaluation value S<sub>CF</sub>(k)=(p&#x2212;r)&#xd7;S<sub>CF</sub>(k)</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0232" num="0249">Correcting the evaluation value as described above prevents a corresponding question from being set because the evaluation value becomes 0 in a situation where the user still remembers the question (or a situation where condition a is satisfied). In a situation where condition b is satisfied, it is possible for the user to have forgotten the contents of learning indicated by a question, so that the evaluation value of a question that the user may have forgotten more likely is corrected more.</p>
<p id="p-0233" num="0250">When the computation of evaluation value S<sub>CF</sub>(k) has been completed, the questioning condition setting block <b>155</b> outputs the computed evaluation value S<sub>CF</sub>(k) to the question selection section <b>143</b>.</p>
<p id="p-0234" num="0251">The detail configuration of the questioning tendency setting section <b>141</b> associated with the second embodiment has been described with reference to <figref idref="DRAWINGS">FIG. 22</figref> through <figref idref="DRAWINGS">FIG. 29</figref>. The following describes the information processing apparatus <b>10</b> associated with the second embodiment with reference to <figref idref="DRAWINGS">FIG. 21</figref> again.</p>
<p id="p-0235" num="0252">The question selection section <b>143</b> is realized by a CPU, a ROM, a RAM, and so on, for example. The question selection section <b>143</b> selects a question to be set from two or more questions on the basis of an evaluation value computed by the questioning condition setting block <b>155</b> and a user correct-answer percentage in a predetermined period or the predetermined number of questions.</p>
<p id="p-0236" num="0253">To be more specific, when evaluation value S<sub>CF</sub>(k) is notified from the questioning condition setting block <b>155</b>, the question selection section <b>143</b> first computes a correct-answer percentage of a question answered by the corresponding user immediately before. The question answered by the corresponding user immediately before may be a question answered a predetermined period (several days for example) before the current point of time or a question answered the predetermined number of questions from the current point of time, for example. This computation of a correct-answer percentage is executable by referencing the log information of correct/wrong evaluation results for each user and a table associated with the final answer date and the number of answer times stored in the storage section <b>111</b>.</p>
<p id="p-0237" num="0254">Next, the question selection section <b>143</b> computes an absolute values of a difference between the computed user correct-answer percentage immediately before and a question correct-answer percentage. The question correct-answer percentage is computable by referencing the correct-answer percentage table stored in the storage section <b>111</b>.</p>
<p id="p-0238" num="0255">Then, by use of the computed absolute values, the question selection section <b>143</b> selects the predetermined number of questions sequentially in the ascending order the absolute values and sorts the selected questions in the order of evaluation value S<sub>CF</sub>(k) associated with the corresponding questions.</p>
<p id="p-0239" num="0256">Next, the question selection section <b>143</b> selects the predetermined number of questions sequentially from the higher evaluation values S<sub>CF</sub>(k) that have been sorted above, thereby providing the questions to be answered by the user.</p>
<p id="p-0240" num="0257">When the questions to be answered by the user have been selected as above, the question selection section <b>143</b> outputs the information about the selected questions to the display control section <b>105</b> and the user answer evaluation section <b>109</b>.</p>
<p id="p-0241" num="0258">It should be noted that, in the description made above, a forgetting curve is generated for each question to set a correct-answer percentage line for each user as a parameter; it is also practicable to use an inverse arrangement. To be more specific, a forgetting curve may be generated for each user to set a correct-answer percentage line as parameter for each question.</p>
<p id="p-0242" num="0259">Actually, with many exercise books, a learning sequence is predetermined between questions, such as the solution of question B requires the knowledge of question A, for example. Hence, it is practicable to define beforehand a learning sequence, such as described above, between two or more questions registered in the storage section <b>111</b>, thereby storing the information associated with a question in attention and associated questions (the information associated with a learning sequence) as so-called meta data. If the information associated with a learning sequence is available, a method of setting questioning tendencies described below can be executed, for example.</p>
<p id="p-0243" num="0260">In other words, assume that the user be proficient with question A and set a question H more difficult than question A as a target. Then, in this case, the questioning tendency setting section <b>141</b> sets a route from question A to question H on the basis of the above-mentioned information associated with learning sequence. This route may be a shortest route for solving question H in a shortest manner or another route that is most efficient, not to say the shortest, for the user to learn without much straining himself. Setting such a route allows the questioning tendency setting section <b>141</b> to efficiently help the user reach a learning level targeted by him by setting questions along this route.</p>
<p id="p-0244" num="0261">Thus, one example of the functions of the information processing apparatus <b>10</b> associated with the second embodiment has been described. Each of the above-mentioned component elements may be configured by a general-purpose member for a circuit or a hardware device dedicated to the function of each component element. The function of each component element may all be carried out by the CPU and so on, for example. Therefore, the configuration to be used may be appropriately changed in accordance with technological levels valid at the time of practicing the second embodiment.</p>
<p id="p-0245" num="0262">It is practicable to write a computer program for realizing each of the functions of the information processing apparatus practiced as the second embodiment of the invention and install the written computer program in a personal computer or the like, for example. In addition, a computer-readable recording media storing such computer programs may be provided. This recording media may include a magnetic disk, an optical disk, a magneto-optical disk, and a flush memory, for example. Also, the above-mentioned computer program may be distributed through networks for example, rather than in a recording media.</p>
<heading id="h-0023" level="1">(2-2) Processing Flow of Questioning Tendency Setting Method</heading>
<p id="p-0246" num="0263">The following briefly describes the processing flow of the questioning tendency setting method that is executed in the information processing apparatus <b>10</b> associated with the second embodiment with reference to <figref idref="DRAWINGS">FIG. 30</figref>. <figref idref="DRAWINGS">FIG. 30</figref> shows the processing flow of the questioning tendency setting method associated with the second embodiment.</p>
<p id="p-0247" num="0264">First, the questioning tendency setting section <b>141</b> sets an initial value of a questioning level in a predetermined method (S<b>201</b>). An example of this initial value may be an average value of the correct-answer percentages of all registered users, for example.</p>
<p id="p-0248" num="0265">Next, on the basis of the questioning level set by the questioning tendency setting section <b>141</b>, the question selection section <b>143</b> determines a question to be set (S<b>203</b>). In this example, because the average value of the correct-answer percentages of all registered users is set as the initial value, for example, the questioning level is set on the basis of this average value of correct-answer percentages, thereby selecting questions.</p>
<p id="p-0249" num="0266">When a question is selected as described above and displayed on the display block, such as a display monitor, the user enters an answer to that question through a predetermined input apparatus. The user answer acquisition section <b>107</b> acquires the entered user answer and outputs the acquired user answer to the user answer evaluation section <b>109</b>.</p>
<p id="p-0250" num="0267">The user answer evaluation section <b>109</b> executes a correct/wrong evaluation on the user answer output from the user answer acquisition section <b>107</b> (S<b>205</b>). Consequently, whether the user answer is correct or wrong is determined. When an evaluation result is established, the user answer evaluation section <b>109</b> outputs the obtained evaluation result to the display control section <b>105</b> and to the user answer analysis block <b>151</b> and the questioning condition setting block <b>155</b> of the questioning tendency setting section <b>141</b>.</p>
<p id="p-0251" num="0268">On the basis of the notified correct/wrong evaluation result of the user answer, the user answer analysis block <b>151</b> executes user answer analysis processing, such as updating of various tables (S<b>207</b>). In addition, when the user answer analysis processing has been completed, the forgetting curves are also updated by the forgetting curve generation block <b>153</b>.</p>
<p id="p-0252" num="0269">Then, in accordance with the analysis result obtained by the user answer analysis block <b>151</b>, the questioning condition setting block <b>155</b> computes a correct-answer percentage, a similarity, and an evaluation value to change questioning levels and questing tendencies (S<b>209</b>). When questioning levels and questing tendencies have been changed, the changed questioning levels and questioning tendencies are notified to the question selection section <b>143</b>.</p>
<p id="p-0253" num="0270">Here, the question selection section <b>143</b> determines whether to continue questioning or not (S<b>211</b>). If a request is made by the user to stop questioning, then the information processing apparatus <b>10</b> ends the processing without continuing the questioning. If the questioning is to be continued, the question selection section <b>143</b> returns to step S<b>203</b> to determine a question to be set on the basis of the questioning level and so on set in step S<b>209</b>.</p>
<p id="p-0254" num="0271">The execution of the processing described above allows the information processing apparatus <b>10</b> associated with the second embodiment to automatically set the questioning tendency of questions in accordance with the user's learning level.</p>
<p id="p-0255" num="0272">It should be noted that, if a questioning level and so on are once set on the basis of a user answer analysis result and the processing shown in <figref idref="DRAWINGS">FIG. 30</figref> is subsequently executed, then it is desired to start the processing not from step S<b>201</b> but from step S<b>203</b>. This allows the user to restart the learning with the learning results acquired up to the last question (namely, the questioning level and so on that have been set) reflected even if the learning through the information processing apparatus <b>10</b> has been discontinued.</p>
<heading id="h-0024" level="1">(3) Third Embodiment</heading>
<heading id="h-0025" level="1">(3-1) Configuration of Information Processing Apparatus</heading>
<p id="p-0256" num="0273">The following briefly describes an information processing apparatus <b>10</b> practiced as a third embodiment of the invention with reference to <figref idref="DRAWINGS">FIG. 31</figref>. The information processing apparatus <b>10</b> associated with the third embodiment has the function of the information processing apparatus <b>10</b> associated with the first embodiment that a text suited to user's positional information and context is automatically selected and the function of the information processing apparatus <b>10</b> associated with the second embodiment that a questioning tendency of questions is automatically set in accordance with user's learning levels.</p>
<p id="p-0257" num="0274">As shown in <figref idref="DRAWINGS">FIG. 31</figref>, the information processing apparatus <b>10</b> associated with the third embodiment mainly has a sensor information acquisition section <b>101</b>, a display control section <b>105</b>, a user answer acquisition section <b>107</b>, a user answer evaluation section <b>109</b>, a storage section <b>111</b>, a questioning tendency setting section <b>161</b>, and a text selection section <b>163</b>.</p>
<p id="p-0258" num="0275">It should be noted that the sensor information acquisition section <b>101</b>, the display control section <b>105</b>, the user answer acquisition section <b>107</b>, and storage section <b>111</b> of the third embodiment are substantially the same in function and effect as those of the first embodiment and the second embodiment. Therefore, detail description of these sections will skipped.</p>
<p id="p-0259" num="0276">The user answer evaluation section <b>109</b> is substantially the same in function and effect as those of the first embodiment and the second embodiment except that the user answer evaluation section <b>109</b> associated with the third embodiment determines a user answer associated with a problem (or a question) set by the text selection section <b>163</b> and outputs the information associated with correct/wrong evaluation to the questioning tendency setting section <b>161</b>. Therefore, detail description of the user answer evaluation section <b>109</b> will be skipped.</p>
<p id="p-0260" num="0277">The questioning tendency setting section <b>161</b> is substantially the same in function and effect as the questioning tendency setting section <b>141</b> associated with the second embodiment except that the questioning tendency setting section <b>161</b> outputs computed evaluation value S<sub>CF</sub>(k) to the text selection section <b>163</b>. Therefore, detail description of the questioning tendency setting section <b>161</b> will be skipped.</p>
<p id="p-0261" num="0278">The text selection section <b>163</b> selects texts corresponding to a question on the basis of evaluation value S<sub>CF</sub>(k) output from the questioning tendency setting section <b>161</b>. Then, the text selection section <b>163</b> selects, from the texts selected on the basis of the evaluation value, a text suited to the information obtained from the sensor information acquisition section <b>101</b> in a method described with reference to the first embodiment. Thus, selecting a text to be presented to the user allows the automatic selection of a text suited to user's learning level and user's positional information and context.</p>
<p id="p-0262" num="0279">Thus, one example of the functions of the information processing apparatus <b>10</b> associated with the third embodiment has been described. Each of the above-mentioned component elements may be configured by a general-purpose member for a circuit or a hardware device dedicated to the function of each component element. The function of each component element may all be carried out by the CPU and so on, for example. Therefore, the configuration to be used may be appropriately changed in accordance with technological levels valid at the time of practicing the third embodiment.</p>
<p id="p-0263" num="0280">It is practicable to write a computer program for realizing each of the functions of the information processing apparatus practiced as the third embodiment of the invention and install the written computer program in a personal computer or the like, for example. In addition, a computer-readable recording media storing such computer programs may be provided. This recording media may include a magnetic disk, an optical disk, a magneto-optical disk, and a flash memory, for example. Also, the above-mentioned computer program may be distributed through networks for example, rather than in a recording media.</p>
<heading id="h-0026" level="1">(4) Exemplary Hardware Configuration of Information Processing Apparatus (Computer) Associated with the Embodiments of the Present Invention</heading>
<p id="p-0264" num="0281">The following describes in detail an exemplary hardware configuration of the information processing apparatus <b>10</b> associated with the embodiments of the present invention with reference to <figref idref="DRAWINGS">FIG. 32</figref>. <figref idref="DRAWINGS">FIG. 32</figref> is a block diagram illustrating an exemplary hardware configuration of the information processing apparatus <b>10</b> associated with the embodiments of the present invention.</p>
<p id="p-0265" num="0282">The information processing apparatus <b>10</b> has mainly a CPU <b>901</b>, a ROM <b>903</b>, and a RAM <b>905</b>. In addition, the information processing apparatus <b>10</b> has a host bus <b>907</b>, a bridge <b>909</b>, an external bus <b>911</b>, an interface <b>913</b>, a sensor <b>914</b>, an input apparatus <b>915</b>, an output apparatus <b>917</b>, a storage apparatus <b>919</b>, a drive <b>921</b>, a connection port <b>923</b>, and a communication apparatus <b>925</b>.</p>
<p id="p-0266" num="0283">The CPU <b>901</b> functions as an arithmetic and logical unit or a control apparatus, thereby controlling all or part of the operations of the information processing apparatus <b>10</b> as instructed by various programs recorded to the ROM <b>903</b>, the RAM <b>905</b>, the storage apparatus <b>919</b>, and a removable recording media <b>927</b>. The ROM <b>903</b> stores programs and parameters that are for use by the CPU <b>901</b>. The RAM <b>905</b> temporarily stores programs for use by the CPU <b>901</b> and parameters and so on that change from time to time in program execution. These functional units are interconnected by the host bus <b>907</b> configured by an internal bus, such as a CPU bus.</p>
<p id="p-0267" num="0284">The host bus <b>907</b> is connected to an external bus, such as a PCI (Peripheral Component Interconnect/Interface) bus, via the bridge <b>909</b>.</p>
<p id="p-0268" num="0285">The sensor <b>914</b> is a detection portion, such as a sensor for detection a user motion and a sensor for obtaining information indicative of a current position, for example. This detection portion includes motion sensors and a GPS sensor, for example. The motion sensors are three-axis acceleration sensors including an acceleration sensor, a gravity detection sensor, and a drop detection sensor or three-axis gyro sensors including an angular velocity sensor, handshake correction sensor, and a geomagnetism sensor. In addition, the sensor <b>914</b> may have various measuring devices, such as a thermometer, an illuminometer, and a hygrometer, for example.</p>
<p id="p-0269" num="0286">The input apparatus <b>915</b> is an operator portion that is operated by the user, such as a mouse, a keyboard, a touch panel, buttons, switches, levers, and so on, for example. Also, the input apparatus <b>915</b> may be a remote control portion (a so-called remote commander) based on infrared radiation or electromagnetic wave or an externally connected device <b>929</b>, such as a mobile phone or PDA corresponding to the operation of the information processing apparatus <b>10</b>, for example. Further, the input apparatus <b>915</b> is configured by an input control circuit and so on that generate an input signal on the basis of information entered by the user through the above-mentioned operator portion, for example, and supplies the generated input signal to the CPU <b>901</b>. Through this input apparatus <b>915</b>, the user of the information processing apparatus <b>10</b> is able to enter various kinds of data into the information processing apparatus <b>10</b> and gives instructions to thereto.</p>
<p id="p-0270" num="0287">The output apparatus <b>917</b> is configured by an apparatus that is able to visually or auditorily notify the user of the obtained information. This apparatus includes a display apparatus, such as a CRT display apparatus, a liquid crystal display apparatus, a plasma display apparatus, an EL display apparatus, or lamps, an audio output apparatus, such as a loudspeaker or headphones, a printer apparatus, a mobile phone, or a facsimile apparatus, for example. The output apparatus <b>917</b> outputs results obtained from various processing operations executed by the information processing apparatus <b>10</b>, for example. To be more specific, the display apparatus displays, in text or image, results obtained by various processing operations executed by the information processing apparatus <b>10</b>. On the other hand, the audio output apparatus converts audio signals composed of reproduced voice data or acoustic data into analog signals and outputs these converted analog signals from the loudspeaker, for example.</p>
<p id="p-0271" num="0288">The storage apparatus <b>919</b> is a data storage apparatus configured as one example of the storage section of the information processing apparatus <b>10</b>. The storage apparatus <b>919</b> is configured by a magnetic storage device like an HDD (Hard Disk Drive), a semiconductor storage device, an optical storage device, or a magneto-optical storage device, for example. This storage apparatus <b>919</b> stores programs and various kinds of data to be executed by the CPU <b>901</b> and various kinds of data externally obtained.</p>
<p id="p-0272" num="0289">The drive <b>921</b> is a reader/writer for recording media, which is incorporated in the information processing apparatus <b>10</b> or connected externally to thereto. The drive <b>921</b> reads information from the removable recording media <b>927</b>, such as a magnetic disk, an optical disk, a magneto-optical disk, or a semiconductor memory that is loaded on the drive <b>921</b> and outputs the read information to the RAM <b>905</b>. In addition, the drive <b>921</b> is able to write information to the removable recording media <b>927</b>, such as a magnetic disk, an optical disk, a magneto-optical disk, or a semiconductor memory that is loaded on the drive <b>921</b>. The removable recording media <b>927</b> is a DVD media, an HD-DVD media, or a Blu-ray media, for example. Also, the removable recording media <b>927</b> may be a compact flash (CF) (registered trade mark), or an SD (Secure Digital) memory card, for example. Further, the removable recording media <b>927</b> may be an IC (Integrated Circuit) card mounted on a non-contact IC chip or an electronic device, for example.</p>
<p id="p-0273" num="0290">The connection port <b>923</b> is a port for connecting a device directly to the information processing apparatus <b>10</b>. One example of the connection port <b>923</b> is a USB (Universal Serial Bus) port, an IEEE1394 port, or a SCSI (Small Computer System Interface) port, for example. Another example of the connection port <b>923</b> is an RS-232C port, an optical audio terminal, or an HDMI (High Definition Multimedia Interface) port, for example. Connecting the externally connected device <b>929</b> to this connection port <b>923</b> allows the information processing apparatus <b>10</b> to acquire various kinds of data from the externally connected device <b>929</b> and provide various kinds of data thereto.</p>
<p id="p-0274" num="0291">The communication apparatus <b>925</b> is a communication interface configured by a communication device or the like for connection to a communication network <b>931</b>. The communication apparatus <b>925</b> is a communication card or the like for wired or wireless LAN (Local Area Network), Bluetooth (registered trademark), or WUSB (Wireless USB), for example. Also, the communication apparatus <b>925</b> may be an optical communication router, an ADSL (Asymmetric Digital Subscriber Line) router, or a communication modem, for example. This communication apparatus <b>925</b> is able to transmit and receive signals and so on with the Internet or other communication apparatuses, for example, in accordance with a predetermined communication protocol, such as TCP/IP, for example. Further the communication network <b>931</b> connected to the communication apparatus <b>925</b> is configured by a network connected wiredly or wirelessly and may be the Internet, a household LAN, infrared ray communication, radio wave communication, or satellite communication, for example.</p>
<p id="p-0275" num="0292">Described above is one example of the hardware configuration that can realize the functions of the information processing apparatus <b>10</b> associated with the embodiments of the present invention. Each of the above-mentioned component elements may be configured by a general-purpose member for a circuit or a hardware device dedicated to the function of each component element. Therefore, the configuration to be used may be appropriately changed in accordance with technological levels valid at the time of practicing these embodiments.</p>
<p id="p-0276" num="0293">While preferred embodiments of the present invention have been described using specific terms, such description is for illustrative purpose only, and it is to be understood that changes and variations may be made without departing from the spirit or scope of the following claims.</p>
<p id="p-0277" num="0294">The present application contains subject matter related to that disclosed in Japanese Priority Patent Application JP 2010-101042 filed in the Japan Patent Office on Apr. 26, 2010, the entire content of which is hereby incorporated by reference.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08626797-20140107-M00001.NB">
<img id="EMI-M00001" he="16.93mm" wi="76.20mm" file="US08626797-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US08626797-20140107-M00002.NB">
<img id="EMI-M00002" he="9.14mm" wi="76.20mm" file="US08626797-20140107-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. An information processing apparatus comprising:
<claim-text>a memory storing instructions; and</claim-text>
<claim-text>one or more processors configured to execute the instructions to:</claim-text>
<claim-text>configure a sensor information acquisition section to acquire sensor information outputted from a sensor for detecting a user motion and sensor information outputted from a sensor for obtaining a user current location;</claim-text>
<claim-text>configure an action pattern detection block to analyze sensor information from said sensor for detecting a user motion acquired by said sensor information acquisition section to detect an action pattern corresponding to the acquired sensor information from a plurality of action patterns obtained by classifying user's actions that are executed in a comparatively short time of several seconds to several minutes;</claim-text>
<claim-text>configure a keyword conversion block to convert, on the basis of said sensor information indicative of a current location acquired by said sensor information acquisition section, said information indicative of a current location into at least one keyword associated with current information associated with said current location by use of databases and network search engines;</claim-text>
<claim-text>configure a positional information analysis block to generate information associated with a place of user's frequent visit and a place of user's probable visit next to the current location by use of information indicative of the current location acquired by said sensor information acquisition section and a log of said information indicative of a current location,</claim-text>
<claim-text>wherein said place of user's frequent visit is determined by computing the frequency of the user's visits from the log of said information and calculating a score on the basis of the computed frequency,</claim-text>
<claim-text>wherein said place of user's probable visit next to the current location is determined by computing a conditional probability of the user's moving from the current position to each of the places written to the log of said information and calculating a score on the basis of the obtained conditional probability, and</claim-text>
<claim-text>wherein the one or more processors further configure said keyword conversion block to convert said information associated with a place of user's frequent visit and a place of user's probable visit next to the current location into a keyword; and</claim-text>
<claim-text>configure a text extraction block to extract a text for user presentation from a plurality of texts on the basis of said action pattern detected by said action pattern detection block and said at least one keyword generated by said keyword conversion block.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more processors are further configured to execute the instructions to:
<claim-text>configure a text analysis block to analyze each of said plurality of texts to assign an attribute to each word making up each analyzed text and assign a context indicative of at least one of motion and state indicated by each of said plurality of texts thereto,</claim-text>
<claim-text>wherein the one or more processors further configure said keyword conversion block to output at least one keyword associated with said current location to said text analysis block, and</claim-text>
<claim-text>wherein the one or more processors further configure said text analysis block to assign an attribute to said at least one keyword associated with said current location outputted from said keyword conversion block.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The information processing apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>,
<claim-text>wherein the one or more processors further configure said text extraction block to refer to said context assigned to each of said plurality of texts, said each word included in each of said plurality of texts, and said attribute assigned to said each word to extract a text that matches at least any one of said action pattern detected by said action pattern detection block, at least one keyword generated by said keyword conversion block, and said attribute assigned to said keyword.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The information processing apparatus according to <claim-ref idref="CLM-00003">claim 3</claim-ref>,
<claim-text>wherein, if a text with which said attribute matches but said keyword does not match has been extracted, the one or more processors further configure said text extraction block to replace the word included in the extracted text by the keyword generated by said keyword conversion block.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,
<claim-text>wherein the one or more processors further configure said keyword conversion block to convert said information indicative of the current location into a keyword indicative of at least one of an address of the current location, a place name of the current location, and a name of a matter located near the current location.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. A text selection method comprising the steps of:
<claim-text>acquiring sensor information outputted from a sensor for detecting a user motion and sensor information outputted from a sensor for obtaining a user current location;</claim-text>
<claim-text>analyzing sensor information from said sensor for detecting a user motion acquired in the sensor information acquisition step to detect an action pattern corresponding to the acquired sensor information from a plurality of action patterns obtained by classifying user's actions that are executed in a comparatively short time of several seconds to several minutes;</claim-text>
<claim-text>converting, on the basis of said sensor information indicative of a current location acquired in the sensor information acquisition step, said information indicative of a current location into at least one keyword associated with current information associated with said current location by use of databases and network search engines;</claim-text>
<claim-text>generating information associated with a place of user's frequent visit and a place of user's probable visit next to the current location by use of information indicative of the current location acquired in the sensor information acquisition step and a log of said information indicative of a current location,</claim-text>
<claim-text>wherein said place of user's frequent visit is determined by computing the frequency of the user's visits from the log of said information and calculating a score on the basis of the computed frequency,</claim-text>
<claim-text>wherein said place of user's probable visit next to the current location is determined by computing a conditional probability of the user's moving from the current position to each of the places written to the log of said information and calculating a score on the basis of the obtained conditional probability, and</claim-text>
<claim-text>wherein said information associated with a place of user's frequent visit and a place of user's probable visit next to the current location is converted into a keyword as in the keyword conversion step; and</claim-text>
<claim-text>extracting a text for user presentation from a plurality of texts on the basis of said action pattern detected in the action pattern detection step and said at least one keyword generated in the keyword conversion step.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. A non-transitory computer-readable medium storing a program for causing, when executed by a processor, a computer capable of communication with a sensor for detecting a user motion and a sensor for detecting a current location to execute a method comprising:
<claim-text>acquiring sensor information outputted from a sensor for detecting a user motion and sensor information outputted from a sensor for obtaining a user current location;</claim-text>
<claim-text>analyzing sensor information from said sensor for detecting a user motion acquired by the sensor information acquisition function to detect an action pattern corresponding to the acquired sensor information from a plurality of action patterns obtained by classifying user's actions that are executed in a comparatively short time of several seconds to several minutes;</claim-text>
<claim-text>converting, on the basis of said sensor information indicative of a current location acquired by the sensor information acquisition function, said information indicative of a current location into at least one keyword associated with current information associated with said current location by use of databases and network search engines;</claim-text>
<claim-text>generating information associated with a place of user's frequent visit and a place of user's probably visit next to the current location by use of information indicative of the current location acquired by said sensor information acquisition function and a log of said information indicative of a current location,</claim-text>
<claim-text>wherein said place of user's frequent visit is determined by computing the frequency of the user's from the log of said information and calculating a score on the basis of the computed frequency,</claim-text>
<claim-text>wherein said place of user's probable visit next t the current location is determine by computing a conditional probability of the user's moving from the current position to each of the places written to the log of said information and calculating a score on the basis of the obtained conditional probability, and</claim-text>
<claim-text>wherein the process further configures said keyword conversion function to convert said information associate with a place of user's frequent visit and a place of user's probable visit next to the current location into a keyword; and</claim-text>
<claim-text>extracting a text for user presentation from a plurality of texts on the basis of said action pattern detected by the action pattern detection function and said at least one keyword generated by the keyword conversion function.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The information processing apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the apparatus is capable of assisting a user in learning a foreign language.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more processors are further configured to execute the instructions to:
<claim-text>configure a display control section to control the display of the contents of a display screen,</claim-text>
<claim-text>wherein controlling the display comprises referring to the text extracted by the text extraction block, and displaying the text onto the display screen.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The information processing apparatus according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the one or more processors are further configured to execute the instructions to:
<claim-text>configure a user answer acquisition section configured to execute the instructions to acquire an answer from the user if the text extracted by the text extraction block is phrased as a question prompting the user for an answer,</claim-text>
<claim-text>wherein the answer from the user may be directly entered through at least one of a keyboard or touch panel, or entered through the selection of an object on the display screen.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The information processing apparatus according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the one or more processors are further configured to execute the instructions to:
<claim-text>configure a user answer evaluation section to determine whether the answer from the user is correct or incorrect,</claim-text>
<claim-text>wherein determining whether the answer from the user is correct or incorrect comprises acquiring information about the correct answer to the question from the memory, and comparing the answer from the user to the correct answer.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The information processing apparatus according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the one or more processors are further configured to execute the instructions to:
<claim-text>configure a questioning tendency section to set a tendency in questioning,</claim-text>
<claim-text>wherein the tendency may include the preferential presentation of questions similar to a particular question,</claim-text>
<claim-text>wherein the tendency may include the preferential presentation of questions of a certain difficulty level, and</claim-text>
<claim-text>wherein the tendency may include repeating questions answered incorrectly.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The information processing apparatus according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, where the questioning tendency section further comprises:
<claim-text>a user analysis block configured to execute the instructions to calculate the percentage of correct answers made by the user and compute the difficulty level of the question;</claim-text>
<claim-text>a curve generation block configured to execute the instructions to generate curves on a graph showing time-dependent changes in the percentage of correct answers made by the user; and</claim-text>
<claim-text>a questioning condition setting block configured to execute the instructions to compute the similarity between two or more questions and update the percentage of correct answers made by the user.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The information processing apparatus according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the one or more processors are further configured to execute the instructions to:
<claim-text>configure a question selection section to select a question to be posed to the user from two or more questions based on predetermined criteria,</claim-text>
<claim-text>wherein the predetermined criteria includes an evaluation value calculated by the questioning condition setting block and the percentage of correct answers made by the user over a set time period or a set number of questions.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
