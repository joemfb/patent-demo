<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08627287-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08627287</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>11947606</doc-number>
<date>20071129</date>
</document-id>
</application-reference>
<us-application-series-code>11</us-application-series-code>
<us-term-of-grant>
<us-term-extension>1462</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>9</main-group>
<subgroup>44</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>9</main-group>
<subgroup>45</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>717124</main-classification>
<further-classification>717126</further-classification>
<further-classification>717141</further-classification>
</classification-national>
<invention-title id="d2e53">Prioritizing quality improvements to source code</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>4853851</doc-number>
<kind>A</kind>
<name>Horsch</name>
<date>19890800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>714 381</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6356285</doc-number>
<kind>B1</kind>
<name>Burkwald et al.</name>
<date>20020300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>717141</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6895577</doc-number>
<kind>B1</kind>
<name>Noble et al.</name>
<date>20050500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>717126</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>7024589</doc-number>
<kind>B2</kind>
<name>Hartman et al.</name>
<date>20060400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>7055130</doc-number>
<kind>B2</kind>
<name>Charisius et al.</name>
<date>20060500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>717126</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>7093238</doc-number>
<kind>B2</kind>
<name>Givoni et al.</name>
<date>20060800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>717126</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>7197427</doc-number>
<kind>B2</kind>
<name>Noonan et al.</name>
<date>20070300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>7861226</doc-number>
<kind>B1</kind>
<name>Episkopos et al.</name>
<date>20101200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>717124</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>7861229</doc-number>
<kind>B2</kind>
<name>Lammel et al.</name>
<date>20101200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>717141</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>7886272</doc-number>
<kind>B1</kind>
<name>Episkopos et al.</name>
<date>20110200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>717124</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>7945898</doc-number>
<kind>B1</kind>
<name>Episkopos et al.</name>
<date>20110500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>717124</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>7975257</doc-number>
<kind>B2</kind>
<name>Fanning et al.</name>
<date>20110700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>717124</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>8332822</doc-number>
<kind>B2</kind>
<name>Nagappan et al.</name>
<date>20121200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>717124</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>8347267</doc-number>
<kind>B2</kind>
<name>Givoni et al.</name>
<date>20130100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>717124</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2003/0037314</doc-number>
<kind>A1</kind>
<name>Apuzzo et al.</name>
<date>20030200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>717125</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2004/0040014</doc-number>
<kind>A1</kind>
<name>Ball</name>
<date>20040200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>717130</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>2005/0015675</doc-number>
<kind>A1</kind>
<name>Kolawa et al.</name>
<date>20050100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>2005/0223361</doc-number>
<kind>A1</kind>
<name>Belbute</name>
<date>20051000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>717124</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>2006/0041864</doc-number>
<kind>A1</kind>
<name>Holloway et al.</name>
<date>20060200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>717124</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>2006/0069961</doc-number>
<kind>A1</kind>
<name>Kalyanaraman</name>
<date>20060300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>2006/0070048</doc-number>
<kind>A1</kind>
<name>Li et al.</name>
<date>20060300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>2006/0101331</doc-number>
<kind>A1</kind>
<name>Wang et al.</name>
<date>20060500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>2006/0236156</doc-number>
<kind>A1</kind>
<name>Cunningham et al.</name>
<date>20061000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>714 38</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>2006/0241909</doc-number>
<kind>A1</kind>
<name>Morgan et al.</name>
<date>20061000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>702183</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>2007/0022407</doc-number>
<kind>A1</kind>
<name>Givoni et al.</name>
<date>20070100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>717124</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>2007/0094189</doc-number>
<kind>A1</kind>
<name>Yamamoto et al.</name>
<date>20070400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>2007/0162894</doc-number>
<kind>A1</kind>
<name>Noller et al.</name>
<date>20070700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>2007/0180429</doc-number>
<kind>A1</kind>
<name>Gogh et al.</name>
<date>20070800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>717126</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>2007/0234309</doc-number>
<kind>A1</kind>
<name>Davia et al.</name>
<date>20071000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>717130</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>2007/0288899</doc-number>
<kind>A1</kind>
<name>Fanning et al.</name>
<date>20071200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>717124</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>US</country>
<doc-number>2008/0263505</doc-number>
<kind>A1</kind>
<name>StClair et al.</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>717101</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>US</country>
<doc-number>2009/0044177</doc-number>
<kind>A1</kind>
<name>Bates et al.</name>
<date>20090200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>717131</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00033">
<document-id>
<country>US</country>
<doc-number>2009/0055804</doc-number>
<kind>A1</kind>
<name>Blaschek et al.</name>
<date>20090200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>717126</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00034">
<document-id>
<country>US</country>
<doc-number>2009/0265693</doc-number>
<kind>A1</kind>
<name>Bakowski</name>
<date>20091000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>717131</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00035">
<document-id>
<country>US</country>
<doc-number>2010/0180258</doc-number>
<kind>A1</kind>
<name>Takahashi</name>
<date>20100700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>717124</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00036">
<document-id>
<country>US</country>
<doc-number>2011/0022551</doc-number>
<kind>A1</kind>
<name>Dixon</name>
<date>20110100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>706 12</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00037">
<document-id>
<country>US</country>
<doc-number>2011/0041120</doc-number>
<kind>A1</kind>
<name>Nagappan et al.</name>
<date>20110200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>717126</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00038">
<document-id>
<country>US</country>
<doc-number>2013/0159964</doc-number>
<kind>A1</kind>
<name>Szpak</name>
<date>20130600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>717124</main-classification></classification-national>
</us-citation>
<us-citation>
<nplcit num="00039">
<othercit>Li et al., &#x201c;Code-coverage guided prioritized test generation&#x201d;, Aug. 9, 2006 Elsevier B. V., pp. 1187-1198.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00040">
<othercit>Ray et al., &#x201c;Code-based prioritization&#x2014;a pre-testing effort to minimize post-release failures&#x201d;, 2012 Springer-Verlag London Limited, pp. 279-292; &#x3c;http://link.springer.com/article/10.1007%2Fs11334-012-0186-3&#x3e;.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00041">
<othercit>J. Jenny Li, &#x201c;Prioritize Code for Testing to Improve Code Coverage of Complex Software&#x201d;, 2005 IEEE, ISSRE'05, pp. 1-11; &#x3c;http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&#x26;arnumber=1544723&#x3e;.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00042">
<othercit>Observable Reliability, 2010 ACM, ISEC'10, Feb. 25-27, 2010, Mysore, India, pp. 69-72; &#x3c;http://dl.acm.org/citation.cfm?doid=1730874.1730889&#x3e;.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00043">
<othercit>Do, et al., &#x201c;Prioritizing JUnit Test Cases: An Empirical Assessment and Cost-Benefits Analysis&#x201d;, at &#x3c;&#x3c;http://esquared.unl.edu/articles/downloadArticle.php?id=200&#x3e;&#x3e;, Aug. 2, 2005, pp. 36.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00044">
<othercit>Elbaum, et al., &#x201c;Prioritizing Test Cases for Regression Testing&#x201d;, at &#x3c;&#x3c;http://delivery.acm.org/10.1145/350000/348910/p102-elbaum.pdf?key1=348910&#x26;key2=7703466811&#x26;coll=GUIDE&#x26;dl=GUIDE&#x26;CFID=26276718&#x26;CFTOKEN=30781887&#x3e;&#x3e;, ACM, 2000, pp. 102-112.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00045">
<othercit>Jeffrey, et al., &#x201c;Test Case Prioritization Using Relevant Slices&#x201d;, available at least as early as Aug. 9, 2007, at &#x3c;&#x3c;http://www.cs.arizona.edu/&#x2dc;ngupta/papers/ngupta-compsac08.pdf&#x3e;&#x3e;, pp. 8.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>20</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>None</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>15</number-of-drawing-sheets>
<number-of-figures>15</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20090144698</doc-number>
<kind>A1</kind>
<date>20090604</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Fanning</last-name>
<first-name>Michael C</first-name>
<address>
<city>Redmond</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Nagappan</last-name>
<first-name>Nachiappan</first-name>
<address>
<city>Redmond</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Ball</last-name>
<first-name>Thomas J</first-name>
<address>
<city>Mercer Island</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="004" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Sandys</last-name>
<first-name>Sean</first-name>
<address>
<city>Millbrae</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Fanning</last-name>
<first-name>Michael C</first-name>
<address>
<city>Redmond</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Nagappan</last-name>
<first-name>Nachiappan</first-name>
<address>
<city>Redmond</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Ball</last-name>
<first-name>Thomas J</first-name>
<address>
<city>Mercer Island</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="004" designation="us-only">
<addressbook>
<last-name>Sandys</last-name>
<first-name>Sean</first-name>
<address>
<city>Millbrae</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Lee &#x26; Hayes, PLLC</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Microsoft Corporation</orgname>
<role>02</role>
<address>
<city>Redmond</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Dao</last-name>
<first-name>Thuy</first-name>
<department>2192</department>
</primary-examiner>
<assistant-examiner>
<last-name>Wang</last-name>
<first-name>Ben C</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">An exemplary method includes receiving source code having a plurality of code segments, providing a desired level of quality for the source code, analyzing the source code to assign a complexity measure to each of the plurality of code segments and assigning a level of code coverage to each of the plurality of code segments based at least in part on the desired level of quality and the complexity measures. An exemplary system prioritizes quality improvements to source code based, in part, on a quality assessment. Such a system can improve code quality by assigning higher test coverage levels to modules with higher complexity.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="169.50mm" wi="240.71mm" file="US08627287-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="269.32mm" wi="177.21mm" file="US08627287-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="265.09mm" wi="178.05mm" file="US08627287-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="216.92mm" wi="171.62mm" file="US08627287-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="263.06mm" wi="174.24mm" file="US08627287-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="257.22mm" wi="132.76mm" file="US08627287-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="264.58mm" wi="163.83mm" file="US08627287-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="256.88mm" wi="170.77mm" orientation="landscape" file="US08627287-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="239.69mm" wi="176.28mm" orientation="landscape" file="US08627287-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="262.30mm" wi="170.77mm" file="US08627287-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="266.02mm" wi="168.40mm" file="US08627287-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="262.30mm" wi="156.89mm" file="US08627287-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="263.23mm" wi="145.80mm" file="US08627287-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="262.30mm" wi="146.73mm" file="US08627287-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="263.23mm" wi="146.22mm" file="US08627287-20140107-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="206.25mm" wi="172.04mm" file="US08627287-20140107-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND</heading>
<p id="p-0002" num="0001">Software developers use various mechanisms to enhance the quality of source code during software development. Such mechanisms can add tremendous value by improving quality and user experience as well as by reducing the need for post-release patches. However, such mechanisms can be costly and constrained.</p>
<p id="p-0003" num="0002">Developers often aim to reach a certain code quality. Source code quality can be measured on attributes such as performance, compliance with expected functionality, robustness, complexity, efficiency, portability, usability of the source code, etc. In general, failures that occur during testing or during use of a software application serve as indicators of how well the application complies with its expected functionality (i.e., does the application function correctly or not?).</p>
<p id="p-0004" num="0003">A particular approach to finding failures in a software application includes generation of so-called &#x201c;test cases&#x201d;. Once generated, a set of test cases can be executed to test sections of source code for compliance with expected functionality. A common definition for a test case is a set of conditions or variables under which a tester will determine if a requirement or use case upon an application is partially or fully satisfied. Sometimes it takes many test cases to determine that a requirement is fully satisfied. Thus, a test case can be characterized by a set of conditions which help to determine whether a requirement upon an application is satisfied or not.</p>
<p id="p-0005" num="0004">In a test case approach to quality, test prioritization helps to determine a test execution order or an order of areas for test case development. For example, some tests may be more pervasive than others and hence results from a pervasive test may be dispositive, i.e., eliminate the need for one or more &#x201c;lower priority&#x201d; tests. In the latter instance, test prioritization may simply decide to develop test cases for highly traveled paths. While no particular logic exists as a standard for test prioritization, test prioritization nevertheless can lower costs by raising quality confidence in an efficient way.</p>
<p id="p-0006" num="0005">Once some source code is tested (or marked for testing), a code coverage metric can be assigned. Often code coverage is assigned after execution of a test case, especially where the degree to which the source code of a program will be tested by a test case is not known a priori.</p>
<p id="p-0007" num="0006">For all but the simplest of source code, testing is an iterative procedure. To assist in this iterative process, software developers use code coverage tools, for example, to highlight sections of the source code that have not been executed during testing. Risk of failure can be difficult to ascertain for an untested section. For such sections of the source code, further test case development can uncover issues and/or verify performance and, hence, improve quality.</p>
<p id="p-0008" num="0007">As described herein, conventional code coverage tools that merely allow a tester to identify untested code do not add significantly to quality control. Further, a need exists for test prioritization techniques that can make testing more efficient.</p>
<heading id="h-0002" level="1">SUMMARY</heading>
<p id="p-0009" num="0008">This summary is provided to introduce simplified concepts of prioritizing quality improvements to source code, which is further described below in the Detailed Description. Implementations for prioritizing quality improvements to the source code are also described.</p>
<p id="p-0010" num="0009">In one implementation, a system for prioritizing quality improvements to source code receives a plurality of modules associated with the source code as input. The system also receives a desired quality level for the source code as input. The system determines a complexity measure for each of the plurality of modules. The system then assigns a test coverage level to each of the plurality of modules based on the corresponding complexity measures and the desired quality level for the source code. The assignment of the test coverage level to each module optionally includes assigning higher test coverage levels to modules with higher complexity measures.</p>
<p id="p-0011" num="0010">This summary is provided to introduce a selection of concepts in a simplified form to be further described below in the Detailed Description. This summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used to limit the scope of the claimed subject matter.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0012" num="0011">The detailed description is set forth with reference to the accompanying figures. In general, same reference numbers in different figures refer to similar or identical items.</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 1</figref> shows exemplary plots illustrating relationships between attributes of bugs present in source code and multiple stages of a software application life cycle with respect to time.</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 2</figref> illustrates an exemplary analysis presenting relationships between code coverage and structural complexity.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 3</figref> is an exemplary plot of correlation co-efficient versus code coverage (e.g., branch coverage).</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 4</figref> illustrates an exemplary environment suitable for implementing prioritization of quality improvements to a source code.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 5</figref> illustrates an exemplary computing device for prioritizing quality improvements to the source code.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 6</figref> presents an exemplary schematic showing various components of the system used to assess a source code's likelihood-to-fail.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 7</figref> illustrates an exemplary presentation of a per-binary breakdown of the structural complexity and the code complexity information for a managed binary.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 8</figref> illustrates an exemplary presentation of sections of source code and associated code coverage measures, structural complexity metrics and risk levels.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 9</figref> illustrates exemplary attributes used to recommend test case development for one or more code segments.</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 10</figref> illustrates exemplary method(s) for executing and analyzing test cases corresponding to the source code.</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 11</figref> illustrates exemplary method(s) for analyzing complexity and recommending code coverage tests for the source code based on the analyzed complexity.</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 12</figref> illustrates exemplary method(s) for implementing prioritization of test case development</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 13</figref> illustrates exemplary method(s) for identification of locations in the source code of the software application for test case development.</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 14</figref> illustrates exemplary method(s) to implement a feedback mechanism for recursively prioritizing quality improvements to the source code based on the changes made to the source code.</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 15</figref> illustrates an exemplary computing device capable of implementing various exemplary methods described herein.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0028" num="0027">Various exemplary techniques can identify and prioritize test cases to improve quality of source code. Various techniques involve prioritizing test cases by assessing a likelihood of source code to fail. Such an assessment can be based on information from already executed test cases or from complexity characteristics and an understanding of relationships between code coverage, code complexity and code quality.</p>
<p id="p-0029" num="0028">As explained in more detail below, for some software applications, code coverage alone, can be a poor or misleading indicator of code quality (i.e., code with higher code coverage can actually have a higher chance of failure, at statistically significant levels, than code with lower code coverage).</p>
<p id="p-0030" num="0029">One measure of quality for a software application is the number of reported failures. Factors affecting cost to fix &#x201c;buggy&#x201d; code include the number of reported failures for a software application and the stage in the software development cycle at which the reported failures occurred. In general, failures found late in the software development cycle are more costly to fix. Thus, a strong incentive exists to fix failures early and to reduce the number of failures that occurring at later stages.</p>
<p id="p-0031" num="0030">Source code quality can be improved during software development using test cases that test functionality of a software application and identify failures in earliest of development stages. Another type of metric that can improve testing is code complexity. A variety of code complexity metrics exist that can identify segments of the source code that warrant testing.</p>
<p id="p-0032" num="0031">In one implementation, an exemplary system receives a quality level for some target source code, for example, as input by a tester. The quality level can be considered a threshold value that indicates a desired level of quality for the source code. For example, a six sigma quality processes exhibit failures of less than 3.4 per (one) million opportunities (DPMO). For complex code, given a stated level of quality, testing may not always provide an absolute guarantee; yet, given the stated quality level, testing can provide some assurance (e.g., at some confidence level). In this implementation, to determine code coverage for the given quality level, the system relies on code complexity and a priori knowledge of relationships between code complexity, code coverage and code quality. For example, if testing or actual use of a prior version of an application provided data as to coverage and quality, the complexity of the source code for the prior version can be used to understand the relationship between complexity, coverage and quality. This relationship can then be used to estimate code coverage for the new version. Again, as described herein, data indicate that code coverage alone is not always a predictor of quality. In general, the estimated code coverage is simply a starting point for an iterative testing process where test data are used as feedback for identifying segments of code that may require additional coverage to reach a desired level of code quality.</p>
<p id="p-0033" num="0032">In the foregoing implementation, the system computes a complexity measure for each of a plurality of code segments. The system can then assign a test coverage level to each of the code segments based on the complexity measure and the desired quality level for the source code. For example, based on some a priori knowledge of a relationship between complexity, coverage and quality, code segments with higher complexity measures are assigned higher levels of coverage. Test coverage levels indicate the number of test cases to be executed for one or more code segments in an effort to attain a desired quality level. Test coverage levels are often stated in terms of percentage of source code to be covered by test cases. As described herein, code coverage levels can be determined based on attributes such as artifacts from prior releases, commonly used travel path, number of people using the source code, frequency of usage of the code segment, etc.</p>
<p id="p-0034" num="0033">Multiple and varied implementations and embodiments are described below. In the following section, an exemplary environment that is suitable for practicing various implementations is described. After discussion of the environment, representative implementations of systems, devices, and processes for prioritizing quality improvements to the source code are described.</p>
<p id="h-0005" num="0000">Exemplary Computing Environment</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 1</figref> shows exemplary plots <b>100</b> illustrating relationships between attributes of bugs present in source code and multiple stages of a software application life cycle with respect to time. The plots <b>100</b> include a plot <b>102</b> representing a relationship <b>104</b> between cost to fix per bug and the stages of the software development cycle.</p>
<p id="p-0036" num="0035">The cost to fix per bug is the cost associated with fixing each bug or failure in a software application. Generally, each bug corresponds to a failure of the software application to comply with an expected functionality. For example, the cost associated with fixing each bug may be based on the market share lost due to the failure in the software application. In another example, the cost associated with fixing each bug may be based on the time required to debug and fix, or rectify the bug. In yet another example, the cost associated with fixing each bug may be based on the amount of money spent on fixing the bug.</p>
<p id="p-0037" num="0036">A software development cycle, generally, starts with a development stage. This stage includes writing a set of instructions, also referred to as coding hereinafter, which when executed demonstrate the expected functionality of the software application. The development stage is followed by a testing stage, where the implemented software application is tested for compliance with the expected functionality. A failure in the software application is reported if the expected functionality is not met by the application. Generally, a beta release stage occurs after testing, which includes exposing the software application to a set of users for the purpose of evaluation and getting feedback. The final stage of the software development cycle is the release or launch stage. In the release stage, the software application is considered to be reasonably &#x201c;bug free&#x201d; and market ready as having had most of the failures fixed.</p>
<p id="p-0038" num="0037">The relationship <b>104</b> represents changes in the cost to fix each bug with respect to the various stages in the software development cycle. The relationship <b>104</b> shows that the cost to fix each bug increases with each stage of the software development cycle. In one example, the cost to fix each bug may be based on the market share lost due to the bug. The cost to fix each bug therefore escalates sharply in the launch stage.</p>
<p id="p-0039" num="0038">In the development stage, the cost to fix each bug is the least as the coding can be easily corrected as developers often work in coordinate teams that expect bugs to be identified and fixed. The cost to fix each bug increases during the testing and the beta release stages. Each bug reported during the testing and the beta release stages requires debugging of the coded parts of the software application to fix the bug, which can lead to deferment of the release date of the software application in the market. However, if the bug is reported after the release, the cost to fix each bug increases exponentially with each bug reported. Failures in the software application, which correspond to the bugs, can decrease the market share of the software application as customers choose competing software applications or seek other solutions.</p>
<p id="p-0040" num="0039">The plots <b>100</b> further include a plot <b>106</b> representing the relationship <b>108</b> between the number of bugs found and the stages of the software development cycle. The relationship <b>108</b> represents changes in the number of bugs found as the stages of software application life cycle progress with time.</p>
<p id="p-0041" num="0040">The relationship <b>108</b> shows that the number of bugs found in the software application decreases with each stage of the software development cycle. Generally, during the development stage, the number of bugs found in the software application is the highest. The relationship <b>108</b> further indicates that the number of bugs found decreases during the testing and the beta release stages.</p>
<p id="p-0042" num="0041">Typically, as most of the software applications are executed during the development stage, a large section of the expected functionality is validated in that stage itself, thereby reducing the number of bugs found in the testing and the beta release stages. The number of bugs found further decreases in the testing and beta release stages, and is therefore the least when the software application is released in the market. However, as shown by the relationship <b>108</b>, the number of bugs in the software application remain non-zero throughout the release stage. Such dormant bugs may be found and reported even long after the release of the software application, especially because users may use an application in a manner that was not predicted (e.g., importing files with new types of formats, etc.)</p>
<p id="p-0043" num="0042">To improve the quality of the code and minimize the number of bugs found after the testing stage, test cases are developed and executed to cover some percentage of the source code. However, as mentioned, the extent of code covered during testing alone is not always a reliable indicator of code quality. This point is discussed below with reference to <figref idref="DRAWINGS">FIGS. 2 and 3</figref>.</p>
<p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. 2</figref> illustrates an exemplary analysis <b>200</b> that presents relationships between code coverage and code complexity. Code coverage is a measure used to describe the degree to which source code of a software program has been tested. Generally, a code coverage value (e.g., percent) can be calculated based on test results obtained from executed test cases for source code of an application. The test cases typically include a set of conditions to determine whether a requirement upon the software application is satisfied. The percentage of source code executed by the test cases can indicate the sections of the source code, which are covered by the test cases and can thus help evaluate code coverage.</p>
<p id="p-0045" num="0044">Code complexity, also referred to as the structural complexity, is related to the amount of effort needed to understand, maintain, and modify and the source code correctly. In one implementation, structural complexity is based on the number of linearly independent paths in the code. For example, structural complexity can be based on the number of decision points in the source code. In one embodiment, cyclomatic complexity can be used to determine structural complexity of source code. Cyclomatic complexity can be calculated by counting the number of decision points, such as if, else, do, while, throw, catch, etc., in code.</p>
<p id="p-0046" num="0045">The exemplary analysis <b>200</b> includes a graphical representation of a low complexity code <b>202</b> and a high complexity code <b>204</b>. The low complexity code <b>202</b> may signify, for example, lower branching within the source code which makes the source code easier to understand and test. The low complexity code <b>202</b> may also signify that modifications can be made to the source code at a lower risk of failure, which can result in lower number of bugs for the software application.</p>
<p id="p-0047" num="0046">The high complexity code <b>204</b> may signify, for example, higher branching code, which is comparatively difficult to understand and test. The high complexity code <b>204</b> can therefore warrant more test cases for better code coverage.</p>
<p id="p-0048" num="0047">The low complexity code <b>202</b> can include one or more nodes <b>206</b>-<b>1</b>, <b>206</b>-<b>2</b>, . . . , <b>206</b>-N, collectively referred to as <b>206</b> hereinafter. In one implementation, the nodes <b>206</b> correspond to modules in the source code. In another implementation, the nodes <b>206</b> can also correspond to a specific function or a code segment to implement a specific functionality in the source code. For example, the node <b>206</b>-<b>1</b> may implement a functionality of the software application for which the node <b>206</b>-<b>1</b> is dependent on the node <b>206</b>-<b>2</b> for execution. After the control is transferred to the node <b>206</b>-<b>2</b>, the node <b>206</b>-<b>2</b> is further dependent on the node <b>206</b>-<b>4</b> for its execution.</p>
<p id="p-0049" num="0048">Similarly, the high complexity code <b>204</b> can include one or more nodes <b>208</b>-<b>1</b>, <b>208</b>-<b>2</b>, . . . , <b>208</b>-N, collectively referred to as <b>208</b> hereinafter. In one implementation, the nodes <b>208</b> correspond to different modules, each of which implements a functionality of the source code. For example, the node <b>208</b>-<b>1</b> is a module which implements a functionality of the software application. The node <b>208</b>-<b>1</b> is dependent on the node <b>208</b>-<b>2</b> for its execution and, therefore, transfers the control to the node <b>208</b>-<b>2</b>. Similarly, the nodes <b>208</b>-<b>4</b>, <b>208</b>-<b>5</b>, <b>208</b>-<b>6</b> are dependent on the node <b>208</b>-<b>8</b> for execution, which is in turn dependent on the node <b>208</b>-<b>3</b>, making the source code complex to test and understand.</p>
<p id="p-0050" num="0049">The exemplary analysis <b>200</b> presents a plot <b>210</b> showing a relationship between the number of test cases (y-axis) with reference to the code coverage (x-axis) for both the low complexity code <b>202</b> and the high complexity code <b>204</b>. Here, 50% code coverage implies that 50% of the source code is covered by the test cases (i.e., 50% of possible pathways, etc., have been traversed). This also implies that the rest 50% of the source code is uncovered.</p>
<p id="p-0051" num="0050">The plot <b>210</b> shows that for a given code, the number of test cases executed is directly proportional to the code coverage. For example, the number of test cases executed increases with the increase in the code coverage percentage for code <b>202</b> and also for code <b>204</b>. For example, the number of test cases for low complexity code <b>202</b> increases when the code coverage increases from 50% to 80%.</p>
<p id="p-0052" num="0051">The plot <b>210</b> also shows that the number of test cases for different source codes can be different at the same code coverage levels. Conversely, if two source codes are tested using similar number of test cases, the code coverage for the two codes can be different. For example, the number of test cases executed for low complexity code <b>202</b> at 50% code coverage is less than the number of test cases for the high complexity code <b>204</b> at 50% code coverage while the number of test cases for the low complexity code <b>202</b> at 80% code coverage and for the high complexity code <b>204</b> at 50% code coverage are similar.</p>
<p id="p-0053" num="0052">The plot <b>210</b> demonstrates that every source code having the same code coverage percentage may not be interpreted as having been tested equally. Therefore, achieving a threshold percentage of code coverage may not be a reliable indicator of the code having been adequately tested to meet a certain quality, especially for higher complexity codes. The high complexity code <b>204</b>, for example, being more complex, can be harder to test and thus may warrant higher code coverage. This shows that the number of test cases to be executed for the source code to reach a particular code quality is not only based on a desired level of code coverage of the source code, but also on the structural complexity of the code. In some cases, if a relationship that links quality to code coverage alone is used for test case development, the tested code may actually have quality that is quite below the desired quality.</p>
<p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. 3</figref> shows an exemplary plot <b>302</b> of correlation coefficient versus branch coverage. A code coverage measure may be based on a branch coverage measure that indicates the percentage of branches in the source code that have been executed. An increase in the branch coverage measure can signify more instructions being executed and tested. The plot <b>302</b> can be used to represent a relationship between correlation coefficient of failures in a software application and arc code coverage. Arc code coverage often refers to a measure of code coverage for branching statements such as if, else, while, etc. in source code.</p>
<p id="p-0055" num="0054">The correlation coefficient is a measure of the correlation between number of failures in a software application and the code coverage measures. The correlation coefficient measure can thereby be used to examine a software application to determine whether code coverage can act as an indicator of number of failures of the application. Generally, it is expected that a higher code coverage results in a lower number of failures, i.e., the correlation coefficient should decrease with an increase in code coverage. However, in some cases, it can be seen that the number of failures uncovered during testing increases with increasing code coverage, at statistically significant levels.</p>
<p id="p-0056" num="0055">Plot <b>302</b> displayed in the <figref idref="DRAWINGS">FIG. 3</figref> shows changes in the correlation coefficient (y-axis) with respect to changes in the branch coverage measure (x-axis). The plot <b>302</b> shows an increase in the values of correlation coefficients with an increase in branch coverage measures when the branch coverage increases over a threshold (e.g., some percentage). For example, the bar <b>304</b> shows the value of the correlation coefficient as 0.15 when the block coverage is Y %, which increases to 0.24 when the coverage increases to a higher percentage Z %. For example, for some software applications, the values of the correlation coefficients decrease when the percentage of code coverage is less than 70% and then increase as the percentage of code coverage increases beyond 70%. This shows that, in some instances, there is little quality assurance and high uncertainty, in terms of failures, for a software application that has a code coverage of less than some particular percentage. Further, for code coverage greater than this percentage, such an application can have a positive correlation between code coverage and the number of failures. Therefore, as described herein, code coverage alone is not always a reliable indicator of quality of source code for a software application. In other words, for most complex applications, a quality based on code coverage alone has little assurance. However, other measures such as complexity of code can be used to improve quality of the code as discussed below.</p>
<p id="p-0057" num="0056">As described herein, code coverage can be based on any of variety of measures. For example, if a code segment has 30 possible states or paths, then code coverage may be given as a percentage of states or paths tested to total states or paths. After tests are run, code coverage is a short-hand phrase for code execution data, which may be a report or other presentation to comprehend what occurred during execution of the tests.</p>
<p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. 4</figref> shows an exemplary network environment <b>400</b> suitable for implementing various techniques for prioritization of quality improvements to source code. For discussion purposes, the network environment <b>400</b> includes a developer <b>402</b>, a tester <b>404</b>, and a supervisor <b>406</b>.</p>
<p id="p-0059" num="0058">The developer <b>402</b> implements functionality of a software application. The developer <b>402</b> writes a set of instructions, act of which is referred to as coding, which when executed implement a specific functionality. Depending on specifics of a development cycle, the developer <b>402</b> may also write and execute test cases for testing the functionality of the software application.</p>
<p id="p-0060" num="0059">In the environment <b>400</b>, the tester <b>404</b> tests a software application for its compliance with an expected functionality. Again, depending on specifics of a development cycle, the tester <b>404</b> may develop test cases to test the software application. In the environment <b>400</b>, the tester executes test cases, regardless of their origin. The tester <b>404</b> reports failures (e.g., &#x201c;bugs&#x201d;) in the software application after execution of the test cases. The test cases are generally classified into white box test cases and black box test cases. The white box test cases include tests cases that are written for a software application with knowledge of the internal working of the source code. The black box test cases, on the other hand, use a functional perspective of a software application to derive a set of test cases. Accordingly, the code coverage of the software application can be calculated based on the extent to which the white box and black box test cases test the source code. In the environment <b>400</b>, the supervisor <b>406</b> monitors source code and test case results to understand better quality of the developer's code.</p>
<p id="p-0061" num="0060">The developer <b>402</b>, the tester <b>404</b>, and the supervisor <b>406</b>, perform their respective tasks and communicate with each other through one or more devices <b>408</b>-<b>1</b>, <b>408</b>-<b>2</b>, . . . , <b>408</b>-<i>n</i>, collectively referred to as <b>408</b> hereinafter, through a network <b>410</b>.</p>
<p id="p-0062" num="0061">The devices <b>408</b> can be implemented as a variety of conventional computing devices, including, for example, a server, a desktop PC, a notebook or portable computer, a workstation, a mainframe computer, a mobile computing device, an Internet appliance, a network router, etc. or a combination thereof that are configurable to prioritize quality improvements to the source code.</p>
<p id="p-0063" num="0062">The network <b>410</b> can be a wireless or a wired network, or a combination thereof. The network <b>410</b> can be a collection of individual networks, interconnected with each other and functioning as a single large network (e.g., the Internet or an intranet). Examples of such individual networks include, but are not limited to, Local Area Networks (LANs), Wide Area Networks (WANs), and Metropolitan Area Networks (MANs). Further, the individual networks may be wireless or wired networks, or a combination thereof.</p>
<p id="p-0064" num="0063">In one embodiment, a device <b>408</b> includes a processor <b>412</b> coupled to a memory <b>414</b>. The memory <b>414</b> includes a code coverage module <b>416</b>, a structural complexity module <b>418</b>, and a quality assessment module <b>420</b>. The memory <b>414</b> may be coupled to or associated with, and/or accessible to other devices, such as network servers, router, and/or other devices <b>408</b>.</p>
<p id="p-0065" num="0064">The code coverage module <b>416</b> determines code coverage for the source code. As already explained, code coverage is a measure used in software application testing to determine the extent to which a software application has been tested. The code coverage module <b>416</b> can also analyze sections of the source code that have not been executed during the execution of test cases for the source code to help recommend test case development. Such recommendations may be performed by the code coverage module <b>416</b> or another module that receives information from the code coverage module <b>416</b>. The code coverage module <b>416</b> may also estimate code coverage based on knowledge of similar code (e.g., a prior build or version or code with similar functionality and complexity) that has been tested.</p>
<p id="p-0066" num="0065">In one implementation, the code coverage module <b>416</b> calculates the code coverage for a binary of a software application. A binary of a software application includes source code representation in a processor understandable form, which can be executed directly by a processor. In another implementation, the code coverage module <b>416</b> calculates code coverage for an intermediate language (IL) code. An IL code is a common intermediate representation language of a source code, such as Microsoft Intermediate Language (MSIL), which is generated on compilation of the source code. MSIL is associated with the .NET framework, which accepts source code programmed in any of a variety of languages (e.g., object oriented programming languages such as C++, C#, etc.), which are then transformed into MSIL to target a common language runtime (CLR).</p>
<p id="p-0067" num="0066">The structural complexity module <b>418</b> determines a complexity measure for source code that can be used to help assess quality of the code. In one implementation, the complexity measure may be used to determine a level of code coverage that may be required for a desired level of quality. For example, a high complexity measure can indicate higher code coverage requirement for the source code. Test cases can then be developed (e.g., by the developer <b>402</b> or the tester <b>404</b>) to attain a pre-determined level of code coverage and thereby assure better quality compliance of the code. In one embodiment, the structural complexity module <b>418</b> can also determine complexity measures for each code segment of a plurality of code segments of the source code. For example, an application may have 10 segments where the complexity module <b>418</b> assigns one or more complexity measures to each segment. In general, each segment provides a corresponding functionality, which may rely on functionality of one or more other segments.</p>
<p id="p-0068" num="0067">In one embodiment, the structural complexity module <b>418</b> can compute a cyclomatic complexity measure to determine the structural complexity of the source code. As already mentioned, a cyclomatic complexity measure depends on number of linearly independent paths through a source code. Thus, an overall complexity measure can be stated for a source code and, optionally, individual complexity measures for segments of the source code.</p>
<p id="p-0069" num="0068">In the example of <figref idref="DRAWINGS">FIG. 4</figref>, the quality assessment module <b>420</b> can recommend test case development for the source code based on one or more complexity measures and a desired level of quality for the source code. In one implementation, the quality assessment module <b>420</b> receives a quality assessment value that corresponds to a desired level of quality for the source code and also receives one or more structural complexity measures for the source code. The quality assessment module <b>420</b> can then compute code coverage required to attain the desired level of quality. The quality assessment module <b>420</b> may also provide some recommendations as to test case development. For example, given a complexity measure of source code and a desired level of quality for the code, a level of code coverage can be determined. In turn, the level of code coverage can be used to estimate how many test cases are required for the code and optionally for specific segments of the code. For example, code with a high complexity measure can lead to a high level (e.g., percentage) of code coverage that would thereby warrant more test cases for improving the code coverage. Again, as explained with respect to <figref idref="DRAWINGS">FIG. 3</figref>, such a technique may rely on an a priori knowledge of a relationship between code coverage and quality.</p>
<p id="p-0070" num="0069">In another implementation, the quality assessment module <b>420</b> computes the quality assessment value based on the complexity measure and existing code coverage. As mentioned, code testing usually occurs in an iterative manner. Hence, after a first iteration, then such an exemplary technique may provide information germane to a subsequent iteration. After each iteration, an assessment may be made as to the nature of the code, for example, is the pattern aligning with a prior build or version? Or, is it deviating? Where the pattern deviates, then the guidance provided by any preexisting relationship to help determine coverage with respect to quality may be diminished.</p>
<p id="p-0071" num="0070">In general, the quality assessment module <b>420</b> can weight the likelihood-to-fail of various code segments, for example, based on one or more analyses or measures. In turn, the quality assessment module <b>420</b> can generate a prioritized list of areas to develop tests against.</p>
<p id="p-0072" num="0071">After a first iteration, a quality assessment value (e.g., failures per test, etc.) can help in identifying segments of the source code that warrant test case development. In such an example, the identification may be based in part on some estimated levels of code coverage to reach a certain quality.</p>
<p id="p-0073" num="0072">As testing usually occurs based on a set of tests, information as to order of the individual tests can be useful. In general, a test that is likely to expose extremely useful information should be performed earlier than a test that is less likely to expose useful information. For example, a test case that tests a key function of an application can be positioned earlier in a set of test cases than a test case that tests some less important function, especially where the less important function may be less complex and easier to fix if a failure occurs. Hence, as described herein, an exemplary method can prioritize test cases or prioritize segments of code for testing. In the example of <figref idref="DRAWINGS">FIG. 4</figref>, the quality assessment module <b>420</b> may prioritize identified code segments, for example, based on levels of code coverage for the code segments.</p>
<p id="p-0074" num="0073">In yet another implementation, the quality assessment module <b>420</b> can recommend test case development based on information, such as artifacts from previous releases, usage of the source code, travel path used in the source code, runtime contexts, manual estimation, etc.</p>
<p id="p-0075" num="0074">As described herein, a runtime collection mechanism can track data (e.g., values for arguments, globals, etc.), frequently executed code paths, information regarding thread execution, etc., and use such information to prioritize test cases or test case development. In particular, the quality assessment module <b>420</b> can rely on structural complexity and/or other information. For example, the module <b>420</b> may use one or more analysis measures such as semantic metrics, dependency analysis, etc.</p>
<p id="p-0076" num="0075">Various exemplary methods are described below that may be implemented using the device <b>408</b>.</p>
<p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. 5</figref> illustrates various components of the exemplary device <b>408</b>. In one embodiment, the device <b>408</b> can include, but is not limited to, the processor <b>412</b>, a network interface <b>502</b>, the system memory <b>414</b>, and an Input/Output Interface <b>504</b>.</p>
<p id="p-0078" num="0077">The network interface <b>502</b> can enable the device <b>408</b> to receive a software application's source code and share results of quality assessment over a network (e.g., the network <b>410</b>). For example, the supervisor <b>406</b> of <figref idref="DRAWINGS">FIG. 4</figref> may monitor changes to the source code of the application and development of test cases.</p>
<p id="p-0079" num="0078">The memory <b>414</b> includes computer-readable media in the form of volatile memory, such as Random Access Memory (RAM) and/or non-volatile memory, such as Read Only Memory (ROM) or flash RAM. The memory <b>414</b> typically includes data and/or program modules for implementing prioritization of quality improvements to source code that are immediately accessible to and/or presently operated on by the processor <b>412</b>. In one embodiment, the memory <b>414</b> includes the code coverage module <b>416</b>, the structural complexity module <b>418</b>, and the quality assessment module <b>420</b>. The other modules <b>508</b> may include other software that assists in the functioning of the computing device <b>408</b>, such as an operating system. The program data <b>510</b> may include an IL code <b>512</b>, source code <b>514</b>, a quality assessment value <b>516</b>, and other data <b>518</b> specific to the system or the applications.</p>
<p id="p-0080" num="0079">Where some testing has already occurred, the code coverage module <b>416</b> can measure code coverage for the source code <b>514</b> of the software application based on the lines of source code that have been executed. The module <b>416</b> can also signify the degree to which the source code <b>514</b> has been tested.</p>
<p id="p-0081" num="0080">In one embodiment, the code coverage module <b>416</b> can evaluate block and arc coverage to help determine a code coverage measure. As mentioned, block code coverage includes a set of instructions that have no branches. Thus, for evaluating block code coverage, the code coverage module <b>416</b> assumes that if one line of the source code from the set of lines or instructions is executed, all the remaining instructions would be executed. The arc code coverage, on the other hand, evaluates conditional statements such as if, while, for, while calculating the code coverage.</p>
<p id="p-0082" num="0081">In one implementation, where some testing has occurred, the code coverage module <b>416</b> receives information about executed test cases and then analyzes the source code <b>514</b> to determine a code coverage measure for the source code and/or code coverage measures for segments of the source code. The code coverage module <b>416</b> can also identify and analyze sections of the source code <b>514</b>, which are not covered by the test cases. A test case development process can then be implemented to increase the percentage of coverage for the source code. Such a process includes providing and executing tests to cover the sections of the source code <b>514</b> that have not been tested. Where suitable test cases are not already written, the process may include writing test cases.</p>
<p id="p-0083" num="0082">In another implementation, the code coverage module <b>416</b> can also analyze a binary or an IL representation of the software application. The binary representation includes presentation of the software application in a processor or a machine readable format. The IL representation is a transformed version of a source code. Object-oriented programming languages such as C#, Visual Basic (VB), C++ can all be represented in the common IL representation such as MSIL.</p>
<p id="p-0084" num="0083">For MSIL, when compiling to managed code that targets the .NET framework CLR, a compiler translates the source code into MSIL, which is a CPU-independent set of instructions that can be efficiently converted to native code for a particular computing architecture. MSIL includes instructions for loading, storing, initializing, and calling methods on objects, as well as instructions for arithmetic and logical operations, control flow, direct memory access, exception handling, and other operations. Before MSIL code can be run on a computing device, it must be converted to CPU-specific code, usually by a just-in-time (JIT) compiler. Because the .NET framework CLR supplies one or more JIT compilers for each supported computer architecture, the same MSIL code can be JIT-compiled and run on any supported architecture.</p>
<p id="p-0085" num="0084">In yet another implementation, the code coverage module <b>416</b> can also analyze specific code segments of the source code. The code coverage for each code segment can be computed independently based on sections of the code segment that are not executed by the test cases also referred to as the uncovered sections. The code coverage module <b>416</b> can also analyze distributed applications (e.g., Web 2.0 applications) to calculate code coverage.</p>
<p id="p-0086" num="0085">In one implementation, the code coverage module <b>416</b> determines percentage of uncovered source code. The uncovered source code includes sections of the source code that have not been executed by the test cases. For example, 95% uncovered code coverage implies that only 5% of the entire source code has test cases corresponding to it to test the functionality. The code coverage measure may be used in addition to the structural complexity measure to assess quality compliance of the code.</p>
<p id="p-0087" num="0086">The structural complexity module <b>418</b> determines one or more complexity measures for the source code <b>514</b>. In one implementation, a complexity measure is used to determine the level of code coverage that may be required for a desired level of quality of the source code <b>514</b>. For example, a higher complexity measure can indicate higher levels of required code coverage. This indicates that the more complex the source code <b>514</b> is, higher is the code coverage requirement, and more the number of test cases that need to be written for the desired level of quality.</p>
<p id="p-0088" num="0087">In one implementation, the structural complexity module <b>418</b> uses a cyclomatic complexity measure to calculate complexity of source code. Cyclomatic complexity uses graph theory and is based on the number of linearly independent paths in a source code. Nodes of the graph correspond to instructions of the source code, and the edges correspond to the sequence of execution of the commands.</p>
<p id="p-0089" num="0088">The structural complexity module <b>418</b> can also identify structural complexity metrics such as coupling, and depth of inheritance to evaluate the structural complexity of the source code. The depth of inheritance is based on the hierarchy of a code segment in the source code. The deeper the code segment is in the hierarchy, the greater the number of code segments it is likely to depend on, making it more complex to predict its behavior. Coupling, on the other hand, indicates the dependency of one code segment over another. A higher coupling between two code segments corresponds to a higher structural complexity.</p>
<p id="p-0090" num="0089">In one embodiment, the structural complexity module <b>418</b> measures the structural complexity of each code segment of the source code that provides a specific functionality to the software application. In another embodiment, the structural complexity module <b>418</b> can also measure the structural complexity based on an intermediate representation <b>512</b> of the source code <b>514</b> (e.g., IL). In yet another embodiment, the structural complexity module <b>418</b> calculates the complexity of one or more components of the software application. Each component can be a logical block of the source code for which one or more complexity metrics can be extracted.</p>
<p id="p-0091" num="0090">In one implementation, the structural complexity module <b>418</b> analyzes the structural complexity using a binary metadata reader. The binary metadata reader extracts metrics (e.g., associated with complexity) from the binary of the source code <b>514</b>. In the .NET framework, metadata is binary information stored in the binary describing the source code <b>514</b>. When the source code <b>514</b> is compiled into the binary, also known as a portable executable (PE) file, metadata is inserted into one portion of the binary, while the IL code <b>512</b> is inserted into the other portion of the binary. The metadata references code segments, types of data, etc., in the source code <b>514</b> so that when the source code <b>514</b> is executed, the metadata can be loaded first in the memory to give information about the code segments, code members, inheritance, and so on.</p>
<p id="p-0092" num="0091">The quality assessment module <b>420</b> uses a quality assessment value to recommend test case development for the source code. In one implementation, the quality assessment value may correspond to a desired level of quality for the source code. In one implementation, the desired level of quality can be determined for each code segment based on artifacts from previous releases, usage of the source code, frequency of usage, travel path used in the source code, runtime contexts, manual estimation, binary owners, etc.</p>
<p id="p-0093" num="0092">The quality assessment value can then be used to identify the level of code coverage required for each code segment, for attainting the desired level of quality based on the complexity measure of the source code. Such level of code coverage can then be used as a reference to write new test cases to improve the existing code coverage. For example, higher levels of code coverage can be associated with a source code having higher complexity measures when compared with another source code having lower complexity measures.</p>
<p id="p-0094" num="0093">In another implementation, the quality assessment module <b>420</b> computes the quality assessment value based on the existing code coverage and code complexity measure of the source code <b>514</b>. An exemplary quality assessment value can be expressed as follows:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>Quality Assessment Value=Code Complexity/Code Coverage.<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0095" num="0094">A quality assessment value can help identify code segments of the source code <b>514</b> that warrant test case development by identifying new levels of code coverage for the code segments. The quality assessment module <b>420</b> can also prioritize the identified code segments based on the quality assessment value. For example, a source code with lower code coverage and high complexity measure can have higher priority for test case development as compared to a source code with lower code coverage and low complexity measure.</p>
<p id="p-0096" num="0095">In yet another implementation, the quality assessment module <b>420</b> identifies code segments of the source code <b>514</b> that need refactoring. The identification includes finding code segments of the source code <b>514</b>, which are complex and less understandable. Such code segments can also be identified based on the complexity measures of the source code. The identified sections of the source code <b>514</b> can be re-written to make the sections more testable and simpler to understand.</p>
<p id="p-0097" num="0096">The level of code coverage can be used to indicate completeness of a test case development process, for example, relative to a desired standard of reliability. The code segments can also be prioritized based on the level of code coverage. For example, code segments with higher complexity can have higher levels of code coverage and be assigned a higher priority.</p>
<p id="p-0098" num="0097">Multiple quality assessment values R1, . . . , Rn, for n different types of code coverage measures and complexity measures, can be provided. The code coverage measures include measures such as block coverage measure, arc coverage measure, statement coverage measure, function coverage measure, and branch coverage measures. The complexity measures can include complexity metrics such as coupling, depth of inheritance. For example, consider the following metrics:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>R</i>1=Overall complexity (method, file, binary, component)/Block Coverage;<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>and<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>R</i>2=Overall complexity (method, file, binary, component)/Arc Coverage.<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0099" num="0098">In one implementation, the overall complexity measures mentioned above includes complexity at various levels of the source code <b>514</b>. For example, the complexity can be calculated for a specific method of the source code <b>514</b> or for the whole file of the source code <b>514</b>. The complexity can also be calculated for the binary using the binary metadata reader, or for a specific component of the source code <b>514</b>.</p>
<p id="p-0100" num="0099">In an exemplary method, a quality assessment value is used for setting expected code coverage bounds in a manner that accounts for complexity of a selected source code. For example, if R1 and R2 measures are set to 30.0 and 25.0 respectively, then a method in the source code (e.g., a segment of the source code) with a cyclomatic complexity of 150 should have a code coverage level of 5% and a method with a cyclomatic complexity of 1500 should have a code coverage level of 50%. Such an approach to quality is relative and can be adjusted according to the complexity of a selected source code to determine appropriate code coverage levels between 0-100%.</p>
<p id="p-0101" num="0100">In the example of <figref idref="DRAWINGS">FIG. 5</figref>, the quality assessment module <b>420</b> can also compute the level of code coverage based on different weightings and coefficients associated with different code coverage and structural complexity measures. In this example, the different weightings and coefficients are adjustable, for example, based on the relevance of the code coverage and the structural complexity measures. For example, an arc code coverage measure may not be very relevant in a source code that does not have many conditional or branching expressions.</p>
<p id="p-0102" num="0101">In one implementation, one or more levels of code coverage can be calculated for a list of code segments based on the desired level of quality and the structural complexity measures. In such an implementation, the quality assessment module <b>420</b> prioritizes a list of methods for test case development based on the level of code coverage of each code segment. For example, a code segment with a structural complexity of 1200 would have a higher priority over a code segment with a structure complexity of 1100.</p>
<p id="p-0103" num="0102">In another implementation, the quality assessment module <b>420</b> calculates the quality assessment value corresponding to the desired level of quality based on a set of artifacts used from one or more previous releases of a software application. The artifacts may include attributes such as, bugs reported, people involved in development of the software application, usage of the software application, etc., that can help generate data to improve quality of the source code <b>514</b> of the software application. The quality assessment module <b>420</b> can receive fine tuning data and calculate a quality assessment value based on the fine tuned data. For example, the bugs found in a previous release can help to assess common types of failures in a software application and can thus help adjust one or more coefficients for a selected source code for purposes of calculating a quality assessment value.</p>
<p id="p-0104" num="0103">In yet another implementation, the quality assessment module <b>420</b> calculates the level of code coverage for prioritizing the code segments based on the travel path of the code segment within the source code. The travel path can be based on attributes such as context, the frequency of usage, security concerns, the number of people using it, etc. For example, a code segment having the same code coverage and the structural complexity as another section but having a higher travel path can be assigned a higher priority for test case development.</p>
<p id="p-0105" num="0104">In one embodiment, the quality assessment module <b>420</b> can identify code segments of the source code <b>514</b> that warrant further test development based on an object code and a program database (PDB) file corresponding to the source code <b>514</b>. In this example, the object code, also referred to as the object file, includes a representation of the source code <b>514</b> that a compiler or assembler generates after processing the source code file <b>514</b>. Typically, the PDB file holds debugging and source code information and can help in retrieving source code information from the object code. Such information can be used for any of a variety of purposes for test case development and, optionally, prioritization of test cases.</p>
<p id="p-0106" num="0105">Further, the quality assessment module <b>420</b> can incorporate a feedback or a learning loop to enhance quality assurance. A feedback or learning loop can include calculating a level of code coverage recursively based on changes made to the source code <b>514</b>. For example, consider a scenario where the quality assessment module <b>420</b> calculates a level of code coverage for a code segment of the source code <b>514</b> that corresponds to one or more methods. The code segment can then be refactored based on the level of code coverage. The quality assessment module <b>420</b> can then calculate a modified level of code coverage based on the refactored segment. Such a feedback mechanism can help evaluate the improvements in the source code <b>514</b>. The feedback can also help in getting a new prioritized list of methods every time the source code <b>514</b> is changed or refactored. The quality assessment module <b>420</b> can also align parameters and coefficients associated with the code coverage measures and the structural complexity metrics based on the feedback.</p>
<p id="p-0107" num="0106">In one implementation, once the quality assessment module <b>420</b> has analyzed the code, it proceeds to prioritize a list of methods based on a desired level of quality (e.g., for each individual method or the source code as a whole) and structural complexity measures for each of the methods in the list. The quality assessment module <b>420</b> can present the prioritized list of methods to a user on a user interface. The presentation of the prioritized list of methods can help a developer <b>402</b> (e.g., or a tester <b>404</b> or supervisor <b>406</b>) visualize a method's reliability by mapping the source code location of the method to one or more analyses provided by the quality assessment module <b>420</b>. Each analysis can be based on the coefficients and weightings of the code coverage and structural complexity measures. In one embodiment, the quality assessment module <b>420</b> prioritizes the sections of the source code <b>514</b> and also presents some lines of instructions for one or more of the sections, optionally with a corresponding analysis. Where a section of the source code <b>514</b> is adequately covered and error free, the section may be marked or listed as needing no further testing.</p>
<p id="p-0108" num="0107">Further, the quality assessment module <b>420</b> can associate risk levels to each method or section of the source code <b>514</b>. The risk levels, which can be ascertained based in part on a quality assessment value, can indicate a likelihood of failure. For example, the risk levels may be presented in the form of a tri-state setting indicating a high level of risk, a medium level of risk, or a low level of risk. In an embodiment, the quality assessment module <b>420</b> can also specify an arbitrary number of priority levels as numeric indexes. The quality assessment module <b>420</b> can also alter underlying thresholds, which determine the priority level of risk to be associated with a method or section of the source code <b>514</b>.</p>
<p id="p-0109" num="0108"><figref idref="DRAWINGS">FIG. 6</figref> presents an exemplary schematic <b>600</b> showing one or more components for the code coverage module <b>416</b>, the structural complexity module <b>418</b>, and the quality assessment module <b>420</b>. The code coverage module <b>416</b> includes a dynamic analysis component <b>602</b> and the structural complexity module <b>418</b> includes a static analysis component <b>604</b> configured to assess a source code's likelihood-to-fail. The dynamic analysis component <b>602</b> analyzes the code coverage of a software application based on the execution of the software application through the test cases. The static analysis component <b>604</b> performs analysis of the structural complexity of a software application without executing the software application.</p>
<p id="p-0110" num="0109">The dynamic analysis component <b>602</b>, also referred to as runtime component, includes a binary <b>606</b> corresponding to a software application. In one implementation, the dynamic analysis component <b>602</b> is configured to provide code coverage information of the binary to a coverage store <b>608</b>. The code coverage information includes the usage of the binary in terms of what sections of the source code of the software application are executed through the test cases. The information can also include the amount of time for which section runs and its frequency of execution.</p>
<p id="p-0111" num="0110">The dynamic analysis component <b>602</b> can also be associated with a runtime context to evaluate the binary's likelihood-to-fail. The runtime context can be based on whether a software application runs on a server or on a client machine. A software application running on a server machine, for example, can include high value information (e.g., stock transactions, banking, etc.) and cost of failure may be significant. Software applications that can fail an corrupt data can also introduce significant cost; such applications may also be classified as high-priority software applications. Another attribute of a runtime context can be security concerns of a software application.</p>
<p id="p-0112" num="0111">In the example of <figref idref="DRAWINGS">FIG. 6</figref>, the static analysis component <b>604</b> is configured to extract complexity metrics from the binary <b>606</b>. In one implementation, the binary <b>606</b> is segregated based on classes <b>610</b>-<b>1</b>, . . . , <b>610</b>-M, referred to as <b>610</b> hereinafter. The complexity metrics <b>612</b>-<b>1</b>, . . . , <b>612</b>-M, referred to as <b>612</b> hereinafter, can be extracted for each class <b>610</b>. In another implementation, the binary <b>606</b> can also be classified based on code segments of the source code or on methods/functions of the source code.</p>
<p id="p-0113" num="0112">The exemplary schematic <b>600</b> further includes one or more components for the quality assessment module <b>420</b>. The quality assessment module <b>420</b> can assess likelihood-to-fail based on the complexity measure and a desired level of quality for the source code. In one implementation, the quality assessment module <b>420</b> determines a level of code coverage required for attaining a desired quality based on the complexity measure and the quality assessment value <b>516</b>.</p>
<p id="p-0114" num="0113">The quality assessment module <b>420</b> includes a dynamic/static analysis component <b>614</b>, a runtime context component <b>616</b>, and a binary owner's component <b>618</b> to determine the quality assessment value <b>516</b>. The dynamic/static analysis component <b>614</b> includes determining the quality assessment value <b>516</b> based on code coverage and one or more complexity measures for source code. For example, an existing level of code coverage can be determined by the code coverage module <b>416</b> based on the uncovered percentage of source code. One or more complexity measures can be computed by the structural complexity module <b>418</b> to determine the complexity of the code. The quality assessment value <b>516</b> can then be determined based on the code coverage and complexity measures.</p>
<p id="p-0115" num="0114">The quality assessment module <b>420</b> can determine the quality assessment value <b>516</b> based on the runtime context component <b>616</b>. The runtime context component <b>616</b> determines the software application's characteristics such as runtime setting (e.g., client or server) and security.</p>
<p id="p-0116" num="0115">The quality assessment module <b>420</b> can also determine the quality assessment value <b>516</b> based on the binary owner's component <b>618</b>. The binary owner's component <b>618</b> includes a static analysis mechanism to determine the quality assessment value <b>516</b> based on the historical knowledge of the software application. A binary owner can include a software developer that manages and develops the software application. In one embodiment, if 60% of a development team is new and lacks historical knowledge, the binary owner's component <b>618</b> can provide such information (e.g., newness of the team), which, in turn, may be used to assess quality. Experience of an individual developer or a team may be used in conjunction with measures such as structural complexity to determine a quality assessment value. For example, complex code developed by an inexperienced developer may benefit from more testing (i.e., greater code coverage to reach a desired level of quality).</p>
<p id="p-0117" num="0116"><figref idref="DRAWINGS">FIG. 7</figref> shows an exemplary presentation of quality assessment information <b>700</b> that can be displayed to a user on a graphical user interface (GUI). The presentation <b>700</b> shows a binary hierarchy <b>702</b> with associated structural complexity <b>704</b>, code coverage information <b>706</b> and risk level <b>708</b>. The example of <figref idref="DRAWINGS">FIG. 7</figref> pertains to managed binaries; as mentioned, the .NET framework provides a CLR for execution of managed binaries.</p>
<p id="p-0118" num="0117">The hierarchy <b>702</b> organizes the source code by namespaces, types, and members of the source code. The hierarchy <b>702</b> can include an author, a date, and a time attribute <b>710</b>. The author attribute shows the creator of the binary, whereas the date and the time attributes show the timestamp of when the binary was created or modified.</p>
<p id="p-0119" num="0118">The structural complexity <b>704</b> shows the structural complexity measures for each class of the binary. For example, the &#x201c;Microsoft.Expression.Web&#x201d; class has a structural complexity of &#x201c;787&#x201d;. In one implementation, the structural complexity is extracted from an IL representation of the code (e.g., using a cyclomatic complexity measure).</p>
<p id="p-0120" num="0119">In the graphic <b>700</b>, the code coverage metric <b>706</b> indicates percentage of the source code that is not covered (i.e., uncovered) by the test cases. In one embodiment, the percentage of blocks that are not covered by the source code, is an indicator of the sections of the source code that do not have corresponding test cases. For example, the &#x201c;Microsoft.Expressions.Web.Profiling&#x201d; class has an uncovered percentage which equals to 100%, thereby implying that there are no test cases executed for the class. This can also imply that there are no test cases written for this class.</p>
<p id="p-0121" num="0120">As mentioned, the quality assessment module <b>420</b> can associate a risk level <b>708</b> with each class of a binary. In one implementation, the risk level associated with each class is based on the level of code coverage required by the class, which in turn is determined based on the quality assessment value <b>516</b> and the complexity measures identified by the structural complexity module <b>418</b>. For example, source code with high complexity measures leads to higher level of code coverage requirement. The risk level <b>708</b> for a segment of code may change as such code is tested and as coverage increases or, alternative, the risk level <b>708</b> may indicate a general level of risk (e.g., associated with complexity, historical information, etc.).</p>
<p id="p-0122" num="0121">The exemplary presentation <b>700</b> associates the risk level <b>708</b> in the form of a tri-state setting indicating a high level of risk, a medium level of risk, or a low level of risk. The risk level column <b>708</b> is sortable (as are all others), for example, by clicking on a column heading. In another implementation, numeric indexes rather than tri-state icons can be reported as the risk level <b>708</b>, which may be sortable and which can provide more granularity for evaluating results.</p>
<p id="p-0123" num="0122"><figref idref="DRAWINGS">FIG. 8</figref> shows an exemplary presentation of quality assessment information <b>800</b> that can be displayed to a user on a graphical user interface (GUI) (e.g., consider the devices <b>408</b> of <figref idref="DRAWINGS">FIG. 4</figref>). The presentation <b>800</b> displays the structural complexity measures <b>704</b>, the code coverage measures <b>706</b> and the risk levels <b>708</b> corresponding to various segments of source code.</p>
<p id="p-0124" num="0123">The presentation <b>800</b> includes a view <b>802</b> showing the hierarchy <b>702</b> drilled down to the member/function level view and showing data related to methods and functions. The view <b>802</b> also shows an author and a timestamp <b>710</b>. Each function in the hierarchy <b>702</b> is associated with a corresponding structural complexity measure and a corresponding code coverage measure. For example, the function ShellExecute (string, bool, class, string) has a structural complexity of <b>480</b> and a 100% uncovered code implying that no test cases have been executed (and/or written) to test and execute the function.</p>
<p id="p-0125" num="0124">The presentation <b>800</b> also includes an Integrated Development Environment (IDE) view <b>804</b> showing source code corresponding to a function <b>806</b> highlighted in the view <b>802</b>. In one implementation, a function in the view <b>802</b> is selected by double-clicking the function. In another implementation, a function is selected using the toolbar <b>808</b> in the view <b>802</b>. The toolbar <b>808</b> can also be used to delete a function from the list of functions in the view <b>802</b>.</p>
<p id="p-0126" num="0125">The view <b>804</b>, as described above, shows the source code corresponding to the function <b>806</b> highlighted in the view <b>802</b>. For example, the highlighted function <b>806</b> refers to the function ShellExecute (string, bool, class, string) having a structural complexity of &#x201c;480&#x201d; and a 100% uncovered code. A 100% uncovered code suggests that no test cases have been executed (and/or written) for the function, and thus a high risk level can be associated with the function in view of its structural complexity.</p>
<p id="p-0127" num="0126">The marked section of the source code <b>810</b> in the view <b>804</b> indicates the path coverage of the source code through the function. The marked section <b>810</b> indicates that there has been no coverage for the code in this section and that the function has not even been entered. The marked section of the source code <b>810</b> can also be highlighted in color to show the travel path coverage of the source code through the function. The specific color associated with unexecuted paths can indicate a function's priority level like high-priority, medium priority, or low priority. An icon <b>812</b> on the left margin can also be used to indicate the priority, such as a high priority, given to the highlighted function <b>806</b> to focus on for test case development. In the example of <figref idref="DRAWINGS">FIG. 8</figref>, based on a quality assessment value which corresponds to a desired level of quality and on a complexity measure (e.g., which is very high: &#x201c;480&#x201d; in the above example function <b>806</b>), the selected portion of the function can be determined to have a high code coverage level requirement.</p>
<p id="p-0128" num="0127">The visual presentation <b>800</b> can also be changed to a textual view. The textual view includes a numeric risk/priority value associated with the function rather than a colored indicator of the risk/priority.</p>
<p id="p-0129" num="0128">The visual presentation <b>800</b> can also be based on a binary and a source code view. The binary view can include a set of &#x201c;.dll&#x201d; files of the binary of the software application. The binary view can map classes in the &#x201c;.dll&#x201d; files with the risk levels based on the levels of code coverage. Locations in the classes can also be identified based on the sections of the classes that warrant further test case development.</p>
<p id="p-0130" num="0129">The source code view, on the other hand, includes a set of source code files such as, .cpp files, within a directory. The locations in the source code files can then be identified based on the sections of the source code files that warrant further test development. For example, consider a directory &#x201c;x&#x201d; that includes two source code files, &#x201c;a.cpp&#x201d; and &#x201c;b.cpp&#x201d;. The locations can be identified for the source code files based on the risk levels associated with the files, which can be optionally based on associated levels of code coverage.</p>
<p id="p-0131" num="0130">The visual presentation <b>800</b> can further include a people view <b>830</b>. The people view <b>830</b> can include a list of members (e.g., a set of names such as, Mike, Jeff, etc.) within a group and associate the different segments of the code with the members of the group (e.g., X1-14, X15-73, etc.). The view <b>830</b> can also associate risk levels with each person based on an assessment of the quality of source code written by the person and, for example, months of experience. For example, consider the Group X that includes members <b>1</b> through <b>5</b> where the overall group has an average experience of 32 months and a low risk. The risk levels can also be associated with each person based on the level of code coverage for the source codes written by them, which is further based on the desired level of quality and complexity measures of the source code.</p>
<p id="p-0132" num="0131">In addition to the level of code coverage, other attributes such as, number of users using the code, frequency of usage, context or functionality of the code, and security level required can also help in determining the extent of test case development required as shown in <figref idref="DRAWINGS">FIG. 9</figref>.</p>
<p id="p-0133" num="0132"><figref idref="DRAWINGS">FIG. 9</figref> illustrates exemplary schematic <b>900</b> including exemplary attributes <b>902</b> that can be used in an analysis <b>904</b> to recommend test case development for one or more code segments <b>906</b>. Multiple attributes such as percentage of users using the code segment, frequency of usage, context of the code segment, security level required, and number of bugs found in similar code segments can help prioritize the levels of code coverage required to attain the desired level of code quality.</p>
<p id="p-0134" num="0133">For example, the exemplary parameters <b>902</b> include multiple attributes for one or more code segments. In one embodiment, the code segments are differentiated based on the context of the code segment. For example, the top menu code segment provides a navigation bar that helps in navigation of the application. Similarly, the music pages generally relate to browsing of the songs.</p>
<p id="p-0135" num="0134">At step <b>904</b>, the code segments are analyzed based on the attributes of the code segments. The analysis includes comparison of the attributes for one or more code segments to determine relevance and importance of the attributes to the code segment. For example, the purchasing/transactions code segment requires a high security level and half of the users accessing the application also use this code segment attaching a very high level of importance to the code segment. Video pages, on the other hand, require low security levels and are mainly browsed making them relatively lower in priority as far as the test case development is concerned.</p>
<p id="p-0136" num="0135">At step <b>906</b>, test case development recommendations are given based on the analysis done at step <b>904</b>. For example, purchasing/transactions code segment can have a higher level of test case development recommendation based on nature of the attributes. The test case development recommendations can also be associated with the levels of code coverage. For example, a code segment with a high level of code coverage needs higher number of test cases to improve the quality of the code and thereby is given a higher level of test case development recommendation.</p>
<p id="h-0006" num="0000">Exemplary Methods</p>
<p id="p-0137" num="0136">Exemplary methods for implementing prioritization of quality improvements to code are described with reference to <figref idref="DRAWINGS">FIGS. 1-9</figref>. These exemplary methods can be described in the general context of computer executable instructions. Generally, computer executable instructions can include routines, programs, objects, components, data structures, procedures, modules, functions, and the like that perform particular functions or implement particular abstract data types. The methods can also be practiced in a distributed computing environment where functions are performed by remote processing devices that are linked through a communication network. In a distributed computing environment, computer executable instructions may be located both in local and remote computer storage media, including memory storage devices.</p>
<p id="p-0138" num="0137">The exemplary methods are illustrated as a collection of blocks in a logical flow graph representing a sequence of operations that can be implemented in hardware, software, firmware, or a combination thereof. The order in which the methods are described is not intended to be construed as a limitation, and any number of the described method blocks can be combined in any order to implement the methods, or alternate methods. Additionally, individual blocks may be deleted from the methods without departing from the spirit and scope of the subject matter described herein. In the context of software, the blocks represent computer instructions that, when executed by one or more processors, perform the recited operations.</p>
<p id="p-0139" num="0138"><figref idref="DRAWINGS">FIG. 10</figref> illustrates an exemplary method <b>1000</b> for executing and analyzing the test cases corresponding to the source code. The method <b>1000</b> includes iterative development of test cases based on the desired level of quality. In one implementation, once the desired level of code quality is achieved, the test case development can be stopped and the test cases can be stored in the test case repository.</p>
<p id="p-0140" num="0139">In the example of <figref idref="DRAWINGS">FIG. 10</figref>, the method <b>1000</b> includes an internal feedback loop and an external feedback loop. The external feedback loop pertains to information obtained from field testing such as beta testing whereas the internal feedback loop pertains to tests performed within the guise of a controlled testing environment with controlled tests (i.e., tests performed using proscribed test cases).</p>
<p id="p-0141" num="0140">The method <b>1000</b> may be used for newly developed code <b>1002</b> or for field implemented code <b>1004</b>. In general, data are available for field implemented code <b>1004</b>. Such data can be used for any of a variety of purposes such as quality assessment, test case development, etc.</p>
<p id="p-0142" num="0141">At block <b>1006</b>, the newly developed code <b>1002</b> or the field implemented code <b>1004</b> are received. As already mentioned, code may include code information such as metadata that gives description about the structure and behavior of code such as class structure, data types of variables. The metadata can also be used to extract structural complexity measures for received code.</p>
<p id="p-0143" num="0142">At block <b>1008</b>, test cases are determined for the code based on the desired level of code quality required. For example, levels of code coverage are determined based on the desired level of quality and on the complexity measures for the code. The level of code coverage can indicate the priority and extent for test case development based on which the test cases are determined.</p>
<p id="p-0144" num="0143">At block <b>1010</b>, the determined test cases are executed for the code (i.e., tests are performed). At block <b>1012</b>, execution results of the test cases are analyzed to check whether a desired level of code quality is achieved. The code coverage module <b>416</b> can also analyze the execution results to compute code coverage for the code. The code coverage can then be used in the analysis.</p>
<p id="p-0145" num="0144">At block <b>1014</b>, a decision occurs as to the quality of the code, for example, whether the results indicate that failures are below a desired level and that code coverage is above a proscribed level. The decision block <b>1014</b> may involve comparing the analyzed test results with the desired level of quality to determine whether additional tests should be performed.</p>
<p id="p-0146" num="0145">At block <b>1016</b>, if the code quality is at or above a desired level of quality, i.e. the &#x201c;yes&#x201d; branch from block <b>1014</b>, the code is declared to have passed and no further test cases may be required. In other words, the level of code coverage is deemed acceptable. The process then proceeds via an optional external feedback loop, which includes a field testing block <b>1022</b> (e.g., beta or other field release).</p>
<p id="p-0147" num="0146">At block <b>1018</b>, on the other hand, if the results are not &#x201c;OK&#x201d; (i.e., not meeting one or more quality related criteria), i.e. the &#x201c;no&#x201d; branch from block <b>1014</b>, then the quality assessment module <b>420</b> identifies one or more code segments that warrant further testing (e.g., test case development). The identified code segments require further testing to improve corresponding levels of code coverage to attain the desired level of quality.</p>
<p id="p-0148" num="0147">At block <b>1020</b>, additional test cases can be determined for the identified code segment or segments, for example, based on the desired level of quality and the complexity measures of the code. The additional test cases can then be executed at block <b>1010</b> forming an internal feedback loop that aims to improve code quality.</p>
<p id="p-0149" num="0148">For the external feedback loop, test cases performed and associated test information (e.g., including complexity, etc.) can be stored in a test case repository and can be referred to at block <b>1006</b> after field testing of the code.</p>
<p id="p-0150" num="0149"><figref idref="DRAWINGS">FIG. 11</figref> illustrates exemplary method(s) <b>1100</b> for analyzing complexity and recommending code coverage tests for the source code based on the analyzed complexity.</p>
<p id="p-0151" num="0150">At block <b>1108</b>, an analysis module (e.g., the quality assessment module <b>420</b>) receives source code written in a programming language code <b>1102</b> (e.g., C#, C++, etc.), in an intermediate language <b>1104</b> (e.g., MSIL) or in a very elementary web application language <b>1106</b> (e.g., HTML). The analysis block <b>1108</b> analyzes the received code for complexity.</p>
<p id="p-0152" num="0151">At block <b>1110</b>, recommendations as to code coverage for a desired level of quality are provided. For example, given some a priori knowledge of complexity, code coverage and quality, a module may recommend code coverage levels as a function of complexity and a desired level of quality. In one implementation, the quality assessment module <b>420</b> computes the level of code coverage from a desired level of quality and the complexity measures. The quality assessment module then recommends the code coverage tests accordingly. The code coverage tests indicate the test cases required for one or more code segments of the code for attaining the desired level of quality.</p>
<p id="p-0153" num="0152">At block <b>1112</b>, the code coverage tests are executed for the received code. At block <b>1114</b>, the executed code coverage tests are analyzed. Such an analysis may compute the code coverage of the code and estimate the quality of the code.</p>
<p id="p-0154" num="0153">Overall, the method <b>1100</b> may be used as an initial step in testing code, which may be followed by one or more subsequent iterations, for example, based on the assessment of the test results per block <b>1114</b>.</p>
<p id="p-0155" num="0154"><figref idref="DRAWINGS">FIG. 12</figref> illustrates exemplary method(s) <b>1200</b> for implementing prioritization of test case development.</p>
<p id="p-0156" num="0155">At block <b>1202</b>, a device <b>408</b> accepts a list of code segments from a particular source code. In one implementation, the list of code segments may also be from different source code files instead of being a part of the same source code file. In another implementation, the input to the device <b>408</b> can also be an IL representation of the source code such as MSIL. In yet another implementation, the device <b>408</b> can also accept one or more sections of the source code, where each section represents a set of instructions from the source code.</p>
<p id="p-0157" num="0156">At block <b>1204</b>, a desired level of quality for the code segments is provided, for example, by a tester. At block <b>1206</b>, structural complexity of the segments are evaluated, for example, to provide complexity measures for each code segment of the source code (see, e.g., the structural complexity module <b>418</b>). In one implementation, the structural complexity module <b>418</b> extracts complexity measures based on the cyclomatic complexity measure. The complexity measures, also referred to as complexity metrics, can include metrics such as coupling and depth of inheritance.</p>
<p id="p-0158" num="0157">At block <b>1208</b>, levels of code coverage, also referred to as coverage levels, are determined for the code segments based on the complexity measures and the desired level of quality, for example, by the quality assessment module <b>420</b>.</p>
<p id="p-0159" num="0158">At block <b>1210</b>, the list of code segments are prioritized based at least in part on the levels of code coverage. For example, a code segment with high complexity code may have a higher required level of code coverage and thus a higher priority for test case development when compared to a code segment with a low complexity. In one implementation, the quality assessment module <b>420</b> prioritizes the test cases.</p>
<p id="p-0160" num="0159">At block <b>1212</b>, the prioritized list of code segments is presented to a user using graphical user interface visualizations. In one implementation, the presentation lists all methods along with corresponding levels of code coverage and corresponding structural complexity measures. Moreover, the risk levels can also be presented with each code segment. The presentation can also include an IDE view to map each code segment of the view directly to the source code of the code segment.</p>
<p id="p-0161" num="0160"><figref idref="DRAWINGS">FIG. 13</figref> illustrates an exemplary method <b>1300</b> for identification of locations in the source code of the software application for test case development.</p>
<p id="p-0162" num="0161">At block <b>1302</b>, the device <b>408</b> accepts source code in IL representation (e.g., MSIL). In one implementation, the device <b>408</b> can accept the source code directly in any implementation language such as C#, VB, C++.</p>
<p id="p-0163" num="0162">At block <b>1304</b>, the device <b>408</b> provides test cases written for the code. At block <b>1306</b>, the structural complexity module <b>418</b> extracts the complexity metrics, also referred to as complexity measures, based on the IL code. In one implementation, the structural complexity module <b>418</b> extracts the structural complexity metrics, such as coupling, depth of inheritance, from the IL representation of the source code. In another implementation, the structural complexity module <b>418</b> extracts the structural complexity metrics from the object code and PDB files.</p>
<p id="p-0164" num="0163">At block <b>1308</b>, an analysis analyzes the provided written test cases in combination with the structural complexity metrics. At block <b>1310</b>, locations that warrant further test case development are identified in the IL representation based on the analysis. In one implementation, the quality assessment module <b>420</b> identifies the locations in the source code from the IL representation based on the analysis.</p>
<p id="p-0165" num="0164">At block <b>1312</b>, the identified locations are presented to the user. In one implementation, the quality assessment module <b>420</b> presents the identified locations to the user on a user interface.</p>
<p id="p-0166" num="0165">In the example of <figref idref="DRAWINGS">FIG. 13</figref>, the method <b>1300</b> allows a user to identify additional test cases based on existing test cases and complexity metrics. Such a method may be performed prior to execution of any test cases or may occur after execution of one or more test cases. Where results from testing are available, such information may be used to help determine locations in the code for additional test case development.</p>
<p id="p-0167" num="0166"><figref idref="DRAWINGS">FIG. 14</figref> illustrates an exemplary method <b>1400</b> to implement a feedback mechanism for recursively prioritizing quality improvements to source code.</p>
<p id="p-0168" num="0167">At block <b>1402</b>, a device (e.g., device <b>408</b>-<b>1</b>, <b>408</b>-<b>2</b>, <b>408</b>-<b>3</b>) receives source code. In one implementation, the device can accept the source code in any implementation language such as C#, VB or C++. In another implementation, the device <b>408</b> can accept the source code in an IL representation. In yet another implementation, the device receives code written in a mark-up language (e.g., Extensible Markup Language (XML) or HyperText Markup Language (HTML)).</p>
<p id="p-0169" num="0168">At block <b>1404</b>, the device computes code coverage measures and structural complexity metrics for the source code. In one implementation, the code coverage module <b>416</b> computes the code coverage based on measures such as statement coverage, block coverage, and arc coverage. In such an implementation, the structural complexity module <b>418</b> can extract complexity metrics based on the cyclomatic complexity measure of the source code.</p>
<p id="p-0170" num="0169">At block <b>1406</b>, a set of locations are identified in the source code based on an analysis of the code coverage, the structural complexity metrics, and other factors such as the number of people using the source code, the frequency of usage, and the number of bugs reported. In one implementation, the quality assessment module <b>420</b> identifies the locations in the code based on sections of the code that are difficult to understand and difficult to test, and thereby warrant refactoring to make the sections simpler to understand and test (e.g., and debug).</p>
<p id="p-0171" num="0170">At block <b>1408</b>, the source code is refactored at the identified source code locations. The refactoring includes changing the source code based on the analysis of the code coverage, the structural complexity metrics, and other factors such as the number of people using the source code, the frequency of usage, and the number of bugs reported.</p>
<p id="p-0172" num="0171">At block <b>1410</b>, modified code coverage and structural complexity metrics are computed based on the refactored source code.</p>
<p id="p-0173" num="0172">At block <b>1412</b>, modified locations are identified in the refactored source code based on the modified code coverage measures and structural complexity metrics. For example, changes in the source code can reduce the complexity of the lines of instructions at one of the multiple identified locations. This location may then not be identified in the modified location as it may have low structural complexity metrics that may not need further test case development. However, while reducing the complexity at this location, the complexity at another location may have increased, which may then be reported as the modified location. Thus, changes in the source code can be given as a feedback to the quality assessment module <b>420</b>, which can then identify the modified locations based on the feedback and the modified code coverage measures, the structural complexity metrics, and other factors such as the number of people, the usage, the frequency of usage, the number of bugs reported.</p>
<p id="p-0174" num="0173">At block <b>1414</b>, the modified locations are presented to the user. The user can then again change the refactored code to improve the quality of the source code.</p>
<p id="h-0007" num="0000">Exemplary Computing Device</p>
<p id="p-0175" num="0174"><figref idref="DRAWINGS">FIG. 15</figref> illustrates an exemplary computing device <b>1500</b> that may be used to implement various exemplary modules (or components) and in forming an exemplary system. For example, the devices of <figref idref="DRAWINGS">FIG. 4</figref> may include various features of the device <b>1500</b>.</p>
<p id="p-0176" num="0175">In a very basic configuration, computing device <b>1500</b> typically includes at least one processing unit <b>1502</b> and system memory <b>1504</b>. Depending on the exact configuration and type of computing device, system memory <b>1504</b> may be volatile (such as RAM), non-volatile (such as ROM, flash memory, etc.) or some combination of the two. System memory <b>1504</b> typically includes an operating system <b>1505</b>, one or more program modules <b>1506</b>, and may include program data <b>1507</b>. The operating system <b>1505</b> include a component-based framework <b>1520</b> that supports components (including properties and events), objects, inheritance, polymorphism, reflection, and provides an object-oriented component-based application programming interface (API), such as that of the .NET&#x2122; Framework manufactured by Microsoft Corporation, Redmond, Wash. The device <b>1500</b> is of a very basic configuration demarcated by a dashed line <b>1508</b>. Again, a terminal may have fewer components but will interact with a computing device that may have such a basic configuration.</p>
<p id="p-0177" num="0176">Computing device <b>1500</b> may have additional features or functionality. For example, computing device <b>1500</b> may also include additional data storage devices (removable and/or non-removable) such as, for example, magnetic disks, optical disks, or tape. Such additional storage is illustrated in <figref idref="DRAWINGS">FIG. 15</figref> by removable storage <b>1509</b> and non-removable storage <b>1510</b>. Computer storage media may include volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information, such as computer readable instructions, data structures, program modules, or other data. System memory <b>1504</b>, removable storage <b>1509</b> and non-removable storage <b>1510</b> are all examples of computer storage media. Computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by computing device <b>1500</b>. Any such computer storage media may be part of device <b>1500</b>. Computing device <b>1500</b> may also have input device(s) <b>1512</b> such as keyboard, mouse, pen, voice input device, touch input device, etc. Output device(s) <b>1514</b> such as a display, speakers, printer, etc. may also be included. These devices are well know in the art and need not be discussed at length here.</p>
<p id="p-0178" num="0177">Computing device <b>1500</b> may also contain communication connections <b>1515</b> that allow the device to communicate with other computing devices <b>1518</b>, such as over a network (e.g., consider the aforementioned network <b>410</b> of <figref idref="DRAWINGS">FIG. 4</figref>). Communication connections <b>1515</b> are one example of communication media. Communication media may typically be embodied by computer readable instructions, data structures, program modules, etc.</p>
<heading id="h-0008" level="1">CONCLUSION</heading>
<p id="p-0179" num="0178">Although the invention has been described in language specific to structural features and/or methodological acts for prioritizing quality improvements to source code, it is to be understood that the invention is not necessarily limited to the specific features or acts described. Rather, the specific features and acts are disclosed as exemplary forms of implementing the invention.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method, implemented at least in part by a computing device, the method comprising:
<claim-text>receiving source code that comprises a plurality of code segments;</claim-text>
<claim-text>providing threshold levels of code coverage for the plurality of code segments, wherein the threshold levels of code coverage comprise a level as a function of complexity of a respective code segment and a desired level of quality for the source code, in which the complexity of the respective code segment determines the threshold level of code coverage for the desired level of quality such that different threshold levels of code coverage are assigned to code segments of different complexity to achieve a same desired level of quality for the source code;</claim-text>
<claim-text>providing results from test cases executed on the source code;</claim-text>
<claim-text>identifying a code segment that does not meet its threshold level of code coverage; and</claim-text>
<claim-text>recommending one or more additional test cases to increase a level of code coverage for the identified code segment, wherein the one or more additional test cases are recommended based at least in part on one or more complexity measures and the desired level of quality for the source code.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising displaying a name for the code segment that does not meet its threshold level of code coverage, a complexity metric for the code segment and a current level of code coverage.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref> further comprising displaying a risk level for the code segment where the risk level corresponds to a risk of failure of the code segment during execution of the code segment.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref> further comprising calculating a quality assessment value for a code segment based on a complexity measure for the code segment divided by a level of code coverage for the code segment.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref> further comprising executing the one or more additional test cases.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref> further comprising determining whether the code segment meets its threshold level of code coverage upon execution of the one or more additional test cases.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. A computing device comprising:
<claim-text>an interface to receive source code that comprises a plurality of code segments;</claim-text>
<claim-text>a processor;</claim-text>
<claim-text>a quality assessment module executable by the processor to determine a desired level of quality for the source code, wherein the quality assessment module defines a quality assessment value based at least in part on a complexity measure and a level of code coverage;</claim-text>
<claim-text>a complexity module executable by the processor to analyze the source code to assign a complexity measure to a code segment of the plurality of code segments, the plurality of code segments comprising a first code segment and a second code segment; and</claim-text>
<claim-text>a code coverage module executable by the processor to assign a level of code coverage to the code segment as a function of the desired level of quality and the complexity measure, in which the complexity measure of the code segment determines the level of code coverage for the code segment for the desired level of quality, wherein a higher level of code coverage is provided for the first code segment having a higher structural complexity than the second code segment having a lower structural complexity to achieve a same desired level of quality.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The computing device of <claim-ref idref="CLM-00007">claim 7</claim-ref> wherein the code coverage module is associated with a relationship between code quality, code complexity and code coverage.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The computing device of <claim-ref idref="CLM-00008">claim 8</claim-ref> wherein the relationship comprises a relationship based at least in part on a prior version of the source code.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The computing device of <claim-ref idref="CLM-00008">claim 8</claim-ref> wherein the relationship comprises a relationship based at least in part on results from test cases written for the source code.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The computing device of <claim-ref idref="CLM-00008">claim 8</claim-ref> wherein the relationship comprises a relationship based at least in part on results from test cases executed on the source code.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The computing device of <claim-ref idref="CLM-00007">claim 7</claim-ref> wherein the source code comprises code written in an object-oriented programming language.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The computing device of <claim-ref idref="CLM-00007">claim 7</claim-ref> wherein the source code comprises code written in an intermediate language.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The computing device of <claim-ref idref="CLM-00007">claim 7</claim-ref> wherein the source code comprises code written in a mark-up language.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. A storage memory device including instructions executable by a processor that, when executed, performs operations comprising:
<claim-text>receiving a list of code segments for source code of an application;</claim-text>
<claim-text>receiving a threshold level of code coverage for one or more of the plurality of code segments, wherein the threshold level of code coverage comprises a level as a function of complexity of a respective code segment and a desired level of quality for the source code, in which the complexity of the respective code segment determines the level for the desired level of quality for the source code, wherein a higher threshold level of code coverage is provided for a code segment more complex than another code segment to achieve the desired level of quality for the source code;</claim-text>
<claim-text>receiving results from test cases executed on the source code;</claim-text>
<claim-text>identifying code segments in the list that do not meet their respective threshold levels;</claim-text>
<claim-text>recommending additional test cases to increase level of code coverage for the identified code segments; and</claim-text>
<claim-text>prioritizing the additional test cases.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The storage memory device of <claim-ref idref="CLM-00015">claim 15</claim-ref> wherein the prioritizing comprises prioritizing the additional test cases based at least in part on complexity.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The storage memory device of <claim-ref idref="CLM-00015">claim 15</claim-ref> wherein the prioritizing comprises prioritizing the additional test cases based at least in part on threshold levels.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The storage memory device of <claim-ref idref="CLM-00015">claim 15</claim-ref> wherein the prioritizing comprises prioritizing the additional test cases based at least in part on a risk of failure for the one or more of the code segments.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The storage memory device of <claim-ref idref="CLM-00015">claim 15</claim-ref> further comprising calculating a quality assessment value for a code segment in the list based at least in part on a complexity measure for the code segment divided by a level of code coverage for the code segment.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The storage memory device of <claim-ref idref="CLM-00015">claim 15</claim-ref>, further comprising calculating a quality assessment value for a code segment in the list based at least in part on a complexity measure for the code segment and a level of code coverage. </claim-text>
</claim>
</claims>
</us-patent-grant>
