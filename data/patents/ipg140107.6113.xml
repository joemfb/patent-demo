<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08627237-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08627237</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13787716</doc-number>
<date>20130306</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<us-term-of-grant>
<disclaimer>
<text>This patent is subject to a terminal disclaimer.</text>
</disclaimer>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20130101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>3</main-group>
<subgroup>033</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>715863</main-classification>
<further-classification>715716</further-classification>
<further-classification>345173</further-classification>
<further-classification>345179</further-classification>
<further-classification>726 27</further-classification>
</classification-national>
<invention-title id="d2e51">Unlocking a device by performing gestures on an unlock image</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5465084</doc-number>
<kind>A</kind>
<name>Cottrell</name>
<date>19951100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5559961</doc-number>
<kind>A</kind>
<name>Blonder</name>
<date>19960900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5677710</doc-number>
<kind>A</kind>
<name>Thompson-Rohrlich</name>
<date>19971000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5821933</doc-number>
<kind>A</kind>
<name>Keller et al.</name>
<date>19981000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5907327</doc-number>
<kind>A</kind>
<name>Ogura et al.</name>
<date>19990500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6151208</doc-number>
<kind>A</kind>
<name>Bartlett</name>
<date>20001100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6160555</doc-number>
<kind>A</kind>
<name>Kang et al.</name>
<date>20001200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6192478</doc-number>
<kind>B1</kind>
<name>Elledge</name>
<date>20010200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6249606</doc-number>
<kind>B1</kind>
<name>Kiraly et al.</name>
<date>20010600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6323846</doc-number>
<kind>B1</kind>
<name>Westerman et al.</name>
<date>20011100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6347290</doc-number>
<kind>B1</kind>
<name>Bartlett</name>
<date>20020200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6421453</doc-number>
<kind>B1</kind>
<name>Kanevsky et al.</name>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>6424711</doc-number>
<kind>B1</kind>
<name>Bayless et al.</name>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37935509</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>6570557</doc-number>
<kind>B1</kind>
<name>Westerman et al.</name>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>6573883</doc-number>
<kind>B1</kind>
<name>Bartlett</name>
<date>20030600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>6633310</doc-number>
<kind>B1</kind>
<name>Andrew et al.</name>
<date>20031000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>6677932</doc-number>
<kind>B1</kind>
<name>Westerman</name>
<date>20040100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>6720860</doc-number>
<kind>B1</kind>
<name>Narayanaswami</name>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>6735695</doc-number>
<kind>B1</kind>
<name>Gopalakrishnan et al.</name>
<date>20040500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>7124433</doc-number>
<kind>B2</kind>
<name>Little</name>
<date>20061000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>7151843</doc-number>
<kind>B2</kind>
<name>Rui et al.</name>
<date>20061200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>7174462</doc-number>
<kind>B2</kind>
<name>Pering et al.</name>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>7236576</doc-number>
<kind>B2</kind>
<name>Schnarel et al.</name>
<date>20070600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37914201</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>7263670</doc-number>
<kind>B2</kind>
<name>Rekimoto</name>
<date>20070800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>7286063</doc-number>
<kind>B2</kind>
<name>Gauthey et al.</name>
<date>20071000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>341 34</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>7395506</doc-number>
<kind>B2</kind>
<name>Tan et al.</name>
<date>20080700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>7480870</doc-number>
<kind>B2</kind>
<name>Anzures et al.</name>
<date>20090100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>7627904</doc-number>
<kind>B2</kind>
<name>Tokkonen</name>
<date>20091200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>726 27</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>7810105</doc-number>
<kind>B2</kind>
<name>Prabandham et al.</name>
<date>20101000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>8095879</doc-number>
<kind>B2</kind>
<name>Goertz</name>
<date>20120100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715716</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>US</country>
<doc-number>8352745</doc-number>
<kind>B2</kind>
<name>McKeeth</name>
<date>20130100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>US</country>
<doc-number>2001/0012022</doc-number>
<kind>A1</kind>
<name>Smith</name>
<date>20010800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00033">
<document-id>
<country>US</country>
<doc-number>2002/0015024</doc-number>
<kind>A1</kind>
<name>Westerman et al.</name>
<date>20020200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00034">
<document-id>
<country>US</country>
<doc-number>2002/0191029</doc-number>
<kind>A1</kind>
<name>Gillespie et al.</name>
<date>20021200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00035">
<document-id>
<country>US</country>
<doc-number>2002/0196274</doc-number>
<kind>A1</kind>
<name>Comfort et al.</name>
<date>20021200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00036">
<document-id>
<country>US</country>
<doc-number>2003/0142138</doc-number>
<kind>A1</kind>
<name>Brown et al.</name>
<date>20030700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00037">
<document-id>
<country>US</country>
<doc-number>2003/0206202</doc-number>
<kind>A1</kind>
<name>Moriya</name>
<date>20031100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00038">
<document-id>
<country>US</country>
<doc-number>2003/0222913</doc-number>
<kind>A1</kind>
<name>Mattila et al.</name>
<date>20031200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00039">
<document-id>
<country>US</country>
<doc-number>2004/0030934</doc-number>
<kind>A1</kind>
<name>Mizoguchi et al.</name>
<date>20040200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00040">
<document-id>
<country>US</country>
<doc-number>2004/0034801</doc-number>
<kind>A1</kind>
<name>Jaeger</name>
<date>20040200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00041">
<document-id>
<country>US</country>
<doc-number>2004/0085351</doc-number>
<kind>A1</kind>
<name>Tokkonen</name>
<date>20040500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00042">
<document-id>
<country>US</country>
<doc-number>2004/0088568</doc-number>
<kind>A1</kind>
<name>Tokkonen</name>
<date>20040500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00043">
<document-id>
<country>US</country>
<doc-number>2004/0230843</doc-number>
<kind>A1</kind>
<name>Jansen</name>
<date>20041100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00044">
<document-id>
<country>US</country>
<doc-number>2004/0250138</doc-number>
<kind>A1</kind>
<name>Schneider</name>
<date>20041200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00045">
<document-id>
<country>US</country>
<doc-number>2004/0260955</doc-number>
<kind>A1</kind>
<name>Mantyla</name>
<date>20041200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00046">
<document-id>
<country>US</country>
<doc-number>2004/0268267</doc-number>
<kind>A1</kind>
<name>Moravcsik</name>
<date>20041200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00047">
<document-id>
<country>US</country>
<doc-number>2005/0050477</doc-number>
<kind>A1</kind>
<name>Robertson et al.</name>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00048">
<document-id>
<country>US</country>
<doc-number>2005/0060554</doc-number>
<kind>A1</kind>
<name>O'Donoghue</name>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00049">
<document-id>
<country>US</country>
<doc-number>2005/0079896</doc-number>
<kind>A1</kind>
<name>Kokko et al.</name>
<date>20050400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00050">
<document-id>
<country>US</country>
<doc-number>2005/0212760</doc-number>
<kind>A1</kind>
<name>Marvit et al.</name>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00051">
<document-id>
<country>US</country>
<doc-number>2005/0216862</doc-number>
<kind>A1</kind>
<name>Shinohara et al.</name>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00052">
<document-id>
<country>US</country>
<doc-number>2005/0248542</doc-number>
<kind>A1</kind>
<name>Sawanobori</name>
<date>20051100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00053">
<document-id>
<country>US</country>
<doc-number>2005/0253817</doc-number>
<kind>A1</kind>
<name>Rytivaara et al.</name>
<date>20051100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345173</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00054">
<document-id>
<country>US</country>
<doc-number>2005/0264833</doc-number>
<kind>A1</kind>
<name>Hiraoka et al.</name>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00055">
<document-id>
<country>US</country>
<doc-number>2005/0289476</doc-number>
<kind>A1</kind>
<name>Tokkonen</name>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00056">
<document-id>
<country>US</country>
<doc-number>2006/0031776</doc-number>
<kind>A1</kind>
<name>Glein et al.</name>
<date>20060200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00057">
<document-id>
<country>US</country>
<doc-number>2006/0161870</doc-number>
<kind>A1</kind>
<name>Hotelling et al.</name>
<date>20060700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00058">
<document-id>
<country>US</country>
<doc-number>2006/0174339</doc-number>
<kind>A1</kind>
<name>Tao</name>
<date>20060800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00059">
<document-id>
<country>US</country>
<doc-number>2006/0267955</doc-number>
<kind>A1</kind>
<name>Hino</name>
<date>20061100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00060">
<document-id>
<country>US</country>
<doc-number>2007/0150826</doc-number>
<kind>A1</kind>
<name>Anzures et al.</name>
<date>20070600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00061">
<document-id>
<country>US</country>
<doc-number>2007/0150842</doc-number>
<kind>A1</kind>
<name>Chaudhri et al.</name>
<date>20070600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00062">
<document-id>
<country>US</country>
<doc-number>2008/0034292</doc-number>
<kind>A1</kind>
<name>Brunner et al.</name>
<date>20080200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00063">
<document-id>
<country>US</country>
<doc-number>2008/0034307</doc-number>
<kind>A1</kind>
<name>Cisler et al.</name>
<date>20080200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00064">
<document-id>
<country>US</country>
<doc-number>2008/0055263</doc-number>
<kind>A1</kind>
<name>Lemay et al.</name>
<date>20080300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00065">
<document-id>
<country>US</country>
<doc-number>2008/0055269</doc-number>
<kind>A1</kind>
<name>Lemay et al.</name>
<date>20080300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00066">
<document-id>
<country>US</country>
<doc-number>2008/0072172</doc-number>
<kind>A1</kind>
<name>Shinohara et al.</name>
<date>20080300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00067">
<document-id>
<country>US</country>
<doc-number>2008/0082934</doc-number>
<kind>A1</kind>
<name>Kocienda et al.</name>
<date>20080400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00068">
<document-id>
<country>US</country>
<doc-number>2008/0094371</doc-number>
<kind>A1</kind>
<name>Forstall et al.</name>
<date>20080400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00069">
<document-id>
<country>US</country>
<doc-number>2008/0122796</doc-number>
<kind>A1</kind>
<name>Jobs et al.</name>
<date>20080500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00070">
<document-id>
<country>US</country>
<doc-number>2008/0134170</doc-number>
<kind>A1</kind>
<name>Astheimer</name>
<date>20080600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00071">
<document-id>
<country>US</country>
<doc-number>2008/0168290</doc-number>
<kind>A1</kind>
<name>Jobs et al.</name>
<date>20080700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00072">
<document-id>
<country>US</country>
<doc-number>2009/0006991</doc-number>
<kind>A1</kind>
<name>Lindberg et al.</name>
<date>20090100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00073">
<document-id>
<country>US</country>
<doc-number>2009/0128581</doc-number>
<kind>A1</kind>
<name>Brid et al.</name>
<date>20090500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00074">
<document-id>
<country>US</country>
<doc-number>2009/0215497</doc-number>
<kind>A1</kind>
<name>Louch</name>
<date>20090800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00075">
<document-id>
<country>US</country>
<doc-number>2009/0241072</doc-number>
<kind>A1</kind>
<name>Chaudhri et al.</name>
<date>20090900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00076">
<document-id>
<country>US</country>
<doc-number>2009/0284482</doc-number>
<kind>A1</kind>
<name>Chin</name>
<date>20091100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00077">
<document-id>
<country>US</country>
<doc-number>2010/0079380</doc-number>
<kind>A1</kind>
<name>Nurmi</name>
<date>20100400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00078">
<document-id>
<country>US</country>
<doc-number>2010/0162180</doc-number>
<kind>A1</kind>
<name>Dunnam et al.</name>
<date>20100600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00079">
<document-id>
<country>US</country>
<doc-number>2010/0235732</doc-number>
<kind>A1</kind>
<name>Bergman</name>
<date>20100900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00080">
<document-id>
<country>US</country>
<doc-number>2010/0306718</doc-number>
<kind>A1</kind>
<name>Shim et al.</name>
<date>20101200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00081">
<document-id>
<country>US</country>
<doc-number>2011/0010672</doc-number>
<kind>A1</kind>
<name>Hope</name>
<date>20110100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00082">
<document-id>
<country>US</country>
<doc-number>2011/0283241</doc-number>
<kind>A1</kind>
<name>Miller et al.</name>
<date>20111100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00083">
<document-id>
<country>US</country>
<doc-number>2012/0009896</doc-number>
<kind>A1</kind>
<name>Bandyopadhyay et al.</name>
<date>20120100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00084">
<document-id>
<country>US</country>
<doc-number>2012/0021724</doc-number>
<kind>A1</kind>
<name>Olsen et al.</name>
<date>20120100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00085">
<document-id>
<country>US</country>
<doc-number>2012/0023458</doc-number>
<kind>A1</kind>
<name>Chaudhri et al.</name>
<date>20120100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00086">
<document-id>
<country>US</country>
<doc-number>2012/0182226</doc-number>
<kind>A1</kind>
<name>Tuli</name>
<date>20120700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00087">
<document-id>
<country>US</country>
<doc-number>2012/0188206</doc-number>
<kind>A1</kind>
<name>Sparf et al.</name>
<date>20120700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00088">
<document-id>
<country>EP</country>
<doc-number>1 284 450</doc-number>
<kind>A2</kind>
<date>20030200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00089">
<document-id>
<country>EP</country>
<doc-number>2 060 970</doc-number>
<kind>A1</kind>
<date>20090500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00090">
<document-id>
<country>GB</country>
<doc-number>2 313 460</doc-number>
<kind>A</kind>
<date>19971100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00091">
<document-id>
<country>JP</country>
<doc-number>60 171560</doc-number>
<date>19850900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00092">
<document-id>
<country>JP</country>
<doc-number>02 249062</doc-number>
<date>19901000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00093">
<document-id>
<country>JP</country>
<doc-number>05 127819</doc-number>
<date>19930500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00094">
<document-id>
<country>JP</country>
<doc-number>06 214954</doc-number>
<date>19940800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00095">
<document-id>
<country>JP</country>
<doc-number>07 084661</doc-number>
<date>19950300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00096">
<document-id>
<country>JP</country>
<doc-number>08 263215</doc-number>
<date>19961000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00097">
<document-id>
<country>JP</country>
<doc-number>09 018566</doc-number>
<date>19970100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00098">
<document-id>
<country>JP</country>
<doc-number>11 203045</doc-number>
<date>19990700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00099">
<document-id>
<country>JP</country>
<doc-number>2000 322199</doc-number>
<date>20001100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00100">
<document-id>
<country>JP</country>
<doc-number>2000 349886</doc-number>
<date>20001200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00101">
<document-id>
<country>JP</country>
<doc-number>2003 091370</doc-number>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00102">
<document-id>
<country>JP</country>
<doc-number>2004 252720</doc-number>
<date>20040900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00103">
<document-id>
<country>JP</country>
<doc-number>2004 348599</doc-number>
<date>20041200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00104">
<document-id>
<country>JP</country>
<doc-number>2005 071008</doc-number>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00105">
<document-id>
<country>JP</country>
<doc-number>2005 167455</doc-number>
<kind>A</kind>
<date>20050600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00106">
<document-id>
<country>WO</country>
<doc-number>WO 01/77792</doc-number>
<kind>A2</kind>
<date>20011000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00107">
<document-id>
<country>WO</country>
<doc-number>WO 02/33882</doc-number>
<kind>A1</kind>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00108">
<document-id>
<country>WO</country>
<doc-number>WO 03/038569</doc-number>
<kind>A2</kind>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00109">
<document-id>
<country>WO</country>
<doc-number>WO 2004/001560</doc-number>
<kind>A1</kind>
<date>20031200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00110">
<document-id>
<country>WO</country>
<doc-number>WO 2004/021108</doc-number>
<kind>A2</kind>
<date>20040300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00111">
<document-id>
<country>WO</country>
<doc-number>WO 2005/041020</doc-number>
<kind>A1</kind>
<date>20050500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00112">
<othercit>Neonode Inc., &#x201c;Welcome to the N1 Guide,&#x201d; neonode.com, Jul. 2004, 42 pages, http://www.ebookspdf.com/gadget/2818/neonode-n1m-manual/.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00113">
<othercit>Bardram, J., &#x201c;The trouble with login: on usability and computer security in ubiquitous computing,&#x201d; Centre for Pervasive Healthcare, Department of Computer Science, University of Aahus, Denmark, Published online: Jul. 23, 2005, 11 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00114">
<othercit>Baudisch, P., &#x201c;Phosphor: Explaining Transitions in the User Interface Using Afterglow Effects,&#x201d; 2006, 10 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00115">
<othercit>DailyTech, &#x201c;Neonode Patented Swipe-to-Unlock 3 Years Before Apple,&#x201d; dailytech.com, http://www.dailytech.com/Analysis+Neonode+SwipetoUnlock+3+Years+Before+Apple, Feb. 20, 2012, 4 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00116">
<othercit>Fitzpatrick et al., &#x201c;Method for Access Control via Gestural Verification,&#x201d; IBM Technical Disclosure Bulletin, vol. 36, No. 09B, Sep. 1993, 2 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00117">
<othercit>GridLock (Palm OS), &#x201c;Graphical Security System for Your Palm,&#x201d; Softonic, Oct. 8, 2003, 2 pages, http://gridlock.en.softonic.com/palm.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00118">
<othercit>Horry et al., &#x201c;A Passive-Style Buttonless Mobile Terminal,&#x201d; IEEE Transactions on Consumer Electronics, vol. 49, No. 3, Aug. 2003, 6 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00119">
<othercit>IBM, &#x201c;Touch Pad Authentication,&#x201d; Sep. 21, 2004, 2 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00120">
<othercit>Jansen, W., &#x201c;Authenticating Users on Handheld Devices,&#x201d; The National Institute of Standards and Technology, Gaithersburg, Maryland, 2003, 13 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00121">
<othercit>Jermyn et al., &#x201c;The Design and Analysis of Graphical Passwords,&#x201d; &#xa9; 1999 by The USENIX Association, 15 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00122">
<othercit>JGUI Professional, &#x201c;Touch Password Protection,&#x201d; printed Dec. 30, 2005, 4 pages, http://www.jgui.net/touch/index.html.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00123">
<othercit>McLean et al., &#x201c;Access/Control Icons (Icon Keys),&#x201d; IBM Technical Disclosure Bulletin, vol. 38, No. 04, Apr. 1995, 3 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00124">
<othercit>Monrose, N., &#x201c;Towards Stronger User Authentication,&#x201d; Ph.d dissertation, 1999, New York University, vol. 60/05-B of Dissertation Abstract International, Order No. AAD99-30229, 128 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00125">
<othercit>Najjar. L., &#x201c;Graphical Passwords,&#x201d; International Technology Disclosure vol. 10, No. 1, Jan. 25, 1992, 1 page.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00126">
<othercit>Neonode, Inc., &#x201c;Welcome to the N1 Guide,&#x201d; neonode.com, Jul. 2004, 42 pages, http://www.ebookspdf.com/gadget/2818/neonode-n1m-manual/.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00127">
<othercit>Neonode, &#x201c;N1 Quick Start Guide,&#x201d; Neonode.com, Apr. 5, 2005, 24 pages, http://www.neonode.com/upload/DOCUMENTS/<sub>&#x2014;</sub>EN/N1<sub>&#x2014;</sub>Quick<sub>&#x2014;</sub>Start<sub>&#x2014;</sub>Guide.pdf.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00128">
<othercit>Plaisant et all., &#x201c;Touchscreen Toggle Design,&#x201d; Proceedings of the Conference on Human Factors in Computing Systems, Addison Wesley, US, May 3, 1992, 2 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00129">
<othercit>Renaud et al., &#x201c;My password is here! An investigation into visuo-spatial authentication mechanisms,&#x201d; Interacting with Computers, vol. 16, &#xa9; 2004 Elsevier B.V., 25 pages, www.sciencedirect.com.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00130">
<othercit>Translation of German Nullity Action Complaint against EP Patent 1 964 022 (DE No. 60 2006 012 876.2), filed Dec. 15, 2010, 37 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00131">
<othercit>Wiedenbeck et al. &#x201c;PassPoints: Design and longitudinal evaluation of a graphical password system,&#x201d; International Journal of Human-Computer Studies, vol. 63, &#xa9; 2005 Elsevier Ltd., 26 pages, www.sciencedirect.com.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00132">
<othercit>European Search Report dated Oct. 13, 2009, received in European Patent Application No. 09170574.9, which corresponds to U.S. Appl. No. 11/322,549 (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00133">
<othercit>Extended European Search Report dated Feb. 7, 2011, received in European Patent Application No. 10 194 359.5, which corresponds to U.S. Appl. No. 11/322,549 (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00134">
<othercit>International Search Report and Written Opinion dated May 25, 2007, received in International Application No. PCT/US2006/061370, which corresponds to U.S. Appl. No. 11/322,549 (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00135">
<othercit>International Search Report and Written Opinion dated Apr. 23, 2007, received in International Application No. PCT/US2006/061380, which corresponds to U.S. Appl. No. 11/322,550 (Anzures).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00136">
<othercit>Office Action dated Feb. 7, 2008, received in U.S. Appl. No. 11/322,549. 28 pages (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00137">
<othercit>Office Action dated Sep. 26, 2008, received in U.S. Appl. No. 11/322,549, 32 pages (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00138">
<othercit>Final Office Action dated Mar. 23, 2009, received in U.S. Appl. No. 11/322,549, 40 pages (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00139">
<othercit>Notice of Allowance dated Aug. 10, 2009, received in U.S. Appl. No. 11/322,549, 9 pages (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00140">
<othercit>Office Action dated Jul. 20, 2011, received in Australian Patent Application No. 2010200661, which corresponds to U.S. Appl. No. 11/322,549, 2 pages (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00141">
<othercit>Notice of Acceptance dated Aug. 2, 2012, received in Australian Patent Application No. 2010200661, which corresponds to U.S. Appl. No. 11/322,549, 3 pages (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00142">
<othercit>Office Action dated Oct. 26, 2011, received in Australian Patent Application No. 2011101192, which corresponds to U.S. Appl. No. 11/322,549, 2 pages (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00143">
<othercit>Certification dated Apr. 12, 2012, of Australian Patent No. 2011101192, which corresponds to U.S. Appl. No. 11/322,549, 1 page (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00144">
<othercit>Office Action dated Oct. 26, 2011, received in Australian Patent Application No. 2011101193, which corresponds to U.S. Appl. No. 11/322,549, 2 pages (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00145">
<othercit>Certification dated Apr. 23, 2012, of Australian Patent Application No. 2011101193, which corresponds to U.S. Appl. No. 11/322,549, 1 page (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00146">
<othercit>Office Action dated Feb. 5, 2010, received in Chinese Application for Invention No. 200680052770.4, which corresponds to U.S. Appl. No. 11/322,549, 4 pages (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00147">
<othercit>Office Action dated Jul. 26, 2011, received in Chinese Patent Application No. 200910175855.7, which corresponds to U.S. Appl. No. 11/322,549, 8 pages (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00148">
<othercit>Office Action dated Jul. 4, 2012, received in Chinese Patent Appl. No. 200910175855.7, which corresponds to U.S. Appl. No. 11/322,549, 9 pages (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00149">
<othercit>Office Action dated Feb. 4, 2009, received in German Patent Application No. 11 2006 003 515.0-53 which corresponds to U.S. Appl. No. 11/322,549, 6 pages (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00150">
<othercit>Office Action dated Feb. 9, 2010, received in German Patent Application No. 11 2006 003 515.0, which corresponds to U.S. Appl. No. 11/322,549, 6 pages (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00151">
<othercit>Office Action dated Mar. 25, 2009, received in the European patent application 06 846 405.6, which corresponds to U.S. Appl. No. 11/322,549, 6 pages (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00152">
<othercit>Office Action dated Apr. 22, 2011, received in Japanese Patent Application No. 2008-547675, which corresponds to U.S. Appl. No. 11/322,549, 3 pages (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00153">
<othercit>Office Action dated Nov. 4, 2011, received in Japanese Patent Application No. 2008-547675, which corresponds to U.S. Appl. No. 11/322,549, 3 pages (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00154">
<othercit>Office Action dated Sep. 18, 2012, received in Japanese Patent Application No. 2008 547675, which corresponds to U.S. Appl. No. 11/322,549, 2 pages (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00155">
<othercit>Office Action dated Mar. 5, 2010, received in Korean Patent Application No. 10-2008-7018109, which corresponds to U.S. Appl. No. 11/322,549, 7 pages(Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00156">
<othercit>Office Action dated Oct. 31, 2007, received in U.S. Appl. No. 11/322,550. 23 pages (Anzures).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00157">
<othercit>Office Action dated Apr. 21, 2008 received in, U.S. Appl. No. 11/322,550, 22 pages (Anzures).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00158">
<othercit>Notice of Allowance dated Sep. 19, 2008, received in U.S. Appl. No. 11/322,550. 8 pages (Anzures).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00159">
<othercit>Office Action dated Jul. 24, 2009, received in U.S. Appl. No. 12/345,584, 11 pages (Anzures).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00160">
<othercit>Office Action dated Nov. 16, 2009, received in U.S. Appl. No. 12/345,584, 17 pages (Anzures).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00161">
<othercit>Notice of Allowance dated Jun. 3, 2010, received in U.S. Appl. No. 12/345,584, 9 pages (Anzures).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00162">
<othercit>Office Action mailed Jan. 29, 2010, received in U.S. Appl. No. 12/477,075, 18 pages (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00163">
<othercit>Office Action dated Sep. 17, 2010, received in U.S. Appl. No. 12/477,075. 9 pages (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00164">
<othercit>Office Action dated Feb. 7, 2011, received in U.S. Appl. No. 12/477,075. 10 pages (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00165">
<othercit>Notice of Allowance dated Aug. 10, 2011, received in U.S. Appl. No. 12/477,075, 12 pages (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00166">
<othercit>Office Action dated Jan. 6, 2012, received in U.S. Appl. No. 13/204,572, 15 pages (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00167">
<othercit>Notice of Allowance dated Jun. 12, 2012, received in U.S. Appl. No. 13/204,572, 9 pages (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00168">
<othercit>Office Action dated Nov. 25, 2011, received in U.S. Appl. No. 12/250,659 (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00169">
<othercit>Notice of Allowance dated May 11, 2012, received in U.S. Appl. No. 13/250,659, 15 pages (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00170">
<othercit>Office Action dated Nov. 19, 2012, received in U.S. Appl. No. 13/563,663, 16 pages (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00171">
<othercit>Ni et al., &#x201c;DiffUser: Differentiated User Access Control on Smartphones,&#x201d; International Conference on Mobile Adhoc and Sensor Systems, Nov. 17, 2009, Mass '09, IEEE 6th, 6 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00172">
<othercit>Statement on the preliminary opinion in the matter of <i>Motorola Mobility Germany GmbH </i>vs <i>Apple, Inc</i>., Feb. 21, 2013, together with Exhibits NK 11-NK18, 156 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00173">
<othercit>Samsung response dated Feb. 21, 2013, to Court's notification in the matter of <i>Samsung Electronics GmbH </i>vs <i>Apple Inc</i>., 6 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00174">
<othercit><i>HTC Europe Co. Ltd</i>. vs. <i>Apple Inc</i>. Nullity Reply Brief dated Nov. 8, 2012, 17 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00175">
<othercit><i>Samsung Electronics GmbH </i>vs. <i>Apple Inc</i>., Second reply brief dated Nov. 19, 2012, together with exhibits D12-D21 and D25, 269 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00176">
<othercit><i>Samsung Electronics GmbH </i>vs. <i>Apple Inc</i>., Supplement to the cancellation request, dated Mar. 1, 2013, together with Exhibits D26-D32, 211 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00177">
<othercit>Office Action dated Mar. 11, 2013, received in Japanese Patent Application No. 2013-007818, 5 pages (Christie).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00178">
<othercit>Grant of application dated May 25, 2013, received in Japanese Patent Application No. 2012-091352, which corresponds to U.S. Appl. No. 11/322,549, 3 pages (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00179">
<othercit>Grant of application dated May 31, 2013, received in Japanese Patent Application No. 2013-007818, which corresponds to U.S. Appl. No. 11/322,549, 4 pages (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00180">
<othercit>Notice of Allowance dated Jul. 5, 2013, received in Japanese Patent Application No. 2013-101691, which corresponds to U.S. Appl. No. 11/322,549, 4 pages (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00181">
<othercit>Notice of Allowance dated Jun. 25, 2013, received in U.S. Appl. No. 13/787,712, 16 pages (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00182">
<othercit>Notice of Allowance dated Jun. 17, 2013, received in U.S. Appl. No. 13/791,799, 16 pages (Chaudhri).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00183">
<othercit>Notice of Allowance dated May 2, 2013, received in U.S. Appl. No. 12/842,899, 11 pages (Shi).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>30</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>715154</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>715863</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345156</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>15</number-of-drawing-sheets>
<number-of-figures>25</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>13563663</doc-number>
<date>20120731</date>
</document-id>
<parent-status>PENDING</parent-status>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13787716</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>13204572</doc-number>
<date>20110805</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>8286103</doc-number>
<date>20121009</date>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13563663</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>12477075</doc-number>
<date>20090602</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>8046721</doc-number>
<date>20111025</date>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13204572</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>11322549</doc-number>
<date>20051223</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>7657849</doc-number>
<date>20100202</date>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>12477075</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20130185678</doc-number>
<kind>A1</kind>
<date>20130718</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only" applicant-authority-category="assignee">
<addressbook>
<orgname>Apple Inc.</orgname>
<address>
<city>Cupertino</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Chaudhri</last-name>
<first-name>Imran</first-name>
<address>
<city>San Francisco</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Ording</last-name>
<first-name>Bas</first-name>
<address>
<city>San Francisco</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Anzures</last-name>
<first-name>Freddy Allen</first-name>
<address>
<city>San Francisco</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="004" designation="us-only">
<addressbook>
<last-name>van Os</last-name>
<first-name>Marcel</first-name>
<address>
<city>San Francisco</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="005" designation="us-only">
<addressbook>
<last-name>Forstall</last-name>
<first-name>Scott</first-name>
<address>
<city>Los Altos</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="006" designation="us-only">
<addressbook>
<last-name>Christie</last-name>
<first-name>Greg</first-name>
<address>
<city>San Jose</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Morgan, Lewis &#x26; Bockius LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Apple Inc.</orgname>
<role>02</role>
<address>
<city>Cupertino</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Refai</last-name>
<first-name>Ramsey</first-name>
<department>2141</department>
</primary-examiner>
<assistant-examiner>
<last-name>Gutierrez</last-name>
<first-name>Andres E</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A device with a touch-sensitive display may be unlocked via gestures performed on the touch-sensitive display. The device is unlocked if contact with the display corresponds to a predefined gesture for unlocking the device. The device displays one or more unlock images with respect to which the predefined gesture is to be performed in order to unlock the device. The performance of the predefined gesture with respect to the unlock image may include moving the unlock image to a predefined location and/or moving the unlock image along a predefined path. The device may also display visual cues of the predefined gesture on the touch screen to remind a user of the gesture.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="124.71mm" wi="160.95mm" file="US08627237-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="267.72mm" wi="187.37mm" file="US08627237-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="275.25mm" wi="182.54mm" file="US08627237-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="266.36mm" wi="184.57mm" file="US08627237-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="260.27mm" wi="192.11mm" file="US08627237-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="237.07mm" wi="179.15mm" file="US08627237-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="248.67mm" wi="174.41mm" file="US08627237-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="167.05mm" wi="131.32mm" file="US08627237-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="260.27mm" wi="176.45mm" file="US08627237-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="239.78mm" wi="181.19mm" file="US08627237-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="269.75mm" wi="166.88mm" file="US08627237-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="273.81mm" wi="188.72mm" file="US08627237-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="138.94mm" wi="140.97mm" file="US08627237-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="251.38mm" wi="172.38mm" file="US08627237-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="256.12mm" wi="179.83mm" file="US08627237-20140107-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="244.52mm" wi="188.72mm" file="US08627237-20140107-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application is a continuation of U.S. application Ser. No. 13/563,663, filed Jul. 31, 2012, which is a continuation of U.S. patent application Ser. No. 13/204,572 filed Aug. 5, 2011, now U.S. Pat. No. 8,286,103, which is a continuation of U.S. patent application Ser. No. 12/477,075, filed Jun. 2, 2009, now U.S. Pat. No. 8,046,721, which is a continuation of U.S. patent application Ser. No. 11/322,549, filed Dec. 23, 2005, now U.S. Pat. No. 7,657,849, which application are incorporated by reference herein in their entirety.</p>
<p id="p-0003" num="0002">This application is related to U.S. patent application Ser. No. 11/322,550, titled &#x201c;Indication of Progress Towards Satisfaction of a User Input Condition,&#x201d; filed Dec. 23, 2005, which application is incorporated by reference herein in its entirety.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">TECHNICAL FIELD</heading>
<p id="p-0004" num="0003">The disclosed embodiments relate generally to user interfaces that employ touch-sensitive displays, and more particularly, to the unlocking of user interfaces on portable electronic devices.</p>
<heading id="h-0003" level="1">BACKGROUND</heading>
<p id="p-0005" num="0004">Touch-sensitive displays (also known as &#x201c;touch screens&#x201d; or &#x201c;touchscreens&#x201d;) are well known in the art. Touch screens are used in many electronic devices to display graphics and text, and to provide a user interface through which a user may interact with the devices. A touch screen detects and responds to contact on the touch screen. A device may display one or more soft keys, menus, and other user-interface objects on the touch screen. A user may interact with the device by contacting the touch screen at locations corresponding to the user-interface objects with which she wishes to interact.</p>
<p id="p-0006" num="0005">Touch screens are becoming more popular for use as displays and as user input devices on portable devices, such as mobile telephones and personal digital assistants (PDAs). One problem associated with using touch screens on portable devices is the unintentional activation or deactivation of functions due to unintentional contact with the touch screen. Thus, portable devices, touch screens on such devices, and/or applications running on such devices may be locked upon satisfaction of predefined lock conditions, such as upon entering an active call, after a predetermined time of idleness has elapsed, or upon manual locking by a user.</p>
<p id="p-0007" num="0006">Devices with touch screens and/or applications running on such devices may be unlocked by any of several well-known unlocking procedures, such as pressing a predefined set of buttons (simultaneously or sequentially) or entering a code or password. These unlock procedures, however, have drawbacks. The button combinations may be hard to perform. Creating, memorizing, and recalling passwords, codes, and the like can be quite burdensome. These drawbacks may reduce the ease of use of the unlocking process and, as a consequence, the ease of use of the device in general.</p>
<p id="p-0008" num="0007">Accordingly, there is a need for more efficient, user-friendly procedures for unlocking such devices, touch screens, and/or applications. More generally, there is a need for more efficient, user-friendly procedures for transitioning such devices, touch screens, and/or applications between user interface states (e.g., from a user interface state for a first application to a user interface state for a second application, between user interface states in the same application, or between locked and unlocked states). In addition, there is a need for sensory feedback to the user regarding progress towards satisfaction of a user input condition that is required for the transition to occur.</p>
<heading id="h-0004" level="1">SUMMARY</heading>
<p id="p-0009" num="0008">In some embodiments, a method of controlling an electronic device with a touch-sensitive display includes: detecting contact with the touch-sensitive display while the device is in a user-interface lock state; moving an image corresponding to a user-interface unlock state of the device in accordance with the contact; transitioning the device to the user-interface unlock state if the detected contact corresponds to a predefined gesture; and maintaining the device in the user-interface lock state if the detected contact does not correspond to the predefined gesture.</p>
<p id="p-0010" num="0009">In some embodiments, a method of controlling a device with a touch-sensitive display includes: displaying an image on the touch-sensitive display while the device is in a user-interface lock state; detecting contact with the touch-sensitive display; transitioning the device to a user-interface unlock state if the detected contact corresponds to moving the image to a predefined location on the touch-sensitive display; and maintaining the device in the user-interface lock state if the detected contact does not correspond to moving the image to the predefined location.</p>
<p id="p-0011" num="0010">In some embodiments, a method of controlling a device with a touch-sensitive display includes: displaying an image on the touch-sensitive display while the device is in a user-interface lock state; detecting contact with the touch-sensitive display; and transitioning the device to a user-interface unlock state if the detected contact corresponds to moving the image on the touch-sensitive display according to a predefined path on the touch-sensitive display; and maintaining the device in the user-interface lock state if the detected contact does not correspond to moving the image according to the predefined path.</p>
<p id="p-0012" num="0011">In some embodiments, a method of controlling a device with a touch-sensitive display includes: displaying first and second images on the touch-sensitive display while the device is in a user-interface lock state; detecting contact with the touch-sensitive display; transitioning the device to a first active state corresponding to the first image if the detected contact corresponds to a predefined gesture with respect to the first image; and transitioning the device to a second active state distinct from the first active state if the detected contact corresponds to a predefined gesture with respect to the second image.</p>
<p id="p-0013" num="0012">The aforementioned methods may be performed by a portable electronic device having a touch-sensitive display with a graphical user interface (GUI), one or more processors, memory and one or more modules, programs or sets of instructions stored in the memory for performing these methods. In some embodiments, the portable electronic device provides a plurality of functions, including wireless communication.</p>
<p id="p-0014" num="0013">Instructions for performing the aforementioned methods may be included in a computer program product configured for execution by one or more processors. In some embodiments, the executable computer program product includes a computer readable storage medium (e.g., one or more magnetic disk storage devices, flash memory devices, or other non-volatile solid state memory devices) and an executable computer program mechanism embedded therein.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0015" num="0014">For a better understanding of the aforementioned embodiments of the invention as well as additional embodiments thereof, reference should be made to the Description of Embodiments below, in conjunction with the following drawings in which like reference numerals refer to corresponding parts throughout the figures.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram illustrating a portable electronic device, according to some embodiments of the invention.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 2</figref> is a flow diagram illustrating a process for transitioning a device to a user-interface unlock state, according to some embodiments of the invention.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 3</figref> is a flow diagram illustrating a process for transitioning a device to a user-interface unlock state, according to some embodiments of the invention.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIGS. 4A-4B</figref> illustrate the GUI display of a device in a user-interface lock state, according to some embodiments of the invention.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIGS. 5A-5D</figref> illustrate the GUI display of a device at various points of the performance of an unlock action gesture, according to some embodiments of the invention.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 6</figref> is a flow diagram illustrating a process for indicating progress towards satisfaction of a user input condition according to some embodiments of the invention.</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIGS. 7A-7D</figref> illustrate the GUI display of a device that is transitioning the optical intensity of user-interface objects, according to some embodiments of the invention.</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIGS. 8A-8C</figref> are graphs illustrating optical intensity as a function of the completion of the user input condition, according to some embodiments of the invention.</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 9</figref> is a flow diagram illustrating a process for transitioning a device to a user interface active state, according to some embodiments of the invention.</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 10</figref> illustrates the GUI of a device in a user-interface lock state that displays a plurality of unlock images, according to some embodiments of the invention.</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIGS. 11A-11F</figref> illustrate the GUI display of a device at various points in the performance of an unlock action gesture, according to some embodiments of the invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0006" level="1">DESCRIPTION OF EMBODIMENTS</heading>
<p id="p-0027" num="0026">Reference will now be made in detail to embodiments, examples of which are illustrated in the accompanying drawings. In the following detailed description, numerous specific details are set forth in order to provide a thorough understanding of the present invention. However, it will be apparent to one of ordinary skill in the art that the present invention may be practiced without these specific details. In other instances, well-known methods, procedures, components, and circuits have not been described in detail so as not to unnecessarily obscure aspects of the embodiments.</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 1</figref> illustrates a portable electronic device, according to some embodiments of the invention. The device <b>100</b> includes a memory <b>102</b>, a memory controller <b>104</b>, one or more processing units (CPU's) <b>106</b>, a peripherals interface <b>108</b>, RF circuitry <b>112</b>, audio circuitry <b>114</b>, a speaker <b>116</b>, a microphone <b>118</b>, an input/output (I/O) subsystem <b>120</b>, a touch screen <b>126</b>, other input or control devices <b>128</b>, and an external port <b>148</b>. These components communicate over the one or more communication buses or signal lines <b>110</b>. The device <b>100</b> can be any portable electronic device, including but not limited to a handheld computer, a tablet computer, a mobile phone, a media player, a personal digital assistant (PDA), or the like, including a combination of two or more of these items. It should be appreciated that the device <b>100</b> is only one example of a portable electronic device <b>100</b>, and that the device <b>100</b> may have more or fewer components than shown, or a different configuration of components. The various components shown in <figref idref="DRAWINGS">FIG. 1</figref> may be implemented in hardware, software or a combination of both hardware and software, including one or more signal processing and/or application specific integrated circuits.</p>
<p id="p-0029" num="0028">The memory <b>102</b> may include high speed random access memory and may also include non-volatile memory, such as one or more magnetic disk storage devices, flash memory devices, or other non-volatile solid state memory devices. In some embodiments, the memory <b>102</b> may further include storage remotely located from the one or more processors <b>106</b>, for instance network attached storage accessed via the RF circuitry <b>112</b> or external port <b>148</b> and a communications network (not shown) such as the Internet, intranet(s), Local Area Networks (LANs), Wide Local Area Networks (WLANs), Storage Area Networks (SANs) and the like, or any suitable combination thereof. Access to the memory <b>102</b> by other components of the device <b>100</b>, such as the CPU <b>106</b> and the peripherals interface <b>108</b>, may be controlled by the memory controller <b>104</b>.</p>
<p id="p-0030" num="0029">The peripherals interface <b>108</b> couples the input and output peripherals of the device to the CPU <b>106</b> and the memory <b>102</b>. The one or more processors <b>106</b> run various software programs and/or sets of instructions stored in the memory <b>102</b> to perform various functions for the device <b>100</b> and to process data.</p>
<p id="p-0031" num="0030">In some embodiments, the peripherals interface <b>108</b>, the CPU <b>106</b>, and the memory controller <b>104</b> may be implemented on a single chip, such as a chip <b>111</b>. In some other embodiments, they may be implemented on separate chips.</p>
<p id="p-0032" num="0031">The RF (radio frequency) circuitry <b>112</b> receives and sends electromagnetic waves. The RF circuitry <b>112</b> converts electrical signals to/from electromagnetic waves and communicates with communications networks and other communications devices via the electromagnetic waves. The RF circuitry <b>112</b> may include well-known circuitry for performing these functions, including but not limited to an antenna system, an RF transceiver, one or more amplifiers, a tuner, one or more oscillators, a digital signal processor, a CODEC chipset, a subscriber identity module (SIM) card, memory, and so forth. The RF circuitry <b>112</b> may communicate with the networks, such as the Internet, also referred to as the World Wide Web (WWW), an Intranet and/or a wireless network, such as a cellular telephone network, a wireless local area network (LAN) and/or a metropolitan area network (MAN), and other devices by wireless communication. The wireless communication may use any of a plurality of communications standards, protocols and technologies, including but not limited to Global System for Mobile Communications (GSM), Enhanced Data GSM Environment (EDGE), wideband code division multiple access (W-CDMA), code division multiple access (CDMA), time division multiple access (TDMA), Bluetooth, Wireless Fidelity (Wi-Fi) (e.g., IEEE 802.11a, IEEE 802.11b, IEEE 802.11g and/or IEEE 802.11n), voice over Internet Protocol (VoIP), Wi-MAX, a protocol for email, instant messaging, and/or Short Message Service (SMS)), or any other suitable communication protocol, including communication protocols not yet developed as of the filing date of this document.</p>
<p id="p-0033" num="0032">The audio circuitry <b>114</b>, the speaker <b>116</b>, and the microphone <b>118</b> provide an audio interface between a user and the device <b>100</b>. The audio circuitry <b>114</b> receives audio data from the peripherals interface <b>108</b>, converts the audio data to an electrical signal, and transmits the electrical signal to the speaker <b>116</b>. The speaker converts the electrical signal to human-audible sound waves. The audio circuitry <b>114</b> also receives electrical signals converted by the microphone <b>116</b> from sound waves. The audio circuitry <b>114</b> converts the electrical signal to audio data and transmits the audio data to the peripherals interface <b>108</b> for processing. Audio data may be may be retrieved from and/or transmitted to the memory <b>102</b> and/or the RF circuitry <b>112</b> by the peripherals interface <b>108</b>. In some embodiments, the audio circuitry <b>114</b> also includes a headset jack (not shown). The headset jack provides an interface between the audio circuitry <b>114</b> and removable audio input/output peripherals, such as output-only headphones or a headset with both output (headphone for one or both ears) and input (microphone).</p>
<p id="p-0034" num="0033">The I/O subsystem <b>120</b> provides the interface between input/output peripherals on the device <b>100</b>, such as the touch screen <b>126</b> and other input/control devices <b>128</b>, and the peripherals interface <b>108</b>. The I/O subsystem <b>120</b> includes a touch-screen controller <b>122</b> and one or more input controllers <b>124</b> for other input or control devices. The one or more input controllers <b>124</b> receive/send electrical signals from/to other input or control devices <b>128</b>. The other input/control devices <b>128</b> may include physical buttons (e.g., push buttons, rocker buttons, etc.), dials, slider switches, sticks, and so forth.</p>
<p id="p-0035" num="0034">The touch screen <b>126</b> provides both an output interface and an input interface between the device and a user. The touch-screen controller <b>122</b> receives/sends electrical signals from/to the touch screen <b>126</b>. The touch screen <b>126</b> displays visual output to the user. The visual output may include text, graphics, video, and any combination thereof. Some or all of the visual output may correspond to user-interface objects, further details of which are described below.</p>
<p id="p-0036" num="0035">The touch screen <b>126</b> also accepts input from the user based on haptic and/or tactile contact. The touch screen <b>126</b> forms a touch-sensitive surface that accepts user input. The touch screen <b>126</b> and the touch screen controller <b>122</b> (along with any associated modules and/or sets of instructions in the memory <b>102</b>) detects contact (and any movement or break of the contact) on the touch screen <b>126</b> and converts the detected contact into interaction with user-interface objects, such as one or more soft keys, that are displayed on the touch screen. In an exemplary embodiment, a point of contact between the touch screen <b>126</b> and the user corresponds to one or more digits of the user. The touch screen <b>126</b> may use LCD (liquid crystal display) technology, or LPD (light emitting polymer display) technology, although other display technologies may be used in other embodiments. The touch screen <b>126</b> and touch screen controller <b>122</b> may detect contact and any movement or break thereof using any of a plurality of touch sensitivity technologies, including but not limited to capacitive, resistive, infrared, and surface acoustic wave technologies, as well as other proximity sensor arrays or other elements for determining one or more points of contact with the touch screen <b>126</b>. The touch-sensitive display may be analogous to the multi-touch sensitive tablets described in the following U.S. Pat. Nos. 6,323,846 (Westerman et al.), U.S. Pat. No. 6,570,557 (Westerman et al.), and/or U.S. Pat. No. 6,677,932 (Westerman), and/or U.S. Patent Publication 2002/0015024A1, each of which is hereby incorporated by reference. However, the touch screen <b>126</b> displays visual output from the portable device, whereas touch sensitive tablets do not provide visual output. The touch screen <b>126</b> may have a resolution in excess of 100 dpi. In an exemplary embodiment, the touch screen <b>126</b> may have a resolution of approximately 168 dpi. The user may make contact with the touch screen <b>126</b> using any suitable object or appendage, such as a stylus, finger, and so forth.</p>
<p id="p-0037" num="0036">In some embodiments, in addition to the touch screen, the device <b>100</b> may include a touchpad (not shown) for activating or deactivating particular functions. In some embodiments, the touchpad is a touch-sensitive area of the device that, unlike the touch screen, does not display visual output. The touchpad may be a touch-sensitive surface that is separate from the touch screen <b>126</b> or an extension of the touch-sensitive surface formed by the touch screen <b>126</b>.</p>
<p id="p-0038" num="0037">The device <b>100</b> also includes a power system <b>130</b> for powering the various components. The power system <b>130</b> may include a power management system, one or more power sources (e.g., battery, alternating current (AC)), a recharging system, a power failure detection circuit, a power converter or inverter, a power status indicator (e.g., a light-emitting diode (LED)) and any other components associated with the generation, management and distribution of power in portable devices.</p>
<p id="p-0039" num="0038">In some embodiments, the software components include an operating system <b>132</b>, a communication module (or set of instructions) <b>134</b>, a contact/motion module (or set of instructions) <b>138</b>, a graphics module (or set of instructions) <b>140</b>, a user interface state module (or set of instructions) <b>144</b>, and one or more applications (or set of instructions) <b>146</b>.</p>
<p id="p-0040" num="0039">The operating system <b>132</b> (e.g., Darwin, RTXC, LINUX, UNIX, OS X, WINDOWS, or an embedded operating system such as VxWorks) includes various software components and/or drivers for controlling and managing general system tasks (e.g., memory management, storage device control, power management, etc.) and facilitates communication between various hardware and software components.</p>
<p id="p-0041" num="0040">The communication module <b>134</b> facilitates communication with other devices over one or more external ports <b>148</b> and also includes various software components for handling data received by the RF circuitry <b>112</b> and/or the external port <b>148</b>. The external port <b>148</b> (e.g., Universal Serial Bus (USB), FIREWIRE, etc.) is adapted for coupling directly to other devices or indirectly over a network (e.g., the Internet, wireless LAN, etc.).</p>
<p id="p-0042" num="0041">The contact/motion module <b>138</b> detects contact with the touch screen <b>126</b>, in conjunction with the touch-screen controller <b>122</b>. The contact/motion module <b>138</b> includes various software components for performing various operations related to detection of contact with the touch screen <b>122</b>, such as determining if contact has occurred, determining if there is movement of the contact and tracking the movement across the touch screen, and determining if the contact has been broken (i.e., if the contact has ceased). Determining movement of the point of contact may include determining speed (magnitude), velocity (magnitude and direction), and/or an acceleration (including magnitude and/or direction) of the point of contact. In some embodiments, the contact/motion module <b>126</b> and the touch screen controller <b>122</b> also detects contact on the touchpad.</p>
<p id="p-0043" num="0042">The graphics module <b>140</b> includes various known software components for rendering and displaying graphics on the touch screen <b>126</b>. Note that the term &#x201c;graphics&#x201d; includes any object that can be displayed to a user, including without limitation text, web pages, icons (such as user-interface objects including soft keys), digital images, videos, animations and the like.</p>
<p id="p-0044" num="0043">In some embodiments, the graphics module <b>140</b> includes an optical intensity module <b>142</b>. The optical intensity module <b>142</b> controls the optical intensity of graphical objects, such as user-interface objects, displayed on the touch screen <b>126</b>. Controlling the optical intensity may include increasing or decreasing the optical intensity of a graphical object. In some embodiments, the increase or decrease may follow predefined functions.</p>
<p id="p-0045" num="0044">The user interface state module <b>144</b> controls the user interface state of the device <b>100</b>. The user interface state module <b>144</b> may include a lock module <b>150</b> and an unlock module <b>152</b>. The lock module detects satisfaction of any of one or more conditions to transition the device <b>100</b> to a user-interface lock state and to transition the device <b>100</b> to the lock state. The unlock module detects satisfaction of any of one or more conditions to transition the device to a user-interface unlock state and to transition the device <b>100</b> to the unlock state. Further details regarding the user interface states are described below.</p>
<p id="p-0046" num="0045">The one or more applications <b>130</b> can include any applications installed on the device <b>100</b>, including without limitation, a browser, address book, contact list, email, instant messaging, word processing, keyboard emulation, widgets, JAVA-enabled applications, encryption, digital rights management, voice recognition, voice replication, location determination capability (such as that provided by the global positioning system (GPS)), a music player (which plays back recorded music stored in one or more files, such as MP3 or AAC files), etc.</p>
<p id="p-0047" num="0046">In some embodiments, the device <b>100</b> may include the functionality of an MP3 player, such as an iPod (trademark of Apple Computer, Inc.). The device <b>100</b> may, therefore, include a 36-pin connector that is compatible with the iPod. In some embodiments, the device <b>100</b> may include one or more optional optical sensors (not shown), such as CMOS or CCD image sensors, for use in imaging applications.</p>
<p id="p-0048" num="0047">In some embodiments, the device <b>100</b> is a device where operation of a predefined set of functions on the device is performed exclusively through the touch screen <b>126</b> and, if included on the device <b>100</b>, the touchpad. By using the touch screen and touchpad as the primary input/control device for operation of the device <b>100</b>, the number of physical input/control devices (such as push buttons, dials, and the like) on the device <b>100</b> may be reduced. In one embodiment, the device <b>100</b> includes the touch screen <b>126</b>, the touchpad, a push button for powering the device on/off and locking the device, a volume adjustment rocker button and a slider switch for toggling ringer profiles. The push button may be used to turn the power on/off on the device by depressing the button and holding the button in the depressed state for a predefined time interval, or may be used to lock the device by depressing the button and releasing the button before the predefined time interval has elapsed. In an alternative embodiment, the device <b>100</b> also may accept verbal input for activation or deactivation of some functions through the microphone <b>118</b>.</p>
<p id="p-0049" num="0048">The predefined set of functions that are performed exclusively through the touch screen and the touchpad include navigation between user interfaces. In some embodiments, the touchpad, when touched by the user, navigates the device <b>100</b> to a main, home, or root menu from any user interface that may be displayed on the device <b>100</b>. In such embodiments, the touchpad may be referred to as a &#x201c;menu button.&#x201d; In some other embodiments, the menu button may be a physical push button or other physical input/control device instead of a touchpad.</p>
<heading id="h-0007" level="1">User Interface States</heading>
<p id="p-0050" num="0049">The device <b>100</b> may have a plurality of user interface states. A user interface state is a state in which the device <b>100</b> responds in a predefined manner to user input. In some embodiments, the plurality of user interface states includes a user-interface lock state and a user-interface unlock state. In some embodiments, the plurality of user interface states includes states for a plurality of applications.</p>
<p id="p-0051" num="0050">In the user-interface lock state (hereinafter the &#x201c;lock state&#x201d;), the device <b>100</b> is powered on and operational but ignores most, if not all, user input. That is, the device <b>100</b> takes no action in response to user input and/or the device <b>100</b> is prevented from performing a predefined set of operations in response to the user input. The predefined set of operations may include navigation between user interfaces and activation or deactivation of a predefined set of functions. The lock state may be used to prevent unintentional or unauthorized use of the device <b>100</b> or activation or deactivation of functions on the device <b>100</b>. When the device <b>100</b> is in the lock state, the device <b>100</b> may be said to be locked. In some embodiments, the device <b>100</b> in the lock state may respond to a limited set of user inputs, including input that corresponds to an attempt to transition the device <b>100</b> to the user-interface unlock state or input that corresponds to powering the device <b>100</b> off. In other words, the locked device <b>100</b> responds to user input corresponding to attempts to transition the device <b>100</b> to the user-interface unlock state or powering the device <b>100</b> off, but does not respond to user input corresponding to attempts to navigate between user interfaces. It should be appreciated that even if the device <b>100</b> ignores a user input, the device <b>100</b> may still provide sensory feedback (such as visual, audio, or vibration feedback) to the user upon detection of the input to indicate that the input will be ignored.</p>
<p id="p-0052" num="0051">In embodiments where the device <b>100</b> includes the touch screen <b>126</b>, while the device <b>100</b> is locked, a predefined set of operations, such as navigation between user interfaces, is prevented from being performed in response to contact on the touch screen <b>126</b> when the device <b>100</b> is locked. In other words, when the contact is being ignored by the locked device <b>100</b>, the touch screen may be said to be locked. A locked device <b>100</b>, however, may still respond to a limited class of contact on the touch screen <b>126</b>. The limited class includes contact that is determined by the device <b>100</b> to correspond to an attempt to transition the device <b>100</b> to the user-interface unlock state.</p>
<p id="p-0053" num="0052">In the user-interface unlock state (hereinafter the &#x201c;unlock state&#x201d;), the device <b>100</b> is in its normal operating state, detecting and responding to user input corresponding to interaction with the user interface. A device <b>100</b> that is in the unlock state may be described as an unlocked device <b>100</b>. An unlocked device <b>100</b> detects and responds to user input for navigating between user interfaces, entry of data and activation or deactivation of functions. In embodiments where the device <b>100</b> includes the touch screen <b>126</b>, the unlocked device <b>100</b> detects and responds to contact corresponding to navigation between user interfaces, entry of data and activation or deactivation of functions through the touch screen <b>126</b>.</p>
<heading id="h-0008" level="1">Unlocking a Device via Gestures</heading>
<p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. 2</figref> is a flow diagram illustrating a process <b>200</b> for transitioning a device to a user-interface unlock state, according to some embodiments of the invention. As used herein, transitioning from one state to another refers to the process of going from one state to another. The process may be, as perceived by the user, instantaneous, near-instantaneous, gradual or at any suitable rate. The progression of the process may be controlled automatically by the device, such as the device <b>100</b> (<figref idref="DRAWINGS">FIG. 1</figref>), independent of the user, once the process is activated; or it may be controlled by the user. While the process flow <b>200</b> described below includes a number of operations that appear to occur in a specific order, it should be apparent that these processes may include more or fewer operations, which may be executed serially or in parallel (e.g., using parallel processors or a multi-threading environment).</p>
<p id="p-0055" num="0054">A device is set to the lock state (<b>202</b>). The device may be set (that is, transition completely to the lock state from any other state) to the locked state upon satisfaction of any of one or more lock conditions. The lock conditions may include events such as the elapsing of a predefined time of inactivity, entry into an active call, or powering on the device. The lock conditions may also include user intervention, namely the user locking the device by a predefined user input. In some embodiments, the user may be allowed to specify the events that serve as lock conditions. For example, the user may configure the device to transition to the lock state upon the elapsing of a predefined time of inactivity but not upon powering on the device.</p>
<p id="p-0056" num="0055">In some embodiments, the locked device displays on the touch screen one or more visual cues of an unlock action that the user may perform to unlock the device (<b>204</b>). The visual cue(s) provide hints or reminders of the unlock action to the user. The visual cues may be textual, graphical or any combination thereof. In some embodiments, the visual cues are displayed upon particular events occurring while the device is locked. The particular events that trigger display of the visual cues may include an incoming call, incoming message, or some other event that may require the user's attention. In some embodiments, the visual cues may also be displayed upon particular user inputs, such as the user interacting with the menu button, the user making contact with the locked touch screen and/or the user interacting with any other input/control device. The locked device, when not displaying the visual cues, may power down the touch screen (which helps to conserve power) or display other objects on the touch screen, such as a screen saver or information that may be of interest to the user (e.g., battery charge remaining, date and time, network strength, etc.).</p>
<p id="p-0057" num="0056">The unlock action includes contact with the touch screen. In some embodiments, the unlock action is a predefined gesture performed on the touch screen. As used herein, a gesture is a motion of the object/appendage making contact with the touch screen. For example, the predefined gesture may include a contact of the touch screen on the left edge (to initialize the gesture), a horizontal movement of the point of contact to the opposite edge while maintaining continuous contact with the touch screen, and a breaking of the contact at the opposite edge (to complete the gesture).</p>
<p id="p-0058" num="0057">While the touch screen is locked, the user may initiate contact with the touch screen, i.e., touch the touch screen (<b>206</b>). For convenience of explanation, contact on the touch screen in the process <b>200</b> and in other embodiments described below will be described as performed by the user using at least one hand using one or more fingers. However, it should be appreciated that the contact may be made using any suitable object or appendage, such as a stylus, finger, etc. The contact may include one or more taps on the touch screen, maintaining continuous contact with the touch screen, movement of the point of contact while maintaining continuous contact, a breaking of the contact, or any combination thereof.</p>
<p id="p-0059" num="0058">The device detects the contact on the touch screen (<b>208</b>). If the contact does not correspond to an attempt to perform the unlock action, or if the contact corresponds to a failed or aborted attempt by the user to perform the unlock action (<b>210</b>&#x2014;no), then the device remains locked (<b>212</b>). For example, if the unlock action is a horizontal movement of the point of contact across the touch screen while maintaining continuous contact with the touch screen, and the detected contact is a series of random taps on the touch screen, then the device will remain locked because the contact does not correspond to the unlock action.</p>
<p id="p-0060" num="0059">If the contact corresponds to a successful performance of the unlock action, i.e., the user performed the unlock action successfully (<b>210</b>&#x2014;yes), the device transitions to the unlock state (<b>214</b>). For example, if the unlock action is a horizontal movement of the point of contact across the touch screen while maintaining continuous contact with the touch screen, and the detected contact is the horizontal movement with the continuous contact, then the device transitions to the unlock state.</p>
<p id="p-0061" num="0060">In some embodiments, the device begins the process of transitioning to the unlock state upon detection of any contact on the touch screen and aborts the transition as soon as the device determines that the contact does not correspond to an unlock action or is a failed/aborted unlock action. For example, if the unlock action is a predefined gesture, the device may begin the process of transitioning to the unlock state as soon as it detects the initial contact of the gesture and continues the progression of the transition as the gesture is performed. If the user aborts the gesture before it is completed, the device aborts the transition and remains in the lock state. If the gesture is completed, the device completes the transition to the unlock state and becomes unlocked. As another example, if the unlock action is a horizontal movement of the point of contact across the touch screen while maintaining continuous contact with the touch screen, and the user taps the touch screen once, the device begins the process of the state transition as soon as it detects the tap but also aborts the process soon after because it realizes that the tap is just a tap and does not correspond to the unlock action.</p>
<p id="p-0062" num="0061">While the device is unlocked, the device may display on the touch screen user-interface objects corresponding to one or more functions of the device and/or information that may be of interest to the user. The user-interface objects are objects that make up the user interface of the device and may include, without limitation, text, images, icons, soft keys (or &#x201c;virtual buttons&#x201d;), pull-down menus, radio buttons, check boxes, selectable lists, and so forth. The displayed user-interface objects may include non-interactive objects that convey information or contribute to the look and feel of the user interface, interactive objects with which the user may interact, or any combination thereof. The user may interact with the user-interface objects by making contact with the touch screen at one or more touch screen locations corresponding to the interactive objects with which she wishes to interact. The device detects the contact and responds to the detected contact by performing the operation(s) corresponding to the interaction with the interactive object(s).</p>
<p id="p-0063" num="0062">While the device is locked, the user may still make contact on the touch screen. However, the locked device is prevented from performing a predefined set of actions in response to any detected contact until the device is unlocked. The prevented predefined set of action may include navigating between user interfaces and entry of data by the user.</p>
<p id="p-0064" num="0063">While the device is locked, the device may display one or more visual cues of the unlock action, as described above. In some embodiments, the device may also display, along with the visual cues, an unlock image. The unlock image is a graphical, interactive user-interface object with which the user interacts in order to unlock the device. In other words, the unlock action is performed with respect to the unlock image. In some embodiments, performing the unlock action with respect to the image includes dragging the unlock image in a predefined manner, which moves the unlock image across the touch screen. In some embodiments, if the unlock action is not completed, the GUI display can show reverse progress towards the locked state by gradually returning the unlock image to its position in the locked state</p>
<p id="p-0065" num="0064">In some embodiments, in addition to visual feedback, the electronic device supplies non-visual feedback to indicate progress towards completion of the unlock action. In some embodiments, in addition to visual feedback, the electronic device supplies non-visual feedback to indicate completion of the unlock action. The additional feedback may include audible feedback (e.g., sound(s)) or physical feedback (e.g., vibration(s)).</p>
<p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. 3</figref> is a flow diagram illustrating a process <b>300</b> for transitioning a device to a user-interface unlock state using an unlock image, according to some embodiments of the invention. The process <b>300</b> is similar to the process <b>200</b> (<figref idref="DRAWINGS">FIG. 2</figref>) with the addition of an unlock image that is displayed with the visual cues. The unlock action in the process <b>300</b> is performed with respect to the unlock image, i.e., the unlock action includes interaction with the unlock image. While the process flow <b>300</b> described below includes a number of operations that appear to occur in a specific order, it should be apparent that these processes can include more or fewer operations, which can be executed serially or in parallel (e.g., using parallel processors or a multi-threading environment).</p>
<p id="p-0067" num="0066">The device is locked upon satisfaction of a lock condition (<b>302</b>), similar to the operation <b>202</b> (<figref idref="DRAWINGS">FIG. 2</figref>). An unlock image and visual cues of the unlock action using the unlock image are displayed (<b>304</b>). The operation <b>304</b> is the same as the operation <b>204</b> (<figref idref="DRAWINGS">FIG. 2</figref>), except that in the operation <b>304</b> an unlock image is displayed in addition to the visual cues.</p>
<p id="p-0068" num="0067">As described above, the unlock action includes interaction with the unlock image. In some embodiments, the unlock action includes the user performing a predefined gesture with respect to the unlock image. In some embodiments, the gesture includes dragging the unlock image to a location on the touch screen that meets one or more predefined unlock criteria. In other words, the user makes contact with the touch screen at a location corresponding to the unlock image and then performs the predefined gesture while maintaining continuous contact with the touch screen, dragging the image to the location that meets the predefined unlock criteria. In some embodiments, the unlock action is completed by breaking the contact with the touch screen (thus releasing the unlock image) upon completion of the predefined gesture.</p>
<p id="p-0069" num="0068">A location meeting one or more predefined unlock criteria is simply a location on the touch screen that is predefined as a location to which the unlock image is to be dragged in order to unlock the device. The location(s) may be defined narrowly or broadly and may be one or more particular locations on the touch screen, one or more regions on the touch screen, or any combination thereof. For example, the location may be defined as a particular marked location, areas at each of the four corners of the touch screen, or a quadrant of the touch screen, etc.</p>
<p id="p-0070" num="0069">In some embodiments, the interaction includes dragging the unlock image to a predefined location on the touch screen. For example, the unlock action may include dragging the unlock image from one corner of the touch screen to another corner of the touch screen. As another example, the unlock action may include dragging the unlock image from one edge of the touch screen to the opposite edge. The emphasis here is on the final destination of the unlock image (and of the finger). Thus, the user can drag the unlock image from its initial location along any desired path. As long as the unlock image reaches the predefined location and is released at that location, the device is unlocked. It should be appreciated that the predefined location may be, as described above, defined narrowly or broadly and may be one or more particular locations on the touch screen, one or more regions on the touch screen, or any combination thereof.</p>
<p id="p-0071" num="0070">In some other embodiments, the unlock action includes dragging the unlock image along a predefined path. For example, the unlock action may include dragging the unlock image clockwise along the perimeter of the touch screen (the path being the perimeter of the touch screen), from one of the corners and back. As another example, the unlock action may include dragging the unlock image from one edge of the touch screen to the opposite edge in a linear path. The emphasis here is on the path along which the unlock image (and the finger) moves. Because of the emphasis on the path, the final location to which the unlock image is to be moved may be defined broadly. For example, the unlock action may be to drag the unlock image from its initial location, along the predefined path, to any spot within a predefined region on the touch screen. The predefined path may include one or more straight lines or lines with twists and turns.</p>
<p id="p-0072" num="0071">The user makes contact with the touch screen (<b>306</b>), similar to the operation <b>206</b> (<figref idref="DRAWINGS">FIG. 2</figref>). The device detects the contact with the touch screen (<b>308</b>), similar to the operation <b>208</b> (<figref idref="DRAWINGS">FIG. 2</figref>). If the contact does not correspond to successful performance of the unlock action with respect to the image (<b>310</b>&#x2014;no), the device remains locked. If the contact does correspond to successful performance of the unlock action with respect to the image (<b>310</b>&#x2014;yes), the device is unlocked (<b>314</b>).</p>
<p id="p-0073" num="0072"><figref idref="DRAWINGS">FIGS. 4A-4B</figref> illustrate the GUI display of a device in a user-interface lock state, according to some embodiments of the invention. In <figref idref="DRAWINGS">FIG. 4A</figref>, device <b>400</b> includes a touch screen <b>408</b> and a menu button <b>410</b>. The device <b>400</b> is locked and the touch screen <b>408</b> is displaying an unlock image <b>402</b> and visual cues. The visual cues shown include a channel <b>404</b> indicating the path of the gesture/movement along which the unlock image <b>402</b> is to be dragged, similar to a groove along which a slider switch moves; and one or more arrows <b>406</b> indicating the direction of the gesture/movement. The end of the channel <b>404</b> (in <figref idref="DRAWINGS">FIGS. 4A-4B</figref> and <b>5</b>A-<b>5</b>D, the &#x201c;end&#x201d; of the channel is the right end) also serves as a predefined location to which the unlock image <b>402</b> is to be dragged. The unlock image <b>402</b> may also include an arrow to further remind the user the direction of the gesture/movement. As described above, the visual cues and the unlock image may be displayed by the device <b>400</b> upon an event that may require the user's attention (e.g., incoming call or message) or upon user intervention (e.g., the user pressing the menu button <b>410</b> while the device is locked).</p>
<p id="p-0074" num="0073">In some embodiments, the arrows <b>406</b> and the arrow on the unlock image <b>402</b> may be animated. For example, the arrow on the unlock image <b>402</b> may appear and disappear in a pulse-like manner and the arrows <b>406</b> may emanate from one end of the channel <b>406</b> in sync with the pulsing of the arrow on the unlock image <b>402</b>. As shown in <figref idref="DRAWINGS">FIG. 4B</figref>, the arrow <b>406</b> may move along the channel <b>404</b> and disappear when it moves to the end of the channel <b>404</b>.</p>
<p id="p-0075" num="0074">The visual cues illustrated in <figref idref="DRAWINGS">FIGS. 4A and 4B</figref> remind the user that the unlock action is a predefined gesture that includes a horizontal movement of the finger (and thus moving the point of contact) along the channel <b>404</b>, from the beginning of the channel <b>404</b>, where the unlock image is initially located, to the end of the channel <b>404</b>. It should be appreciated, however, that the visual cues shown in <figref idref="DRAWINGS">FIGS. 4A-4B</figref> are merely exemplary and that more or fewer visual cues, or alternative visual cues may be used. The content of the visual cues may be based on the particulars of the unlock action.</p>
<p id="p-0076" num="0075"><figref idref="DRAWINGS">FIGS. 5A-5D</figref> illustrate the GUI display of a device at various points of the performance of an unlock action gesture, according to some embodiments of the invention. In <figref idref="DRAWINGS">FIG. 5A</figref>, the user, represented by the hand and finger <b>502</b> (not drawn to scale), begins the unlock action by touching the touch screen <b>408</b> of device <b>400</b> with her finger <b>502</b>. In some embodiments, the touch screen <b>408</b> is initially in sleep mode and/or dark, and the screen <b>408</b> displays the unlock image <b>402</b> when touched. The user touches the touch screen <b>408</b> at the location corresponding to the unlock image <b>402</b>, which is located initially at the left end of the channel <b>404</b>. The contact, either overlapping with the unlock image <b>402</b> or in proximity to the unlock image <b>402</b>, is detected by the device <b>400</b> and is determined to be an attempt to unlock the touch screen, based on the fact that the user <b>502</b> is interacting with the unlock image <b>402</b>.</p>
<p id="p-0077" num="0076">In <figref idref="DRAWINGS">FIG. 5B</figref>, the user is in the process of performing the gesture by moving her finger, which is in continuous contact with the touch screen <b>408</b>, in the direction of movement <b>504</b>. The unlock image <b>402</b> is dragged along the channel <b>404</b> as a result of the gesture. The channel <b>404</b> reminds the user that the unlock gesture is a horizontal motion. In some embodiments, the channel <b>404</b> indicates the predefined location (in <figref idref="DRAWINGS">FIGS. 5A-5D</figref>, the right end of the channel) to which the user drags the unlock image <b>402</b> to complete the unlock action and/or the predefined path along which the user drags the unlock image <b>402</b> to complete the unlock action.</p>
<p id="p-0078" num="0077">In <figref idref="DRAWINGS">FIG. 5C</figref>, the user has dragged the unlock image <b>402</b> to the right end of the channel <b>404</b>. Once the user releases the unlock image <b>402</b> at the right end of the channel <b>404</b>, the unlock action is complete. Upon completion of the unlock gesture, the device unlocks and displays on the touch screen <b>408</b> user-interface objects associated with normal operation of the device <b>400</b>. <figref idref="DRAWINGS">FIG. 5D</figref> illustrates an example of user-interface objects that may be displayed when the device <b>400</b> is unlocked. In <figref idref="DRAWINGS">FIG. 5D</figref>, the device <b>400</b> displays a menu <b>506</b>. The menu <b>506</b> includes interactive user-interface objects corresponding to various applications or operations. A user may interact with the user-interface objects to activate an application or perform an operation. It should be appreciated, however, that the device <b>400</b>, upon being unlocked, may display additional or alternative user-interface objects.</p>
<p id="p-0079" num="0078">In some embodiments, the unlock image <b>402</b> may also be used to indicate failure of performance of the unlock action. For example, if the user breaks the contact with the touch screen before the unlock image reaches the right end of the channel <b>404</b>, the unlock action has failed. The device <b>400</b> may display the unlock image <b>402</b> returning to its initial position on the left end of the channel <b>404</b>, allowing the user to attempt the unlock action again, if she so chooses. In some embodiments, the device goes back to sleep if no gesture is applied in a predetermined period of time.</p>
<p id="p-0080" num="0079">In some embodiments, the user may unlock the device <b>400</b> by contacting the touch screen <b>408</b> and moving the point of contact horizontally along a fraction of the channel <b>404</b>, i.e., the user need not move all the way to the right end of the channel. In some embodiments, the user may unlock the device <b>400</b> by making contact anywhere on the touch screen <b>408</b> and moving the point of contact horizontally as if he or she were following the channel <b>404</b>.</p>
<p id="p-0081" num="0080">In some embodiments, the lock/unlock feature may apply to specific applications that are executing on the device <b>400</b> as opposed to the device <b>400</b> as a whole. In some embodiments, an unlock gesture transitions from one application to another, for example, from a telephone application to a music player or vice versa. The lock/unlock feature may include a hold or pause feature. In some embodiments, as the user transitions from a first application and to a second application, a user interface for the second application may fade in (i.e., increase in intensity) and a user interface for the first application may fade out (i.e., decrease in intensity). The fade in and fade out may occur smoothly over a pre-determined time interval, such as 0.2 s, 1 s or 2 s. The pre-determined time interval may be in accordance with the unlock gesture, such as the time it takes the user to perform the gesture.</p>
<heading id="h-0009" level="1">Indication of Progress Towards Satisfaction of a User Input Condition</heading>
<p id="p-0082" num="0081"><figref idref="DRAWINGS">FIG. 6</figref> is a flow diagram illustrating a process <b>600</b> for indicating progress towards satisfaction of a user input condition according to some embodiments of the invention. While the process flow <b>600</b> described below includes a number of operations that appear to occur in a specific order, it should be apparent that these processes can include more or fewer operations, which can be executed serially or in parallel (e.g., using parallel processors or a multi-threading environment).</p>
<p id="p-0083" num="0082">While an electronic device is in a first user-interface state, progress is detected (<b>602</b>) towards satisfaction of a user input condition needed to transition to a second user-interface state. In some embodiments, the first user-interface state is for a first application and the second user-interface state is for a second application. In some embodiments, the first user-interface state is a lock state and the second user-interface state is an unlock state.</p>
<p id="p-0084" num="0083">While the device is in the first user-interface state, progress is indicated (<b>604</b>) towards satisfaction of the condition by transitioning an optical intensity of one or more user interface objects associated with the second user-interface state. The change in optical intensity of the user-interface objects provides a user with sensory feedback of the progress in transitioning between user interface states.</p>
<p id="p-0085" num="0084">In some embodiments, in addition to visual feedback, the device supplies non-visual feedback to indicate progress towards satisfaction of the user input condition. The additional feedback may include audible feedback (e.g., sound(s)) or physical feedback (e.g., vibration(s)).</p>
<p id="p-0086" num="0085">The device transitions (<b>606</b>) to the second user-interface state if the condition is satisfied. In some embodiments, in addition to visual feedback, the device supplies non-visual feedback to indicate satisfaction of the user input condition. The additional feedback may include audible feedback (e.g., sound(s)) or physical feedback (e.g., vibration(s)).</p>
<p id="p-0087" num="0086">The optical intensity of a user-interface object, as used herein, is the object's degree of visual materialization. The optical intensity may be measured along a scale between a predefined minimum and a predefined maximum. In some embodiments, the optical intensity may be measured along a logarithmic scale. In some embodiments, the optical intensity may be perceived by users as a transparency effect (or lack thereof) applied to the user-interface object. In some embodiments, the minimum optical intensity means that the object is not displayed at all (i.e., the object is not perceptible to the user), and the maximum optical intensity means that the object is displayed without any transparency effect (i.e., the object has completely materialized visually and is perceptible to the user). In some other embodiments, the optical intensity may be the visual differentiation between the user-interface object and the background, based on color, hue, color saturation, brightness, contrast, transparency, and any combination thereof.</p>
<p id="p-0088" num="0087">In some embodiments, the optical intensity of the user-interface objects to be displayed in the second user-interface state is increased smoothly. Smoothly may include a transition time that is greater than a pre-defined threshold, for example, 0.2 s, 1 s or 2 s. The rate of the transition of the optical intensity may be any predefined rate.</p>
<p id="p-0089" num="0088">In some embodiments, the indication of progress towards completion of the user input condition is a function of the user's satisfaction of the condition. For example, for a transition to an unlock state, the indication of progress towards completion is a function of the user's performance of an unlock action. For a linear function, the indication of progress is 10% complete when the unlock action is 10% complete; the indication of progress is 50% complete when the unlock action is 50% complete, and so forth, up to 100% completion of the unlock action, at which point the transition to the unlock state occurs. Correspondingly, for a linear function, the transition of the optical intensity from an initial value to a final value is 10% complete when the unlock action is 10% complete; the transition is 50% complete when the unlock action is 50% complete, and so forth, up to 100% completion of the unlock action, at which point the optical intensity is at its final value. In some embodiments, the user may perceive the optical intensity transition as a fading in of the user-interface objects as the unlock action is performed. It should be appreciated that the function need not be linear and alternative functions may be used, further details of which are described below, in relation to <figref idref="DRAWINGS">FIGS. 8A-8C</figref>.</p>
<p id="p-0090" num="0089">If the user input condition includes a predefined gesture then the indication of progress of the gesture may be defined in terms of how much of the gesture is completed and how much of the gesture is remaining. For example, if the gesture includes moving the finger from one edge of the screen to the opposite edge horizontally, then the indication of progress may be defined in terms of the distance between the two edges because the distance remaining objectively measures how much further the user has to move her finger to complete the gesture.</p>
<p id="p-0091" num="0090">If the user input condition includes dragging an image to a predefined location, then the indication of progress may be defined in terms of the distance between the initial location of the image and the predefined location to which the image is to be dragged in order to complete the input condition.</p>
<p id="p-0092" num="0091">If the user input condition includes dragging an image along a predefined path, then the indication of progress may be defined in terms of the length of the predefined path.</p>
<p id="p-0093" num="0092"><figref idref="DRAWINGS">FIGS. 7A-7D</figref> illustrate the GUI display of a device that is transitioning the optical intensity of user-interface objects concurrent with a transition from a first user interface state to a second user interface state, according to some embodiments of the invention. In <figref idref="DRAWINGS">FIG. 7A</figref>, the device <b>700</b> is locked and has received an incoming call. The device <b>700</b> is displaying a prompt <b>706</b> to the user, informing the user of the incoming call, on the touch screen <b>714</b>. The device is also displaying the unlock image <b>702</b> and channel <b>704</b> so that the user can unlock the device <b>700</b> in order to accept or decline the incoming call. The user begins the unlock action by making contact on the touch screen with her finger <b>710</b> on the unlock image <b>702</b>.</p>
<p id="p-0094" num="0093">In <figref idref="DRAWINGS">FIG. 7B</figref>, the user is in the process of dragging the unlock image <b>702</b> along the channel <b>704</b> in the direction of movement <b>712</b>. As the user drags the unlock image, a set of virtual buttons <b>708</b> appears and increases in optical intensity. The virtual buttons <b>708</b> are shown with dotted outlines to indicate that they are not yet at their final optical intensity levels. The virtual buttons <b>708</b> are associated with the prompt <b>706</b>; the virtual buttons shown in <figref idref="DRAWINGS">FIG. 7B-7D</figref> allow the user to decline or accept the incoming call. However, the user cannot interact with the virtual buttons <b>708</b> until the device is unlocked and the virtual buttons have reached their final optical intensity. In <figref idref="DRAWINGS">FIG. 7C</figref>, the user drags the unlock image <b>702</b> further along the channel <b>704</b> in the direction of movement <b>712</b>. The virtual buttons <b>708</b> have increased further in optical intensity relative to their optical intensity in <figref idref="DRAWINGS">FIG. 7B</figref>, as illustrated by their different style of dotted outlines. The increases in optical intensity indicate to the user progress towards completion of the unlock action.</p>
<p id="p-0095" num="0094">In <figref idref="DRAWINGS">FIG. 7D</figref>, the user completes the unlock action by dragging the unlock image to the right end of the channel <b>704</b> and releasing the unlock image <b>702</b>. The device <b>700</b> transitions to the unlock state. The unlock image <b>702</b> and the channel <b>704</b> disappear from the display and the virtual buttons <b>708</b> are at their final optical intensity levels, as illustrated by their solid outlines. At this point the user may interact with the virtual buttons <b>708</b> and accept or decline the incoming call.</p>
<p id="p-0096" num="0095">As described above in relation to <figref idref="DRAWINGS">FIGS. 5A-5D</figref>, if the unlock action fails because the user releases the unlock image prematurely, the unlock image may return to its initial location. In some embodiments, the optical intensity of the virtual buttons <b>708</b> or other user-interface objects that were increasing in optical intensity as the unlock action was performed may, concurrent with the return of the unlock image to its initial location, have their optical intensity decreased smoothly, back to their initial levels.</p>
<p id="p-0097" num="0096"><figref idref="DRAWINGS">FIGS. 8A-8C</figref> are graphs illustrating optical intensity as a function of the completion of the user input condition, according to some embodiments of the invention. In <figref idref="DRAWINGS">FIG. 8A</figref>, the optical intensity is a linear function of the completion of the user input condition. At 0% completion, the optical intensity is at an initial value (in this case, the initial value is 0). As the completion percentage increases, the optical intensity increases linearly with the completion percentage, until it reaches the final value at 100% completion.</p>
<p id="p-0098" num="0097">In <figref idref="DRAWINGS">FIG. 8B</figref>, the optical intensity is a nonlinear function of the completion of the user input condition. At 0% completion, the optical intensity is at an initial value (in this case, the initial value is 0). As the completion percentage increases, the optical intensity increases gradually at first, but the increase becomes steeper as the completion percentage increases, until it reaches the final value at 100% completion.</p>
<p id="p-0099" num="0098">In <figref idref="DRAWINGS">FIG. 8C</figref>, the optical intensity is another nonlinear function of the completion of the user input condition. At 0% completion, the optical intensity is at an initial value (in this case, the initial value is 0). As the completion percentage increases, the optical intensity increases steeply at first, but the increase becomes more gradual as the completion percentage increases, until it reaches the final value at 100% completion. In some embodiments, the optical intensity may increase according to a logarithmic scale.</p>
<p id="p-0100" num="0099">In some embodiments, the optical intensity may reach its final value prior to 100% completion of the user input condition (e.g., at 90% completion).</p>
<heading id="h-0010" level="1">User Interface Active States Corresponding to Events or Applications</heading>
<p id="p-0101" num="0100"><figref idref="DRAWINGS">FIG. 9</figref> is a flow diagram illustrating a process <b>900</b> for transitioning a device to a user interface active state corresponding to one of a plurality of unlock images, according to some embodiments of the invention. In some embodiments, the device may have one or more active applications running when the device becomes locked. Additionally, while locked, the device may continue to receive events, such as incoming calls, messages, voicemail notifications, and so forth. The device may display multiple unlock images on the touch screen, each unlock image corresponding to an active application or incoming event. Performing the unlock action using one of the multiple unlock images unlocks the device and displays the application and/or event corresponding to the unlock image. The user interface active state, as used herein, means that the device is unlocked and a corresponding application or event is displayed on the touch screen to the user. While the process flow <b>900</b> described below includes a number of operations that appear to occur in a specific order, it should be apparent that these processes can include more or fewer operations, which can be executed serially or in parallel (e.g., using parallel processors or a multi-threading environment).</p>
<p id="p-0102" num="0101">The device is locked upon satisfaction of a predefined lock condition (<b>902</b>). The device may have active applications running when it is locked and the active applications may continue running while the device is locked. Additionally, while the device is locked, the device may receive events, such as incoming calls, messages, and voicemail notifications.</p>
<p id="p-0103" num="0102">The device displays a plurality of unlock images, each displayed unlock image corresponding to an active application running or an event received while the device is locked (<b>904</b>). In some embodiments, the device also displays visual cues of the unlock action with respect to each unlock image. The device may display additional unlock images and visual cues as additional events are received. The user makes contact with the touch screen (<b>906</b>). The device detects the contact gesture (<b>908</b>). If the detected contact gesture does not correspond to successful performance of the unlock action with respect to any one of the displayed unlock images (e.g., because the contact is not an attempt to perform the unlock action or the unlock action failed/was aborted) (<b>910</b>&#x2014;no), the device remains locked (<b>912</b>). If the detected contact gesture does correspond to successful performance of the unlock action with respect to one of the displayed unlock images (<b>910</b>&#x2014;yes), the touch screen is unlocked and the running application or event corresponding to the one of the unlock images is displayed on the touch screen (<b>914</b>). In other words, the device transitions to a first active state corresponding to the first image if the detected contact corresponds to a predefined gesture with respect to the first image; the device transitions to a second active state distinct from the first active state and corresponding to the second image if the detected contact corresponds to a predefined gesture with respect to the second image; and so on.</p>
<p id="p-0104" num="0103">The device becomes unlocked and makes the corresponding event or application visible to the user, active, or running in the foreground, as opposed to running in the background, upon performance of the unlock action with respect to the particular unlock image. The user-interface active state includes the running application or incoming event corresponding to the particular unlock image with which the user interacted being displayed prominently on the touch screen, in addition to the device being unlocked. Thus, unlocking using a first unlock image (if multiple unlock images are displayed) transitions the device to a first user-interface active state, in which the device is unlocked and the application/event corresponding to the first unlock image is displayed prominently. Unlocking using a second image transitions the device to a second user-interface active state, in which the device is unlocked and the application/event corresponding to the second unlock image is displayed prominently.</p>
<p id="p-0105" num="0104">In some embodiments, the device may prioritize which unlock images to display. The device may display a subset of the corresponding unlock images on the touch screen at one time. The device may decide which subset to display based on one or more predefined criteria. For example, the device may display only unlock images corresponding to the most recent events and/or running applications. As another example, the device may display only unlock images corresponding to incoming events.</p>
<p id="p-0106" num="0105"><figref idref="DRAWINGS">FIG. 10</figref> illustrates the GUI of a device <b>1000</b> in a user-interface lock state that displays a plurality of unlock images, according to some embodiments of the invention. In <figref idref="DRAWINGS">FIG. 10</figref>, the touch screen <b>1014</b> of the device <b>1000</b> is locked. A first unlock image <b>1002</b> is displayed with corresponding visual cues, such as the first channel <b>1004</b> and arrow <b>1006</b>. A second unlock image <b>1008</b> is displayed with corresponding visual cues, such as the second channel <b>1010</b> and arrow <b>1012</b>. The touch screen <b>1014</b> may display additional unlock images and visual cues. The first unlock image <b>1002</b> corresponds to a first running application or received event. The second unlock image <b>1008</b> corresponds to a second running application or received event. The first and second unlock images and visual cues are similar to the unlock image and visual cues described above, in relation to <figref idref="DRAWINGS">FIGS. 4A and 4B</figref>. The arrows <b>1006</b> and <b>1012</b> may be animated to move from one end of the channels <b>1004</b> and/or <b>1010</b> to the other end, in order to indicate the proper direction of the predefined gesture or movement of the unlock image.</p>
<p id="p-0107" num="0106"><figref idref="DRAWINGS">FIGS. 11A-11F</figref> illustrate the GUI display of a device at various points in the performance of an unlock action gesture corresponding to one of a plurality of unlock images, according to some embodiments of the invention. In <figref idref="DRAWINGS">FIG. 11A</figref>, the user makes contact with the touch screen <b>1014</b> using her finger <b>1102</b> (not shown to scale), at the location corresponding to the second unlock image <b>1008</b>. The user performs the unlock action gesture by moving the point of contact, dragging the second unlock image <b>1008</b>. <figref idref="DRAWINGS">FIG. 11B</figref> shows a snapshot of the device <b>1000</b> during the pendency of the unlock action. The second unlock image <b>1008</b> is moved along in the channel <b>1010</b> in the direction of movement <b>1104</b>.</p>
<p id="p-0108" num="0107"><figref idref="DRAWINGS">FIG. 11C</figref> shows the second unlock image <b>1008</b> moved to the end of the channel <b>1010</b>, where the unlock action with respect to the second unlock image <b>1008</b> will be completed once the user breaks the contact (and releases the second unlock image <b>1008</b>). In some embodiments, the unlock action is completed when the unlock image <b>1008</b> is moved to the end of the channel <b>1010</b>, with or without the user breaking contact, and the second unlock image <b>1008</b> disappears. As shown in <figref idref="DRAWINGS">FIG. 11D</figref>, upon completion of the unlock action with respect to the second unlock image <b>1008</b>, the device displays on the touch screen the user-interface objects <b>1106</b> associated with the application or event corresponding to the second unlock image <b>1008</b>. In <figref idref="DRAWINGS">FIG. 11D</figref>, the event corresponding to the second unlock image is an incoming text message event and a prompt for the user to read it.</p>
<p id="p-0109" num="0108">The user, instead of performing the unlock action with respect to the second unlock image <b>1108</b>, may instead perform the unlock action gesture with respect to the first unlock image <b>1002</b>. In <figref idref="DRAWINGS">FIG. 11E</figref>, the user does so and performs the unlock action with respect to the first unlock image <b>1002</b> by dragging the first unlock image, in the direction <b>1104</b>, to the right end of the channel <b>1004</b>. Upon completion of the unlock action, the device <b>1000</b> displays the user-interface objects <b>1108</b> associated with the application or event corresponding to the first unlock image <b>1002</b>. In <figref idref="DRAWINGS">FIG. 11F</figref>, the application corresponding to the first unlock image is a music player application.</p>
<p id="p-0110" num="0109">In some embodiments, the transition to a user interface active state, as described in FIGS. <b>9</b> and <b>11</b>A-<b>11</b>E, may also include a concurrent transition in the optical intensity of user-interface objects, similar to that described above in relation to <figref idref="DRAWINGS">FIGS. 6</figref>, <b>7</b>A-<b>7</b>D, and <b>8</b>A-<b>8</b>C. Concurrent with the transition to a user interface active state, the user-interface objects associated with the application or event corresponding to the unlock image with which the user interacted to unlock the device increase in intensity. For example, the optical intensity of the user-interface objects <b>1106</b> associated with the text message prompt in <figref idref="DRAWINGS">FIG. 11D</figref> may be increased smoothly, as a function of the progress towards completion of the unlock action with respect to the second unlock image <b>1008</b>. As another example, the optical intensity of the user-interface objects <b>1108</b> associated with music player application in <figref idref="DRAWINGS">FIG. 11F</figref> may be increased smoothly, as a function of the progress towards completion of the unlock action with respect to the first unlock image <b>1002</b>.</p>
<p id="p-0111" num="0110">The foregoing description, for purpose of explanation, has been described with reference to specific embodiments. However, the illustrative discussions above are not intended to be exhaustive or to limit the invention to the precise forms disclosed. Many modifications and variations are possible in view of the above teachings. The embodiments were chosen and described in order to best explain the principles of the invention and its practical applications, to thereby enable others skilled in the art to best utilize the invention and various embodiments with various modifications as are suited to the particular use contemplated.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method, comprising:
<claim-text>at a device with a touch-sensitive display:</claim-text>
<claim-text>displaying a first unlock image on the touch-sensitive display while the device is in a first user-interface lock state, wherein the first unlock image corresponds to a first application on the device;</claim-text>
<claim-text>detecting a gesture on the touch-sensitive display with the first unlock image, the gesture moving the first unlock image from a first location to a second location by continuous contact with the touch-sensitive display;</claim-text>
<claim-text>determining that the detected gesture with the first unlock image satisfies a predefined condition; and</claim-text>
<claim-text>transitioning the device to a user-interface active state that displays user-interface objects for the first application in response to the detected gesture satisfying the predefined condition.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the second location is a predefined location on the touch-sensitive display.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the predefined condition includes the first unlock image moving from the first location along a predefined path on the touch-sensitive display.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first application is a music player application.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first application is a phone application.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first application is a messaging application.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first application is an email application.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, including displaying a second unlock image on the touch-sensitive display simultaneously with the first unlock image while the device is in the first user-interface lock state.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the second unlock image corresponds to an event.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the event is an incoming phone call, an incoming text message, an incoming electronic mail, or a voicemail notification.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. A computer readable memory including instructions that, when executed by a processor of a portable device with a touch-sensitive display, cause the device to perform operations comprising:
<claim-text>displaying a first unlock image on the touch-sensitive display while the device is in a first user-interface lock state, wherein the first unlock image corresponds to a first application on the device;</claim-text>
<claim-text>detecting a gesture on the touch-sensitive display with the first unlock image, the gesture moving the first unlock image from a first location to a second location by continuous contact with the touch-sensitive display;</claim-text>
<claim-text>determining that the detected gesture with the first unlock image satisfies a predefined condition; and</claim-text>
<claim-text>transitioning the device to a user-interface active state that displays user-interface objects for the first application in response to the detected gesture satisfying the predefined condition.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The computer readable memory of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the second location is a predefined location on the touch-sensitive display.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The computer readable memory of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the predefined condition includes the first unlock image moving from the first location along a predefined path on the touch-sensitive display.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The computer readable memory of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the first application is a music player application.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The computer readable memory of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the first application is a phone application.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The computer readable memory of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the first application is a messaging application.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The computer readable memory of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the first application is an email application.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The computer readable memory of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the operations include displaying a second unlock image on the touch-sensitive display simultaneously with the first unlock image while the device is in the first user-interface lock state.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The computer readable memory of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the second unlock image corresponds to an event.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The computer readable memory of <claim-ref idref="CLM-00019">claim 19</claim-ref> wherein the event is an incoming phone call, an incoming text message, an incoming electronic mail, or a voicemail notification.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. A portable device, comprising:
<claim-text>a touch-sensitive display;</claim-text>
<claim-text>at least one processor in communication with the touch-sensitive display; and</claim-text>
<claim-text>at least one memory in communication with the processor, the memory containing instructions that when executed by the processor cause the device to perform operations including:</claim-text>
<claim-text>displaying a first unlock image on the touch-sensitive display while the device is in a first user-interface lock state, wherein the first unlock image corresponds to a first application on the device;</claim-text>
<claim-text>detecting a gesture on the touch-sensitive display with the first unlock image, the gesture moving the first unlock image from a first location to a second location by continuous contact with the touch-sensitive display;</claim-text>
<claim-text>determining that the detected gesture with the first unlock image satisfies a predefined condition; and</claim-text>
<claim-text>transitioning the device to a user-interface active state that displays user-interface objects for the first application in response to the detected gesture satisfying the predefined condition.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The portable device of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the second location is a predefined location on the touch-sensitive display.</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The portable device of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the predefined condition includes the first unlock image moving from the first location along a predefined path on the touch-sensitive display.</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. The portable device of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the first application is a music player application.</claim-text>
</claim>
<claim id="CLM-00025" num="00025">
<claim-text>25. The portable device of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the first application is a phone application.</claim-text>
</claim>
<claim id="CLM-00026" num="00026">
<claim-text>26. The portable device of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the first application is a messaging application.</claim-text>
</claim>
<claim id="CLM-00027" num="00027">
<claim-text>27. The portable device of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the first application is an email application.</claim-text>
</claim>
<claim id="CLM-00028" num="00028">
<claim-text>28. The portable device of <claim-ref idref="CLM-00025">claim 25</claim-ref>, wherein the operations include displaying a second unlock image on the touch-sensitive display simultaneously with the first unlock image while the device is in the first user-interface lock state.</claim-text>
</claim>
<claim id="CLM-00029" num="00029">
<claim-text>29. The portable device of <claim-ref idref="CLM-00028">claim 28</claim-ref>, wherein the second unlock image corresponds to an event.</claim-text>
</claim>
<claim id="CLM-00030" num="00030">
<claim-text>30. The portable device of <claim-ref idref="CLM-00029">claim 29</claim-ref>, wherein the event is an incoming phone call, an incoming text message, an incoming electronic mail, or a voicemail notification. </claim-text>
</claim>
</claims>
</us-patent-grant>
