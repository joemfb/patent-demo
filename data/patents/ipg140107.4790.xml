<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625883-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625883</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13125179</doc-number>
<date>20091012</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="regional">
<country>EP</country>
<doc-number>08291021</doc-number>
<date>20081031</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>142</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382154</main-classification>
</classification-national>
<invention-title id="d2e71">Process of correcting an image provided on a support which is subsequently submitted to a deformation process</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>7298889</doc-number>
<kind>B2</kind>
<name>Massen</name>
<date>20071100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382154</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>7742633</doc-number>
<kind>B2</kind>
<name>Huang et al.</name>
<date>20100600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382154</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>2006/0104503</doc-number>
<kind>A1</kind>
<name>Huang et al.</name>
<date>20060500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>2007/0083383</doc-number>
<kind>A1</kind>
<name>Van Bael et al.</name>
<date>20070400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>EP</country>
<doc-number>0495285</doc-number>
<date>19920700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>FR</country>
<doc-number>2 870 028</doc-number>
<kind>A1</kind>
<date>20051100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>GB</country>
<doc-number>2227334</doc-number>
<date>19900700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>WO</country>
<doc-number>WO 2007/042237</doc-number>
<kind>A1</kind>
<date>20070400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00009">
<othercit>Danick Desjardins et al., &#x201c;Dense Stereo Range Sensing with Marching Pseudo-Random Patterns&#x201d;, Computer and Robot Vision, 2007, CRV '07, Fourth Canadian Conference NCE on, IEEE, PI, May 1, 2007, pp. 216-226.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00010">
<othercit>S. Y. Chen et al., &#x201c;Vision Processing for Realtime 3-D Data Acquisition Based on Coded Structured Light&#x201d;, IEEE Transactions on Image Processing, IEEE Service Center, Piscataway, NJ, US, vol. 17, No. 2, Feb. 1, 2008, pp. 167-176.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00011">
<othercit>Joaquim Salvi et al., &#x201c;Pattern codification strategies in structured light systems&#x201d;, Pattern Recognition, Elsevier, GB, vol. 37, No. 4, Apr. 1, 2004, pp. 827-849.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00012">
<othercit>Phillip N. Azariadis et al., &#x201c;On using planar developments to perform texture mapping on arbitrarily curved surfaces&#x201d;, Computers and Graphics, Elsevier, GB, vol. 24, No. 4, Aug. 1, 2000, pp. 539-554.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00013">
<othercit>European Search Report, Application No. 08291021, Sep. 30, 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00014">
<othercit>Office Action for Canadian Application No. 2,471,005 dated Apr. 12, 2013.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>20</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382154</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382180</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382209</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382215</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382216</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345419-420</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>9</number-of-drawing-sheets>
<number-of-figures>19</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20110299762</doc-number>
<kind>A1</kind>
<date>20111208</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Marre</last-name>
<first-name>Olivier</first-name>
<address>
<city>Le Molay Littry</city>
<country>FR</country>
</address>
</addressbook>
<residence>
<country>FR</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Le Pouliquen</last-name>
<first-name>Franck</first-name>
<address>
<city>Cherbourg</city>
<country>FR</country>
</address>
</addressbook>
<residence>
<country>FR</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Marre</last-name>
<first-name>Olivier</first-name>
<address>
<city>Le Molay Littry</city>
<country>FR</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Le Pouliquen</last-name>
<first-name>Franck</first-name>
<address>
<city>Cherbourg</city>
<country>FR</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Alston &#x26; Bird LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Quadrazis</orgname>
<role>03</role>
<address>
<city>Cherbourg Octeville</city>
<country>FR</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Mariam</last-name>
<first-name>Daniel</first-name>
<department>2665</department>
</primary-examiner>
</examiners>
<pct-or-regional-filing-data>
<document-id>
<country>WO</country>
<doc-number>PCT/EP2009/007321</doc-number>
<kind>00</kind>
<date>20091012</date>
</document-id>
<us-371c124-date>
<date>20110808</date>
</us-371c124-date>
</pct-or-regional-filing-data>
<pct-or-regional-publishing-data>
<document-id>
<country>WO</country>
<doc-number>WO2010/049059</doc-number>
<kind>A </kind>
<date>20100506</date>
</document-id>
</pct-or-regional-publishing-data>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">The invention relates to a method for adapting a visual representation which subsequently is subjected to a deformation, like in packaging. To be able to take into account the deformations on the visual representation the method comprises the steps of: providing a pattern on a support, wherein the pattern comprises a distribution of codes, which are arranged such that each code is unique, deforming the support with the pattern, taking at least two images of the deformed support under different points of view, and determining a 3D surface model based on the matching of at least one code of the pattern in the at least two images.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="158.16mm" wi="108.20mm" file="US08625883-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="190.84mm" wi="158.16mm" file="US08625883-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="180.76mm" wi="134.87mm" file="US08625883-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="204.39mm" wi="168.91mm" file="US08625883-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="183.47mm" wi="150.11mm" file="US08625883-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="217.68mm" wi="145.71mm" file="US08625883-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="226.48mm" wi="137.75mm" file="US08625883-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="243.33mm" wi="154.60mm" file="US08625883-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="149.10mm" wi="156.46mm" file="US08625883-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="228.09mm" wi="172.30mm" file="US08625883-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<p id="p-0002" num="0001">The invention relates to a method for adapting a visual representation which subsequently is subjected to a deformation.</p>
<p id="p-0003" num="0002">In the packaging and conditioning industries, the labelling or packaging processes have a step during which a support, e.g. a foil, with a visual representation is subjected to a deformation step. This situation typically occurs when a two dimensional label is mount on an irregular three dimensional subject, like a bottle, a can or other product containers. Some of these processes comprise a step of shrinking the label onto the object. Other processes deal with stretching of the labels. As usually a visual representation, comprising e.g. drawings, images, text, photographs etc. or a combination thereof, is part of the label, the shrinking and/or stretching leads to an undesired deformation and/or distortion effects on the label. These deformations and/or distorsions are highly unwanted. In fact, they negatively influence the aesthetic appearance of a product and the image of the company which fabricates the product and/or the trademark under which the product is put on the market. The same kind of situation also occurs in other fields, like for design products, publicity or marketing products, and actually whenever a visual representation is provided on an object which is subject to a deformation.</p>
<p id="p-0004" num="0003">Processes have been proposed to take into account these deformations by pre-deforming the visual representations before providing them on the support. It is the intention of these processes to pre-deform such that together with the shrink and/or stretch fabrication step, the desired visual effect is achieved. WO 2007/042237 proposes to create a deformation model based on a simulation of the shrinking process and to adapt the visual representation accordingly. As an alternative WO2007/042237 but also FR 2 870 028 propose to obtain a deformed model based on a measurement of a regular grid which is subjected to the deformation process. Using a 3D scanner tool the x, y, z coordinates of the deformed object are directly measured and used to determine the deformed grid, based on which the visual representation can be pre-deformed.</p>
<p id="p-0005" num="0004">Whereas the first process needs a profound understanding of the shrinking and/or stretching process, the second kind of process needs special equipment and as a consequence can only be provided by specialized service providers.</p>
<p id="p-0006" num="0005">It is therefore the object of the present invention to provide a method for adapting a visual representation which is subsequently subjected to a deformation which is simplified, such that a pre-deformation of the visual representation can be carried out without having to know technical details about the deformation process and using less sophisticated hardware. It is a further object of the invention to provide a data analysis method that facilitates the pre-deformation process.</p>
<p id="p-0007" num="0006">This first object is achieved with the method according to claim <b>1</b>. The inventive method for adapting a visual representation which is subsequently subjected to a deformation, comprises the steps of: providing a pattern on a support, wherein the pattern comprises a distribution of codes, which are arranged such that each code in the pattern is unique, deforming, in particular plastically, the support with the pattern; taking at least two images of the deformed support under different points of view, and determining a 3D model, in particular a 3D surface model based on the matching of at least one code in the pattern in the at least two images.</p>
<p id="p-0008" num="0007">According to the invention each code is unique on the pattern, also called texture map, so that the matching of codes between different images is direct. Thus, two images taken from two different angles of view are sufficient to obtain the necessary data using automatic image analysis to determine the 3D surface model which carries all the deformation information. Indeed, it is the provision of a pattern with a distribution of unique codes which provides these advantages. Namely, having identified one code in the image taken, one knows its reference position on the original pattern due to its uniqueness. As a consequence, the method can be applied anywhere, thus without sophisticated hardware like a 3D scanner, by using a camera, like a standard numeric CCD or CMOS camera, and a computer. In addition, no understanding of the deformation process as such is necessary neither, as the analysis is image based.</p>
<p id="p-0009" num="0008">In this context, unique means that each code is used only once in the pattern. Furthermore, deformation relates to a distortion of the shape so that lengths and/or angles in the original representation are not conserved. Deformation particularly relates to plastic deformations i.e. meaning that the deformation is not reversible, like by a shrinking or stretching process of foils.</p>
<p id="p-0010" num="0009">Preferably, the pattern can comprise a grid build up by polygons, in particular quadrilaterals, and wherein one code is associated with one polygon, in particular one quadrilateral. Polygons have the advantage that, as a first approximation, they can still be approximated by polygons even after a high deformation due to an industrial process. Thus thanks to the use of polygons, like quadrilaterals, especially when their size is adapted for automatic image analysis, the grid can be identified automatically by e.g. looking for the corners and/or the center of the polygon.</p>
<p id="p-0011" num="0010">Advantageously, one code can comprise at least one symbol, in particular a colour and/or a texture. The symbol can also be a graphical symbol, like a circle, a star, the plus, the minus sign or the multiplication sign. The term texture can relate to the screening of the image process. By combining various types of symbols the amount of different codes can be increased.</p>
<p id="p-0012" num="0011">Colours have the advantage that they can be preserved during the deformation and the acquisition process, even if the illumination during step c) varies. Graphical symbol can advantageously be used, if the number of different colours is limited, e.g. due to the industrial process or if the pattern is not big enough to cover the whole product. Also one can take advantage of the screening of the image leading to characteristic textures to identify the colour in an image analysing process. In a printing process, the screening of the image builds up a colour by means of a mixture of base colours (using C.M.Y.K these colours are e.g. Cyan, Magenta, Yellow and/or black). Thus one characteristic texture can be associated with each colour and can thus be used to identify the colour, in particular if the screening of the image provides one kind of texture per colour.</p>
<p id="p-0013" num="0012">Preferably, each code can comprise its symbol and the symbols of a predetermined number of neighbouring codes. By doing so the number of available unique variants for one code can be raised without having to provide more symbols, which could render the image analysis more demanding. In the case a polygon of the pattern is associated with one code and a colour is used as a symbol, this code is then composed of its colour and the colours of the neighbouring polygons. In case of a quadrilateral one can take the four next nearest neighbours or the eight connected quadrilaterals which are arranged around one central quadrilateral (thus the 4 nearest and 4 next nearest neighbours), or even the nearest 24 neighbours (corresponding to two circles around one central code). To build a code for one polygon P, the colours are preferably taken from connected polygons around polygon P in an ordered way, e.g. the clockwise order. Later on during image analysis the same ordered way is used to determine a code.</p>
<p id="p-0014" num="0013">Advantageously, each unique code can be invariant in rotation. This property guaranties that all the rotated representations of the quadrilaterals around one central code relate to the same code and only one of them is actually used in the pattern. Thus, visible codes can be extracted independently of the global camera orientation and position, and as a consequence the images in step c) can be taken from any positions.</p>
<p id="p-0015" num="0014">According to a preferred embodiment, the colour of a code is chosen out of a predetermined number of colours, in particular out of five colours. When taking into account a predetermined number of neighbouring codes, in particular it's eight nearest in the case of quadrilaterals, and with 5 different colours, 244140 ((5^9)/8) possible rotationally invariant codes exist and it is possible to create a pattern with 256*256 unique codes.</p>
<p id="p-0016" num="0015">Preferably, intersection lines between the polygons can have a colour different to the one of the codes. This facilitates the image analysis.</p>
<p id="p-0017" num="0016">According to a preferred embodiment, the pattern is a predetermined pattern and its size is adapted to the size of the support by scaling (that is enlarging or scaling down) and/or cutting the pattern or by adding graphical symbols with colours. Thus with such a pattern, it is possible to deal with any size and form of the support in so far as the codes remain sufficiently large to be detectable through automatic image analysis.</p>
<p id="p-0018" num="0017">Preferably, in case of repeating the pattern, at least one special code can be provided, in particular in the interfacing region between two repeated patterns, to enable an identification of the individual patterns after step b). The special codes can be of a different colour with respect to the standard codes, or of any other kind of symbol which can be identified during image analysis. Thus it becomes possible to cover large surfaces without having to create very large patterns by repeating the one predetermined pattern and identifying them via their special code.</p>
<p id="p-0019" num="0018">According to a preferred variant, the potential codes on the borders of the pattern (for border quadrilaterals) can be configured such that codes remain unique when at least two border regions abut against each other. More and more labels are used which are no longer attached to the object, but are just wrapped around the object wherein abutting or overlapping parts of the label are glued together and held in place by shrinking it onto the object. This use of labels facilitates recycling. Other types of labels, like sealing caps for wine bottles, also have overlapping parts. By keeping the code unique also in the overlapping or abutting regions, the risk of an erroneous interpretation is reduced.</p>
<p id="p-0020" num="0019">Preferably, the step of matching can further comprise determining a confidence value in an identified code in one image as a function of the number of its correctly identified neighbour codes. Starting from the predetermined distribution of codes, it therefore becomes possible to discriminate between regions in the image which are easier to identify than regions, e.g. with strong shrinking, in which artefacts can occur. In case one code has been identified several times in one image within one pattern, the process can be configured such that only the one with the highest confidence, thus with most of its neighbours correctly identified, is taken into account. This step can be carried out either manually or automatically.</p>
<p id="p-0021" num="0020">In this context, it is further preferred to only use codes for which in at least two images more than one, in particular all its neighbouring codes have been correctly identified to establish the 3D surface model base on matching. This improves the accuracy of the 3D surface model.</p>
<p id="p-0022" num="0021">According to a preferred embodiment, the 3D surface model building step can further comprise determining at least one passage matrix between at least two images, comprising a rotational and translational part corresponding to the difference between their two points of view. Thus the method can be carried out without computing the points of view, e.g. the global positions of a camera, with respect to each other.</p>
<p id="p-0023" num="0022">The second object of the invention is also achieved with the method according to claim <b>14</b>. The features of this claim can furthermore form an advantageous embodiment of the method according to claim <b>1</b>. Accordingly the method comprises a further step of determining a bijective or reverse relation (transformation) between a 2D space represented by the pattern before plastic deformation and a 3D space represented by the 3D surface model after plastic deformation. Indeed, the 3D surface model building step gives a relation between the 3D surface points and their 2D coordinates, in the pattern of codes, as all the vertices of the 3D surface model are linked with the 2D grid of the pattern of codes. As the determined 3D surface model has undergone the plastic deformation, the relations between that 2D pattern, e.g. the grid of polygons, and the 3D grid of vertices represent the deformation process.</p>
<p id="p-0024" num="0023">Preferably, the method can further comprise the steps of projecting a 2D initial grid, in particular a 2D regular grid, onto the 3D surface model and determining a 2D resultant grid in the 2D space based on the bijective relation. In fact, the resultant grid corresponds to the initial 2D grid deformed by the inverse of the original deformation in step b) of claim <b>1</b>. Thanks to the mapping between the initial and the resultant 2D grid, at least one pre-deformed label apply on the object can be compute through at least one particular point of view around the 3D surface model, by use of that mapping called grid mapping. If the starting grid is regular then the resultant grid shows how the labels could be pre-deformed, that grid is a pre-deformed grid. Whereas a regular grid is preferred as it facilitates analysis, any other kind of grid can also be used according to the invention.</p>
<p id="p-0025" num="0024">According to a variant, the method can further comprise a step of determining a projection of the 2D regular grid from one point of view or a combination of points of view on the 3D surface model, using a point of view different from those of the images of step c). The projection can be an orthographic or perspective projection and is again based on the bijective relation without that one has to stick to the images taken by the camera.</p>
<p id="p-0026" num="0025">Preferably, the method can further comprise a step of projecting one or several 2D regular grids from one or several point of views on a part or the complete surface of the 3D surface model. Once the 3D surface model established and for example output on a display, the user of the system can choose independently of the images taken, the point of view/s which is/are the most important for the product. Using the bijective relation it is then possible to determine the inverse deformation to obtain the 2D resultant grid for this kind of grid/s. By calculating the inverse deformation for different views, the predeformation of a label can be optimized, in particular to different points of view, e.g. the front and backside of a packaging.</p>
<p id="p-0027" num="0026">According to a preferred embodiment, the method can further comprise a step of using a 2D warping method for determining the necessary predeformation of the visual representation based on the 2D regular grid and its transformation into the 2D space via the 3D space of the 3D surface model. It is a particular advantage of this invention, that base on the two grids, the initial and the resultant grid, it is possible to use standard image treatment routines, e.g. embedded into standard graphic software packages, to determine the predeformation necessary to obtain an optimized final product.</p>
<p id="p-0028" num="0027">Advantageously, the method can further comprise a step of identifying surface regions of the 3D surface model whose normal vectors have an angular difference with the projection angle of view larger than an angular threshold, in particular 80&#xb0;. In the chosen point of view to get the pre-deformed grid, such regions have to be correctly treated to avoid that the pre-deforming is exaggerated. By identifying these regions, artefact can be taken into account, e.g. by manually adapting the pre-deformation during the distortion correction.</p>
<p id="p-0029" num="0028">According to a preferred embodiment, the method can further comprise a step of providing the pattern on the support on an object between step a) and b) and deforming the pattern on the support together with the object. Thus the deformation is carried out by the same process as for the final object (product), so that the analysis is precise.</p>
<p id="p-0030" num="0029">The invention furthermore relates to a computer program product, comprising one or more computer readable media having computer-executable instructions for performing the steps of: a) receiving image data of at least two images of a support with a pattern comprising a distribution of unique codes which was plastically deformed and wherein the images represent two different points of view, and b) determining a 3D surface model based on the matching of at least one code in the at least two images. Thus with this computer program product and an imaging device, e.g. a camera, all the advantageous effects as described above can be achieved. The computer program product can comprise further process steps like described above.</p>
<p id="p-0031" num="0030">Preferably, the computer program product can further comprise instructions for performing the additional method steps described above in relation to exploiting the 3D surface model, in particular in determining the bijective relation.</p>
<p id="p-0032" num="0031">The invention furthermore relates to a system comprising a data treatment unit configured to carry out the method according to one of the above described methods. With this system, all the advantages as described above can be achieved.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<p id="p-0033" num="0032">In the following embodiments, advantageous features of the invention will be described in detail in conjunction with the accompanying figures:</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIGS. 1</figref><i>a </i>and <b>1</b><i>b </i>illustrate the effect of the deformation of a visual representation and the advantageous effects of pre-deforming visual representation,</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 2</figref> is a flowchart illustrating a first embodiment of the method for adapting a visual representation which subsequently is subjected to a deformation,</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIGS. 3</figref><i>a </i>to <b>3</b><i>c </i>illustrate a pattern with a unique code used in the method illustrated in <figref idref="DRAWINGS">FIG. 2</figref>,</p>
<p id="p-0037" num="0036"><figref idref="DRAWINGS">FIGS. 4</figref><i>a </i>and <b>4</b><i>b </i>illustrate three alternative ways of providing a pattern with a unique distribution of codes on a support,</p>
<p id="p-0038" num="0037"><figref idref="DRAWINGS">FIGS. 5</figref><i>a </i>to <b>5</b><i>g </i>illustrate the steps to carry out a pre-deformation according to the invention,</p>
<p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. 6</figref> illustrates a second embodiment of the invention dealing with unwanted artefacts on the 3D surface model, and</p>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIGS. 7</figref><i>a </i>to <b>7</b><i>c </i>illustrate a third embodiment of the invention applied to overlapping visual representations.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. 1</figref><i>a </i>illustrates a visual representation <b>101</b>, here a label, and a three-dimensional object <b>103</b>, here a can. A visual representation can comprise a drawing and/or an image and/or text and/or a photograph etc. or a combination thereof. The object can be any object on which a visual representation is to be placed.</p>
<p id="p-0042" num="0041">In this embodiment, the visual representation <b>101</b> is provided on a support, e.g. printed on a shrink foil, which is fixed to the object <b>103</b> and is then shrunk onto the object <b>103</b>, typically using heat, to obtain the final product <b>105</b> with the label <b>107</b> shrunk onto the object <b>103</b>. Fixing is typically achieved by attaching using an adhesive or by wrapping around and gluing overlapping parts of the foil to each other, but any other suitable process deforming the visual representation can be applied. Instead of a shrink foil, any suitable other support like a stretch or shrink sleeve could be used as support or any forming process like thermoforming, embossing, stamping, blow moulding or vacuum-squeeze moulding could find its application. Here deformation relates to any process that changes the angular and/or length properties of the visual representation due, for example, to retraction or expansion of the surface of an object.</p>
<p id="p-0043" num="0042">As can be seen on the right hand side of <figref idref="DRAWINGS">FIG. 1</figref><i>a</i>, an unwanted deformation of the label <b>107</b> is observed due to the irregular form of object <b>103</b>. Here, the originally round logo <b>109</b> is now stretched out into an irregular approximately elliptic form.</p>
<p id="p-0044" num="0043">The deformations negatively influence the appearance of a product and can, furthermore, have an impact on the image of a company or the trademark under which the product is marketed.</p>
<p id="p-0045" num="0044">The problem of deformation can be overcome by pre-deforming the visual representation <b>101</b>, like illustrated on the left hand side of <figref idref="DRAWINGS">FIG. 1</figref><i>b</i>. In fact, the label <b>111</b> is deformed such that the special geometric properties of the object <b>103</b> are taken into account. In this case, this leads to a squeezing of the logo <b>113</b>, like shown on the left hand side of <figref idref="DRAWINGS">FIG. 1</figref><i>b</i>. When applying the pre-deformed label <b>111</b> on the object <b>103</b> and carrying out the shrinking step, the desired round shaped logo <b>115</b> is obtained on the final product.</p>
<p id="p-0046" num="0045">In the following, the first embodiment of the inventive method for adapting a visual representation which is subsequently subjected to a deformation will be explained in conjunction with <figref idref="DRAWINGS">FIGS. 2 and 3</figref>.</p>
<p id="p-0047" num="0046">This method comprises the following steps: providing a pattern with a distribution of unique codes on a support (Step <b>201</b>), placing that support with the pattern on an object (Step <b>203</b>), deforming the object using the same production process as for the final product (Step <b>205</b>), taking at least two images of the deformed object (Step <b>207</b>), through image analysis extracting codes out of the images (Step <b>209</b>), estimating a passage matrix between the points of view of the two images (Step <b>211</b>), getting a 3-D model of the deformed object from the extracted codes of those images (Step <b>213</b>), determining a bijective relation between the non deformed 2D space and the 3D space represented by the deformed object (step <b>215</b>), choosing a point of view which is a virtual projection view of that 3-D model (Step <b>217</b>), and calculating its corresponding inverse deformation (Step <b>219</b>) to have the pre-deformed data need to correct a label, like label <b>111</b> of <figref idref="DRAWINGS">FIG. 1B</figref>. Steps <b>209</b> to <b>219</b> can be carried out by a computer program product.</p>
<p id="p-0048" num="0047">Compared to prior art systems which either had to develop a numerical model of the deformation process itself, or used sophisticated hardware, like a 3-D scanner, to determine deformation properties like described above, the inventive method is based on the finding that using an intelligent pattern with a distribution of unique codes provides sufficient information. Starting from only two images taken, the deformed pattern on the object is obtained from two different angles of view and provides the necessary data to make a 3-D model from which, in turn, an inverse deformation can be determined which is then used to pre-deform the visual representation <b>101</b>.</p>
<p id="p-0049" num="0048">Compared to the prior art systems and methods, the inventive method is therefore simple to carry out and only needs a camera and computer. Furthermore such a system is mobile.</p>
<p id="p-0050" num="0049"><figref idref="DRAWINGS">FIGS. 3</figref><i>a </i>and <b>3</b><i>b </i>illustrate a portion of a pattern <b>301</b> according to the invention with a distribution of unique codes <b>303</b><sub>i</sub>, with i=1, 2, . . . , n, N, with N being the amount of codes in a given pattern (of which pattern <b>301</b> is only a part), and which finds its application in Step <b>201</b> (and step (a) of claim <b>1</b>). Here, the term &#x201c;unique&#x201d; means that within the pattern, one particular code only appears once.</p>
<p id="p-0051" num="0050">In this embodiment, five different colours are used as symbols for the codes <b>303</b><sub>i</sub>. The different colours are indicated by different hatchings. However, any other suitable kind of symbols, like graphical symbols or textures could also find their application instead of colours or in addition. Furthermore, the number of different colours is not limited to five and could be more or less, as a function of the amount of unique codes necessary for the pattern <b>301</b> and the computing power available. In <figref idref="DRAWINGS">FIG. 3</figref><i>a</i>, The same portion of pattern <b>301</b> is represented in hatchings on <figref idref="DRAWINGS">FIG. 3</figref><i>a </i>and in numbers on <figref idref="DRAWINGS">FIG. 3</figref><i>b </i>with one number representing one colour.</p>
<p id="p-0052" num="0051">The portion of the pattern <b>301</b> is built up of a grid of n polygons (here n=30, but n could of course be any number). In this embodiment, quadrilaterals are used as polygons. Instead, other kinds of polygons, like triangles, could also be used in the pattern <b>301</b>. Polygons have the advantage, that under deformation they can be approximated again by polygons. Thus a quadrilateral can be approximated by a quadrilateral and its corners or centre can be automatically determined by image analysis algorithms.</p>
<p id="p-0053" num="0052">In this embodiment, one code <b>303</b><sub>i </sub>is associated with one quadrilateral. The code <b>303</b><sub>i </sub>is further defined by its own colour and the colours of the connected neighbouring quadrilaterals. By using the neighbouring colours to define one code, it becomes possible to greatly enhance the amount of unique codes.</p>
<p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. 3</figref><i>b </i>illustrates this situation. The code of the centre quadrilateral <b>303</b><sub>7 </sub>is defined by its own colour, here represented by the number 3 and the colours of its connected quadrilaterals illustrated by the frame <b>305</b>. The connected quadrilaterals of code <b>303</b><sub>7 </sub>are thus its eight neighbour quadrilaterals <b>305</b>, adjacent to code <b>303</b><sub>7</sub>. Instead of using the eight adjacent quadrilaterals, one could also use the four nearest neighbours <b>303</b><sub>2</sub>, <b>303</b><sub>6</sub>, <b>303</b><sub>8 </sub>and <b>303</b><sub>12</sub>. According to a further variant one could also use the twenty four nearest neighbouring quadrilaterals.</p>
<p id="p-0055" num="0054">In this embodiment, the quadrilaterals of the codes <b>303</b><sub>i </sub>are separated by the intersection lines <b>307</b> of the grid. In this embodiment, the intersection lines <b>307</b> have a different colour compared to the ones used for the quadrilaterals of the codes <b>303</b><sub>i </sub>to facilitate the image analysis.</p>
<p id="p-0056" num="0055">In this variant, the five colours are chosen from the CYMK colour space, which is the one usually used in printing. To facilitate the image analysis, the chosen colours are the most uniformly distributed among the 360 degrees in the colour space (e.g. Lab.CIE or HSL can be used). Nevertheless, any other suitable colour space can be used to define the five colours.</p>
<p id="p-0057" num="0056">In this embodiment, the uniqueness of each code <b>303</b><sub>i </sub>also relates to rotationally invariants. <figref idref="DRAWINGS">FIG. 3</figref><i>c </i>illustrates the eight rotationally invariants of code <b>303</b><sub>8</sub>, in the entire pattern (of which pattern <b>301</b> is only a part), only one of the eight representations shown in <figref idref="DRAWINGS">FIG. 3</figref><i>c</i>, will be used. This has the advantage, that the image analysis is facilitated. Indeed, one does not have to worry about the exact positioning of the cameras used to take images in step <b>207</b>.</p>
<p id="p-0058" num="0057">Whereas in principle no limit is put on the amount of different colours, the use of six different colours, one colour for the intersection lines and the five others for quadrilaterals is sufficient to build a pattern with a larger number of unique codes. With a code <b>303</b><sub>i </sub>being defined by five colours and nine quadrilaterals (see <b>307</b>), 244.140 (5^9/8) different codes exist and for instance a pattern can be built with 250*250 codes. However, more or fewer colours and more or less neighbours (4 or even 24) can be used to make the codes if a larger or smaller pattern is necessary for the desired application.</p>
<p id="p-0059" num="0058">To build up a pattern <b>301</b> according to the invention, one starts with an empty grid of quadrilaterals. That grid then has to be filled with colours of the model. The first code <b>303</b><sub>7 </sub>is established by randomly choosing a colour for the centre quadrilateral <b>303</b><sub>7 </sub>and the eight nearest neighbours <b>307</b>. For the next code <b>303</b><sub>8</sub>, three new colours need to be found for the three quadrilaterals of frame <b>309</b>, such that the new code and any one of its rotated versions (see <figref idref="DRAWINGS">FIG. 3</figref><i>c</i>) does not yet exist in the pattern. This step is then repeated with the next neighbour quadrilaterals and so on. The information of each code <b>303</b><sub>i </sub>already in the pattern is stored and used to judge, whether a code or any one of its rotationally invariants already exists in the pattern.</p>
<p id="p-0060" num="0059">Once the pattern <b>301</b> with a distribution of unique codes <b>303</b><sub>i </sub>is established, it can be used in any application of the inventive method. Thereto, the pattern <b>301</b> is provided, for example by printing, on a support <b>401</b> out of the same material and with the same dimensions as the one on which the visual representation (for example label <b>101</b>) is printed (Step <b>203</b>). The pattern <b>301</b> covers the surface <b>401</b> of the support, like illustrated in <figref idref="DRAWINGS">FIG. 4</figref><i>a</i>. If the size of the pattern <b>301</b> is larger than the surface of the support <b>401</b>, one can only use a part of the pattern.</p>
<p id="p-0061" num="0060">If a larger pattern <b>301</b> is necessary, either the number of different colours can be increased or, according to a variant other symbols, like geometric ones (+, &#x2212;, o, x, *) can be added to each coloured quadrilateral. Then the number of possible codes allows to build large pattern for instance with 1000*1000 codes or more. Such a pattern can cover bigger supports for a given product or it can increase the precision of the 3D surface model. It is also possible to scale the pattern <b>301</b>. With a scaling up or down of the pattern it is also possible to adapt the precision of the process. In fact, the precision depends on the fact that after deformation (shrinking or extension) of the pattern, codes can still be identified during image analysis. Thus, depending on the resolution of the camera used, one can choose the size of the grid. On the other side, also the printing process of providing the pattern on the support might, however, be limiting with respect to the resolution.</p>
<p id="p-0062" num="0061">In case the pattern is not large enough, it could also be duplicated like illustrated in <figref idref="DRAWINGS">FIG. 4</figref><i>b</i>. In this case, it becomes necessary to be able to clearly identify the position of each one of the duplicated regions (I-III). This can, for example, be done by providing at least one special code <b>403</b>, <b>405</b>, <b>407</b> per region which is not present in the other regions. Upon identification of that at least one special code, the algorithms used to analyse the images can identify the region.</p>
<p id="p-0063" num="0062">The next step <b>205</b> of the process consists in deforming the support such that the support fits on the object like illustrated in <figref idref="DRAWINGS">FIGS. 1</figref><i>a </i>and <b>1</b><i>b </i>or deforming the support together with the object <b>103</b>. Here the same process as for the final product is applied. During heat shrinking, like described above, a plastic deformation of the support and the pattern is achieved. Nevertheless, the inventive method is not limited to heat shrinking and any deforming processes during which length and/or angular dimensions change, falls under the invention.</p>
<p id="p-0064" num="0063">The next step <b>207</b> then consists in taking at least two images of the deformed surface. To do so a camera, in particular a numeric camera of CCD or CMOS type, is used. Like already mentioned above, at least two images have to be taken to be able to establish a 3D surface model. With more images the precision of the model can be improved or a larger covering surface of the object <b>103</b> can be obtained. Apart from the number of images, the only additional requirement concerning the images is that there is sufficient light to take the images and that they are in focus so that the codes <b>303</b><sub>i </sub>can be extracted. Thus, there is no need for a special illumination of the object or a special background scene in which the object is placed. There is also no need for a special calibration. The calibration process only serves to estimate the internal camera parameters. The calibration is carried out once per camera and those internal parameters are then used in the system. The calibration method is based on different images of a special and known target, for instance the pattern <b>301</b> put on a plane.</p>
<p id="p-0065" num="0064">In the next step <b>209</b>, the images taken are numerically analysed using state of the art algorithms (decomposition RGB in lab-CIE or HSL, contour extraction). During image analysis, a quadrilateral of the pattern <b>301</b> is determined by analysing the positions of its corners and/or its centre and its main colour. To analyse the colour it is also possible to take advantage of the screening of the image, which provides a typical texture due to the printing process. Once for a given code, e.g. <b>303</b><sub>7</sub>, the nine colours (see frame <b>305</b>) are extracted, the code can be identified and via its neighbours a confidence value is computed.</p>
<p id="p-0066" num="0065">To limit the errors in code extraction, an additional verification step can be added according to a variant of the first embodiment. In case one code is extracted several times in the same image, the method is carried out such that only the one with the highest confidence value is kept.</p>
<p id="p-0067" num="0066">During the verification step, two confidence values are computed for each code <b>303</b><sub>i</sub>, one is the building confidence value link to the code building process and the other, a neighbouring confidence value relates to the probability that an identified code is correctly identified with respect to its neighbouring codes.</p>
<p id="p-0068" num="0067">The correctness of the code extraction is not completely guaranteed, as errors can occur during the determination of the symbols. Indeed, if for one quadrilateral, one colour of one of its neighbouring quadrilaterals is not correctly identified, then the identified code is also wrong. Nevertheless, that bad code could be found in the pattern <b>301</b> at another position and would become validated. A verification step of a code with respect to the neighbouring codes gives the possibility to eliminate a code which has been determined wrongly.</p>
<p id="p-0069" num="0068">As already mentioned above, a code is linked with colours of its neighbouring quadrilaterals and also with its neighbouring codes. If for one code it was possible to identify all colours of its neighbouring quadrilaterals, this code has a higher building confidence than a code for which only some colours of the neighbouring quadrilaterals have been identified. Nevertheless, if one or more neighbouring quadrilaterals are missing, a code can still be built thanks to the identification of its neighbouring codes but its building confidence is lower.</p>
<p id="p-0070" num="0069">For example, one can give a building confidence value &#x201c;k&#x201d; of 1 for a code for which not all neighbouring quadrilaterals have been identified, but for which an identification became possible due to correctly identified neighbouring codes, for example more than three neighbouring codes. A building confidence value of 2 can be accorded for a code which has all its neighbouring quadrilaterals, whereas a building confidence value of 0 is provided in case that e.g. less than three nearest neighbouring codes have been identified.</p>
<p id="p-0071" num="0070">A neighbouring confidence in one code then relates to the sum of the building confidence values k<sub>i </sub>of its neighbouring codes. For k<sub>i </sub>neighbouring possible codes that confidence is</p>
<p id="p-0072" num="0071">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mrow>
  <munderover>
    <mo>&#x2211;</mo>
    <mrow>
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
    </mrow>
    <mi>k</mi>
  </munderover>
  <mo>&#x2062;</mo>
  <mrow>
    <mrow>
      <mi>conf</mi>
      <mo>&#x2061;</mo>
      <mrow>
        <mo>(</mo>
        <msub>
          <mi>k</mi>
          <mi>i</mi>
        </msub>
        <mo>)</mo>
      </mrow>
    </mrow>
    <mo>.</mo>
  </mrow>
</mrow>
</math>
</maths>
<br/>
Thus in the embodiment of <figref idref="DRAWINGS">FIG. 3</figref>, when summing up the building confidence values for all eight neighbouring codes, one can thus attribute a neighbouring confidence value for a given code varying from 0 to 16.
</p>
<p id="p-0073" num="0072">Following the numerical extraction of the codes in each image, the data is used to establish a 3-D surface model based on 3D triangulation. To be able to use a 3D triangulation of the codes using the at least two images, first of all the camera position associated with each image needs to be known with respect to the object. This is done during step <b>211</b>.</p>
<p id="p-0074" num="0073">To keep the method simple to carry out, it is preferred to use the images themselves to determine the camera positions with respect to each other and with respect to the object. According to an alternative, the exact camera positions could, however, also be determined using a calibrated positioning tool with which the camera positions, as well as the object position, can be determined with respect to each other beforehand.</p>
<p id="p-0075" num="0074">One image position can be defined by a rotational matrix and a translational vector which actually puts the camera realising that image in a 3-D environment. A passage matrix, comprising a rotation and a translation, then allows positioning a second image with respect to the first one.</p>
<p id="p-0076" num="0075">According to the invention, the passage matrix between two images is obtained by matching corresponding codes in the two images. This can be done based on the distribution of the unique codes <b>303</b><sub>i </sub>in the pattern <b>301</b>. The passage matrix is a 3&#xd7;3 matrix and has eight parameters which need to be established. Thus, having identified at least eight corresponding codes in the two images, it becomes possible to determine the passage matrix.</p>
<p id="p-0077" num="0076">In case more than two images have been taken, it is enough to identify at least one passage matrix between each image and at least one of the others, it is not mandatory to establish all the passage matrices between all possible combinations of images. Each matched code between two images provides an equation which can then be exploited to establish the passage matrix between those two images.</p>
<p id="p-0078" num="0077">As the estimation of the passage matrix is important to obtain a precise 3-D model of the deformed object, preferably only codes with a maximum confidence value, e.g. confidence value of 16 in the above described example, are used in this step.</p>
<p id="p-0079" num="0078">Having matched the necessary number of codes between the at least two images and having determined the passage matrix between the different points of view, the next step <b>213</b> consists in determining the 3-D surface model. To do so, a triangulation process is carried out. The triangulation of one code provides the coordinates in the 3D space (x, y, z) and is based on the position of the code in the two images for which the positions relative to each other have been previously defined with the passage matrix. This technique comprises determining for each one of the points in 2-D (corresponding to the centre of the quadrilateral corresponding to the central point of a symbol of a code <b>303</b><sub>i</sub>), a 3-D line which passes via this point in 3-D and the centre of the camera.</p>
<p id="p-0080" num="0079">The positioning of the corresponding point in 3-D is then obtained by intersecting the two lines. In case no intersection is achieved, the method is configured to take the point in 3-D which is the closest to both lines. To improve the resolution, it is of course advantageous to use more than two images so that more than two lines are determined from which the 3D position can be obtained.</p>
<p id="p-0081" num="0080">Due to the fact that the codes <b>303</b><sub>i </sub>in the pattern <b>301</b> are connected to each other in a unique way, it is also possible to estimate the position of codes which have not been identified in the images or which have been rejected due to a low confidence value. If, for example, one code is missing but it eight nearest neighbouring codes have been identified, the mean value of the position of the eight corresponding 3-D coordinates provides a position in 3D for the lacking one. If only some of the neighbouring codes were identified, it is still possible to carry out its estimation. For example one can take the average position of four codes, arranged in a cross like manner with respect to the lacking code.</p>
<p id="p-0082" num="0081">Once the 3D surface model of the deformed pattern <b>301</b> established, the next step <b>215</b> consist in determining a bijective relation between a 2D space represented by the pattern <b>301</b> before plastic deformation and a 3D space represented by the 3D surface model after plastic deformation. The bijective relation corresponds to a mathematical description that allows to pass from the deformed 3D space into the 2D space and vice versa.</p>
<p id="p-0083" num="0082">The next process step <b>217</b> consists in choosing a point of view of the 3-D model corresponding to certain point of view with respect to the object <b>103</b> and based on which an inverse deformation can then be determined. The inverse deformation uses the bijective relation and provides the information necessary to pre-deform the visual representation. According to the invention, the inverse deformation is calculated from a point of view chosen by the user, and which does not have to correspond to one of the points of views of the images taken of the deformed object.</p>
<p id="p-0084" num="0083">In packaging of a product, some points of views of a product, in particular the ones showing the name of the product and/or the trademark of the manufacturer are privileged. Any aesthetic deficiency in this particular point of view can harm. Other views are less important. Therefore, the inverse deformation shall be calculated starting from such a privileged position which is also called the facing position.</p>
<p id="p-0085" num="0084">Once the privileged position chosen, a 2D grid, e.g. a 2D regular grid, is projected onto the 3D surface model and a 2D resultant grid is calculated in the 2D space based on the bijective relation established during step <b>215</b>. This situation is illustrated in <figref idref="DRAWINGS">FIG. 5</figref><i>a</i>. It illustrates the chosen privileged view of the deformed object <b>103</b> (see <figref idref="DRAWINGS">FIG. 1</figref><i>a</i>) and a grid of 20*20 columns and lines of the 2D regular initial grid <b>501</b>. The view illustrated in <figref idref="DRAWINGS">FIG. 5</figref><i>a </i>is a projection of the 3D surface model onto the plane of the chosen angle of view.</p>
<p id="p-0086" num="0085">Here, the size of the grid corresponds only to a possible example. Depending on the size of the label <b>109</b> and the desired resolution more or less column and lines can be taken. Using the bijective relation the projected initial grid is then back transferred into the 2D space, the result of which is illustrated in <figref idref="DRAWINGS">FIG. 5</figref><i>b</i>. In the 2D space the regular 2D grid <b>501</b> is now deformed and forms the 2D resultant grid <b>505</b>. One now has for every crossing point of grid <b>501</b> a corresponding position in the 2D resultant grid <b>505</b>, see e.g. crossing point <b>507</b> in <figref idref="DRAWINGS">FIGS. 5</figref><i>a </i>and <b>5</b><i>b. </i></p>
<p id="p-0087" num="0086">Next, the inverse deformation (step <b>219</b>) is computed using the positions of the codes in the projected direction showing the deformation information and the known position of the codes in the original pattern. The process underlying this calculus is a standard warping process. This is possible as actually the properties of the initial regular grid and the resultant grid match the requirements of 2D warping. To obtain the pre-deformed label <b>113</b> like shown in <figref idref="DRAWINGS">FIG. 1B</figref>, one then has to combine the original label <b>1</b> with the data from the inverse deformation calculation (Step <b>219</b>). The results of this step are illustrated in <figref idref="DRAWINGS">FIGS. 5</figref><i>c </i>and <b>5</b><i>d. </i></p>
<p id="p-0088" num="0087"><figref idref="DRAWINGS">FIG. 5</figref><i>c </i>illustrates on the left side the shape of the undeformed logo <b>509</b> (see also <b>109</b> in <figref idref="DRAWINGS">FIG. 1</figref><i>a</i>). and the result of its warping into the resultant grid <b>505</b> leads to the pre-deformed logo <b>511</b> (see also <b>113</b> in <figref idref="DRAWINGS">FIG. 1</figref><i>b</i>).</p>
<p id="p-0089" num="0088"><figref idref="DRAWINGS">FIG. 5</figref><i>d </i>then illustrates the effect of the deformation process on the pre-deformed logo <b>511</b> and which shows the desired round shape even after the deformation process.</p>
<p id="p-0090" num="0089">According to a variant, it is also possible to use several 2D regular grids from several points of views on a part or the complete surface of the 3D surface model. In fact, the several points of views represent a kind of trajectory on the object <b>103</b>. In this case one can for example also take into the backside of a product.</p>
<p id="p-0091" num="0090">In this variant, a initial grid of a predetermined size and with a predetermined amount of columns and lines is projected onto the 3D surface model for each point of view on the trajectory. Then using the same algorithms as already described above with respect to step <b>219</b>, the predeformation is determined base on the bijective relation established during step <b>215</b>. It only has to be ensured that no overlapping occurs between the several initial grid projections.</p>
<p id="p-0092" num="0091">In one special embodiment of this variant, one does not use a 2D grid but for each point on the trajectory only a line with a predetermined amount of reference points is projected onto the 3D surface model. This situation is illustrated in <figref idref="DRAWINGS">FIGS. 5</figref><i>e </i>to <b>5</b><i>g. </i></p>
<p id="p-0093" num="0092"><figref idref="DRAWINGS">FIG. 5</figref><i>e </i>illustrates a part of a can <b>513</b> and a series of lines <b>515</b>, j=1, 2, . . . , m, . . . , M with reference points <b>517</b><sub>k</sub>, k=1, 2, . . . , K.</p>
<p id="p-0094" num="0093"><figref idref="DRAWINGS">FIG. 5</figref><i>f </i>illustrates the result of the warping step and shows the effect of the pre-deformation on the logo <b>519</b> and <b>521</b> (see also logo <b>109</b> in <figref idref="DRAWINGS">FIG. 1</figref><i>a</i>).</p>
<p id="p-0095" num="0094">Finally, <figref idref="DRAWINGS">FIG. 5</figref><i>g </i>shows the result after applying the visual representation of <figref idref="DRAWINGS">FIG. 5</figref><i>f </i>on the object <b>513</b> after deformation. Again, the desired round shape of the logo <b>523</b> and <b>525</b> can be observed.</p>
<p id="p-0096" num="0095">According to a second embodiment, an additional data treatment step is carried out between steps <b>215</b> and <b>217</b> to take into account artefacts which can occur when a surface region in the 3-D model has an angular difference between its surface normal vector and the surface normal vector of the projection image plane which is larger than a predetermined angle, in particular larger than 80&#xb0;.</p>
<p id="p-0097" num="0096">This situation is illustrated in <figref idref="DRAWINGS">FIG. 6</figref>, illustrating an object <b>601</b> of which two images have been taken from two different points of view <b>603</b> and <b>605</b> and wherein the protection angle of view <b>607</b> is positioned such that the surface normal <b>609</b> of the projection image <b>607</b> forms a large angle, nearly 90&#xb0; with the surface normal of region <b>611</b>.</p>
<p id="p-0098" num="0097">In the projection <b>607</b>, the neighbouring codes in the surface region <b>611</b> are very close to each other. As a consequence, the warping transformation will lead to a large pre-deformation in the corresponding label <b>111</b>. This would not cause a problem in the case object <b>601</b> would only be looked at from the angle of view <b>607</b>. However, in reality an object is looked at from an angular range and not form only one view. Thus under points of view other than the one corresponding to <b>607</b> would show an exaggerated deformation.</p>
<p id="p-0099" num="0098">To limit the impact of strong deformations in region <b>611</b> on the final pre-deformation process, the second embodiment comprises an additional step of identifying such areas (either manually or automatically) and correcting the pre-deformation grid by taking the pre-deformation values of a point corresponding to the angular threshold value, e.g. 80&#xb0;.</p>
<p id="p-0100" num="0099"><figref idref="DRAWINGS">FIG. 7</figref><i>a </i>illustrates a third embodiment of the inventive method which can be combined with the first two embodiments independently or in combination. The third embodiment deals with artefacts which can occur when the visual representation on the support in one region overlaps or abuts with another region of the visual representation in the final product. This situation occurs, for example, for sealing caps <b>701</b> of bottles <b>703</b> which are shrunk onto the cork. In <figref idref="DRAWINGS">FIG. 7</figref><i>a </i>reference numeral <b>705</b> indicates the visible intersection line of the two overlapping ends of cap <b>701</b>. <figref idref="DRAWINGS">FIG. 7</figref><i>b </i>is a cut view along line A-A of <figref idref="DRAWINGS">FIG. 7</figref><i>a</i>. It shows the bottle neck <b>703</b>, the cap <b>701</b> with overlapping regions <b>707</b> and <b>709</b>. This situation also occurs when one does not want to attach a label to an underlying object which occurs in the case of labelling of PET bottles to facilitate the recycling of the plastics.</p>
<p id="p-0101" num="0100">When applying the process to this kind of product, there will be abutting or overlapping of the pattern of unique codes such that in the overlapped region <b>705</b> new codes would be identified which are present or not in the original pattern, e.g. pattern <b>301</b>, and they can therefore not be attached to a 2D position on the original pattern, which could lead to an erroneous pre-deforming result. To overcome this problem, the third embodiment of the invention proposes to arrange the codes in the border region such that even if two regions of the pattern abut against each other, the uniqueness of each code is still guaranteed (indicated by using roman numbers on the left side <b>707</b> and letters on the right side <b>709</b>. As a result, boarder regions can be identified by the uniqueness of codes even in the boarder region. As a consequence also deformations in overlapping regions can be dealt with, such that even in this region pre-deformation data can be obtained from the method.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08625883-20140107-M00001.NB">
<img id="EMI-M00001" he="8.81mm" wi="76.20mm" file="US08625883-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A computer implemented method comprising the steps of:
<claim-text>a) providing a pattern on a support, wherein the pattern comprises a distribution of codes which are arranged such that each code in the pattern is unique,</claim-text>
<claim-text>b) deforming, plastically, the support with the pattern,</claim-text>
<claim-text>c) taking at least two images of the plastically deformed support under different angles of view, and</claim-text>
<claim-text>d) determining a 3D surface model of the plastically deformed support based on the matching of at least one code in the pattern in the at least two images.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the pattern comprises a grid of quadrilateral polygons, and wherein one code is associated with one quadrilateral polygon.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein one code comprises at least one symbol, wherein the symbol is at least one of a texture and a color.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each code comprises its symbol and the symbols of a predetermined number of neighboring codes.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each unique code is rotationally invariant.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the colors to build a code are chosen out of a predetermined number of colors.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein intersection lines between the polygons have a color different to the one of the codes.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the pattern is a predetermined pattern and its wherein the size of the pattern is adapted to the size of the support by at least one of scaling, cutting, and repeating the pattern.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein in case of repeating the pattern, at least one special code is provided to enable an identification of the individual patterns after step b).</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the codes on the borders of the pattern are unique when at least two border regions abut against each other or overlap.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step d) of matching further comprises building a confidence value for an identified code in one image as a function of the number of identified neighbouring neighboring codes.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the step d) of matching further comprises only using codes for which, in the at least two images, more than one neighbor code has been correctly identified.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the 3D surface model determining step further comprises determining at least one passage matrix between two images comprising a rotational and a translational part corresponding to the difference between the point of view of each image.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising a step of determining a bijective relation between a 2D space represented by the pattern before plastic deformation and a 3D space represented by the 3D surface model.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The method according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, further comprising the steps of projecting a 2D regular grid onto the 3D surface model and determining a 2D resultant grid in the 2D space based on the bijective relation using a point of view different from those of the images of step c).</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The method according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, further comprising a step of projecting one or several 2D regular grids from one or several point of views on a part or the complete surface of the 3D surface model.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The method according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, further comprising a step of using a 2D warping method for determining the necessary predeformation of the visual representation based on the 2D regular grid and its transformation into the 2D space via the 3D space of the 3D surface model.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. A computer program product, comprising one or more non-transitory computer readable media having computer-executable instructions that, when executed, cause an apparatus to perform the steps of:
<claim-text>a) receiving image data of at least two images of a support with a pattern comprising a distribution of unique codes, and which was deformed, plastically, and wherein the images are taken from two different angles of view, and</claim-text>
<claim-text>b) determining a 3D surface model based on the matching of at least one code of the pattern in the at least two images.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The computer program product according to <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the computer-executable instructions, when executed, further cause the apparatus to perform the step of:
<claim-text>determining a bijective relation between a 2D space represented by the pattern before plastic deformation and a 3D space represented by the 3D surface model after plastic deformation.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. A system comprising an imaging device and a data treatment unit configured to:
<claim-text>a) provide a pattern on a support, wherein the pattern comprises a distribution of codes which are arranged such that each code in the pattern is unique;</claim-text>
<claim-text>b) deform, plastically, the support with the pattern;</claim-text>
<claim-text>c) take at least two images of the plastically deformed support under different angles of view; and</claim-text>
<claim-text>d) determine a 3D surface model of the plastically deformed support based on the matching of at least one code in the pattern in the at least two images. </claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
