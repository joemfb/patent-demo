<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626434-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626434</doc-number>
<kind>B1</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13410217</doc-number>
<date>20120301</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>01</class>
<subclass>C</subclass>
<main-group>21</main-group>
<subgroup>16</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>701400</main-classification>
<further-classification>701412</further-classification>
<further-classification>345625</further-classification>
</classification-national>
<invention-title id="d2e43">Automatic adjustment of a camera view for a three-dimensional navigation system</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5276785</doc-number>
<kind>A</kind>
<name>Mackinlay et al.</name>
<date>19940100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345427</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6017003</doc-number>
<kind>A</kind>
<name>Mullins</name>
<date>20000100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6104406</doc-number>
<kind>A</kind>
<name>Berry et al.</name>
<date>20000800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6201544</doc-number>
<kind>B1</kind>
<name>Ezaki</name>
<date>20010300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6320582</doc-number>
<kind>B1</kind>
<name>Yamamoto et al.</name>
<date>20011100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6500069</doc-number>
<kind>B1</kind>
<name>Ohba et al.</name>
<date>20021200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6556206</doc-number>
<kind>B1</kind>
<name>Benson et al.</name>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>7058896</doc-number>
<kind>B2</kind>
<name>Hughes</name>
<date>20060600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>7613566</doc-number>
<kind>B1</kind>
<name>Bolton</name>
<date>20091100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>7812841</doc-number>
<kind>B2</kind>
<name>Kamiwada et al.</name>
<date>20101000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>7933395</doc-number>
<kind>B1</kind>
<name>Bailly et al.</name>
<date>20110400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>8089479</doc-number>
<kind>B2</kind>
<name>Deb et al.</name>
<date>20120100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2002/0085046</doc-number>
<kind>A1</kind>
<name>Furuta et al.</name>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2002/0140698</doc-number>
<kind>A1</kind>
<name>Robertson et al.</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2004/0001110</doc-number>
<kind>A1</kind>
<name>Khan</name>
<date>20040100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2006/0227134</doc-number>
<kind>A1</kind>
<name>Khan et al.</name>
<date>20061000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>2007/0273712</doc-number>
<kind>A1</kind>
<name>O'Mullan et al.</name>
<date>20071100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>2008/0062173</doc-number>
<kind>A1</kind>
<name>Tashiro</name>
<date>20080300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>2009/0204920</doc-number>
<kind>A1</kind>
<name>Beverley et al.</name>
<date>20090800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>2010/0045666</doc-number>
<kind>A1</kind>
<name>Kornmann et al.</name>
<date>20100200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345419</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>2010/0161208</doc-number>
<kind>A1</kind>
<name>Akita et al.</name>
<date>20100600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>701201</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>EP</country>
<doc-number>0471484</doc-number>
<kind>A2</kind>
<date>19920200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00023">
<othercit>Gruber, D., &#x201c;The Mathematics of the 3D Rotation Matrix,&#x201d; Xtreme Game Developers Conference, Sep.-Oct. 1, 2000, available on the World Wide Web at http://www.fastgraph.com/makegames/3drotation/.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00024">
<othercit>U.S. Appl. No. 12/423,434, Varandhan et al., filed Apr. 14, 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00025">
<othercit>Non-Final Rejection mailed Aug. 19, 2011, in U.S. Appl. No. 12/423,434, Varadhan et al., filed Apr. 14, 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00026">
<othercit>Final Rejection mailed Feb. 29, 2012, in U.S. Appl. No. 12/423,434, Varadhan et al., filed Apr. 14, 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00027">
<othercit>Non-Final Rejection mailed Nov. 7, 2012, in U.S. Appl. No. 12/423,434, Varadhan et al., filed Apr. 14, 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00028">
<othercit>Buchholz, H. et al., &#x201c;Smart and Physically-Based Navigation in 3D Geovirtual Environment,&#x201d; Proceedings Ninth International Conference on Information Visualisation, Jul. 6-8, 2005, pp. 629-635.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00029">
<othercit>Burtnyk, N. et al., &#x201c;Style-Cam: Interactive Stylized 3D navigation Using Integrated Spatial &#x26; Temporal Controls,&#x201d; ACM Symposium on User Interface Software and Technology, Oct. 27, 2002, pp. 101-110.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00030">
<othercit>Google Earth, Jun. 29, 2005, http://web.archive.org/web/20050629095320/http://earth.google.com/ (one page).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00031">
<othercit>Mackinlay, J.D., et al., &#x201c;Rapid Controlled Movement Through a Virtual 3D Workspace,&#x201d; Computer Graphics, vol. 24, No. 4, Aug. 6, 1990, pp. 171-176.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00032">
<othercit>International Search Report and the Written Opinion of the International Search Authority, or the Declaration, International Appln. No. PCT/US2009/002309 filed on Apr. 14, 2009, Report and Opinion mailed on Aug. 12, 2009, 20 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00033">
<othercit>International Preliminary Report on Patentability and the Written Opinion of the International Searching Authority, International application No. PCT/US2009/002309 filed on Apr. 14, 2009, Report issued on Oct. 19, 2010, 12 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00034">
<othercit>Ehnes, Jochen et al., &#x201c;Projected Augmentation&#x2014;Augmented Reality using Rotatable Video Projectors,&#x201d; <i>Proceedings of the Third IEEE and ACM International Symposium on Mixed and Augmented Reality </i>(&#x201c;<i>ISMAR </i>2004&#x201d;), IEEE Computer Society, 10 pages 2004.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00035">
<othercit>Final Rejection mailed Apr. 8, 2013 in U.S. Appl. No. 12/423,434, Varadhan et al., filed Apr. 14, 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>22</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>345625</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>701412</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>701400</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>9</number-of-drawing-sheets>
<number-of-figures>9</number-of-figures>
</figures>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Kornmann</last-name>
<first-name>David</first-name>
<address>
<city>Tucson</city>
<state>AZ</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Birch</last-name>
<first-name>Peter</first-name>
<address>
<city>San Francisco</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Kornmann</last-name>
<first-name>David</first-name>
<address>
<city>Tucson</city>
<state>AZ</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Birch</last-name>
<first-name>Peter</first-name>
<address>
<city>San Francisco</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Sterne, Kessler, Goldstein &#x26; Fox PLLC</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Google Inc.</orgname>
<role>02</role>
<address>
<city>Mountain View</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Cheung</last-name>
<first-name>Mary</first-name>
<department>3667</department>
</primary-examiner>
<assistant-examiner>
<last-name>Wong</last-name>
<first-name>Yuen</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Methods and systems for automatically adjusting a three-dimensional navigation system are provided. A method for automatically adjusting a display viewpoint for a three-dimensional navigation system may include receiving a velocity of a vehicle and a look-ahead distance between the vehicle and a look-ahead point, determining a range distance between the vehicle and the display viewpoint, determining the first position of the display viewpoint, displaying a three-dimensional view of a navigation route for the vehicle from the first position, receiving a change in the look-ahead distance, determining a new range distance, determining a second position of the display viewpoint, and automatically moving the position of the display viewpoint to the second position on a curvilinear swoop path located above and behind the vehicle. A corresponding system may include a navigation information collector, a range determiner and a display viewpoint adjuster.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="186.44mm" wi="168.91mm" file="US08626434-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="234.10mm" wi="159.00mm" file="US08626434-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="242.32mm" wi="172.13mm" orientation="landscape" file="US08626434-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="239.86mm" wi="145.80mm" file="US08626434-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="228.85mm" wi="178.05mm" file="US08626434-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="177.29mm" wi="153.16mm" orientation="landscape" file="US08626434-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="225.47mm" wi="173.65mm" orientation="landscape" file="US08626434-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="233.76mm" wi="164.68mm" orientation="landscape" file="US08626434-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="235.88mm" wi="174.50mm" file="US08626434-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="249.00mm" wi="183.73mm" file="US08626434-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0002" num="0001">1. Technical Field</p>
<p id="p-0003" num="0002">Embodiments of the present invention generally relate to vehicle navigation and route planning systems and more particularly to vehicle navigation systems capable of displaying three-dimensional maps and imagery.</p>
<p id="p-0004" num="0003">2. Background Information</p>
<p id="p-0005" num="0004">Electronic vehicle navigation systems and portable navigation devices (PND) provide route calculations, turn-by-turn instructions, voice prompts and imagery to help guide and direct a driver to a destination. Such systems typically utilize a global positioning system (GPS) to determine a vehicle's location in relation to a digital map displayed to a driver. These systems also typically include a processor, other computer hardware, and software designed to accept user input, calculate navigation routes and convey real-time navigation information to a driver.</p>
<p id="p-0006" num="0005">In 2010, the global vehicle navigation system and PND market was estimated at nearly 40 million devices. Increasing popularity of GPS-enabled smartphones and numerous free and low-cost navigation applications should lead to continued growth in the overall number of devices used for vehicle navigation.</p>
<p id="p-0007" num="0006">Existing vehicle navigation systems allow a driver to manually set a zoom level for displayed map imagery. However, the zoom level set at one point in time remains constant, even when driving conditions change. In addition, many commercial navigation systems include as many as ten or more zoom levels, which require manual scrolling on a slider or tapping to carry out adjustment. Thus, a driver must either manually adjust the zoom level while driving or accept an unadjusted and potentially confusing navigation display. Both scenarios can lead to driving mistakes, driver frustration and increased safety risks.</p>
<heading id="h-0002" level="1">BRIEF SUMMARY OF THE INVENTION</heading>
<p id="p-0008" num="0007">Methods and systems for automatically adjusting a camera view for a three-dimensional navigation system are provided.</p>
<p id="p-0009" num="0008">In an embodiment, a method for automatically adjusting a display viewpoint of a three-dimensional navigation system includes receiving a velocity of a vehicle and a look-ahead distance between the vehicle and a look-ahead point. A range distance between the vehicle and the display viewpoint is determined based on the received velocity and look-ahead distance. A first display viewpoint position is determined based on the range distance. A three-dimensional view of a navigation route is then displayed from the first display viewpoint position. Next, a change is received in the look-ahead distance. A new range distance is determined based on the velocity and the change in look-ahead distance. A second display viewpoint position is then determined based on the new range distance. Finally, the tilt of the display viewpoint is automatically adjusted and the position of the display viewpoint is automatically moved to the second position located on a curvilinear swoop path above and behind the vehicle.</p>
<p id="p-0010" num="0009">In an additional embodiment, a system for automatically adjusting a display viewpoint of a three-dimensional navigation system for a vehicle includes a navigation information collector configured to receive a velocity of the vehicle and a look-ahead distance between the vehicle and a look-ahead point. The system also includes a range determiner configured to determine a range distance between the vehicle and the display viewpoint based on the velocity and the look-ahead distance. The system further includes a display viewpoint adjuster configured to determine a first position of the display viewpoint based on the range distance, to display a three-dimensional view of a navigation route for the vehicle from the first position, to receive a change in the look-ahead distance, to determine a new range distance based on the velocity and the change in the look-ahead distance, to determine a second position of the display viewpoint based on the updated range distance, and to automatically adjust a tilt of the display viewpoint and move the position of the display viewpoint to the second position located along a curvilinear swoop path above and behind the vehicle.</p>
<p id="p-0011" num="0010">In another embodiment, a computer-readable storage device having control logic recorded thereon is executed by a processor, which causes the processor to automatically adjust a display viewpoint of a three-dimensional navigation system for a vehicle. The control logic includes a first computer-readable program code to cause the processor to receive a velocity of the vehicle and a look-ahead distance between the vehicle and a look-ahead point. The control logic also includes a second computer-readable program code to cause the processor to determine a range distance between the vehicle and the display viewpoint based on the velocity and the look-ahead distance. Further, the control logic includes a third computer-readable program code to cause the processor to determine the first position of the display viewpoint based on the range distance, to display a three-dimensional view of a navigation route for the vehicle from the first position, to receive a change in the look-ahead distance, to determine a new range distance based on the velocity and the change in the look-ahead distance, to determine a second position of the display viewpoint based on the updated range distance, and to automatically adjust the display viewpoint tilt and move the position of the display viewpoint to the second position located along a curvilinear swoop path above and behind the vehicle.</p>
<p id="p-0012" num="0011">Further embodiments, features, and advantages of the invention, as well as the structure and operation of the various embodiments of the invention are described in detail below with reference to accompanying drawings.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE FIGURES</heading>
<p id="p-0013" num="0012">Embodiments of the invention are described with reference to the accompanying drawings. In the drawings, like reference numbers may indicate identical or functionally similar elements. The drawing in which an element first appears is generally indicated by the left-most digit in the corresponding reference number.</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram of a system for automatically adjusting a display viewpoint of a three-dimensional navigation system according to an embodiment.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 2</figref> is an illustration of a virtual camera display viewpoint for a three-dimensional navigation system according to an embodiment.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 3</figref> is a flow diagram of a method for automatically adjusting a display viewpoint of a three-dimensional navigation system according to an embodiment.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 4</figref> is an illustration of the general movement of a virtual camera along a curvilinear swoop path according to an embodiment.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 5</figref> is an illustration of a navigation system screen layout according to an embodiment.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 6</figref> is an illustration of a navigation display view according to an embodiment.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 7</figref> is an additional illustration of a navigation display view according to an embodiment.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 8</figref> is an illustration of a temporary alternative viewpoint according to an embodiment.</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 9</figref> is a diagram of a computer system that may be used in an embodiment of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION OF THE INVENTION</heading>
<p id="p-0023" num="0022">Embodiments are described herein with reference to the illustrative embodiments for particular applications, it should be understood that the invention is not limited to the described embodiments. Those skilled in the art with access to the teachings provided herein will recognize additional modifications, applications, and embodiments within the scope thereof and additional fields in which the invention would be of significant utility.</p>
<p id="p-0024" num="0023">In the detailed description of embodiments that follows, references to &#x201c;one embodiment&#x201d;, &#x201c;an embodiment&#x201d;, &#x201c;an example embodiment&#x201d;, etc., indicate that the embodiment described may include a particular feature, structure, or characteristic, but every embodiment may not necessarily include the particular feature, structure, or characteristic. Moreover, such phrases are not necessarily referring to the same embodiment. Further, when a particular feature, structure, or characteristic is described in connection with an embodiment, it is submitted that it is within the knowledge of one skilled in the art to effect such feature, structure, or characteristic in connection with other embodiments whether or not explicitly described.</p>
<p id="p-0025" num="0024">Vehicle navigation systems provide audio and visual route navigation instructions with corresponding map imagery to guide a driver to a destination. Such systems also may be used to find alternative routes and to locate points of interest. However, existing vehicle navigation systems do not provide continuous, automated adjustment of a vehicle navigation display according to real-time driving conditions. As a result, a driver must either manually adjust zoom and display settings or accept a stale display view until an update occurs.</p>
<p id="p-0026" num="0025">Embodiments described herein relate to automatic adjustment of a camera view for a three-dimensional navigation system. Such embodiments provide the perspective of a personalized, virtual flying camera positioned at various points along a curvilinear swoop path located above and behind a vehicle, or a representation of the vehicle. The position and tilt of the virtual camera viewpoint are automatically adjusted based on changes in driving conditions, without any manual intervention. Thus, a driver is automatically presented with navigation imagery tailored to constantly changing driving conditions, thereby reducing distraction and improving the driving experience.</p>
<p id="p-0027" num="0026">Additional embodiments may also extend and apply to any moving object with variable speed. Such objects may include, but are not limited to, boats, ships, trains, buses, motorcycles, taxis, and driverless vehicles. These and other types of moving objects with variable speed are generally referred to herein as a vehicle. In addition, embodiments are not limited to vehicle navigation and may be applied towards visualization relating to any moving object, including the use of informational displays.</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram of exemplary system <b>100</b> for automatically adjusting a display viewpoint of a three-dimensional navigation system, according to an embodiment. Three-dimensional views provided from the display viewpoint may be rendered for display as two-dimensional scenes. Such rendering may include the use of stereoscopic, photorealistic or other imaging techniques to provide the perception of three-dimensional depth in a two-dimensional display.</p>
<p id="p-0029" num="0028">System <b>100</b>, or any combination of its components, may be part of or may be implemented with a computing device. Examples of computing devices include, but are not limited to, a computer, workstation, distributed computing system, computer cluster, embedded system, stand-alone electronic device, networked device, mobile device (e.g. mobile phone, smart phone, navigation device, tablet or mobile computing device), rack server, set-top box, or other type of computer system having at least one processor and memory. Such a computing device may include software, firmware, hardware, or a combination thereof. Software may include one or more applications and an operating system. Hardware may include, but is not limited to, a processor, memory and user interface display.</p>
<p id="p-0030" num="0029">The computing device can be configured to access content hosted on web servers over a network. The network can be any network or combination of networks that can carry data communications. Such a network can include, but is not limited to, a wired (e.g., Ethernet) or a wireless (e.g., Wi-Fi and 4 G) network. In addition, the network can include, but is not limited to, a local area network, medium area network, and/or wide area network such as the Internet. The network can support protocols and technology including, but not limited to, Internet or World Wide Web protocols and/or services. Intermediate network routers, gateways, or servers may be provided between servers and clients depending upon a particular application or environment.</p>
<p id="p-0031" num="0030">System <b>100</b> may include a navigation system <b>102</b> in communication with a display viewpoint adjustment system <b>110</b>, either directly or using an application programming interface (API). Navigation system <b>102</b> may access default navigation settings <b>104</b> and driver preferences <b>106</b>, which may be available locally or network accessible. In addition, display viewpoint adjustment system <b>110</b> may include a navigation information collector <b>112</b>, a range determiner <b>114</b>, and a display viewpoint adjuster <b>116</b>.</p>
<p id="p-0032" num="0031">In one embodiment, system <b>100</b> can be a computing device integrated into a vehicle, such as an on-board navigation system installed directly into the dashboard of the vehicle during the manufacturing process. According to another embodiment, system <b>100</b> can also be a computing device that is separate from the vehicle, but travels along with the vehicle. For example, system <b>100</b> can be implemented using a computing device such as a GPS-enabled smartphone or a portable-navigation device. An additional embodiment of system <b>100</b> may include a remote computing device configured to receive information about the movement of the vehicle from a GPS tracking device.</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 2</figref> is an illustration of a virtual camera display viewpoint for a three-dimensional navigation system according to an embodiment. Virtual camera <b>202</b> having tilt <b>204</b> is positioned at a display viewpoint position located on a curvilinear swoop path <b>206</b> above and behind vehicle representation <b>210</b>. Look-ahead distance <b>212</b> is the distance that a driver wishes to see ahead when operating vehicle <b>210</b>. Look-ahead distance <b>212</b> may be a specific distance or a distance calculated based on the number of seconds a driver wishes to see ahead when traveling at a certain velocity. Further, look-ahead distance <b>212</b> may be based on one or more of default navigation system settings <b>104</b> and driver preferences <b>106</b>, and may be further adjusted based on any number of driving conditions.</p>
<p id="p-0034" num="0033">According to an embodiment, vehicle <b>210</b> travels down a roadway at varying speeds along a navigation route displayed by navigation system <b>102</b>. Vehicle <b>210</b> has a velocity at any given moment in time with changes in velocity occurring as vehicle <b>210</b> accelerates and decelerates.</p>
<p id="p-0035" num="0034">In an embodiment, navigation information collector <b>112</b> receives the velocity of vehicle <b>210</b> and look-ahead distance <b>212</b>. Range determiner <b>114</b> then determines a range distance <b>216</b>, which is the distance between vehicle <b>210</b> and the virtual camera display viewpoint position, based on the received velocity and look-ahead distance <b>212</b>. Display viewpoint adjuster <b>116</b> then determines a first display viewpoint position along a curvilinear swoop path <b>206</b> and also a display viewpoint tilt <b>204</b> based on the determined range distance <b>216</b>. Virtual camera <b>202</b> is then automatically moved to the determined first position, tilt <b>204</b> is automatically adjusted, and a three-dimensional navigation view from the first position is displayed by navigation system <b>102</b>.</p>
<p id="p-0036" num="0035">Navigation information collector <b>112</b> then receives an updated velocity as vehicle <b>210</b> accelerates or decelerates. Navigation information collector <b>112</b> may also receive an updated look-ahead distance <b>212</b> in addition to a change in speed. Range determiner <b>114</b> determines a new range distance <b>216</b> based on the updated velocity and the updated look-ahead distance. Display viewpoint adjuster <b>116</b> determines a second position of the display viewpoint based on the updated range distance. Display viewpoint adjuster then automatically moves the display viewpoint to a second position along curvilinear swoop path <b>206</b> located above and behind the vehicle and automatically adjusts the tilt <b>204</b> of the display viewpoint. As the velocity of vehicle <b>210</b> and look-ahead distance <b>212</b> continue to change, this process will be repeated to automatically determine and adjust the display viewpoint position and tilt based on changes in speed and driving conditions.</p>
<p id="p-0037" num="0036">According to a number of embodiments, range distance <b>216</b> may be calculated as a function having one or more input variables including one or more of a velocity of vehicle <b>210</b>, a number of seconds a driver wishes to look ahead, a distance a driver wishes to look ahead, a position where the vehicle is to appear on a three-dimensional navigation display, a position where the look-ahead point is to appear on a three-dimensional navigation display, an equation of a curvilinear swoop path, and one or more driver preferences relating to camera orientation and tilt. Such embodiments may calculate range distance <b>216</b> as a mathematical equation, which may incorporate one or more additional variables. Further, range distance <b>216</b> may be updated or recalculated periodically, based on an external prompt or event, and based on changes to one or more input variables.</p>
<p id="p-0038" num="0037">Other embodiments may determine range distance <b>216</b> based on incremental refinement of the display viewpoint along a curvilinear swoop path. For example, the display viewpoint may be adjusted incrementally on a curvilinear swoop path <b>206</b> based on knowing the equation of curvilinear swoop path <b>206</b>, a position where vehicle <b>210</b> is to be displayed on a three-dimensional navigation display, and a position where look-ahead point <b>214</b> is to be displayed on a three-dimensional navigation display. User preferences relating to navigation display also may be used to influence an incremental refinement system.</p>
<p id="p-0039" num="0038">For example, according to additional embodiments, the position of the display viewpoint can be incrementally adjusted higher on curvilinear swoop path <b>206</b> as a vehicle accelerates and look-ahead distance increases. Alternatively, the position of the display viewpoint can be adjusted lower on curvilinear swoop path <b>206</b> as a vehicle decelerates and look-ahead distance decreases. Additional embodiments may include use of systems and methods presented in U.S. patent application Ser. No. 12/423,434, entitled SWOOP NAVIGATION (US 2009/0259976), which is incorporated herein by reference in its entirety.</p>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. 3</figref> is a flow diagram of a method for automatically adjusting a display viewpoint of a three-dimensional navigation system according to an embodiment. Method <b>300</b> begins at step <b>302</b> when vehicle <b>210</b> is in motion. The velocity of vehicle <b>210</b> and look-ahead distance <b>212</b> to look-ahead point <b>214</b> are received. Velocity may be received directly from vehicle <b>210</b>, navigation system <b>102</b> or another device capable of capturing or determining velocity. In addition, a future velocity may be anticipated based on a rate of acceleration or deceleration. Look-ahead distance <b>212</b> may also be supplied using default navigation settings <b>104</b> or driver preferences <b>102</b>, which may be stored locally or remotely, or provided in real-time using voice commands or computer system input. Step <b>302</b> may be performed by navigation information collector <b>112</b>.</p>
<p id="p-0041" num="0040">At step <b>304</b>, a range distance <b>216</b> between vehicle <b>210</b> and the display viewpoint is determined based on the received velocity and look-ahead distance <b>214</b>. Step <b>304</b> may be performed by range determiner <b>114</b>.</p>
<p id="p-0042" num="0041">At step <b>306</b>, a first position of the display viewpoint is determined based on the range distance <b>216</b>. Step <b>306</b> may be performed by display viewpoint adjuster <b>116</b>.</p>
<p id="p-0043" num="0042">At step <b>308</b>, a three-dimensional view of a navigation route for the vehicle is displayed to the driver, from the determined first position. Step <b>308</b> may be performed by display viewpoint adjuster <b>116</b>.</p>
<p id="p-0044" num="0043">At step <b>310</b>, a change in the velocity of vehicle <b>210</b> and the look-ahead distance is received. Step <b>310</b> may be performed by navigation information collector <b>112</b>.</p>
<p id="p-0045" num="0044">At step <b>312</b>, a new range distance is determined based on the velocity and change in look-ahead distance. Step <b>312</b> may be performed by range determiner <b>114</b>.</p>
<p id="p-0046" num="0045">At step <b>314</b>, a second position of the display viewpoint is determined based on the new range distance. Step <b>314</b> may be performed by display viewpoint adjuster <b>116</b>.</p>
<p id="p-0047" num="0046">At step <b>316</b>, the position of the display viewpoint is automatically adjusted to the second position along a curvilinear swoop path located above and behind the vehicle and the tilt of the display viewpoint is also automatically adjusted. Step <b>316</b> may be performed by display viewpoint adjuster <b>116</b>.</p>
<p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. 4</figref> is an illustration of the general movement of a virtual camera along a curvilinear swoop path, according to an embodiment.</p>
<p id="p-0049" num="0048">The size and curvature of curvilinear swoop path <b>404</b> are determined dynamically based on vehicle velocity. In addition, the display viewpoint position and tilt of the display viewpoint are adjusted based on changes in velocity. In an embodiment, the characteristics of curvilinear swoop path <b>404</b> and the display viewpoint remain unchanged at a constant velocity.</p>
<p id="p-0050" num="0049">Display viewpoint <b>402</b> illustrates a display viewpoint position for a vehicle traveling at a high velocity, according to an embodiment. Display viewpoint <b>406</b> illustrates a display viewpoint position for a vehicle moving at a low speed, according to another embodiment.</p>
<p id="p-0051" num="0050">Generally, as vehicle velocity increases, the display viewpoint position moves higher along curvilinear swoop path <b>404</b>, increasing both the range distance and the visible area within range of the display viewpoint. As velocity decreases, the display viewpoint position moves closer to the to the vehicle, providing a more focused view of the navigation route and its surroundings.</p>
<p id="p-0052" num="0051">The tilt of the display viewpoint is also adjusted based on velocity. For example, as vehicle <b>210</b> accelerates, the curvature of curvilinear swoop path <b>404</b> increases and the display viewpoint position moves away from vehicle <b>210</b> as the display viewpoint position moves higher along the curve. As a result, the display viewpoint tilt must be adjusted downward based on one or more of display viewpoint position, velocity, look-ahead point, range distance, default navigation settings, and driver preferences. On the other hand, as vehicle <b>210</b> slows down, virtual display viewpoint approaches the ground and the tilt must be adjusted horizontally to maintain focus on the look-ahead point in front of the vehicle, which can also be fine-tuned using the same parameters.</p>
<p id="p-0053" num="0052">Display viewpoint settings may be based on pre-configured system settings, such as default navigation system settings. Display viewpoint settings may also be parameterized based on user preference. For example, user preferences may include look-ahead distance, look-ahead time, range distance, tilt angle, and other settings when traveling at a particular speed.</p>
<p id="p-0054" num="0053">In addition, the position and tilt of the display viewpoint may be further influenced and adjusted based on changes in attributes related to a driver, a vehicle, a navigation route, weather, and visibility. Driver-related attributes may include a driving habit of a driver, driving performance of a driver, driver familiarity with route, vital signs, vision, and age. Vehicle attributes may include vehicle size, weight, and performance capabilities. Navigation route factors may include terrain, route complexity, route characteristics, traffic pattern changes, actual or anticipated traffic volume, events occurring along the navigation route, approaching objects, obstructions to the display view, trip duration and proximity to a final destination point. Weather conditions, visibility, time of day, and available daylight also may be used as influencing factors when determining the display viewpoint position and tilt.</p>
<p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. 5</figref> is an illustration of a navigation system display according to an embodiment. Navigation system display <b>500</b> includes navigation system screen <b>502</b> with a horizontal field of view <b>504</b> and a vertical field of view <b>506</b>, both corresponding to the look-ahead view of a navigation route. In an embodiment, near distance <b>508</b> is where the vehicle will be positioned on the navigation screen. Near distance <b>508</b> is approximately 25% from the bottom of navigation system screen <b>502</b>. Far distance <b>510</b> is where the look-ahead point will be positioned on the navigation screen. Far distance <b>510</b>, is located approximately 25% from the top of navigation system screen <b>502</b>. Near distance <b>508</b> and far distance <b>510</b> also may be parameterized and positioned in other locations of the navigation display based on user preference.</p>
<p id="p-0056" num="0055">Existing navigation systems typically center a vehicle on a navigation system display. However, in an embodiment, the spacing between the near distance and the far distance remains constant on the navigation system display. This is accomplished by further adjusting a determined display viewpoint position and tilt to accommodate navigation system or user preferred near distance and far distance positions on the navigation screen.</p>
<p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. 6</figref> is an illustration of a navigation display view according to an embodiment. Display view <b>600</b> generalizes a virtual camera navigation display view for a vehicle traveling at a relatively low speed. According to an embodiment, navigation system screen <b>602</b> displays vehicle <b>604</b> and look-ahead point <b>606</b>. Near distance <b>608</b> is consistently displayed approximately 25% from the bottom of navigation system screen <b>602</b>. Far distance <b>610</b> is consistently displayed approximately 25% from the top of navigation system screen <b>602</b>.</p>
<p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. 7</figref> is an additional illustration of a navigation display view according to an embodiment. Display view <b>700</b> generalizes a virtual camera navigation display view for a vehicle traveling at a higher speed than the embodiment illustrated in <figref idref="DRAWINGS">FIG. 6</figref>. Navigation system screen <b>702</b> displays vehicle <b>704</b> and look-ahead point <b>706</b>. Near distance <b>708</b> is consistently displayed approximately 25% from the bottom of navigation system screen <b>702</b>. Far distance <b>710</b> is consistently displayed approximately 25% from the top of navigation system screen <b>702</b>.</p>
<p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. 8</figref> is an illustration of a temporary alternative viewpoint according to an embodiment. Temporary alternative display view <b>800</b> illustrates a non-traditional navigation display view that may be provided when any physical object such as a building, tree, or other obstruction threatens to interfere or actually interferes with the three-dimensional navigation view of a display viewpoint located along a curvilinear swoop path above and behind a vehicle. A temporary alternative display viewpoint is not fixed and may be located anywhere alongside, above, or in front of a vehicle for the purpose of providing a driver with an unobstructed or improved navigation view. In addition, a temporary display viewpoint may also follow its own curvilinear swoop path located above any position surrounding the representation of a vehicle.</p>
<p id="p-0060" num="0059">Vehicle <b>802</b> is seen from a temporary alternative viewpoint located near the side of the vehicle representation to prevent tall building <b>804</b> from blocking the navigation display view during a turn. Alternatively, a temporary alternative viewpoint may be positioned, for example, in front of and facing a representation of vehicle <b>802</b> around the corner from building <b>804</b>. Once the obstruction has been passed, the display viewpoint is readjusted and returned to a position located along a traditional curvilinear swoop path above and behind the vehicle. A temporary display viewpoint also may be used in response to other conditions such as sudden movement of a vehicle, reverse direction of a vehicle, an approaching object, an anticipated driving maneuver, a traffic pattern, and also may be further customized based on user preference.</p>
<p id="p-0061" num="0060">Temporary display viewpoint user preferences may include one or more of a general or specific display viewpoint position or orientation, an amount of space surrounding a vehicle, and a degree of tilt for specific conditions. For example, temporary alternative viewpoint preferences may be defined for one or more driving scenarios such as driving environment, driving maneuver, speed, and navigation viewpoint obstacle type.</p>
<p id="h-0005" num="0000">Example Computer Embodiment</p>
<p id="p-0062" num="0061">In an embodiment of the present invention, the system and components of embodiments described herein are implemented using well known computers, such as example computer system <b>900</b> shown in <figref idref="DRAWINGS">FIG. 9</figref>. For example, navigation system <b>102</b> and display viewpoint adjustment system <b>110</b> or any of its respective modules may be implemented using computer system <b>900</b>.</p>
<p id="p-0063" num="0062">Computer system <b>900</b> can be any commercially available and well-known computer capable of performing the functions described herein. Such computer systems may include embedded computer systems, mobile computers, on-board computer systems and vehicle mounted computer systems.</p>
<p id="p-0064" num="0063">Computer system <b>900</b> includes one or more processors (also called central processing units, or CPUs), such as a processor <b>904</b>. Processor <b>904</b> is connected to a communication infrastructure <b>906</b>. Such processors may include ARM or SuperH-based processors.</p>
<p id="p-0065" num="0064">Computer system <b>900</b> also includes a main or primary memory <b>908</b>, such as random access memory (RAM). Main memory <b>908</b> has stored control logic (computer software), and data.</p>
<p id="p-0066" num="0065">Computer system <b>900</b> also includes one or more secondary storage devices <b>910</b>. Secondary storage device <b>910</b> includes, for example, a hard disk drive <b>912</b> and/or a removable storage device or drive <b>914</b>, as well as other types of storage devices, such as memory cards, memory sticks and flash-based drives. Removable storage drive <b>914</b> represents a floppy disk drive, a magnetic tape drive, a compact disk drive, an optical storage device, tape backup, etc.</p>
<p id="p-0067" num="0066">Removable storage drive <b>914</b> interacts with a removable storage unit <b>918</b>. Removable storage unit <b>918</b> includes a computer useable or readable storage device having stored thereon computer software (control logic) and/or data. Removable storage unit <b>918</b> represents a floppy disk, magnetic tape, compact disk, DVD, optical storage disk, memory card, or any other computer data storage device. Removable storage drive <b>914</b> reads from and/or writes to removable storage unit <b>918</b> in a well-known manner.</p>
<p id="p-0068" num="0067">Computer system <b>900</b> also includes input/output/display devices <b>930</b>, such as monitors, keyboards, pointing devices, etc., which communicate with communication infrastructure <b>906</b> through a display interface <b>902</b>.</p>
<p id="p-0069" num="0068">Computer system <b>900</b> further includes a communication or network interface <b>924</b>. Communications interface <b>924</b> enables computer system <b>900</b> to communicate with remote devices. For example, communications interface <b>924</b> allows computer system <b>900</b> to communicate over communications path <b>926</b> (representing a form of a computer useable or readable medium), such as LANs, WANs, the Internet, etc. Communications interface <b>924</b> may interface with remote sites or networks via wired or wireless connections.</p>
<p id="p-0070" num="0069">Control logic may be transmitted to and from computer system <b>900</b> via communication path <b>926</b>. More particularly, computer system <b>900</b> may receive and transmit carrier waves (electromagnetic signals) modulated with control logic via communication path <b>926</b>.</p>
<p id="p-0071" num="0070">Any apparatus or article of manufacture comprising a computer useable or readable medium having control logic (software) stored thereon is referred to herein as a computer program product or program storage device. This includes, but is not limited to, computer system <b>900</b>, main memory <b>908</b>, secondary storage device <b>910</b>, and removable storage unit <b>918</b>. Such computer program products, having control logic stored thereon that, when executed by one or more data processing devices, causes such data processing devices to operate as described herein, represent embodiments of the invention.</p>
<p id="p-0072" num="0071">Embodiments of the invention can work with software, hardware, and/or operating system implementations other than those described herein. Any software, hardware, and operating system implementations suitable for performing the functions described herein can be used. Embodiments of the invention are applicable to both a client and to a server or a combination of both.</p>
<p id="p-0073" num="0072">Embodiments of the present invention have been described above with the aid of functional building blocks illustrating the implementation of specified functions and relationships thereof. The boundaries of these functional building blocks have been arbitrarily defined herein for the convenience of the description. Alternate boundaries can be defined so long as the specified functions and relationships thereof are appropriately performed. The breadth and scope of the present invention should not be limited by any of the above-described exemplary embodiments.</p>
<p id="p-0074" num="0073">In addition, the foregoing description of the specific embodiments will so fully reveal the general nature of the invention that others can, by applying knowledge within the skill of the art, readily modify and/or adapt for various applications such specific embodiments, without undue experimentation, without departing from the general concept of the present invention. Therefore, such adaptations and modifications are intended to be within the meaning and range of equivalents of the disclosed embodiments, based on the teaching and guidance presented herein. It is to be understood that the phraseology or terminology herein is for the purpose of description and not of limitation, such that the terminology or phraseology of the present specification is to be interpreted by the skilled artisan in light of the teachings and guidance.</p>
<p id="p-0075" num="0074">It is to be appreciated that the Detailed Description section, and not the Summary and Abstract sections, is intended to be used to interpret the claims. The Summary and Abstract sections may set forth one or more but not all exemplary embodiments of the present invention as contemplated by the inventors, and thus, are not intended to limit the present invention and the appended claims in any way.</p>
<p id="p-0076" num="0075">The breadth and scope of the present invention should not be limited by any of the above-described exemplary embodiments, but should be defined only in accordance with the following claims and their equivalents.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A computer-implemented method for automatically adjusting a virtual camera display viewpoint of a three-dimensional navigation system for a vehicle comprising:
<claim-text>receiving a velocity of the vehicle and a look-ahead distance between the vehicle and a look-ahead point;</claim-text>
<claim-text>determining a range distance between the vehicle and the virtual camera display viewpoint based on the velocity and the look-ahead distance;</claim-text>
<claim-text>determining, by a processor-based device, a first position of the virtual camera display viewpoint based on the range distance;</claim-text>
<claim-text>displaying a three-dimensional view of a navigation route for the vehicle from the first position;</claim-text>
<claim-text>receiving a change in the look-ahead distance;</claim-text>
<claim-text>determining a new range distance based on the velocity and the change in the look-ahead distance;</claim-text>
<claim-text>determining a second position of the virtual camera display viewpoint based on the updated range distance; and</claim-text>
<claim-text>automatically adjusting a tilt of the virtual camera display viewpoint and moving the position of the virtual camera display viewpoint to the second position, wherein the virtual camera display viewpoint moves to the second position along a curvilinear swoop path located above and behind a representation of the vehicle.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determination of the new range distance is based on a second received velocity and the change in the look-ahead distance.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determination of the new range distance is based on an anticipated velocity and the change in the look-ahead distance.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determination of the position of the virtual camera display viewpoint and the tilt of the virtual camera display viewpoint are made such that the location of the vehicle and the location of the look-ahead point on a three-dimensional navigation system display remain constant.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determination of the second position of the virtual camera display viewpoint is made to provide an unobstructed display viewpoint to the driver.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>determining a temporary alternative display viewpoint position and tilt based on a condition; and</claim-text>
<claim-text>automatically readjusting the three-dimensional view of the navigation route for the vehicle to reflect the temporary alternative display viewpoint position and tilt.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the temporary alternative display viewpoint position and tilt are based on sudden movement of the vehicle.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the temporary alternative display viewpoint position and tilt are based on a reverse direction of the vehicle.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein use of a temporary alternative display viewpoint is based on a condition existing further along the navigation route in front of the vehicle.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method old <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the determination of the temporary alternative display viewpoint position and tilt is based on avoidance of an obstruction to the three-dimensional navigation viewpoint.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising:
<claim-text>automatically returning the three-dimensional view of the navigation route to a previously displayed viewpoint and tilt.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. A system for automatically adjusting a virtual camera display viewpoint of a three-dimensional navigation system for a vehicle comprising:
<claim-text>a navigation information collector configured to receive a velocity of the vehicle and a look-ahead distance between the vehicle and a look-ahead point;</claim-text>
<claim-text>a range determiner implemented by a processor-based device and configured to determine a range distance between the vehicle and the virtual camera display viewpoint based on the velocity and the look-ahead distance; and</claim-text>
<claim-text>a virtual camera display viewpoint adjuster implemented by a processor-based device and configured to:</claim-text>
<claim-text>determine a first position of the virtual camera display viewpoint based on the range distance,</claim-text>
<claim-text>display a three-dimensional view of a navigation route for the vehicle from the first position,</claim-text>
<claim-text>receive a change in the look-ahead distance,</claim-text>
<claim-text>determine a new range distance based on the velocity and the change in the look-ahead distance,</claim-text>
<claim-text>determine a second position of the virtual camera display viewpoint based on the updated range distance, and</claim-text>
<claim-text>automatically adjust a tilt of the virtual camera display viewpoint and move the position of the virtual camera display viewpoint to the second position, wherein the virtual camera display viewpoint moves to the second position along a curvilineal swoop path located above and behind a representation of the vehicle.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the determination of the new range distance is based on a second received velocity and the change in the look-ahead distance.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the determination of the new range distance is based on an anticipated velocity and the change in the look-ahead distance.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the determination of the position of the virtual camera display viewpoint and the tilt of the virtual camera display viewpoint are made in part based on the vehicle's proximity to a final destination point.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the determination of the second position of the virtual camera display viewpoint is made to provide an unobstructed display viewpoint to the driver.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, further comprising:
<claim-text>determining a temporary alternative display viewpoint position and tilt based on a condition; and</claim-text>
<claim-text>automatically readjusting the three-dimensional view of the navigation route for the vehicle to reflect the temporary alternative display viewpoint position and tilt.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the temporary alternative display viewpoint position and tilt are based on sudden movement of the vehicle.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the temporary alternative display viewpoint position and tilt arc based on a reverse direction of the vehicle.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the determination of the temporary alternative display viewpoint position and tilt is based on avoidance of an obstruction to the three-dimensional navigation viewpoint.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, further comprising:
<claim-text>automatically returning the three-dimensional view of the navigation route to a previously displayed viewpoint and tilt.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. A computer-readable storage device having control logic recorded thereon that when executed by a processor, causes the processor to automatically adjust a virtual camera display viewpoint of a three-dimensional navigation system for a vehicle, the control logic comprising:
<claim-text>a first computer-readable program code to cause the processor to receive a velocity of the vehicle and a look-ahead distance between the vehicle and a look-ahead point;</claim-text>
<claim-text>a second computer-readable program code to cause the processor to determine a range distance between the vehicle and the virtual camera display viewpoint based on the velocity and the look-ahead distance; and</claim-text>
<claim-text>a third computer-readable program code to cause the processor to:</claim-text>
<claim-text>determine a first position of the virtual camera display viewpoint based on the range distance,</claim-text>
<claim-text>display a three-dimensional view of a navigation route for the vehicle from the first position,</claim-text>
<claim-text>receive a change in the look-ahead distance,</claim-text>
<claim-text>determine a new range distance based on the velocity and the change in the look-ahead distance,</claim-text>
<claim-text>determine a second position of the virtual camera display viewpoint based on the updated range distance, and</claim-text>
<claim-text>automatically adjust a tilt of the virtual camera display viewpoint and move the position of the virtual camera display viewpoint to the second position, wherein the virtual camera display viewpoint moves to the second position along a curvilinear swoop path located above and behind a representation of the vehicle. </claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
