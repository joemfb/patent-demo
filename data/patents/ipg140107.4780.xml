<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625873-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625873</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13774229</doc-number>
<date>20130222</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>JP</country>
<doc-number>2012-038600</doc-number>
<date>20120224</date>
</priority-claim>
<priority-claim sequence="02" kind="national">
<country>JP</country>
<doc-number>2012-038660</doc-number>
<date>20120224</date>
</priority-claim>
</priority-claims>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382131</main-classification>
<further-classification>382128</further-classification>
<further-classification>25036302</further-classification>
<further-classification>25036304</further-classification>
<further-classification>25036305</further-classification>
</classification-national>
<invention-title id="d2e79">Medical image processing apparatus</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5630034</doc-number>
<kind>A</kind>
<name>Oikawa et al.</name>
<date>19970500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345424</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>2003/0021385</doc-number>
<kind>A1</kind>
<name>Izuhara</name>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>378195</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>2005/0275654</doc-number>
<kind>A1</kind>
<name>Matsumoto</name>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345421</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>2010/0014760</doc-number>
<kind>A1</kind>
<name>Mohammad et al.</name>
<date>20100100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382203</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>2010/0098299</doc-number>
<kind>A1</kind>
<name>Muquit et al.</name>
<date>20100400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382115</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2010/0202675</doc-number>
<kind>A1</kind>
<name>Takanaka et al.</name>
<date>20100800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>JP</country>
<doc-number>2010-201157</doc-number>
<date>20100900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>JP</country>
<doc-number>2010-284301</doc-number>
<date>20101200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>25</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382128</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382131</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>250455</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>25036302</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>25036304</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>25036305</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>15</number-of-drawing-sheets>
<number-of-figures>15</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20130223589</doc-number>
<kind>A1</kind>
<date>20130829</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only" applicant-authority-category="assignee">
<addressbook>
<orgname>Kabushiki Kaisha Toshiba</orgname>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only" applicant-authority-category="assignee">
<addressbook>
<orgname>Toshiba Medical Systems Corporation</orgname>
<address>
<city>Otawara</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Fujisawa</last-name>
<first-name>Yasuko</first-name>
<address>
<city>Nasushiobara</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Tsukagoshi</last-name>
<first-name>Shinsuke</first-name>
<address>
<city>Nasushiobara</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Ikeda</last-name>
<first-name>Yoshihiro</first-name>
<address>
<city>Sakura</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Oblon, Spivak, McClelland, Maier &#x26; Neustadt, L.L.P.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Kabushiki Kaisha Toshiba</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
<assignee>
<addressbook>
<orgname>Toshiba Medical Systems Corporation</orgname>
<role>03</role>
<address>
<city>Otawara-shi</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Bayat</last-name>
<first-name>Ali</first-name>
<department>2666</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Provided is a medical image processing apparatus allowing the generation of image data by changing the reconstruction conditions in correspondence with the positional relation of an observation target based on the projected data chronologically acquired by an X-ray CT scanner. The medical image processing apparatus includes a photographing unit, a reconfiguration processing unit, an extracting unit, and an analyzing unit. The photographing unit scans the flexible site of the living body configured from multiple parts in order to acquire projected data. The reconfiguration processing unit carries out reconfiguration processing on the projected data and generates image data of the flexible site regarding the plurality of timing points. The extracting unit extracts the plurality of components configuring the flexible site from the respective image data. The analyzing unit obtains the positional relation of the plurality of components.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="182.88mm" wi="478.03mm" file="US08625873-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="255.19mm" wi="184.40mm" orientation="landscape" file="US08625873-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="228.77mm" wi="179.24mm" orientation="landscape" file="US08625873-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="176.95mm" wi="166.03mm" file="US08625873-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="178.99mm" wi="163.32mm" file="US08625873-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="172.80mm" wi="170.77mm" file="US08625873-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="212.26mm" wi="161.97mm" file="US08625873-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="210.23mm" wi="171.45mm" file="US08625873-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="159.26mm" wi="152.40mm" file="US08625873-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="210.23mm" wi="168.74mm" file="US08625873-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="188.47mm" wi="140.89mm" file="US08625873-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="193.21mm" wi="136.74mm" file="US08625873-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="159.94mm" wi="127.93mm" file="US08625873-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="189.82mm" wi="141.56mm" file="US08625873-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="187.79mm" wi="140.89mm" file="US08625873-20140107-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="202.78mm" wi="136.06mm" file="US08625873-20140107-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading>
<p id="p-0002" num="0001">This application is based upon and claims the benefit of priority from Japanese Patent Applications No. 2012-038600 and No. 2012-038660, filed on Feb. 24, 2012; the entire contents of which are incorporated herein by reference.</p>
<heading id="h-0002" level="1">FIELD</heading>
<p id="p-0003" num="0002">The embodiment relates to a technology for generating medical images based on a projection data obtained using an X-ray CT scanner.</p>
<heading id="h-0003" level="1">BACKGROUND</heading>
<p id="p-0004" num="0003">An apparatus that uses an X-ray CT (Computed Tomography) scanner to scan test objects and processes the collected data with a computer, thereby imaging the inside of the test object.</p>
<p id="p-0005" num="0004">Specifically, the X-ray CT scanner exposes X-rays onto the test object from different directions multiple times, extracts the X-rays penetrating the test object to an X-ray detector, and collectes multiple detection data. The collected detection data is A/D-converted in a data collection unit, then sent to a console apparatus. The console apparatus carries out pretreatment, etc. on the detected data and produces projected data. Then, the console apparatus carries out reconfiguration processing based on the projected data and produces volume data based on tomographic image data or a plurality of tomographic image data. The volume data is data set expressing a 3-dimensional distribution of a CT number corresponding to a 3-dimensional region of the test object.</p>
<p id="p-0006" num="0005">Moreover, the X-ray CT system includes an apparatus such as a multi-slice X-ray CT system that can carry out high-definition (high resolution) imaging over a wide range per unit time. This multi-slice X-ray CT system uses detector elements in m column in an anteroposterior direction and the n rows in the direction orthogonally intersecting the anteroposterior direction as the detector used in the single slice X-ray CT system, that is, a two-dimensional detector of a configuration with the m columns and n rows arranged.</p>
<p id="p-0007" num="0006">Due to such a multi-slice X-ray CT system, the larger a detector is (the greater the number of detector elements configuring the detector), the greater the possibility of acquiring projection data over a wider region in a single image. In other words, by imaging with a multi-slice X-ray CT system provided with such a detector over time, it is possible to generate volume data for a specific site at a high frame rate (hereinafter, sometimes referred to as a &#x201c;Dynamic Volume scan&#x201d;). This makes it possible for an operator to assess the movement of the specific region within a unit of time by means of three-dimensional images.</p>
<p id="p-0008" num="0007">In addition, a medical image processing apparatus exists that reconstructs volume data and generates medical images from the volume data based on projected data obtained by the X-ray CT system.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. 1A</figref> is a block diagram showing the configuration of the medical image processing apparatus according to the present embodiment.</p>
<p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. 1B</figref> is a block diagram showing the detailed configuration of the image processor.</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 2A</figref> explains the analysis of the positional relation in bones.</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 2B</figref> explains the analysis of the positional relation in bones.</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 2C</figref> explains the analysis of the positional relation in bones.</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 2D</figref> explains the analysis of the positional relation in bones.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 2E</figref> explains the analysis of the positional relation in bones.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 2F</figref> explains the analysis of the positional relation in bones.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 2G</figref> explains the analysis of the positional relation in bones.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 3A</figref> is a flow chart showing a series of operations of the medical image processing apparatus pertaining to the present embodiment.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 3B</figref> is a flow chart showing the operations related to the analysis of the positional relation of Embodiment 1.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 3C</figref> is a flow chart showing the operations related to the analysis of the positional relation of Embodiment 2.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 4A</figref> explains the analysis of the shape based on the surface layer of the test object.</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 4B</figref> explains the analysis of the shape based on the surface layer of the test object.</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 5</figref> is a flow chart showing a series of operations of the medical image processing apparatus according to the present embodiment.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0024" num="0023">The purpose of the embodiment is to provide a medical image processing apparatus capable of changing the reconstruction conditions according to the positional relation of the observation subject and generating the image data based on the projected data over time obtained by the X-ray CT system. Moreover, another purpose is to provide the X-ray CT system allowing to control the operation according to acquiring the projected data in correspondence with the positional relation of the observation subject during acquiring of the projected data.</p>
<p id="p-0025" num="0024">The meaning of &#x201c;in correspondence with the positional relation of the observation subject&#x201d; includes a case of high degree of correspondence as a result of matching by means of external images obtained by imaging the observation subject, for example. It is intended to allow generation of image data by changing reconfiguration conditions and/or to allow controlling actions accompanied by obtaining projection data. These external images may be projection data, perspective data through computed tomography or angiography, or data obtained by videography.</p>
<p id="p-0026" num="0025">The present embodiment pertains to a medical image processing apparatus comprising a photographing unit, a reconfiguration processing unit, an extracting unit, and an analyzing unit in order to achieve the purposes. The photographing unit scans flexible site of the living body configured from multiple parts and acquires the projected data. The reconfiguration processing unit carries out reconfiguration processing on the projected data and generates the image data of the flexible site regarding plurality of timing points. The extracting unit extracts a plurality of components configuring the flexible sites from the respective image data. The analyzing unit obtains the positional relation of the plurality of components configuring the extracted flexible site regarding a plurality of timing point and conditionally determines the obtained positional relation, thereby specifying the image data of the specific timing corresponding with the determination result.</p>
<heading id="h-0006" level="1">Embodiment 1</heading>
<p id="p-0027" num="0026">The medical image processing apparatus according to Embodiment 1 reconstructs the projected data obtained from the X-ray CT system to generate the volume data, and generates medical images based on the volume data. Hereinafter, the configuration of the medical image processing apparatus according to the present embodiment will be described with reference to <figref idref="DRAWINGS">FIG. 1A</figref> and <figref idref="DRAWINGS">FIG. 1B</figref> together with the configuration of the X-ray CT system. As shown in <figref idref="DRAWINGS">FIG. 1A</figref>, the medical image displaying apparatus according to the present embodiment operates together with the X-ray photographing unit <b>500</b> (that is, the X-ray CT system), and comprises a projected data storage <b>13</b>, a reconfiguration processing unit <b>14</b>, an image data storage <b>10</b>, an image processor <b>20</b>, a display controller <b>30</b>, and a U/I <b>40</b>. In addition, the U/I <b>40</b> is a user interface that includes a display <b>401</b> and an operation part <b>402</b>.</p>
<p id="h-0007" num="0000">(X-Ray Photographing Unit <b>500</b>)</p>
<p id="p-0028" num="0027">The X-ray photographing unit <b>500</b> is explained as an example of the medical image processing apparatus allowing to obtain three-dimensional data in the same manner as, for example, CT, MRI, ultrasonic diagnostic equipment, etc. The X-ray photographing unit <b>500</b> comprises a gantry <b>1</b>, a high-voltage generator <b>7</b>, an X-ray controller <b>8</b>, and a gantry/bed controller <b>9</b>. The gantry <b>1</b> comprises a rotating ring <b>2</b>, an X-ray source (X-ray generating unit) <b>3</b>, an X-ray filter <b>4</b>, an X-ray detector <b>5</b>, a data accumulating unit <b>11</b>, a pretreating unit <b>12</b>, and a scan control <b>501</b>. The X-ray detector <b>5</b> is an array type X-ray detector. That is, a channel-wise m row and a slice-wise n column of detecting elements are arranged in a matrix regarding the X-ray detector <b>5</b>. The X-ray source <b>3</b> and the X-ray detector <b>5</b> are installed on the rotating ring <b>2</b>, facing each other while sandwiching the test object (not illustrated) laid on a sliding bed <b>6</b>. Respective channels are associated with the respective detecting elements configuring the X-ray detector <b>5</b>. The X-ray source <b>3</b> faces the test object via the X-ray filter <b>4</b>. When a trigger signal is supplied from the X-ray controller <b>8</b>, the high-voltage generator <b>7</b> drives the X-ray source <b>3</b>. The high-voltage generator <b>7</b> adds high voltage to the X-ray source <b>3</b> upon receiving the trigger signal. Thereby, X-rays are generated in the X-ray source <b>3</b> and the gantry/bed controller <b>9</b> synchronously controls the rotation of the rotating ring <b>2</b> of the gantry <b>1</b> and the slide of the sliding bed.</p>
<p id="p-0029" num="0028">The scan control <b>501</b> configures a central control of all systems and controls the X-ray controller <b>8</b>, the gantry/bed controller <b>9</b>, as well as the sliding bed <b>6</b> based on conditions for acquiring the projected data specified in advance (hereinafter, may be referred to as the &#x201c;scanning conditions&#x201d;). That is, the scan control <b>501</b> rotates the rotating ring <b>2</b> along a predetermined course around the periphery of the test object while irradiating X-rays from the X-ray source <b>3</b>. Furthermore, the resolution and resolving power of the projected data are determined based on the scanning conditions determined in advance. In other words, the scanning conditions are determined in advance according to the required resolution and resolving power, while the scan control <b>501</b> controls the movement of the respective parts according to the scanning conditions. By means of the resolving power of the projected data generated according to the scanning conditions (that is, the frame rate) and resolution, the maximum frame rate and resolution of the reconstructed image data is determined by the reconfiguration processing unit <b>14</b> mentioned later.</p>
<p id="h-0008" num="0000">(In the Case of Changing the Scanning Conditions or the Scan Stopping Process)</p>
<p id="p-0030" num="0029">Moreover, the scan control <b>501</b> is instructed from the position analyzing part <b>212</b> of the image processor <b>20</b> to change the scanning conditions or stop the processes related to acquiring the projected data (hereinafter, may be referred to as a &#x201c;scan&#x201d;). The image processor <b>20</b> and the position analyzing part <b>212</b> are described later. When instructions are given to change the scanning conditions, the scan control <b>501</b> changes the scanning conditions to other conditions determined in advance, which are different from the scanning conditions prior to instructions being given. Thereby, for example, before receiving instructions, the resolving power and resolution are lowered in order to acquire rough projected data, and once the instructions are received, the resolving power and resolution are increased more than before receiving instructions in order to obtain the projected data. Thereby, the rough projected data is received until instructions are received, and regarding the operation following receiving instructions (that is, the operation requiring attention), the projected data may be acquired at a level in which further careful movements may be observed. Furthermore, the projected data prior to receiving instructions may realize a resolving power and resolution allowing for analysis processing by the image processor <b>20</b> mentioned later. That is, if the conditions are satisfied, the scanning conditions may have lower resolving power and resolution settings than the scanning conditions following receiving instructions.</p>
<p id="p-0031" num="0030">Moreover, when the scanning is instructed to stop, the scan control <b>501</b> controls the X-ray controller <b>8</b>, the gantry/bed controller <b>9</b>, and the sliding bed <b>6</b> to stop scanning. Thereby, the scan control <b>501</b> may automatically stop scanning with the instructions as the trigger.</p>
<p id="h-0009" num="0000">(Commonalities in the Case of Changing the Frame Rate, Changing the Scanning Conditions, and the Scan Stopping Process)</p>
<p id="p-0032" num="0031">The detected elements configuring the X-ray detector <b>5</b> may measure the strength of X-rays generated by the X-ray source <b>3</b> regarding both cases of when the test object is interpositioned between the X-ray source <b>3</b> and the detected element, and when it is not interpositioned. Accordingly, the respective detected elements measure the intensity of at least one X-ray and output an analog output signal corresponding to the intensity. The signals output from the detected element are classified into columns by time sharing in the data accumulating unit <b>11</b> and then read (that is, successively accumulated).</p>
<p id="p-0033" num="0032">The data accumulating unit <b>11</b> comprises an integrated amplifier and an A/D converter. The electric signals from the respective detected element comprised in the data accumulating unit <b>11</b> are time-shared via a common integrated amplifier and then converted into digital data by the A/D converter. The data accumulating unit <b>11</b> outputs to the pretreating unit <b>12</b> the signals from the detected elements converted to digital data.</p>
<p id="p-0034" num="0033">The pretreating unit <b>12</b> carries out processes such as correction by sensitivity, etc. on the digital data sent from the data accumulating unit <b>11</b>, realizing the projected data. The pretreating unit <b>12</b> associates the projected data with the column, which is the read-out element of the digital data, which is the generating element thereof, and stores it in the projected data storage <b>13</b>. The projected data storage <b>13</b> is the storage for storing the acquired projected data.</p>
<p id="p-0035" num="0034">Furthermore, the pretreating unit <b>12</b> may supplement identification information showing the timing point (hereinafter, referred to as a &#x201c;notification flag&#x201d;) to the projected data when instructions to change the scanning conditions are sent from the scan control <b>501</b>. Thereby, the reconfiguration processing unit <b>14</b> functioning in the latter part may specify the timing point in which the scanning condition was changed within the projected data based on the notification flag.</p>
<p id="h-0010" num="0000">(Reconfiguration Processing Unit <b>14</b>)</p>
<p id="p-0036" num="0035">The reconfiguration processing unit <b>14</b> reads out the projected data stored in the projected data storage <b>13</b>. The reconfiguration processing unit <b>14</b> uses, for example, a reconstruction algorithm referred to as the Feldkamp method to back project the read project data in order to generate the image data (tomographic image data and volume data). Any method may be adopted to reconstruct the tomographic image data, such as, for example, the 2-dimensional Fourier transformation method, convolution back projection method, etc. The volume data is prepared by interpolation processing the plurality of reconstructed tomographic image data. Any method may be adopted for reconstructing the volume data such as, for example, the cone beam reconstructing method, multi-slice reconstructing method, expanded reconstructing method, etc. As mentioned above, extensive volume data may be reconstructed by means of a volume scan using an X-ray detector with many columns, as mentioned above. Moreover, when carrying out CT examination, the accumulation rate of the detected data is shortened; therefore, the reconstruction time by the reconfiguration processing unit <b>14</b> is shortened. Accordingly, the real time image data corresponding to the scan may be prepared. Hereinafter, the volume data is referred to as &#x201c;image data.&#x201d;</p>
<p id="p-0037" num="0036">The image data according to the present embodiment is reconstructed so as to be capable of extracting bones as an example of the flexible site. The flexible site is explained exemplifying a part configured by two bones as well as a joint connecting these bones. The joint is a joint connecting the bones and including joint fluid, a synovial, and a joint capsule. Further, the side of the bone connected through the joint has cartilage and by means of this cartilage, the flexible site can be smoothly moved. In other words, this bone also includes cartilage. In addition, this flexible site comprises a plurality of construction sites, and in the above case, these construction sites include two bones to be connected by the joint. Muscles are included as an example of flexible site in addition to bones.</p>
<p id="p-0038" num="0037">In this manner, the reconfiguration processing unit <b>14</b> carries out reconfiguration processing on the read projected data based on the reconstruction conditions determined in advance, and generates or reconstructs image data (that is, volume data) for each timing point based on the reconstructing conditions (that is, the predetermined volume rate). Furthermore, the timing point for reconstructing the image data is synchronized with the timing point for obtaining the projected data (that is, the resolving power related to acquiring the projected data). Specifically, a time lag is present between the timing point in which the projected data for generating the image data from the projected data is acquired and the timing point in which the image data based on the projected data corresponding with the timing is reconstructed. However, the process related to the reconstruction is at a high speed compared to the movement of the test object (for example, the activity of moving the arms and legs), and in the medical image processing apparatus related to the present embodiment, the time lag is at a level in which it is negligible. Furthermore, when the time lag is considered, the timing for carrying out the process based on the reconstructed image data (for example, process of the position analyzing part <b>212</b> mentioned later) may be adjusted based on the time lag.</p>
<p id="p-0039" num="0038">Furthermore, the X-ray CT scanner related to the embodiment analyses the reconstructed image data, thereby comprehending the position and angle of the respective parts configuring an observation subject as well as the relative positional relation thereof (hereinafter, this is generally simply referred to as a &#x201c;positional relation&#x201d;). Therefore, the reconfiguration processing unit <b>14</b> reconstructs the image data for analysis separately from the image data for display. Specifically, the reconfiguration processing unit <b>14</b> successively reads the acquired projected data from the projected data storage <b>13</b> in parallel with the processing related to acquiring the projected data using the X-ray photographing unit <b>500</b>. The reconfiguration processing unit <b>14</b> carries out reconfiguration processing on the read projected data based on the reconstruction conditions for analysis determined in advance, thereby generating the image data for analysis for each timing point based on the reconstruction conditions.</p>
<p id="p-0040" num="0039">In the present embodiment, the reconstruction conditions for reconstructing the image data for analysis are configured such that the bones in the test object may be extracted from the projected data. That is, the image data is reconstructed so as to be capable of extracting bones. Furthermore, the bones also include cartilage. Moreover, the reconstruction conditions at this time are referred to as the &#x201c;first conditions,&#x201d; and the image data generated based on the reconstruction conditions may be referred to as the &#x201c;first image data.&#x201d; The reconfiguration processing unit <b>14</b> stores the image data for analysis successively generated for each of the timing points in the image data storage <b>10</b>. The image data storage <b>10</b> is the storage for storing the image data.</p>
<p id="p-0041" num="0040">Furthermore, the positional relation of components of the flexible site such as bones, etc. was described using the 2-dimensional positional relation between two bones as an example; however, the positional relation may be 3-dimensionally shaped in some cases. The example described a case when the first bone is pointing up and the second bone is pointing right, and when the second bone is pointing to the upper right with respect to this. However, a case may be considered in which the movement of the bone shifts in the rotational direction by adding a twist, etc., in addition to the movement in the 2-dimensional direction. A case may also be considered in which the position of the second bone does not move with respect to the first bone regardless of the rotation of the second bone. Accordingly, the positional relation of the components of the flexible site may be 3-dimensionally grasped, the movement in the 3-dimensional rotational direction may be obtained from among the changes in the shape characteristics of three points and the shape feature of two points, thereby the amount of change in the positional relation is also obtained regarding the twisting, and the determination process with respect to the amount of change may be carried out. The determination process itself with respect to the amount of change is the same as in the case of the 2-dimensional positional relation.</p>
<p id="h-0011" num="0000">(In the Case of Changing Scanning Conditions and the Scan Stopping Process)</p>
<p id="p-0042" num="0041">Moreover, the reconfiguration processing unit <b>14</b> reads the projected data from the projected data storage <b>13</b> and carries out reconfiguration processing based on the reconstruction conditions for display that have been determined in advance, thereby generating the image data for display for each timing point based on the reconstruction conditions. Furthermore, when the scanning conditions are changed while acquiring the projected data, the reconfiguration processing unit <b>14</b> may change the reconstruction conditions before and after changing the scanning conditions and reconstruct the image data for display. In this case, the reconfiguration processing unit <b>14</b> may specify the timing point at which the scanning conditions were changed based on the notification flag supplemented to the projected data. By means of operating in this manner, the reconfiguration processing unit <b>14</b> may reconstruct the image data at an increased volume rate and resolution compared to before changing regarding the image data following the change in the scanning conditions. Furthermore, hereinafter, the reconstruction conditions for display may be referred to as the &#x201c;second conditions,&#x201d; and the image data generated based on the reconstruction conditions may be referred to as the &#x201c;second image data.&#x201d; The reconfiguration processing unit <b>14</b> stores the image data for display that has been generated for each of the timing points in the image data storage <b>10</b>.</p>
<p id="p-0043" num="0042">Furthermore, the image data for display does not necessarily need to be operated in parallel with the processing related to acquiring the projected data. For example, the reconfiguration processing unit <b>14</b> may reconstruct the image data for display after acquiring a series of projected data.</p>
<p id="p-0044" num="0043">Moreover, the first image data may have the reconstruction conditions allowing analysis processing by the image processor <b>20</b> mentioned later. That is, as long as the conditions are satisfied, for example, the volume rate of the first image data may be lower than the volume data at generating the image data for display. Moreover, the resolution of the first image data may be lower than the resolution of the image data for display. The processing load during analysis may be reduced by means of operating in this manner.</p>
<p id="h-0012" num="0000">(In the Case of Changing the Frame Rate)</p>
<p id="p-0045" num="0044">Furthermore, the reconfiguration processing unit <b>14</b> first generates image data for analysis and outputs this to the image processor <b>20</b>; then, it receives the analysis results from the image processor <b>20</b> and generates image data for display. Details of the image processor <b>20</b> are described later. Detailed operations of the reconfiguration processing unit <b>14</b> are described in the following. Furthermore, the maximum volume rate and resolution of the reconstructed image data become the resolving power (that is, the frame rate) and resolution of the projected data. Accordingly, in the medical image processing apparatus related to the present embodiment, the projected data must be acquired under conditions allowing for realization of the volume rate and resolution of the image data for analysis and displaying.</p>
<p id="p-0046" num="0045">The reconfiguration processing unit <b>14</b> first carries out reconfiguration processing with respect to the read projected data based on the reconstruction conditions for analysis determined in advance, and then generates image data for each timing point based on the reconstruction conditions. Furthermore, in the present embodiment, the reconstruction conditions are configured such that the bones in the test object may be extracted from the projected data. That is, the image data is reconstructed so as to be capable of extracting bones. Furthermore, the bones also include cartilage. Moreover, the image data generated based on the reconstruction conditions at this time corresponds with the &#x201c;first image data.&#x201d; The reconfiguration processing unit <b>14</b> stores image data for analysis successively generated for each of the timing points in the image data storage <b>10</b>. The image data storage <b>10</b> is the storage for storing image data. Furthermore, the first image data may have reconstruction conditions allowing for analysis processing by the image processor <b>20</b> described later. That is, if the conditions are satisfied, for example, the volume rate of the first image data may be lower than the volume data upon generation of the image data for display. Moreover, the resolution of the first image data may be lower than the resolution of the image data for display. The processing load during analysis may be reduced by means of operating in this manner.</p>
<p id="p-0047" num="0046">Moreover, the reconfiguration processing unit <b>14</b> receives a notification of a time width comprising a plurality of timing points from the position analyzing part <b>212</b>. The time width corresponds with a part in a series of time widths in which the first image data is generated based on the reconstruction conditions. The reconfiguration processing unit <b>14</b> carries out reconfiguration processing while changing the reconstruction conditions between the notified time width and another time width, and reconstructs the image data for display. The reconstruction conditions may be stored in the reconfiguration processing unit <b>14</b> in advance, or may be made allowing for an operator to specify via an operation part <b>402</b>. For example, it is advisable to determine the reconstruction conditions such that the volume rate becomes higher than other time widths regarding the specified time width. Moreover, without limiting the volume rate, the reconstruction conditions may be determined such that, for example, the resolution of the specified time width becomes higher compared to other time frames. Furthermore, the image data generated based on the reconstruction conditions with respect to the specified time width corresponds with the &#x201c;second image data.&#x201d; The reconfiguration processing unit <b>14</b> stores the image data for analysis successively generated for each of the timing points in the image data storage <b>10</b>. By means of changing the reconstruction conditions of a part of the time width and generating the image data, for example, when observing the chronologically operating test object, the image data may be generated at a higher volume rate than other areas regarding one area in the series of operations (for example, an area requiring attention). That is, regarding the one area, the medical image may be generated and displayed at a higher frame rate than other areas based on the chronologically generated image data.</p>
<p id="p-0048" num="0047">Furthermore, regarding the other time width, the first image data generated for analysis may be operated such that it may be used for display as well. For example, if the observation subject may be specified by the object extracted for analysis and the volume data is also the same, there is no need to generate the image data for display again. In such cases, the reconfiguration processing unit <b>14</b> should only reconstruct image data for display regarding the specified time width. In other words, the reconfiguration processing unit <b>14</b> should only reconstruct the second projected data as the image data for display. Moreover, the reconfiguration processing unit <b>14</b> may be operated such that it only reconstructs the image data for display regarding only the specified time width (in other words, only the specified time width becomes the display subject). The operations should be appropriately changed depending on the operation. Furthermore, the reconfiguration processing unit <b>14</b> may supplement different identifying information regarding the image data for analysis and the image data for display to the respective image data, thereby storing these in the image data storage <b>10</b> allowing for differentiation.</p>
<p id="h-0013" num="0000">(Commonalities in Changing the Frame Rate, Changing the Scanning Conditions, and the Scan Stopping Process)</p>
<p id="p-0049" num="0048">An X-ray CT scan is used as an example for describing the present embodiment, but an MRI scan may be used for the present embodiment instead of an X-ray CT scan. An MRI scan uses the nuclear magnetic resonance (NMR) phenomenon to magnetically activate a nuclear spin in the desired inspection part of the test object placed in the magnetostatic field using a high frequency signal of Larmor frequency, calculates the density distribution, the distribution of relaxation time, etc. based on FID (free induction decay) signals and echo signals generated along with the activation, and displays the image of any section of the test object from the measured data.</p>
<p id="h-0014" num="0000">(Image Processing Unit <b>20</b>)</p>
<p id="p-0050" num="0049">The image processor <b>20</b> includes a configuration extracting part <b>21</b>, an image processor <b>22</b>, and an image storage <b>23</b>.</p>
<p id="h-0015" num="0000">(Configuration Extracting Part <b>21</b>)</p>
<p id="p-0051" num="0050">The configuration extracting part <b>21</b> includes an object extracting part <b>211</b>, and a position analyzing part <b>212</b>. The configuration extracting part <b>21</b> successively reads the image data for analysis, which is generated by the reconfiguration processing unit <b>14</b> at each timing point and stored in the image data storage <b>10</b>, from the image data storage <b>10</b>. At this time, operations by the reconfiguration processing unit <b>14</b> and operations related to reading the image data for analysis to the configuration extracting part <b>21</b> may be synchronized. The configuration extracting part <b>21</b> successively outputs the first image per read-out timing to the object extracting part <b>211</b>, and instructs extraction of the object from the first image data.</p>
<p id="p-0052" num="0051">The object extracting part <b>211</b> successively receives image data for each timing point from the configuration extracting part <b>21</b>. The object extracting part <b>211</b> according to the present embodiment extracts bone parts and as objects based on the voxel data in this image data. Here, <figref idref="DRAWINGS">FIG. 2A</figref> is referred. <figref idref="DRAWINGS">FIG. 2A</figref> is a diagram for explaining analysis of the positional relation in bones, and shows an example when bone objects forming arm regions are extracted. As shown in FIG. <b>2</b>A, the object extracting part <b>211</b> extracts bone objects M<b>11</b>, M<b>12</b>, and M<b>13</b>, forming arm regions, from the first image data. In this manner, the object extracting part <b>211</b> extracts the bone objects for each of the first image data at each timing point. The object extracting part <b>211</b> outputs information indicating bone objects (for example, information indicating the form, the position, and the size of the object) extracted regarding the first image data of each timing point (in other words, extracted at each timing point) to the position analyzing part <b>212</b>, while relating them with the information showing the corresponding timing. Furthermore, the object extracting part <b>211</b> corresponds to &#x201c;an extracting part&#x201d;.</p>
<p id="p-0053" num="0052">The position analyzing part <b>212</b> receives information showing the bone object at each timing point from the object extracting part <b>211</b>. Or, it successively receives information showing the bone object extractor at each timing point. The position analyzing part <b>212</b> analyzes the positional relation of the bone of each timing point based on the information. A detailed example thereof is explained in the following.</p>
<p id="p-0054" num="0053">The position analyzing part <b>212</b> first specifies at least two or more objects (that is, the objects subject to observation) to be used for analyzing the positional relation from among bone objects M<b>11</b>, M<b>12</b>, and M<b>13</b>. Specifically, for example, the position analyzing part <b>212</b> stores in advance the known bio-information of each part configuring a living body (for example, information showing the positional relation of bones configuring the upper arm and lower arm), and specifies the object based on the bio-information. Moreover, as another method, the position analyzing part <b>212</b> stores in advance the information showing the shape of the object subject to observation, and specifies the object corresponding to this shape as the object subject to observation. Hereinafter, the position analyzing part <b>212</b> is explained assuming that the objects M<b>11</b> and M<b>13</b> have been specified.</p>
<p id="p-0055" num="0054">When the object subject for analysis M<b>11</b> and M<b>13</b> are specified, the position analyzing part <b>212</b> extracts at least three portions having characteristics in its shape (hereinafter, referred to as &#x201c;shape characteristics&#x201d;) from the respective objects. For example, as shown in <figref idref="DRAWINGS">FIG. 2A</figref>, the position analyzing part <b>212</b> extracts the shape characteristics M<b>111</b>, M<b>112</b>, and M<b>113</b> from the object M<b>11</b>. The position analyzing part <b>212</b> extracts the shape characteristics M<b>131</b>, M<b>132</b>, and M<b>133</b> from the object M<b>13</b>.</p>
<p id="p-0056" num="0055">Next, the position analyzing part <b>212</b> forms planes for grasping the positions and directions of respective objects by portions (namely, points) indicating the extracted three shape characteristics, associating the plane with an object that is the origin for extracting the shape characteristics. Here, <figref idref="DRAWINGS">FIG. 2B</figref> will be referred. <figref idref="DRAWINGS">FIG. 2B</figref> is a diagram explaining the analysis of the positional relation of the bone, and shows the plane shaped based on the shape characteristics shaped from the respective objects M<b>11</b> and M<b>13</b>. As shown in <figref idref="DRAWINGS">FIG. 2B</figref>, the position analyzing part <b>212</b> shapes the plane P<b>11</b> by means of the shape characteristics M<b>111</b>, M<b>112</b>, and M<b>113</b>, and associates this with the object M<b>11</b>. In the same manner, the position analyzing part <b>212</b> shapes the plane P<b>13</b> by means of the shape characteristics M<b>131</b>, M<b>132</b>, and M<b>133</b>, and associates this with the object M<b>13</b>.</p>
<p id="p-0057" num="0056">Movement of the joint changes the position and direction of each of a plurality of bones constructing the joint and their relative positional relations (hereinafter, sometimes they are simply referred to as &#x201c;positional relations&#x201d;); however, the shape and the size of each bone are not changed. In other words, positional relation of the objects M<b>11</b> and M<b>13</b> extracted at each timing point are changed along the time sequence; however, the shape and the size of each object are not changed. The same applies to the planes P<b>11</b> and P<b>13</b> extracted based on the shape characteristic of each object. The position analyzing part <b>212</b> according to the present embodiment uses the characteristic and identifies the positional relation of the objects M<b>11</b> and M<b>13</b> based on the position and direction of the respective planes P<b>11</b> and P<b>13</b>. Thus, by shaping the plane from each object in this manner, there is no need to carry out complicated analysis of the shape in order to grasp the position and direction of the object. Accordingly, the processing load of the position analyzing part <b>212</b> identifying the positional relation of the objects M<b>11</b> and M<b>13</b> may be reduced.</p>
<p id="p-0058" num="0057">Here, <figref idref="DRAWINGS">FIG. 2C</figref> is referred to. <figref idref="DRAWINGS">FIG. 2C</figref> is a diagram explaining the analysis of the positional relation of bones, and is an example showing the positional relation of the objects M<b>11</b> and M<b>13</b> shown in <figref idref="DRAWINGS">FIG. 2A</figref> and <figref idref="DRAWINGS">FIG. 2B</figref> on planes P<b>11</b> and P<b>13</b>. The position analyzing part <b>212</b> specifies the relative positional relation of the objects M<b>11</b> and M<b>13</b> based on the angle configured by the planes P<b>11</b> and P<b>13</b>. Moreover, the position analyzing part <b>212</b> may specify the relative positional relation of the objects M<b>11</b> and M<b>13</b> based on the distance between the planes P<b>11</b> and P<b>13</b> by replacing the angle. Furthermore, hereinafter, the position analyzing part <b>212</b> is explained assuming that it specifies the positional relation of the objects M<b>11</b> and M<b>13</b> based on the planes P<b>11</b> and P<b>13</b>.</p>
<p id="p-0059" num="0058">In this manner, the position analyzing part <b>212</b> specifies the positional relation of the objects M<b>11</b> and M<b>13</b> based on the planes P<b>11</b> and P<b>13</b> extracted at each timing point. Here, <figref idref="DRAWINGS">FIG. 2D</figref> will be referred. <figref idref="DRAWINGS">FIG. 2D</figref> is a diagram explaining the analysis of the positional relation of the bones and illustrates an example of the positional relation of the planes P<b>11</b> and P<b>13</b> at plural timing points. Furthermore, the example of <figref idref="DRAWINGS">FIG. 2D</figref> shows the change in position and direction of the plane P<b>13</b> at each timing point assuming that the position and direction of the plane P<b>11</b> (namely, the object M<b>11</b>) is the same, in order to make the explanation easier to comprehend. P<b>13</b><i>a </i>to P<b>13</b><i>d </i>in <figref idref="DRAWINGS">FIG. 2D</figref> are showing the plane P<b>13</b> corresponding to different timing points, respectively.</p>
<p id="p-0060" num="0059">In addition, the position analyzing part <b>212</b> is not limited to the above-described method based on the planes P<b>11</b> and P<b>13</b> if the position and direction of the standard objects M<b>11</b> and M<b>13</b> may be specified. For example, the position and direction of each object may be specified based on the outline of the objects M<b>11</b> and M<b>13</b>. In this case, the position analyzing part <b>212</b> specifies a three-dimensional positional relation. Moreover, when specifying the two-dimensional positional relation, a line connecting at least two shape characteristics may be extracted from the objects M<b>11</b> and M<b>13</b>, and the positional relation may be specified based on the two extracted lines. For example, as showing in <figref idref="DRAWINGS">FIG. 2C</figref> and <figref idref="DRAWINGS">FIG. 2D</figref>, the line P<b>111</b> is extracted based on the shape characteristics M<b>111</b> and M<b>113</b>. Moreover, the line P<b>131</b> is extracted based on the shape characteristics M<b>132</b> and M<b>133</b>. The position analyzing part <b>212</b> can identify the two-dimensional positional relation of the objects M<b>11</b> and M<b>13</b> from the lines P<b>111</b> and P<b>113</b> extracted in this manner. In addition, the position and direction may be identified by carrying out alignment of the object itself based on pixel value information from the voxel construction of an object using Mutual Information. For example, based on the distribution of the pixel value information (information showing shading), it is possible to identify the position and direction of the object</p>
<p id="h-0016" num="0000">(In the Case of Changing the Frame Rate)</p>
<p id="p-0061" num="0060">When the positional relation of the objects M<b>11</b> and M<b>13</b> are specified regarding a series of timing points, the position analyzing part <b>212</b> determines whether or not the specified positional relation is included in the predetermined range (satisfies the conditions determined in advance), and specifies the timing included in the range. This predetermined range may be determined in advance based on, for example, the positional relation requiring attention from among the series of movements of the observation subject. The position analyzing part <b>212</b> specifies the time width shaped by the specified timing point (in other words, the time width in which the positional relation of the objects M<b>11</b> and M<b>13</b> satisfy the predetermined conditions). The processing related to the specifications of the time width is explained using a detailed example in the following with reference to <figref idref="DRAWINGS">FIG. 2E</figref>. <figref idref="DRAWINGS">FIG. 2E</figref> is a diagram explaining the time width specified by the position analyzing part <b>212</b>.</p>
<p id="p-0062" num="0061">The planes P<b>13</b><i>e</i>, P<b>13</b><i>f</i>, and P<b>13</b><i>g </i>in <figref idref="DRAWINGS">FIG. 2E</figref> respectively show the position of the plane P<b>13</b> corresponding to the object M<b>13</b> when the positional relation of the objects M<b>11</b> and M<b>13</b> is included in the predetermined range. That is, this indicates that in the time width T<b>21</b> from the timing point corresponding to the plane P<b>13</b><i>f </i>to the timing point corresponding to the plane P<b>13</b><i>g</i>, the positional relation of objects M<b>11</b> and M<b>13</b> is included in the predetermined range. The position analyzing part <b>212</b> specifies the time width T<b>21</b>. Furthermore, T<b>11</b> and T<b>12</b> in <figref idref="DRAWINGS">FIG. 2E</figref> show the time widths other than the time width T<b>21</b> from among the series of time widths. Furthermore, the explanations hereinafter are provided under the assumption that the position analyzing part <b>212</b> specified the time width T<b>21</b> shown in <figref idref="DRAWINGS">FIG. 2E</figref>.</p>
<p id="p-0063" num="0062">Furthermore, when the positional relation is specified based on the angle configured by the planes P<b>11</b> and P<b>13</b>, the position analyzing part <b>212</b> determines whether or not the angle satisfies the condition of the angle determined in advance, and specifies the timing point corresponding to the angle satisfying the condition. This range may be stored in the position analyzing part <b>212</b> in advance as an absolute value.</p>
<p id="p-0064" num="0063">Moreover, the position analyzing part <b>212</b> may specify the flexible range of each object based on the information showing the bone object at each timing point, and specify the relative value with respect to the flexible range. In such cases, the angle to become the standard of the range (absolute value) is stored in the position analyzing part <b>212</b> in advance. The position analyzing part <b>212</b> should specify the timing point corresponding to the standard, and specify the range of the predetermined width including the predetermined timing point. For example, if the plane P<b>13</b><i>e </i>in <figref idref="DRAWINGS">FIG. 2E</figref> satisfies the condition of the angle to be the standard, the position analyzing part <b>212</b> first specifies the plane P<b>13</b><i>e</i>. Upon doing so, the position analyzing part <b>212</b> should specify the planes P<b>13</b><i>f </i>and P<b>13</b><i>g </i>shaping the range of the predetermined width with the plane P<b>13</b><i>e </i>as the standard, and specify the time width T<b>21</b> shaped at the timing point corresponding to the plane.</p>
<p id="p-0065" num="0064">Moreover, it is possible to specify the timing point that becomes the standard for the operator to specify the time width T<b>21</b> via the operation part <b>402</b>. In this case, the configuration extracting part <b>21</b> outputs the series of first image data to the image processor <b>22</b>, generate medical data regarding each timing point, and display this on the display <b>401</b> via the display controller <b>30</b>. The operation part <b>402</b> receives the selection of medical images to be the standard from among the displayed image data, and notifies the position analyzing part <b>212</b> regarding the timing point corresponding to the medical image. After receiving the notification, the position analyzing part <b>212</b> may specify the time width T<b>21</b> based on the notified timing point.</p>
<p id="p-0066" num="0065">Moreover, the timing point to become the standard for specifying the time width T<b>21</b> may be specified while acquiring the projected data and supplementing the information showing the timing point in the projected data, thereby specifying the standard based on the information. Specifically, for example, reaction of the test object is monitored using external equipment such as a microphone, camera, etc. while acquiring the projected data. If reaction is detected when the test object shows a predetermined reaction (for example, uttering a voice, etc.), the information showing the timing point thereof is supplemented to the projected data. When the projected data is reconstructed to generate the first image data for analysis, the reconfiguration processing unit <b>14</b> supplements information such that it may be differentiated from other image data with respect to the image data corresponding to the timing point comprising the timing point. Thereby, the position analyzing part <b>212</b> may specify the timing point that becomes the standard for specifying the time width T<b>21</b>.</p>
<p id="p-0067" num="0066">The position analyzing part <b>212</b> notifies the specified time width T<b>21</b> to the reconfiguration processing unit <b>14</b>. The reconfiguration processing unit <b>14</b> carries out reconfiguration processing while changing the reconstruction conditions between the notified time width T<b>21</b> and the other time widths T<b>11</b> and T<b>12</b>, reconstructing the image data for display for each timing point based on the reconstruction conditions. At this time, the reconfiguration processing unit <b>14</b> carries out reconfiguration processing regarding, for example, the time width T<b>21</b> in which the positional relation of the objects M<b>11</b> and M<b>13</b> is included in the predetermined range such that the volume rate becomes higher than the other time widths T<b>11</b> and T<b>12</b>. Thereby, the medical image may be generated and displayed at a frame rate higher than the time widths T<b>11</b> and T<b>12</b> regarding the time width T<b>21</b>. Moreover, without limiting the volume rate, the reconfiguration processing unit <b>14</b> may, for example, carry out reconfiguration processing at a higher resolution than the time widths T<b>11</b> and T<b>12</b> regarding the time width T<b>21</b>. In this manner, the reconfiguration processing unit <b>14</b> differentiates the time width T<b>21</b> from other time widths T<b>11</b> and T<b>12</b> so as to be capable of reconstructing the image data for display under respectively different reconstruction conditions.</p>
<p id="p-0068" num="0067">The reconfiguration processing unit <b>14</b> stores the series of reconstructed image data for display in the image data storage <b>10</b>. Furthermore, the position analyzing part <b>212</b> corresponds to the &#x201c;analyzing unit.&#x201d;</p>
<p id="p-0069" num="0068">When analysis of the positional relation of the objects M<b>11</b> and M<b>13</b> regarding the series of timing points are finished in this manner and the series of reconstructed image data for display is stored in the image data storage <b>10</b>, the configuration extracting part <b>21</b> reads and transfers it to the image processor <b>22</b>. Furthermore the time widths T<b>11</b> and T<b>12</b> may be operated such that the first image data generated for analysis may also be used for display. In this case, the position analyzing part <b>212</b> only reads the second projected data corresponding to the specified time width T<b>21</b> from the image data storage <b>10</b>. Subsequently, the position analyzing part <b>212</b> may replace the area corresponding to the time width T<b>21</b> from among the series of first image data read for analysis with the read second projected data, and transmit the series of image data to the image processor <b>22</b>.</p>
<p id="h-0017" num="0000">(In the Case of Changing the Scanning Conditions and the Scan Stopping Process)</p>
<p id="p-0070" num="0069">When the positional relation of the objects M<b>11</b> and M<b>13</b> is specified, the position analyzing part <b>212</b> determines whether or not the specified positional relation becomes the predetermined positional relation. The predetermined positional relation may be determined in advance based on, for example, the positional relation corresponding to the timing point desired to control the scan from among the series of movements of the observation subject (in other words, the positional relation corresponding to the timing point at which changing the scanning condition is desired or stopping the scan is desired).</p>
<p id="p-0071" num="0070">In this manner, the position analyzing part <b>212</b> successively analyzes the information showing the bone object successively output from the object extracting part <b>211</b> for each timing point, and determines whether or not the positional relation of the objects M<b>11</b> and M<b>13</b>, which are subject for observation, satisfies the predetermined conditions. Thereby, the position analyzing part <b>212</b> detects the timing point at which the objects M<b>11</b> and M<b>13</b> achieve the predetermined positional relation. Furthermore, this timing point corresponds to &#x201c;one timing point.&#x201d; When the position analyzing part <b>212</b> detects the timing point, it instructs the scan control <b>501</b> to change the scanning conditions or stop the scan. The scan control <b>501</b> receives the instructions and carries out the instructed operation (that is, change the scanning conditions or stopping the scan). Furthermore, whether changing the scanning conditions or stopping the scan is to be instructed should be associated in advance with the information showing the detected timing point (in other words, the positional relation corresponding to the timing point). A detailed example of the operation is explained in the following with reference to <figref idref="DRAWINGS">FIG. 2G</figref>. <figref idref="DRAWINGS">FIG. 2G</figref> is a diagram explaining an example of the timing point at which the position analyzing part <b>212</b> instructs the scan control <b>501</b> to change the scanning condition or stop the scan.</p>
<p id="p-0072" num="0071">The planes P<b>13</b><i>p</i>, P<b>13</b><i>q</i>, P<b>13</b><i>r</i>, and P<b>13</b><i>s </i>in <figref idref="DRAWINGS">FIG. 2G</figref> respectively show the position of the plane P<b>13</b> corresponding to the object M<b>13</b> at each timing point. Specifically, the plane P<b>13</b><i>p </i>in <figref idref="DRAWINGS">FIG. 2G</figref> corresponds to the timing point at which the scan is commenced. In the example of <figref idref="DRAWINGS">FIG. 2G</figref>, the position analyzing part <b>212</b> determines whether or not the positional relation of the objects M<b>11</b> and M<b>13</b> satisfy the predetermined conditions in the order of the planes P<b>13</b><i>p</i>, P<b>13</b><i>q</i>, P<b>13</b><i>r</i>, and P<b>13</b><i>s</i>. In this example, the position analyzing part <b>212</b> associates the operation related to the change in scanning conditions with the information showing the positional relation corresponding to the plane P<b>13</b><i>r</i>, and associates the operation related to returning the changed scanning condition with the information showing the positional relation corresponding to the plane P<b>13</b><i>s</i>. Moreover, the position analyzing part <b>212</b> associates the operation of stopping the scan with the information showing the positional relation corresponding to the plane P<b>13</b><i>r. </i></p>
<p id="p-0073" num="0072">Thereby, scanning is commenced from, for example, the timing point corresponding to the plane P<b>13</b><i>p</i>, and until the timing point corresponding to the plane P<b>13</b><i>r</i>, rough projected data is obtained under the scanning conditions of low resolving power as well as resolution. The position analyzing part <b>212</b> instructs the scan control <b>501</b> to change the scanning conditions when the timing point corresponding to the plane P<b>13</b><i>r </i>is detected. Thereby, from the timing point onwards, the projected data capable of observing detailed movements under the scanning conditions of high resolving power as well as resolution is obtained. Moreover, when the position analyzing part <b>212</b> detects the timing point corresponding to the plane P<b>13</b><i>s</i>, it instructs the scan control <b>501</b> to change back the altered scanning conditions to the basis (that is, change the scanning conditions again). Thereby, after the timing point onwards, rough projected data is reacquired. By means of operating in this manner, projected data is obtained that is capable of observing only detailed movements regarding the area of interest from among the series of movements of the observation subject, and regarding other timing points, it may be operated such that the projected data may be acquired at a rough level. This makes it possible to reduce the processing load related to acquiring the projected data.</p>
<p id="p-0074" num="0073">Moreover, when the position analyzing part <b>212</b> detects the timing point corresponding to the plane P<b>13</b><i>q</i>, it instructs the scan control <b>501</b> to stop the scan. The scan control <b>501</b> receives the instructions and stops the operation related to acquiring the projected data (that is, the scan). By means of operating in this manner, the X-ray CT system itself may stop scanning at the timing point at which the objects M<b>11</b> and M<b>13</b> reach the predetermined positional relation even without the operator instructing to stop the scan.</p>
<p id="p-0075" num="0074">Furthermore, the position analyzing part <b>212</b> does not necessarily need to give instructions to the scan control <b>501</b> at the detected timing. For example, the position analyzing part <b>212</b> may be operated such that it gives instructions to the scan control <b>501</b> from the detected timing point after a predetermined time has elapsed. In this manner, if it is operated such that it gives instructions to the scan control <b>501</b> based on the detected timing point, the operation of the position analyzing part <b>212</b> is not limited. Furthermore, when giving instructions to the scan control <b>501</b> at a timing point different from the detected timing point, needless to say, the timing point for giving instructions is after the detected timing point.</p>
<p id="p-0076" num="0075">When the second projected data reconstructed for display is stored in the image data storage <b>10</b>, the configuration extracting part <b>21</b> reads and transmits this to the image processor <b>22</b>. It may also be operated such that the first image data generated for analysis is used. In this case, the position analyzing part <b>212</b> should transfer the image data for analysis that has already been read to the image processor <b>22</b>.</p>
<p id="h-0018" num="0000">(Commonalities in Changing the Frame Rate, Changing the Scanning Conditions, and the Scan Stopping Process)</p>
<p id="p-0077" num="0076">In addition, if the positional relation of the bones can be analyzed, it is not always necessary for the whole images of respective bones such as the image of the upper arm and the lower arm to be taken as illustrated in <figref idref="DRAWINGS">FIGS. 2A to 2C</figref>. For example, <figref idref="DRAWINGS">FIG. 2F</figref> is showing the joint between the upper arm and the lower arm, and this example indicates an example in which the objects M<b>12</b> and M<b>13</b> are identified as the subject for analysis. In this case, the position analyzing part <b>212</b> extracts the shape characteristics M<b>121</b>, M<b>122</b>, and M<b>123</b> from the object M<b>12</b>. Moreover, the position analyzing part <b>212</b> extracts the shape characteristics M<b>134</b>, M<b>135</b>, and M<b>136</b> from the object M<b>13</b>. The position analyzing part <b>212</b> extracts the plane P<b>12</b> shaped by the shape characteristics M<b>121</b>, M<b>122</b>, and M<b>123</b>, and associates this to the object M<b>12</b>. In the same manner, the position analyzing part <b>212</b> extracts the plane P<b>13</b>&#x2032; shaped by the shape characteristics M<b>134</b>, M<b>135</b>, and M<b>136</b>, and associates this with the object M<b>13</b>. Hereinafter, the position analyzing part <b>212</b> identifies the positional relation of the objects M<b>12</b> and M<b>13</b> based on the positional relation of the planes P<b>12</b> and P<b>13</b>&#x2032;. If the position and direction of each bone and the relative positional relation may be identified in this manner based on the shape characteristics, the same process as the process mentioned above is possible even regarding cases in which the entire image of each parts are not photographed, as in <figref idref="DRAWINGS">FIG. 2F</figref>.</p>
<p id="h-0019" num="0000">(Image Processor <b>22</b>)</p>
<p id="p-0078" num="0077">The image processor <b>22</b> receives the series of images reconstructed at each predetermined timing point from the configuration extracting part <b>21</b>. The image processor <b>22</b> carries out image process to all image data for each timing point based on the image processing condition determined in advance, thereby respectively generating medical images.</p>
<p id="p-0079" num="0078">The image processor <b>22</b> causes image storage <b>23</b> to store the generated medical images and the information indicating a timing point corresponding to image data as a generation origin while relating them with each other. The image storage <b>23</b> is storage for storing the medical images.</p>
<p id="h-0020" num="0000">(Display Controller <b>30</b>)</p>
<p id="p-0080" num="0079">When medical images are generated for a series of timing points, the display controller <b>30</b> reads a series of medical images stored in the image storage <b>23</b>. With reference to the information indicating the timing point attached to read respective medical images, the display controller <b>30</b> arranges this series of medical images along the time sequence to generate motion images. The display controller <b>30</b> causes the display <b>401</b> to display the generated motion images.</p>
<p id="p-0081" num="0080">Next the series of operations of the medical image processing apparatus according to the present embodiment (related to acquiring the projected data of the X-ray CT system) is explained, with reference to <figref idref="DRAWINGS">FIG. 3A</figref> and <figref idref="DRAWINGS">FIG. 3B</figref>. <figref idref="DRAWINGS">FIG. 3A</figref> is a flow chart showing the series of operations of the medical image processing apparatus according to the present embodiment (related to acquiring the projected data of the X-ray CT system). Moreover, <figref idref="DRAWINGS">FIG. 3B</figref> is a flow chart showing the operation related to the analysis of the positional relation in the present embodiment. Furthermore, the flow chart shown in <figref idref="DRAWINGS">FIG. 3B</figref> corresponds to the process of step S<b>20</b> of <figref idref="DRAWINGS">FIG. 3A</figref>.</p>
<p id="p-0082" num="0081"><figref idref="DRAWINGS">FIG. 3A</figref> shows flow charts during the frame rate changing process. Moreover, <figref idref="DRAWINGS">FIG. 5</figref> shows flow charts in the case of changing the scanning conditions and the scan stopping process. <figref idref="DRAWINGS">FIG. 3A</figref> and <figref idref="DRAWINGS">FIG. 5</figref> correspond with each other and are explained together until S<b>20</b>.</p>
<p id="h-0021" num="0000">(In the Case of Changing the Scanning Conditions, the Scan Stopping Process)</p>
<p id="h-0022" num="0000">(Step S<b>10</b>)</p>
<p id="p-0083" num="0082">If trigger signals are supplied from the X-ray controller <b>8</b>, the high-voltage generator <b>7</b> drives the X-ray source <b>3</b>. The high-voltage generator <b>7</b> applies a high voltage to the X-ray source <b>3</b> at the timing point of receiving the trigger signals. Thereby, the X-rays are generated in the X-ray source <b>3</b>, and the gantry/bed controller <b>9</b> synchronously controls the rotation of the rotating ring <b>2</b> of the gantry <b>1</b> and sliding of the sliding bed <b>6</b>.</p>
<p id="p-0084" num="0083">The detected elements configuring the X-ray detector <b>5</b> can measure the intensities of the X-rays generated by the X-ray source <b>3</b> in both the case in which the test object is put between the X-ray source <b>3</b> and the detected element, and the case in which the test object is not put between the X-ray source <b>3</b> and the detected element. Accordingly, the respective detected elements measure at least one of the X-ray intensities, and output an analog signal corresponding to this intensity. The output signals from respective detected elements are read as distinguished for each row along the time sequence by the data accumulating unit <b>11</b> (that is, they are sequentially collected).</p>
<p id="p-0085" num="0084">The data accumulating unit <b>11</b> comprises an integral amplifier and an A/D converter. Electric signals from respective detected elements included in the data accumulating unit <b>11</b> are time-divided through a common integral amplifier, then converted into digital data by the A/D converter. The data accumulating unit <b>11</b> outputs signals converted into digital data from the detected element to the pretreating unit <b>12</b>.</p>
<p id="p-0086" num="0085">The pretreating unit <b>12</b> carries out processing such as correction by sensitivity on the digital data to be transmitted from the data accumulating unit <b>11</b> to turn this digital data into projected data. The pretreating unit <b>12</b> causes the projected data storage <b>13</b> to store this projected data as related to a row that is a reading origin of the digital data that is a generation origin of this projected data.</p>
<p id="h-0023" num="0000">(Step S<b>11</b>)</p>
<p id="p-0087" num="0086">The reconfiguration processing unit <b>14</b> sequentially reads the obtained projected data from the projected data storage <b>13</b> in parallel with the processing for obtaining the projected data by means of the X-ray photographing unit <b>500</b>. The reconfiguration processing unit <b>14</b> carries out reconfiguration processing on this read projected data based on conditions predetermined for analysis in advance, thereby generating image data for analysis for each timing point based on these reconstruct conditions.</p>
<p id="p-0088" num="0087">According to the present embodiment, reconstruction conditions for reconstructing image data for analysis are configured so as to be capable of extracting the bones in the test object from the projected data. In other words, this image data is reconstructed so as to be capable of extracting the bones. Sometimes, the reconstruct conditions in this case are referred to as &#x201c;first conditions&#x201d; and image data generated based on these reconstruction conditions is referred to as &#x201c;first image data.&#x201d; The reconfiguration processing unit <b>14</b> causes the image data storage <b>10</b> to store image data for analysis sequentially generated for each timing point.</p>
<p id="h-0024" num="0000">(Commonalities in the Case of Changing the Frame Rate, Changing the Scanning Conditions, the Scan Stopping Process)</p>
<p id="h-0025" num="0000">(Step S<b>11</b>)</p>
<p id="p-0089" num="0088">At first, <figref idref="DRAWINGS">FIGS. 3A and 5</figref> will be referred. The reconfiguration processing unit <b>14</b> reads the projected data stored in the projected data storage <b>13</b>. At first, the reconfiguration processing unit <b>14</b> carries out reconfiguration processing on the read projected data based on the predetermined reconstruction conditions for analysis and generates first image data for each timing point based on these reconstruction conditions. The reconfiguration processing unit <b>14</b> causes the image data storage <b>10</b> to store this first image data generated for each timing point. Further, the first image data may be reconstruction conditions capable of carrying out analysis processing by means of an image processing unit <b>20</b> (to be described later). In other words, if these conditions are satisfied, for example, the volume rate of the first image data may be lower than the volume rate upon generating image data for display. In addition, resolution of the first image data may be lower than the resolution of the image data for display. Due to this operation, it becomes possible to reduce the processing load upon analysis.</p>
<p id="h-0026" num="0000">(Step S<b>201</b>)</p>
<p id="p-0090" num="0089">Next, <figref idref="DRAWINGS">FIG. 3B</figref> will be referred. At first, a configuration extracting part <b>21</b> reads the first image data reconstructed for analysis for each timing point. Alternatively, the configuration extracting part <b>21</b> sequentially reads the image data for analysis generated by the reconfiguration processing unit <b>14</b> for each timing point and stored in the image data storage <b>10</b> from the image data storage <b>10</b>. In this case, the operation by means of the reconfiguration processing unit <b>14</b> and the operation according to reading of the image data for analysis by means of the configuration extracting part <b>21</b> may be synchronized with each other. The configuration extracting part <b>21</b> outputs each of the first image data for each read timing point to an object extracting part <b>211</b>, providing instructions to extract the object.</p>
<p id="p-0091" num="0090">The object extracting part <b>211</b> sequentially receives the first image data for each timing point from the configuration extracting part <b>21</b>. According to the present embodiment, the object extracting part <b>211</b> extracts bone parts as the objects based on the voxel data in this first image data. Here, <figref idref="DRAWINGS">FIG. 2A</figref> will be referred. <figref idref="DRAWINGS">FIG. 2A</figref> is a view for explaining analysis of the positional relationship in bones, and illustrates an example when bone objects forming arm regions are extracted. As illustrated in <b>2</b>A, the object extracting part <b>211</b> extracts bone objects M<b>11</b>, M<b>12</b>, and M<b>13</b>, forming arm regions from the first image data. Thus, the object extracting part <b>211</b> extracts the bone objects for each of the first image data at each timing point. The object extracting part <b>211</b> outputs information indicating bone objects (for example, information indicating the form, the position, and the size of the object) extracted from each timing point (in other words to the position analyzing unit <b>212</b>, extracted at each timing point) as related to information indicating the corresponding timing point.</p>
<p id="h-0027" num="0000">(Step S<b>202</b>)</p>
<p id="p-0092" num="0091">The position analyzing unit <b>212</b> receives information indicating the bone objects from the object extracting part <b>211</b> for each timing point. The position analyzing unit <b>212</b> identifies at least two or more objects used for analysis of the positional relationship (that is, the analysis object) among bone objects M<b>11</b>, M<b>12</b>, and M<b>13</b>. Alternatively, the position analyzing unit <b>212</b> sequentially receives the information indicating the bone objects extracted for each timing point from the object extracting part <b>211</b>. The position analyzing unit <b>212</b> sequentially analyzes the positional relationship of the bones based on this information. Hereinafter, an explanation will be provided assuming that the position analyzing unit <b>212</b> identifies the objects M<b>11</b> and M<b>13</b>.</p>
<p id="p-0093" num="0092">At first, the position analyzing unit <b>212</b> identifies at least two or more objects used for analysis of the positional relationship (that is, the analysis object) among the bone objects M<b>11</b>, M<b>12</b>, and M<b>13</b>. Specifically, for example, the position analyzing unit <b>212</b> stores the biological body information of respective parts constructing the known biological body (for example, the information indicating the positional relationship of the bones comprising the brachial region and the antebrachial region) and it identifies an object based on this biological body information. In addition, as another method, the position analyzing part <b>212</b> stores the information indicating the shape of the observation object in advance and identifies an object coinciding with this shape as the analysis object. Hereinafter, this will be described assuming that the position analyzing unit <b>212</b> identifies the objects M<b>11</b> and M<b>13</b>.</p>
<p id="p-0094" num="0093">If the analysis objects M<b>11</b> and M<b>13</b> are identified, the position analyzing unit <b>212</b> extracts at least three points having features in its shape (hereinafter, referred to as &#x201c;shape features&#x201d;) from each of these objects. For example, as illustrated in <figref idref="DRAWINGS">FIG. 2A</figref>, the position analyzing unit <b>212</b> extracts shape features M<b>111</b>, M<b>112</b>, and M<b>113</b> from the object M<b>11</b>. The position analyzing unit <b>212</b> extracts the shape features M<b>131</b>, M<b>132</b>, and M<b>133</b> from the object M<b>13</b>.</p>
<p id="p-0095" num="0094">Next, the position analyzing part <b>212</b> forms planes for grasping the positions and directions of respective objects by simulation by portions (namely, points) indicating the extracted three points of shape features, relating the plane with the object that is the origin for extracting the shape features. Here, <figref idref="DRAWINGS">FIG. 2B</figref> will be referred. <figref idref="DRAWINGS">FIG. 2B</figref> is a view explaining the analysis of the positional relationship of the bones, indicating a plane formed based on the shape features formed by each of the objects M<b>11</b> and M<b>13</b>. As illustrated in <figref idref="DRAWINGS">FIG. 2B</figref>, the position analyzing unit <b>212</b> forms a plane P<b>11</b> due to the shape features M<b>111</b>, M<b>112</b>, and M<b>113</b> and relates this to the object M<b>11</b>. In the same way, the position analyzing unit <b>212</b> forms a plane P<b>13</b> due to the shape features M<b>131</b>, M<b>132</b>, and M<b>133</b> and relates this to the object M<b>13</b>.</p>
<p id="p-0096" num="0095">When the joint is moved, the position and direction of each of a plurality of bones comprising the joint and their relative positional relationships (hereinafter, they are simply referred to as the &#x201c;positional relationship&#x201d;) are changed; however, the shape and size of each bone are not changed. In other words, the objects M<b>11</b> and M<b>13</b> extracted at each timing point are changed in the positional relationship at each timing point; however, the shape and size of each object are not changed. The same applies to the planes P<b>11</b> and P<b>13</b> extracted based on the shape feature of each object. According to the present embodiment, using this feature, the position analyzing unit <b>212</b> identifies the positional relationships of the objects M<b>11</b> and M<b>13</b> based on the position and direction of each of the planes P<b>11</b> and P<b>13</b>. Thus, by forming a plane from each object, there is no need to carry out analysis of a complete shape in order to grasp the position and direction of the object, making it possible to reduce the processing load. Therefore, the position analyzing unit <b>212</b> can reduce the processing load for identifying the positional relationships of the objects M<b>11</b> and M<b>13</b>.</p>
<p id="p-0097" num="0096">Here, <figref idref="DRAWINGS">FIG. 2C</figref> will be referred. <figref idref="DRAWINGS">FIG. 2C</figref> is a view explaining the analysis of the positional relationship of bones and illustrates an example in which the positional relationship between the objects M<b>11</b> and M<b>13</b> illustrated in <figref idref="DRAWINGS">FIG. 2A</figref> and <figref idref="DRAWINGS">FIG. 2B</figref> is represented by planes P<b>11</b> and P<b>13</b>. The position analyzing unit <b>212</b> identifies the relative positional relationship between the objects M<b>11</b> and M<b>13</b>, for example, based on the angle formed by the planes P<b>11</b> and P<b>13</b>. In addition, the position analyzing unit <b>212</b> may identify the relative positional relationship between the objects M<b>11</b> and M<b>13</b> based on the distance between the planes P<b>11</b> and P<b>13</b> in place of the angle. Hereinafter, this will be described assuming that the position analyzing unit <b>212</b> identifies the positional relationship between the objects M<b>11</b> and M<b>13</b> based on P<b>11</b> and P<b>13</b>.</p>
<p id="p-0098" num="0097">Here, <figref idref="DRAWINGS">FIG. 2D</figref> will be referred. <figref idref="DRAWINGS">FIG. 2D</figref> is a view explaining the analysis of the positional relationship of the bones and illustrates an example of the positional relationship between the planes P<b>11</b> and P<b>13</b> at a plurality of timing points. According to the example of <figref idref="DRAWINGS">FIG. 2D</figref>, in order to simplify the explanation, assuming that the position and direction of the plane P<b>11</b> (that is, the object M<b>11</b>) are not changed, the position and direction of the plane P<b>13</b> for each timing point are illustrated. P<b>13</b><i>a </i>to P<b>13</b><i>d </i>in <figref idref="DRAWINGS">FIG. 2D</figref> illustrates the plane P<b>13</b> corresponding to different timing points, respectively.</p>
<p id="p-0099" num="0098">When the positional relationship between the object M<b>11</b> and M<b>13</b> is identified, the position analyzing unit <b>212</b> determines whether or not the identified positional relationship is a specific positional relationship. This specific positional relationship may be decided in advance, for example, based on the positional relationship corresponding to a timing point desiring to control the scan in a series of movements in the observation object (in other words, the positional relationship corresponding to a timing point desired to change the scanning conditions, or desired to stop the scan).</p>
<p id="p-0100" num="0099">As described above, in the event of changing the frame rate, the processing common to the case of changing the scanning conditions and the case of scan stopping processing have been described. Hereinafter, respective cases will be separately explained.</p>
<p id="h-0028" num="0000">(In the Case of Changing the Frame Rate)</p>
<p id="h-0029" num="0000">(Step S<b>203</b>)</p>
<p id="p-0101" num="0100">Thus, based on the planes P<b>11</b> and P<b>13</b> extracted at each timing point, the position analyzing unit <b>212</b> identifies the positional relationship between the objects M<b>11</b> and M<b>13</b> at each timing point thereof. When a timing point that does not identify the positional relationship between the objects M<b>11</b> and M<b>13</b> exists (Step S<b>203</b>, N), the position analyzing unit <b>212</b> identifies the positional relationship between the objects M<b>11</b> and M<b>13</b> with respect to the timing point.</p>
<p id="h-0030" num="0000">(Step S<b>204</b>)</p>
<p id="p-0102" num="0101">When the positional relationship between the objects M<b>11</b> and M<b>13</b> is identified with respect to a series of timing points (Step S<b>203</b>,Y), the position analyzing unit <b>212</b> determines whether or not the identified positional relationship is included in a specific range (whether or not it satisfies a predetermined condition), identifying the timing point included in the range. This specific range may be predetermined, for example, based on a positional relationship to be focused on in a series of movements of the observation object. The position analyzing unit <b>212</b> identifies a time width formed by the identified timing point (in other words, a time width in which the positional relationship between the objects M<b>11</b> and M<b>13</b> satisfies specific conditions). In the following descriptions, the position analyzing unit <b>212</b> will be explained assuming that a time width T<b>21</b> illustrated in <figref idref="DRAWINGS">FIG. 2E</figref> is identified. The position analyzing unit <b>212</b> notifies the reconfiguration processing unit <b>14</b> of the identified time width T<b>21</b>.</p>
<p id="h-0031" num="0000">(Step S<b>31</b>)</p>
<p id="p-0103" num="0102">Here, <figref idref="DRAWINGS">FIG. 3A</figref> will be referred. The reconfiguration processing unit <b>14</b> reconstructs the image data for display for each timing point based on reconstruction conditions while altering the reconstruction conditions between the notified time width T<b>21</b> and other time widths T<b>11</b> and T<b>12</b>, and carrying out reconfiguration processing. The reconfiguration processing unit <b>14</b> causes the image data storage <b>10</b> to store the reconstructed series of image data for display.</p>
<p id="p-0104" num="0103">Thus, if analysis of the positional relationship of the objects M<b>11</b> and M<b>13</b> with respect to a series of timing points is completed and the reconstructed series of image data for display is stored in the image data storage <b>10</b>, the configuration extracting part <b>21</b> reads these image data and transfers them to an image processor <b>22</b>.</p>
<p id="h-0032" num="0000">(Step S<b>32</b>)</p>
<p id="p-0105" num="0104">The image processor <b>22</b> receives the reconstructed series of image data for each specific timing point from the configuration extracting part <b>21</b>. The image processor <b>22</b> generates medical images respectively by subjecting each of the image data for each timing point to image processing based on predetermined image processing conditions. The image processor <b>22</b> causes the image storage <b>23</b> to store the generated medical images and the information indicating a timing point corresponding to image data as a generation origin while relating them with each other.</p>
<p id="h-0033" num="0000">(Step S<b>33</b>)</p>
<p id="p-0106" num="0105">Thus, the image processor <b>22</b> generates medical images by subjecting the image data corresponding to each timing point to image processing. When timing point with no medical images generated exists (Step S<b>33</b>, N), the image processor <b>22</b> generates medical images with respect to this timing point.</p>
<p id="h-0034" num="0000">(Step S<b>34</b>)</p>
<p id="p-0107" num="0106">When medical images are generated for a series of timing points (Step S<b>33</b>, Y), the display controller <b>30</b> reads a series of medical images stored in the image storage <b>23</b>. With reference to the information indicating the timing point attached to read respective medical images, the display controller <b>30</b> arranges this series of medical images along the time sequence to generate motion images. The display controller <b>30</b> causes the display <b>401</b> to display the generated motion images.</p>
<p id="p-0108" num="0107">As described above, according to the present embodiment, the medical image processing apparatus analyzes changes in the positional relationship of at least two sites that temporarily work with each other by means of the bone objects corresponding to these sites, the sites being related to a surgery site of muscules, tendons or bones such as a joint and a spondylus. Moreover, the medical image processing apparatus identifies a time width having a positional relationship of the bone objects corresponding to these sites included in a specific range and reconstructs image data for display by changing the reconstruction conditions in this time width and other time widths in addition to carrying out reconfiguration processing. Thereby, the medical image processing apparatus according to the present embodiment can display medical images at a frame rate higher than other time widths with respect to a time width having a positional relationship of two or more sites included in a specific range.</p>
<p id="h-0035" num="0000">(In the Case of Changing the Scanning Conditions, the Scan Stopping Process)</p>
<p id="h-0036" num="0000">(Step S<b>31</b>)</p>
<p id="p-0109" num="0108">Thus, the position analyzing unit <b>212</b> sequentially analyzes the information indicating the bone objects to be sequentially output from the object extracting part <b>211</b> for each timing point, and determines whether or not the positional relationship of the objects M<b>11</b> and M<b>13</b> that are observation objects satisfies specific conditions. Thereby, the position analyzing unit <b>212</b> detects a timing point wherein the objects M<b>11</b> and M<b>13</b> have a specific positional relationship. This timing point corresponds to &#x201c;one timing point.&#x201d;</p>
<p id="h-0037" num="0000">(Step S<b>32</b>)</p>
<p id="p-0110" num="0109">If the position analyzing unit <b>212</b> detects this timing point (Step S<b>31</b>, Y), it instructs the scan control <b>501</b> to control the operation according to the scan (that is, it changes the scanning conditions or stops scanning). When this timing point is not detected (Step S<b>31</b>, N), the position analyzing unit <b>212</b> shifts to the next process without providing instructions to control operation of the scan to the scan control <b>501</b>.</p>
<p id="h-0038" num="0000">(Step S<b>33</b>)</p>
<p id="p-0111" num="0110">The X-ray CT system according to the present embodiment carries out the above-described series of processing unless the operator instructs the completion of imaging (Step S<b>33</b>, N). If the operator instructs the completion of imaging (Step S<b>33</b>, Y), the X-ray CT system according to the present embodiment terminates the process for obtaining the projected data, in addition to terminating the analysis processing for controlling this.</p>
<p id="p-0112" num="0111">As described above, the X-ray CT system according to the present embodiment analyzes changes in the positional relationship of at least two or more sites constructing flexible sites in accordance with the bones corresponding to these sites, the flexible sites being related to a surgery site of muscules, tendons or bones such as a joint and a spondylus. Moreover, the X-ray CT system detects a timing point in which the positional relationship of the bone objects corresponding to these sites satisfies specific conditions, and it controls the operation for acquiring projected data based on this timing point (that is, it changes the scanning conditions or stops scanning). Thereby, in the X-ray CT system according to the present embodiment, the X-ray CT system itself can automatically control the operation for acquiring projected data without the operator when the positional relationship of two or more sites satisfies specific conditions.</p>
<p id="p-0113" num="0112">According to the above-described embodiments, as a flexible site, bones and joints are exemplified; however, as a flexible site, it is also possible to focus on cartilage. In the above-described embodiments, three points of shape features regarding the bones have been identified; however, for example, by identifying three points of shape features regarding cartilage and two shape features, the above-described processing can also be carried out. As a merit of analyzing cartilage as a flexible site in place of a bone, improved diagnosis accuracy of disc hernias can be cited. Disc hernias occur due to protrusion of cartilage in the joints.</p>
<p id="p-0114" num="0113">Acquiring image data of cartilage by means of a medical imaging apparatus, the positional relationship of cartilage is analyzed as well as the above-described positional relationship of the bones. Depending on the analysis result, it is possible to determine whether or not cartilage protrudes. If cartilage protrudes, disc hernias occur, allowing a diagnosis to be made without waiting for analyses regarding the bones. This analysis processing can be carried out in place of analysis processing regarding the bones; however, the analysis processing can be carried out together with analysis processing regarding the bones. When acquisition and analysis of images are carried out in parallel with processing of the bones and it is found that a disk hernia has occurred from analysis results regarding images of the cartilage, by completing analysis without waiting for analysis of the bones, it is possible to acquire an accurate diagnosis at an early stage. Further, other than the case in which cartilage protrudes, the case in which cartilage is crushed by sites such as other bones is also considered. When cartilage is crushed more than a certain extent, crushing of the cartilage is defined as an analysis result, and based on this result, it is possible to change the frame rate, or the processing can shift to changing of the scanning conditions or stopping of the scan.</p>
<heading id="h-0039" level="1">Second Embodiment</heading>
<p id="p-0115" num="0114">Next, a medical image displaying apparatus according to a second embodiment will be described. In the medical image displaying apparatus according the first embodiment, the time width to change the reconstruction conditions based on the positional relationship of the bone objects, and the timing point for controlling the operation for acquiring projected data are identified. In the medical image processing apparatus according to the present embodiment, the time width to change the reconstruction conditions based on changes in the outline of the test object, and the timing point for controlling the operation for acquiring projected data are identified. The medical image processing apparatus according to the present embodiment will be described below focusing on different points from the first embodiment.</p>
<p id="h-0040" num="0000">(In the Case of Changing the Frame Rate)</p>
<p id="p-0116" num="0115">At first, the reconfiguration processing unit <b>14</b> according to the present embodiment carries out reconfiguration processing on the read projected data based on the predetermined reconstruction conditions for analysis, then generates image data for each timing point based on these reconstruction conditions. Further, in the present embodiment, these reconstruction conditions are configured so as to be capable of extracting the surface layer of the test object from the projected data (that is, skin). Specifically, these reconstruction conditions have a range of CT numbers that is the target of reconstruction and which is adjusted at a level capable of extracting the surface layer. Thereby, this image data is reconstructed so as to be capable of extracting the surface layer. Image data generated based on the reconstruction conditions in this case correspond to the &#x201c;first image data.&#x201d; The reconfiguration processing unit <b>14</b> causes the image data storage <b>10</b> to store image data generated for each timing point. Such extraction of the surface layer of the test object from image data makes it possible to identify the outline of the test object based on the extracted surface layer. In the present embodiment, by analyzing shape change in the outline of the test object at a plurality of timing points based on the surface layer of the test object thus reconstructed, reconstruction conditions of image data for display are changed depending on whether or not the shape of the outline thereof is included within the specific range.</p>
<p id="p-0117" num="0116">Further, processing for reconstructing image data for display by the reconfiguration processing unit <b>14</b> according to the present embodiment is identical with the operation of the reconfiguration processing unit <b>14</b> according to the first embodiment. In other words, the reconfiguration processing unit <b>14</b> receives notification of a time width T<b>21</b> including a plurality of timing points from the position analyzing unit <b>212</b>. The reconfiguration processing unit <b>14</b> carries out reconfiguration processing while changing the reconstruction conditions in the notified time width T<b>21</b>, and other time widths T<b>11</b> and T<b>12</b>, then reconstructs image data for display to cause the image data storage <b>10</b> to store the reconstructed image data.</p>
<p id="h-0041" num="0000">(In the Case of Changing the Scanning Conditions, the Scan Stopping Process)</p>
<p id="p-0118" num="0117">The X-ray CT system according to the present embodiment grasps the position and direction of respective sites configuring the observation object and the relative positional relationship thereof (hereinafter, it is simply referred to as the &#x201c;positional relationship&#x201d;) by analyzing the reconstructed image data. Therefore, the reconfiguration processing unit <b>14</b> reconstructs image data for analysis separately from the image data for display. Specifically, the reconfiguration processing unit <b>14</b> sequentially reads the acquired projected data from the projected data storage <b>13</b> in parallel with processing for acquiring projected data by means of the X-ray photographing unit <b>500</b>. The reconfiguration processing unit <b>14</b> generates image data for analysis for each timing point based on these reconstruction conditions by carrying out reconfiguration processing on this read projected data based on the predetermined reconstruction conditions for analysis.</p>
<p id="p-0119" num="0118">According to the present embodiment, the reconstruction conditions for reconstructing the image data for analysis are configured so as to be capable of extracting the surface layer of the test object from the projected data (that is, skin). Specifically, these reconstruction conditions have a range of CT numbers that are the target of reconstruction, with a range that is adjusted at a level capable of extracting the surface layer. Thereby, this image data is reconstructed so as to be capable of extracting the surface layer. The reconstruction conditions in this case correspond to &#x201c;first conditions&#x201d; according to the present embodiment, and the image data generated based on these reconstruction conditions correspond to the &#x201c;first image data.&#x201d; The reconfiguration processing unit <b>14</b> causes the image data storage <b>10</b> to store the image data generated for each timing point. Such extraction of a surface layer of the test object from image data makes it possible to identify the outline of the test object based on the extracted surface layer. In the present embodiment, by analyzing shape change in the outline of the test object along a time sequence based on the surface layer of the test object thus reconstructed, the timing point of controlling the operation for obtaining the projected data is identified depending on whether or not the shape of the outline thereof satisfies specific conditions.</p>
<p id="p-0120" num="0119">Further, processing for reconstructing image data for display by the reconfiguration processing unit <b>14</b> according to the present embodiment is identical with operation of the reconfiguration processing unit <b>14</b> according to the first embodiment. In other words, the reconfiguration processing unit <b>14</b> generates image data for display for each timing point based on these reconstruction conditions by reading the projected data from the projected data storage <b>13</b> and carrying out the reconfiguration processing based on predetermined reconstruction conditions for display. Hereinafter, sometimes reconstruction conditions for display are referred to as &#x201c;second conditions&#x201d; and the image data generated based on these reconstruction conditions is referred to as &#x201c;second projected data.&#x201d; The reconfiguration processing unit <b>14</b> causes the image data storage <b>10</b> to store this image data for display generated for each timing point.</p>
<p id="p-0121" num="0120">The image data for display does not always need to be operated in parallel with the processing for obtaining projected data. For example, the reconfiguration processing unit <b>14</b> may reconstruct the image data for display after a series of projected data is obtained. This operation is also identical with the first embodiment.</p>
<p id="p-0122" num="0121">The configuration extracting part <b>21</b> (in the case of changing the frame rate) reads the first image data reconstructed for analysis for each timing point. The configuration extracting part <b>21</b> outputs each of the read first image data for each timing point to the object extracting part <b>211</b>, and instructs it to extract the object. This operation of the configuration extracting part <b>21</b> is identical with the first embodiment.</p>
<p id="p-0123" num="0122">The configuration extracting part <b>21</b> (in the case of changing the scanning conditions or the scan stopping process) sequentially reads the image data for analysis sequentially generated by the reconfiguration processing unit <b>14</b> for each timing point stored in the image data storage <b>10</b> from the image data storage <b>10</b>. In this case, the operation by the reconfiguration processing unit <b>14</b> and the operation for reading the image data for analysis by the configuration extracting part <b>21</b> may be synchronized with each other. The configuration extracting part <b>21</b> sequentially outputs the read first image data object for each timing point to the object extracting part <b>211</b>, and instructs it to extract the object from the first image data. The operation of this configuration extracting part <b>21</b> is the same as the first embodiment.</p>
<p id="h-0042" num="0000">(Common to the Cases of Changing the Frame Rate, Changing the Scanning Conditions, and the Scan Stopping Process)</p>
<p id="p-0124" num="0123">The object extracting part <b>211</b> sequentially receives the first image data for each timing point from the configuration extracting part <b>21</b>. The object extracting part <b>211</b> according to the present embodiment detects the surface layer of the test object based on voxel data in this first image data, and extracts the object in the region formed by the detected surface layer. This object represents the outline shape of the test object. Hereinafter, sometimes this object is referred to as the outline object. Here, <figref idref="DRAWINGS">FIGS. 4A and 4B</figref> are referred. <figref idref="DRAWINGS">FIGS. 4A and 4B</figref> are the views for explaining analysis of the shape based on the surface layer of the test object. <figref idref="DRAWINGS">FIGS. 4A and 4B</figref> illustrate joint parts between an antebrachial region and a brachial region, with each of them corresponding to different timing points, respectively. The objects M<b>11</b> to M<b>13</b> in <figref idref="DRAWINGS">FIG. 4A</figref> represent the objects of the bones, while M<b>21</b><i>a</i>denotes the outline object. In addition, the objects M<b>11</b> to M<b>13</b> in <figref idref="DRAWINGS">FIG. 4B</figref> represent the objects of the bones, and they correspond to the objects M<b>11</b> to M<b>13</b> in <figref idref="DRAWINGS">FIG. 4A</figref>. In addition, M<b>21</b><i>b </i>in <figref idref="DRAWINGS">FIG. 4B</figref> denotes the outline object at a different timing point from that of <figref idref="DRAWINGS">FIG. 4A</figref>, and represents the different shape from the object M<b>21</b><i>a </i>by the movement of the joint.</p>
<p id="p-0125" num="0124">The object extracting part <b>211</b> outputs information indicating the outline object (for example, information indicating the shape, the position, and the size of the object) extracted for each of the first image data at each timing point (that is, extracted for each timing point) to the position analyzing unit <b>212</b> as related to the information indicating the corresponding timing point.</p>
<p id="p-0126" num="0125">The position analyzing unit <b>212</b> receives the information indicating the outline object from the object extracting part <b>211</b> for each timing point. The position analyzing unit <b>212</b> analyzes changes in the outline along a time sequence based on this information. An example of a specific method thereof will be described below.</p>
<p id="h-0043" num="0000">(In the Case of Changing the Frame Rate)</p>
<p id="p-0127" num="0126">At first, the position analyzing unit <b>212</b> identifies a standard object from the outline objects for each timing point. As a specific example of a method of identifying this standard object, the position analyzing unit <b>212</b> analyzes each shape of the outline object for each timing point and identifies the object of a specific shape (satisfying the specific conditions). In the case of identifying the object of this specific shape, for example, the position analyzing unit <b>212</b> identifies a standard object by storing the information of the object with a standard shape in advance and comparing this with the outline object for each timing point, respectively. This comparison may be carried out via comparison of the outline shapes of the objects, or the standard object may be identified by extracting a plurality of shape features and comparing these features. In addition, the standard object may be identified by extracting axes of the parts corresponding to the brachial region and the antebrachial region and comparing these axes.</p>
<p id="p-0128" num="0127">In addition, the operator may specify the standard outline object via an operation part <b>402</b>. In this case, the configuration extracting part <b>21</b> outputs a series of first image data to the image processor <b>22</b> to generate medical images for each timing point, then causes the display <b>401</b> to display the medical images via the display controller <b>30</b>. The operation part <b>402</b> receives the standard medical image selected from the displayed image data from the operator to notify the position analyzing unit <b>212</b> of the timing point corresponding to this medical image. Upon receiving this notification, the position analyzing unit <b>212</b> may identify the outline object corresponding to the notified timing point. Hereinafter, sometimes this standard outline object is referred to as a &#x201c;standard object.&#x201d;</p>
<p id="p-0129" num="0128">Identifying the standard object, the position analyzing unit <b>212</b> compares this standard object, the outline object for each timing point, and the standard object with each other, calculating the change amount between the objects for each timing point. Specifically, the position analyzing unit <b>212</b> compares the shapes of both objects, and calculates the difference (for example, the number of pixels of the part not overlapping between the objects) as the change amount. In addition, as another method, for example, extracting the axes from the brachial region and the antebrachial region, respectively, the position analyzing unit <b>212</b> may obtain the change amount based on the positional relationship between these axes (for example, the angle and the distance).</p>
<p id="p-0130" num="0129">When the change amount is calculated for each timing point, the position analyzing unit <b>212</b> determines whether or not this change amount is within the predetermined amount (hereafter, referred to as a &#x201c;specific amount&#x201d;), then identifies the time width in which the change amount is formed by a timing point within the specific amount. This specific amount may be decided in advance, for example, based on a flexible range to be focused on in a series of movements of the observation object. This time width corresponds to a time width T<b>21</b> (refer to <figref idref="DRAWINGS">FIG. 2E</figref>) in the first embodiment, and other time widths correspond to time widths T<b>11</b> and T<b>12</b>. Hereinafter, the position analyzing unit <b>212</b> will be described assuming that this time width T<b>21</b> is identified. The position analyzing unit <b>212</b> notifies the reconfiguration processing unit <b>14</b> of the identified time width T<b>21</b>.</p>
<p id="h-0044" num="0000">(In the Case of Changing the Scanning Conditions and the Scan Stopping Process)</p>
<p id="p-0131" num="0130">The position analyzing unit <b>212</b> analyzes the shape of each outline object for each timing point, and determines whether or not the shape corresponds to the specific shape (whether or not it satisfies the specific conditions). In the case of determining whether or not the shape corresponds to this specific shape, for example, the position analyzing unit <b>212</b> determines this specific shape by storing the information indicating the standard shape in advance, and comparing this with each outline object for each timing point. This comparison may be carried out due to comparison of the outline shape of the object, or may identify the specific shape by extracting a plurality of shape features and comparing these shape features. In addition, the specific shape may be identified by extracting the axes of the parts corresponding to the brachial region and the antebrachial region, and comparing these axes. Further, this specific shape may be decided in advance, for example, based on the shape of the observation object corresponding to a timing point desired to control the scan in a series of movements in the observation object (in other words, a shape corresponding to a timing point desired to change the scanning conditions, or desired to stop the scan).</p>
<p id="p-0132" num="0131">In addition, obtaining images of the appearance of the observation object (hereinafter, referred to as an appearance image) in advance by a photographing unit for imaging an appearance (for example, a camera, etc.), the observation object, the position analyzing unit <b>212</b> may determine whether or not the shape of the outline object corresponds to this appearance image.</p>
<p id="p-0133" num="0132">The determination of correspondence is not necessarily limited to complete correspondence and includes a case of high degree of correspondence as a result of matching by means of external images obtained by imaging the observation subject, for example. These external images may be projection data, perspective data through computed tomography or angiography, or data obtained by videography.</p>
<p id="p-0134" num="0133">In this case, obtaining the information indicating the positional relationship between this photographing unit and the test object, the position analyzing unit <b>212</b> may identify the imaging position of this photographing unit based on this information. In this case, with this imaging position as a viewpoint, the position analyzing unit <b>212</b> may project the shape of the outline object and the outline object, and compare the projection with the appearance image.</p>
<p id="p-0135" num="0134">Thus, the position analyzing unit <b>212</b> sequentially analyzes the information indicating the outline object to be sequentially output from the object extracting part <b>211</b> for each timing point, and determines whether or not the shape of the outline object corresponds to the specific shape. Thereby, the position analyzing unit <b>212</b> detects the timing point wherein the outline object has a specific shape. This timing point corresponds to &#x201c;one timing point.&#x201d; Upon detecting this timing point, the position analyzing unit <b>212</b> instructs the scan control <b>501</b> to change the scanning conditions or stop the scan. Upon receiving these instructions, the scan control <b>501</b> carries out the instructed operation (that is, it changes the scanning conditions or stops the scan). Whether changes in the scanning conditions or stopping of the scan should be instructed may be related to the information indicating the detected timing point (in other words, the shape corresponding to this timing point) in advance.</p>
<p id="h-0045" num="0000">(Commonalities in the Case of Changing the Frame Rate, Changing the Scanning Conditions, and the Scan Stopping Process)</p>
<p id="p-0136" num="0135">Further, the following processing is the same as the first embodiment. In other words, the reconfiguration processing unit <b>14</b> carries out reconfiguration processing while changing the reconstruction conditions in the notified time width T<b>21</b>, and other time widths T<b>11</b> and T<b>12</b>, then reconstructs image data for display for each timing point based on the reconstruction conditions. The reconfiguration processing unit <b>14</b> causes the image data storage <b>10</b> to store the reconstructed series of image data for display. As other embodiments, when the reconstructed second projected data for display is stored in the image data storage <b>10</b>, the configuration extracting part <b>21</b> reads this data and transfers it to the image processor <b>22</b>. Further, the first image data generated for analysis may be operated so as to be also used for display. In this case, the position analyzing unit <b>212</b> may transfer the image data that has already been read for analysis to the image processor <b>22</b>. The image processor <b>22</b> carries out image processing on this image data and generates medical images, storing these medical images in the image storage <b>23</b> as related to the information indicating a corresponding timing point. The display controller <b>30</b> reads these medical images from the image storage <b>23</b> and arranges these medical images along a time sequence to cause the display <b>401</b> to display them as motion images.</p>
<p id="p-0137" num="0136">Next, with reference to <figref idref="DRAWINGS">FIGS. 3A and 3C</figref>, the series of operations of a medical image processing apparatus according to the present embodiment will be described. <figref idref="DRAWINGS">FIG. 3C</figref> is a flow chart showing the operation related to the analysis of the positional relationship according to the present embodiment. Further, the flow chart shown in <figref idref="DRAWINGS">FIG. 3C</figref> corresponds to the processing of Step S<b>20</b> in <figref idref="DRAWINGS">FIG. 3A</figref>. In addition, processing other than the processing related to Step S<b>11</b> and Step S<b>20</b> in <figref idref="DRAWINGS">FIG. 3A</figref> and <figref idref="DRAWINGS">FIG. 5</figref> is the same as that of the first embodiment. Therefore, an explanation will be provided focusing on the processes related to Step S<b>11</b> and Step S<b>20</b> different from the first embodiment, namely, the processes related to Steps S<b>211</b> to S<b>214</b> illustrated in <figref idref="DRAWINGS">FIG. 3C</figref>.</p>
<p id="h-0046" num="0000">(Step S<b>11</b>)</p>
<p id="p-0138" num="0137">The reconfiguration processing unit <b>14</b> sequentially reads the acquired projected data from the projected data storage <b>13</b> in parallel with the processing for acquiring projected data by means of the X-ray photographing unit <b>500</b>. The reconfiguration processing unit <b>14</b> generates image data for each timing point based on these reconstruction conditions by carrying out reconfiguration processing on this read projected data based on the predetermined reconstruction conditions for analysis. Further, according to the present embodiment, these reconstruction conditions are configured so as to be capable of extracting the surface layer of the test object from the projected data (that is, skin). Specifically, these reconstruction conditions have a range of CT numbers that is a target of reconstruction, which is adjusted at a level capable of extracting the surface layer. Thereby, this image data is reconstructed so as to be capable of extracting the surface layer. The reconfiguration processing unit <b>14</b> causes the image data storage <b>10</b> to store the image data generated for each timing point.</p>
<p id="p-0139" num="0138">According to the present embodiment, the reconstruction conditions for reconstructing the image data for analysis are configured so as to be capable of extracting the surface layer of the test object from the projected data (that is, skin). Specifically, these reconstruction conditions have a range of CT numbers that is a target of reconstruction, which is adjusted at a level capable of extracting the surface layer. Thereby, this image data is reconstructed so as to be capable of extracting the surface layer. The reconstruction conditions in this case correspond to &#x201c;first conditions&#x201d; in the present embodiment, and the image data generated based on these reconstruction conditions corresponds to &#x201c;first image data.&#x201d; The reconfiguration processing unit <b>14</b> causes the image data storage <b>10</b> to store the image data generated for each timing point.</p>
<p id="h-0047" num="0000">(Step S<b>211</b>)</p>
<p id="p-0140" num="0139">At first, the configuration extracting part <b>21</b> reads the first image data reconstructed for analysis for each timing point. The configuration extracting part <b>21</b> sequentially reads the image data for analysis sequentially generated by the reconfiguration processing unit <b>14</b> for each timing point and stored in the image data storage <b>10</b> from the image data storage <b>10</b>. In this case, the operation by the reconfiguration processing unit <b>14</b> and the operation for reading the image data for analysis by the configuration extracting part <b>21</b> may be synchronized with each other. The configuration extracting part <b>21</b> outputs each of the read first image data for each timing point to the object extracting part <b>211</b>, and instructs it to extract the object. This operation of the configuration extracting part <b>21</b> is identical with the first embodiment.</p>
<p id="p-0141" num="0140">The object extracting part <b>211</b> sequentially receives the first image data for each timing point from the configuration extracting part <b>21</b>. The object extracting part <b>211</b> according to the present embodiment detects the surface layer of the test object based on voxel data in this first image data, and extracts the object in the region formed by the detected surface layer. This object represents the outline shape of the test object. Hereinafter, this object is sometimes referred to as the outline object. The object extracting part <b>211</b> outputs information indicating the outline object (for example, objects M<b>21</b><i>a </i>and M<b>2</b><i>b </i>in <figref idref="DRAWINGS">FIG. 4A</figref> and <figref idref="DRAWINGS">FIG. 4B</figref>) extracted for each of the first image data at each timing point (that is, extracted for each timing point) to the position analyzing unit <b>212</b> as related to the information indicating the corresponding timing point.</p>
<p id="p-0142" num="0141">Here, <figref idref="DRAWINGS">FIG. 4A</figref> and <figref idref="DRAWINGS">FIG. 4B</figref> are referred. <figref idref="DRAWINGS">FIG. 4A</figref> and <figref idref="DRAWINGS">FIG. 4B</figref> are the views for explaining analysis of the shape based on the surface layer of the test object. <figref idref="DRAWINGS">FIGS. 4A and 4B</figref> illustrate joint parts between the antebrachial region and the brachial region, with each of them corresponding to different timing points, respectively. The objects M<b>11</b> to M<b>13</b> in <figref idref="DRAWINGS">FIG. 4A</figref> represent the objects of the bones, while M<b>21</b><i>a </i>denotes the outline object. In addition, the objects M<b>11</b> to M<b>13</b> in <figref idref="DRAWINGS">FIG. 4B</figref> represent the objects of the bones and correspond to the objects M<b>11</b> to M<b>13</b> in <figref idref="DRAWINGS">FIG. 4A</figref>. In addition, M<b>21</b><i>b </i>in <figref idref="DRAWINGS">FIG. 4B</figref> denotes the outline object at a different timing point from that of <figref idref="DRAWINGS">FIG. 4A</figref>, representing a different shape from the object M<b>21</b><i>a </i>by the movement of the joint.</p>
<p id="p-0143" num="0142">The object extracting part <b>211</b> outputs information indicating the outline object (for example, information indicating the shape, the position, and the size of the object) extracted for each of the first image data at each timing point (that is, extracted for each timing point) to the position analyzing unit <b>212</b> as related to the information indicating the corresponding timing point.</p>
<p id="h-0048" num="0000">(Step S<b>212</b>)</p>
<p id="p-0144" num="0143">The position analyzing unit <b>212</b> receives the information indicating the outline object from the object extracting part <b>211</b> for each timing point. The position analyzing unit <b>212</b> analyzes changes in the outline along a time sequence based on this information. Specifically, at first, the position analyzing unit <b>212</b> identifies a standard object from the outline objects for each timing point. As a specific example of a method of identifying this standard object, the position analyzing unit <b>212</b> analyzes each shape of the outline object for each timing point and identifies the object of a specific shape (satisfying the specific conditions).</p>
<p id="h-0049" num="0000">(In the Case of Changing the Scanning Conditions and the Scan Stopping Process)</p>
<p id="p-0145" num="0144">The position analyzing unit <b>212</b> analyzes the shape of each outline object for each timing point, and determines whether or not the shape corresponds to the specific shape (whether or not it satisfies the specific conditions). In the case of determining whether or not the shape corresponds to this specific shape, for example, the position analyzing unit <b>212</b> determines this specific shape by storing the information indicating the standard shape in advance, and comparing this with each outline object for each timing point. This comparison may be carried out due to comparison of the outline shape of the object, or may identify the specific shape by extracting a plurality of shape features and comparing these shape features. In addition, the specific shape may be identified by extracting the axes of the parts corresponding to the brachial region and the antebrachial region, and comparing these axes. Further, this specific shape may be decided in advance, for example, based on the shape of the observation object corresponding to a timing point desired to control the scan in a series of movements in the observation object (in other words, a shape corresponding to a timing point desired to change the scanning conditions, or desired to stop the scan).</p>
<p id="p-0146" num="0145">Thus, the position analyzing unit <b>212</b> sequentially analyzes the information indicating the outline object to be sequentially output from the object extracting part <b>211</b> for each timing point, and determines whether or not the shape of the outline object corresponds to the specific shape. Thereby, the position analyzing unit <b>212</b> detects the timing point wherein the outline object has a specific shape. This timing point corresponds to &#x201c;one timing point.&#x201d;</p>
<p id="p-0147" num="0146">Further, the following processing is the same as the first embodiment. In other words, upon detecting a timing point, the position analyzing unit <b>212</b> instructs the scan control <b>501</b> to change the scanning conditions or stop the scan. Upon receiving these instructions, the scan control <b>501</b> carries out the instructed operation (that is, it changes the scanning conditions or stops the scan).</p>
<p id="p-0148" num="0147">As described above, the X-ray CT system according to the present embodiment analyzes changes in the positional relationship between at least two or more sites configuring flexible sites due to changes in the shape of the outline of the test object, the flexible sites being related to a surgery site of muscules, tendons or bones such as a joint and a spondylus. Moreover, the X-ray CT system detects a timing point in which the shape of the outline corresponds to a specific shape, and based on this timing point, it controls the operation related to acquisition of the projected data (that is, it changes the scanning conditions or stops the scan). Thereby, in the X-ray CT system according to the present embodiment, the X-ray CT system itself can automatically control the operation for acquiring projected data without the operator when a positional relationship between two or more sites satisfies specific conditions.</p>
<p id="h-0050" num="0000">(In the Case of Changing the Frame Rate)</p>
<p id="p-0149" num="0148">Identifying the standard object, the position analyzing unit <b>212</b> compares this standard object, the outline object for each timing point, and the standard object with each other, calculating the change amount between the objects for each timing point. Specifically, the position analyzing unit <b>212</b> compares the shapes of both objects and calculates the difference (for example, the number of pixels of the part not overlapping between the objects) as the change amount.</p>
<p id="h-0051" num="0000">(Step S<b>213</b>)</p>
<p id="p-0150" num="0149">When a timing point in which the change amount is not calculated exists (Step S<b>213</b>,N), the position analyzing unit <b>212</b> calculates the change amount by comparing the outline object corresponding to this timing point with the standard object.</p>
<p id="h-0052" num="0000">(Step S<b>214</b>)</p>
<p id="p-0151" num="0150">Calculating the change amount for each timing point (Step S<b>213</b>, Y), the position analyzing unit <b>212</b> determines whether or not this change amount is within the range of predetermined amount (hereinafter, referred to as the &#x201c;specific amount&#x201d;), and identifies the time width that is formed by a timing point having the change amount within the specific amount. This time width corresponds to the time width T<b>21</b> (refer to <figref idref="DRAWINGS">FIG. 2E</figref>) in the first embodiment, while other time widths correspond to the time widths T<b>11</b> and T<b>12</b>. Hereinafter, the position analyzing unit <b>212</b> will be described assuming that this time width T<b>21</b> is identified. The position analyzing unit <b>212</b> notifies the reconfiguration processing unit <b>14</b> of the identified time width T<b>21</b>.</p>
<p id="p-0152" num="0151">Further, the following processing is the same as the first embodiment. In other words, the reconfiguration processing unit <b>14</b> carries out reconfiguration processing while changing the reconstruction conditions in the notified time width T<b>21</b>, and other time widths T<b>11</b> and T<b>12</b>, then reconstructs image data for display for each timing point based on the reconstruction conditions. The reconfiguration processing unit <b>14</b> causes the image data storage <b>10</b> to store the reconstructed series of image data for display. The image processor <b>22</b> carries out image processing on this image data and generates medical images, storing these medical images in the image storage <b>23</b> as related to the information indicating a corresponding timing point. The display controller <b>30</b> reads these medical images from the image storage <b>23</b> and arranges these medical images along a time sequence to cause the display <b>401</b> to display them as motion images.</p>
<p id="p-0153" num="0152">As described above, the medical image processing apparatus according to the present embodiment analyzes changes in the positional relationship between at least two or more sites temporarily working with each other in accordance with changes in the shape of the outline of the test object, the flexible sites being related to a surgery site of muscules, tendons or bones such as a joint and a spondylus. Moreover, the medical image processing apparatus identifies a time width in which the change amount of the shape is included in the specific range. Thereby, with respect to a time width in which the positional relationship in two or more sites is included in the specific range, the medical image processing apparatus according to the present embodiment can display the medical images at a higher frame rate than other time widths similar to the first embodiment.</p>
<heading id="h-0053" level="1">Third Embodiment</heading>
<p id="p-0154" num="0153">Next, the medical image processing apparatus according to the third embodiment will be described. In the first and the second embodiments, for example, based on the case in which the joint of the test object has a predetermined shape, identifying the time width to change the reconstruction conditions, the operation related to acquisition of the projected data is controlled. The medical image processing apparatus according to the present embodiment identifies the time width to change the reconstruction conditions in accordance with the change amount per unit time, and the control timing point related to acquisition of the projected data. Specifically explaining this with an example, as the speed of the movement of the observation object such as a joint is not always constant, in the case of displaying a series of operations at the same frame rate, sometimes it is difficult to observe fine movements of the observation object at a timing point in which the observation object moves fast. Therefore, the medical image processing apparatus according to the present embodiment reconstructs a time width in which the observation object (for example, each site configuring the joint) is moving fast at a higher volume rate than other time widths. Alternatively, detecting that the observation object is moving fast, based on this timing point, the scanning conditions are changed such that, at a high resolving power, the projected data is acquired. Due to such a configuration, when the observation object moves fast, by increasing the frame rate, it becomes possible to display the observation object such that fine movements thereof can be observed. Hereinafter, the configuration of the medical image processing apparatus according to the present embodiment will be described focusing on parts different from the first embodiment.</p>
<p id="h-0054" num="0000">(In the Case of Changing the Frame Rate)</p>
<p id="p-0155" num="0154">At first, the reconfiguration processing unit <b>14</b> carries out reconfiguration processing on the read projected data based on the predetermined reconstruction conditions for analysis, then generates image data for each timing point based on these reconstruction conditions. Here, it is described assuming that these reconstruction conditions are configured so as to be capable of extracting the bones in the test object from the projected data, similar to the first embodiment. The reconfiguration processing unit <b>14</b> causes the image data storage <b>10</b> to store image data generated for each timing point.</p>
<p id="p-0156" num="0155">Further, processing for reconstructing image data for display by the reconfiguration processing unit <b>14</b> according to the present embodiment is identical with the operation of the reconfiguration processing unit <b>14</b> according to the first embodiment. In other words, the reconfiguration processing unit <b>14</b> receives notification of a time width T<b>21</b> including a plurality of timing points from the position analyzing unit <b>212</b>. The reconfiguration processing unit <b>14</b> carries out reconfiguration processing while changing the reconstruction conditions in the notified time width T<b>21</b>, and other time widths T<b>11</b> and T<b>12</b>, then reconstructs image data for display to cause the image data storage <b>10</b> to store the reconstructed image data.</p>
<p id="p-0157" num="0156">At first, the configuration extracting part <b>21</b> reads the first image data reconstructed for analysis for each timing point. The configuration extracting part <b>21</b> outputs each of the read first image data for each timing point to the object extracting part <b>211</b>, and instructs it to extract the object. Upon receiving these instructions, the object extracting part <b>211</b> extracts the objects of the bones based on voxel data in this first image data. The object extracting part <b>211</b> outputs information indicating the objects of the bones extracted for each of the first image data at each timing point to the position analyzing unit <b>212</b> as related to the information indicating the corresponding timing point. The operations of this configuration extracting part <b>21</b> and object extracting part <b>211</b> are identical with the first embodiment.</p>
<p id="p-0158" num="0157">The position analyzing unit <b>212</b> receives the information indicating the objects of the bones from the object extracting part <b>211</b> for each timing point. The position analyzing unit <b>212</b> identifies the positional relationship of the bones at each timing point based on this information. This identifying method is the same as the first embodiment. Hereinafter, it is described assuming that, as illustrated in <figref idref="DRAWINGS">FIGS. 2A to 2C</figref>, the planes P<b>11</b> and P<b>13</b> are extracted from the objects M<b>11</b> and M<b>13</b>, and based on this, the positional relationship between the objects M<b>11</b> and M<b>13</b> is identified.</p>
<p id="p-0159" num="0158">When the positional relationship between the objects M<b>11</b> and M<b>13</b> is identified with respect to a series of timing points, the position analyzing unit <b>212</b> compares the positional relationships formed by the objects M<b>11</b> and M<b>13</b> between the adjacent timing points in order to calculate the change amount. This comparison of the positional relationship may be calculated based on the angle and/or the distance of the planes P<b>11</b> and P<b>13</b> similar to the first embodiment. The first image data is reconstructed at a constant volume rate across the entire series of time widths. Therefore, if the change amount between the adjacent timing points is calculated, this change amount is increased particularly between the timing points in which the test object is moving fast. In other words, the position analyzing unit <b>212</b> determines whether or not each change amount calculated between the adjacent timing points is not less than a predetermined amount (hereinafter, referred to as the &#x201c;specific amount&#x201d;) and identifies a time width formed between the timing points with a change amount no less than the specific amount. This time width corresponds to the time width T<b>21</b> (refer to <figref idref="DRAWINGS">FIG. 2E</figref>) in the first embodiment, while other time widths correspond to the time widths T<b>11</b> and T<b>12</b>. Hereinafter, it is explained that the position analyzing unit <b>212</b> identifies this time width T<b>21</b>. The position analyzing unit <b>212</b> notifies the reconfiguration processing unit <b>14</b> of the identified time width T<b>21</b>.</p>
<p id="p-0160" num="0159">The following processes are identical with the first embodiment. In other words, the reconfiguration processing unit <b>14</b> carries out reconfiguration processing while changing the reconstruction conditions between the notified time width T<b>21</b> and other time widths T<b>11</b> and T<b>12</b>, then reconstructs the image data for display for each timing point based on the reconstruction conditions. The reconfiguration processing unit <b>14</b> causes the image data storage <b>10</b> to store the reconstructed series of image data for display. The image processor <b>22</b> carries out image processing on this image data and generates medical images, storing these medical images in the image storage <b>23</b> as related to the information indicating a corresponding timing point. The display controller <b>30</b> reads these medical images from the image storage <b>23</b> and arranges these medical images along a time sequence to cause the display <b>401</b> to display them as motion images.</p>
<p id="p-0161" num="0160">Further, in the above description, similar to the first embodiment, the time width T<b>21</b> to change the reconstruction conditions is identified based on the positional relationship between the objects of the bones; however, similar to the second embodiment, the time width T<b>21</b> may be identified based on the change in the shape of the outline of the test object. In this case, the position analyzing unit <b>212</b> may compare the outline objects between the adjacent timing points and may define the difference as the change amount.</p>
<p id="p-0162" num="0161">As described above, the medical image processing apparatus according to the present embodiment identifies the time width to change the reconstruction conditions in accordance with the change amount per unit time of at least two or more sites temporally working with each other, the flexible sites being related to a surgery site of muscules, tendons or bones such as a joint and a spondylus. Moreover, with respect to the identified time width, the image data is reconstructed at a higher volume rate than other time widths. Thereby, the medical image processing apparatus according to the present embodiment can display the medical images at a high frame rate when the observation object is moving fast.</p>
<p id="h-0055" num="0000">(In the Case of Changing the Scanning Conditions and the Scan Stopping Process)</p>
<p id="p-0163" num="0162">The X-ray CT system according to the present embodiment grasps the position and direction of respective sites configuring the observation object as well as the relative positional relationship thereof (hereinafter, it is simply referred to as the &#x201c;positional relationship&#x201d;) by analyzing the reconstructed image data. Therefore, the reconfiguration processing unit <b>14</b> reconstructs image data for analysis separately from the image data for display. Specifically, the reconfiguration processing unit <b>14</b> sequentially reads the acquired projected data from the projected data storage <b>13</b> in parallel with processing for acquiring projected data by means of the X-ray photographing unit <b>500</b>. The reconfiguration processing unit <b>14</b> generates image data for analysis for each timing point based on these reconstruction conditions by carrying out reconfiguration processing on this read projected data based on the predetermined reconstruction conditions for analysis.</p>
<p id="p-0164" num="0163">According to the present embodiment, the reconstruction conditions for reconstructing the image data for analysis is configured so as to be capable of extracting the bones in the test object from the projected data. In other words, this image data is reconstructed so as to be capable of extracting the bones. Further, sometimes the reconstruction conditions in this case are referred to as &#x201c;first conditions&#x201d; and the image data generated based on these reconstruction conditions is referred to as &#x201c;first image data.&#x201d; The reconfiguration processing unit <b>14</b> causes the image data storage <b>10</b> to store the image data sequentially generated for each timing point.</p>
<p id="p-0165" num="0164">Further, processing for reconstructing image data for display by the reconfiguration processing unit <b>14</b> according to the present embodiment is identical with the operation of the reconfiguration processing unit <b>14</b> according to the first embodiment. In other words, the reconfiguration processing unit <b>14</b> generates image data for display for each timing point based on these reconstruction conditions by reading the projected data from the projected data storage <b>13</b> and carrying out reconfiguration processing based on the predetermined reconstruction conditions for display. Hereinafter, sometimes the reconstruction conditions for display are referred to as &#x201c;second conditions&#x201d; and the image data generated based on these reconstruction conditions are referred to as &#x201c;second projected data.&#x201d; The reconfiguration processing unit <b>14</b> causes the image data storage <b>10</b> to store this image data for display generated for each timing point.</p>
<p id="p-0166" num="0165">The image data for display is not always needed to be operated in parallel with the processing for obtaining projected data. For example, the reconfiguration processing unit <b>14</b> may reconstruct the image data for display after a series of projected data is obtained. This operation is also identical with the first embodiment.</p>
<p id="p-0167" num="0166">The configuration extracting part <b>21</b> sequentially reads from the image data storage <b>10</b> the image data for analysis sequentially generated by the reconfiguration processing unit <b>14</b> for each timing point and stored in the image data storage <b>10</b>. In this case, the operation by the reconfiguration processing unit <b>14</b> and the operation for reading the image data for analysis by the configuration extracting part <b>21</b> may be synchronized with each other. The configuration extracting part <b>21</b> successively outputs the first image per read-out timing to the object extracting part <b>211</b>, and instructs extraction of the object from the first image data. This operation of the configuration extracting part <b>21</b> is the same as the first embodiment.</p>
<p id="p-0168" num="0167">The object extracting part <b>211</b> sequentially receives the first image data for each timing point from the configuration extracting part <b>21</b>. The object extracting part <b>211</b> extracts the objects of the bones based on voxel data in this first image data. The object extracting part <b>211</b> sequentially outputs information indicating the objects of the bones extracted for each of the first image data at each timing point to the position analyzing unit <b>212</b> as related to the information indicating the corresponding timing point. These operations of the configuration extracting part <b>21</b> and the object extracting part <b>211</b> are the same as the first embodiment.</p>
<p id="p-0169" num="0168">The position analyzing unit <b>212</b> sequentially receives the information indicating the objects of the bones from the extracting part <b>211</b> for each timing point. The position analyzing unit <b>212</b> identifies the positional relationship of the bones at each timing point based on this information. This identifying method is the same as the first embodiment. Hereinafter, it is described assuming that, as illustrated in <figref idref="DRAWINGS">FIGS. 2A to 2C</figref>, the planes P<b>11</b> and P<b>13</b> are extracted from the objects M<b>11</b> and M<b>13</b>, and based on this, the positional relationship between the objects M<b>11</b> and M<b>13</b> is identified.</p>
<p id="p-0170" num="0169">When the positional relationship formed by the objects M<b>11</b> and M<b>13</b> is identified, the position analyzing unit <b>212</b> compares the positional relationships formed by the objects M<b>11</b> and M<b>13</b> between the adjacent timing points to calculate the change amount. This comparison of the positional relationship may be calculated based on the angle and the distance of the planes P<b>11</b> and P<b>13</b> similar to the first embodiment. If the change amount between the adjacent timing points is calculated, this change amount is increased particularly between the timing points in which the test object is moving fast. In other words, the position analyzing unit <b>212</b> determines whether or not the change amount calculated between the adjacent timing points is not less than a predetermined amount (hereinafter, referred to as a &#x201c;specific amount&#x201d;) and identifies a time width formed between the timing points with a change amount no less than the specific amount.</p>
<p id="p-0171" num="0170">Thus, the position analyzing unit <b>212</b> sequentially calculates the change amount between the adjacent timing points, then determines whether or not the calculated change amount is not less than the specific amount. Thereby, the position analyzing unit <b>212</b> detects a timing point in which the change amount between the adjacent timing points is no less than the specific amount, namely, a timing point in which the speed of the operating test object is made faster. Further, this timing point corresponds to &#x201c;one timing point.&#x201d; Detecting this timing point, the position analyzing unit <b>212</b> instructs the scan control <b>501</b> to change the scanning condition or stop the scan. Upon receiving these instructions, the scan control <b>501</b> carries out the instructed operation (that is, change of the scanning condition or stop the scan). Whether change of the scanning condition or stop of the scan should be instructed may be related to the information indicating the detected timing point (in other words, the change amount for detecting the timing point) in advance.</p>
<p id="p-0172" num="0171">In addition, the position analyzing unit <b>212</b> may be operated so as to determine whether or not the change amount is not more than the specific amount. For example, the position analyzing unit <b>212</b> may detect a timing point in which the change amount is no more than the specific amount, and based on this timing point, may instruct the scan control <b>501</b> to stop the scan. By causing the position analyzing unit <b>212</b> to thus operate, for example, at a timing point in which the joints completely bend and the positional relationship between the sites constructing the joints never changes, it becomes possible to cause the X-ray CT system itself to stop acquisition of the projected data.</p>
<p id="p-0173" num="0172">Further, the following operations are the same as the first embodiment. In other words, if the second projected data reconstructed for display is stored in the image data storage <b>10</b>, the configuration extracting part <b>21</b> reads this and transfers it to the image processor <b>22</b>. The first image data generated for analysis may be operated for display. In this case, the position analyzing unit <b>212</b> may transfer the image data that has been already read for analysis to the image processor <b>22</b>. The image processor <b>22</b> carries out the image processing on this image data and generates medical images, storing these medical images in the image storage <b>23</b> as related to the information indicating a corresponding timing point. The display controller <b>30</b> reads these medical images from the image storage <b>23</b> and arranges these medical images along a time sequence to cause the display <b>401</b> to display them as motion images.</p>
<p id="p-0174" num="0173">Further, in the above description, similar to the first embodiment, based on the positional relationship between the objects of the bones, the timing point in which the operation related to acquisition of the projected data is controlled is identified; however, similar to the second embodiment, based on the change in the shape of the outline of the test object, this timing point may be identified. In this case, the position analyzing unit <b>212</b> may compare the outline object between the adjacent timing points and may define the difference as the change amount.</p>
<p id="p-0175" num="0174">As described above, the X-ray CT system according to the present embodiment controls the operation related to acquisition of the projected data in accordance with the change amount per unit time of the positional relationship between at least two or more sites constructing a flexible site related to a surgery site of muscules, tendons or bones such as a joint and a spondylus. Thereby, in the X-ray CT system according to the present embodiment, the X-ray CT system itself can automatically control the operation related to acquisition of the projected data without the operator when the positional relationship between two or more sites satisfies the specific conditions.</p>
<p id="p-0176" num="0175">While certain embodiments have been described, these embodiments have been presented by way of example only, and are not intended to limit the scope of the inventions. Indeed, the novel systems described herein may be embodied in a variety of their forms; furthermore, various omissions, substitutions and changes in the form of the systems described herein may be made without departing from the spirit of the inventions. The accompanying claims and their equivalents are intended to cover such forms or modifications as would fall within the scope and spirit of the inventions.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A medical image processing apparatus, comprising:
<claim-text>a photographing unit configured to scan a flexible site of the living body having a plurality of components in order to obtain projected data,</claim-text>
<claim-text>a reconfiguration processing unit configured to carry out reconfiguration processing on the projected data to generate image data of the flexible site at a plurality of timing points,</claim-text>
<claim-text>an extracting unit configured to extract the plurality of components configuring the flexible site from the respective image data, and</claim-text>
<claim-text>an analyzing unit configured to determine a positional relation of the plurality of extracted components configuring the flexible site at the plurality of timing points and to specify image data of a specific timing point through conditional determination of the obtained positional relation, the specific timing point corresponding to the determination result.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The medical image processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein
<claim-text>the reconfiguration processing unit is configured to carry out reconstructing processing on the projected data at a first frame rate to generate first image data of the flexible site,</claim-text>
<claim-text>the extracting unit is configured to extract the plurality of components from the first image data, and</claim-text>
<claim-text>the reconfiguration processing unit is further configured to carry out reconfiguration processing at a second frame rate, which is different from the first frame rate, on the projected data within the specific timing from among the projected data regarding the specific timing point specified by the analyzing unit, to generate second image data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The medical image processing apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein
<claim-text>the sites are bones,</claim-text>
<claim-text>the extracting unit is configured to respectively extract the bones, and</claim-text>
<claim-text>the analyzing unit is configured to carry out the conditional determination of the positional relation of the bones configuring the extracted flexible site.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The medical image processing apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein
<claim-text>the analyzing unit is configured to form planes from three or more points of shape characteristics regarding the respective extracted components and to determine the positional relation of two of the formed planes, to realize the positional relation of the components.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The medical image processing apparatus according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein
<claim-text>the analyzing unit is configured to carry out the conditional determination of the positional relation of the planes based on the angle configured from the two formed planes.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The medical image processing apparatus according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein
<claim-text>the analyzing unit is configured to carry out the conditional determination of the positional relation of the planes based on a distance between the two formed planes.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The medical image processing apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein
<claim-text>the analyzing unit is configured to connect two points of shape characteristics regarding the respective extracted components in order to shape lines, and to determine the positional relation of the two shaped lines, to realize the positional relation of the components.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The medical image processing apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein
<claim-text>the analyzing unit is configured to specify the positional relation of the components based on outlines of the extracted components.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The medical image processing apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein
<claim-text>the analyzing unit is configured to specify the positional relation of the components based on information showing shades of the extracted components.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The medical image processing apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein
<claim-text>the extracting unit is configured to extract the plurality of components based on surface layers of the plurality of components.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The medical image processing apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein
<claim-text>the extracting unit extracts a region containing the respective sites configuring the flexible site based on the respective surface layers of a tissue, and</claim-text>
<claim-text>the analyzing unit conditionally determines the amount of change between the shape of the region and a shape determined in advance.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The medical image processing apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein
<claim-text>the analyzing unit calculates the amount of change in the position of the plurality of components between the pluralities of timing points, and conditionally makes a determination based on the calculated amount of change.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The medical image processing apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, comprising one timing point specifying unit that specifies one timing point in the projected data in advance, wherein,
<claim-text>with the positional relation of the plurality of components corresponding to the specific timing point as the standard, the analyzing unit conditionally makes a determination based on the amount of change from the standard.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The medical image processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising
<claim-text>a controlling means for stopping the acquisition of the projected data or changing the imaging conditions by means of the photographing unit when the specific timing point is specified according to the conditional determination by the analyzing unit.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The medical image processing apparatus according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein
<claim-text>the sites are bones,</claim-text>
<claim-text>the extracting unit respectively extracts the bones, and</claim-text>
<claim-text>the analyzing unit conditionally determines the positional relation of the bones configuring the extracted flexible site.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The medical image processing apparatus according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein
<claim-text>the analyzing unit shapes surfaces shaped from three or more points of shape characteristics regarding the respective plurality of extracted components and conditionally determines the positional relation of the surface corresponding with the components, thereby specifying the specific timing point.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The medical image processing apparatus according to <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein
<claim-text>the analyzing unit calculates the angle configured from the surfaces corresponding with the plurality of shaped components, specifies the image data wherein the calculated angle is the predetermined angle, and realizes the timing point corresponding to the image data as the specific timing point.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The medical image processing apparatus according to <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein
<claim-text>the analyzing unit calculates the distance between the surfaces corresponding with the plurality of shaped components, specifies the image data in which the calculated distance is the predetermined distance, and realizes the timing corresponding to the image data as the specific timing point.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The medical image processing apparatus according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein
<claim-text>the analyzing unit shapes the lines shaped by two points of shape characteristics regarding the respective plurality of extracted components, and conditionally determines the positional relation of the line corresponding to the components.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The medical image processing apparatus according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein
<claim-text>the analyzing unit specifies the positional relation of the components based on the external form of the respective plurality of extracted components.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The medical image processing apparatus according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein
<claim-text>the analyzing unit specifies the positional relation of the components based on information showing the shades of the respective plurality of extracted components.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The medical image processing apparatus according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein
<claim-text>the extracting unit extracts the plurality of components based on the respective surface layers of the flexible site.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The medical image processing apparatus according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein
<claim-text>the extracting unit extracts the plurality of components as the region comprising a plurality of components based on the surface layers of the flexible site, and</claim-text>
<claim-text>the analyzing unit specifies the timing point in which the shape of the region between the pluralities of timing points corresponds with the shape determined in advance, and realizes the timing point corresponding to the image data as the specific timing point.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. The medical image processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein
<claim-text>the analyzing unit successively analyzes the positional relation of the plurality of components extracted from the image data for each of the timing points between adjacent timing points and conditionally determines the amount of change of the positional relation between the timing points, thereby specifying the specific timing point corresponding with the determination result.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00025" num="00025">
<claim-text>25. The medical image processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein
<claim-text>the analyzing unit successively analyzes the positional relation of the plurality of components extracted from the image data for each of the timing points, specifies the specific timing point in which the plurality of components achieves the predetermined positional relation, and supplements information to distinguish from other image data with respect to the image data corresponding to the specific timing point. </claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
