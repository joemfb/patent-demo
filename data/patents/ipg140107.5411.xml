<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626511-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626511</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12692307</doc-number>
<date>20100122</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<us-term-of-grant>
<us-term-extension>396</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20130101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>10</class>
<subclass>L</subclass>
<main-group>21</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>704270</main-classification>
<further-classification>704  8</further-classification>
<further-classification>704  9</further-classification>
<further-classification>704231</further-classification>
<further-classification>704235</further-classification>
<further-classification>704275</further-classification>
<further-classification>715777</further-classification>
<further-classification>707769</further-classification>
<further-classification>379 45</further-classification>
</classification-national>
<invention-title id="d2e53">Multi-dimensional disambiguation of voice commands</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5027406</doc-number>
<kind>A</kind>
<name>Roberts et al.</name>
<date>19910600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704244</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5712957</doc-number>
<kind>A</kind>
<name>Waibel et al.</name>
<date>19980100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704240</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5797123</doc-number>
<kind>A</kind>
<name>Chou et al.</name>
<date>19980800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5895466</doc-number>
<kind>A</kind>
<name>Goldberg et al.</name>
<date>19990400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>  1  1</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6021384</doc-number>
<kind>A</kind>
<name>Gorin et al.</name>
<date>20000200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704  1</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6064959</doc-number>
<kind>A</kind>
<name>Young et al.</name>
<date>20000500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704251</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6453292</doc-number>
<kind>B2</kind>
<name>Ramaswamy et al.</name>
<date>20020900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704235</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6513006</doc-number>
<kind>B2</kind>
<name>Howard et al.</name>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704257</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6523061</doc-number>
<kind>B1</kind>
<name>Halverson et al.</name>
<date>20030200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709202</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6556970</doc-number>
<kind>B1</kind>
<name>Sasaki et al.</name>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704257</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6587824</doc-number>
<kind>B1</kind>
<name>Everhart et al.</name>
<date>20030700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704275</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6629066</doc-number>
<kind>B1</kind>
<name>Jackson et al.</name>
<date>20030900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704  9</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>6633235</doc-number>
<kind>B1</kind>
<name>Hsu et al.</name>
<date>20031000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>340 1228</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>6643620</doc-number>
<kind>B1</kind>
<name>Contolini et al.</name>
<date>20031100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704270</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>6795808</doc-number>
<kind>B1</kind>
<name>Strubbe et al.</name>
<date>20040900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704275</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>6839668</doc-number>
<kind>B2</kind>
<name>Kuo et al.</name>
<date>20050100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704244</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>6839669</doc-number>
<kind>B1</kind>
<name>Gould et al.</name>
<date>20050100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704246</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>6993482</doc-number>
<kind>B2</kind>
<name>Ahlenius</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704235</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>7020609</doc-number>
<kind>B2</kind>
<name>Thrift et al.</name>
<date>20060300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>7042701</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>7099829</doc-number>
<kind>B2</kind>
<name>Gomez</name>
<date>20060800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704275</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>7315818</doc-number>
<kind>B2</kind>
<name>Stevens et al.</name>
<date>20080100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704235</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>7437297</doc-number>
<kind>B2</kind>
<name>Chaar et al.</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704275</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>7447299</doc-number>
<kind>B1</kind>
<name>Partovi et al.</name>
<date>20081100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>379 8801</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>7450698</doc-number>
<kind>B2</kind>
<name>Bushey et al.</name>
<date>20081100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>7457751</doc-number>
<kind>B2</kind>
<name>Shostak</name>
<date>20081100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704251</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>7519534</doc-number>
<kind>B2</kind>
<name>Maddux et al.</name>
<date>20090400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704255</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>7555431</doc-number>
<kind>B2</kind>
<name>Bennett</name>
<date>20090600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704255</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>7567907</doc-number>
<kind>B2</kind>
<name>Greene et al.</name>
<date>20090700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704275</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>7599838</doc-number>
<kind>B2</kind>
<name>Gong et al.</name>
<date>20091000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704258</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>7603360</doc-number>
<kind>B2</kind>
<name>Ramer et al.</name>
<date>20091000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>  1  1</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>US</country>
<doc-number>7693720</doc-number>
<kind>B2</kind>
<name>Kennewick et al.</name>
<date>20100400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704275</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>US</country>
<doc-number>7729920</doc-number>
<kind>B2</kind>
<name>Chaar et al.</name>
<date>20100600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704275</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00033">
<document-id>
<country>US</country>
<doc-number>7840579</doc-number>
<kind>B2</kind>
<name>Samuelson et al.</name>
<date>20101100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>707758</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00034">
<document-id>
<country>US</country>
<doc-number>7917368</doc-number>
<kind>B2</kind>
<name>Weinberg et al.</name>
<date>20110300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704275</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00035">
<document-id>
<country>US</country>
<doc-number>7949529</doc-number>
<kind>B2</kind>
<name>Weider et al.</name>
<date>20110500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704270</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00036">
<document-id>
<country>US</country>
<doc-number>8065148</doc-number>
<kind>B2</kind>
<name>Huerta et al.</name>
<date>20111100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704257</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00037">
<document-id>
<country>US</country>
<doc-number>8069041</doc-number>
<kind>B2</kind>
<name>Kuboyama et al.</name>
<date>20111100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704236</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00038">
<document-id>
<country>US</country>
<doc-number>8175887</doc-number>
<kind>B2</kind>
<name>Shostak</name>
<date>20120500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704275</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00039">
<document-id>
<country>US</country>
<doc-number>8234120</doc-number>
<kind>B2</kind>
<name>Agapi et al.</name>
<date>20120700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704275</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00040">
<document-id>
<country>US</country>
<doc-number>8239206</doc-number>
<kind>B1</kind>
<name>LeBeau et al.</name>
<date>20120800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704275</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00041">
<document-id>
<country>US</country>
<doc-number>8244544</doc-number>
<kind>B1</kind>
<name>LeBeau et al.</name>
<date>20120800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704275</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00042">
<document-id>
<country>US</country>
<doc-number>8271107</doc-number>
<kind>B2</kind>
<name>Bodin et al.</name>
<date>20120900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>700 94</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00043">
<document-id>
<country>US</country>
<doc-number>8275617</doc-number>
<kind>B1</kind>
<name>Morgan et al.</name>
<date>20120900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704251</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00044">
<document-id>
<country>US</country>
<doc-number>8296383</doc-number>
<kind>B2</kind>
<name>Lindahl</name>
<date>20121000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709206</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00045">
<document-id>
<country>US</country>
<doc-number>8326328</doc-number>
<kind>B2</kind>
<name>LeBeau et al.</name>
<date>20121200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>4554564</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00046">
<document-id>
<country>US</country>
<doc-number>8380514</doc-number>
<kind>B2</kind>
<name>Bodin et al.</name>
<date>20130200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704270</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00047">
<document-id>
<country>US</country>
<doc-number>8473289</doc-number>
<kind>B2</kind>
<name>Jitkoff et al.</name>
<date>20130600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704231</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00048">
<document-id>
<country>US</country>
<doc-number>8478590</doc-number>
<kind>B2</kind>
<name>LeBeau et al.</name>
<date>20130700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704235</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00049">
<document-id>
<country>US</country>
<doc-number>2001/0041980</doc-number>
<kind>A1</kind>
<name>Howard et al.</name>
<date>20011100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704270</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00050">
<document-id>
<country>US</country>
<doc-number>2002/0049805</doc-number>
<kind>A1</kind>
<name>Yamada et al.</name>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709202</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00051">
<document-id>
<country>US</country>
<doc-number>2002/0128843</doc-number>
<kind>A1</kind>
<name>Firman</name>
<date>20020900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704270</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00052">
<document-id>
<country>US</country>
<doc-number>2002/0143535</doc-number>
<kind>A1</kind>
<name>Kist et al.</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704251</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00053">
<document-id>
<country>US</country>
<doc-number>2002/0198714</doc-number>
<kind>A1</kind>
<name>Zhou</name>
<date>20021200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704252</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00054">
<document-id>
<country>US</country>
<doc-number>2003/0093419</doc-number>
<kind>A1</kind>
<name>Bangalore et al.</name>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>707  3</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00055">
<document-id>
<country>US</country>
<doc-number>2003/0149564</doc-number>
<kind>A1</kind>
<name>Gong et al.</name>
<date>20030800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704246</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00056">
<document-id>
<country>US</country>
<doc-number>2003/0149566</doc-number>
<kind>A1</kind>
<name>Levin et al.</name>
<date>20030800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704256</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00057">
<document-id>
<country>US</country>
<doc-number>2004/0181747</doc-number>
<kind>A1</kind>
<name>Hull et al.</name>
<date>20040900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>7155001</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00058">
<document-id>
<country>US</country>
<doc-number>2004/0193420</doc-number>
<kind>A1</kind>
<name>Kennewick et al.</name>
<date>20040900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704257</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00059">
<document-id>
<country>US</country>
<doc-number>2004/0199375</doc-number>
<kind>A1</kind>
<name>Ehsani et al.</name>
<date>20041000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704  4</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00060">
<document-id>
<country>US</country>
<doc-number>2005/0004799</doc-number>
<kind>A1</kind>
<name>Lyudovyk</name>
<date>20050100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704254</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00061">
<document-id>
<country>US</country>
<doc-number>2005/0021826</doc-number>
<kind>A1</kind>
<name>Kumar</name>
<date>20050100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709232</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00062">
<document-id>
<country>US</country>
<doc-number>2005/0283364</doc-number>
<kind>A1</kind>
<name>Longe et al.</name>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704257</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00063">
<document-id>
<country>US</country>
<doc-number>2006/0106614</doc-number>
<kind>A1</kind>
<name>Mowatt et al.</name>
<date>20060500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704275</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00064">
<document-id>
<country>US</country>
<doc-number>2006/0129387</doc-number>
<kind>A1</kind>
<name>Mitchell et al.</name>
<date>20060600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704201</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00065">
<document-id>
<country>US</country>
<doc-number>2006/0143007</doc-number>
<kind>A1</kind>
<name>Koh et al.</name>
<date>20060600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704243</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00066">
<document-id>
<country>US</country>
<doc-number>2007/0033043</doc-number>
<kind>A1</kind>
<name>Hyakumoto</name>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704255</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00067">
<document-id>
<country>US</country>
<doc-number>2007/0203701</doc-number>
<kind>A1</kind>
<name>Ruwisch</name>
<date>20070800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704254</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00068">
<document-id>
<country>US</country>
<doc-number>2007/0208567</doc-number>
<kind>A1</kind>
<name>Amento et al.</name>
<date>20070900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704270</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00069">
<document-id>
<country>US</country>
<doc-number>2007/0239531</doc-number>
<kind>A1</kind>
<name>Beaufays et al.</name>
<date>20071000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>705 14</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00070">
<document-id>
<country>US</country>
<doc-number>2008/0221903</doc-number>
<kind>A1</kind>
<name>Kanevsky et al.</name>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704275</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00071">
<document-id>
<country>US</country>
<doc-number>2008/0243501</doc-number>
<kind>A1</kind>
<name>Hafsteinsson et al.</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704235</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00072">
<document-id>
<country>US</country>
<doc-number>2008/0243514</doc-number>
<kind>A1</kind>
<name>Gopinath et al.</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704270</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00073">
<document-id>
<country>US</country>
<doc-number>2009/0030684</doc-number>
<kind>A1</kind>
<name>Cerra et al.</name>
<date>20090100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704236</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00074">
<document-id>
<country>US</country>
<doc-number>2009/0030696</doc-number>
<kind>A1</kind>
<name>Cerra et al.</name>
<date>20090100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00075">
<document-id>
<country>US</country>
<doc-number>2009/0094030</doc-number>
<kind>A1</kind>
<name>White</name>
<date>20090400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704246</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00076">
<document-id>
<country>US</country>
<doc-number>2009/0150156</doc-number>
<kind>A1</kind>
<name>Kennewick et al.</name>
<date>20090600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704257</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00077">
<document-id>
<country>US</country>
<doc-number>2009/0177461</doc-number>
<kind>A1</kind>
<name>Ehsani et al.</name>
<date>20090700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704  2</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00078">
<document-id>
<country>US</country>
<doc-number>2009/0204410</doc-number>
<kind>A1</kind>
<name>Mozer et al.</name>
<date>20090800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704275</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00079">
<document-id>
<country>US</country>
<doc-number>2009/0216538</doc-number>
<kind>A1</kind>
<name>Weinberg et al.</name>
<date>20090800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704275</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00080">
<document-id>
<country>US</country>
<doc-number>2009/0240488</doc-number>
<kind>A1</kind>
<name>White et al.</name>
<date>20090900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704  9</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00081">
<document-id>
<country>US</country>
<doc-number>2009/0248415</doc-number>
<kind>A1</kind>
<name>Jablokov et al.</name>
<date>20091000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704251</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00082">
<document-id>
<country>US</country>
<doc-number>2009/0306989</doc-number>
<kind>A1</kind>
<name>Kaji</name>
<date>20091200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704270</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00083">
<document-id>
<country>US</country>
<doc-number>2009/0326936</doc-number>
<kind>A1</kind>
<name>Nagashima</name>
<date>20091200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704235</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00084">
<document-id>
<country>US</country>
<doc-number>2010/0042414</doc-number>
<kind>A1</kind>
<name>Lewis et al.</name>
<date>20100200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>7042701</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00085">
<document-id>
<country>US</country>
<doc-number>2010/0049521</doc-number>
<kind>A1</kind>
<name>Ruback et al.</name>
<date>20100200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704257</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00086">
<document-id>
<country>US</country>
<doc-number>2010/0184011</doc-number>
<kind>A1</kind>
<name>Comerford et al.</name>
<date>20100700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>434321</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00087">
<document-id>
<country>US</country>
<doc-number>2010/0185446</doc-number>
<kind>A1</kind>
<name>Homma et al.</name>
<date>20100700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704251</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00088">
<document-id>
<country>US</country>
<doc-number>2010/0185448</doc-number>
<kind>A1</kind>
<name>Meisel</name>
<date>20100700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>7042561</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00089">
<document-id>
<country>US</country>
<doc-number>2010/0232595</doc-number>
<kind>A1</kind>
<name>Bushey et al.</name>
<date>20100900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37926502</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00090">
<document-id>
<country>US</country>
<doc-number>2010/0286983</doc-number>
<kind>A1</kind>
<name>Cho</name>
<date>20101100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704246</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00091">
<document-id>
<country>US</country>
<doc-number>2011/0126146</doc-number>
<kind>A1</kind>
<name>Samuelson et al.</name>
<date>20110500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715777</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00092">
<document-id>
<country>US</country>
<doc-number>2011/0131045</doc-number>
<kind>A1</kind>
<name>Cristo et al.</name>
<date>20110600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704249</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00093">
<document-id>
<country>US</country>
<doc-number>2011/0153325</doc-number>
<kind>A1</kind>
<name>Ballinger et al.</name>
<date>20110600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704235</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00094">
<document-id>
<country>US</country>
<doc-number>2011/0161347</doc-number>
<kind>A1</kind>
<name>Johnston</name>
<date>20110600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>707769</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00095">
<document-id>
<country>US</country>
<doc-number>2011/0166851</doc-number>
<kind>A1</kind>
<name>LeBeau et al.</name>
<date>20110700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704  9</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00096">
<document-id>
<country>US</country>
<doc-number>2011/0166858</doc-number>
<kind>A1</kind>
<name>Arun</name>
<date>20110700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704243</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00097">
<document-id>
<country>US</country>
<doc-number>2011/0184730</doc-number>
<kind>A1</kind>
<name>LeBeau et al.</name>
<date>20110700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704201</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00098">
<document-id>
<country>US</country>
<doc-number>2011/0184740</doc-number>
<kind>A1</kind>
<name>Gruenstein et al.</name>
<date>20110700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704275</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00099">
<document-id>
<country>US</country>
<doc-number>2011/0231191</doc-number>
<kind>A1</kind>
<name>Miyazaki</name>
<date>20110900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704243</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00100">
<document-id>
<country>US</country>
<doc-number>2011/0246944</doc-number>
<kind>A1</kind>
<name>Byrne et al.</name>
<date>20111000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715835</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00101">
<document-id>
<country>US</country>
<doc-number>2011/0289064</doc-number>
<kind>A1</kind>
<name>Lebeau et al.</name>
<date>20111100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>707706</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00102">
<document-id>
<country>US</country>
<doc-number>2011/0294476</doc-number>
<kind>A1</kind>
<name>Roth et al.</name>
<date>20111200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>4554141</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00103">
<document-id>
<country>US</country>
<doc-number>2011/0301955</doc-number>
<kind>A1</kind>
<name>Byrne et al.</name>
<date>20111200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704251</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00104">
<document-id>
<country>US</country>
<doc-number>2012/0015674</doc-number>
<kind>A1</kind>
<name>LeBeau et al.</name>
<date>20120100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>4554563</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00105">
<document-id>
<country>US</country>
<doc-number>2012/0022853</doc-number>
<kind>A1</kind>
<name>Ballinger et al.</name>
<date>20120100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704  8</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00106">
<document-id>
<country>US</country>
<doc-number>2012/0023097</doc-number>
<kind>A1</kind>
<name>LeBeau et al.</name>
<date>20120100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>707723</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00107">
<document-id>
<country>US</country>
<doc-number>2012/0035924</doc-number>
<kind>A1</kind>
<name>Jitkoff et al.</name>
<date>20120200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704235</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00108">
<document-id>
<country>US</country>
<doc-number>2012/0035932</doc-number>
<kind>A1</kind>
<name>Jitkoff et al.</name>
<date>20120200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704254</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00109">
<document-id>
<country>US</country>
<doc-number>2012/0084079</doc-number>
<kind>A1</kind>
<name>Gruenstein et al.</name>
<date>20120400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704201</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00110">
<document-id>
<country>US</country>
<doc-number>2012/0245944</doc-number>
<kind>A1</kind>
<name>Gruber et al.</name>
<date>20120900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00111">
<document-id>
<country>US</country>
<doc-number>2012/0310645</doc-number>
<kind>A1</kind>
<name>Gruenstein et al.</name>
<date>20121200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704235</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00112">
<document-id>
<country>US</country>
<doc-number>2013/0024200</doc-number>
<kind>A1</kind>
<name>Yoon et al.</name>
<date>20130100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704275</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00113">
<document-id>
<country>US</country>
<doc-number>2013/0041670</doc-number>
<kind>A1</kind>
<name>Morgan et al.</name>
<date>20130200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704275</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00114">
<document-id>
<country>US</country>
<doc-number>2013/0211815</doc-number>
<kind>A1</kind>
<name>Seligman et al.</name>
<date>20130800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704  2</main-classification></classification-national>
</us-citation>
<us-citation>
<nplcit num="00115">
<othercit>European Search Report for European Application No. EP 10 17 5449, mailed Aug. 12, 2011, 3 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>19</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>704270</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704275</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704257</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>7</number-of-drawing-sheets>
<number-of-figures>9</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20110184730</doc-number>
<kind>A1</kind>
<date>20110728</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>LeBeau</last-name>
<first-name>Michael J.</first-name>
<address>
<city>Palo Alto</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Byrne</last-name>
<first-name>William J.</first-name>
<address>
<city>Davis</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Jitkoff</last-name>
<first-name>Nicholas</first-name>
<address>
<city>Palo Alto</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="004" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Gruenstein</last-name>
<first-name>Alexander H.</first-name>
<address>
<city>Mountain View</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>LeBeau</last-name>
<first-name>Michael J.</first-name>
<address>
<city>Palo Alto</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Byrne</last-name>
<first-name>William J.</first-name>
<address>
<city>Davis</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Jitkoff</last-name>
<first-name>Nicholas</first-name>
<address>
<city>Palo Alto</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="004" designation="us-only">
<addressbook>
<last-name>Gruenstein</last-name>
<first-name>Alexander H.</first-name>
<address>
<city>Mountain View</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Fish &#x26; Richardson P.C.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Google Inc.</orgname>
<role>02</role>
<address>
<city>Mountain View</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Desir</last-name>
<first-name>Pierre-Louis</first-name>
<department>2659</department>
</primary-examiner>
<assistant-examiner>
<last-name>Sirjani</last-name>
<first-name>Fariba</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for processing voice commands. In one aspect, a method includes receiving an audio signal at a server, performing, by the server, speech recognition on the audio signal to identify one or more candidate terms that match one or more portions of the audio signal, identifying one or more possible intended actions for each candidate term, providing information for display on a client device, the information specifying the candidate terms and the actions for each candidate term, receiving from the client device an indication of an action selected by a user, where the action was selected from among the actions included in the provided information, and invoking the action selected by the user.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="181.19mm" wi="120.99mm" file="US08626511-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="189.57mm" wi="143.76mm" orientation="landscape" file="US08626511-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="180.09mm" wi="138.09mm" orientation="landscape" file="US08626511-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="158.92mm" wi="127.17mm" file="US08626511-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="143.76mm" wi="130.98mm" file="US08626511-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="227.67mm" wi="187.71mm" orientation="landscape" file="US08626511-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="223.77mm" wi="186.69mm" orientation="landscape" file="US08626511-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="173.14mm" wi="134.28mm" orientation="landscape" file="US08626511-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND</heading>
<p id="p-0002" num="0001">This specification relates to search engines.</p>
<p id="p-0003" num="0002">As the amount of information available on the Internet has dramatically expanded, users have had an increasingly difficult time formulating effective search queries for locating specific, relevant information. In recent years, competition among search engine providers has caused an explosive acceleration in the evolution of search engine algorithms, as well as in the user interfaces that are used to display search results.</p>
<p id="p-0004" num="0003">Various mechanisms can be used to provide queries to a search engine. For example, a user may type a query explicitly into a search box using a keyboard on a computing device and may then submit the query. Queries may also be implicit, such as by a user panning around a map that is displayed on their computing device, and queries being sent to display annotation information for businesses in the area of the map. In addition, users may enter queries by speaking them, particularly when using mobile devices (e.g., smartphones or app phones) for which keyboards may be small or hands-free operation may be important.</p>
<heading id="h-0002" level="1">SUMMARY</heading>
<p id="p-0005" num="0004">The proper interpretation of voice commands generally requires that they be disambiguated in at least two dimensions. Disambiguation in the first dimension, referred to by this specification as the &#x201c;quality of recognition&#x201d; dimension, involves matching the sounds included in the voice command to terms that the speaker may have spoken. In one example, disambiguation in the quality of recognition dimension may attempt to determine whether, when the speaker issued the voice command with the sounds &#x201c;k&#x3b1;: bil&#x201d; (represented using International Phonetic Alphabet (IPA) phonetic notation), they intended to speak the similar-sounding terms &#x201c;call bill,&#x201d; &#x201c;call Phil,&#x201d; &#x201c;cobble,&#x201d; &#x201c;cable,&#x201d; &#x201c;kill bill,&#x201d; &#x201c;call bull,&#x201d; or &#x201c;Kabul.&#x201d; This specification refers to the one or more words that are output as a result of performing a speech recognition process on a voice command as &#x201c;candidate terms.&#x201d;</p>
<p id="p-0006" num="0005">Disambiguation in the second dimension, referred to by this specification as the &#x201c;speaker intent&#x201d; dimension, relates to determining what action the speaker may possibly have intended when they spoke the terms that make up the voice command. For instance, if the speaker is assumed to have spoken the term &#x201c;call bill&#x201d; during a voice command, disambiguation in the speaker intent dimension may attempt to determine whether the speaker possibly intended to &#x201c;call&#x201d; a first contact named &#x201c;Bill&#x201d; or a second contact named &#x201c;Bill,&#x201d; whether the speaker possibly intended to &#x201c;call&#x201d; a contact named &#x201c;Bill&#x201d; at home or at work, or whether the speaker is possibly instructing that a &#x201c;call&#x201d; should be made to request a &#x201c;bill&#x201d; (or &#x201c;invoice&#x201d;) from a service provider. This specification refers to the actions that the speaker may possibly have intended when they spoke the terms that make up the voice command, as &#x201c;possible intended actions.&#x201d;</p>
<p id="p-0007" num="0006">Accordingly, the disambiguation of a voice command in these two dimensions may result in one candidate term that has more than one associated action, such as the case where the single term &#x201c;call bill&#x201d; may be associated with the possible intended actions of calling one of two contacts named &#x201c;Bill.&#x201d; Additionally, the disambiguation of a voice command may result in two or more candidate terms, each with one or more associated actions, such as the case where the same voice command is matched to the term &#x201c;call bill,&#x201d; to call a contact named &#x201c;Bill,&#x201d; and to the term &#x201c;Kabul,&#x201d; to buy a plane ticket to the city of Kabul. In either case, according to one innovative aspect of the subject matter described in this specification, the results of the multi-dimensional disambiguation may be provided to the user for selection, if the ambiguity between the different candidate terms and actions cannot be automatically resolved.</p>
<p id="p-0008" num="0007">In general, another innovative aspect of the subject matter described in this specification may be embodied in methods that include the actions receiving an audio signal at a server, performing, by the server, speech recognition on the audio signal to identify one or more candidate terms that match one or more portions of the audio signal, identifying one or more possible intended actions for each candidate term, providing information for display on a client device, the information specifying the candidate terms and the actions for each candidate term, receiving from the client device an indication of an action selected by a user, where the action was selected from among the actions included in the provided information, and invoking the action selected by the user. Other embodiments of this aspect include corresponding systems, apparatus, and computer programs, configured to perform the actions of the methods, encoded on computer storage devices.</p>
<p id="p-0009" num="0008">These and other embodiments may each optionally include one or more of the following features. For instance, the server may be a search engine. An index may be received from the client device, where performing the speech recognition further includes identifying a candidate term in the index that matches the audio signal. The index may be an index of contacts stored by the client device.</p>
<p id="p-0010" num="0009">In further examples, when a candidate term is a name of a person, one or more of the actions associated with the candidate term may initiate a call, an email, or an instant messaging session with the person. When a candidate term comprises a point of interest (POI), one or more of the actions associated with the candidate term may obtain a map of, directions to, detailed information about, or a distance to the POI. When a candidate term identifies media content, one or more of the actions associated with the candidate term may play, initiate a download for, or obtain detailed information about the media content. One or more of the actions associated with a candidate term may initiate a web search query using the candidate term as a query term. An ambiguity value may be determined for each action, where the ambiguity value associated with the user-selected action may be incremented.</p>
<p id="p-0011" num="0010">In additional examples, the actions may also include determining that the ambiguity value for a particular action satisfies a threshold, automatically invoking the particular action based on determining that the ambiguity value satisfies the threshold, determining that a user has cancelled the particular action, and decrementing the ambiguity value for the particular action based on determining that the user has cancelled the particular action, where the information is provided based on determining that the user has cancelled the particular action.</p>
<p id="p-0012" num="0011">In other examples, a speech recognition confidence metric may be determined for each candidate term, where the ambiguity value for each action may be determined based on the speech recognition confidence metric determined for the candidate term associated with the action. A ranking of the actions may be generated based on their respective ambiguity value, where providing the information may further include providing the ranking. The information identifying a particular action may be an icon. Invoking the user-selected application may further include providing, to the client device, information identifying an application stored by the client device.</p>
<p id="p-0013" num="0012">In general, another innovative aspect of the subject matter described in this specification may be embodied in methods that include the actions of providing an audio signal to a server, by a client device, obtaining information specifying one or more candidate terms that match one or more portions of the audio signal and one or more possible intended actions for each candidate term, receiving a user selection of an action, providing an indication of the user-selected action to the server, and invoking the action selected by the user. Other embodiments of this aspect include corresponding systems, apparatus, and computer programs, configured to perform the actions of the methods, encoded on computer storage devices.</p>
<p id="p-0014" num="0013">The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other potential features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 1</figref> is a conceptual diagram demonstrating the invocation of an action in response to a voice command.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIGS. 2 and 7</figref> illustrate exemplary systems.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIGS. 3 and 4</figref> are flowcharts of exemplary processes.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIGS. 5A-5B</figref> and <b>6</b>A-<b>6</b>B illustrate exemplary user interfaces.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<p id="p-0019" num="0018">Like reference numbers represent corresponding parts throughout.</p>
<heading id="h-0004" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 1</figref> is a conceptual diagram of an exemplary framework for performing multi-dimensional disambiguation of voice commands. More particularly, the diagram depicts a client device <b>104</b> (e.g., a cellular telephone, a PDA, or a personal computer) and a server <b>116</b> that collectively make up an information search system <b>100</b>, and also depicts both a flow of data between the client device <b>104</b> and the server <b>116</b>, and a user interface of the client device <b>104</b> in various states (labeled as user interfaces <b>102</b>, <b>130</b>, <b>136</b>, <b>150</b> in time-sequenced states (a), (h), (l) and (n), respectively). The server <b>116</b> may be a search engine, or a server used by a search engine to perform speech recognition.</p>
<p id="p-0021" num="0020">As shown in state (a), a user interface <b>102</b> displayed on the client device <b>104</b> includes a control <b>106</b> that acts as both a search box for receiving query terms, and a command prompt for receiving commands. When the control <b>106</b> is implementing search functionality, a user of the client device <b>104</b> may initiate a search for information stored on a public or private network by entering a part of a query term, or one or more complete query terms into the control.</p>
<p id="p-0022" num="0021">When the control <b>106</b> is implementing command functionality, the user of the client device may issue any appropriate command, for example a command to instruct the client device <b>104</b> to call a contact in the user's contact list, a command to instruct the client device <b>104</b> to compose and send electronic communication (e.g., e-mail, instant message, text message) to a contact in their contact list, a command to instruct the client device <b>104</b> to play, download, or otherwise interact with various types of media content (e.g., audio, video), or a command to instruct the client device to perform any other type of function. The control <b>106</b> thus provides a simplified, intuitive, and powerful user interface for allowing a user to enter or speak multiple types of commands and request multiple types of functionality through a single multi-function command box or prompt.</p>
<p id="p-0023" num="0022">Unlike a client device that provides different functionalities on different portions of a user interface or on different user interfaces, the client device <b>104</b> may implement both a search functionality and a command functionality using the control <b>106</b>, without requiring the user to first input a navigation function in order to invoke a particular desired functionality. In the case where the client device is a mobile device, such an approach allows the user interface <b>102</b> to make the best use of the small screen size, the limited processing capabilities, and the lack of a full keyboard that may exist on the client device <b>104</b>.</p>
<p id="p-0024" num="0023">In the example framework illustrated in <figref idref="DRAWINGS">FIG. 1</figref>, terms are input to the control <b>106</b> by way of a voice command. In some examples, the user may select a user interface control to activate voice control before speaking voice commands, to allow voice commands to be used to enter terms into the control <b>106</b>. The control <b>106</b> may be a persistent or semi-persistent feature of the user interface <b>102</b>, for example a search box on a browser toolbar that remains on the user interface <b>102</b> as long as the browser is executing, or the user may navigate to the control <b>106</b>, for example by entering a uniform resource locator (URL) associated with the control <b>106</b>.</p>
<p id="p-0025" num="0024">As illustrated in state (b), terms are input to the control <b>106</b> using a keyboard or other input mechanism. When voice control is activated, a user <b>108</b> may speak sounds <b>110</b> into a microphone <b>112</b>. The user <b>108</b> may, for example, press a button on the microphone <b>112</b> before speaking, speak the voice command, then release the button on the microphone to initiate the disambiguation process. As another example, the microphone <b>112</b> may be embedded into or otherwise installed on the client device <b>104</b> and the user <b>108</b> may select a user interface control (e.g., an icon of a microphone) before speaking. As yet another example, the microphone <b>112</b> may be &#x201c;on&#x201d; before the user <b>108</b> speaks or may be in an &#x201c;always-on&#x201d; state (e.g., the user <b>108</b> may simply speak without turning &#x201c;on&#x201d; the microphone <b>112</b>).</p>
<p id="p-0026" num="0025">The sounds <b>110</b> may include one or more phonemes, where a phoneme is the smallest unit of sound employed to form meaningful contrasts between utterances. For example, the sound <b>110</b> includes the phonemes &#x201c;ka:&#x201d; and &#x201c;bil&#x201d;. The sounds <b>110</b> may represent one of the phrases &#x201c;call Bill&#x201d;, &#x201c;kill Bill&#x201d;, &#x201c;cobble,&#x201d; or &#x201c;Kabul&#x201d;, to name a few examples.</p>
<p id="p-0027" num="0026">As illustrated in state (c), an audio signal <b>114</b> corresponding to the sounds <b>110</b> is provided to a server <b>116</b> from the client device <b>104</b>. For example, the client device <b>104</b> may create a sound file or data stream by recording the sounds <b>110</b> and may send the sound file over a network, such as the Internet. The server <b>116</b> may be one or more server computing devices. Sending audio signals to the search <b>116</b> and having the a server <b>116</b> voice recognition, command disambiguation, and some or all processing related to command execution may allow for low processing requirements for client device <b>104</b>. In other example implementations, however, a speech recognition may occur at the client device <b>104</b>, and the result of the speech recognition are sent to the server <b>116</b> instead of the audio signal.</p>
<p id="p-0028" num="0027">As illustrated in state (d), the server <b>116</b> performs a speech recognition process on the received audio signal to identify one or more candidate terms <b>118</b><i>a</i>-<i>d </i>that match the audio signal. A term may include one or more words. For example, speech recognition performed on a received sound file that includes a recording of the sounds <b>110</b> may identify the candidate terms &#x201c;call Bill&#x201d; <b>118</b><i>a</i>, &#x201c;kill Bill&#x201d; <b>118</b><i>b</i>, &#x201c;Kabul&#x201d; <b>118</b><i>c</i>, and &#x201c;cable&#x201d; <b>118</b><i>d</i>, as shown in the leftmost column of table <b>120</b>.</p>
<p id="p-0029" num="0028">The identification of candidate terms <b>118</b><i>a</i>-<i>d </i>may be performed by selecting candidate terms that have a corresponding speech recognition confidence metric above a recognition threshold. For example, a voice recognition process may identify a probability that a candidate term returned by a speech recognition module of the server <b>116</b> matches a recorded sound or matches what the user <b>108</b> said. The confidence metric reflects a probability; the higher the score, the more likely the candidate term matches the recorded sound.</p>
<p id="p-0030" num="0029">A threshold, such as a confidence of five percent, may be identified. Candidate terms that satisfy the threshold (e.g., greater than or equal to five percent) may be selected for disambiguation in the speaker intent dimension, and candidate terms that have a confidence that does not satisfy the threshold may be ignored or otherwise excluded from a list of identified candidate terms <b>118</b>. With a threshold of five percent, the confidence metric values of fifty percent <b>122</b><i>a</i>, thirty percent <b>122</b><i>b</i>, fifteen percent <b>122</b><i>c</i>, and five percent <b>122</b><i>d</i>, corresponding to the candidate terms <b>118</b><i>a</i>-<i>d</i>, respectively, each satisfy the threshold.</p>
<p id="p-0031" num="0030">As another example, the N candidate terms having the N highest speech recognition confidence metric values may be identified, where N is a positive integer (e.g., five). The selection of N may be based, for example, on an estimation of the size of the user interface <b>102</b>, or based on an estimation of the size of the display of a typical mobile device (e.g., as discussed below, candidate terms may be presented on a user interface, and the selection of N may be based on how many candidate terms might fit vertically in a particular or a typical user interface display). As another example, the confidence metrics <b>122</b><i>a</i>-<i>d </i>may be selected so that the sum of the confidence metric values <b>122</b><i>a</i>-<i>d </i>equals one hundred percent, or sixty percent. Confidence metric values may be adjusted (e.g., increased) for candidate terms that have been historically selected by the user <b>108</b> or by users in general. The increase amount may be based on the volume or frequency of historical selections.</p>
<p id="p-0032" num="0031">Candidate terms may be identified based at least in part on their inclusion in an index or database of data received from a client device. For example, the server <b>116</b> may periodically receive an index of contacts from the client device <b>104</b>. Speech recognition may include recognizing candidate terms that are included in the index and that match a received audio signal. For example, the candidate term &#x201c;call Bill&#x201d; <b>118</b><i>a </i>may be recognized in part due to the inclusion of the name &#x201c;Bill&#x201d; in an index of contacts received from the client device <b>104</b>. If &#x201c;Bill&#x201d; was not included in the client-provided index, the candidate term &#x201c;call Bill&#x201d; <b>118</b><i>a </i>might not be recognized, may be recognized with a lower confidence metric, or the name &#x201c;Bill&#x201d; may be recognized merely as a word that is synonymous with &#x201c;invoice.&#x201d; Client-provided indices may also include other information, such as information relating to application programs a client device is able to execute, user interface dimensions, media content available on the client device, etc.</p>
<p id="p-0033" num="0032">As illustrated in state (e) and in table <b>123</b>, the server <b>116</b> identifies one or more actions <b>124</b><i>a</i>-<i>e </i>associated with each identified candidate term <b>118</b><i>a</i>-<i>d</i>. For example, &#x201c;call Bill at home&#x201d; <b>124</b><i>a</i>, &#x201c;call Bill at work&#x201d; <b>124</b><i>b</i>, and &#x201c;perform a web search for &#x2018;Call Bill&#x2019;&#x201d; <b>124</b><i>c </i>actions may be identified for the &#x201c;call Bill&#x201d; candidate term <b>118</b><i>a</i>. Other examples include identifying a &#x201c;download &#x2018;Kill Bill&#x2019; movie&#x201d; action <b>124</b><i>d </i>associated with the &#x201c;kill Bill&#x201d; candidate term <b>118</b><i>b</i>, identifying a &#x201c;get directions to Kabul&#x201d; action <b>124</b><i>e </i>associated with the &#x201c;Kabul&#x201d; candidate term <b>118</b><i>c</i>, and identifying a &#x201c;pay cable bill&#x201d; action <b>124</b><i>f </i>associated with the &#x201c;cable&#x201d; candidate term <b>118</b><i>d</i>. Other examples of actions include initiating an email or an instant messaging session with a person based on their name, playing or obtaining detailed information about media content based on the title of the media content, and obtaining a map of, detailed information about, or a distance to a POI, based on the name of the POI.</p>
<p id="p-0034" num="0033">Where the candidate term does not itself identify an action or command, appropriate actions may be determined using a term/action mapper, or other rule engine. For example, an &#x201c;initiate contact&#x201d; action may be selected for a candidate term that includes a name, a &#x201c;download&#x201d; action may be selected for a candidate term that identifies media content, a &#x201c;pay&#x201d; action may be selected for a candidate term that identifies a business with that the user <b>108</b> has a relationship, and a &#x201c;map&#x201d; action may be selected for a candidate term that identifies a POI.</p>
<p id="p-0035" num="0034">As illustrated in state (f), an ambiguity value may be determined for each action, where the ambiguity value represents a likelihood that the user intended the action. For example, ambiguity values of thirty <b>128</b><i>a</i>, ten <b>128</b><i>b</i>, ten <b>128</b><i>c</i>, ten <b>128</b><i>d</i>, zero <b>128</b><i>e</i>, and ten <b>128</b><i>f </i>may be determined for the actions <b>124</b><i>a</i>-<i>f</i>, respectively. A high ambiguity value may represent a high confidence of probability that a user intended the associated action. For example, the ambiguity value <b>128</b><i>a </i>of thirty may mean that the associated action <b>124</b><i>a </i>has a higher probability of being the action the user intended than the action <b>124</b><i>b</i>, which has a lower ambiguity value (e.g., ten). In some implementations, a high ambiguity value may indicate that an associated action is highly ambiguous (e.g., not likely to have been intended).</p>
<p id="p-0036" num="0035">For example, in some implementations the action <b>124</b><i>a </i>may be considered less likely to have been intended than the action <b>124</b><i>b</i>, based on the ambiguity value of thirty <b>128</b><i>a </i>being higher than the ambiguity value of ten <b>128</b><i>b</i>. High ambiguity values may be associated with actions that the user or other users of the server <b>116</b> have selected in the past. Low ambiguity values may be associated with actions that the user or other users of the server <b>116</b> have never selected, or only infrequently selected, in the past. As described more fully below, low ambiguity values may also be associated with actions that the user has cancelled in the past.</p>
<p id="p-0037" num="0036">Ambiguity values may correlate with speech recognition confidence metrics. For example, a high speech recognition confidence metric may result in a high ambiguity value (e.g., high likelihood of intent) for an associated action, and vice versa. As will be discussed in more detail below, over time, ambiguity values associated with user-selected actions may be incremented and ambiguity values associated with user-canceled actions may be decremented. An initial ambiguity value for an action may be zero if the action has not been selected by the user before or if the corresponding candidate term has not been recognized before for the user.</p>
<p id="p-0038" num="0037">A total of M actions may be identified, where M is a positive integer (e.g., seven). The selection of M may be based, for example, on an estimation of the size of the user interface <b>102</b>, or based on an estimation of the size of the display of a typical mobile device (e.g., as discussed below, candidate term/action pairs may be presented on a user interface, and the selection of M may be based on how many candidate term/action pairs might fit vertically in a particular or a typical user interface display). As another example, actions having a corresponding ambiguity value above a threshold (e.g., ten) may be included in the identified actions <b>124</b><i>a</i>-<i>e </i>and actions having a corresponding ambiguity value below a threshold may be excluded from the identified actions <b>124</b><i>a</i>-<i>e</i>. As yet another example, a maximum number of actions of each type may be included in the identified actions. For instance, in some examples a maximum of three actions of a &#x201c;call contact&#x201d; type may be included, even if more than three &#x201c;call contact&#x201d; actions might be otherwise identified based on the identified candidate terms.</p>
<p id="p-0039" num="0038">One or more default actions may be identified for the candidate terms, regardless of whether other types of actions have been identified for a candidate term. For example, every candidate term may have an associated &#x201c;web search&#x201d; action. Other default actions that may be identified for a candidate term may include looking up a candidate term in an online dictionary or in an online encyclopedia. For some audio signals, only one action may be identified.</p>
<p id="p-0040" num="0039">For example, for some audio signals, only one candidate term might be identified and only one default action (e.g., web search) may be identified for the candidate term. For some audio signals, the speech recognition process may be unable to identify any appropriate candidate terms with an acceptable confidence metric, and in some implementations, the server <b>116</b> might not identify any actions associated with the unrecognizable audio signal. In other implementations, the server <b>116</b> may identify an action for an unrecognized audio signal that may trigger the client device <b>104</b> to ask the user to repeat their command.</p>
<p id="p-0041" num="0040">If the set of ambiguity values indicate that it is highly probable that the user intended a particular action, that action may be automatically invoked, using an &#x201c;implicit&#x201d; invocation process. Such a high probability may be established from a high ambiguity value, indicating that the user's intent is quite unambiguous, in either absolute or relative terms. An action may be implicitly invoked if an action has an ambiguity value higher than a threshold (e.g., thirty). As another example, an action having the highest ambiguity value may be implicitly invoked if the highest ambiguity value is at least three times the second highest ambiguity value. As a third example, an action having the highest ambiguity value may be implicitly invoked if the highest ambiguity value is at least four times the second highest ambiguity value and also greater than a threshold (e.g., twenty). In other words, in some implementations, in an example such as where the highest ambiguity value is four and every other ambiguity value is one or less, the action having the highest ambiguity value might not be implicitly invoked even though the highest ambiguity value is at least four times greater than every other ambiguity value, because the highest ambiguity value did not exceed a threshold.</p>
<p id="p-0042" num="0041">State (g) illustrates the implicit invocation of the particular &#x201c;call Bill at home&#x201d; action <b>124</b><i>a</i>. The action <b>124</b><i>a </i>may be implicitly invoked, for example, due to the associated ambiguity value <b>124</b><i>c </i>being above a threshold (e.g., the ambiguity value of thirty may exceed a threshold such as twenty). The server <b>116</b> may send metadata to the client device <b>104</b> indicating the type of action to invoke (e.g., dial contact) and also the associated candidate term (e.g., the name of the contact, such as &#x201c;Bill&#x201d;). In some examples, the server <b>116</b> may also send other information, such as the phone number of the contact. In other words, to decrease a wait time for the user <b>108</b>, the server may send the number to dial to the client device <b>104</b> so that the client device <b>104</b> does not need to look up the number of the contact. As noted above, phone numbers associated with the contact may be included in an index that is periodically sent from the client device <b>104</b> to the server <b>116</b>.</p>
<p id="p-0043" num="0042">Information received from the server <b>116</b> may trigger the client device <b>104</b> to invoke the action <b>124</b><i>a</i>. In some examples, the action <b>124</b><i>a </i>may be invoked immediately by the client device <b>104</b> (e.g., the phone number for Bill may be dialed immediately upon receipt of information from the server <b>116</b>). In other examples, a window <b>130</b> may be displayed on the user interface <b>102</b> before the action <b>124</b><i>a </i>is invoked. The window <b>130</b> may implement a &#x201c;countdown timer&#x201d; where the user <b>108</b> is presented with a countdown message indicating that the action <b>124</b><i>a </i>will be performed when the countdown timer reaches zero. If the user intended to call Bill, the user <b>108</b> may let the countdown timer reach zero, and at that point the call to Bill may be invoked.</p>
<p id="p-0044" num="0043">The amount of time that the countdown timer counts down may be based on the ambiguity value of the associated action. For example, if the ambiguity value <b>128</b><i>a </i>indicates that the probability that the user <b>108</b> intended the action <b>124</b><i>a </i>is high (e.g., seventy five percent), a countdown timer might not be used at all. If the ambiguity value <b>128</b><i>a </i>indicates that the probability that the user <b>108</b> intended the action <b>124</b><i>a </i>is medium (e.g., fifty percent), the countdown timer might start at a small number of seconds (e.g., two) and if the ambiguity value <b>128</b><i>a </i>indicates a low probability (e.g., twenty percent), the countdown timer might start at a higher number of seconds (e.g., four). In some implementations, the server <b>116</b> determines the countdown timer start value and sends the value to the client device <b>104</b>.</p>
<p id="p-0045" num="0044">A cancel control <b>132</b> may be included in the window <b>130</b> to allow the user <b>108</b> to cancel the action <b>124</b><i>a </i>before the action <b>124</b><i>a </i>is invoked (or, for some actions such as the dialing of a phone number, to cancel the action once it is in progress). As illustrated in state (h), the user <b>108</b> did not intend to call Bill, selects the cancel control <b>132</b> to cancel the invocation of the action <b>124</b><i>a</i>. In response to the selection of the cancel control <b>132</b>, an interrupt signal may be sent from the client device <b>104</b> to the server <b>116</b> (as illustrated by state (i)).</p>
<p id="p-0046" num="0045">In response to receiving the interrupt signal, the server <b>116</b> may decrement the ambiguity value for the action <b>124</b><i>a</i>, to account for the fact that the action <b>124</b><i>a </i>was not the action the user <b>108</b> intended. For example, state (j) illustrates a reduction of the associated ambiguity value <b>128</b><i>a </i>by a value of five. Decrementing the ambiguity value for an action that the user manually cancels will reduce the chances that the cancelled action will be automatically invoked if a similar sound pattern is spoken in a future voice command.</p>
<p id="p-0047" num="0046">Also in response to receiving the interrupt signal, the server <b>116</b> may send a list <b>134</b> of candidate term/action pairs to the client device <b>104</b>, so that the user <b>108</b> can pick the action that was actually intended. The list <b>134</b> may also be sent to the client device <b>134</b> if none of the ambiguity values <b>128</b><i>a</i>-<i>d </i>are above a threshold, or with the metadata that implicitly invokes an action (e.g., in state (g)). In other words, the list <b>134</b> may be sent to the client device <b>104</b> if none of the ambiguity values <b>128</b><i>a</i>-<i>d </i>are high enough to cause an implicit invocation of the associated action. In some implementations, the list <b>134</b> is a ranking of the actions <b>124</b><i>a</i>-<i>c </i>based on their respective ambiguity values. In other implementations, the list <b>134</b> is unranked (e.g., unsorted) when sent by the server <b>116</b> and in such examples the list <b>134</b> may be subsequently ranked by the client device <b>104</b>. The term/action pairs may be packaged in another type of data structure instead of a list <b>116</b>, including a table, database, or an XML file.</p>
<p id="p-0048" num="0047">Upon receiving the list <b>134</b>, the client device <b>104</b> may display a window <b>136</b> on the user interface <b>102</b>. In some implementations, the server <b>116</b> generates code (e.g., HTML (HyperText Markup Language) code) to display the window <b>136</b> and sends the code in conjunction with the list <b>134</b>. The window <b>136</b> includes a list <b>138</b> that displays list items <b>140</b><i>a</i>-<i>f </i>which correspond to the actions <b>124</b><i>a</i>-<i>f</i>, respectively. In some implementations, if the user <b>108</b> cancels the invocation of an implicit action (e.g., the user <b>108</b> may cancel the implicit invocation of the action <b>124</b><i>a </i>by selecting the cancel control <b>132</b>) the list <b>138</b> might not include a corresponding list item for the canceled action. For example, the list <b>138</b> might not include the list item <b>140</b><i>a </i>if the user had canceled the implicit invocation of the action <b>124</b><i>a </i>(e.g., in state (h)).</p>
<p id="p-0049" num="0048">Each list item <b>140</b><i>a</i>-<i>f </i>includes a corresponding icon <b>142</b><i>a</i>-<i>f</i>. The icons <b>142</b><i>a</i>-<i>f </i>indicate the type of action corresponding to the respective list item <b>140</b><i>a</i>-<i>f</i>. For example, icons <b>142</b><i>a</i>-<i>b </i>indicate that the list items <b>140</b><i>a</i>-<i>b </i>correspond to &#x201c;call contact&#x201d; actions. The icon <b>142</b><i>c </i>indicates that the list item <b>140</b><i>c </i>corresponds to a &#x201c;web search&#x201d; action. The icon <b>142</b><i>d </i>indicates that the list item <b>140</b><i>d </i>corresponds to a play movie (e.g., stream, or purchase and download) action. The icon <b>142</b><i>e </i>indicates that the list item <b>140</b><i>e </i>corresponds to a map or directions action and the icon <b>142</b><i>f </i>indicates that the list item <b>140</b><i>f </i>corresponds to a &#x201c;pay bill&#x201d; action.</p>
<p id="p-0050" num="0049">The window <b>136</b> includes a search box <b>144</b>, which the user <b>108</b> may use to enter or speak a different command or action if the action the user <b>108</b> intended is not displayed in the list <b>138</b>. If the action the user <b>108</b> intended is displayed in a corresponding list item <b>140</b><i>a</i>-<i>d</i>, the user <b>108</b> may select the list item <b>140</b><i>a</i>-<i>d </i>that corresponds to the intended action. For example, as illustrated in state (l), the user <b>108</b> may select the list item <b>140</b><i>e</i>, to indicate that they wish to invoke the corresponding action <b>124</b><i>e</i>, to display directions to Kabul. In response to the user selection of a list item <b>140</b><i>a</i>-<i>e</i>, the client device <b>104</b> sends an indication of the action which corresponds to the selected list item <b>140</b><i>a</i>-<i>e </i>to the server <b>116</b>. For example, state (m) illustrates the sending of an indication to the server <b>116</b> that indicates the selection of action <b>124</b><i>e </i>corresponding to requesting directions to Kabul. Contrasted with the implicit invocation process of state (g), states (k) and (l) provide for the manual selection of an action, referred to by this specification as an &#x201c;explicit&#x201d; invocation process.</p>
<p id="p-0051" num="0050">In response to receiving the indication of the user-selected action, the server <b>116</b> invokes the user-selected action. For some types of actions, such as generating a map or generating directions to a location, processing is done on the server <b>116</b> and information used to display the results (e.g., a display image, HTML code) is sent to the client device <b>104</b>. For example, for the user-selected action <b>124</b><i>e</i>, a map of Kabul and directions to Kabul may be generated by a map application running on the server <b>116</b>. HTML code to display the map and directions may be sent to the client device <b>104</b>, as illustrated by state (n). The client device may display the generated map and directions in the user interface <b>102</b>, such as in a window <b>150</b>.</p>
<p id="p-0052" num="0051">For some types of actions, the server <b>116</b> may send a message to the client device <b>104</b> indicating the type of action and possibly metadata relating to the action. Upon receipt of the message, the client device <b>104</b> may perform the indicated action. For example, if the user <b>108</b> selects list item <b>140</b><i>b </i>to indicate a selection of the action <b>124</b><i>b </i>to call Bill at work, the server <b>116</b> may send a message to the client device <b>104</b> indicating that the client device <b>104</b> should initiate a call. The message sent from the server <b>116</b> to the client device <b>104</b> may include Bill's work number.</p>
<p id="p-0053" num="0052">For some types of actions, invocation of the action involves processing on both the server <b>116</b> and the client device <b>104</b> (e.g., processing other than simply looking up information or simply displaying information). For example, for an action to play a media title, the server <b>116</b> may download the title from a media server, process a credit card transaction, and unlock digital media rights. The server <b>116</b> may send the media content to the client device <b>104</b>. The client device <b>104</b> may decode the media content and may play the content.</p>
<p id="p-0054" num="0053">In response to the user-selection of an action, an associated ambiguity value may be incremented. For example, state (o) illustrates an increase of five for the ambiguity value <b>128</b><i>e </i>corresponding to the user-selected action <b>124</b><i>e</i>. Over time, an ambiguity value may increase to the point where it is at or above a threshold value such that the associated action may be implicitly invoked if the action is subsequently mapped to a candidate term.</p>
<p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. 2</figref> illustrates an exemplary system <b>200</b> that may be used for invoking actions in response to a voice command or other audio signal. The system <b>200</b> includes a server <b>202</b> connected to one or more client devices <b>204</b> over a network <b>206</b>. The server <b>202</b> includes, among other things, one or more processors <b>208</b>, a network interface <b>210</b>, a query reviser <b>212</b>, a user interface <b>213</b>, and a medium <b>214</b>. The server <b>202</b> may be a search engine, or the server <b>202</b> may be used by a search engine to perform speech recognitions. The client device <b>204</b> includes one or more processors <b>220</b>, a user interface <b>222</b>, a network interface <b>224</b>, a microphone <b>226</b>, and a medium <b>228</b>. The client device <b>204</b> may be a mobile phone, a laptop computer, PDA, smart phone, Blackberry&#x2122; or other handheld or mobile device. In another implementation, the client device <b>204</b> is not portable or mobile, but rather is a desktop computer or a server.</p>
<p id="p-0056" num="0055">The mediums <b>214</b> and <b>228</b> store and record information or data, and each may be an optical storage medium, magnetic storage medium, flash memory, or any other appropriate storage medium type. The medium <b>214</b> includes a term disambiguator application <b>230</b> and possibly one or more other applications <b>232</b>. The term disambiguator application includes a speech recognition module <b>234</b>, a term/action mapper <b>236</b>, a confidence score generator <b>238</b>, an action initiator <b>240</b>, and a client device index <b>242</b>.</p>
<p id="p-0057" num="0056">The speech recognition module <b>234</b> performs speech recognition on a received audio signal to identify one or more candidate terms that match the audio signal. The speech recognition module may determine a speech recognition confidence metric for each identified term which indicates a confidence that the candidate term matches the audio signal. The term/action mapper <b>236</b> identifies one or more actions associated with each candidate term. The confidence score generator <b>238</b> determines an ambiguity value for each action which represents a likelihood that the user intended the action. The query reviser <b>212</b> may adjust ambiguity values, such as increasing ambiguity values for user-selected actions and decreasing ambiguity values for user-canceled actions.</p>
<p id="p-0058" num="0057">The client-device index <b>242</b> may include one or more types of indices received from one or more client devices <b>204</b>. For example, the server <b>202</b> may periodically receive an index of contacts from the client device <b>204</b>. The speech recognition module <b>234</b> may recognize candidate terms that are included in the client-device index <b>242</b> and that match a received audio signal. The client-device index <b>242</b> may also include other information, such as information relating to application programs the client device <b>204</b> is able to execute, user interface dimensions, media content available on the client device <b>204</b>, etc. The other applications <b>232</b> may include, for example, among other applications, a map generator application, a transaction application (e.g., for paying electronic bills or for managing purchasing of media content), and a search application.</p>
<p id="p-0059" num="0058">The medium <b>228</b> includes a search application <b>250</b> and possibly one or more other applications <b>252</b>. The medium <b>228</b> also includes user preferences <b>254</b>, an icon database <b>256</b>, and a contact database <b>258</b>. The contact database <b>258</b> may include, for example, a list of personal contacts stored on the client device <b>204</b>. Some or all of the contents of the contact database <b>258</b> may be periodically sent to the server <b>202</b>. The icon database <b>256</b> may include icons that indicate particular types of actions. Icons may be displayed next to or along with candidate term/action pairs in a list of suggested candidate term/action pairs presented to the user of the client device <b>204</b>.</p>
<p id="p-0060" num="0059">The search application <b>250</b> may provide a search user interface to users of the client device <b>204</b> which allows users to enter voice commands to perform Internet searches and perform other actions such as dialing contacts, communicating with contacts through email or other electronic communication, making electronic bill payments, getting directions to or other information about a POI, or other actions. The user preferences <b>254</b> may include custom threshold values for particular users, such as values which indicate that a user generally intends by voice commands, for example, to call contacts rather than perform web searches, or that the user prefers that a command be invoked immediately rather than use a countdown timer. Other user preferences <b>254</b> may indicate that the user prefers that particular types of actions (e.g., dial contact) are invoked automatically or that the user prefers to see suggestions of web search actions presented first in a list of suggested candidate term/action pairs.</p>
<p id="p-0061" num="0060">The server <b>202</b> may be connected to the network <b>206</b> and possibly to one or more other networks over the network interface <b>210</b>. Similarly, the client device <b>204</b> may be connected to the network <b>206</b> and possibly to one or more other networks over the network interface <b>224</b>. The network <b>206</b> may include, for example, one or more of the Internet, Wide Area Networks (WANs), Local Area Networks (LANs), analog or digital wired and wireless telephone networks (e.g., a PSTN, Integrated Services Digital Network (ISDN), and Digital Subscriber Line (xDSL)), radio, television, cable, satellite, and/or any appropriate other delivery or tunneling mechanism for carrying data services. Networks may include multiple networks or subnetworks, each of which may include, for example, a wired or wireless data pathway.</p>
<p id="p-0062" num="0061">The processor <b>208</b> includes one or more processors and processes operating system or application program computer instructions for the server <b>202</b>. Similarly, the processor <b>220</b> includes one or more processors and processes operating system or application program computer instructions for the client device <b>204</b>. The user interface <b>222</b> displays application user interfaces that include user interface controls for applications that run on the client device <b>204</b>. For example, the user interface <b>222</b> may display an interface for the search application <b>250</b>. The user interface <b>213</b> displays application user interfaces for applications that run on the server <b>202</b>. For example, the user interface <b>213</b> may display an interface for an administrator application that is used to configure, monitor, and invoke the term disambiguator application <b>230</b>.</p>
<p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. 3</figref> is a flowchart illustrating a computer-implemented process <b>300</b> for invoking an action based on a speech command. Briefly, the process <b>300</b> includes: receiving an audio signal at a server, performing, by the server, speech recognition on the audio signal to identify one or more candidate terms that match one or more portions of the audio signal, identifying one or more possible intended actions for each candidate term, providing information for display on a client device, the information specifying the candidate terms and the actions for each candidate term, receiving from the client device an indication of an action selected by a user, where the action was selected from among the actions included in the provided information, and invoking the action selected by the user. Using the process <b>300</b>, sounds that make up a voice command are disambiguated in at least two dimensions.</p>
<p id="p-0064" num="0063">In further detail, when the process <b>300</b> begins (<b>302</b>), an audio signal is received from a client device by a server (<b>303</b>). The server may receive the audio signal from the client device indirectly, for instance where the client device transmits the audio signal to a search engine that, in turn, transmits the audio signal to the server.</p>
<p id="p-0065" num="0064">The server performs speech recognition on an audio signal to identify one or more candidate terms which match one or more portions of the audio signal (<b>304</b>). As shown in <figref idref="DRAWINGS">FIG. 1</figref>, the server <b>116</b> performs speech recognition on the audio signal <b>114</b> to identify the candidate terms <b>118</b><i>a</i>-<i>d</i>. Performing speech recognition may include identifying a candidate term, (e.g., name of a contact) in a client-received index that matches the audio signal.</p>
<p id="p-0066" num="0065">Speech recognition is used to perform disambiguation of the voice command in the &#x201c;quality of recognition&#x201d; dimension. Using speech recognition, the sounds that make up the voice command are matched to terms that the speaker may have spoken. In one example, disambiguation in the quality of recognition dimension may attempt to determine whether, when the speaker issued a voice command, they intended to speak the similar-sounding terms &#x201c;directions to Paris,&#x201d; &#x201c;directions to Perris,&#x201d; &#x201c;direct to Perry's house,&#x201d; &#x201c;do you reckon two pairs,&#x201d; &#x201c;door erect chintz toupee wrist,&#x201d; or other possible candidate terms.</p>
<p id="p-0067" num="0066">One or more possible intended actions are identified for each candidate term (<b>306</b>). For example, as shown in <figref idref="DRAWINGS">FIG. 1</figref>, actions <b>124</b><i>a</i>-<i>e </i>are identified for the associated actions <b>118</b><i>a</i>-<i>d</i>. Actions may include, for example, dialing a contact, performing a web search for the candidate terms, getting directions to a POI, playing media content, or paying an online bill. Other examples of actions include initiating an email or an instant messaging session with a person based on their name, playing or obtaining detailed information about media content based on the title of the media content, and obtaining a map of, detailed information about, or a distance to a POI based on the name of the POI.</p>
<p id="p-0068" num="0067">Disambiguation in the &#x201c;speaker intent&#x201d; second dimension is performed by a module on a client device or server that matches candidate terms to appropriate actions, and obtains an ambiguity value associated with each action. In doing so, the action the speaker may have intended when they spoke the terms that make up the voice command.</p>
<p id="p-0069" num="0068">Information specifying the candidate terms and the actions for each candidate term are provided for display on a client device (<b>308</b>). For example, a user interface may be displayed on a client device that includes a list of candidate term/action pairs, with icons representing action types next to or included with each candidate term/action pair. The candidate term/action pairs may be ranked and presented in the user interface in an order based on the likelihood that the user intended an action.</p>
<p id="p-0070" num="0069">For the above-noted &#x201c;directions to Paris&#x201d; example, a user interface may provide a user with the option of obtaining directions to Paris, France, Paris, Tex., or Perris, Calif., of determining a route to the home of the user's friend &#x201c;Perry,&#x201d; to performing a web search with the term &#x201c;do you reckon two pairs,&#x201d; or to determining show times for the movie &#x201c;Directions To Paris.&#x201d; If the user lives nowhere near France, Texas, or California but they do have a friend named &#x201c;Perry,&#x201d; or if their user preferences indicate that they enjoy movies, the route and show time actions may be ranked higher than the directions and web search actions.</p>
<p id="p-0071" num="0070">A user-selected action is identified, and an indication of the action selected by the user is received (<b>310</b>). The action is selected from among the actions included in the provided information. For example, the user may select a candidate term/action pair in a user interface to indicate that they desire to invoke the selected action. A user may, for example, speak the number &#x201c;two&#x201d; to select the second identified action.</p>
<p id="p-0072" num="0071">The action selected by the user is invoked (<b>312</b>), thereby ending the process <b>300</b> (<b>314</b>). For some types of actions, such as generating a map or generating directions to a location, processing is done on a server and information used to display the results (e.g., a display image, HTML code) is sent to a client device. For other types of actions, a server may send a message to a client device indicating a type of action to invoke on the client device and possibly metadata relating to the action. Upon receipt of the message, the client device may perform the indicated action. For example, the client device may initiate a call to a contact. For some types of actions, invocation of the action involves processing on both a server and a client device. For example, for an action to play a media title, a server may download the title from a media server, process a credit card transaction, and unlock digital media rights. The server may send the media content to the client device and the client device may decode and play the media content.</p>
<p id="p-0073" num="0072"><figref idref="DRAWINGS">FIG. 4</figref> is a flowchart illustrating a computer-implemented process <b>300</b> for invoking an action based on an audio signal. Briefly, the process <b>400</b> includes: providing an audio signal to a server, by a client device, obtaining information specifying one or more candidate terms which match one or more portions of the audio signal and one or more possible intended actions for each candidate term, receiving a user selection of an action, providing an indication of the user-selected action to the server, and invoking the action selected by the user.</p>
<p id="p-0074" num="0073">In further detail, when the process <b>400</b> begins (<b>402</b>), an audio signal is provided to a server by a client device (<b>404</b>). For example and as shown in <figref idref="DRAWINGS">FIG. 1</figref>, the client device <b>104</b> sends the audio signal <b>114</b> to the server <b>116</b>. The audio signal <b>114</b> may be created, for example, by recording a voice command spoken by the user <b>108</b> into the microphone <b>112</b>.</p>
<p id="p-0075" num="0074">Information identifying one or more candidate terms that match the audio signal and one or more actions associated with each candidate term is obtained (<b>406</b>). For example and as shown in <figref idref="DRAWINGS">FIG. 1</figref>, the server <b>116</b> may send the list <b>134</b> of candidate term/action pairs to the client device <b>104</b>. The list <b>134</b> may be ranked by ambiguity value, or, in some implementations, the list <b>134</b> is unranked.</p>
<p id="p-0076" num="0075">User selection of an action is received (<b>408</b>). For example, a user may select an action from a list of suggested candidate term/action pairs displayed on a user interface of a client device. For example and as shown in <figref idref="DRAWINGS">FIG. 1</figref>, the user <b>108</b> may select the list item <b>140</b><i>e </i>corresponding to the &#x201c;directions to Kabul&#x201d; action <b>124</b><i>e. </i></p>
<p id="p-0077" num="0076">Information identifying the user-selected action is provided to the server (S<b>410</b>). For example and as shown in <figref idref="DRAWINGS">FIG. 1</figref>, an indication of the selection of the action <b>124</b><i>e </i>may be sent from the client device <b>104</b> to the server <b>116</b>.</p>
<p id="p-0078" num="0077">The user-selected action is invoked (<b>412</b>), thereby ending the process <b>400</b> (<b>414</b>). For some types of actions, such as generating a map or generating directions to a location, processing is done on a server and information used to display the results (e.g., a display image, HTML code) is sent to a client device. For other types of actions, a server may send a message to a client device indicating a type of action to invoke on the client device and possibly metadata relating to the action. Upon receipt of the message, the client device may perform the indicated action. For example, the client device may initiate a call to a contact. For some types of actions, invocation of the action involves processing on both a server and a client device. For example, for an action to play a media title, a server may download the title from a media server, process a credit card transaction, and unlock digital media rights. The server may send the media content to the client device and the client device may decode and play the media content.</p>
<p id="p-0079" num="0078"><figref idref="DRAWINGS">FIGS. 5A-5B</figref> and <b>6</b>A-<b>6</b>B illustrate exemplary user interfaces. <figref idref="DRAWINGS">FIG. 5A</figref> illustrates an example user interface <b>500</b> that may be displayed, for example, on a mobile client device. The user interface <b>500</b> includes a microphone control <b>502</b>, that the user may select before speaking a voice command. The user's voice command may be recorded and a corresponding audio signal may be sent to a server. The server may perform speech recognition to identify one or more candidate terms that match the audio signal. The server may also identify one or more actions corresponding to each identified candidate term. The server may send a list of candidate term/action pairs, that may be identified in list items <b>504</b><i>a</i>-<i>h </i>in a list area <b>506</b> of the user interface <b>500</b>. In some implementations, the server generates information (e.g., a display image, HTML code) to display the list area <b>506</b> and a mobile client device renders the list area <b>506</b> using the received information.</p>
<p id="p-0080" num="0079">The list items <b>504</b><i>a</i>-<i>h </i>each include an associated icon <b>508</b><i>a</i>-<i>h</i>, respectively. Each of the icons <b>508</b><i>a</i>-<i>h </i>indicate an action or a type of action corresponding to the respective list item <b>504</b><i>a</i>-<i>h</i>. For example, icons <b>508</b><i>a</i>-<i>f</i>, that each include a picture of a corresponding contact, each indicate a &#x201c;call contact&#x201d; action type. Icons <b>508</b><i>g</i>-<i>h </i>each indicate a &#x201c;web search&#x201d; action type. The user may select a list item <b>504</b><i>a</i>-<i>h </i>to cause a corresponding action to be invoked. For example, the user may select the list item <b>504</b><i>b </i>to call a contact named &#x201c;Huan&#x201d; on his mobile phone. As another example, the user may select the list item <b>504</b><i>g </i>to initiate a web search for the phrase &#x201c;call Bill&#x201d;. If the user selects the list item <b>504</b><i>e </i>or the list item <b>504</b><i>f</i>, the user may be prompted to enter the phone number of the corresponding &#x201c;Jeff&#x201d; or &#x201c;billB&#x201d; contact, because the mobile client device does not have the phone numbers for those contacts. In response to the user selection of a list item <b>504</b><i>a</i>-<i>h</i>, a server may increment an ambiguity value associated with the user-selected action.</p>
<p id="p-0081" num="0080"><figref idref="DRAWINGS">FIG. 5B</figref> illustrates an example user interface <b>550</b>. The user interface <b>550</b> may be displayed, for example, on a mobile client device if a user selects the list item <b>504</b><i>a </i>describe above with reference to <figref idref="DRAWINGS">FIG. 5A</figref>. As another example, the user interface <b>550</b> may be displayed if a &#x201c;call Bill at home&#x201d; action is implicitly invoked by a server, such as if a &#x201c;call Bill at home&#x201d; action is deemed highly probable to be the action that a user intended when speaking a voice command (e.g., the &#x201c;call Bill at home&#x201d; action might be a frequently requested action by the user and/or a speech recognition process may have determined a high confidence threshold for identifying a candidate term that was subsequently mapped to the &#x201c;call Bill at home&#x201d; action). In some implementations, the &#x201c;call Bill at home&#x201d; action may be invoked implicitly without displaying the user interface <b>550</b>.</p>
<p id="p-0082" num="0081">The user interface <b>550</b> may be displayed by a client device after receipt of a message by a server indicating to the mobile client device to implicitly invoke the &#x201c;call Bill at home&#x201d; action. In some examples, the &#x201c;call Bill at home&#x201d; action may be initiated immediately by the mobile client device upon display of the user interface <b>550</b>. In other examples, a window <b>552</b> may be displayed on the user interface <b>550</b> before the &#x201c;call Bill at home&#x201d; action is invoked. In some examples, the &#x201c;call Bill at home&#x201d; action is invoked in response to the user selection of a &#x201c;dial&#x201d; control <b>554</b>.</p>
<p id="p-0083" num="0082">In other examples, the window <b>552</b> may implement a &#x201c;countdown timer&#x201d; where the user is presented with a countdown message indicating that the &#x201c;call Bill at home&#x201d; action will be performed when the countdown timer reaches zero. If the user intended to call Bill, the user may let the countdown timer reach zero, and at that point the call to Bill may be invoked. The user may also select the dial control <b>554</b> to invoke the call action before the timer reaches zero. The amount of time that the countdown timer counts down may be based on an ambiguity value of the associated action. For example, if an ambiguity value indicates that the probability that the user intended the &#x201c;call Bill at home&#x201d; action is high, a countdown timer might not be used at all. If the ambiguity value indicates that the probability that the user intended the &#x201c;call Bill at home&#x201d; action is medium, the countdown timer might start at a small number of seconds (e.g., two) and if the ambiguity value indicates a low probability, the countdown timer might start at a higher number of seconds (e.g., four).</p>
<p id="p-0084" num="0083">A cancel control <b>556</b> may be included in the window <b>552</b> to allow the user to cancel the &#x201c;call Bill at home&#x201d; action before the call is placed or, for example, to cancel the call if the mobile device has dialed but is waiting for Bill's phone to answer. If the user selects the cancel control <b>556</b> to cancel the call to Bill, an interrupt signal may be sent from the mobile client device to a server. In response to receiving the interrupt signal, the server may decrement an ambiguity value for the &#x201c;call Bill at home&#x201d; action, to account for the fact that the action was not the action the user intended. Additionally, if the user interface <b>550</b> was displayed due to a determination to implicitly invoke the &#x201c;call Bill at home&#x201d; action, a server may, in response to the user selecting the cancel control <b>556</b>, send a list of candidate term/action pairs to the mobile client device, instructing the mobile client device to display the interface <b>500</b> described above with respect to <figref idref="DRAWINGS">FIG. 5A</figref>, to allow the user to select the action that they intended to invoke.</p>
<p id="p-0085" num="0084"><figref idref="DRAWINGS">FIG. 6A</figref> illustrates an example user interface <b>600</b> that may be displayed, for example, on a mobile client device. The user interface <b>600</b> may be displayed in response to a server sending a list of candidate term/action pairs corresponding to a set of candidate terms matching an audio signal associated with a voice command sent by the mobile client device to the server. For example, the user of the mobile client device may have spoken the voice command &#x201c;directions to Paris&#x201d;. The list of received candidate term/action pairs may be used to display list items <b>604</b><i>a</i>-<i>h </i>in a list area <b>606</b> of the user interface <b>600</b>.</p>
<p id="p-0086" num="0085">The list items <b>604</b><i>a</i>-<i>h </i>each include an associated icon <b>608</b><i>a</i>-<i>h</i>, respectively. Each of the icons <b>608</b><i>a</i>-<i>h </i>indicate a type of action corresponding to the respective list item <b>604</b><i>a</i>-<i>h</i>. For example, icons <b>608</b><i>a</i>-<i>d </i>each indicate a &#x201c;get directions&#x201d; action type (e.g., associated with &#x201c;directions to Paris&#x201d;, &#x201c;directions to parents&#x201d;, &#x201c;directions to parris&#x201d;, and &#x201c;directions to tears&#x201d; actions, respectively). Icons <b>608</b><i>e</i>-<i>g </i>each indicate a &#x201c;web search&#x201d; action type (e.g., corresponding to web searches for the phrases &#x201c;directions to Paris&#x201d;, &#x201c;directions to parents&#x201d;, and &#x201c;directions to tears&#x201d;, respectively). The icon <b>608</b><i>h </i>indicates a movie download action type (e.g., associated with a &#x201c;download &#x2018;Directions to Paris&#x2019; movie&#x201d; action).</p>
<p id="p-0087" num="0086">The user may select a list item <b>604</b><i>a</i>-<i>h </i>to cause a corresponding action to be invoked. For example, the user may select the list item <b>604</b><i>b </i>to get directions to &#x201c;parents&#x201d;. As another example, the user may select the list item <b>604</b><i>g </i>to initiate a web search for the phrase &#x201c;directions to tears&#x201d;. In response to the user selection of a list item <b>604</b><i>a</i>-<i>h</i>, a server may increment an ambiguity value associated with the user-selected action.</p>
<p id="p-0088" num="0087"><figref idref="DRAWINGS">FIG. 6B</figref> illustrates an example user interface <b>650</b>. The user interface <b>650</b> may be displayed, for example, on a mobile client device if a user selects the list item <b>604</b><i>a </i>describe above with reference to <figref idref="DRAWINGS">FIG. 6A</figref>. As another example, the user interface <b>650</b> may be displayed if a &#x201c;directions to Paris&#x201d; action is implicitly invoked by a server, such as if a &#x201c;directions to Paris&#x201d; action is deemed highly probable to be the action that a user intended when speaking a voice command. The user interface <b>650</b> may be displayed by a mobile client device after receipt of a message by a server indicating to the mobile client device to implicitly invoke the &#x201c;directions to Paris&#x201d; action. In some examples, the &#x201c;directions to Paris&#x201d; action may be initiated immediately by the mobile client device upon display of the user interface <b>650</b>. In other examples, a window <b>652</b> may be displayed on the user interface <b>650</b> before the &#x201c;directions to Paris&#x201d; action is invoked. In some examples, the &#x201c;directions to Paris&#x201d; action is invoked in response to the user selection of a &#x201c;go&#x201d; control <b>654</b>.</p>
<p id="p-0089" num="0088">In other examples, the window <b>652</b> may implement a &#x201c;countdown timer&#x201d; where the user is presented with a countdown message indicating that the &#x201c;directions to Paris&#x201d; action will be performed when the countdown timer reaches zero. The user may let the countdown timer reach zero, and at that point the action may be invoked. The user may also select the &#x201c;go&#x201d; control <b>654</b> to invoke the action before the timer reaches zero. A cancel control <b>656</b> may be included in the window <b>652</b> to allow the user to cancel the &#x201c;directions to Paris&#x201d; action before the action is invoked. If the user selects the cancel control <b>656</b> to cancel the action, an interrupt signal may be sent from the mobile client device to a server. In response to receiving the interrupt signal, the server may decrement an ambiguity value for the &#x201c;directions to Paris&#x201d; action, to account for the fact that the action was not the action the user intended. Additionally, if the user interface <b>650</b> was displayed due to a determination to implicitly invoke the &#x201c;directions to Paris&#x201d; action, a server may, in response to the user selecting the cancel control <b>656</b>, send a list of candidate term/action pairs to the mobile client device, instructing the mobile client device to display the interface <b>600</b> described above with respect to <figref idref="DRAWINGS">FIG. 6A</figref>, to allow the user to select the action that they intended to invoke.</p>
<p id="p-0090" num="0089"><figref idref="DRAWINGS">FIG. 7</figref> is a schematic diagram of an example of a generic computer system <b>700</b>. The system <b>700</b> includes a processor <b>710</b>, a memory <b>720</b>, a storage device <b>730</b>, and an input/output device <b>740</b>. Each of the components <b>710</b>, <b>720</b>, <b>730</b>, and <b>740</b> are interconnected using a system bus <b>750</b>. The processor <b>710</b> is capable of processing instructions for execution within the system <b>700</b>. In one implementation, the processor <b>710</b> is a single-threaded processor. In another implementation, the processor <b>710</b> is a multi-threaded processor. The processor <b>710</b> is capable of processing instructions stored in the memory <b>720</b> or on the storage device <b>730</b> to display graphical information for a user interface on the input/output device <b>740</b>.</p>
<p id="p-0091" num="0090">The memory <b>720</b> stores information within the system <b>700</b>. In one implementation, the memory <b>720</b> is a computer-readable medium. In another implementation, the memory <b>720</b> is a volatile memory unit. In yet another implementation, the memory <b>720</b> is a non-volatile memory unit.</p>
<p id="p-0092" num="0091">The storage device <b>730</b> is capable of providing mass storage for the system <b>700</b>. In one implementation, the storage device <b>730</b> is a computer-readable medium. In various different implementations, the storage device <b>730</b> may be a floppy disk device, a hard disk device, an optical disk device, or a tape device.</p>
<p id="p-0093" num="0092">The input/output device <b>740</b> provides input/output operations for the system <b>700</b>. In one implementation, the input/output device <b>740</b> includes a keyboard and/or pointing device. In another implementation, the input/output device <b>740</b> includes a display unit for displaying graphical user interfaces.</p>
<p id="p-0094" num="0093">The features described may be implemented in digital electronic circuitry, or in computer hardware, or in combinations of computer hardware and firmware or software. The apparatus may be implemented in a computer program product tangibly embodied in a machine-readable storage device, for execution by a programmable processor; and method steps may be performed by a programmable processor executing a program of instructions to perform functions of the described implementations by operating on input data and generating output. The described features may be implemented advantageously in one or more computer programs that are executable on a programmable system including at least one programmable processor coupled to receive data and instructions from, and to transmit data and instructions to, a data storage system, at least one input device, and at least one output device. A computer program is a set of instructions that may be used, directly or indirectly, in a computer to perform a certain activity or bring about a certain result. A computer program may be written in any appropriate form of programming language, including compiled or interpreted languages, and it may be deployed in any appropriate form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.</p>
<p id="p-0095" num="0094">Suitable processors for the execution of a program of instructions include, by way of example, both general and special purpose microprocessors, and the sole processor or one of multiple processors of any relevant kind of computer. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for executing instructions and one or more memories for storing instructions and data. Generally, a computer will also include, or be operatively coupled to communicate with, one or more mass storage devices for storing data files; such devices include magnetic disks, such as internal hard disks and removable disks; magneto-optical disks; and optical disks. Storage devices suitable for tangibly embodying computer program instructions and data include all forms of non-volatile memory, including by way of example semiconductor memory devices, such as EPROM, EEPROM, and flash memory devices; magnetic disks such as internal hard disks and removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks. The processor and the memory may be supplemented by, or incorporated in, ASICs (application-specific integrated circuits).</p>
<p id="p-0096" num="0095">To provide for interaction with a user, the features may be implemented on a computer having a display device such as a CRT (cathode ray tube) or LCD (liquid crystal display) monitor for displaying information to the user and a keyboard and a pointing device such as a mouse or a trackball by that the user may provide input to the computer.</p>
<p id="p-0097" num="0096">The features may be implemented in a computer system that includes a back-end component, such as a data server, or that includes a middleware component, such as an application server or an Internet server, or that includes a front-end component, such as a client computer having a graphical user interface or an Internet browser, or any operable combination of them. The components of the system may be connected by any relevant form or medium of digital data communication such as a communication network. Examples of communication networks include, e.g., a LAN, a WAN, and the computers and networks forming the Internet.</p>
<p id="p-0098" num="0097">The computer system may include clients and servers. A client and server are generally remote from each other and typically interact through a network, such as the described one. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.</p>
<p id="p-0099" num="0098">An electronic document (which for brevity will simply be referred to as a document) may, but need not, correspond to a file. A document may be stored in a portion of a file that holds other documents, in a single file dedicated to the document in question, or in multiple coordinated files.</p>
<p id="p-0100" num="0099">Embodiments of the subject matter and the operations described in this specification may be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification may be implemented as one or more computer programs, i.e., one or more modules of computer program instructions, encoded on computer storage medium for execution by, or to control the operation of, data processing apparatus.</p>
<p id="p-0101" num="0100">A computer storage medium may be, or be included in, a computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them. Moreover, while a computer storage medium is not a propagated signal, a computer storage medium may be a source or destination of computer program instructions encoded in an artificially-generated propagated signal. The computer storage medium may also be, or be included in, one or more separate physical components or media (e.g., multiple CDs, disks, or other storage devices). The operations described in this specification may be implemented as operations performed by a data processing apparatus on data stored on one or more computer-readable storage devices or received from other sources.</p>
<p id="p-0102" num="0101">The term &#x201c;data processing apparatus&#x201d; encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, a system on a chip, or multiple ones, or combinations, of the foregoing The apparatus may include special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). The apparatus may also include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them. The apparatus and execution environment may realize various different computing model infrastructures, such as web services, distributed computing and grid computing infrastructures.</p>
<p id="p-0103" num="0102">A computer program (also known as a program, software, software application, script, or code) may be written in any appropriate form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it may be deployed in any operable form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment. A computer program may, but need not, correspond to a file in a file system. A program may be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub-programs, or portions of code). A computer program may be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.</p>
<p id="p-0104" num="0103">The processes and logic flows described in this specification may be performed by one or more programmable processors executing one or more computer programs to perform actions by operating on input data and generating output. The processes and logic flows may also be performed by, and apparatus may also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).</p>
<p id="p-0105" num="0104">Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any appropriate kind of digital computer. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for performing actions in accordance with instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such devices. Moreover, a computer may be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a universal serial bus (USB) flash drive), to name just a few. Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks. The processor and the memory may be supplemented by, or incorporated in, special purpose logic circuitry.</p>
<p id="p-0106" num="0105">To provide for interaction with a user, embodiments of the subject matter described in this specification may be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user may provide input to the computer. Other kinds of devices may be used to provide for interaction with a user as well; for example, feedback provided to the user may be any appropriate form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any relevant form, including acoustic, speech, or tactile input. In addition, a computer may interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's client device in response to requests received from the web browser.</p>
<p id="p-0107" num="0106">Embodiments of the subject matter described in this specification may be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface or a Web browser through which a user may interact with an implementation of the subject matter described in this specification, or any combination of one or more such back-end, middleware, or front-end components. The components of the system may be interconnected by any appropriate form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (&#x201c;LAN&#x201d;) and a wide area network (&#x201c;WAN&#x201d;), an inter-network (e.g., the Internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).</p>
<p id="p-0108" num="0107">The computing system may include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, a server transmits data (e.g., an HTML page) to a client device (e.g., for purposes of displaying data to and receiving user input from a user interacting with the client device). Data generated at the client device (e.g., a result of the user interaction) may be received from the client device at the server.</p>
<p id="p-0109" num="0108">A number of implementations have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. Accordingly, other implementations are within the scope of the following claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A computer-implemented method comprising:
<claim-text>obtaining two or more candidate transcriptions of a single voice command;</claim-text>
<claim-text>identifying one or more possible intended actions for each of the two or more candidate transcriptions of the single voice command, including identifying two or more possible intended actions for a particular transcription of the two or more candidate transcriptions of the single voice command;</claim-text>
<claim-text>providing information for display, the information identifying (i) the two or more candidate transcriptions of the single voice command, and (ii) the one or more possible intended actions for each of the two or more transcriptions of the single voice command, including the two or more possible intended actions for the particular transcription;</claim-text>
<claim-text>receiving, by one or more computers, data indicating a selection of a particular possible intended action from among the displayed one or more possible intended actions for each of the two or more transcriptions of the single voice command, and the displayed two or more possible intended actions for the particular transcription; and</claim-text>
<claim-text>invoking the selected particular possible intended action.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein, when the particular transcription comprises a name of a person, the two or more possible intended actions for the particular transcription includes two or more of (i) an action that initiates a call, (ii) an action that initiates an email, and (iii) an action that initiates an instant messaging session with the person.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein, when the particular transcription comprises a point of interest (POI), the two or more possible intended actions for the particular transcription includes two or more of (i) an action that obtains a map of the POI, (ii) an action that obtains directions to the POI, (iii) an action that obtains detailed information about the POI, and (iv) an action that obtains a distance to the POI.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein, when the particular transcription identifies media content, the two or more possible intended actions for the particular transcription includes two or more of (i) an action that plays the media content, (ii) an action that initiates a download for the media content, and (iii) an action that obtains detailed information about the media content.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the two or more possible intended actions for the particular transcription includes an action that initiates a web search query using the particular transcription as a query term.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising determining an ambiguity value for each of the one or more possible intended actions for the particular transcription and for each of the two or more possible intended actions for the particular transcription, wherein an ambiguity value reflects a server-determined level of certainty that the user actually intended to perform a possible intended action.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising:
<claim-text>responsive to receiving data indicating the selection of the particular possible intended action, incrementing the ambiguity value associated with the particular possible intended action.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising:
<claim-text>determining that the ambiguity value for the particular possible intended action satisfies a threshold;</claim-text>
<claim-text>automatically invoking the particular possible intended action based on determining that the ambiguity value satisfies the threshold;</claim-text>
<claim-text>determining that a user has cancelled the particular possible intended action; and</claim-text>
<claim-text>decrementing the ambiguity value for the particular possible intended action based on determining that the user has cancelled the particular possible intended action.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising:
<claim-text>generating a ranking of the one or more possible intended actions for each of the two or more candidate transcriptions and the two or more possible intended actions for the particular transcription based on their respective ambiguity values,</claim-text>
<claim-text>wherein providing the information further comprises providing the ranking.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the information specifying the particular possible intended action comprises an icon.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein invoking the selected particular possible intended action comprises providing data specifying an application that is stored by a client device.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. A system comprising:
<claim-text>one or more computers; and</claim-text>
<claim-text>a computer-readable medium coupled to the one or more computers having instructions stored thereon which, when executed by the one or more computers, cause the one or more computers to perform operations comprising:
<claim-text>obtaining two or more candidate transcriptions of a single voice command;</claim-text>
<claim-text>identifying one or more possible intended actions for each of the two or more candidate transcriptions of the single voice command, including identifying two or more possible intended actions for a particular transcription of the two or more candidate transcriptions of the single voice command;</claim-text>
<claim-text>providing information for display, the information identifying (i) the two or more candidate transcriptions of the single voice command, and (ii) the one or more possible intended actions for each of the two or more transcriptions of the single voice command, including the two or more possible intended actions for the particular transcription;</claim-text>
<claim-text>receiving data indicating a selection of a particular possible intended action from among the displayed one or more possible intended actions for each of the two or more transcriptions of the single voice command, and the displayed two or more possible intended actions for the particular transcription; and</claim-text>
<claim-text>invoking the selected particular possible intended action.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein, when the particular command transcription comprises a name of a person, the two or more possible intended actions for the particular transcription includes two or more of (i) an action that initiates a call, (ii) an action that initiates an email, and (iii) an action that initiates an instant messaging session with the person.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. A computer-implemented method comprising:
<claim-text>obtaining information specifying two or more displayed candidate transcriptions of a single voice command and one or more displayed possible intended actions for each of the two or more displayed candidate transcriptions of the single voice command, including two or more displayed possible intended actions for a particular transcription of the two or more displayed candidate transcriptions of the single voice command;</claim-text>
<claim-text>receiving data indicating a selection of a particular displayed possible intended action from among the one or more possible displayed intended actions for each of the two or more transcriptions of the single voice command and the two or more possible displayed intended actions for the particular transcription;</claim-text>
<claim-text>providing data indicating the selection of the particular displayed possible intended action to a server; and</claim-text>
<claim-text>invoking the selected particular displayed possible intended action.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, comprising transmitting an index of contacts to the server.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, comprising:
<claim-text>determining an ambiguity value for each of the one or more displayed possible intended actions for each of the two or more displayed candidate transcriptions and for each of the two or more displayed possible intended actions for the particular transcription;</claim-text>
<claim-text>determining that the ambiguity value for the particular displayed possible intended action satisfies a threshold;</claim-text>
<claim-text>automatically invoking the particular displayed possible intended action based on determining that the ambiguity value satisfies the threshold;</claim-text>
<claim-text>determining that the user has cancelled the particular displayed possible intended action; and</claim-text>
<claim-text>decrementing the ambiguity value for the particular displayed possible intended action based on determining that the user has cancelled the particular displayed possible intended action.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. A non-transitory computer storage medium encoded with a computer program, the program comprising instructions that when executed by data processing apparatus cause the data processing apparatus to perform operations comprising:
<claim-text>obtaining information specifying two or more displayed candidate transcriptions of a single voice command and one or more displayed possible intended actions for each of the two or more displayed candidate transcriptions of the single voice command, including two or more displayed possible intended actions for a particular transcription of the two or more displayed candidate transcriptions of the single voice command;</claim-text>
<claim-text>receiving data indicating a selection of a particular displayed possible intended action from among the one or more possible displayed intended actions for each of the two or more transcriptions of the single voice command and the two or more possible displayed intended actions for the particular transcription;</claim-text>
<claim-text>providing data indicating the selection of the particular displayed possible intended action to a server; and</claim-text>
<claim-text>invoking the selected particular displayed possible intended action.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The medium of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the operations further comprise transmitting an index of contacts to the server.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The medium of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the operations further comprise:
<claim-text>determining an ambiguity value for each of the one or more displayed possible intended actions for each of the two or more displayed candidate transcriptions and for each of the two or more displayed possible intended actions for the particular transcription;</claim-text>
<claim-text>determining that the ambiguity value for the particular displayed possible intended action satisfies a threshold;</claim-text>
<claim-text>automatically invoking the particular displayed possible intended action based on determining that the ambiguity value satisfies the threshold;</claim-text>
<claim-text>determining that the user has cancelled the particular displayed possible intended action; and</claim-text>
<claim-text>decrementing the ambiguity value for the particular displayed possible intended action based on determining that the user has cancelled the particular displayed possible intended action.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
