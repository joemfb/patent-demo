<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626918-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626918</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>11129987</doc-number>
<date>20050516</date>
</document-id>
</application-reference>
<us-application-series-code>11</us-application-series-code>
<us-term-of-grant>
<us-term-extension>3099</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>15</main-group>
<subgroup>173</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>9</main-group>
<subgroup>46</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>709226</main-classification>
<further-classification>709223</further-classification>
<further-classification>718105</further-classification>
</classification-national>
<invention-title id="d2e53">Workload allocation based upon heat re-circulation causes</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>6104003</doc-number>
<kind>A</kind>
<name>Jones</name>
<date>20000800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>219400</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6374627</doc-number>
<kind>B1</kind>
<name>Schumacher et al.</name>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification> 622592</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6415617</doc-number>
<kind>B1</kind>
<name>Seem</name>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification> 62186</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6795928</doc-number>
<kind>B2</kind>
<name>Bradley et al.</name>
<date>20040900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>713320</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6889908</doc-number>
<kind>B2</kind>
<name>Crippen et al.</name>
<date>20050500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>236 493</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2002/0004915</doc-number>
<kind>A1</kind>
<name>Fung</name>
<date>20020100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>713320</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2004/0228087</doc-number>
<kind>A1</kind>
<name>Coglitore</name>
<date>20041100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>361687</main-classification></classification-national>
</us-citation>
<us-citation>
<nplcit num="00008">
<othercit>Moore, J. et al., &#x201c;Making Scheduling &#x201c;Cool&#x201d;: Temperature-Aware Workload Placement in Data Centers&#x201d;, Apr. 13, 2005.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>23</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>709223</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>709226</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>718105</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>6</number-of-drawing-sheets>
<number-of-figures>6</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20060259622</doc-number>
<kind>A1</kind>
<date>20061116</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Moore</last-name>
<first-name>Justin</first-name>
<address>
<city>Durham</city>
<state>NC</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Ranganathan</last-name>
<first-name>Parthasarathy</first-name>
<address>
<city>Fremont</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Sharma</last-name>
<first-name>Ratnesh</first-name>
<address>
<city>Union City</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Moore</last-name>
<first-name>Justin</first-name>
<address>
<city>Durham</city>
<state>NC</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Ranganathan</last-name>
<first-name>Parthasarathy</first-name>
<address>
<city>Fremont</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Sharma</last-name>
<first-name>Ratnesh</first-name>
<address>
<city>Union City</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Hewlett-Packard Development Company, L.P.</orgname>
<role>02</role>
<address>
<city>Houston</city>
<state>TX</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Serrao</last-name>
<first-name>Ranodhi</first-name>
<department>2444</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A method of allocating workload among servers in a geographically collocated cluster of compute equipment includes calibrating causes of heat re-circulation in the cluster of compute equipment. In addition, workload is allocated among the servers to address causes of the heat re-circulation to reduce costs associated with cooling the compute equipment.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="158.58mm" wi="220.05mm" file="US08626918-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="184.23mm" wi="122.51mm" orientation="landscape" file="US08626918-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="191.26mm" wi="125.14mm" orientation="landscape" file="US08626918-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="234.78mm" wi="159.26mm" orientation="landscape" file="US08626918-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="82.80mm" wi="126.83mm" file="US08626918-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="221.40mm" wi="157.56mm" orientation="landscape" file="US08626918-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="217.59mm" wi="137.58mm" orientation="landscape" file="US08626918-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND</heading>
<p id="p-0002" num="0001">A data center may be defined as a location, for instance, a room that houses computer systems arranged in a number of racks. A standard rack, for example, an electronics cabinet, is defined as an Electronics Industry Association (EIA) enclosure, 78 in. (2 meters) high, 24 in. (0.61 meter) wide and 30 in. (0.76 meter) deep. These racks are configured to house a number of computer systems, about forty (40) systems, with future configurations of racks being designed to accommodate 200 or more systems. The computer systems typically include a number of printed circuit boards (PCBs), mass storage devices, power supplies, processors, micro-controllers, and semi-conductor devices, that dissipate relatively significant amounts of heat during their operation. For example, a typical computer system containing multiple microprocessors dissipates approximately 250 W of power. Thus, a rack containing forty (40) computer systems of this type dissipates approximately 10 KW of power.</p>
<p id="p-0003" num="0002">Current approaches to provisioning cooling to dissipate the heat generated by the cooling systems are typically based on using energy balance to size the air conditioning units and intuition to design air distributions in the data center. In many instances, the provisioning of the cooling is based on the nameplate power ratings of all of the servers in the data center, with some slack for risk tolerance. This type of cooling provisioning oftentimes leads to excessive and inefficient cooling solutions. This problem is further exacerbated by the fact that in most data centers, the cooling is provisioned for worst-case or peak load scenarios. Since it is estimated that typical data center operations only utilize a fraction of the servers, provisioning for these types of scenarios often increases the inefficiencies found in conventional cooling arrangements.</p>
<p id="p-0004" num="0003">As such, it would be beneficial to have effective thermal management that does not suffer from the inefficiencies found in conventional data center cooling arrangements.</p>
<heading id="h-0002" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0005" num="0004">A method of allocating workload among servers in a geographically collocated cluster of compute equipment is disclosed herein. In the method, causes of heat re-circulation in the cluster of compute equipment are calibrated. In addition, workload is allocated among the servers to address causes of the heat re-circulation to reduce costs associated with cooling the compute equipment.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0006" num="0005">Features of the present invention will become apparent to those skilled in the art from the following description with reference to the figures, in which:</p>
<p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. 1A</figref> shows a simplified perspective view of a data center, according to an embodiment of the invention;</p>
<p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. 1B</figref> is a perspective view of a component that may be housed in the racks depicted in <figref idref="DRAWINGS">FIG. 1A</figref>;</p>
<p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram of a power distribution system according to an embodiment of the invention;</p>
<p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. 3</figref> illustrates a flow diagram of method for distributing power among servers, according to an embodiment of the invention;</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 4</figref> shows a flow diagram of a method for distributing power among servers, according to an embodiment of the invention;</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 5</figref> illustrates a computer system, which may be employed to perform the various functions of the power distribution system, according to an embodiment of the invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION OF THE INVENTION</heading>
<p id="p-0013" num="0012">For simplicity and illustrative purposes, the present invention is described by referring mainly to an exemplary embodiment thereof. In the following description, numerous specific details are set forth in order to provide a thorough understanding of the present invention. It will be apparent however, to one of ordinary skill in the art, that the present invention may be practiced without limitation to these specific details. In other instances, well known methods and structures have not been described in detail so as not to unnecessarily obscure the present invention.</p>
<p id="p-0014" num="0013">The effectiveness of a server thermal package generally depends on external environmental controls to maintain inlet air temperatures within a safe operating range below the &#x201c;redline&#x201d; of approximately 25&#xb0; C. A variety of factors may cause temperature variations and thermal hot spots due to, for instance, heat re-circulation, in a geographically collocated cluster of compute equipment (hereinafter &#x201c;data center&#x201d;), such as, a data center, a collection of racks, a single rack, a cluster of servers, etc. For instance, non-uniform equipment loads in the data center may cause some areas to have higher temperatures as compared with other areas, while irregular airflows may cause some areas to have lower temperatures than other areas. In data centers having relatively high heat densities, for instance, heat dissipation of around 2000 W/m<sup>2 </sup>(200 W/ft<sup>2</sup>) or more, mixing of hot and cold streams generally leads to complex airflow patterns that can create hot spots. Hot spots typically create a risk of redlining servers by exceeding the specified maximum inlet air temperature, damaging electronic components and causing them to fail prematurely. In addition, thermal imbalances often interfere with efficient cooling operation.</p>
<p id="p-0015" num="0014">As described in greater detail herein below, the inefficiencies and potential hazards often associated with heat re-circulation may be substantially reduced, thereby leading to a more efficient and safer operating environment for the components in the data center. The heat re-circulation may be reduced through workload placement on various groups of servers based upon the calculated effects the workload placement has on the various groups of servers. In this regard, methods and algorithms are described below that may be implemented to determine the causes of the re-circulation and to determine workload placement distributions that address the causes of the re-circulation and to thereby reduce the re-circulation in the data center.</p>
<p id="p-0016" num="0015">With reference first to <figref idref="DRAWINGS">FIG. 1A</figref>, there is shown a simplified perspective view of a data center <b>100</b>. The terms &#x201c;data center&#x201d; are generally meant to denote a room or other space and are not meant to limit the invention to any specific type of room where data is communicated or processed, nor should it be construed that use of the terms &#x201c;data center&#x201d; limits the invention in any respect other than its definition hereinabove. The terms &#x201c;data center&#x201d; as referenced throughout the present disclosure may also denote any physically collocated collection of computing equipment, such as, for instance, computing equipment contained in a single rack, a cluster of racks, etc. In addition, although particular reference is made throughout to CRAC units, various other types of air conditioning units may be employed. For instance, if the &#x201c;data center&#x201d; as referenced herein comprises a rack of computing equipment, the CRAC units may comprise, for instance, server air conditioning units, fans and cooling systems specific to the rack, etc.</p>
<p id="p-0017" num="0016">The data center <b>100</b> depicted in <figref idref="DRAWINGS">FIG. 1A</figref> represents a generalized illustration and other components may be added or existing components may be removed or modified without departing from a scope of the data center <b>100</b>. For example, the data center <b>100</b> may include any number of racks and various other apparatuses known to be housed in data centers. Thus, although the data center <b>100</b> is illustrated as containing four rows of racks <b>102</b>-<b>108</b> and two computer room air conditioning (CRAC) units <b>110</b>, it should be understood that the data center <b>100</b> may include any number of racks, for instance, 100 racks, and CRAC units <b>110</b>. The depiction of four rows of racks <b>102</b>-<b>108</b> and two CRAC units <b>110</b> is thus for illustrative and simplicity of description purposes only and is not intended to limit the data center <b>100</b> in any respect.</p>
<p id="p-0018" num="0017">The data center <b>100</b> is depicted as having a plurality of racks <b>102</b>-<b>108</b>, for instance, electronics cabinets, aligned in substantially parallel rows. The racks <b>102</b>-<b>108</b> are illustrated as having open front sides such that the components <b>112</b> housed therein are visible. It should, however, be understood that the data center <b>100</b> may include racks <b>102</b>-<b>108</b> having panels that cover the front sides of the racks <b>102</b>-<b>108</b> without departing from a scope of the data center <b>100</b>.</p>
<p id="p-0019" num="0018">The components <b>112</b> may comprise, for instance, computers, servers, monitors, hard drives, disk drives, etc., designed to perform various operations, for instance, computing, switching, routing, displaying, etc. These components <b>112</b> may comprise subsystems (not shown), for example, processors, micro-controllers, high-speed video cards, memories, semi-conductor devices, and the like to perform these functions. In the performance of these electronic functions, the subsystems and therefore the components <b>112</b>, generally dissipate relatively large amounts of heat. Because the racks <b>102</b>-<b>108</b> have generally been known to include upwards of 200 or more components <b>112</b>, they may require substantially large amounts of cooling resources to maintain the subsystems and the components <b>112</b> generally within predetermined operating temperature ranges.</p>
<p id="p-0020" num="0019">A relatively small number of components <b>112</b> are illustrated as being housed in the racks <b>102</b>-<b>108</b> for purposes of simplicity. It should, however, be understood that the racks <b>102</b>-<b>108</b> may include any number of components <b>112</b>, for instance, forty or more components <b>112</b>, or 200 or more blade systems. In addition, although the racks <b>102</b>-<b>108</b> are illustrated as containing components <b>112</b> throughout the heights of the racks <b>102</b>-<b>108</b>, it should be understood that some of the racks <b>102</b>-<b>108</b> may include slots or areas that do not include components <b>112</b> without departing from the scope of the racks <b>102</b>-<b>108</b>.</p>
<p id="p-0021" num="0020">The rows of racks <b>102</b>-<b>108</b> are shown as containing four racks (a-d) positioned on a raised floor <b>114</b>. A plurality of wires and communication lines (not shown) may be located in a space <b>116</b> beneath the raised floor <b>114</b>. The space <b>116</b> may also function as a plenum for delivery of cooling airflow from the CRAC units <b>110</b> to the racks <b>102</b>-<b>108</b>. The cooled airflow may be delivered from the space <b>116</b> to the racks <b>102</b>-<b>108</b> through a plurality of vent tiles <b>118</b> located between some or all of the racks <b>102</b>-<b>108</b>. The vent tiles <b>118</b> are shown in <figref idref="DRAWINGS">FIG. 1A</figref> as being located between racks <b>102</b> and <b>104</b> and <b>106</b> and <b>108</b>. One or more temperature sensors (not shown) may also be positioned in the space <b>116</b> to detect the temperatures of the airflow supplied by the CRAC units <b>110</b>.</p>
<p id="p-0022" num="0021">The CRAC units <b>110</b> generally operate to receive heated airflow from the data center <b>100</b>, cool the heated airflow, and to deliver the cooled airflow into the plenum <b>116</b>. The CRAC units <b>110</b> may comprise vapor-compression type air conditioning units, water-chiller type air conditioning units, etc. In one regard, the CRAC units <b>110</b> may operate in manners generally consistent with conventional CRAC units <b>110</b>. Alternatively, the CRAC units <b>110</b> and the vent tiles <b>118</b> may be operated to vary characteristics of the cooled airflow delivery as described, for instance, in commonly assigned U.S. Pat. No. 6,574,104, filed on Oct. 5, 2001, which is hereby incorporated by reference in its entirety.</p>
<p id="p-0023" num="0022">In an ideal system, the airflow delivered into the racks <b>102</b>-<b>108</b> comprises only the cooled airflow supplied directly by the CRAC units <b>110</b> and the airflow returning into the CRAC units <b>110</b> comprises only the airflow heated by the components <b>112</b> in the racks <b>102</b>-<b>108</b>. However, there are typically areas in the data center <b>100</b> where the cooled airflow and the heated airflow mix. In other words, there typically are areas in the data center <b>100</b> where the airflow heated by the components <b>112</b> re-circulates back into the cooled airflow prior to the cooled airflow being supplied into the racks <b>102</b>-<b>108</b>. By way of example, cooled air may mix with heated air around the sides or over the tops of one or more of the racks <b>102</b>-<b>108</b>. A metric for quantifying the amount of heat that re-circulates in the data center <b>100</b> has previously been termed the return heat index (RHI) in co-pending and commonly assigned U.S. patent application Ser. No. 10/446,854, filed on May 29, 2003, the disclosure of which is hereby incorporated by reference in its entirety.</p>
<p id="p-0024" num="0023">As disclosed in that co-pending application, RHI may be determined through the following equation:</p>
<p id="p-0025" num="0024">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
  <mrow>
    <mi>Equation</mi>
    <mo>&#x2062;</mo>
    <mstyle>
      <mspace width="1.1em" height="1.1ex"/>
    </mstyle>
    <mo>&#x2062;</mo>
    <mrow>
      <mo>(</mo>
      <mn>1</mn>
      <mo>)</mo>
    </mrow>
    <mo>&#x2062;</mo>
    <mstyle>
      <mtext>:</mtext>
    </mstyle>
  </mrow>
</math>
</maths>
<maths id="MATH-US-00001-2" num="00001.2">
<math overflow="scroll">
  <mrow>
    <mrow>
      <mi>RHI</mi>
      <mo>=</mo>
      <mfrac>
        <mi>Q</mi>
        <mrow>
          <mi>Q</mi>
          <mo>+</mo>
          <mrow>
            <mi>&#x3b4;</mi>
            <mo>&#x2062;</mo>
            <mi>Q</mi>
          </mrow>
        </mrow>
      </mfrac>
    </mrow>
    <mo>,</mo>
  </mrow>
</math>
</maths>
<br/>
where Q is the total heat produced by the components <b>112</b> and &#x3b4;Q is the amount of heat that re-circulates in the data center <b>100</b>. As the amount of re-circulating heat approaches zero, RHI approaches one. Thus, RHI is a measure of the amount of heat that exits from the components <b>112</b> and flows directly into the CRAC unit <b>110</b> return vents without re-circulating back into the component <b>112</b> inlets.
</p>
<p id="p-0026" num="0025">U.S. patent application Ser. No. 10/446,854 generally focuses on the consequences of the re-circulation. The present disclosure differs from that application by focusing more on the causes of the re-circulation.</p>
<p id="p-0027" num="0026">As such, higher RHI values are indicative of lower heat re-circulation, which correspond to lower costs associated with cooling. Typically, smaller differences in RHI at higher utilization produce a more pronounced difference in cooling cost because of the total amount of heat in the system. More particularly, a data center <b>100</b> with the same RHI at low and high utilizations has more total heat re-circulating at higher utilizations, and thus the costs associated with cooling the data center <b>100</b> are higher. Various systems and methods are described herein below to distribute power amongst the components <b>112</b> in manners that substantially minimize the total amount of heat recirculation in the data center <b>100</b> while substantially maximizing the power budget, and therefore, the potential utilization, of each component <b>112</b>.</p>
<p id="p-0028" num="0027">With reference back to <figref idref="DRAWINGS">FIG. 1A</figref>, there is also shown a resource manager <b>120</b>, depicted as an individual computing device. Although the resource manager <b>120</b> is illustrated as being separate from and located away from the racks <b>102</b>-<b>108</b>, the resource manager <b>120</b> may also comprise a server or other computing device housed in one of the racks <b>102</b>-<b>108</b>. In addition, if the resource manager <b>120</b> is comprised in a server or other computing device, the resource manager <b>120</b> may be implemented on the local application scheduler level, the operating system, virtual machine scheduler, hardware, etc. In any regard, the resource manager <b>120</b> is generally configured to control various operations in the data center <b>100</b>. For instance, the resource manager <b>120</b> may be configured to control power distribution amongst the various components <b>112</b>, as described in greater detail herein below. As another example, the resource manager <b>120</b> may be configured to control various operations of the CRAC units <b>110</b> and the vent tiles <b>118</b>, collectively considered herein as the cooling system.</p>
<p id="p-0029" num="0028">The CRAC units <b>110</b> may include sensors (not shown) configured to detect at least one environmental condition, for instance, temperature, pressure, humidity, etc. These sensors may comprise any reasonably suitable conventional sensors configured to detect one or more of these environmental conditions. The sensors may be positioned at various locations of the data center <b>100</b>. The sensors may be positioned, for instance, to detect the temperature of the cooled airflow supplied by the CRAC units <b>110</b>. The sensors may comprise devices separate from the CRAC units <b>110</b> or they may comprise devices integrated with the CRAC units <b>110</b>.</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 1B</figref> is a perspective view of a component <b>112</b>, depicted here as a server, that may be housed in the racks <b>102</b>-<b>108</b> depicted in <figref idref="DRAWINGS">FIG. 1A</figref>. The component <b>112</b> may comprise a server that is configured for substantially horizontal mounting in a rack <b>102</b>-<b>108</b> or a server that is configured for substantially vertical mounting in a rack <b>102</b>, <b>108</b>, such as, a blade system. In any regard, the component <b>112</b> will be considered as a server throughout the remainder of the present disclosure. In addition, it should be understood that the server <b>112</b> depicted in <figref idref="DRAWINGS">FIG. 1B</figref> represents a generalized illustration and, therefore, other devices and design features may be added or existing devices or design features may be removed, modified, or rearranged without departing from the scope of the server <b>112</b>. For example, the server <b>112</b> may include various openings for venting air through an interior of the server <b>112</b>. As another example, the various devices shown in the server <b>112</b> may be re-positioned, removed, or changed.</p>
<p id="p-0031" num="0030">As shown in <figref idref="DRAWINGS">FIG. 1B</figref>, the server <b>112</b> includes a housing <b>130</b> with a top section of the housing <b>130</b> removed for purposes of illustration. In addition, a part of a front section <b>132</b> of the housing <b>130</b> has been cut-away to more clearly show some of the devices contained in the server <b>112</b>. The front section <b>132</b> is illustrated as containing various features to enable access to various devices contained in the server <b>112</b>. For instance, the front section <b>132</b> is shown as including openings <b>134</b> and <b>136</b> for insertion of various media, for example, diskettes, flash memory cards, CD-Roms, etc. Located substantially directly behind the openings <b>134</b> and <b>136</b> are data storage devices <b>138</b> and <b>140</b> configured to read and/or write onto the various media. The front section <b>132</b> also includes vents <b>142</b> for enabling airflow into an interior of the housing <b>130</b>.</p>
<p id="p-0032" num="0031">The housing <b>130</b> also includes a plurality of side sections <b>144</b> and <b>146</b> and a rear section <b>148</b>. The rear section <b>148</b> includes openings <b>150</b> to generally enable airflow out of the housing <b>130</b>. Although not clearly shown in <figref idref="DRAWINGS">FIG. 1B</figref>, the rear section <b>148</b> also includes openings for insertion of wires, cables, and the like, into the housing <b>130</b> for connection to various devices contained in the housing <b>130</b>. In addition, some of the openings <b>150</b> in the rear section <b>148</b> may include devices to enable the interfacing of certain devices contained in the housing <b>130</b> with various other electronic devices.</p>
<p id="p-0033" num="0032">Contained within the housing <b>130</b> is a plurality of electronic components <b>154</b> which, during operation, generate heat (hereinafter referred to as &#x201c;heat-generating devices&#x201d;). Some of the heat-generating devices <b>154</b> may comprise microprocessors, power converters, memory controllers, power supplies, disk drives, etc. In addition, some of the heat-generating devices <b>154</b> may include heat sinks <b>156</b> configured to dissipate relatively larger amounts of heat generated by these devices <b>154</b> by providing a relatively larger surface area from which heat may be dissipated through convection.</p>
<p id="p-0034" num="0033">Also illustrated in the server <b>112</b> is an optional fan cell <b>158</b>. The fan cell <b>158</b> is considered optional because the additional airflow produced through use of the fan cell <b>158</b> may not be required in certain servers <b>112</b>. In any regard, the optional fan cell <b>158</b> is depicted as being composed of fans <b>160</b> for blowing air through the server <b>112</b>. The optional fan cell <b>158</b> is depicted as containing five fans <b>160</b> for illustrative purposes only and may therefore contain any reasonably suitable number of fans, for instance, from 1 to 10 or more fans. The fans <b>160</b> contained in the fan cell <b>158</b> may comprise relatively low capacity fans or they may comprise high capacity fans that may be operated at low capacity levels. In addition, the fans may have sufficiently small dimensions to enable their placement in the housing <b>130</b> without, for instance, substantially interfering with the operations of other devices contained in the server <b>112</b>. Moreover, the optional fan cell <b>158</b> may be positioned at locations in or around the server <b>112</b> without departing from a scope of the server <b>112</b>.</p>
<p id="p-0035" num="0034">The server <b>112</b> is also illustrated as including an inlet sensor <b>162</b> and an outlet sensor <b>164</b>. The inlet sensor <b>162</b> may comprise a sensor configured to detect temperature of airflow supplied into the server <b>112</b>. Likewise, the outlet sensor <b>164</b> may be configured to detect the temperature of the airflow exiting the server <b>112</b>. In this regard, the sensors <b>162</b> and <b>164</b> may comprise any reasonably suitable temperature sensors, such as, a thermocouples, thermistors, thermometers, etc. In addition, the sensors <b>162</b> and <b>164</b> may be integrally manufactured with the server <b>112</b> or the sensors <b>162</b> and <b>164</b> may be installed in the server <b>112</b> as an after-market device.</p>
<p id="p-0036" num="0035">In addition, or alternatively, the inlet sensor <b>162</b> and the outlet sensor <b>164</b> may comprise pressure sensors, which may be used to determine a pressure drop across the server <b>112</b>. In this case, the inlet and outlet sensors <b>162</b> and <b>164</b> may be employed to calculate the mass flow rate of air flow through the server <b>112</b>. More particularly, the mass airflow rate may be correlated to a detected pressure drop across the server <b>112</b>.</p>
<p id="p-0037" num="0036">The mass airflow rate through the server <b>112</b> may also, or alternatively, be determined through use of other means. In a first example, the mass airflow rate may be calculated as a function of the speeds at which the fans <b>160</b> in the fan cell <b>158</b> are operated. In a second example, the mass airflow rate may be detected directly through use of an anemometer, for instance.</p>
<p id="p-0038" num="0037">As will be described in greater detail below, the temperature measurements obtained through use of the temperature sensor <b>162</b> and the mass flow rate of airflow through the server <b>112</b>, along with other information, may be employed to calculate the amount of heat re-circulating in the server <b>112</b> and the data center <b>100</b>. The calculated re-circulation amount may be employed to determine power distribution schemes amongst the various servers <b>112</b> to substantially minimize the total amount of heat recirculation in the data center <b>100</b>, while substantially maximizing the power budget, and therefore, the potential utilization, of each server <b>112</b>. Initially, however, a system depicting an environment in which various power distribution methods may be implemented is discussed with respect to <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0039" num="0038">More particularly, <figref idref="DRAWINGS">FIG. 2</figref> is a block diagram <b>200</b> of a power distribution system <b>202</b> that may implement the power distribution methods described below. It should be understood that the following description of the block diagram <b>200</b> is but one manner of a variety of different manners in which such a power distribution system <b>202</b> may be configured. In addition, it should be understood that the power distribution system <b>202</b> may include additional components and that some of the components described herein may be removed and/or modified without departing from the scope of the power distribution system <b>202</b>. For instance, the power distribution system <b>202</b> may include any number of sensors, servers, CRAC units, etc., as well as other components, which may be implemented in the operations of the power distribution system <b>202</b>.</p>
<p id="p-0040" num="0039">As shown, the power distribution system <b>202</b> may comprise a general computing environment and includes the resource manager <b>120</b> depicted in <figref idref="DRAWINGS">FIG. 1A</figref>. As described herein above, the resource manager <b>120</b> is configured to perform various functions in the data center <b>100</b>. In this regard, the resource manager <b>120</b> may comprise a computing device, for instance, a computer system, a server, etc. In addition, the resource manager <b>120</b> may comprise a microprocessor, a micro-controller, an application specific integrated circuit (ASIC), and the like, configured to perform various processing functions. In one respect, the resource manager <b>120</b> may comprise a controller of another computing device. Alternatively, the resource manager <b>120</b> may comprise software operating in a computing device.</p>
<p id="p-0041" num="0040">Data may be transmitted to various components of the power distribution system <b>202</b> over a system bus <b>204</b> that operates to couple the various components of the power distribution system <b>202</b>. The system bus <b>204</b> represents any of several types of bus structures, including, for instance, a memory bus, a memory controller, a peripheral bus, an accelerated graphics port, a processor bus using any of a variety of bus architectures, and the like.</p>
<p id="p-0042" num="0041">One or more input sources <b>206</b> may be employed to input information into the power distribution system <b>202</b>. The input sources <b>206</b> may comprise, for instance, computing devices connected over an internal network or an external network, such as, the Internet. The input sources <b>206</b> may also comprise peripheral devices, such as, a disk drive, removable media, flash drives, a keyboard, a mouse, and the like. The input sources <b>206</b> may be used, for instance, as a means to request that a workload or application be performed by some of the servers <b>112</b> in the data center <b>100</b>. By way of example, a request to perform a multimedia application may be received into the power distribution system <b>202</b> from or through an input source <b>206</b>.</p>
<p id="p-0043" num="0042">The resource manager <b>120</b> may communicate with the input source <b>206</b> via an Ethernet-type connection or through a wired protocol, such as IEEE 802.3, etc., or wireless protocols, such as IEEE 802.11b, 802.11g, wireless serial connection, Bluetooth, etc., or combinations thereof. In addition, the input source <b>206</b> may be connected to the resource manager <b>120</b> through an interface <b>208</b> that is coupled to the system bus <b>204</b>. The input source <b>206</b> may, however, be coupled by other conventional interface and bus structures, such as, parallel ports, USB ports, etc.</p>
<p id="p-0044" num="0043">The resource manager <b>120</b> may be connected to a memory <b>210</b> through the system bus <b>204</b>. Alternatively, the resource manager <b>120</b> may be connected to the memory <b>210</b> through a memory bus, as shown in <figref idref="DRAWINGS">FIG. 2</figref>. Generally speaking, the memory <b>210</b> may be configured to provide storage of software, algorithms, and the like, that provide the functionality of the power distribution system <b>202</b>. By way of example, the memory <b>210</b> may store an operating system <b>212</b>, application programs <b>214</b>, program data <b>216</b>, and the like. The memory <b>210</b> may be implemented as a combination of volatile and non-volatile memory, such as DRAM, EEPROM, MRAM, flash memory, and the like. In addition, or alternatively, the memory <b>210</b> may comprise a device configured to read from and write to a removable media, such as, a floppy disk, a CD-ROM, a DVD-ROM, or other optical or magnetic media.</p>
<p id="p-0045" num="0044">The memory <b>210</b> may also store modules programmed to perform various power distribution functions. More particularly, the memory <b>210</b> may store a re-circulation detection module <b>218</b>, a pod formulation module <b>220</b>, a heat re-circulation factor (HRF) calculation module <b>222</b>, a summed re-circulation factor (SRF) calculation module <b>224</b>, and a power distribution module <b>226</b>. The resource manager <b>120</b> may implement one or more of the modules <b>218</b>-<b>226</b> stored in the memory <b>210</b> to perform some or all of the power distribution methods described herein below.</p>
<p id="p-0046" num="0045">The power distribution system <b>202</b> also includes a plurality of server sensors A-N <b>230</b><i>a</i>-<b>230</b><i>n </i>configured to detect one or more conditions in respective servers <b>112</b><i>a</i>-<b>112</b><i>n</i>. As shown, the &#x201c;n&#x201d; denoting the sever sensors A-N <b>230</b><i>a</i>-<b>230</b><i>n </i>and the servers A-N <b>112</b><i>a</i>-<b>112</b><i>n</i>, indicates a non-negative integer. In addition, the ellipses between server sensor <b>230</b><i>b </i>and the server sensor <b>230</b><i>n </i>generally indicate that the power distribution system <b>202</b> may include any reasonably suitable number of sensors. Moreover, the ellipses between the server <b>112</b><i>b </i>and server <b>112</b><i>n </i>generally indicate that the resource manager <b>120</b> may allocate power to any reasonably suitable number of servers <b>112</b>. Data collected by the sensors <b>230</b><i>a</i>-<b>230</b><i>n </i>may be communicated to the resource manager <b>120</b> through the interface <b>208</b>. The interface <b>208</b> may comprise at least one of hardware and software configured to enable such data transfer.</p>
<p id="p-0047" num="0046">Data collected by additional sensors <b>232</b><i>a</i>-<b>232</b><i>n </i>may also be communicated to the resource manager <b>120</b> through the interface <b>208</b>. The additional sensors A-N <b>232</b><i>a</i>-<b>232</b><i>n </i>generally comprise sensors positioned and configured to detect the temperatures of airflow supplied by respective CRAC units <b>110</b>, as shown in <figref idref="DRAWINGS">FIG. 2</figref>. The ellipses between the CRAC B sensor <b>232</b><i>b </i>and the CRAC N sensor <b>232</b><i>n </i>generally denote that the resource manager may receive temperature information from any reasonably suitable number of CRAC sensors <b>232</b><i>a</i>-<b>232</b><i>n. </i></p>
<p id="p-0048" num="0047">The data received from the server sensors <b>230</b><i>a</i>-<b>230</b><i>n </i>and the CRAC sensors <b>232</b><i>a</i>-<b>232</b><i>n </i>may be stored in the memory <b>210</b>. In addition, the stored data may be used in various algorithms described below in determining how power is to be distributed among the servers <b>112</b><i>a</i>-<b>112</b><i>n</i>. In this regard, for instance, the resource manager <b>120</b> may implement the re-circulation detection module <b>218</b> to detect re-circulation based upon the conditions detected by the server sensors <b>230</b><i>a</i>-<b>230</b><i>n </i>and the CRAC sensors <b>232</b><i>a</i>-<b>232</b><i>n. </i></p>
<p id="p-0049" num="0048">The resource manager <b>120</b> may implement the pod formulation module <b>220</b> to bin the servers <b>112</b><i>a</i>-<b>112</b><i>n </i>into pods, where each pod contains s servers. The resource manager <b>120</b> may additionally implement the HRF calculation module <b>222</b> to calculate the heat re-circulation factor (HRF) for each of the pods based upon the detected re-circulation. In addition, the resource manager <b>120</b> may implement the SRF calculation module <b>224</b> to calculate a summed re-circulation factor (SRF). The resource manager <b>120</b> may, moreover, implement the power distribution module <b>226</b>, which comprises a means for allocating workload among a plurality of servers, to determine the power distributions for the pods based upon the calculated SRF and the HRF calculated for the respective pods.</p>
<p id="p-0050" num="0049">Various manners in which the power distributions for the pods may be determined and in certain instances, implemented, are described in greater detail herein below with respect to the <figref idref="DRAWINGS">FIGS. 3 and 4</figref>.</p>
<p id="p-0051" num="0050">With reference first to <figref idref="DRAWINGS">FIG. 3</figref>, there is shown a flow diagram of a method <b>300</b> for distributing power among servers <b>112</b><i>a</i>-<b>112</b><i>n </i>to address causes of heat re-circulation and to substantially maximize power budgets of the servers <b>112</b><i>a</i>-<b>112</b><i>n</i>, according to an example. It is to be understood that the following description of the method <b>300</b> is but one manner of a variety of different manners in which an embodiment of the invention may be practiced. It should also be apparent to those of ordinary skill in the art that the method <b>300</b> represents a generalized illustration and that other steps may be added or existing steps may be removed, modified or rearranged without departing from a scope of the method <b>300</b>.</p>
<p id="p-0052" num="0051">The description of the method <b>300</b> is made with reference to the block diagram <b>200</b> illustrated in <figref idref="DRAWINGS">FIG. 2</figref>, and thus makes reference to the elements cited therein. It should, however, be understood that the method <b>300</b> is not limited to the elements set forth in the block diagram <b>200</b>. Instead, it should be understood that the method <b>300</b> may be practiced by a power distribution system having a different configuration than that set forth in the block diagram <b>200</b>.</p>
<p id="p-0053" num="0052">At step <b>302</b>, the causes of heat re-circulation in a geographically collocated cluster of compute equipment are calibrated. The calibration of the causes of heat re-circulation may include calibration through at least one of modeling and actual measurements of the cluster of compute equipment. In addition, at step <b>304</b>, workload is allocated among the servers to address causes of the heat re-circulation to reduce costs associated with cooling the compute equipment. The workload may be allocated in a number of different manners. For instance, the workload may be allocated through linear sorting, as described herein. As another example, the workload may be allocated according to the discretization approaches disclosed in co-pending and commonly assigned U.S. patent application Ser. No. 11/129,988, now U.S. Pat. No. 7,461,273, filed on even date herewith, and entitled &#x201c;Power Distribution Among Servers&#x201d;, the disclosure of which is hereby incorporated by reference in its entirety.</p>
<p id="p-0054" num="0053">The steps outlined in the method <b>300</b> are described in greater detail herein below with respect to <figref idref="DRAWINGS">FIG. 4</figref>. In addition, <figref idref="DRAWINGS">FIG. 4</figref> describes additional steps that may be performed in conjunction with the steps outlined in the method <b>300</b>.</p>
<p id="p-0055" num="0054">With reference now to <figref idref="DRAWINGS">FIG. 4</figref>, there is shown a flow diagram of a method <b>400</b> for distributing power among servers <b>112</b><i>a</i>-<b>112</b><i>n </i>to address causes of heat re-circulation and to substantially maximize power budgets of the servers <b>112</b><i>a</i>-<b>112</b><i>n</i>. It is to be understood that the following description of the method <b>400</b> is but one manner of a variety of different manners in which an embodiment of the invention may be practiced. It should also be apparent to those of ordinary skill in the art that the method <b>400</b> represents a generalized illustration and that other steps may be added or existing steps may be removed, modified or rearranged without departing from a scope of the method <b>400</b>.</p>
<p id="p-0056" num="0055">The description of the method <b>400</b> is made with reference to the block diagram <b>200</b> illustrated in <figref idref="DRAWINGS">FIG. 2</figref>, and thus makes reference to the elements cited therein. It should, however, be understood that the method <b>400</b> is not limited to the elements set forth in the block diagram <b>200</b>. Instead, it should be understood that the method <b>400</b> may be practiced by a power distribution system having a different configuration than that set forth in the block diagram <b>200</b>.</p>
<p id="p-0057" num="0056">The method <b>400</b> may generally be implemented to substantially minimize the total heat re-circulation in the data center <b>100</b> while substantially maximizing the power budget, and therefore the potential utilization, of the servers <b>112</b><i>a</i>-<b>112</b><i>n</i>. More particularly, the method <b>400</b> may be implemented to determine how power should be distributed among the servers <b>112</b><i>a</i>-<b>112</b><i>n </i>to substantially achieve these goals. In addition, the method <b>400</b> may include allocation of power to the servers <b>112</b><i>a</i>-<b>112</b><i>n </i>based upon the determined distribution scheme.</p>
<p id="p-0058" num="0057">The method <b>400</b> may be initiated, for instance, through receipt of a workload or application request by the resource manager <b>120</b> at step <b>402</b>. In addition or alternatively, the method <b>400</b> may be manually initiated, initiated according to an operating schedule, etc. Once initiated, the resource manager <b>120</b> may run a reference workload that generates a given amount of heat, Q<sub>ref</sub>, at step <b>404</b>. More particularly, the resource manager <b>120</b> may cause one or more of the servers <b>112</b><i>a</i>-<b>112</b><i>n </i>to operate at the reference workload at step <b>404</b>. At step <b>406</b>, the resource manager <b>120</b> may implement the re-circulation detection module <b>218</b> to calculate the amount of heat re-circulation, &#x3b4;Q<sub>ref</sub>, in the data center <b>100</b>. In other words, the re-circulation detection module <b>218</b> comprises a means for calculating heat re-circulation levels. The amount of heat re-circulation, &#x3b4;Q<sub>ref</sub>, may be determined through the following equation:</p>
<p id="p-0059" num="0058">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
  <mrow>
    <mi>Equation</mi>
    <mo>&#x2062;</mo>
    <mstyle>
      <mspace width="1.1em" height="1.1ex"/>
    </mstyle>
    <mo>&#x2062;</mo>
    <mrow>
      <mo>(</mo>
      <mn>2</mn>
      <mo>)</mo>
    </mrow>
    <mo>&#x2062;</mo>
    <mstyle>
      <mtext>:</mtext>
    </mstyle>
  </mrow>
</math>
</maths>
<maths id="MATH-US-00002-2" num="00002.2">
<math overflow="scroll">
  <mrow>
    <mrow>
      <mi>&#x3b4;</mi>
      <mo>&#x2062;</mo>
      <mstyle>
        <mspace width="0.3em" height="0.3ex"/>
      </mstyle>
      <mo>&#x2062;</mo>
      <msub>
        <mi>Q</mi>
        <mi>ref</mi>
      </msub>
    </mrow>
    <mo>=</mo>
    <mrow>
      <munderover>
        <mo>&#x2211;</mo>
        <mrow>
          <mi>i</mi>
          <mo>=</mo>
          <mn>1</mn>
        </mrow>
        <mi>n</mi>
      </munderover>
      <mo>&#x2062;</mo>
      <mstyle>
        <mspace width="0.3em" height="0.3ex"/>
      </mstyle>
      <mo>&#x2062;</mo>
      <mrow>
        <msub>
          <mi>C</mi>
          <mi>p</mi>
        </msub>
        <mo>&#xb7;</mo>
        <msub>
          <mi>m</mi>
          <mi>i</mi>
        </msub>
        <mo>&#xb7;</mo>
        <mrow>
          <mrow>
            <mo>(</mo>
            <mrow>
              <msubsup>
                <mi>T</mi>
                <mi>i</mi>
                <mi>in</mi>
              </msubsup>
              <mo>-</mo>
              <msub>
                <mi>T</mi>
                <mi>sup</mi>
              </msub>
            </mrow>
            <mo>)</mo>
          </mrow>
          <mo>.</mo>
        </mrow>
      </mrow>
    </mrow>
  </mrow>
</math>
</maths>
</p>
<p id="p-0060" num="0059">In Equation (2), n is the number of servers in the data center <b>100</b>, C<sub>p </sub>is the specific heat of air, m<sub>i </sub>is the mass flow of air through server i, which may be in kg/sec, T<sub>i</sub><sup>in </sup>is the inlet temperature for server i, and T<sub>sup </sub>is the temperature of the cooled air supplied by the CRAC units <b>110</b>. The variables in Equation (2) may be detected or calculated in various manners as described hereinabove with respect to <figref idref="DRAWINGS">FIG. 2</figref>. Generally speaking, in a data center having no heat re-circulation, &#x3b4;Q<sub>ref</sub>=0, T<sub>i</sub><sup>in </sup>will equal T<sub>sup </sub>for each server <b>112</b><i>a</i>-<b>112</b><i>n</i>. In any respect, the calculated heat re-circulation &#x3b4;Q<sub>ref </sub>may be stored in the memory <b>210</b>.</p>
<p id="p-0061" num="0060">At step <b>408</b>, the servers <b>112</b><i>a</i>-<b>112</b><i>n </i>may be grouped or binned into pods, where each pod contains s servers <b>112</b><i>a</i>-<b>112</b><i>n</i>. More particularly, for instance, the resource manager <b>120</b> may implement the pod formulation module <b>220</b> to group or bin the servers <b>112</b><i>a</i>-<b>112</b><i>n </i>into the pods. The servers <b>112</b><i>a</i>-<b>112</b><i>n </i>may be grouped into the pods according to their geographic locations. In addition, the servers <b>112</b><i>a</i>-<b>112</b><i>n </i>that are located in relatively close proximities to each other may be grouped into respective pods. The number of servers <b>112</b><i>a</i>-<b>112</b><i>n </i>grouped into the pods may be based upon, for instance, the amount of required to perform the distribution of workload among the servers <b>112</b><i>a</i>-<b>112</b><i>n</i>, the amount of acceptable loss in accuracy pertaining to grouping the servers <b>112</b><i>a</i>-<b>112</b><i>n </i>into pods, etc.</p>
<p id="p-0062" num="0061">At step <b>410</b>, for a first pod j, the central processing unit (CPU) utilizations for the servers <b>112</b><i>a</i>-<b>112</b><i>n </i>in the first pod j are maximized, thereby increasing the overall data center <b>100</b> power consumption and heat re-circulation, &#x3b4;Q<sub>ref</sub>. A period of time is allowed to elapse, at step <b>412</b>, to enable the new power load and the heat distribution to substantially stabilize. The period of time may comprise a set time based upon the magnitude of the power load change. In addition, or alternatively, the period of time elapsed at step <b>412</b> may be equivalent to a period of time during which conditions in the data center <b>100</b> are monitored to determine when the power load and heat distribution have substantially stabilized.</p>
<p id="p-0063" num="0062">In any respect, at step <b>414</b>, the heat re-circulation factor (HRF) for the first pod j may be calculated. More particularly, the resource manager <b>120</b> may implement the HRF calculation module <b>222</b>, which comprises a means for calculating a heat re-circulation factor for a plurality of servers, to calculate the HRF for the first pod j based upon the following equation:</p>
<p id="p-0064" num="0063">
<maths id="MATH-US-00003" num="00003">
<math overflow="scroll">
  <mrow>
    <mi>Equation</mi>
    <mo>&#x2062;</mo>
    <mstyle>
      <mspace width="1.1em" height="1.1ex"/>
    </mstyle>
    <mo>&#x2062;</mo>
    <mrow>
      <mo>(</mo>
      <mn>3</mn>
      <mo>)</mo>
    </mrow>
    <mo>&#x2062;</mo>
    <mstyle>
      <mtext>:</mtext>
    </mstyle>
  </mrow>
</math>
</maths>
<maths id="MATH-US-00003-2" num="00003.2">
<math overflow="scroll">
  <mrow>
    <msub>
      <mi>HRF</mi>
      <mi>j</mi>
    </msub>
    <mo>=</mo>
    <mrow>
      <mfrac>
        <mrow>
          <msub>
            <mi>Q</mi>
            <mi>j</mi>
          </msub>
          <mo>-</mo>
          <msub>
            <mi>Q</mi>
            <mi>ref</mi>
          </msub>
        </mrow>
        <mrow>
          <mrow>
            <mi>&#x3b4;</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <msub>
              <mi>Q</mi>
              <mi>j</mi>
            </msub>
          </mrow>
          <mo>-</mo>
          <mrow>
            <mi>&#x3b4;</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <msub>
              <mi>Q</mi>
              <mi>ref</mi>
            </msub>
          </mrow>
        </mrow>
      </mfrac>
      <mo>=</mo>
      <mrow>
        <mfrac>
          <mrow>
            <mi>&#x394;</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <msub>
              <mi>Q</mi>
              <mi>j</mi>
            </msub>
          </mrow>
          <mrow>
            <mi>&#x394;&#x3b4;</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <msub>
              <mi>Q</mi>
              <mi>j</mi>
            </msub>
          </mrow>
        </mfrac>
        <mo>.</mo>
      </mrow>
    </mrow>
  </mrow>
</math>
</maths>
</p>
<p id="p-0065" num="0064">In Equation (3), Q<sub>j </sub>is the new amount of heat generated with the CPU utilizations of the servers <b>112</b><i>a</i>-<b>112</b><i>n </i>in the first pod j maximized and &#x3b4;Q<sub>j </sub>is the amount of heat re-circulation with the CPU utilizations of the servers <b>112</b><i>a</i>-<b>112</b><i>n </i>in the first pod j maximized. The HRF for the first pod may therefore be defined as a ratio of the difference in heat generated by the pod j and all of the servers <b>112</b><i>a</i>-<b>112</b><i>n </i>over the difference in the heat re-circulation of the pod j and all of the servers <b>112</b><i>a</i>-<b>112</b><i>n</i>. The HRF for the first pod j may be stored in the memory <b>210</b>. In addition, at step <b>416</b>, the CPU utilizations of the servers <b>112</b><i>a</i>-<b>112</b><i>n </i>in the first pod j may be reduced to the reference workload level set at step <b>404</b>.</p>
<p id="p-0066" num="0065">At step <b>418</b>, it may be determined as to whether the HRF of another pod is to be calculated. If a &#x201c;yes&#x201d; condition is reached, the CPU utilizations for the servers <b>112</b><i>a</i>-<b>112</b><i>n </i>in the another or next pod, j+1, may be maximized at step <b>420</b>. As stated above with respect to step <b>410</b>, the increased CPU utilizations increases the overall data center <b>100</b> power consumption and heat re-circulation, &#x3b4;Q<sub>ref</sub>. A period of time is allowed to elapse, at step <b>422</b>, to enable the new power load and the heat distribution to substantially stabilize, as described above with respect to step <b>412</b>.</p>
<p id="p-0067" num="0066">At step <b>424</b>, the HRF for the next pod, j+1, may be calculated in manners as described above with respect to step <b>414</b>. The HRF for the next pod, j+1, may also be stored in the memory <b>210</b>. In addition, at step <b>426</b>, the CPU utilizations of the servers <b>112</b><i>a</i>-<b>112</b><i>n </i>in the next pod j+1 may be reduced to the reference workload level set at step <b>404</b>.</p>
<p id="p-0068" num="0067">Following step <b>426</b>, it may again be determined whether the HRF for another pod is to be calculated at step <b>418</b>. If a &#x201c;yes&#x201d; condition is reached, steps <b>420</b>-<b>426</b> may be repeated for the another pod to thereby calculate the HRF for the another pod. In addition, steps <b>420</b>-<b>426</b> may be repeated to calculate the HRF's of the remaining pods.</p>
<p id="p-0069" num="0068">In any event, if a &#x201c;no&#x201d; condition is reached at step <b>418</b>, a summed re-circulation factor (SRF) may be calculated at step <b>428</b>. More particularly, the resource manager <b>120</b> may implement the SRF calculation module <b>224</b> to calculate the SRF based upon the following equation:</p>
<p id="p-0070" num="0069">
<maths id="MATH-US-00004" num="00004">
<math overflow="scroll">
  <mrow>
    <mi>Equation</mi>
    <mo>&#x2062;</mo>
    <mstyle>
      <mspace width="1.1em" height="1.1ex"/>
    </mstyle>
    <mo>&#x2062;</mo>
    <mrow>
      <mo>(</mo>
      <mn>4</mn>
      <mo>)</mo>
    </mrow>
    <mo>&#x2062;</mo>
    <mstyle>
      <mtext>:</mtext>
    </mstyle>
  </mrow>
</math>
</maths>
<maths id="MATH-US-00004-2" num="00004.2">
<math overflow="scroll">
  <mrow>
    <mi>SRF</mi>
    <mo>=</mo>
    <mrow>
      <munderover>
        <mo>&#x2211;</mo>
        <mrow>
          <mi>j</mi>
          <mo>=</mo>
          <mn>1</mn>
        </mrow>
        <mfrac>
          <mi>n</mi>
          <mi>s</mi>
        </mfrac>
      </munderover>
      <mo>&#x2062;</mo>
      <mstyle>
        <mspace width="0.3em" height="0.3ex"/>
      </mstyle>
      <mo>&#x2062;</mo>
      <mrow>
        <msub>
          <mi>HRF</mi>
          <mi>j</mi>
        </msub>
        <mo>.</mo>
      </mrow>
    </mrow>
  </mrow>
</math>
</maths>
<br/>
In Equation (4), n is the number of servers in the data center <b>100</b> and s is the number of servers in a pod. As shown by Equation (4), the SRF is the sum of the HRF's for each of the pods.
</p>
<p id="p-0071" num="0070">At step <b>430</b>, power distribution allocations for the pods may be calculated. More specifically, the power distribution allocation for a particular pod may be determined by multiplying the total power load by that pod's HRF by the SRF.</p>
<p id="p-0072" num="0071">At step <b>432</b>, power may be distributed to the servers <b>112</b><i>a</i>-<b>112</b><i>n </i>contained in the pods according to the power distribution allocated to the pods at step <b>430</b>. In one respect, the power may be substantially evenly distributed among the servers <b>112</b><i>a</i>-<b>112</b><i>n </i>in the respective pods, such that, the sum of the power distributed to the servers <b>112</b><i>a</i>-<b>112</b><i>n </i>of a particular pod equal the power distribution allocated to that pod. In addition or alternatively, power may be distributed to further reduce inefficiencies in cooling the servers <b>112</b><i>a</i>-<b>112</b><i>n</i>. For instance, power may be distributed in one or more of the manners disclosed and described in co-pending and commonly assigned U.S. patent application Ser. No. 11/129,988, now U.S. Pat. No. 7,461,273, filed on even date herewith, and entitled &#x201c;Power Distribution Among Servers&#x201d;, the disclosure of which is hereby incorporated by reference in its entirety.</p>
<p id="p-0073" num="0072">Thus, by way of example, the method <b>400</b> may determine various geographic locations, and more particularly, sets of servers <b>112</b><i>a</i>-<b>112</b><i>n</i>, at which various levels of power are to be distributed to substantially reduce heat re-circulation. In addition, the methods described in the co-pending application Ser. No. 11/129,988 may be used to further refine workload placement among the sets of servers <b>112</b><i>a</i>-<b>112</b><i>n </i>and thus further improve cooling efficiency.</p>
<p id="p-0074" num="0073">At step <b>434</b>, it may be determined as to whether the method <b>400</b> is to continue. The method <b>400</b> may be continued for a predetermined period of time, a predetermined number of iterations, substantially indefinitely, etc. If it is determined that the method <b>400</b> is to continue, steps <b>404</b>-<b>434</b> may be repeated until it is determined that the method <b>400</b> is to discontinue. In this case, the method <b>400</b> may be discontinued once the period of time has elapsed, the number of iterations has been performed, manually discontinued, etc. If it is determined that the method <b>400</b> is to be discontinued, the method <b>400</b> may end as indicated at step <b>436</b>.</p>
<p id="p-0075" num="0074">Through implementation of the method <b>400</b>, the heat re-circulation from each pod will be identical because of the amount of workload placed on each of the pods. As such, the power budget for each pod may substantially be maximized. That is, the number of pods with sufficient power to run a workload may substantially be maximized while substantially minimizing the total heat re-circulation in the data center <b>100</b> by effectively addressing the causes of the heat re-circulation in the data center <b>100</b>.</p>
<p id="p-0076" num="0075">The operations set forth in the methods <b>300</b> and <b>400</b> may be contained as a utility, program, or subprogram, in any desired computer accessible medium. In addition, the methods <b>300</b> and <b>400</b> may be embodied by a computer program, which can exist in a variety of forms both active and inactive. For example, it can exist as software program(s) comprised of program instructions in source code, object code, executable code or other formats. Any of the above can be embodied on a computer readable medium, which include storage devices and signals, in compressed or uncompressed form.</p>
<p id="p-0077" num="0076">Exemplary computer readable storage devices include conventional computer system RAM, ROM, EPROM, EEPROM, and magnetic or optical disks or tapes. Exemplary computer readable signals, whether modulated using a carrier or not, are signals that a computer system hosting or running the computer program can be configured to access, including signals downloaded through the Internet or other networks. Concrete examples of the foregoing include distribution of the programs on a CD ROM or via Internet download. In a sense, the Internet itself, as an abstract entity, is a computer readable medium. The same is true of computer networks in general. It is therefore to be understood that any electronic device capable of executing the above-described functions may perform those functions enumerated above.</p>
<p id="p-0078" num="0077"><figref idref="DRAWINGS">FIG. 5</figref> illustrates a computer system <b>500</b>, which may be employed to perform the various functions of the resource manager <b>120</b> described hereinabove, according to an embodiment. In this respect, the computer system <b>500</b> may be used as a platform for executing one or more of the functions described hereinabove with respect to the resource manager <b>120</b>.</p>
<p id="p-0079" num="0078">The computer system <b>500</b> includes one or more controllers, such as a processor <b>502</b>. The processor <b>502</b> may be used to execute some or all of the steps described in the methods <b>300</b> and <b>400</b>. Commands and data from the processor <b>502</b> are communicated over a communication bus <b>504</b>. The computer system <b>500</b> also includes a main memory <b>506</b>, such as a random access memory (RAM), where the program code for, for instance, the resource manager <b>120</b>, may be executed during runtime, and a secondary memory <b>508</b>. The secondary memory <b>508</b> includes, for example, one or more hard disk drives <b>510</b> and/or a removable storage drive <b>512</b>, representing a floppy diskette drive, a magnetic tape drive, a compact disk drive, etc., where a copy of the program code for the power distribution system may be stored.</p>
<p id="p-0080" num="0079">The removable storage drive <b>510</b> reads from and/or writes to a removable storage unit <b>514</b> in a well-known manner. User input and output devices may include a keyboard <b>516</b>, a mouse <b>518</b>, and a display <b>520</b>. A display adaptor <b>522</b> may interface with the communication bus <b>504</b> and the display <b>520</b> and may receive display data from the processor <b>502</b> and convert the display data into display commands for the display <b>520</b>. In addition, the processor <b>502</b> may communicate over a network, for instance, the Internet, LAN, etc., through a network adaptor <b>524</b>.</p>
<p id="p-0081" num="0080">It will be apparent to one of ordinary skill in the art that other known electronic components may be added or substituted in the computer system <b>500</b>. In addition, the computer system <b>500</b> may include a system board or blade used in a rack in a data center, a conventional &#x201c;white box&#x201d; server or computing device, etc. Also, one or more of the components in <figref idref="DRAWINGS">FIG. 5</figref> may be optional (for instance, user input devices, secondary memory, etc.).</p>
<p id="p-0082" num="0081">What has been described and illustrated herein are embodiments of the invention along with some of their variations. The terms, descriptions and figures used herein are set forth by way of illustration only and are not meant as limitations. Those skilled in the art will recognize that many variations are possible within the spirit and scope of the invention, which is intended to be defined by the following claims&#x2014;and their equivalents&#x2014;in which all terms are meant in their broadest reasonable sense unless otherwise indicated.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001 MATH-US-00001-2" nb-file="US08626918-20140107-M00001.NB">
<img id="EMI-M00001" he="10.92mm" wi="76.20mm" file="US08626918-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002 MATH-US-00002-2" nb-file="US08626918-20140107-M00002.NB">
<img id="EMI-M00002" he="12.70mm" wi="76.20mm" file="US08626918-20140107-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00003 MATH-US-00003-2" nb-file="US08626918-20140107-M00003.NB">
<img id="EMI-M00003" he="10.92mm" wi="76.20mm" file="US08626918-20140107-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00004 MATH-US-00004-2" nb-file="US08626918-20140107-M00004.NB">
<img id="EMI-M00004" he="14.82mm" wi="76.20mm" file="US08626918-20140107-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00005" nb-file="US08626918-20140107-M00005.NB">
<img id="EMI-M00005" he="8.47mm" wi="76.20mm" file="US08626918-20140107-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00006" nb-file="US08626918-20140107-M00006.NB">
<img id="EMI-M00006" he="7.03mm" wi="76.20mm" file="US08626918-20140107-M00006.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00007" nb-file="US08626918-20140107-M00007.NB">
<img id="EMI-M00007" he="7.03mm" wi="76.20mm" file="US08626918-20140107-M00007.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00008" nb-file="US08626918-20140107-M00008.NB">
<img id="EMI-M00008" he="7.03mm" wi="76.20mm" file="US08626918-20140107-M00008.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00009" nb-file="US08626918-20140107-M00009.NB">
<img id="EMI-M00009" he="8.47mm" wi="76.20mm" file="US08626918-20140107-M00009.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00010" nb-file="US08626918-20140107-M00010.NB">
<img id="EMI-M00010" he="7.03mm" wi="76.20mm" file="US08626918-20140107-M00010.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00011" nb-file="US08626918-20140107-M00011.NB">
<img id="EMI-M00011" he="10.58mm" wi="76.20mm" file="US08626918-20140107-M00011.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method of allocating workload among servers in a geographically collocated cluster of compute equipment, said method comprising steps performed by a computer system of:
<claim-text>calculating a first heat re-circulation level resulting from a first server utilization level of the compute equipment;</claim-text>
<claim-text>calculating a second heat re-circulation level resulting from a second server utilization level of the compute equipment, wherein the first and second heat re-circulation levels are measures of an infiltration of heated airflow into cooled airflow prior to the heated and cooled airflow being supplied into one or more of the servers;</claim-text>
<claim-text>calculating a heat re-circulation factor (HRF) for the cluster of compute equipment from the first heat re-circulation level and the second heat re-circulation level, wherein the HRF is a ratio of the difference in heat generated at the first server utilization level and at the second server utilization level over the difference in the heat re-circulation generated at the first server utilization level and at the second server utilization level; and</claim-text>
<claim-text>allocating workload among the servers based upon the calculated heat re-circulation factor.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step of calculating the first heat re-circulation levels further comprises calculating the first and second heat re-circulation levels, &#x3b4;Q<sub>ref</sub>, through the following equation:</claim-text>
<claim-text>
<maths id="MATH-US-00005" num="00005">
<math overflow="scroll">
<mrow>
  <mrow>
    <mrow>
      <mi>&#x3b4;</mi>
      <mo>&#x2062;</mo>
      <mstyle>
        <mspace width="0.3em" height="0.3ex"/>
      </mstyle>
      <mo>&#x2062;</mo>
      <msub>
        <mi>Q</mi>
        <mi>ref</mi>
      </msub>
    </mrow>
    <mo>=</mo>
    <mrow>
      <mrow>
        <munderover>
          <mo>&#x2211;</mo>
          <mrow>
            <mi>i</mi>
            <mo>-</mo>
            <mn>1</mn>
          </mrow>
          <mi>n</mi>
        </munderover>
        <mo>&#x2062;</mo>
        <msub>
          <mi>c</mi>
          <mi>p</mi>
        </msub>
      </mrow>
      <mo>-</mo>
      <msub>
        <mi>m</mi>
        <mi>i</mi>
      </msub>
      <mo>-</mo>
      <mrow>
        <mo>(</mo>
        <mrow>
          <msubsup>
            <mi>T</mi>
            <mi>i</mi>
            <mi>in</mi>
          </msubsup>
          <mo>-</mo>
          <msub>
            <mi>T</mi>
            <mi>sup</mi>
          </msub>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mrow>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
</maths>
</claim-text>
<claim-text>wherein n is the number of servers in the cluster of compute equipment, C<sub>p </sub>is the specific heat of air, m<sub>i </sub>is the mass flow of air through server i, T<sub>i</sub><sup>in </sup>is the inlet temperature for server i, and T<sub>sup </sub>is the temperature of the cooled air supplied by at least one air conditioning unit.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>running a reference workload on the servers prior to the step of calculating the heat re-circulation level in the cluster of compute equipment at the first server utilization level, wherein the reference workload is correlated to the first server utilization level.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>grouping the servers into a plurality of pods, wherein each of the pods contains at least one server, and wherein each server comprises at least one central processing unit (CPU);</claim-text>
<claim-text>maximizing CPU utilizations of the servers grouped into a first pod; and</claim-text>
<claim-text>wherein the step of calculating the heat re-circulation level in the cluster of compute equipment at the second server utilization level further comprises calculating the heat re-circulation level in the cluster of compute equipment with the CPU utilizations of the servers grouped into the first pod maximized.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the step of grouping the servers further comprises grouping the servers according to a trade-off between the amount of time required to allocate workload among the servers and the amount of accuracy loss related to the grouping of the servers.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, further comprising:
<claim-text>calculating a heat re-circulation factor (HRF) for the first pod, wherein the HRF is characterized by the following equation:</claim-text>
</claim-text>
<claim-text>
<maths id="MATH-US-00006" num="00006">
<math overflow="scroll">
<mrow>
  <mrow>
    <msub>
      <mi>HRF</mi>
      <mi>j</mi>
    </msub>
    <mo>=</mo>
    <mrow>
      <mfrac>
        <mrow>
          <msub>
            <mi>Q</mi>
            <mi>j</mi>
          </msub>
          <mo>-</mo>
          <msub>
            <mi>Q</mi>
            <mi>ref</mi>
          </msub>
        </mrow>
        <mrow>
          <mrow>
            <mi>&#x3b4;</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <msub>
              <mi>Q</mi>
              <mi>j</mi>
            </msub>
          </mrow>
          <mo>-</mo>
          <mrow>
            <mi>&#x3b4;</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <msub>
              <mi>Q</mi>
              <mi>ref</mi>
            </msub>
          </mrow>
        </mrow>
      </mfrac>
      <mo>=</mo>
      <mfrac>
        <mrow>
          <mi>&#x394;</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <msub>
            <mi>Q</mi>
            <mi>j</mi>
          </msub>
        </mrow>
        <mrow>
          <mi>&#x394;&#x3b4;</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <msub>
            <mi>Q</mi>
            <mi>j</mi>
          </msub>
        </mrow>
      </mfrac>
    </mrow>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
</maths>
</claim-text>
<claim-text>wherein j is the pod identification, Q<sub>j </sub>is the amount of heat generated at the second server utilization level, Q<sub>ref</sub>, is the amount of heat generated at the first server utilization level, &#x3b4;Q<sub>ref </sub>is the amount of heat re-circulation generated at the second server utilization level, and &#x3b4;Q<sub>ref </sub>is the amount of heat re-circulation generated at the first server utilization level.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, further comprising:
<claim-text>reducing CPU utilizations of the servers grouped into the first pod to the first server utilization level;</claim-text>
<claim-text>maximizing CPU utilizations of the servers grouped into a second pod, thereby creating a third server utilization level;</claim-text>
<claim-text>calculating a third heat re-circulation level in the cluster of compute equipment at the third server utilization level; and</claim-text>
<claim-text>wherein the step of allocating workload among the servers further comprises allocating workload among the servers based upon the first, second, and third heat re-circulation levels.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising:
<claim-text>calculating a heat re-circulation factor (HRF) for the second pod, wherein the HRF is characterized by the following equation:</claim-text>
</claim-text>
<claim-text>
<maths id="MATH-US-00007" num="00007">
<math overflow="scroll">
<mrow>
  <mrow>
    <msub>
      <mi>HRF</mi>
      <mi>j</mi>
    </msub>
    <mo>=</mo>
    <mrow>
      <mfrac>
        <mrow>
          <msub>
            <mi>Q</mi>
            <mi>j</mi>
          </msub>
          <mo>-</mo>
          <msub>
            <mi>Q</mi>
            <mi>ref</mi>
          </msub>
        </mrow>
        <mrow>
          <mrow>
            <mi>&#x3b4;</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <msub>
              <mi>Q</mi>
              <mi>j</mi>
            </msub>
          </mrow>
          <mo>-</mo>
          <mrow>
            <mi>&#x3b4;</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <msub>
              <mi>Q</mi>
              <mi>ref</mi>
            </msub>
          </mrow>
        </mrow>
      </mfrac>
      <mo>=</mo>
      <mfrac>
        <mrow>
          <mi>&#x394;</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <msub>
            <mi>Q</mi>
            <mi>j</mi>
          </msub>
        </mrow>
        <mrow>
          <mi>&#x394;</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mi>&#x3b4;</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <msub>
            <mi>Q</mi>
            <mi>j</mi>
          </msub>
        </mrow>
      </mfrac>
    </mrow>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
</maths>
</claim-text>
<claim-text>wherein j is the pod identification, Q<sub>j </sub>is the amount of heat generated at the second server utilization level, Q<sub>ref</sub>, is the amount of heat generated at the first server utilization level, &#x3b4;Q<sub>j </sub>is the amount of heat re-circulation generated at the second server utilization level, and &#x3b4;Q<sub>ref </sub>is the amount of heat re-circulation generated at the first server utilization level.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, further comprising:
<claim-text>sequentially maximizing CPU utilizations of the servers grouped into the remaining pods;</claim-text>
<claim-text>sequentially measuring heat re-circulation at different server utilization levels according to which of the CPU utilizations of the remaining pods are maximized; and</claim-text>
<claim-text>sequentially reducing the CPU utilizations of the servers following measuring of the heat re-circulation at the associated server utilization levels.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising:
<claim-text>calculating a heat re-circulation factor (HRF) for each of the pods, wherein the HRF is characterized by the following equation:</claim-text>
</claim-text>
<claim-text>
<maths id="MATH-US-00008" num="00008">
<math overflow="scroll">
<mrow>
  <mrow>
    <msub>
      <mi>HRF</mi>
      <mi>j</mi>
    </msub>
    <mo>=</mo>
    <mrow>
      <mfrac>
        <mrow>
          <msub>
            <mi>Q</mi>
            <mi>j</mi>
          </msub>
          <mo>-</mo>
          <msub>
            <mi>Q</mi>
            <mi>ref</mi>
          </msub>
        </mrow>
        <mrow>
          <mrow>
            <mi>&#x3b4;</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <msub>
              <mi>Q</mi>
              <mi>j</mi>
            </msub>
          </mrow>
          <mo>-</mo>
          <mrow>
            <mi>&#x3b4;</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <msub>
              <mi>Q</mi>
              <mi>ref</mi>
            </msub>
          </mrow>
        </mrow>
      </mfrac>
      <mo>=</mo>
      <mfrac>
        <mrow>
          <mi>&#x394;</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <msub>
            <mi>Q</mi>
            <mi>j</mi>
          </msub>
        </mrow>
        <mrow>
          <mi>&#x394;</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mi>&#x3b4;</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <msub>
            <mi>Q</mi>
            <mi>j</mi>
          </msub>
        </mrow>
      </mfrac>
    </mrow>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
</maths>
</claim-text>
<claim-text>wherein j is the pod identification, Q<sub>j </sub>is the amount of heat generated at the second server utilization level, Q<sub>ref</sub>, is the amount of heat generated at the first server utilization level, &#x3b4;Q<sub>j </sub>is the amount of heat re-circulation generated at the second server utilization level, and &#x3b4;Q<sub>ref </sub>is the amount of heat re-circulation generated at the first server utilization level;
<claim-text>summing the HRF's of each of the pods; and</claim-text>
<claim-text>wherein the step of allocating workload among the servers further comprises allocating workload among the servers contained in pods maximize CPU utilizations of the servers contained in the pods.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step of allocating the workload further comprises allocating the workload based upon linear sorting.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step of allocating the workload further comprises allocating the workload based upon discretization approaches.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. A computing device for allocating power among a plurality of servers, said computing device comprising:
<claim-text>a re-circulation detection module for calculating a first heat re-circulation level resulting from a first server utilization level of the compute equipment and a second heat re-circulation level resulting from a second server utilization level of the compute equipment, wherein the first and second heat re-circulation levels are measures of an infiltration of heated airflow into cooled airflow prior to the heated and cooled airflow being supplied into one or more of the servers;</claim-text>
<claim-text>a heat re-circulation factor (HRF) calculation module for calculating a HRF for the plurality of servers based upon on the first and second heat re-circulation levels, wherein the HRF is a ratio of the difference in heat generated at the first server utilization level and at the second server utilization level over the difference in the heat re-circulation generated at the first server utilization level and at the second server utilization level; and</claim-text>
<claim-text>a resource manager configured to allocate workload among the plurality of servers based upon the heat re-circulation factor.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The computing device according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, said system further comprising:
<claim-text>server temperature sensors con figured to detect the temperatures of airflow at respective inlets of the plurality of servers;</claim-text>
<claim-text>a CRAC unit temperature sensor configured detect the temperature of airflow supplied by the CRAC unit; and</claim-text>
<claim-text>wherein the resource manager is further configured to receive the detected server inlet temperatures and the CRAC unit supply temperature, said resource manager being configured to calculate the first and second heat re-circulation levels based upon the detected server inlet temperatures and the CRAC unit supply temperature.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The computing device according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein said plurality of servers each include at least one central processing unit (CPU), and wherein the resource manager is further configured to vary the CPU utilizations of the plurality of servers and to calculate heat re-circulation levels at the various CPU utilizations, and wherein said resource manager is further configured to allocate workload among the plurality of servers based upon the heat re-circulation levels calculated at the various CPU utilizations, to thereby reduce heat re-circulation.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The computing device according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the resource manager is further configured to solve the following equation to calculate the first and second heat re-circulation levels, &#x3b4;Q<sub>ref</sub>:</claim-text>
<claim-text>
<maths id="MATH-US-00009" num="00009">
<math overflow="scroll">
<mrow>
  <mrow>
    <mrow>
      <mi>&#x3b4;</mi>
      <mo>&#x2062;</mo>
      <mstyle>
        <mspace width="0.3em" height="0.3ex"/>
      </mstyle>
      <mo>&#x2062;</mo>
      <msub>
        <mi>Q</mi>
        <mi>ref</mi>
      </msub>
    </mrow>
    <mo>=</mo>
    <mrow>
      <mrow>
        <munderover>
          <mo>&#x2211;</mo>
          <mrow>
            <mi>i</mi>
            <mo>-</mo>
            <mn>1</mn>
          </mrow>
          <mi>n</mi>
        </munderover>
        <mo>&#x2062;</mo>
        <msub>
          <mi>c</mi>
          <mi>p</mi>
        </msub>
      </mrow>
      <mo>-</mo>
      <msub>
        <mi>m</mi>
        <mi>i</mi>
      </msub>
      <mo>-</mo>
      <mrow>
        <mo>(</mo>
        <mrow>
          <msubsup>
            <mi>T</mi>
            <mi>i</mi>
            <mi>in</mi>
          </msubsup>
          <mo>-</mo>
          <msub>
            <mi>T</mi>
            <mi>sup</mi>
          </msub>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mrow>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
</maths>
</claim-text>
<claim-text>wherein n is the number of servers, C<sub>p </sub>is the specific heat of air, m<sub>i </sub>is the mass flow of air through server i, T<sub>i</sub><sup>in </sup>is the inlet temperature for server i, and T<sub>sup </sub>is the temperature of the cooled air supplied by the CRAC unit.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The computing device according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the resource manager is further configured to group the plurality of servers into a number of pods, wherein each of the pods contains at least one server, and wherein the resource manager is further configured to maximize the CPU utilizations of the servers grouped into a first pod to thereby calculate the heat re-circulation caused by the maximized CPU utilizations.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The computing device according to <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the resource manager is further configured to sequentially maximize the CPU utilizations of the servers grouped into the remaining pods to thereby calculate the heat re-circulation caused by the maximized CPU utilizations for the remaining pods.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The computing device according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the heat re-circulation factor calculation module is further configured to calculate a heat re-circulation factor (HRF) for each of the pods, wherein the heat re-circulation factor calculation module is configured to calculate the HRF for each of the pods through the following equation:</claim-text>
<claim-text>
<maths id="MATH-US-00010" num="00010">
<math overflow="scroll">
<mrow>
  <mrow>
    <msub>
      <mi>HRF</mi>
      <mi>j</mi>
    </msub>
    <mo>=</mo>
    <mrow>
      <mfrac>
        <mrow>
          <msub>
            <mi>Q</mi>
            <mi>j</mi>
          </msub>
          <mo>-</mo>
          <msub>
            <mi>Q</mi>
            <mi>ref</mi>
          </msub>
        </mrow>
        <mrow>
          <mrow>
            <mi>&#x3b4;</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <msub>
              <mi>Q</mi>
              <mi>j</mi>
            </msub>
          </mrow>
          <mo>-</mo>
          <mrow>
            <mi>&#x3b4;</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <msub>
              <mi>Q</mi>
              <mi>ref</mi>
            </msub>
          </mrow>
        </mrow>
      </mfrac>
      <mo>=</mo>
      <mfrac>
        <mrow>
          <mi>&#x394;</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <msub>
            <mi>Q</mi>
            <mi>j</mi>
          </msub>
        </mrow>
        <mrow>
          <mi>&#x394;</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mi>&#x3b4;</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <msub>
            <mi>Q</mi>
            <mi>j</mi>
          </msub>
        </mrow>
      </mfrac>
    </mrow>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
</maths>
</claim-text>
<claim-text>wherein j is the pod identification, Q<sub>j </sub>is the amount of heat generated at a second CPU utilization level, Qref, is the amount of heat generated at a first CPU utilization level, &#x3b4;Q<sub>j </sub>is the amount of heat re-circulation generated at the second CPU utilization level, and &#x3b4;Q<sub>ref </sub>is the amount of heat re-circulation generated at the first CPU utilization level.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The computing device according to <claim-ref idref="CLM-00019">claim 19</claim-ref>, further comprising:
<claim-text>a summed re-circulation factor calculation module configured to sum the HRF's of each of the pods through the following equation:</claim-text>
</claim-text>
<claim-text>
<maths id="MATH-US-00011" num="00011">
<math overflow="scroll">
<mrow>
  <mrow>
    <mi>SRF</mi>
    <mo>=</mo>
    <mrow>
      <munderover>
        <mo>&#x2211;</mo>
        <mrow>
          <mi>j</mi>
          <mo>-</mo>
          <mn>1</mn>
        </mrow>
        <mfrac>
          <mi>n</mi>
          <mi>s</mi>
        </mfrac>
      </munderover>
      <mo>&#x2062;</mo>
      <msub>
        <mi>HRF</mi>
        <mi>j</mi>
      </msub>
    </mrow>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
</maths>
</claim-text>
<claim-text>wherein SRF is the summed re-circulation factor for the pods j, n is the number of servers, and s is the number of servers in a pod; and
<claim-text>wherein the resource manager is further configured to allocate workload to the plurality of servers according to their respective pods based upon a correlation between the HRF of the pod and the SRF.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The computing device according to <claim-ref idref="CLM-00020">claim 20</claim-ref>, wherein the resource manager is further configured to allocate power to the pods according to a multiplication of the total power by the HRF of the pod by the SRF.</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. A geographically collocated cluster of compute equipment having a system for allocating workload among a plurality of servers, each having at least one central processing unit (CPU), said data center comprising:
<claim-text>means for calculating heat re-circulation levels in the cluster of compute equipment at different utilization levels of the plurality of servers, wherein the heat re-circulation levels are measures of an infiltration of heated airflow into cooled airflow prior to the heated and cooled airflow being supplied into one or more of the plurality of servers;</claim-text>
<claim-text>means for calculating a heat re-circulation factor for the plurality of servers based upon the calculated heat re-circulation levels, wherein the heat re-circulation factor is a ratio of the difference in heat generated at the first server utilization level and at a second server utilization level over the difference in the heat re-circulation generated at the first server utilization level and at the second server utilization level; and</claim-text>
<claim-text>means for allocating workload among the plurality of servers based upon the calculated heat re-circulation factor.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. A non-transitory computer readable storage medium on which is embedded one or more computer programs, said one or more computer programs implementing a method of allocating workload among servers in a cluster of compute equipment, said one or more computer programs comprising a set of instructions for:
<claim-text>calculating a first heat re-circulation level resulting from a first server utilization level of the compute equipment;</claim-text>
<claim-text>calculating a second heat re-circulation level resulting from a second server utilization level of the compute equipment, wherein the first and second heat re-circulation levels are measures of an infiltration of heated airflow into cooled airflow prior to the heated and cooled airflow being supplied into one or more of the servers;</claim-text>
<claim-text>calculating a heat re-circulation factor (HRF) for the cluster of compute equipment from the first heat re-circulation level and the second heat re-circulation level, wherein the HRF is a ratio of the difference in heat generated at the first server utilization level and at the second server utilization level over the difference in the heat re-circulation generated at the first server utilization level and at the second server utilization level; and</claim-text>
<claim-text>allocating workload among the servers based upon the calculated heat re-circulation factor.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
