<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08624959-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08624959</doc-number>
<kind>B1</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12558467</doc-number>
<date>20090911</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<us-term-of-grant>
<us-term-extension>700</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>13</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>348 42</main-classification>
<further-classification>348156</further-classification>
</classification-national>
<invention-title id="d2e53">Stereo video movies</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5612735</doc-number>
<kind>A</kind>
<name>Haskell et al.</name>
<date>19970300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348 43</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6215516</doc-number>
<kind>B1</kind>
<name>Ma et al.</name>
<date>20010400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348 43</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>7180536</doc-number>
<kind>B2</kind>
<name>Wolowelsky et al.</name>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348 42</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>8063930</doc-number>
<kind>B2</kind>
<name>Rotem et al.</name>
<date>20111100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348 42</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>8330802</doc-number>
<kind>B2</kind>
<name>Koppal et al.</name>
<date>20121200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348 46</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>8384762</doc-number>
<kind>B2</kind>
<name>Markham et al.</name>
<date>20130200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348 42</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>8384763</doc-number>
<kind>B2</kind>
<name>Tam et al.</name>
<date>20130200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348 43</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>8488868</doc-number>
<kind>B2</kind>
<name>Tam et al.</name>
<date>20130700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382154</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>8508580</doc-number>
<kind>B2</kind>
<name>Mcnamer et al.</name>
<date>20130800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348 43</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>8514225</doc-number>
<kind>B2</kind>
<name>Genova</name>
<date>20130800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345422</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2002/0191841</doc-number>
<kind>A1</kind>
<name>Harman</name>
<date>20021200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382154</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2003/0080923</doc-number>
<kind>A1</kind>
<name>Suyama et al.</name>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345  6</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2004/0032980</doc-number>
<kind>A1</kind>
<name>Harman</name>
<date>20040200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382154</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2004/0101043</doc-number>
<kind>A1</kind>
<name>Flack et al.</name>
<date>20040500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524001</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2004/0212612</doc-number>
<kind>A1</kind>
<name>Epstein et al.</name>
<date>20041000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345419</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2004/0240056</doc-number>
<kind>A1</kind>
<name>Tomisawa et al.</name>
<date>20041200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>359462</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>2007/0024614</doc-number>
<kind>A1</kind>
<name>Tam et al.</name>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345419</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>2007/0291233</doc-number>
<kind>A1</kind>
<name>Culbertson et al.</name>
<date>20071200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>353 34</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>2008/0253685</doc-number>
<kind>A1</kind>
<name>Kuranov et al.</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382284</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>2008/0303813</doc-number>
<kind>A1</kind>
<name>Joung et al.</name>
<date>20081200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345419</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>2009/0010323</doc-number>
<kind>A1</kind>
<name>Su et al.</name>
<date>20090100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524001</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>2009/0322857</doc-number>
<kind>A1</kind>
<name>Jacobs et al.</name>
<date>20091200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348 42</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>2010/0074594</doc-number>
<kind>A1</kind>
<name>Nakamura et al.</name>
<date>20100300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>386 92</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>2010/0309990</doc-number>
<kind>A1</kind>
<name>Schoenblum</name>
<date>20101200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524029</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>2011/0239252</doc-number>
<kind>A1</kind>
<name>Kazama et al.</name>
<date>20110900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>725 41</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>2011/0255775</doc-number>
<kind>A1</kind>
<name>McNamer et al.</name>
<date>20111000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>2012/0050490</doc-number>
<kind>A1</kind>
<name>Chen et al.</name>
<date>20120300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348 47</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>2013/0009952</doc-number>
<kind>A1</kind>
<name>Tam et al.</name>
<date>20130100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345419</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>EP</country>
<doc-number>0307283</doc-number>
<kind>A1</kind>
<date>19890300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>GB</country>
<doc-number>2180719</doc-number>
<kind>A</kind>
<date>19870400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>WO</country>
<doc-number>W003032649</doc-number>
<kind>A1</kind>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>WO</country>
<doc-number>WO2010032058</doc-number>
<kind>A1</kind>
<date>20100300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00033">
<othercit>Gruen, &#x201c;Adaptive Least Squares Correlation: a Powerful Image Matching Technique&#x201d;, SAfr J of Photogrammetry, Remote Sensing and Cartography 14 (3), 1985, pp. 175-187.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00034">
<othercit>McAllister, &#x201c;Display Technology: Stereo &#x26; 3D Display Technologies&#x201d;, pp. 1-50, retrieved Sep. 23, 2009 http://research.csc.ncsu.edu/stereographics/wiley<sub>&#x2014;</sub>pdf.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00035">
<othercit>Nielsen, &#x201c;Least Squares Adjustment: Linear and Nonlinear Weighted Regression Analysis&#x201d;, Feb. 21, 207, pp. 1-47 http://www2.imm.dtu.dk/pubdb/views/edoc<sub>&#x2014;</sub>download.php/2804/pdf/imm2804.pdf.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00036">
<othercit>Moffitt et al., &#x201c;Photogrammetry&#x2014;Third Edition&#x201d;, 1980, Harper &#x26; Row, Inc., pp. 146-147, 358-360, 618-631.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00037">
<othercit>Usery, &#x201c;Autostereoscopy&#x2014;Three-Dimensional Visualization Solution or Myth?&#x201d;, pp. 1-5, retrieved Sep. 23, 2009 http://www.ucgis.org/visualization/whitepapers/usery-autostereoscopy.pdf.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00038">
<othercit>Runnels et al., &#x201c;Real-Time 3-D Visualization of Various Geo-Referenced Digital Data on Multiple Platforms&#x201d;, University of S. Mississippi Center of Higher Learning High Performance Visualization Center, Oct. 2003, Report CHL-HPVC-03-02, pp. 1-6.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00039">
<othercit>PCT search report dated Jul. 26, 2012 regarding application PCT/US2011/058143, filed Oct. 27, 2011, applicant reference 09-0436-PCT, applicant The Boeing Company, 12 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>19</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>348 42</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348 49</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348 51</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348144</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348 25</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348 40</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348 43</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348156</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348419</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348427</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348646</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382154</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>10</number-of-drawing-sheets>
<number-of-figures>13</number-of-figures>
</figures>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Houck, II</last-name>
<first-name>Dewey Rush</first-name>
<address>
<city>Lorton</city>
<state>VA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Perlik, III</last-name>
<first-name>Andrew T.</first-name>
<address>
<city>Nokesville</city>
<state>VA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Griglak</last-name>
<first-name>Brian Joseph</first-name>
<address>
<city>Manassas</city>
<state>VA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="004" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Cooney</last-name>
<first-name>William A.</first-name>
<address>
<city>Fairfax</city>
<state>VA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="005" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Jackson</last-name>
<first-name>Michael Gordon</first-name>
<address>
<city>Reston</city>
<state>VA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Houck, II</last-name>
<first-name>Dewey Rush</first-name>
<address>
<city>Lorton</city>
<state>VA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Perlik, III</last-name>
<first-name>Andrew T.</first-name>
<address>
<city>Nokesville</city>
<state>VA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Griglak</last-name>
<first-name>Brian Joseph</first-name>
<address>
<city>Manassas</city>
<state>VA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="004" designation="us-only">
<addressbook>
<last-name>Cooney</last-name>
<first-name>William A.</first-name>
<address>
<city>Fairfax</city>
<state>VA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="005" designation="us-only">
<addressbook>
<last-name>Jackson</last-name>
<first-name>Michael Gordon</first-name>
<address>
<city>Reston</city>
<state>VA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Yee &#x26; Associates, P.C.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>The Boeing Company</orgname>
<role>02</role>
<address>
<city>Chicago</city>
<state>IL</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Jacobs</last-name>
<first-name>Lashonda</first-name>
<department>2457</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A method and apparatus for processing a video data stream. The video data stream is received from a video camera system. The video data stream comprises a plurality of images of a scene. A plurality of image pairs is selected from the plurality of images. A first image of a first area in the scene overlaps a second image of a second area in the scene in each image pair in the plurality of image pairs. Each image pair in the plurality of image pairs is adjusted to form a plurality of adjusted image pairs. The plurality of adjusted image pairs is configured to provide a perception of depth for the scene when the plurality of adjusted image pairs is presented as a video.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="187.20mm" wi="249.09mm" file="US08624959-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="250.27mm" wi="187.03mm" orientation="landscape" file="US08624959-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="282.62mm" wi="197.53mm" orientation="landscape" file="US08624959-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="220.56mm" wi="185.17mm" file="US08624959-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="263.99mm" wi="191.43mm" file="US08624959-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="269.83mm" wi="176.28mm" file="US08624959-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="239.52mm" wi="164.25mm" orientation="landscape" file="US08624959-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="236.81mm" wi="156.04mm" file="US08624959-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="239.52mm" wi="166.54mm" orientation="landscape" file="US08624959-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="151.05mm" wi="116.50mm" file="US08624959-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="196.85mm" wi="157.65mm" file="US08624959-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND INFORMATION</heading>
<p id="p-0002" num="0001">1. Field:</p>
<p id="p-0003" num="0002">The present disclosure relates generally to processing information and, in particular, to a method and apparatus for processing a video data stream. Still more particularly, the present disclosure relates to a method and apparatus for generating a stereoscopic video data stream from a monoscopic video data stream.</p>
<p id="p-0004" num="0003">2. Background:</p>
<p id="p-0005" num="0004">The gathering of information about a scene may be useful and/or necessary when performing different types of missions. These types of missions include, for example, topographical mapping, intelligence gathering, surveillance and reconnaissance, target acquisition, and/or other suitable types of missions. Information is also gathered about objects at a scene. These objects include, for example, vehicles, structures, people, land masses, water bodies, ravines, hills, mountains, and/or other suitable objects at the scene.</p>
<p id="p-0006" num="0005">One manner in which information about a scene is gathered is through taking images and/or video of the scene. A video is a sequence of a plurality of images in a video data stream that is presented in a manner that provides a viewer a capability to perceive motion of objects in the scene and/or the viewpoint of the viewer. As one example, data for the video is taken from a platform above the scene. The platform may be in the form of an aircraft, a satellite, a helicopter, an unmanned aerial vehicle (UAV), or some other suitable platform.</p>
<p id="p-0007" num="0006">Some currently available systems for taking video of a scene include video camera systems that generate monoscopic video. Monoscopic video is video that allows a viewer to perceive a scene in two-dimensions. The video camera systems that generate monoscopic video may take the form of monoscopic video cameras. These monoscopic video cameras generate video data streams. These video data streams are monoscopic video data streams.</p>
<p id="p-0008" num="0007">Other currently available systems include video camera systems that generate stereoscopic video. Stereoscopic video is a video that allows a viewer to perceive a scene in three-dimensions. The video camera systems that generate stereoscopic video may take the form of stereoscopic video camera systems. These stereoscopic video camera systems generate video data streams. These video data streams are stereoscopic video data streams. Stereoscopic is also referred to as stereo.</p>
<p id="p-0009" num="0008">One currently available stereoscopic video camera system uses a pair of monoscopic video cameras. The monoscopic video cameras are positioned within a selected distance from each other. Further, the monoscopic video cameras generate video data streams from two different perspectives of the same scene. The video data streams from this pair of monoscopic video cameras are used to form stereoscopic video data streams.</p>
<p id="p-0010" num="0009">A stereoscopic video data stream provides a viewer with more information about a scene as compared to a monoscopic video data stream. For example, a stereoscopic video data stream may provide a viewer with more information about objects at a scene as compared to a monoscopic video stream. More specifically, a stereoscopic video data stream may provide a viewer with a perception of depth for the scene.</p>
<p id="p-0011" num="0010">This type of information may be desirable to a viewer. In this type of situation, a stereoscopic video camera system may be used instead of a monoscopic video camera. If a platform already has a monoscopic video camera, a replacement or modification of the monoscopic video camera may be performed to generate a stereoscopic video stream.</p>
<p id="p-0012" num="0011">Replacing an existing monoscopic video camera with a stereoscopic video camera system requires maintenance, time, and/or expense. Further, attaching a currently available stereoscopic video camera system to a platform may add weight to the platform. The additional weight may reduce performance of the platform and/or the distance that the platform can travel.</p>
<p id="p-0013" num="0012">Therefore, it would be advantageous to have a method and apparatus that takes into account at least some of the issues discussed above, and possibly other issues.</p>
<heading id="h-0002" level="1">SUMMARY</heading>
<p id="p-0014" num="0013">In one illustrative embodiment, a method is present for processing a video data stream. The video data stream is received from a video camera system. The video data stream comprises a plurality of images of a scene. A plurality of image pairs is selected from the plurality of images. A first image of a first area in the scene overlaps a second image of a second area in the scene in each image pair in the plurality of image pairs. Each image pair in the plurality of image pairs is adjusted to form a plurality of adjusted image pairs. The plurality of adjusted image pairs is configured to provide a perception of depth for the scene when the plurality of adjusted image pairs is presented as a video.</p>
<p id="p-0015" num="0014">In another illustrative example, an apparatus comprises a processor unit and a display system. The processor unit is configured to receive a monoscopic video data stream from a video camera system. The monoscopic video data stream comprises a plurality of images for a scene. The processor unit processes the monoscopic video data stream to form a stereoscopic video data stream that provides a perception of depth for the scene. The display system is configured to display a video using the stereoscopic video data stream of the scene generated by the processor unit.</p>
<p id="p-0016" num="0015">In yet another illustrative embodiment, a computer program product comprises a computer readable medium and program instructions, stored on the computer readable medium. Program instructions are present for receiving a video data stream from a video camera system. The video data stream comprises a plurality of images of a scene. Program instructions are present for selecting a plurality of image pairs from the plurality of images. A first image of a first area in the scene overlaps a second image of a second area in the scene in each image pair in the plurality of image pairs. Program instructions are also present for adjusting each image pair in the plurality of image pairs to form a plurality of adjusted image pairs. The plurality of adjusted image pairs is configured to provide a perception of depth for the scene when presented as a video.</p>
<p id="p-0017" num="0016">The features, functions, and advantages can be achieved independently in various embodiments of the present disclosure or may be combined in yet other embodiments in which further details can be seen with reference to the following description and drawings.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0018" num="0017">The novel features believed characteristic of the illustrative embodiments are set forth in the appended claims. The illustrative embodiments, however, as well as a mode of use, further objectives, and advantages thereof, will best be understood by reference to the following detailed description of an illustrative embodiment of the present disclosure when read in conjunction with the accompanying drawings, wherein:</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 1</figref> is a pictorial illustration of a video imaging environment in accordance with an illustrative embodiment;</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 2</figref> is an illustration of a video imaging environment in accordance with an illustrative embodiment;</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 3</figref> is an illustration of a data processing system in accordance with an illustrative embodiment;</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 4</figref> is an illustration of images in a video data stream in accordance with an illustrative embodiment;</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 5</figref> is an illustration of images in a video data stream in accordance with an illustrative embodiment;</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 6</figref> is an illustration of a video data stream in accordance with an illustrative embodiment;</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 7</figref> is an illustration of a video data stream in accordance with an illustrative embodiment;</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 8</figref> is an illustration of an adjustment of an image pair in accordance with an illustrative example;</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 9</figref> is an illustration of a matrix in accordance with an illustrative embodiment;</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 10</figref> is an illustration of a matrix in accordance with an illustrative embodiment;</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 11</figref> is an illustration of selection of a portion of an adjusted image pair for viewing in accordance with an illustrative embodiment;</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 12</figref> is an illustration of a flowchart of a method for processing a video data stream in accordance with an illustrative embodiment; and</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 13</figref> is an illustration of a flowchart of a method for processing a video data stream in accordance with an illustrative embodiment.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0032" num="0031">The different illustrative embodiments recognize and take into account a number of different considerations. For example, the different illustrative embodiments recognize and take into account that currently available stereoscopic video camera systems may generate images using a pair of monoscopic video cameras. Each image is also referred to as a frame. The images taken from the pair of monoscopic video cameras are processed to generate stereoscopic images. These stereoscopic images may be viewed as a video.</p>
<p id="p-0033" num="0032">The different illustrative embodiments recognize and take into account that the frame rate of the stereoscopic images is limited by the frame rate of the monoscopic video cameras. In some cases, the frame rate of the monoscopic video cameras may not be as fast as desired. The frame rate is the frequency at which images are produced.</p>
<p id="p-0034" num="0033">Further, images generated by monoscopic video cameras with faster frame rates may need additional processing as compared to images generated by monoscopic video cameras with slower frame rates. This additional processing may increase the amount of time needed to view the video as the video data stream is received. The different illustrative embodiments recognize and take into account that additional processing resources may be used to perform the additional processing, while reducing and/or not increasing the amount of time needed to view the video as the video data stream is received.</p>
<p id="p-0035" num="0034">The different illustrative embodiments also recognize and take into account that using a single monoscopic video camera may generate a video data stream that requires less processing as compared to the video data streams generated by a pair of monoscopic video cameras. As a result, the stereoscopic video data stream processed may provide frame rates equal to or greater than the frame rate of the single monoscopic video camera.</p>
<p id="p-0036" num="0035">Further, the different illustrative embodiments recognize and take into account that stereoscopic video camera systems may be capable of providing a user with a greater amount of information in a shorter time period as compared to monoscopic video cameras.</p>
<p id="p-0037" num="0036">Additionally, the different illustrative embodiments also recognize and take into account that currently available systems for processing video data streams may include a greater number of hardware and/or software components than desired. The different illustrative embodiments recognize and take into account that a processing system that interpolates a video data stream may include a fewer number of hardware and/or software components as compared to currently available systems.</p>
<p id="p-0038" num="0037">Thus, the different illustrative embodiments provide a method and apparatus for processing a video data stream. The video data stream is received from a video camera system. The video data stream comprises a plurality of images of a scene. A plurality of image pairs is selected from the plurality of images. A first image of a first area in the scene overlaps a second image of a second area in the scene in each image pair in the plurality of image pairs. Each image pair is adjusted in the plurality of image pairs to form a plurality of adjusted image pairs. The plurality of adjusted image pairs is configured to provide a perception of depth for the scene when presented as a video to a viewer. The viewer may be, for example, an intelligence agent, a map maker, a land planning engineer, or some other type of person.</p>
<p id="p-0039" num="0038">With reference now to <figref idref="DRAWINGS">FIG. 1</figref>, a pictorial illustration of a video imaging environment is depicted in accordance with an illustrative embodiment. In this example, video imaging environment <b>100</b> includes aircraft <b>101</b>.</p>
<p id="p-0040" num="0039">As depicted, aircraft <b>101</b> generates a video data stream for scene <b>102</b> while in flight over scene <b>102</b> in the direction of arrow <b>103</b>. In these examples, scene <b>102</b> is a scene that may be viewed by a viewer of the video data stream. A scene may be an actual geographic location. For example, a scene may be a city, a street, a field, a mountain, or some other suitable geographic location.</p>
<p id="p-0041" num="0040">Aircraft <b>101</b> includes a video camera system that generates the video data stream. The video data stream is a monoscopic video data stream in this example. In this illustrative example, the video data stream includes images of scene <b>102</b>. In other words, the video data stream includes monoscopic images that provide information about scene <b>102</b> in two dimensions. These images are taken at a frequency of about 30 Hertz or about 30 images per second in this example. In other examples, images may be taken at a frequency of, for example, about 24 Hertz or about 40 Hertz.</p>
<p id="p-0042" num="0041">In the generation of images for the video data stream, aircraft <b>101</b> generates an image of area <b>104</b> in scene <b>102</b>, while aircraft <b>101</b> is at position <b>106</b> over scene <b>102</b>. Aircraft <b>101</b> then moves in the direction of arrow <b>103</b> to position <b>110</b>. At position <b>110</b>, aircraft <b>101</b> generates an image of area <b>108</b> in scene <b>102</b>.</p>
<p id="p-0043" num="0042">In this illustrative example, area <b>104</b> and area <b>108</b> overlap each other in overlapped area <b>112</b> in scene <b>102</b>. As a result, the image of area <b>104</b> and the image of area <b>108</b> also overlap each other. In other words, overlapped area <b>112</b> is seen in both the image of area <b>104</b> and the image of area <b>108</b>.</p>
<p id="p-0044" num="0043">In this illustrative example, the video data stream is sent to remote location <b>114</b> for processing. Remote location <b>114</b> is a location remote to aircraft <b>101</b>, such as a ground station. In other illustrative examples, the video data stream may be processed in aircraft <b>101</b>. The video data stream is processed at remote location <b>114</b> to generate a stereoscopic video data stream for scene <b>102</b>.</p>
<p id="p-0045" num="0044">As depicted in this example, aircraft <b>101</b> sends the video data stream to remote location <b>114</b> using wireless signals <b>116</b>. In particular, wireless signals <b>116</b> may be sent over a communications link such as, for example, a wireless communications link. In other illustrative examples, aircraft <b>101</b> may send the video data stream to remote location <b>114</b> using some other suitable type of communications link.</p>
<p id="p-0046" num="0045">With reference now to <figref idref="DRAWINGS">FIG. 2</figref>, an illustration of a video imaging environment is depicted in accordance with an illustrative embodiment. Video imaging environment <b>200</b> depicts components that may be used in the pictorial illustration of video imaging environment <b>100</b> in <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0047" num="0046">In this illustrative example, video imaging environment <b>200</b> includes platform <b>201</b>. In these illustrative examples, platform <b>201</b> is selected from one of a mobile platform, a stationary platform, an aircraft, an unmanned aerial vehicle (UAV), a helicopter, a satellite, or some other suitable platform.</p>
<p id="p-0048" num="0047">Video camera system <b>202</b> is associated with platform <b>201</b>. A first component may be considered to be associated with a second component by being secured to the second component, bonded to the second component, fastened to the second component, and/or connected to the second component in some other suitable manner. The first component also may be connected to the second component through the use of a third component. The first component also may be considered to be associated with the second component by being formed as part of and/or as an extension of the second component.</p>
<p id="p-0049" num="0048">In this illustrative example, video camera system <b>202</b> is configured to generate video data stream <b>210</b> for scene <b>206</b>. Scene <b>206</b> is a geographic location, in this example, and includes number of objects <b>208</b> at scene <b>206</b>. Number of objects <b>208</b> may include, for example, without limitation, land masses, mountains, ravines, water bodies, structures, vehicles, animals, trees, people, and/or other suitable objects at scene <b>206</b>. A number, when referring to items, means one or more items.</p>
<p id="p-0050" num="0049">Video data stream <b>210</b> includes information about scene <b>206</b>. This information may include information about number of objects <b>208</b> at scene <b>206</b>. In this illustrative example, video data stream <b>210</b> comprises plurality of images <b>211</b> of scene <b>206</b>. Plurality of images <b>211</b> in video data stream <b>210</b> is generated while platform <b>201</b> moves video camera system <b>202</b> over scene <b>206</b>. In this example, video data stream <b>210</b> is a monoscopic video data stream, and plurality of images <b>211</b> is a plurality of monoscopic images. These monoscopic images provide information about scene <b>206</b> in two dimensions.</p>
<p id="p-0051" num="0050">In this illustrative example, video camera system <b>202</b> takes the form of monoscopic video camera <b>214</b>. In this particular example, monoscopic video camera <b>214</b> may be attached to the bottom of platform <b>201</b> to generate plurality of images <b>211</b> as platform <b>201</b> moves over scene <b>206</b>.</p>
<p id="p-0052" num="0051">Further, monoscopic video camera <b>214</b> is stationary on platform <b>201</b> in these illustrative examples. In other words, monoscopic video camera <b>214</b> does not move or rotate with respect to platform <b>201</b>. Adjustments may be made to video data stream <b>210</b> to account for movement of platform <b>201</b>. In other illustrative embodiments, monoscopic video camera <b>214</b> may be rotated to adjust monoscopic video camera <b>214</b> as platform <b>201</b> moves.</p>
<p id="p-0053" num="0052">In these illustrative examples, video camera system <b>202</b> sends video data stream <b>210</b> to computer system <b>204</b> as video data stream <b>210</b> is generated. In this illustrative example, computer system <b>204</b> may be one or more computers. Computer system <b>204</b> is configured to execute program <b>215</b>. Program <b>215</b> is executed to process video data stream <b>210</b> for viewing. In other examples, computer system <b>204</b> may be a hardware device configured to process video data stream <b>210</b> for viewing. Computer system <b>204</b> processes video data stream <b>210</b> at a rate fast enough to be substantially real-time or near real-time.</p>
<p id="p-0054" num="0053">As depicted, computer system <b>204</b> is located at remote location <b>213</b>. Remote location <b>213</b> is a location remote to platform <b>201</b>. For example, remote location <b>213</b> may be a ground station, a truck, a building, a house, an aircraft, a satellite station, or some other suitable location remote to platform <b>201</b>. In other illustrative embodiments, computer system <b>204</b> may be associated with platform <b>201</b>.</p>
<p id="p-0055" num="0054">In these illustrative examples, computer system <b>204</b> selects plurality of image pairs <b>216</b> from plurality of images <b>211</b> in video data stream <b>210</b> based on an overlap between images in plurality of images <b>211</b>.</p>
<p id="p-0056" num="0055">As one illustrative example, computer system <b>204</b> selects image pair <b>217</b> from images in plurality of images <b>211</b>. In this depicted example, image pair <b>217</b> includes first image <b>218</b> and second image <b>220</b>. First image <b>218</b> is generated while platform <b>201</b> is at first position <b>219</b> above scene <b>206</b>. Second image <b>220</b> is generated while platform <b>201</b> is at second position <b>221</b> above scene <b>206</b>.</p>
<p id="p-0057" num="0056">First image <b>218</b> is an image of first area <b>222</b> in scene <b>206</b>, and second image <b>220</b> is an image of second area <b>224</b> in scene <b>206</b>. In this illustrative example, first area <b>222</b> and second area <b>224</b> overlap each other in overlapped area <b>225</b>. As a result, at least a portion of first image <b>218</b> overlaps with second image <b>220</b>. Overlapped area <b>225</b> is seen in both first image <b>218</b> and second image <b>220</b>.</p>
<p id="p-0058" num="0057">Image pair <b>217</b> is selected such that first image <b>218</b> and second image <b>220</b> overlap with desired overlap <b>226</b>. In these illustrative examples, desired overlap <b>226</b> may be selected such that the overlap between first image <b>218</b> and second image <b>220</b> provides an amount of image separation between first image <b>218</b> and second image <b>220</b>. This amount of image separation is selected to take into account a separation of the eyes of a viewer when image pair <b>217</b> is used to generate a stereoscopic video data stream. In these examples, desired overlap <b>226</b> may be, for example, about 60 percent.</p>
<p id="p-0059" num="0058">As depicted, adjustment <b>228</b> is made to plurality of image pairs <b>216</b> to form plurality of adjusted image pairs <b>230</b>. Plurality of adjusted image pairs <b>230</b> is configured to provide perception of depth <b>232</b> for scene <b>206</b>. In these examples, perception of depth <b>232</b> for scene <b>206</b> is for overlapped area <b>225</b> in scene <b>206</b>.</p>
<p id="p-0060" num="0059">Perception of depth <b>232</b> is a perception of three dimensions for scene <b>206</b>. These dimensions are typically referred to as length, width, and height. Perception of depth <b>232</b> also includes a perception of the relationship of number of objects <b>208</b> at scene <b>206</b> relative to each other. In other words, perception of depth <b>232</b> may be a perception of the positions of number of objects <b>208</b> relative to each other. Perception of depth <b>232</b> also may include a perception of differences in height between number of objects <b>208</b>.</p>
<p id="p-0061" num="0060">In these illustrative examples, one portion of adjustment <b>228</b> involves adjusting a first image and a second image in each image pair in plurality of image pairs <b>216</b> relative to each other to form plurality of adjusted image pairs <b>230</b>. One or both images in each image pair may be adjusted. For example, first image <b>218</b> and second image <b>220</b> in image pair <b>217</b> are adjusted relative to each other to form adjusted image pair <b>242</b>. In these illustrative examples, this adjustment may involve using, for example, without limitation, a least squares algorithm.</p>
<p id="p-0062" num="0061">As one example, adjustment <b>228</b> involves selecting first position <b>234</b> and first orientation <b>236</b> for first image <b>218</b>. Thereafter, second position <b>238</b> and second orientation <b>240</b> for second image <b>220</b> are adjusted relative to first position <b>234</b> and first orientation <b>236</b> of first image <b>218</b> in image pair <b>217</b> in plurality of image pairs <b>216</b>. In these illustrative examples, first position <b>234</b>, first orientation <b>236</b>, second position <b>238</b>, and second orientation <b>240</b> are for platform <b>201</b>. In this manner, first image <b>218</b> and second image <b>220</b> are adjusted relative to each other. This process is discussed in greater detail in the description of <figref idref="DRAWINGS">FIG. 8</figref> below.</p>
<p id="p-0063" num="0062">In these illustrative examples, the position of platform <b>201</b> is a three-dimensional position. The position may be described using, for example, without limitation, a latitude, a longitude, and an altitude of platform <b>201</b>. An orientation of platform <b>201</b> is, more specifically, an orientation of video camera system <b>202</b> associated with platform <b>201</b>. In these examples, the orientation of platform <b>201</b> is described using rotations about three axes. These three axes may be, for example, roll, pitch, and yaw. When platform <b>201</b> is an aircraft, the orientation of platform <b>201</b> is attitude <b>244</b>. Of course, the position and orientation of platform <b>201</b> may be described using other types of coordinate systems such as, for example, quaternions.</p>
<p id="p-0064" num="0063">In these illustrative examples, first position <b>234</b> and first orientation <b>236</b> may be provided by positioning system <b>243</b> associated with platform <b>201</b>. Positioning system <b>243</b> may include at least one of a global positioning system, an inertial positioning system, a gyroscopic system, and/or some other suitable type of positioning system.</p>
<p id="p-0065" num="0064">As used herein, the phrase &#x201c;at least one of&#x201d;, when used with a list of items, means that different combinations of one or more of the listed items may be used and only one of each item in the list may be needed. For example, &#x201c;at least one of item A, item B, and item C&#x201d; may include, for example, without limitation, item A or item A and item B. This example also may include item A, item B, and item C, or item B and item C.</p>
<p id="p-0066" num="0065">For example, positioning system <b>243</b> may include a global positioning system to provide first position <b>234</b> and a gyroscopic system to provide first orientation <b>236</b>. In some illustrative embodiments, positioning system <b>243</b> also provides second position <b>238</b> and second orientation <b>240</b>.</p>
<p id="p-0067" num="0066">In these illustrative examples, another portion of adjustment <b>228</b> involves adjusting adjusted image pair <b>242</b> for viewing such that perception of depth <b>232</b> is provided. This adjustment may be performed using a process referred to as epipolar warping <b>245</b>. Epipolar warping <b>245</b> involves selecting portion <b>241</b> of adjusted image pair <b>242</b> in plurality of adjusted image pairs <b>230</b> using number of common points <b>239</b> in adjusted image pair <b>242</b>. This type of selection is made for each adjusted image pair in plurality of adjusted image pairs <b>230</b>. In this manner, plurality of adjusted image pairs <b>230</b> is configured to provide perception of depth <b>232</b> when viewed. This process is discussed in greater detail in <figref idref="DRAWINGS">FIG. 9</figref> below.</p>
<p id="p-0068" num="0067">Adjustment <b>228</b> is performed such that the two images in adjusted image pair <b>242</b> have differences that allow depth to be perceived. In other words, adjustment <b>228</b> generates adjusted image pair <b>242</b> to provide perception of depth <b>232</b> for scene <b>206</b> when properly viewed.</p>
<p id="p-0069" num="0068">In these illustrative examples, adjustment <b>228</b> also may increase perception of depth <b>232</b> beyond what viewer <b>250</b> would normally see in the real world. For example, this increase in perception of depth <b>232</b> may be dependent on the value selected for desired overlap <b>226</b> between first image <b>218</b> and second image <b>220</b>. An increase in perception of depth <b>232</b> would allow more information to be presented to viewer <b>250</b> as compared to a real-world perception of depth. For example, features of interest on number of objects <b>208</b> may be magnified with an increase in perception of depth <b>232</b>.</p>
<p id="p-0070" num="0069">In this illustrative example, plurality of adjusted image pairs <b>230</b> is sent in video data stream <b>247</b> to display system <b>248</b>. Video data stream <b>247</b> is a stereoscopic video data stream in this example. Display system <b>248</b> is configured to present plurality of adjusted image pairs <b>230</b> in video data stream <b>247</b> in the form of plurality of stereoscopic images <b>249</b> to viewer <b>250</b>.</p>
<p id="p-0071" num="0070">For example, the images in adjusted image pair <b>242</b> are presented by display system <b>248</b> in a manner such that viewer <b>250</b> sees stereoscopic image <b>252</b> in plurality of stereoscopic images <b>249</b>. Stereoscopic image <b>252</b> provides viewer <b>250</b> with perception of depth <b>232</b> for scene <b>206</b>. In other words, stereoscopic image <b>252</b> provides viewer <b>250</b> with a perception of scene <b>206</b> in three dimensions.</p>
<p id="p-0072" num="0071">Plurality of stereoscopic images <b>249</b> is generated in the sequence that plurality of images <b>211</b> was generated. This sequence allows plurality of stereoscopic images <b>249</b> to form stereoscopic video <b>254</b> of scene <b>206</b>. Stereoscopic video <b>254</b> provides perception of depth <b>232</b> for scene <b>206</b>.</p>
<p id="p-0073" num="0072">In this illustrative example, display system <b>248</b> may include a number of devices configured to allow viewer <b>250</b> to view stereoscopic video <b>254</b>. For example, display system <b>248</b> may include hardware device <b>255</b> configured to process video data stream <b>247</b>. Hardware device <b>255</b> may be, for example, without limitation, a video adapter, a digital signal processor, and/or some other suitable hardware component.</p>
<p id="p-0074" num="0073">Hardware device <b>255</b> sends video data stream <b>247</b> to display device <b>256</b> in display system <b>248</b> for viewing. In these examples, display device <b>256</b> may be a display screen, a monitor, a projection screen, and/or some other suitable display device.</p>
<p id="p-0075" num="0074">In other illustrative embodiments, display system <b>248</b> may be a head-mounted device such as, for example, linearly polarized glasses, circularly polarized glasses, anachrome optical glasses, or some other suitable type of device. In some illustrative embodiments, display system <b>248</b> may be a computer with a display device, such as a flat screen monitor. This computer may display stereoscopic video <b>254</b> using, for example, autostereograms, a Pulfrich effect paradigm, or wiggle stereoscopy.</p>
<p id="p-0076" num="0075">The illustration of video imaging environment <b>200</b> in <figref idref="DRAWINGS">FIG. 2</figref> is not meant to imply physical or architectural limitations to the manner in which different advantageous embodiments may be implemented. Other components in addition to and/or in place of the ones illustrated may be used. Some components may be unnecessary in some advantageous embodiments. Also, the blocks are presented to illustrate some functional components. One or more of these blocks may be combined and/or divided into different blocks when implemented in different advantageous embodiments.</p>
<p id="p-0077" num="0076">For example, video camera system <b>202</b> may include additional video cameras in addition to monoscopic video camera <b>214</b>. These additional video cameras may be used to gather information about additional scenes in addition to scene <b>206</b>. In other illustrative embodiments, positioning system <b>243</b> may be located remote to platform <b>201</b>. For example, positioning system <b>243</b> may be associated with a satellite station, a ground station, or some other suitable location remote to platform <b>201</b>.</p>
<p id="p-0078" num="0077">Turning now to <figref idref="DRAWINGS">FIG. 3</figref>, an illustration of a data processing system is depicted in accordance with an illustrative embodiment. In this illustrative example, data processing system <b>300</b> is an example of one implementation for computer system <b>204</b> in <figref idref="DRAWINGS">FIG. 2</figref>. Data processing system <b>300</b> includes communications fabric <b>302</b>, which provides communications between processor unit <b>304</b>, memory <b>306</b>, persistent storage <b>308</b>, communications unit <b>310</b>, input/output (I/O) unit <b>312</b>, and display <b>314</b>.</p>
<p id="p-0079" num="0078">Processor unit <b>304</b> serves to execute instructions for software that may be loaded into memory <b>306</b>. Processor unit <b>304</b> may be a set of one or more processors or may be a multi-processor core, depending on the particular implementation. Further, processor unit <b>304</b> may be implemented using one or more heterogeneous processor systems, in which a main processor is present with secondary processors on a single chip. As another illustrative example, processor unit <b>304</b> may be a symmetric multi-processor system containing multiple processors of the same type.</p>
<p id="p-0080" num="0079">Memory <b>306</b> and persistent storage <b>308</b> are examples of storage devices <b>316</b>. A storage device is any piece of hardware that is capable of storing information, such as, for example, without limitation, data, program code in functional form, and/or other suitable information either on a temporary basis and/or a permanent basis. Memory <b>306</b>, in these examples, may be, for example, a random access memory, or any other suitable volatile or non-volatile storage device.</p>
<p id="p-0081" num="0080">Persistent storage <b>308</b> may take various forms, depending on the particular implementation. For example, persistent storage <b>308</b> may contain one or more components or devices. For example, persistent storage <b>308</b> may be a hard drive, a flash memory, a rewritable optical disk, a rewritable magnetic tape, or some combination of the above. The media used by persistent storage <b>308</b> may be removable. For example, a removable hard drive may be used for persistent storage <b>308</b>.</p>
<p id="p-0082" num="0081">Communications unit <b>310</b>, in these examples, provides for communication with other data processing systems or devices. In these examples, communications unit <b>310</b> is a network interface card. Communications unit <b>310</b> may provide communications through the use of either or both physical and wireless communications links.</p>
<p id="p-0083" num="0082">Input/output unit <b>312</b> allows for the input and output of data with other devices that may be connected to data processing system <b>300</b>. For example, input/output unit <b>312</b> may provide a connection for user input through a keyboard, a mouse, and/or some other suitable input device. Further, input/output unit <b>312</b> may send output to a printer. Display <b>314</b> provides a mechanism to display information to a user.</p>
<p id="p-0084" num="0083">Instructions for the operating system, applications, and/or programs may be located in storage devices <b>316</b>, which are in communication with processor unit <b>304</b> through communications fabric <b>302</b>. These instructions may include instructions for program <b>215</b> in <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0085" num="0084">In these illustrative examples, the instructions are in a functional form on persistent storage <b>308</b>. These instructions may be loaded into memory <b>306</b> for execution by processor unit <b>304</b>. The processes of the different embodiments may be performed by processor unit <b>304</b> using computer-implemented instructions, which may be located in a memory, such as memory <b>306</b>.</p>
<p id="p-0086" num="0085">These instructions are referred to as program code, computer usable program code, or computer readable program code that may be read and executed by a processor in processor unit <b>304</b>. The program code, in the different embodiments, may be embodied on different physical or computer readable storage media, such as memory <b>306</b> or persistent storage <b>308</b>.</p>
<p id="p-0087" num="0086">Program code <b>318</b> is located in a functional form on computer readable media <b>320</b> that is selectively removable and may be loaded onto or transferred to data processing system <b>300</b> for execution by processor unit <b>304</b>. Program code <b>318</b> and computer readable media <b>320</b> form computer program product <b>322</b>. In one example, computer readable media <b>320</b> may be computer readable storage media <b>324</b> or computer readable signal media <b>326</b>.</p>
<p id="p-0088" num="0087">Computer readable storage media <b>324</b> may include, for example, an optical or magnetic disk that is inserted or placed into a drive or other device that is part of persistent storage <b>308</b> for transfer onto a storage device, such as a hard drive, that is part of persistent storage <b>308</b>. Computer readable storage media <b>324</b> also may take the form of a persistent storage, such as a hard drive, a thumb drive, or a flash memory that is connected to data processing system <b>300</b>. In some instances, computer readable storage media <b>324</b> may not be removable from data processing system <b>300</b>.</p>
<p id="p-0089" num="0088">Alternatively, program code <b>318</b> may be transferred to data processing system <b>300</b> using computer readable signal media <b>326</b>. Computer readable signal media <b>326</b> may be, for example, a propagated data signal containing program code <b>318</b>. For example, computer readable signal media <b>326</b> may be an electro-magnetic signal, an optical signal, and/or any other suitable type of signal. These signals may be transmitted over communications links, such as wireless communications links, an optical fiber cable, a coaxial cable, a wire, and/or any other suitable type of communications link. In other words, the communications link and/or the connection may be physical or wireless in the illustrative examples.</p>
<p id="p-0090" num="0089">In some illustrative embodiments, program code <b>318</b> may be downloaded over a network to persistent storage <b>308</b> from another device or data processing system through computer readable signal media <b>326</b> for use within data processing system <b>300</b>. For instance, program code stored in a computer readable storage media in a server data processing system may be downloaded over a network from the server to data processing system <b>300</b>. The data processing system providing program code <b>318</b> may be a server computer, a client computer, or some other device capable of storing and transmitting program code <b>318</b>.</p>
<p id="p-0091" num="0090">The different components illustrated for data processing system <b>300</b> are not meant to provide architectural limitations to the manner in which different embodiments may be implemented. The different illustrative embodiments may be implemented in a data processing system including components in addition to or in place of those illustrated for data processing system <b>300</b>.</p>
<p id="p-0092" num="0091">Other components shown in <figref idref="DRAWINGS">FIG. 3</figref> can be varied from the illustrative examples shown. The different embodiments may be implemented using any hardware device or system capable of executing program code. As one example, data processing system <b>300</b> may include organic components integrated with inorganic components and/or may be comprised entirely of organic components excluding a human being. For example, a storage device may be comprised of an organic semiconductor.</p>
<p id="p-0093" num="0092">As another example, a storage device in data processing system <b>300</b> is any hardware apparatus that may store data. Memory <b>306</b>, persistent storage <b>308</b>, and computer readable media <b>320</b> are examples of storage devices in a tangible form.</p>
<p id="p-0094" num="0093">In another example, a bus system may be used to implement communications fabric <b>302</b> and may be comprised of one or more buses, such as a system bus or an input/output bus. Of course, the bus system may be implemented using any suitable type of architecture that provides for a transfer of data between different components or devices attached to the bus system. Additionally, a communications unit may include one or more devices used to transmit and receive data, such as a modem or a network adapter. Further, a memory may be, for example, memory <b>306</b> or a cache such as found in an interface and memory controller hub that may be present in communications fabric <b>302</b>.</p>
<p id="p-0095" num="0094">With reference now to <figref idref="DRAWINGS">FIG. 4</figref>, an illustration of images in a video data stream is depicted in accordance with an illustrative embodiment. In this illustrative example, video data stream <b>400</b> is an example of one implementation of video data stream <b>210</b> in <figref idref="DRAWINGS">FIG. 2</figref>. Video data stream <b>400</b> is generated for scene <b>402</b> by a platform such as, for example, without limitation, platform <b>201</b> in <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0096" num="0095">In this depicted example, video data stream <b>400</b> includes plurality of images <b>401</b>. Plurality of images <b>401</b> includes images, such as image <b>404</b>, image <b>406</b>, image <b>408</b>, and image <b>410</b>. Plurality of images <b>401</b> for video data stream <b>400</b> is generated in a sequence in the direction of arrow <b>412</b>. Each image in plurality of images <b>401</b> is an image of an area in scene <b>402</b>. Plurality of images <b>401</b> is generated at a frequency of about 30 images per second or about 30 Hertz in this example.</p>
<p id="p-0097" num="0096">In this illustrative example, the platform moves across scene <b>402</b> with a speed of about 25 meters per second. As a result, each image in plurality of images <b>401</b> is taken about 0.8333 meters farther in scene <b>402</b> in the direction of arrow <b>412</b> than a preceding image in plurality of images <b>401</b>.</p>
<p id="p-0098" num="0097">With reference now to <figref idref="DRAWINGS">FIG. 5</figref>, an illustration of images in a video data stream is depicted in accordance with an illustrative embodiment. In this illustrative example, video data stream <b>500</b> is an example of a portion of video data stream <b>400</b> in <figref idref="DRAWINGS">FIG. 4</figref>. Video data stream <b>500</b> comprises plurality of images <b>501</b> of scene <b>502</b>.</p>
<p id="p-0099" num="0098">Image <b>504</b> may be one example of image <b>404</b> in <figref idref="DRAWINGS">FIG. 4</figref>, and image <b>506</b> may be one example of image <b>406</b> in plurality of images <b>401</b> in <figref idref="DRAWINGS">FIG. 4</figref>. Image <b>504</b> is an image of area <b>508</b> in scene <b>502</b>. Image <b>506</b> is an image of area <b>510</b> in scene <b>502</b>. Both area <b>508</b> and area <b>510</b> are about 100 meters by about 100 meters. In other illustrative embodiments, the areas represented by images <b>504</b> and <b>506</b> in plurality of images <b>501</b> may be, for example, about 500 meters by about 500 meters.</p>
<p id="p-0100" num="0099">In this illustrative example, plurality of images <b>501</b> is taken from a platform moving in the direction of arrow <b>512</b> over scene <b>502</b>. The platform is moving at a speed of about 25 meters per second. Plurality of images <b>501</b> is generated at a frequency of about 30 images per second or about 30 Hertz in this example.</p>
<p id="p-0101" num="0100">As depicted, area <b>510</b> is farther in the direction of arrow <b>512</b> from area <b>508</b> by distance <b>514</b>. Distance <b>514</b> is about 0.8333 meters in this example. Further, distance <b>514</b> is about 0.8333 meters between consecutive images in plurality of images <b>501</b>. Area <b>508</b> and area <b>510</b> overlap each other to form overlapped area <b>516</b>. Overlapped area <b>516</b> may have an overlap of about 99 percent. As a result, image <b>504</b> and image <b>506</b> also overlap each other by about 99 percent in this example.</p>
<p id="p-0102" num="0101">In the different illustrative embodiments, distance <b>514</b> is selected based on the desired overlap for overlapped area <b>516</b>. For example, in some illustrative examples, distance <b>514</b> may be selected to provide about 60 percent overlap for image <b>504</b> and image <b>506</b>. When area <b>508</b> and area <b>510</b> are about 500 meters by about 500 meters, distance <b>514</b> is selected as about 200 meters to provide about 60 percent overlap.</p>
<p id="p-0103" num="0102">With reference now to <figref idref="DRAWINGS">FIG. 6</figref>, an illustration of a video data stream is depicted in accordance with an illustrative embodiment. In this illustrative example, video data stream <b>600</b> is an example of one implementation for video data stream <b>210</b> in <figref idref="DRAWINGS">FIG. 2</figref>. Video data stream <b>600</b> is generated using a video camera system, such as video camera system <b>202</b> in <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0104" num="0103">As depicted, video data stream <b>600</b> includes plurality of images <b>601</b>. Plurality of images <b>601</b> is a plurality of monoscopic images in these illustrative examples. Plurality of images <b>601</b> is generated in a sequence in the direction of arrow <b>607</b>. Plurality of images <b>601</b> in video data stream <b>600</b> includes, for example, first image <b>602</b>, number of images <b>604</b>, and second image <b>606</b>. In these examples, first image <b>602</b> is the first image generated in the sequence of images in plurality of images <b>601</b> in video data stream <b>600</b>.</p>
<p id="p-0105" num="0104">A first image pair in plurality of images <b>601</b> is selected such that the image pair has a desired overlap. This desired overlap is about 60 percent. In this illustrative example, first image <b>602</b> and second image <b>606</b> are selected as first image pair <b>608</b>. First image pair <b>608</b> is the first image pair selected from the sequence of images in plurality of images <b>601</b>.</p>
<p id="p-0106" num="0105">First image <b>602</b> and second image <b>606</b> overlap each other to form overlapped portion <b>610</b> with about a 60 percent overlap. In these illustrative examples, number of images <b>604</b> may be 240 images. This number of images in number of images <b>604</b> may be chosen to provide overlapped portion <b>610</b> with an overlap of about 60 percent. In other words, in this example, having more than 240 images between first image <b>602</b> and second image <b>606</b> provides a decreased overlap as compared to fewer than 240 images.</p>
<p id="p-0107" num="0106">The period of time between the generation of first image <b>602</b> and the generation of second image <b>606</b> leads to a time delay in the selection of first image pair <b>608</b>. This time delay may be estimated using the following equation:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>time delay=(length*(1&#x2212;overlap))/(speed).&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0108" num="0107">In this equation, length is a length of the area represented by an image in plurality of images <b>601</b>, overlap is the overlap for overlapped portion <b>610</b>, and speed is the speed of the platform having the video camera system generating video data stream <b>600</b>. The overlap is a percent of overlap expressed as a fraction. When the platform is an aircraft, the speed may be airspeed.</p>
<p id="p-0109" num="0108">With reference now to <figref idref="DRAWINGS">FIG. 7</figref>, an illustration of a video data stream is depicted in accordance with an illustrative embodiment. In this illustrative example, video data stream <b>700</b> is an example of a video data stream, such as video data stream <b>600</b> in <figref idref="DRAWINGS">FIG. 6</figref>. As depicted, video data stream <b>700</b> is depicted at a later point in time in the generation of the video data stream, as compared to video data stream <b>600</b> in <figref idref="DRAWINGS">FIG. 6</figref>.</p>
<p id="p-0110" num="0109">As depicted, video data stream <b>700</b> includes plurality of images <b>701</b>. Plurality of images <b>701</b> includes an additional image as compared to the number of images in plurality of images <b>601</b> in <figref idref="DRAWINGS">FIG. 6</figref>. Plurality of images <b>701</b> is generated in a sequence in the direction of arrow <b>703</b>. Plurality of images <b>701</b> in video data stream <b>700</b> includes, for example, first image <b>702</b>, number of images <b>704</b>, and second image <b>706</b>. In these examples, first image <b>702</b> is the first image generated in the sequence of images in plurality of images <b>701</b>.</p>
<p id="p-0111" num="0110">In this illustrative example, first image <b>702</b> and second image <b>706</b> are selected as first image pair <b>708</b>. First image pair <b>708</b> is the first image pair selected from the sequence of images in plurality of images <b>701</b>. First image <b>702</b> and second image <b>706</b> have an overlap of about 60 percent.</p>
<p id="p-0112" num="0111">As depicted, a second image pair is selected from plurality of images <b>701</b> such that the second image pair has substantially the same overlap as first image pair <b>708</b>. Third image <b>710</b> is the next image generated in the sequence after first image <b>702</b>. Fourth image <b>712</b> is the next image generated in the sequence after second image <b>706</b>.</p>
<p id="p-0113" num="0112">Third image <b>710</b> and fourth image <b>712</b> are selected as second image pair <b>714</b> with overlapped portion <b>716</b>. Second image pair <b>714</b> is the next image pair selected in the sequence after first image pair <b>708</b>. Number of images <b>704</b> between third image <b>710</b> and fourth image <b>712</b> are 240 images, similar to number of images <b>604</b> in <figref idref="DRAWINGS">FIG. 6</figref>. Number of images <b>704</b> is selected to provide an overlap of about 60 percent in overlapped portion <b>716</b>.</p>
<p id="p-0114" num="0113">The processing of image pairs selected from plurality of images <b>701</b> is performed by a computer system, such as computer system <b>204</b> in <figref idref="DRAWINGS">FIG. 2</figref>. In these illustrative examples, first image pair <b>708</b> is adjusted by an adjustment, such as adjustment <b>228</b> in <figref idref="DRAWINGS">FIG. 2</figref>. First image pair <b>708</b> is adjusted based on a position and an orientation of the platform taking video data stream <b>700</b>. The position and orientation of the platform may be provided by a positioning system, such as positioning system <b>243</b> in <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0115" num="0114">The positioning system may provide the position and orientation of the platform for certain periods of time. These periods of time may be fixed or variable. Adjustment <b>228</b> in <figref idref="DRAWINGS">FIG. 2</figref> is made for second image pair <b>714</b> using the position and orientation of the platform as provided by the positioning system at the time of imaging of second image pair <b>714</b>. A number of image pairs selected from plurality of images <b>701</b> are adjusted in a similar manner until the next period of time that the positioning system provides the position and orientation of the platform. This type of adjustment may be referred to as interpolation. The interpolation also may involve using, for example, a least squares algorithm.</p>
<p id="p-0116" num="0115">With reference now to <figref idref="DRAWINGS">FIG. 8</figref>, an illustration of an adjustment of an image pair is depicted in accordance with an illustrative example. In this illustrative example, image pair <b>800</b> is adjusted using an adjustment, such as adjustment <b>228</b> in <figref idref="DRAWINGS">FIG. 2</figref>. Image pair <b>800</b> may be generated by a video camera system associated with a platform, such as video camera system <b>202</b> associated with platform <b>201</b> in <figref idref="DRAWINGS">FIG. 2</figref>. Image pair <b>800</b> includes first image <b>802</b> and second image <b>804</b>.</p>
<p id="p-0117" num="0116">First image <b>802</b> and second image <b>804</b> are images of scene <b>806</b>. Scene <b>806</b> is a region of land in this example. First image <b>802</b> has first position <b>808</b> and first orientation <b>809</b>, and second image <b>804</b> has second position <b>810</b> and second orientation <b>811</b>. Distance <b>812</b> is the distance between first position <b>808</b> and second position <b>810</b>.</p>
<p id="p-0118" num="0117">In this illustrative example, point <b>814</b> in first image <b>802</b> and point <b>816</b> in second image <b>804</b> are identified for point <b>818</b> in scene <b>806</b>. In these illustrative examples, point <b>814</b> and point <b>816</b> are pixels. Point <b>818</b> may be, for example, a position of an object at a scene, a location on an object, a feature on an object, or some other suitable point. Point <b>814</b> and point <b>816</b> may be identified based on, for example, a pattern matching algorithm that matches a pattern of point <b>818</b> to point <b>814</b> and point <b>816</b>. Point <b>814</b> includes data for point <b>818</b> in scene <b>806</b>. Point <b>816</b> also includes data for point <b>818</b> in scene <b>806</b>. In this example, point <b>814</b>, point <b>816</b>, and point <b>818</b> lie in plane <b>820</b>.</p>
<p id="p-0119" num="0118">As depicted in this example, scene <b>806</b> has coordinate system <b>822</b> with X1 axis <b>824</b>, Y1 axis <b>826</b>, and Z1 axis <b>828</b>. First image <b>802</b> has coordinate system <b>830</b> with X2 axis <b>832</b>, Y2 axis <b>834</b>, and Z2 axis <b>836</b>. Second image <b>804</b> has coordinate system <b>838</b> with X3 axis <b>840</b>, Y3 axis <b>842</b>, and Z3 axis <b>844</b>. Vector <b>846</b> is the position vector for position <b>808</b> with respect to coordinate system <b>822</b> for scene <b>806</b>. Vector <b>848</b> is the position vector for position <b>810</b> with respect to coordinate system <b>822</b> for scene <b>806</b>.</p>
<p id="p-0120" num="0119">At least one of first image <b>802</b> and second image <b>804</b> may be adjusted relative to each other. In this illustrative example, second image <b>804</b> is adjusted relative to first image <b>802</b>. In particular, second position <b>810</b> and second orientation <b>811</b> of second image <b>804</b> are adjusted relative to first position <b>808</b> and first orientation <b>809</b> of first image <b>802</b>. In other words, first position <b>808</b> and first orientation <b>809</b> form a reference position and a reference orientation used to make changes to second position <b>810</b> and second orientation <b>811</b>. In this manner, second image <b>804</b> is adjusted relative to first image <b>802</b>.</p>
<p id="p-0121" num="0120">In this example, first position <b>808</b> and first orientation <b>809</b> are provided by a positioning system such as, for example, positioning system <b>243</b> in <figref idref="DRAWINGS">FIG. 2</figref>. More specifically, the positioning system provides data for the position and the orientation of the platform. This data is used to provide estimates for first position <b>808</b> and first orientation <b>809</b> through a process such as interpolation. These estimates are used to adjust second image <b>804</b> relative to first image <b>802</b>.</p>
<p id="p-0122" num="0121">In some illustrative embodiments, both first image <b>802</b> and second image <b>804</b> are adjusted relative to each other. In these examples, the positioning system provides data for the position and orientation of the platform. This data is used to provide estimates for first position <b>808</b>, first orientation <b>809</b>, second position <b>810</b>, and second orientation <b>811</b> through interpolation. Further, first image <b>802</b> and second image <b>804</b> are both adjusted using a least squares algorithm. The adjustment of both first image <b>802</b> and second image <b>804</b> may allow the determination of more accurate positions and orientations as compared to the adjustment of second image <b>804</b> relative to first image <b>802</b>.</p>
<p id="p-0123" num="0122">For image pair <b>800</b>, point <b>814</b> and point <b>816</b> may form related point <b>817</b>. In other words, point <b>814</b> and point <b>816</b> both correspond to point <b>818</b>. Related point <b>817</b> may be related to two scalar equations for each image in image pair <b>800</b>. Thus, for an N number of related points for image pair <b>800</b>, four scalar equations are present for each of N related points. In this depicted example, four scalar equations are present for related point <b>817</b>.</p>
<p id="p-0124" num="0123">When making adjustments to a plurality of image pairs, a number of scalar equations that is greater than or equal to the number of unknowns is desired. The position of a platform is based on latitude, longitude, and altitude. The position of the platform may present at least two unknowns. In these illustrative examples, one unknown may be based on altitude, and the other unknown may be based on one of latitude, longitude, and a combination of latitude and longitude. The orientation of the platform presents three unknowns. These three unknowns are based on rotations about three axes. These rotations are, for example, roll, pitch, and yaw. Further, each related point for an image pair presents three scalar unknowns.</p>
<p id="p-0125" num="0124">In this manner, the total number of unknowns is equal to 5 plus 3N. Thus, the following equation is present for N related points in image pair <b>800</b>:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>4<i>N&#x2267;</i>5+3<i>N,</i>&#x2003;&#x2003;(2)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where N is the number of related points per image pair.
</p>
<p id="p-0126" num="0125">The equation may be solved such that at least five related points per image pair are selected for performing an adjustment to the image pair in this illustrative example. Of course, some other number of related points may be selected in other implementations. This adjustment may be performed using a least squares algorithm.</p>
<p id="p-0127" num="0126">The adjustment to image pair <b>800</b> depends on a condition that point <b>814</b>, point <b>816</b>, and point <b>818</b> lie in plane <b>820</b>. This condition is expressed by the following equation:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>F</i><sub>P</sub>(<i>x</i><sub>P1</sub><i>y</i><sub>P1</sub><i>,x</i><sub>P2</sub><i>y</i><sub>P2</sub><i>,{right arrow over (r)}</i><sub>1</sub>,&#x3c9;<sub>1</sub>,&#x3a6;<sub>1</sub>,&#x3ba;<sub>1</sub><i>,{right arrow over (r)}</i><sub>2</sub>,&#x3c9;<sub>2</sub>,&#x3a6;<sub>2</sub>,&#x3ba;<sub>2</sub>)=(<i>{right arrow over (r)}</i><sub>2</sub><i>&#x2212;{right arrow over (r)}</i><sub>1</sub>)&#xb7;(<i>{right arrow over (t)}</i><sub>1P</sub><i>&#xb7;{right arrow over (t)}</i><sub>2P</sub>)=0&#x2003;&#x2003;(3)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0128" num="0127">In this equation, F is the condition, P is point <b>818</b>, P1 is point <b>814</b>, P2 is point <b>816</b>, x<sub>P1 </sub>is the coordinate of point <b>814</b> with respect to X2 axis <b>832</b>, y<sub>P1 </sub>is the coordinate of point <b>814</b> with respect to Y2 axis <b>834</b>, x<sub>P2 </sub>is the coordinate of point <b>816</b> with respect to X3 axis <b>840</b>, and y<sub>P2 </sub>is the coordinate of point <b>816</b> with respect to Y3 axis <b>842</b>. Further, {right arrow over (r)}<sub>1 </sub>is vector <b>846</b>, {right arrow over (r)}<sub>2 </sub>is vector <b>848</b>, &#x3c9;<sub>1 </sub>is the roll angle for the platform at first position <b>808</b>, &#x3c6;<sub>1 </sub>is the pitch angle for the platform at first position <b>808</b>, &#x3ba;<sub>1 </sub>is the yaw angle for the platform at first position <b>808</b>, &#x3c9;<sub>2 </sub>is the roll angle for the platform at first position <b>808</b>, &#x3c6;<sub>2 </sub>is the pitch angle for the platform at first position <b>808</b>, &#x3ba;<sub>2 </sub>is the yaw angle for the platform at first position <b>808</b>, {right arrow over (t)}<sub>1P </sub>is the vector from first position <b>808</b> to point <b>818</b>, and {right arrow over (t)}<sub>2P</sub>, is the vector from second position <b>810</b> to point <b>818</b>.</p>
<p id="p-0129" num="0128">In this illustrative example, F is solved for a set of points similar to point <b>818</b> in scene <b>806</b> to form a set of conditions F for the set of related points in image pair <b>800</b>. This set of conditions is expressed by the equations:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>{right arrow over (f)}</i><sup>T</sup><i>=[&#x2212;F</i><sub>1</sub><i>&#x2212;F</i><sub>2</sub><i>&#x25cf;&#x25cf;F</i><sub>P</sub><i>&#x25cf;&#x2212;F</i><sub>N</sub>], and&#x2003;&#x2003;(4)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>{right arrow over (f)}=A{right arrow over (v)}+B{right arrow over (v)}.</i>&#x2003;&#x2003;(5)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
In this equation, T indicates a transpose, {right arrow over (f)} is the set of conditions F, A is a matrix, {right arrow over (v)} is a vector, B is a matrix, and {right arrow over (&#x394;)} is a vector.
</p>
<p id="p-0130" num="0129">In this illustrative example, {right arrow over (v)} represents the errors that may be present after first image <b>802</b> and second image <b>804</b> are adjusted relative to each other. In particular, these errors are residual errors from the performing of the least squares algorithm. In this illustrative, example, {right arrow over (v)} is partitioned by the following equation:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>{right arrow over (v)}</i><sup>T</sup><i>==[{right arrow over (v)}</i><sub>1</sub><sup>T</sup><i>{right arrow over (v)}</i><sub>2</sub><sup>T </sup><i>&#x25cf;&#x25cf;{right arrow over (v)}</i><sub>P</sub><sup>T</sup><i>&#x25cf;{right arrow over (v)}</i><sub>N</sub><sup>T</sup>]&#x2003;&#x2003;(6)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
In this equation, each entry in {right arrow over (v)}<sub>P</sub>, is further partitioned to provide residual errors for each related point, such as related point <b>817</b>. For example, each entry of {right arrow over (v)}<sub>P</sub>, may be partitioned into the following vector:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>{right arrow over (v)}</i><sub>P</sub><i>==[v</i><sub>Px1 </sub><i>v</i><sub>Py1 </sub><i>v</i><sub>Px2 </sub><i>v</i><sub>Py2</sub>]&#x2003;&#x2003;(6)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where (v<sub>Px1</sub>, v<sub>Py1</sub>) is the residual error for related point <b>817</b> on first image <b>802</b>, and (v<sub>Px2</sub>, v<sub>Py2</sub>) is the residual error for related point <b>817</b> on second image <b>804</b>.
</p>
<p id="p-0131" num="0130">In this depicted example, {right arrow over (&#x394;)} represents the adjustments that must be made to second image <b>804</b>. These adjustments also may be made to the ground coordinates of point <b>818</b>. In this example, {right arrow over (&#x394;)} is partitioned as follows:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>{right arrow over (&#x394;)}<sup>T</sup>=[{right arrow over (&#x394;)}<sub>1</sub><sup>T</sup><img id="CUSTOM-CHARACTER-00001" he="2.79mm" wi="0.68mm" file="US08624959-20140107-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/>{right arrow over (&#x394;)}<sub>2</sub>]<sup>T</sup>&#x2003;&#x2003;(8)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
Vector {right arrow over (&#x394;)}<sub>1 </sub>represents corrections for second position <b>810</b> and second orientation <b>811</b> in second image <b>804</b>.
<br/>
Vector {right arrow over (&#x394;)}<sub>2 </sub>represents corrections to the ground coordinates of point <b>818</b>.
<br/>
Further, {right arrow over (&#x394;)}<sup>T </sup>is partitioned by a combination of the following three equations:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>{right arrow over (&#x394;)}<sub>1</sub><i>==[&#x3b4;r</i><sub>2y </sub><i>&#x3b4;r</i><sub>2z </sub>&#x3b4;&#x3c9;<sub>2 </sub>&#x3b4;&#x3c6;<sub>2 </sub>&#x3b4;&#x3ba;<sub>2</sub>]&#x2003;&#x2003;(9)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>{right arrow over (&#x394;)}<sub>2</sub><sup>T</sup><i>==[{right arrow over (&#x3b4;t)}</i><sub>1</sub><sup>T </sup><i>{right arrow over (&#x3b4;t)}</i><sub>2</sub><sup>T</sup><i>&#x25cf;&#x25cf;{right arrow over (&#x3b4;t)}</i><sub>P</sub><sup>T</sup><i>&#x25cf;&#x25cf;&#x25cf;{right arrow over (&#x3b4;t)}</i><sub>N</sub><sup>T</sup>]&#x2003;&#x2003;(10)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>{right arrow over (&#x3b4;t)}</i><sub>P</sub><sup>T</sup><i>=[{right arrow over (&#x3b4;t)}</i><sub>Px </sub><i>{right arrow over (&#x3b4;t)}</i><sub>Py </sub><i>{right arrow over (&#x3b4;t)}</i><sub>Pz</sub><i>],P=</i>1<i>,N.</i>&#x2003;&#x2003;(11)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0132" num="0131">The adjustments that are to be made to second image <b>804</b> are solved by performing a least squares adjustment algorithm using the following set of equations:</p>
<p id="p-0133" num="0132">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mrow>
            <msub>
              <mover>
                <mi>&#x394;</mi>
                <mo>-&#x3e;</mo>
              </mover>
              <mi>k</mi>
            </msub>
            <mo>=</mo>
            <mrow>
              <msup>
                <mrow>
                  <mo>[</mo>
                  <mrow>
                    <msup>
                      <mrow>
                        <msubsup>
                          <mi>B</mi>
                          <mi>k</mi>
                          <mi>T</mi>
                        </msubsup>
                        <mo>&#x2061;</mo>
                        <mrow>
                          <mo>(</mo>
                          <mrow>
                            <msub>
                              <mi>A</mi>
                              <mi>k</mi>
                            </msub>
                            <mo>&#x2062;</mo>
                            <msubsup>
                              <mi>A</mi>
                              <mi>k</mi>
                              <mi>T</mi>
                            </msubsup>
                          </mrow>
                          <mo>)</mo>
                        </mrow>
                      </mrow>
                      <mrow>
                        <mo>-</mo>
                        <mn>1</mn>
                      </mrow>
                    </msup>
                    <mo>&#x2062;</mo>
                    <msub>
                      <mi>B</mi>
                      <mi>k</mi>
                    </msub>
                  </mrow>
                  <mo>]</mo>
                </mrow>
                <mrow>
                  <mo>-</mo>
                  <mn>1</mn>
                </mrow>
              </msup>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>[</mo>
                <mrow>
                  <msup>
                    <mrow>
                      <msubsup>
                        <mi>B</mi>
                        <mi>k</mi>
                        <mi>T</mi>
                      </msubsup>
                      <mo>&#x2061;</mo>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <msub>
                            <mi>A</mi>
                            <mi>k</mi>
                          </msub>
                          <mo>&#x2062;</mo>
                          <msubsup>
                            <mi>A</mi>
                            <mi>k</mi>
                            <mi>T</mi>
                          </msubsup>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                    <mrow>
                      <mo>-</mo>
                      <mn>1</mn>
                    </mrow>
                  </msup>
                  <mo>&#x2062;</mo>
                  <msub>
                    <mover>
                      <mi>f</mi>
                      <mo>-&#x3e;</mo>
                    </mover>
                    <mi>k</mi>
                  </msub>
                </mrow>
                <mo>]</mo>
              </mrow>
            </mrow>
          </mrow>
          <mo>,</mo>
          <mstyle>
            <mtext>
</mtext>
          </mstyle>
          <mo>&#x2062;</mo>
          <mrow>
            <msub>
              <mover>
                <mi>v</mi>
                <mo>-&#x3e;</mo>
              </mover>
              <mi>k</mi>
            </msub>
            <mo>=</mo>
            <mrow>
              <msup>
                <mrow>
                  <msubsup>
                    <mi>A</mi>
                    <mi>k</mi>
                    <mi>T</mi>
                  </msubsup>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <msub>
                        <mi>A</mi>
                        <mi>k</mi>
                      </msub>
                      <mo>&#x2062;</mo>
                      <msubsup>
                        <mi>A</mi>
                        <mi>k</mi>
                        <mi>T</mi>
                      </msubsup>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mrow>
                  <mo>-</mo>
                  <mn>1</mn>
                </mrow>
              </msup>
              <mo>&#x2062;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <msub>
                    <mover>
                      <mi>f</mi>
                      <mo>-&#x3e;</mo>
                    </mover>
                    <mi>k</mi>
                  </msub>
                  <mo>-</mo>
                  <mrow>
                    <msub>
                      <mi>B</mi>
                      <mi>k</mi>
                    </msub>
                    <mo>&#x2062;</mo>
                    <msub>
                      <mover>
                        <mi>&#x394;</mi>
                        <mo>-&#x3e;</mo>
                      </mover>
                      <mi>k</mi>
                    </msub>
                  </mrow>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
          <mo>,</mo>
          <mi>and</mi>
        </mrow>
        <mo>&#x2062;</mo>
        <mstyle>
          <mtext>
</mtext>
        </mstyle>
        <mo>&#x2062;</mo>
        <mrow>
          <msub>
            <mover>
              <mi>m</mi>
              <mo>-&#x3e;</mo>
            </mover>
            <mn>2</mn>
          </msub>
          <mo>=</mo>
          <mrow>
            <msub>
              <mover>
                <mi>m</mi>
                <mo>-&#x3e;</mo>
              </mover>
              <mn>20</mn>
            </msub>
            <mo>+</mo>
            <mrow>
              <munderover>
                <mo>&#x2211;</mo>
                <mrow>
                  <mi>j</mi>
                  <mo>=</mo>
                  <mn>1</mn>
                </mrow>
                <mi>k</mi>
              </munderover>
              <mo>&#x2062;</mo>
              <mrow>
                <msub>
                  <mover>
                    <mi>&#x394;</mi>
                    <mo>-&#x3e;</mo>
                  </mover>
                  <msub>
                    <mn>1</mn>
                    <mi>j</mi>
                  </msub>
                </msub>
                <mo>.</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>12</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
In this set of equations, {right arrow over (m)}<sub>2 </sub>is the adjustment that is to be made for second position <b>810</b> in second image <b>804</b>, {right arrow over (m)}<sub>20 </sub>is the adjustment that is to be made for second orientation <b>811</b>, and k is the kth iteration for performing the least squares algorithm. In these examples, less than five iterations are performed.
</p>
<p id="p-0134" num="0133">With reference now to <figref idref="DRAWINGS">FIG. 9</figref>, an illustration of a matrix is depicted in accordance with an illustrative embodiment. In this illustrative example, matrix <b>900</b> is an example of an implementation for the matrix A as described by equation 6.</p>
<p id="p-0135" num="0134">Matrix <b>900</b> is an N&#xd7;4N matrix, where N is the number of points in the set of points being used to make the adjustment to image pair <b>800</b> in <figref idref="DRAWINGS">FIG. 8</figref>. Matrix <b>900</b> has entries <b>901</b> that fill rows <b>902</b> and columns <b>904</b>.</p>
<p id="p-0136" num="0135">In this illustrative example, entries <b>901</b> are filled by taking the partial derivatives of the condition F as described in equation 4, with respect to x<sub>P1</sub>, y<sub>p1</sub>, x<sub>P1</sub>, and y<sub>P2</sub>. Diagonal entries <b>905</b> of entries <b>901</b> are populated by the following partial derivatives:</p>
<p id="p-0137" num="0136">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <msub>
            <mi>a</mi>
            <mi>PP</mi>
          </msub>
          <mo>=</mo>
          <mrow>
            <mo>[</mo>
            <mrow>
              <mfrac>
                <mrow>
                  <mo>&#x2202;</mo>
                  <msub>
                    <mi>F</mi>
                    <mi>P</mi>
                  </msub>
                </mrow>
                <mrow>
                  <mo>&#x2202;</mo>
                  <msub>
                    <mi>x</mi>
                    <mrow>
                      <mi>P</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <mn>1</mn>
                    </mrow>
                  </msub>
                </mrow>
              </mfrac>
              <mo>&#x2062;</mo>
              <mfrac>
                <mrow>
                  <mo>&#x2202;</mo>
                  <msub>
                    <mi>F</mi>
                    <mi>P</mi>
                  </msub>
                </mrow>
                <mrow>
                  <mo>&#x2202;</mo>
                  <msub>
                    <mi>y</mi>
                    <mrow>
                      <mi>P</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <mn>1</mn>
                    </mrow>
                  </msub>
                </mrow>
              </mfrac>
              <mo>&#x2062;</mo>
              <mfrac>
                <mrow>
                  <mo>&#x2202;</mo>
                  <msub>
                    <mi>F</mi>
                    <mi>P</mi>
                  </msub>
                </mrow>
                <mrow>
                  <mo>&#x2202;</mo>
                  <msub>
                    <mi>x</mi>
                    <mrow>
                      <mi>P</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <mn>2</mn>
                    </mrow>
                  </msub>
                </mrow>
              </mfrac>
              <mo>&#x2062;</mo>
              <mfrac>
                <mrow>
                  <mo>&#x2202;</mo>
                  <msub>
                    <mi>F</mi>
                    <mi>P</mi>
                  </msub>
                </mrow>
                <mrow>
                  <mo>&#x2202;</mo>
                  <msub>
                    <mi>y</mi>
                    <mi>P2</mi>
                  </msub>
                </mrow>
              </mfrac>
            </mrow>
            <mo>]</mo>
          </mrow>
        </mrow>
        <mo>,</mo>
        <mrow>
          <mi>P</mi>
          <mo>=</mo>
          <mn>1</mn>
        </mrow>
        <mo>,</mo>
        <mi>N</mi>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>13</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0138" num="0137">With reference now to <figref idref="DRAWINGS">FIG. 10</figref>, an illustration of a matrix is depicted in accordance with an illustrative embodiment. In this illustrative example, matrix <b>1000</b> is an example of an implementation for the matrix B as described by equation 6.</p>
<p id="p-0139" num="0138">Matrix <b>1000</b> is an N&#xd7;(5+3N) matrix, where N is the number of points in the set of points being used to make the adjustment to image pair <b>800</b> in <figref idref="DRAWINGS">FIG. 8</figref>. Matrix <b>1000</b> has entries <b>1001</b> that fill rows <b>1002</b> and columns <b>1004</b>.</p>
<p id="p-0140" num="0139">In this illustrative example, entries <b>1001</b> are filled by taking the partial derivatives of the condition F as described in equation 4. These partial derivatives are expressed in the following two equations:</p>
<p id="p-0141" num="0140">
<maths id="MATH-US-00003" num="00003">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <msub>
            <mi>B</mi>
            <mi>P</mi>
          </msub>
          <mo>=</mo>
          <mrow>
            <mo>[</mo>
            <mrow>
              <mfrac>
                <mrow>
                  <mo>&#x2202;</mo>
                  <msub>
                    <mi>F</mi>
                    <mi>P</mi>
                  </msub>
                </mrow>
                <mrow>
                  <mo>&#x2202;</mo>
                  <msub>
                    <mover>
                      <mi>r</mi>
                      <mo>-&#x3e;</mo>
                    </mover>
                    <mrow>
                      <mn>2</mn>
                      <mo>&#x2062;</mo>
                      <mi>y</mi>
                    </mrow>
                  </msub>
                </mrow>
              </mfrac>
              <mo>&#x2062;</mo>
              <mfrac>
                <mrow>
                  <mo>&#x2202;</mo>
                  <msub>
                    <mi>F</mi>
                    <mi>P</mi>
                  </msub>
                </mrow>
                <mrow>
                  <mo>&#x2202;</mo>
                  <msub>
                    <mover>
                      <mi>r</mi>
                      <mo>-&#x3e;</mo>
                    </mover>
                    <mrow>
                      <mn>2</mn>
                      <mo>&#x2062;</mo>
                      <mi>z</mi>
                    </mrow>
                  </msub>
                </mrow>
              </mfrac>
              <mo>&#x2062;</mo>
              <mfrac>
                <mrow>
                  <mo>&#x2202;</mo>
                  <msub>
                    <mi>F</mi>
                    <mi>P</mi>
                  </msub>
                </mrow>
                <mrow>
                  <mo>&#x2202;</mo>
                  <msub>
                    <mi>&#x3c9;</mi>
                    <mn>2</mn>
                  </msub>
                </mrow>
              </mfrac>
              <mo>&#x2062;</mo>
              <mfrac>
                <mrow>
                  <mo>&#x2202;</mo>
                  <msub>
                    <mi>F</mi>
                    <mi>P</mi>
                  </msub>
                </mrow>
                <mrow>
                  <mo>&#x2202;</mo>
                  <msub>
                    <mi>&#x3d5;</mi>
                    <mn>2</mn>
                  </msub>
                </mrow>
              </mfrac>
              <mo>&#x2062;</mo>
              <mfrac>
                <mrow>
                  <mo>&#x2202;</mo>
                  <msub>
                    <mi>F</mi>
                    <mi>P</mi>
                  </msub>
                </mrow>
                <mrow>
                  <mo>&#x2202;</mo>
                  <msub>
                    <mi>&#x3ba;</mi>
                    <mn>2</mn>
                  </msub>
                </mrow>
              </mfrac>
            </mrow>
            <mo>]</mo>
          </mrow>
        </mrow>
        <mo>,</mo>
        <mrow>
          <mi>P</mi>
          <mo>=</mo>
          <mn>1</mn>
        </mrow>
        <mo>,</mo>
        <mi>N</mi>
        <mo>,</mo>
        <mi>and</mi>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>14</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <msub>
            <mi>B</mi>
            <mi>CP</mi>
          </msub>
          <mo>=</mo>
          <mrow>
            <mrow>
              <mrow>
                <mo>[</mo>
                <mrow>
                  <mfrac>
                    <mrow>
                      <mo>&#x2202;</mo>
                      <msub>
                        <mi>F</mi>
                        <mi>P</mi>
                      </msub>
                    </mrow>
                    <mrow>
                      <mo>&#x2202;</mo>
                      <msub>
                        <mi>t</mi>
                        <mi>Px</mi>
                      </msub>
                    </mrow>
                  </mfrac>
                  <mo>&#x2062;</mo>
                  <mfrac>
                    <mrow>
                      <mo>&#x2202;</mo>
                      <msub>
                        <mi>F</mi>
                        <mi>P</mi>
                      </msub>
                    </mrow>
                    <mrow>
                      <mo>&#x2202;</mo>
                      <msub>
                        <mi>t</mi>
                        <mi>Py</mi>
                      </msub>
                    </mrow>
                  </mfrac>
                  <mo>&#x2062;</mo>
                  <mfrac>
                    <mrow>
                      <mo>&#x2202;</mo>
                      <msub>
                        <mi>F</mi>
                        <mi>P</mi>
                      </msub>
                    </mrow>
                    <mrow>
                      <mo>&#x2202;</mo>
                      <msub>
                        <mi>t</mi>
                        <mi>Pz</mi>
                      </msub>
                    </mrow>
                  </mfrac>
                </mrow>
                <mo>]</mo>
              </mrow>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.8em" height="0.8ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mi>P</mi>
            </mrow>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
        </mrow>
        <mo>,</mo>
        <mrow>
          <mi>N</mi>
          <mo>.</mo>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>15</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0142" num="0141">With reference now to <figref idref="DRAWINGS">FIG. 11</figref>, an illustration of selection of a portion of an adjusted image pair for viewing is depicted in accordance with an illustrative embodiment. In this illustrative example, adjusted image pair <b>1100</b> is depicted after a first adjustment, such as adjustment <b>228</b> in <figref idref="DRAWINGS">FIG. 2</figref>, has been implemented. In this illustrative example, adjustment <b>228</b> in <figref idref="DRAWINGS">FIG. 2</figref> may be made to adjusted image pair <b>1100</b> by computer system <b>204</b> in <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0143" num="0142">Adjusted image pair <b>1100</b> includes first adjusted image <b>1102</b> and second adjusted image <b>1104</b>.</p>
<p id="p-0144" num="0143">First adjusted image <b>1102</b> and second adjusted image <b>1104</b> are images of scene <b>1106</b>. Scene <b>1106</b> is a region of land in this example. First adjusted image <b>1102</b> has position <b>1108</b>, and second adjusted image <b>1104</b> has position <b>1110</b>. Distance <b>1111</b> is the distance between first adjusted position <b>1108</b> and second adjusted position <b>1110</b>.</p>
<p id="p-0145" num="0144">As depicted in this example, scene <b>1106</b> has coordinate system <b>1112</b> with X1 axis <b>1114</b>, Y1 axis <b>1116</b>, and Z1 axis <b>1118</b>. First adjusted image <b>1102</b> has coordinate system <b>1120</b> with X2 axis <b>1122</b>, Y2 axis <b>1124</b>, and Z2 axis <b>1126</b>. Second adjusted image <b>1104</b> has coordinate system <b>1128</b> with X3 axis <b>1130</b>, Y3 axis <b>1132</b>, and Z3 axis <b>1134</b>. Vector <b>1131</b> is the position vector for position <b>1108</b> with respect to coordinate system <b>1112</b> for scene <b>1106</b>. Vector <b>1133</b> is the position vector for position <b>1110</b> with respect to coordinate system <b>1112</b> for scene <b>1106</b>.</p>
<p id="p-0146" num="0145">In this illustrative example, a portion of adjusted image pair <b>1100</b> is selected for viewing. In this example, axis <b>1135</b> passes through position <b>1108</b> and position <b>1110</b>. Plane <b>1136</b> rotates around axis <b>1135</b>. As plane <b>1136</b> rotates around axis <b>1135</b>, plane <b>1136</b> intersects scene <b>1106</b>, first adjusted image <b>1102</b>, and second adjusted image <b>1104</b>. In this depicted example, the intersection of plane <b>1136</b> with first adjusted image <b>1102</b> and second adjusted image <b>1104</b> generates a number of common points between adjusted image pair <b>1100</b>.</p>
<p id="p-0147" num="0146">For example, as plane <b>1136</b> passes through first adjusted image <b>1102</b>, a line of points in first adjusted image <b>1102</b> is formed. This line of points is line <b>1138</b>. In a similar manner, as plane <b>1136</b> passes through second adjusted image <b>1104</b>, a line of points in second adjusted image <b>1104</b> is formed. This line of points is line <b>1140</b>. Line <b>1138</b> and line <b>1140</b> in first adjusted image <b>1102</b> and in second adjusted image <b>1104</b>, respectively, have a number of common points. In other words, line <b>1138</b> and line <b>1140</b> share a number of common points. As plane <b>1136</b> passes through adjusted image pair <b>1100</b>, the number of common points is identified to select a portion of adjusted image pair <b>1100</b> for viewing. The selected portion of adjusted image pair <b>1100</b> has differences that allow depth to be perceived by a viewer.</p>
<p id="p-0148" num="0147">The selected portion of adjusted image pair <b>1100</b> may then be sent to a display system, such as display system <b>248</b> in <figref idref="DRAWINGS">FIG. 2</figref>, for viewing as a stereoscopic image.</p>
<p id="p-0149" num="0148">With reference now to <figref idref="DRAWINGS">FIG. 12</figref>, a flowchart of a method for processing a video data stream is depicted in accordance with an illustrative embodiment. The process illustrated in <figref idref="DRAWINGS">FIG. 12</figref> may be implemented in video imaging environment <b>200</b> in <figref idref="DRAWINGS">FIG. 2</figref>. In particular, the process may be implemented using video camera system <b>202</b> and computer system <b>204</b> in <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0150" num="0149">The process begins by receiving a video data stream from a video camera system (operation <b>1200</b>). In these illustrative examples, the video camera system is a monoscopic video camera that is stationary on a platform. In other words, the monoscopic video camera does not move on the platform. The video data stream comprises a plurality of images of a scene. In this illustrative example, the video camera system may be associated with the platform. The process then selects a plurality of image pairs from the plurality of images (operation <b>1202</b>). A first image of a first area in the scene overlaps a second image of a second area in the scene in each image pair in the plurality of image pairs.</p>
<p id="p-0151" num="0150">Thereafter, the process adjusts each image pair in the plurality of image pairs to form a plurality of adjusted image pairs (operation <b>1204</b>). The plurality of adjusted image pairs is configured to provide a perception of depth for the scene when presented as a video. The plurality of image pairs may be adjusted using, for example, adjustment <b>228</b> in <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0152" num="0151">The process then displays the plurality of adjusted image pairs on a display system to present the video (operation <b>1206</b>), with the process terminating thereafter. In operation <b>1206</b>, the perception of depth for the scene is provided to a viewer of the video. The video is a stereoscopic video in these examples. In these illustrative examples, a plurality of stereoscopic images forms the stereoscopic video of the scene to provide information, such as the perception of depth for the scene.</p>
<p id="p-0153" num="0152">With reference now to <figref idref="DRAWINGS">FIG. 13</figref>, a flowchart of a method for processing a video data stream is depicted in accordance with an illustrative embodiment. The process illustrated in <figref idref="DRAWINGS">FIG. 13</figref> may be implemented in video imaging environment <b>200</b> in <figref idref="DRAWINGS">FIG. 2</figref>. In particular, the process may be implemented using video camera system <b>202</b> and computer system <b>204</b> in <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0154" num="0153">The process begins by receiving a video data stream from a video camera system (operation <b>1300</b>). The video camera system may be associated with a platform moving over a scene. The video data stream may comprise a plurality of images of the scene. The process then selects a plurality of image pairs from the plurality of images in the video data stream (operation <b>1302</b>). Each image pair in the plurality of image pairs may be selected such that a first image in the image pair overlaps a second image in the image pair with a desired overlap.</p>
<p id="p-0155" num="0154">Thereafter, the process uses the first position and a first orientation for the first image in each image pair in the plurality of image pairs (operation <b>1304</b>). In operation <b>1304</b>, the first position and the first orientation may be interpolated based upon position and attitude determined using a least squares adjustment of the first image pair selected in the sequence. The process then adjusts a second position and a second orientation of the second image relative to the first position and the first orientation of the first image in each image pair in the plurality of image pairs to form a plurality of adjusted image pairs (operation <b>1306</b>). In these examples, the first position, the first orientation, the second position, and the second orientation are for a platform.</p>
<p id="p-0156" num="0155">Thereafter, the process selects a portion of each adjusted image pair in the plurality of adjusted image pairs using a number of common points in each adjusted image pair to form the plurality of adjusted image pairs (operation <b>1308</b>). The process executed in operation <b>1308</b> is performed using epipolar warping in these illustrative examples.</p>
<p id="p-0157" num="0156">The process then displays the plurality of adjusted image pairs on a display system to present the video (operation <b>1310</b>), with the process terminating thereafter. The perception of depth for the scene is provided to a viewer of the video in operation <b>1310</b>. The video is a stereoscopic video. In this illustrative example, a plurality of stereoscopic images forms the stereoscopic video of the scene to provide additional information that may not be available with monoscopic video. In these examples, the additional information includes a perception of depth for the scene. The perception of depth for the scene may include, for example, a depth between objects in the scene, a depth between different parts of an object in the scene, and/or other suitable information including depth in the scene.</p>
<p id="p-0158" num="0157">The flowcharts and block diagrams in the different depicted embodiments illustrate the architecture, functionality, and operation of some possible implementations of apparatus and methods in different illustrative embodiments. In this regard, each block in the flowcharts or block diagrams may represent a module, segment, function, and/or a portion of an operation or step. In some alternative implementations, the function or functions noted in the blocks may occur out of the order noted in the figures. For example, in some cases, two blocks shown in succession may be executed substantially concurrently, or the blocks may sometimes be executed in the reverse order, depending upon the functionality involved. In addition, operations not shown may be included.</p>
<p id="p-0159" num="0158">Thus, the different advantageous embodiments provide a method and apparatus for processing a video data stream. The video data stream is received from a video camera system. The video data stream comprises a plurality of images of a scene. A plurality of image pairs is selected from the plurality of images. A first image of a first area in the scene overlaps a second image of a second area in the scene in each image pair in the plurality of image pairs. Each image pair in the plurality of image pairs is adjusted to form a plurality of adjusted image pairs. The plurality of adjusted image pairs is configured to provide a perception of depth for the scene when the plurality of adjusted image pairs is presented as a video.</p>
<p id="p-0160" num="0159">The different embodiments can take the form of an entirely hardware embodiment, an entirely software embodiment, or an embodiment containing both hardware and software elements. One embodiment may be implemented in software, which includes, but is not limited to, firmware, resident software, microcode, or other forms.</p>
<p id="p-0161" num="0160">Furthermore, the different embodiments can take the form of a computer program product accessible from a computer usable or computer readable medium providing program code for use by or in connection with a computer or any instruction execution system. For the purposes of this description, a computer usable or computer readable medium can be any tangible apparatus that can contain, store, communicate, propagate, or transport the program for use by or in connection with the instruction execution system, apparatus, or device.</p>
<p id="p-0162" num="0161">The medium can be an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system, or a propagation medium. The medium also may be a physical medium or a tangible medium on which computer readable program code can be stored. Examples of a computer readable medium include a semiconductor or solid state memory, magnetic tape, a removable computer diskette, a random access memory (RAM), a read-only memory (ROM), a rigid magnetic disk, an optical disk, or some other physical storage device configured to hold computer readable program code. Current examples of optical disks include compact disk-read only memory (CD-ROM), compact disk-read/write (CD-R/W), and DVD.</p>
<p id="p-0163" num="0162">Further, a computer storage medium may contain or store a computer readable program code such that when the computer readable program code is executed on a computer, the execution of this computer readable program code causes the computer to transmit another computer readable program code over a communications link. This communications link may use a medium that is, for example, without limitation, physical or wireless.</p>
<p id="p-0164" num="0163">A data processing system suitable for storing and/or executing program code will include at least one processor coupled directly or indirectly to memory elements through a system bus. The memory elements can include local memory employed during actual execution of the program code, bulk storage, and cache memories, which provide temporary storage of at least some program code in order to reduce the number of times code must be retrieved from bulk storage during execution.</p>
<p id="p-0165" num="0164">Input/output or I/O devices (including, but not limited to, keyboards, displays, pointing devices, etc.) can be coupled to the system either directly or through intervening I/O controllers. Network adapters also may be coupled to the data processing system to enable the data processing system to become coupled to other data processing systems or remote printers or storage devices through intervening private or public networks. Modems, cable modems, and Ethernet cards are just a few of the currently available types of network adapters.</p>
<p id="p-0166" num="0165">The description of the different illustrative embodiments has been presented for purposes of illustration and description, and it is not intended to be exhaustive or limited to the embodiments in the form disclosed. Many modifications and variations will be apparent to those of ordinary skill in the art. Further, different illustrative embodiments may provide different advantages as compared to other illustrative embodiments. The embodiment or embodiments selected are chosen and described in order to best explain the principles of the embodiments, the practical application, and to enable others of ordinary skill in the art to understand the disclosure for various embodiments with various modifications as are suited to the particular use contemplated.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08624959-20140107-M00001.NB">
<img id="EMI-M00001" he="22.27mm" wi="76.20mm" file="US08624959-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US08624959-20140107-M00002.NB">
<img id="EMI-M00002" he="7.03mm" wi="76.20mm" file="US08624959-20140107-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00003" nb-file="US08624959-20140107-M00003.NB">
<img id="EMI-M00003" he="17.61mm" wi="76.20mm" file="US08624959-20140107-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method for processing a video data stream, the method comprising:
<claim-text>receiving, by a data processing system, the video data stream from a single monoscopic video camera, wherein the video data stream is a monoscopic video data stream comprising a plurality of monoscopic images of a scene from a plurality of different directions with respect to the scene;</claim-text>
<claim-text>selecting, by the data processing system, a plurality of image pairs from the plurality of monoscopic images, wherein a first image of a first area in the scene overlaps a second image of a second area in the scene in each image pair in the plurality of image pairs, and wherein the first image and the second image in each image pair in the plurality of image pairs are from different directions with respect to the scene; and</claim-text>
<claim-text>adjusting, by the data processing system, the each image pair in the plurality of image pairs to form a plurality of adjusted image pairs, wherein the plurality of adjusted image pairs is configured to comprise a plurality of stereoscopic images to provide a perception of depth for the scene when the plurality of adjusted image pairs is presented as a video, and wherein each adjusted image in an adjusted image pair comprising a stereoscopic image in the plurality of stereoscopic images is from a different direction with respect to the scene.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step of adjusting the each image pair in the plurality of image pairs to form the plurality of adjusted image pairs comprises:
<claim-text>adjusting at least one of the first image and the second image in the each image pair.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising:
<claim-text>displaying the plurality of adjusted image pairs on a display system to present the video, wherein the perception of depth for the scene is provided to a viewer of the video.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the single monoscopic video camera is associated with a platform, and wherein the first image is taken from a first position of the platform and the second image is taken from a second position of the platform as the platform moves.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the step of adjusting the each image pair in the plurality of image pairs to form the plurality of adjusted image pairs comprises:
<claim-text>adjusting the second image relative to the first image in the each image pair in the plurality of image pairs based on a position and an orientation for the platform.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the step of adjusting the each image pair in the plurality of image pairs to form the plurality of adjusted image pairs further comprises:
<claim-text>selecting a portion of each adjusted image pair in the plurality of adjusted image pairs using a number of common points in the each adjusted image pair to form the plurality of adjusted image pairs.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the step of selecting the portion of the each adjusted image pair in the plurality of adjusted image pairs using the number of common points in the each adjusted image pair to form the plurality of adjusted images pairs is performed using epipolar warping.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step of selecting the plurality of image pairs from the plurality of monoscopic images comprises:
<claim-text>selecting the plurality of image pairs from the plurality of monoscopic images such that the first image overlaps the second image with a desired overlap in the each image pair in the plurality of image pairs, wherein the desired overlap between the first image and the second image provides an amount of image separation between the first image and the second image that is selected to take into account a separation of eyes of a viewer.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. An apparatus comprising:
<claim-text>a processor unit configured to receive a monoscopic video data stream from a single monoscopic video camera, wherein the monoscopic video data stream comprises a plurality of images for a scene from a plurality of different directions with respect to the scene and wherein the processor unit processes the monoscopic video data stream to form a stereoscopic video data stream that provides a perception of depth for the scene, wherein the stereoscopic video data stream comprises a plurality of stereoscopic images and wherein each of the plurality of stereoscopic images comprises a pair of images from different directions with respect to the scene; and</claim-text>
<claim-text>a display system configured to display a video using the stereoscopic video data stream of the scene generated by the processor unit.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The apparatus of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the processor unit is configured to select a plurality of image pairs from the plurality of images, wherein a first image of a first area in the scene overlaps a second image of a second area in the scene in each image pair in the plurality of image pairs, wherein an overlap between the first image and the second image provides an amount of image separation between the first image and the second image that is selected to take into account a separation of eyes of a viewer, and adjust at least one of the first image and the second image in the each image pair in the plurality of image pairs to form a plurality of adjusted image pairs, wherein the plurality of adjusted image pairs form the stereoscopic video data stream.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The apparatus of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the processor unit is further configured to adjust the second image relative to the first image in the each image pair in the plurality of image pairs based on a position and an orientation for a platform associated with the single monoscopic video camera.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The apparatus of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the processor unit is further configured to select a portion of each adjusted image pair in the plurality of adjusted image pairs using a number of common points in the each adjusted image pair to form the plurality of adjusted image pairs.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The apparatus of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the display system is configured to display the plurality of adjusted image pairs on the display system to present the video, wherein the perception of depth for the scene is provided to a viewer of the video.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The apparatus of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the single monoscopic video camera is associated with a platform, and wherein the plurality of images for the scene are taken while the platform moves the single monoscopic video camera over the scene.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. A computer program product comprising:
<claim-text>a computer readable storage device;</claim-text>
<claim-text>program instructions, stored on the computer readable storage device, for receiving a video data stream from a single monoscopic video camera, wherein the video data stream is a monoscopic video data stream comprising a plurality of monoscopic images of a scene from a plurality of different directions with respect to the scene;</claim-text>
<claim-text>program instructions, stored on the computer readable storage device, for selecting a plurality of image pairs from the plurality of monoscopic images, wherein a first image of a first area in the scene overlaps a second image of a second area in the scene in each image pair in the plurality of image pairs, and wherein the first image and the second image in each image pair in the plurality of image pairs are from different directions with respect to the scene; and</claim-text>
<claim-text>program instructions, stored on the computer readable storage device, for adjusting the each image pair in the plurality of image pairs to form a plurality of adjusted image pairs, wherein the plurality of adjusted image pairs is configured to comprise a plurality of stereoscopic images to provide a perception of depth for the scene when the plurality of adjusted image pairs is presented as a video, and wherein each adjusted image in an adjusted image pair comprising a stereoscopic image in the plurality of stereoscopic images is from a different direction with respect to the scene.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The computer program product of <claim-ref idref="CLM-00015">claim 15</claim-ref> further comprising:
<claim-text>program instructions, stored on the computer readable storage device, for displaying the plurality of adjusted image pairs to present the video on a display system.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The computer program product of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the program instructions, stored on the computer readable storage device, for selecting the plurality of image pairs from the plurality of monoscopic images further comprises:
<claim-text>program instructions, stored on the computer readable storage device, for selecting the plurality of image pairs from the plurality of monoscopic images such that the first image overlaps the second image with a desired overlap in the each image pair in the plurality of image pairs, wherein the desired overlap between the first image and the second image provides an amount of image separation between the first image and the second image that is selected to take into account a separation of eyes of a viewer.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The computer program product of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the program instructions, stored on the computer readable storage device, for adjusting the each image pair in the plurality of image pairs to form the plurality of adjusted image pairs further comprises:
<claim-text>program instructions, stored on the computer readable storage device, for adjusting the second image relative to the first image in the each image pair in the plurality of image pairs based on a position and an orientation for a platform associated with the single monoscopic video camera.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The computer program product of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the program instructions, stored on the computer readable storage device, for adjusting the each image pair in the plurality of image pairs to form the plurality of adjusted image pairs further comprises:
<claim-text>program instructions, stored on the computer readable storage device, for selecting a portion of each adjusted image pair in the plurality of adjusted image pairs using a number of common points in the each adjusted image pair to form the plurality of adjusted image pairs.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
