<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08624962-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08624962</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12364122</doc-number>
<date>20090202</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<us-term-of-grant>
<us-term-extension>923</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>13</main-group>
<subgroup>02</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>348 50</main-classification>
</classification-national>
<invention-title id="d2e53">Systems and methods for simulating three-dimensional virtual interactions from two-dimensional camera images</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5210604</doc-number>
<kind>A</kind>
<name>Carpenter</name>
<date>19930500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>358 93</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5365266</doc-number>
<kind>A</kind>
<name>Carpenter</name>
<date>19941100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>348 61</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5534917</doc-number>
<kind>A</kind>
<name>MacDougall</name>
<date>19960700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>348169</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5850352</doc-number>
<kind>A</kind>
<name>Moezzi et al.</name>
<date>19981200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>364514</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6151009</doc-number>
<kind>A</kind>
<name>Kanade et al.</name>
<date>20001100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>345113</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6166744</doc-number>
<kind>A</kind>
<name>Jaszlics et al.</name>
<date>20001200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>345435</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6191812</doc-number>
<kind>B1</kind>
<name>Tzidon et al.</name>
<date>20010200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>348140</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6292215</doc-number>
<kind>B1</kind>
<name>Vincent</name>
<date>20010900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>348169</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6411266</doc-number>
<kind>B1</kind>
<name>Maguire, Jr.</name>
<date>20020600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>345  8</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6678635</doc-number>
<kind>B2</kind>
<name>Tovinkere et al.</name>
<date>20040100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>702179</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6795068</doc-number>
<kind>B1</kind>
<name>Marks</name>
<date>20040900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345419</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6856324</doc-number>
<kind>B2</kind>
<name>Sauer et al.</name>
<date>20050200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>345633</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>6898307</doc-number>
<kind>B1</kind>
<name>Harrington</name>
<date>20050500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>382154</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>6950123</doc-number>
<kind>B2</kind>
<name>Martins</name>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>348157</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>6951515</doc-number>
<kind>B2</kind>
<name>Ohshima et al.</name>
<date>20051000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>463 31</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>6987885</doc-number>
<kind>B2</kind>
<name>Gonzalez-Banos et al.</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>382192</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>6990681</doc-number>
<kind>B2</kind>
<name>Wang et al.</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>725105</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>7044613</doc-number>
<kind>B2</kind>
<name>Debevee</name>
<date>20060500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>362 11</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>7058204</doc-number>
<kind>B2</kind>
<name>Hildreth et al.</name>
<date>20060600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>7139409</doc-number>
<kind>B2</kind>
<name>Paragios et al.</name>
<date>20061100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>7170492</doc-number>
<kind>B2</kind>
<name>Bell</name>
<date>20070100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>345158</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>7173672</doc-number>
<kind>B2</kind>
<name>Gibbs et al.</name>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>348589</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>7177861</doc-number>
<kind>B2</kind>
<name>Tovinkere et al.</name>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>707  3</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>7199793</doc-number>
<kind>B2</kind>
<name>Oh et al.</name>
<date>20070400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>345419</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>7209181</doc-number>
<kind>B2</kind>
<name>Kriegman</name>
<date>20070400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>348586</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>7227526</doc-number>
<kind>B2</kind>
<name>Hildreth et al.</name>
<date>20070600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>345156</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>7253832</doc-number>
<kind>B2</kind>
<name>Iwaki et al.</name>
<date>20070800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>348 50</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>7259747</doc-number>
<kind>B2</kind>
<name>Bell</name>
<date>20070800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>345156</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>7277571</doc-number>
<kind>B2</kind>
<name>Hara</name>
<date>20071000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>382154</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>7285047</doc-number>
<kind>B2</kind>
<name>Gelb et al.</name>
<date>20071000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>463 31</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>US</country>
<doc-number>7348963</doc-number>
<kind>B2</kind>
<name>Bell</name>
<date>20080300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>345156</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>US</country>
<doc-number>7348989</doc-number>
<kind>B2</kind>
<name>Stevens et al.</name>
<date>20080300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>345582</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00033">
<document-id>
<country>US</country>
<doc-number>7379566</doc-number>
<kind>B2</kind>
<name>Hildreth</name>
<date>20080500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>382107</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00034">
<document-id>
<country>US</country>
<doc-number>7385600</doc-number>
<kind>B2</kind>
<name>Marion</name>
<date>20080600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>345419</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00035">
<document-id>
<country>US</country>
<doc-number>7389591</doc-number>
<kind>B2</kind>
<name>Jaiswal et al.</name>
<date>20080600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification> 3336611</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00036">
<document-id>
<country>US</country>
<doc-number>7421093</doc-number>
<kind>B2</kind>
<name>Hildreth et al.</name>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00037">
<document-id>
<country>US</country>
<doc-number>7427996</doc-number>
<kind>B2</kind>
<name>Yonezawa et al.</name>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>345629</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00038">
<document-id>
<country>US</country>
<doc-number>7428542</doc-number>
<kind>B1</kind>
<name>Fink et al.</name>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>707100</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00039">
<document-id>
<country>US</country>
<doc-number>7430312</doc-number>
<kind>B2</kind>
<name>Gu</name>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>382154</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00040">
<document-id>
<country>US</country>
<doc-number>8072470</doc-number>
<kind>B2</kind>
<name>Marks</name>
<date>20111200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345632</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00041">
<document-id>
<country>US</country>
<doc-number>2002/0186314</doc-number>
<kind>A1</kind>
<name>Debevec</name>
<date>20021200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>348371</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00042">
<document-id>
<country>US</country>
<doc-number>2003/0012408</doc-number>
<kind>A1</kind>
<name>Bouguet et al.</name>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00043">
<document-id>
<country>US</country>
<doc-number>2003/0038813</doc-number>
<kind>A1</kind>
<name>Lu</name>
<date>20030200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>345582</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00044">
<document-id>
<country>US</country>
<doc-number>2003/0071810</doc-number>
<kind>A1</kind>
<name>Shoov et al.</name>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345420</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00045">
<document-id>
<country>US</country>
<doc-number>2004/0183775</doc-number>
<kind>A1</kind>
<name>Bell</name>
<date>20040900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>345156</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00046">
<document-id>
<country>US</country>
<doc-number>2004/0193313</doc-number>
<kind>A1</kind>
<name>Cornet et al.</name>
<date>20040900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>700231</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00047">
<document-id>
<country>US</country>
<doc-number>2005/0018045</doc-number>
<kind>A1</kind>
<name>Thomas et al.</name>
<date>20050100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>348157</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00048">
<document-id>
<country>US</country>
<doc-number>2005/0088407</doc-number>
<kind>A1</kind>
<name>Bell et al.</name>
<date>20050400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>345156</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00049">
<document-id>
<country>US</country>
<doc-number>2005/0089194</doc-number>
<kind>A1</kind>
<name>Bell</name>
<date>20050400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00050">
<document-id>
<country>US</country>
<doc-number>2005/0093889</doc-number>
<kind>A1</kind>
<name>Sauer et al.</name>
<date>20050500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>345633</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00051">
<document-id>
<country>US</country>
<doc-number>2005/0099414</doc-number>
<kind>A1</kind>
<name>Kaye et al.</name>
<date>20050500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345419</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00052">
<document-id>
<country>US</country>
<doc-number>2005/0110964</doc-number>
<kind>A1</kind>
<name>Bell et al.</name>
<date>20050500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>353122</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00053">
<document-id>
<country>US</country>
<doc-number>2005/0122308</doc-number>
<kind>A1</kind>
<name>Bell et al.</name>
<date>20050600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>345156</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00054">
<document-id>
<country>US</country>
<doc-number>2005/0162381</doc-number>
<kind>A1</kind>
<name>Bell et al.</name>
<date>20050700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>345156</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00055">
<document-id>
<country>US</country>
<doc-number>2005/0204287</doc-number>
<kind>A1</kind>
<name>Wang</name>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>715716</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00056">
<document-id>
<country>US</country>
<doc-number>2006/0074921</doc-number>
<kind>A1</kind>
<name>Lefevre</name>
<date>20060400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>707100</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00057">
<document-id>
<country>US</country>
<doc-number>2006/0188849</doc-number>
<kind>A1</kind>
<name>Shamaie</name>
<date>20060800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>434 85</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00058">
<document-id>
<country>US</country>
<doc-number>2006/0192782</doc-number>
<kind>A1</kind>
<name>Hildreth</name>
<date>20060800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>345473</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00059">
<document-id>
<country>US</country>
<doc-number>2006/0210112</doc-number>
<kind>A1</kind>
<name>Cohen et al.</name>
<date>20060900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00060">
<document-id>
<country>US</country>
<doc-number>2006/0250395</doc-number>
<kind>A1</kind>
<name>Kwon et al.</name>
<date>20061100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>345424</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00061">
<document-id>
<country>US</country>
<doc-number>2006/0258457</doc-number>
<kind>A1</kind>
<name>Brigham</name>
<date>20061100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>463 36</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00062">
<document-id>
<country>US</country>
<doc-number>2006/0260624</doc-number>
<kind>A1</kind>
<name>Schur et al.</name>
<date>20061100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>128898</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00063">
<document-id>
<country>US</country>
<doc-number>2007/0031005</doc-number>
<kind>A1</kind>
<name>Paragios et al.</name>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00064">
<document-id>
<country>US</country>
<doc-number>2007/0057940</doc-number>
<kind>A1</kind>
<name>Petschnigg et al.</name>
<date>20070300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>345419</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00065">
<document-id>
<country>US</country>
<doc-number>2007/0236485</doc-number>
<kind>A1</kind>
<name>Trepte</name>
<date>20071000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>345207</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00066">
<document-id>
<country>US</country>
<doc-number>2008/0018595</doc-number>
<kind>A1</kind>
<name>Hildreth et al.</name>
<date>20080100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>345156</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00067">
<document-id>
<country>US</country>
<doc-number>2008/0018668</doc-number>
<kind>A1</kind>
<name>Yamauchi</name>
<date>20080100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>345633</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00068">
<document-id>
<country>US</country>
<doc-number>2008/0030460</doc-number>
<kind>A1</kind>
<name>Hildreth et al.</name>
<date>20080200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>345156</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00069">
<document-id>
<country>US</country>
<doc-number>2008/0056536</doc-number>
<kind>A1</kind>
<name>Hildreth et al.</name>
<date>20080300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00070">
<document-id>
<country>US</country>
<doc-number>2008/0062123</doc-number>
<kind>A1</kind>
<name>Bell</name>
<date>20080300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>345156</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00071">
<document-id>
<country>US</country>
<doc-number>2008/0137913</doc-number>
<kind>A1</kind>
<name>Hildreth</name>
<date>20080600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>382107</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00072">
<document-id>
<country>US</country>
<doc-number>2008/0150890</doc-number>
<kind>A1</kind>
<name>Bell et al.</name>
<date>20080600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>345156</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00073">
<document-id>
<country>US</country>
<doc-number>2008/0150913</doc-number>
<kind>A1</kind>
<name>Bell et al.</name>
<date>20080600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>345175</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00074">
<document-id>
<country>US</country>
<doc-number>2008/0166022</doc-number>
<kind>A1</kind>
<name>Hildreth</name>
<date>20080700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>382107</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00075">
<document-id>
<country>US</country>
<doc-number>2008/0187178</doc-number>
<kind>A1</kind>
<name>Shamaie</name>
<date>20080800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00076">
<document-id>
<country>US</country>
<doc-number>2008/0199071</doc-number>
<kind>A1</kind>
<name>Gu</name>
<date>20080800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>382154</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00077">
<document-id>
<country>US</country>
<doc-number>2008/0205701</doc-number>
<kind>A1</kind>
<name>Shamaie et al.</name>
<date>20080800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00078">
<document-id>
<country>US</country>
<doc-number>2008/0218515</doc-number>
<kind>A1</kind>
<name>Fukushima et al.</name>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>345424</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00079">
<document-id>
<country>US</country>
<doc-number>2008/0219502</doc-number>
<kind>A1</kind>
<name>Shamaie</name>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00080">
<document-id>
<country>US</country>
<doc-number>2008/0235965</doc-number>
<kind>A1</kind>
<name>Jaiswal et al.</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification> 3336611</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00081">
<document-id>
<country>US</country>
<doc-number>2008/0252596</doc-number>
<kind>A1</kind>
<name>Bell et al.</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>345156</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00082">
<document-id>
<country>US</country>
<doc-number>2008/0267447</doc-number>
<kind>A1</kind>
<name>Kelusky et al.</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>382100</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00083">
<document-id>
<country>US</country>
<doc-number>2008/0273755</doc-number>
<kind>A1</kind>
<name>Hildreth</name>
<date>20081100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00084">
<document-id>
<country>US</country>
<doc-number>2010/0014781</doc-number>
<kind>A1</kind>
<name>Liu et al.</name>
<date>20100100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382285</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00085">
<document-id>
<country>DE</country>
<doc-number>100 40 805</doc-number>
<kind>A1</kind>
<date>20020300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-cpc-text>G06F 19/00</classification-cpc-text>
</us-citation>
<us-citation>
<patcit num="00086">
<document-id>
<country>WO</country>
<doc-number>WO97/46975</doc-number>
<kind>A1</kind>
<date>19971200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-cpc-text>G06T 17/50</classification-cpc-text>
</us-citation>
<us-citation>
<patcit num="00087">
<document-id>
<country>WO</country>
<doc-number>WO 02/39382</doc-number>
<kind>A2</kind>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-cpc-text>G06T 15/20</classification-cpc-text>
</us-citation>
<us-citation>
<patcit num="00088">
<document-id>
<country>WO</country>
<doc-number>WO 2006/112743</doc-number>
<kind>A1</kind>
<date>20061000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-cpc-text>G07F 17/02</classification-cpc-text>
</us-citation>
<us-citation>
<patcit num="00089">
<document-id>
<country>WO</country>
<doc-number>WO 2007/017597</doc-number>
<kind>A2</kind>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-cpc-text>G06T 15/00</classification-cpc-text>
</us-citation>
<us-citation>
<patcit num="00090">
<document-id>
<country>WO</country>
<doc-number>WO 2007/017600</doc-number>
<kind>A2</kind>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-cpc-text>H04N 5/262</classification-cpc-text>
</us-citation>
<us-citation>
<patcit num="00091">
<document-id>
<country>WO</country>
<doc-number>WO 2010/087729</doc-number>
<date>20100800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-cpc-text>H04N 13/00</classification-cpc-text>
</us-citation>
<us-citation>
<nplcit num="00092">
<othercit>Baker, et al., &#x201c;A Layered Approach to Stereo Recognition&#x201d;, Conference on Computer Vision and Pattern Recognition, Santa Barbara, CA; 8 pages, Jun. 23, 1998.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00093">
<othercit>Erdem, et al., &#x201c;Temporal Stabilization of Video Object Segmentation for 3D-TV Applications&#x201d;, Signal Processing: Image Commumication, vol. 20, No. 2; pp. 151-167, Feb. 1, 2005.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00094">
<othercit>Kanade, et al., &#x201c;Virtualized Reality: Constructing Virtual Worlds from Real Scenes&#x201d;, IEEE Multimedia, IEEE Service Center, New York, NY; vol. 4, No. 1; pp. 34-47, Jan. 1, 1997.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00095">
<othercit>Park, et al., &#x201c;Efficiently Capturing Object Contours for Non-Photorealistic Rendering&#x201d;, Image Analysis: 15<sup>th </sup>Scandinavian Conference, Lecture notes in Computer Science, Springer Berlin Heidelberg, pp. 442-451, Jun. 10, 2007.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00096">
<othercit>Vedula, et al., &#x201c;Modeling, Combining, and Rendering Dynamic Real-World Events from Image Sequences&#x201d;, The Robotics Institute, Carnegie Mellon University, Proceedings International Conference on Virtual Systems and Multimedia; vol. 1, pp, 326-332, Nov. 18, 1998.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00097">
<othercit>International PCT Search Report, PCT/PT2009/000064, 2 pages, Mar. 4, 2010.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00098">
<othercit>Seneca, &#x201c;A Falar Para O Boneco&#x201d;, Portugal Faz Bern Realidade Aumentada, with English translation, 5 pages, 2008.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00099">
<othercit>Lok, &#x201c;Online Model Reconstruction for Interactive Virtual Environments&#x201d;, University of North Carolina at Chapel Hill, Dept. of Computer Science, 5 pages, 1999.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00100">
<othercit>Sawhney et al., &#x201c;Video Flashlights&#x2014;Real Time Rendering of Multiple Videos for Immersive Model Visualization&#x201d;, Vision Technologies Laboratory, 12 pages, 2002.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00101">
<othercit>Lepetit et al., &#x201c;Monocular Model-Based 3D Tracking of Rigid Objects: A Survey&#x201d;, vol. 1, No. 1, pp. 1-89, 2005.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00102">
<othercit>Mascioni, &#x201c;Gesture-Based Digital Signage: A New Marketing Future&#x201d;, Media Coverage, Gesturetek II Corporate Information, http://www.gesturetek.com/newscenter/media.php?media=28, 3 pages, Oct. 30, 2008.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00103">
<othercit>Beck, &#x201c;CES: 3DV Systems Does Wii-Like Fun With No Controller&#x201d;, The Technology Chronicles: CES: 3DV Systems does Wii-Like Fun With No Controller, http://www.sfgate.com/cgi-bin/blogs/sfgate/detail?blogid=19%&#x26;entry<sub>&#x2014;</sub>id=23275, 2 pages, Oct. 30, 2008.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00104">
<othercit>Billinghurst, &#x201c;The MagicBook&#x2014;Moving Seamlessly Between Reality and Virtuality&#x201d;, Univ. of Washington, 3 pages, May/Jun. 2001.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00105">
<othercit>Fumanski et al., &#x201c;Augmented-Reality Visualizations Guided by Cognition: Perceptual Heuristics for Combining Visible and Obscured Information&#x201d;, HRL Laboratories, LLC, 10 pages, 2002.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00106">
<othercit>Line-Plane Intersection, From Wikipedia, the free encyclopedia, http://en. wikipedia.org/wiki/Line-plane<sub>&#x2014;</sub>intersection, 3 pages, Jan. 12, 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00107">
<othercit>Murph, Flapi: Ydreams' Augmented Reality Mascot, http://engadget.com/2008/04/19/flapi-ydreams-augmented-reality-mascot/, 8 pages, Apr. 19, 2008.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00108">
<othercit>Video Texturing on the ATI Rage 128 with Direct3D, AMD Developer Central, http://developer.amd.com/gpu/archive/Rage128SDKSampleCodeandDemos/VideoTextruingonRage128withDirect3D/Pages/de . . . , 2 pages, Nov. 17, 2008.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>4</number-of-claims>
<us-exemplary-claim>4</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>348 50</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>10</number-of-drawing-sheets>
<number-of-figures>11</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20100194863</doc-number>
<kind>A1</kind>
<date>20100805</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Lopes</last-name>
<first-name>Gon&#xe7;alo Cardoso</first-name>
<address>
<city>Lisbon</city>
<country>PT</country>
</address>
</addressbook>
<residence>
<country>PT</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>de Almeida</last-name>
<first-name>Andr&#xe9; Rui Soares Pereira</first-name>
<address>
<city>Almada</city>
<country>PT</country>
</address>
</addressbook>
<residence>
<country>PT</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>da Silva Fraz&#xe3;o</last-name>
<first-name>Jo&#xe3;o Pedro Gomes</first-name>
<address>
<city>Lisbon</city>
<country>PT</country>
</address>
</addressbook>
<residence>
<country>PT</country>
</residence>
</us-applicant>
<us-applicant sequence="004" app-type="applicant" designation="us-only">
<addressbook>
<last-name>de Almada</last-name>
<first-name>Ant&#xe3;o Bastos Carri&#xe7;o Vaz</first-name>
<address>
<city>Lisbon</city>
<country>PT</country>
</address>
</addressbook>
<residence>
<country>PT</country>
</residence>
</us-applicant>
<us-applicant sequence="005" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Cardoso</last-name>
<first-name>Nuno Ricardo Sequeira</first-name>
<address>
<city>Lisbon</city>
<country>PT</country>
</address>
</addressbook>
<residence>
<country>PT</country>
</residence>
</us-applicant>
<us-applicant sequence="006" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Franco</last-name>
<first-name>Ivan de Almeida Soares</first-name>
<address>
<city>Almada</city>
<country>PT</country>
</address>
</addressbook>
<residence>
<country>PT</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Lopes</last-name>
<first-name>Gon&#xe7;alo Cardoso</first-name>
<address>
<city>Lisbon</city>
<country>PT</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>de Almeida</last-name>
<first-name>Andr&#xe9; Rui Soares Pereira</first-name>
<address>
<city>Almada</city>
<country>PT</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>da Silva Fraz&#xe3;o</last-name>
<first-name>Jo&#xe3;o Pedro Gomes</first-name>
<address>
<city>Lisbon</city>
<country>PT</country>
</address>
</addressbook>
</inventor>
<inventor sequence="004" designation="us-only">
<addressbook>
<last-name>de Almada</last-name>
<first-name>Ant&#xe3;o Bastos Carri&#xe7;o Vaz</first-name>
<address>
<city>Lisbon</city>
<country>PT</country>
</address>
</addressbook>
</inventor>
<inventor sequence="005" designation="us-only">
<addressbook>
<last-name>Cardoso</last-name>
<first-name>Nuno Ricardo Sequeira</first-name>
<address>
<city>Lisbon</city>
<country>PT</country>
</address>
</addressbook>
</inventor>
<inventor sequence="006" designation="us-only">
<addressbook>
<last-name>Franco</last-name>
<first-name>Ivan de Almeida Soares</first-name>
<address>
<city>Almada</city>
<country>PT</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>King &#x26; Spalding L.L.P.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Ydreams&#x2014;Informatica, S.A. Ydreams</orgname>
<role>03</role>
<address>
<city>Caparica</city>
<country>PT</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Dennison</last-name>
<first-name>Jerry</first-name>
<department>2443</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A computer implemented method for incorporating a representation of a participant into a virtual 3D environment substantially in real-time is provided. An image including a participant is captured by a camera. A contour of the participant is automatically determined. Depth data is automatically associated with the participant contour. A first virtual 3D representation of the participant is automatically generated by extruding the participant contour based on the associated depth data. An interaction between the first virtual 3D representation of the participant and a second virtual 3D representation of a second object is displayed.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="261.70mm" wi="141.56mm" file="US08624962-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="265.09mm" wi="170.26mm" orientation="landscape" file="US08624962-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="235.88mm" wi="180.42mm" orientation="landscape" file="US08624962-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="267.89mm" wi="175.34mm" orientation="landscape" file="US08624962-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="252.14mm" wi="179.07mm" orientation="landscape" file="US08624962-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="159.60mm" wi="150.37mm" file="US08624962-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="229.45mm" wi="172.55mm" orientation="landscape" file="US08624962-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="243.33mm" wi="165.18mm" orientation="landscape" file="US08624962-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="262.81mm" wi="139.70mm" file="US08624962-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="262.81mm" wi="182.71mm" orientation="landscape" file="US08624962-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="237.32mm" wi="187.88mm" file="US08624962-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">TECHNICAL FIELD</heading>
<p id="p-0002" num="0001">The present disclosure relates generally to graphics processing techniques and more particularly to the use of a single camera in three dimensional graphics applications.</p>
<heading id="h-0002" level="1">BACKGROUND</heading>
<p id="p-0003" num="0002">Three-dimensional (3D) graphics processing and display are powerful technologies for capturing and retaining a person's attention. This is because 3D video productions and holographic displays capture a degree of realism or invoke greater perceived familiarity that a two-dimensional (2D) production or display simply cannot match. A prime application of 3D processing is in the areas of augmented reality (AR) and augmented virtuality (AV). These two different terms generally describe approaches to integrating information or context from a real environment with a virtual (or computer generated) one, but in AR the real elements predominate while in AV the virtual ones predominate. Similar to AR, many special effects in television programs and movies incorporate virtual images with computer generated ones. Alternatively, primarily animated programs that occasionally incorporate images, video, or the motions (e.g., animating a computer generated character based on the observed motions of an actor) of real actors or scenes are closer to AV. AR and AV differ from movies and most television in that they are typically performed in real-time with live participants.</p>
<p id="p-0004" num="0003">The term participant is used in the present disclosure because AR and AV applications are generally interactive in nature such that a person captured by the camera or cameras of such a system is invited to participate in some activity or experience. The real-time interface allows a participant to interact with a virtual scene or virtual 3D object and see those interactions as they happen. For the purposes of the present disclosure, the term AR will be used inclusively to refer to augmented reality and augmented virtuality applications.</p>
<p id="p-0005" num="0004">Existing techniques for capturing information about a participant and the participant's environment and then registering that information in 3D typically require specialized hardware components and/or complex setups of hardware components. The term registration is used to describe direct capture or inference of the 3D position and volume of a participant. The registration process is analogous to an architect adding a realistic model of an existing building to a 3D model of a downtown block to show a model of a planned building design in the context of its intended location. In this analogy, the entire model represents the virtual 3D scene with 3D representations of existing real buildings and a 3D representation of a purely virtual building. Registration in the context of AR includes direct measurement of a participant and entry of the participant's measurements, and also includes inferences based on one or more observations of the participant.</p>
<p id="p-0006" num="0005">In one example of a system for registering a participant in a virtual 3D scene, two or more cameras may be used in a carefully positioned arrangement (e.g., a stereoscopic camera rig) to enable computerized depth perception. Alternatively, a specialized time-of-flight camera provides a similar level of depth perception using special sensor and/or lighting elements. In yet another setup, a laser range-finder or radar setup may provide this depth information to augment video captured by a camera. Finally, additional information may be gathered from the scene by projecting (typically intermittently) a uniform grid across the camera's field of view (much like a high end digital camera auto focuses its lens) or by using fiducial markers (easily recognized reference markers) attached to the participant. The latter technique is used in medical imaging and studio production of movies and television.</p>
<p id="p-0007" num="0006">Each of these techniques has limitations such as a requirement for specialized hardware or complex arrangements of hardware, significant computer processing requirements that make real-time interaction expensive and difficult, and/or require a participant to first attach tags or markers to their body and/or clothing before participating in the AR environment.</p>
<heading id="h-0003" level="1">SUMMARY</heading>
<p id="p-0008" num="0007">In accordance with the teachings of the present disclosure, disadvantages and problems associated with the use of specialized hardware, complex arrangements of hardware, and undesirable participant constraints in virtual and augmented reality applications have been reduced.</p>
<p id="p-0009" num="0008">In certain embodiments, a computer implemented method for incorporating a representation of a participant into a virtual 3D environment substantially in real-time is provided. An image including a participant is captured by a camera. A contour of the participant is automatically determined. Depth data is automatically associated with the participant contour. A first virtual 3D representation of the participant is automatically generated by extruding the participant contour based on the associated depth data. An interaction between the first virtual 3D representation of the participant and a second virtual 3D representation of a second object is displayed.</p>
<p id="p-0010" num="0009">In certain embodiments, software embodied in tangible computer-readable media is provided. The software is executable by a processor to: receive an image captured by a camera, the image including a participant; determine a contour of the participant; associate depth data with the contour of the participant; generate a first virtual 3D representation of the participant by extruding the contour based on the associated depth data; determine an interaction between the first virtual 3D representation of the participant and a second virtual 3D representation of a second object based at least on the extruded contour; and cause a display of the interaction between the first virtual 3D representation of the participant and the second virtual 3D representation of a second object based at least on the extruded contour. The computer software is operable to perform such functions substantially in real-time.</p>
<p id="p-0011" num="0010">In certain embodiments, a computing system includes a processor, memory coupled to the processor, and an interactive media subsystem. The interactive media subsystem is enabled to receive an image captured by a camera, the image including a participant; automatically determine a contour of the participant; automatically associate depth data with the contour of the participant; automatically generate a first virtual 3D representation of the participant by extruding the contour based on the associated depth data; determine an interaction between the first virtual 3D representation of the participant and a second virtual 3D representation of a second object based at least on the extruded contour; and automatically cause a display of the interaction between the first virtual 3D representation of the participant and the second virtual 3D representation of a second object based at least on the extruded contour. The interactive media subsystem operates substantially in real-time.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0012" num="0011">A more complete understanding of the present embodiments and advantages thereof may be acquired by referring to the following description taken in conjunction with the accompanying drawings, in which like reference numbers indicate like features, and wherein:</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 1</figref> illustrates an interactive media system for capturing a video image of a participant and displaying a virtual 3D scene that incorporates a virtual 3D representation of the participant, according to an example embodiment of the present disclosure;</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 2</figref> illustrates representative elements of an image captured by a camera, including a contour of a participant, according to certain embodiments of the present disclosure;</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 3</figref> is a side view of the system of <figref idref="DRAWINGS">FIG. 1</figref> illustrating various geometric relationships between certain elements of the system, according to certain embodiments of the present disclosure;</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 4</figref> illustrates a 3D view of an example extrusion of participant contour by a uniform depth to generate a solid volume, according to one embodiment of the present disclosure;</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 5</figref> illustrates an example virtual 3D scene including a virtual 3D participant and a virtual ball, according to an example embodiment of the present disclosure;</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 6</figref> illustrates a top-down view illustrating various extrusion techniques for generating a virtual 3D participant and their impact on interactions between the generated virtual 3D participant and other virtual objects in a virtual 3D scene, according to certain embodiments of the present disclosure;</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 7</figref> illustrates a side view of a curved, non-uniform depth, bidirectional extrusion of a participant contour, according to certain embodiments of the present disclosure;</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 8</figref> illustrates an example method for generating a virtual 3D participant from a 2D camera image, generating a 3D virtual scene including the virtual 3D participant, and managing interactions between the virtual 3D participant and other virtual objects, in accordance with certain embodiments of the present disclosure;</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 9</figref> illustrates an example 3D virtual scene including various types of interactions between virtual 3D elements, according to certain embodiments of the present disclosure;</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 10</figref> illustrates the display of an example 3D virtual scene including several virtual 3D participants interacting with one external virtual 3D object, according to certain embodiments of the present disclosure; and</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 11</figref> illustrates the display of an example 3D virtual scene including several virtual 3D participants interacting with one external virtual 3D object, according to certain embodiments of the present disclosure.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0024" num="0023">Preferred embodiments and their advantages are best understood by reference to the figures below. However, the present disclosure may be more easily understood in the context of a high level description of certain embodiments.</p>
<p id="h-0006" num="0000">System Overview</p>
<p id="p-0025" num="0024">In general, the present disclosure describes systems and methods for recording a participant (e.g., a person) using a video camera, and generating and displaying a virtual 3D representation of the participant in real time. In certain embodiments, an image of a participant is captured by a single camera, the participant's contour (e.g., outline) is determined, depth data is associated with the contour, a 3D solid representation of the participant is generated by extruding the contour according to the depth data, and the generated 3D solid representation of the participant is introduced into a virtual 3D scene that may be displayed to the participant. In certain embodiments, the virtual 3D representation of the participant interacts with other virtual elements in the virtual 3D scene (e.g., as determined by a physics engine), and the interactions are displayed to the participant.</p>
<p id="p-0026" num="0025">One example embodiment includes an interactive media system including a video camera, a computer and a display. The system may be designed to capture and retain the attention of a person (referred to herein as a participant) in the field of view of the video camera. The video camera captures a series of images (e.g., frames) that include the participant. For each image, and in real-time, the system determines a 2D contour (e.g., outline) of the participant, extends the 2D contour into a 3D volume, and inserts that 3D representation into a virtual 3D scene. This virtual 3D scene may also include virtual 3D representations of other participants and/or other virtual 3D objects. For example, the virtual 3D scene may be a room with a virtual bubble machine throwing virtual bubbles into the air to float down to the floor and pop, and the 3D representation of the person inserted into the virtual 3D scene may interact with the virtual bubbles. The virtual 3D scene is displayed to the person in real time such that the participant move around within the field of view of the camera to interact with the virtual bubbles. The processing time from image capture to display of the virtual 3D scene may be substantially in real-time such that the participants can coordinate their movements relative to the virtual bubbles.</p>
<p id="p-0027" num="0026">In some embodiments, a true, real-time scheduling environment is utilized such that processing of the virtual 3D scene is guaranteed to occur as least as fast as the input frame rate from the camera. The input frame rate may be the camera frame rate or a selected subset of frames captured by the camera (e.g., only processing every fourth frame from the camera, which is capturing sixty frames per second, gives an input frame rate of 15 frames per second). In other embodiments, processing is performed substantially in real-time (e.g., through the use of a sufficiently high performance computer system <b>120</b>) wherein an occasional, intermittent inability to timely process an incoming camera frame may not interfere with a participant's interaction with the system. This occasional inability to process a frame in time is much like a dropped frame on a digital video stream of a sporting event, where the viewer might notice the dropped frame, but he can still see the progress of the game. In all cases, some time lag between the participant moving and the display reflecting the new position of the participant will exist, but if minimized this lag will not interfere with the participant's ability to interact with one or more virtual 3D objects.</p>
<p id="p-0028" num="0027">In this example &#x201c;virtual bubble&#x201d; scenario, various interactions are possible between the virtual 3D representation of a participant&#x2014;referred to herein as a &#x201c;virtual participant&#x201d;&#x2014;and a virtual bubble in the displayed virtual scene. For example, contact between the virtual participant and a virtual bubble may cause the virtual bubble to stick to the virtual participant, bounce, or pop. As another example, a virtual bubble may pass in front of or behind the virtual participant. If a virtual bubble passes in front of the virtual participant, it may completely or partially obscure the displayed view of the virtual participant and may cast a shadow on the virtual participant. Alternatively, a virtual bubble may distort a part of the image of the virtual participant rather than obscure it. If a virtual bubble passes behind the virtual participant, the virtual bubble may disappear on one side of the virtual participant and then reappear on the other side. As yet another example, if there are two or more virtual participants, a virtual bubble may pass between two of the virtual participants or pass by one and collide with another. The system may enforce the laws of physics&#x2014;or some approximation to allow real-time processing of each captured image&#x2014;to give the scene a more realistic appearance. For example, the system may include a physics engine to determine and manage interactions between virtual participants and other virtual 3D objects in the virtual 3D scene.</p>
<p id="p-0029" num="0028">As discussed above, a virtual participant is generated based on a 2D contour of a participant extracted from a captured video image. In some embodiments, the 3D position of the virtual participant is estimated based on an assumption that the participant is standing on the ground. With this assumption combined with some system calibration, the relative position of the participant's contour within the camera image can be converted into a 3D position. However, the single camera alone cannot capture the depth&#x2014;the third dimension&#x2014;of the participant. Thus, the system generates depth data for the participant, e.g., based on a system of depth rules. In some embodiments, the system assumes that each participant has a uniform depth and extrudes the contour to that depth.</p>
<p id="p-0030" num="0029">An analogous process would be to convert a picture of a person into a life-size, cardboard cut-out; make a duplicate cut-out; space the original and duplicate cut-outs a few inches apart; and enclose the space between the two cut-outs to form a 3D approximation of a person. The approximation is crude, but grossly captures the general dimensions of that person.</p>
<p id="p-0031" num="0030">In other embodiments, the system generates a non-uniform depth for the participant based on a particular algorithm or other depth rules. For example, the system may generate a tapered depth or a curved or rounded depth for a participant.</p>
<p id="p-0032" num="0031">As used herein, the term &#x201c;participant&#x201d; refers to a person or other physical object in the field of view of the video camera that is inserted into a virtual 3D scene, e.g., as discussed below. This term broadly includes, e.g., humans, animals, plants, machines, and any other moving or stationary objects. For example, if a person is pushing a cart through the video camera's field of view, the cart may be treated as a participant and inserted into a virtual 3D scene for causing interactions with other virtual 3D objects.</p>
<p id="p-0033" num="0032">Although certain example embodiments are described in detail below, it should be understood that various changes, substitutions and alterations can be made to the embodiments without departing from their spirit and scope.</p>
<p id="p-0034" num="0033">Preferred embodiments and their advantages are best understood by reference to <figref idref="DRAWINGS">FIGS. 1 through 11</figref>, wherein like numbers are used to indicate like and corresponding parts.</p>
<p id="h-0007" num="0000">Example System</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 1</figref> illustrates an interactive media system <b>100</b> for capturing a video image of a participant and displaying a virtual 3D scene that incorporates a virtual 3D representation of the participant, according to an example implementation of the present disclosure. System <b>100</b> includes a video camera <b>101</b>, a display <b>110</b>, and a computer system <b>120</b>. A participant (here, a person) <b>130</b> is standing on the ground (with point <b>131</b> representing a point of contact between participant <b>130</b> and the ground), and is facing generally in the direction of camera <b>101</b>. System <b>100</b> may also include one or more reference markers, e.g., reference markers <b>140</b> and/or <b>141</b>. Display <b>110</b> displays a view of a virtual 3D scene <b>190</b> that includes a virtual participant <b>155</b> (which is a virtual 3D representation of participant <b>130</b>) holding a virtual ball <b>156</b>.</p>
<p id="p-0036" num="0035">System <b>100</b> may include the hardware and software components embodying the present disclosure. System <b>100</b> may be situated in a public place (e.g., a shopping mall, a public sidewalk, or a store) or in a private place (e.g., in a home, a classroom, or a conference room). System <b>100</b> may be an interactive advertisement, a game, or a conferencing setup, for example. System <b>100</b> may be in communication with one or more other systems <b>100</b> to allow shared elements in virtual scenes <b>190</b> (e.g., a virtual tennis match where the shared elements include the net and ball). In some embodiments, camera <b>101</b>, display <b>110</b>, and computer system <b>120</b> may be incorporated into a single kiosk or other integrated structure while in others they may be physically separated components coupled via wires and/or wirelessly. In some embodiments, multiple cameras <b>101</b> and displays <b>110</b> may be used with one or more computer system <b>120</b>. In the displayed example, camera <b>101</b> is located above and in close proximity to display <b>110</b>. Alternatively, camera <b>101</b> may be located to the side or, below, behind, remote from, or in any other location relative to display <b>110</b>. As another example, camera <b>101</b> may be installed directly above or directly below participant <b>130</b> (e.g., aimed through a mesh or glass floor). As another example, if display <b>110</b> includes a projection screen or a collection of display panels, camera <b>101</b> may be located within the overall boundary of the screen or collection of display panels.</p>
<p id="p-0037" num="0036">Camera <b>101</b> may be any type of video camera or other camera configured to capture a stream of images, e.g., an analog video camera, a security camera, a webcam, or a point and shoot digital camera. Camera <b>101</b> provides a stream of images or video data to computer system <b>120</b>. For the purposes of this disclosure, video data is treated as a series of images, even though actual video data is typically encoded (to save space and/or bandwidth) into a repeating sequence of a full frame (a full image) followed by several partial frames indicating changes to the full frame. This treatment is appropriate because it is possible to decode such a repeating sequence of full and partial frames into a series of full frames by applying the indicated changes to the full frame and storing the results in memory (e.g., memory <b>122</b> of computer system <b>120</b>). If image or video data is provided by the camera as analog data, system <b>100</b> may include an analog-to-digital converter to enable input of image or video data into computer system <b>120</b>. In some embodiments, camera <b>101</b> is a high resolution digital camera capable of capturing accurate and high-fidelity images of the scene including participant <b>130</b> for greater realism. In other embodiments, camera <b>101</b> is a low resolution, monochrome, or infrared digital camera, which may reduce the processing complexity and/or provide alternative visual effects. Camera <b>101</b> may be a time-of-flight camera or other specialized 3D camera, but the approach of the present disclosure does not require the additional 3D information captured by such a camera.</p>
<p id="p-0038" num="0037">As shown in <figref idref="DRAWINGS">FIG. 1</figref>, camera <b>101</b> is aimed along ray <b>102</b> and captures a field of view <b>103</b>. Optical distortion caused by one or more lenses of camera <b>101</b> may be corrected for or may be ignored. While camera <b>101</b> is a physical object incorporating having a volume and including various components (e.g., one or more lenses, one or more light sensor arrays, and an enclosure), camera <b>101</b> is treated as a single point in space for the purposes of the geometric calculations discussed herein.</p>
<p id="p-0039" num="0038">Display <b>110</b> includes any suitable system or device for displaying virtual scene <b>190</b>. For example, display <b>110</b> may be a monitor, a flat-panel display, or a projector and projection screen or wall. In the example embodiment shown in <figref idref="DRAWINGS">FIG. 1</figref>, display <b>110</b> is located just below camera <b>101</b> such that virtual 3D participant <b>155</b> in virtual scene <b>190</b> appears to participant <b>130</b> as a near mirror image view of participant <b>130</b>. Alternatively, display <b>110</b> may be oriented away from the field of view of camera <b>101</b> to enable other applications. For example, participant <b>130</b> (in field of view <b>103</b>) may be asked to navigate a virtual maze based on verbal instructions from an observer viewing display <b>110</b> or based on audible feedback indicating proximity to or collision with a maze wall. In contrast, if display <b>110</b> is placed remote from camera <b>101</b>, participant <b>130</b> may see, e.g., a top, bottom, or a side-view of himself on display <b>110</b>, which may cause perceptual dissonance for participant <b>130</b>.</p>
<p id="p-0040" num="0039">A wearable or portable display <b>110</b> (e.g., 3D goggles, a tablet computer, or a video-enabled cell phone) coupled to a fixed camera may cause a similar perceptual dissonance if worn or held by the participant. This perceptual dissonance may be desirable in some cases (e.g., an art system). Further, a wearable or portable display <b>110</b> can be used with a similarly portable camera <b>101</b>. For example, in some embodiments, portable display <b>110</b> is integrated with camera <b>101</b> and computer system <b>120</b> (e.g., in a camera phone, or a laptop computer with webcam) while in other embodiments portable display <b>110</b> and portable camera <b>101</b> are each coupled to a remote computer system <b>120</b>. In these embodiments, system <b>100</b> may include additional elements for providing regularly updated position and orientation information regarding camera <b>101</b> and/or participant <b>130</b>. For example, a set of reference markers <b>140</b> and/or <b>141</b> may be placed around a room such that one or more is always within field of view <b>103</b>; or camera <b>101</b> may be coupled to one or more sensors for determining the position and orientation of camera <b>101</b> relative to objects in the camera's field of view.</p>
<p id="p-0041" num="0040">Computer system <b>120</b> may be any type of general purpose or specialized computer system incorporating at least one processor <b>121</b> and at least one memory <b>122</b>. Processor <b>121</b> executes instructions retrieved from memory <b>122</b> to process received image data from camera <b>101</b>, generate virtual 3D participant <b>155</b>, generate virtual 3D scene <b>190</b>, and output virtual scene <b>190</b> to display <b>110</b>. In certain embodiments, computer system <b>120</b> may be a personal computer running an operating system from APPLE, MICROSOFT, or one of the various distributions of UNIX. In these embodiments, processor <b>121</b> may be, for example, an X86 compatible processor connected to memory <b>122</b> including volatile random access memory (RAM) and non-volatile program and/or data storage (e.g., hard drive, flash drive, or solid-state storage). Some embodiments may also include a video graphics processor, which may further include video memory usable as memory <b>122</b>. In other embodiments, computer system <b>120</b> may be a laptop computer, a video game console, an embedded system, a cell phone, a personal digital assistant, or any other type of computing device. Memory <b>122</b> may store software or firmware instructions for performing some or all of the data processing steps of the present disclosure.</p>
<p id="p-0042" num="0041">Reference markers <b>140</b> and <b>141</b> are reference objects (e.g., fiducial markers) designed to be automatically recognized by computer system <b>120</b> when observed in an image captured by camera <b>101</b>. Reference marker <b>140</b> may be a planar fiducial marker located on the floor within field of view <b>103</b>. Marker <b>140</b> may include at least four distinct, recognizable points, e.g., as known in the field of AR applications. Reference marker <b>140</b> is shown in <figref idref="DRAWINGS">FIG. 2</figref> with exaggerated perspective distortion, for illustrative purposes. The at least four recognizable points allow computer system <b>120</b> to determine the orientation of the marker regardless of the camera perspective even if only one marker <b>140</b> is in field of view <b>103</b>. Based in part on this determination, computer system <b>120</b> can also determine the position and orientation of camera <b>101</b>. Multiple markers may still be useful, e.g., where participants may occasionally obscure all or part of a marker <b>140</b>, or where camera <b>101</b> is moveable. Reference marker <b>140</b> may have additional features that enable computer system <b>120</b> to determine and counteract optical distortions introduced by the focusing system of camera <b>101</b>.</p>
<p id="p-0043" num="0042">Reference marker <b>141</b> is an alternative reference object, particularly a shaped object (e.g., the letter &#x201c;X&#x201d; drawn on the floor or on a wall or other object, or a three dimensional object such as a sphere or box) or a regularly repeating, automatically recognizable point. In some embodiments, computer system <b>120</b> may be able to determine the position and orientation of camera <b>101</b> automatically, e.g., by analyzing four or more markers <b>141</b> in field of view <b>103</b>.</p>
<p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. 2</figref> illustrates representative elements of an image <b>200</b> captured by camera <b>101</b>, including a contour <b>201</b> of participant <b>130</b>, according to certain embodiments of the present disclosure. Image <b>200</b> may be a still image or video stream frame captured by camera <b>101</b>. Image <b>200</b> is a 2D picture including at least one participant <b>130</b> located within the camera's field of view <b>103</b>. Image <b>200</b> may be compressed or uncompressed, color or monochromatic, data representing the camera's field of view <b>103</b>. Image <b>200</b> is comprised of pixels, each represented by one or more bits of data further representing monochrome darkness or color information.</p>
<p id="p-0045" num="0044">Image <b>200</b> includes a participant contour <b>201</b> and a background image <b>202</b> (partially obscured by participant contour <b>201</b>). Participant contour <b>201</b> represents the outer contour, or outline, of participant <b>130</b> in image <b>200</b>, and may be determined by computer system <b>120</b> in any suitable manner, as described below. Participant contour <b>201</b> contacts a ground plane <b>250</b> at one or more ground contact points <b>260</b>. The topmost point on participant contour <b>201</b> is represented by a top contour point <b>261</b>. Image <b>200</b> may include any number of reference markers, e.g., markers <b>140</b> and/or <b>141</b>.</p>
<p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. 3</figref> is a side view of system <b>100</b> illustrating various geometric relationships between certain elements of system <b>100</b>, according to certain embodiments of the present disclosure. Camera <b>101</b> is located at camera reference point <b>301</b>, located at height h above ground level <b>250</b>. The camera's field of view <b>103</b> is bounded by lines <b>103</b><i>a </i>and <b>103</b><i>b</i>. Camera <b>101</b> is oriented along ray <b>102</b>, which is angled at c degrees from vertical. In this side view, participant contour <b>201</b> appears as a vertical line extending upward from ground plane <b>250</b>. Participant contour <b>201</b> is located at a horizontal distance b from the edge of the field of view <b>103</b><i>a</i>, and located at a horizontal distance b plus distance x from camera reference point <b>301</b>. Lines <b>305</b><i>a </i>and <b>305</b><i>b </i>represent a cone originating at camera reference point <b>301</b> and intersecting the topmost and bottommost visible points&#x2014;points <b>261</b> and <b>260</b>, respectively&#x2014;of participant contour <b>201</b>. One or more reference points, e.g., markers <b>140</b> and/or <b>141</b>, may be located along ground plane <b>250</b> within the field of view of the camera.</p>
<p id="h-0008" num="0000">System Calibration</p>
<p id="p-0047" num="0046">Various aspects of system <b>100</b> may be calibrated in order to determine location and/or dimension data regarding a participant in field of view <b>130</b> (e.g., participant <b>130</b> shown in <figref idref="DRAWINGS">FIGS. 1-3</figref>) such that participant <b>130</b> may be accurately modeled in 3D in virtual 3D scene <b>190</b>. Such calibrations may include any or all of the following:</p>
<p id="p-0048" num="0047">(a) determining a physical location and/or orientation of camera <b>101</b>;</p>
<p id="p-0049" num="0048">(b) generating or accessing one or more formulas and/or look-up tables for determining a 3D physical location of a participant <b>130</b> based on the location (e.g., pixel) of the participant <b>130</b> within an image <b>200</b> captured by camera <b>101</b>;</p>
<p id="p-0050" num="0049">(c) generating or accessing one or more formulas and/or look-up tables for determining a height of a participant <b>130</b> based on (a) aspects of the participant contour <b>201</b> (e.g., the distance between the bottom and topmost points <b>260</b> and <b>261</b> of contour <b>201</b>) and/or (b) the location of participant contour <b>201</b> within image <b>200</b>;</p>
<p id="p-0051" num="0050">(d) generating or accessing one or more formulas and/or look-up tables for determining a depth (e.g., dimension a shown in <figref idref="DRAWINGS">FIG. 3</figref>) for which to extrude participant contour <b>201</b> to generate a virtual 3D representation of participant <b>130</b> (e.g., virtual 3D participant <b>155</b> shown in <figref idref="DRAWINGS">FIG. 1</figref>) for use in virtual 3D scene <b>130</b>; and/or</p>
<p id="p-0052" num="0051">(e) any other suitable calibrations for determining any location and/or dimension data regarding participant <b>130</b> or other aspect of system <b>100</b>.</p>
<p id="p-0053" num="0052">Any or all of calibrations (b), (c), and (d) may be based at least in part on calibration (a), the physical location and/or orientation of camera <b>101</b>. Conversely, any or all of calibrations (b), (c), and (d) may be determined without using camera location and/or orientation data. Each of the calibration types (a)-(d) are discussed below.</p>
<p id="p-0054" num="0053">A. Calibrating Physical Location and/or Orientation of Camera <b>101</b>.</p>
<p id="p-0055" num="0054">The physical location and/or orientation of camera <b>101</b> may be calibrated manually, semi-automatically, or automatically. In a manual calibration, the location of one or more physical reference points (e.g., visual markers <b>140</b> and <b>141</b> or other reference points) relative to camera <b>101</b> may be manually measured (e.g., using a measuring tape), and the measurements then manually input into computer system <b>120</b>. In a semi-automatic calibration method, a graphical user interface may display an image <b>200</b> (or background image <b>202</b>), and allow a user to select physical reference points on the image and enter manually determined distances/locations for the selected physical reference points. In an automatic calibration method, computer system <b>120</b> may automatically identify one or more visual markers <b>140</b> and/or <b>141</b>, extract information from visual markers <b>140</b> and/or <b>141</b>, and apply one or more geometric algorithms to determine the physical position and/or orientation of camera <b>101</b>.</p>
<p id="p-0056" num="0055">In some embodiments, if a planar fiducial marker <b>140</b> is used, the well-known size and layout of the graphical elements in the marker allow computer system <b>120</b> to precisely determine the perspective of camera <b>101</b>. Alternatively, this approach works with several fiducial markers <b>141</b> of known size set out in the field of view <b>103</b>.</p>
<p id="p-0057" num="0056">In other embodiments, system <b>120</b> may automatically determine the configuration of camera <b>101</b> using one or more sensors incorporated into system <b>100</b> or used by an operator. For example, camera <b>101</b> may include a digital level or a sensor to determine the height of camera <b>101</b>. In such embodiments, the height of the camera and the angle of the camera from vertical may be combined into a mathematical function for determining the location of any observed contact with the ground plane (or other reference point) based on the relative position within the captured image. In certain embodiments, a POLHEMUS tracking system may be used to determine configuration of the camera.</p>
<p id="p-0058" num="0057">B. Calibrating Camera Image <b>200</b> for Determining 3D Locations of Participants.</p>
<p id="p-0059" num="0058">As mentioned above, system <b>100</b> may generate or access one or more formulas and/or look-up tables for determining a 3D physical location of a participant <b>130</b> based on the location (e.g., pixel) of the participant <b>130</b> within an image <b>200</b> captured by camera <b>101</b>. For example, system <b>100</b> may generate an algorithm or look-up table to convert each pixel of background image <b>202</b> to a 3D physical location, which algorithm or look-up table may be used to determine the 3D physical position of a participant contour <b>201</b> (e.g., based on the location of ground contact point <b>260</b> within image <b>200</b>). Alternatively, system <b>100</b> may generate an algorithm or look-up table to convert a distance (in pixels) from an edge of image <b>202</b> to 3D physical locations, which algorithm or look-up table may be used to determine the 3D physical position of a participant contour <b>201</b> (e.g., by measuring the distance in pixels between ground contact point <b>260</b> of participant contour <b>201</b> and an edge of image <b>200</b>). Alternatively, system <b>100</b> may generate an algorithm or look-up table to convert a distance (in pixels) from a reference marker (e.g., marker <b>140</b> or <b>141</b>) to 3D physical locations, which algorithm or look-up table may be used to determine the 3D physical position of a participant contour <b>201</b> (e.g., by measuring the distance in pixels between ground contact point <b>260</b> of participant contour <b>201</b> and the reference marker).</p>
<p id="p-0060" num="0059">The algorithms or look-up tables discussed above may be generated based on particular geometric relationships shown in <figref idref="DRAWINGS">FIG. 2</figref> and <figref idref="DRAWINGS">FIG. 3</figref>. For example, computer system <b>120</b> may generate an algorithm or look-up table to determine the ground-level distance to a participant <b>130</b> (e.g., distance b or distance b+x shown in <figref idref="DRAWINGS">FIG. 3</figref>) based on the pixel distance V (<figref idref="DRAWINGS">FIG. 2</figref>) between ground contact point <b>260</b> and the lower edge of image <b>200</b>.</p>
<p id="p-0061" num="0060">Similarly, computer system <b>120</b> may generate an algorithm or look-up table to determine the lateral location of a participant <b>130</b> based on the pixel distance between ground contact point <b>260</b> (or other point on participant contour <b>201</b>) and the left or right edge of image <b>200</b>.</p>
<p id="p-0062" num="0061">In other embodiments, computer system <b>120</b> may generate an algorithm or look-up table to determine both (a) the ground-level distance to a participant <b>130</b> (e.g., distance b or distance b+x shown in <figref idref="DRAWINGS">FIG. 3</figref>) and (b) the lateral location of the participant <b>130</b> based on the pixel location of ground contact point <b>260</b> (or other point on participant contour <b>201</b>) within image <b>200</b>, accounting for both the vertical and horizontal location of such pixel.</p>
<p id="p-0063" num="0062">C. Calibrating Camera Image <b>200</b> for Determining Participant Height.</p>
<p id="p-0064" num="0063">As mentioned above, system <b>100</b> may generate or access one or more formulas and/or look-up tables for determining a height of a participant <b>130</b> based on (a) aspects of the participant contour <b>201</b> (e.g., the distance between the bottom and topmost points <b>260</b> and <b>261</b> of contour <b>201</b>) and/or (b) the location of participant contour <b>201</b> within image <b>200</b>. For example, system <b>100</b> may generate an algorithm or look-up table to convert a vertical dimension of a participant contour <b>201</b> (e.g., the distance between ground contact point <b>260</b> and top contour point <b>261</b>) into a height of participant <b>130</b>. Such algorithm or look-up table may be generated based on particular geometric relationships shown in <figref idref="DRAWINGS">FIG. 3</figref>, or based on assumptions about participant height as a matter of distance between ground contact point <b>260</b> and top contour point <b>261</b> given the position and orientation of camera <b>101</b> (or based on a typical camera position and orientation).</p>
<p id="p-0065" num="0064">In some embodiments, the participant height calibration is performed after the participant location calibration. Similar to the participant location calibration, the participant height calibration may be manual, semi-automatic, or fully automatic. An example of semi-automatic calibration is provided as follows. A pole having a camera-recognizable marking at a known height on the pole may be held vertically at various ground locations in the camera's field of view <b>103</b>. At each location, computer system <b>120</b> may generate an image <b>202</b>, determine the 3D location of the pole on the ground (e.g., using a location algorithm or look-up table discussed above), and determine the height (in pixels) of the marking on the pole. System <b>100</b> may use data collected in this manner to generate an algorithm or look-up table to convert a measured height (in pixels) of a participant contour <b>201</b> to an actual or relative height based on the 3D physical location of the participant contour <b>201</b>.</p>
<p id="p-0066" num="0065">D. Calibrating Participant Depth.</p>
<p id="p-0067" num="0066">As mentioned above, system <b>100</b> may generate or access one or more formulas and/or look-up tables for determining a depth (e.g., dimension a shown in <figref idref="DRAWINGS">FIG. 3</figref>) for which to extrude participant contour <b>201</b> to generate a virtual 3D representation of participant <b>130</b> (e.g., virtual 3D participant <b>155</b> shown in <figref idref="DRAWINGS">FIG. 1</figref>) for use in virtual 3D scene <b>130</b>. This extrusion depth may be determined based on various factors. For example, system <b>100</b> may determine the extrusion depth for a particular participant <b>130</b> based on any one or more of the following factors:</p>
<p id="p-0068" num="0067">(a) the physical location of the participant (e.g., as determined using a location algorithm or look-up table discussed above);</p>
<p id="p-0069" num="0068">(b) the height of the participant (e.g., as determined using a height algorithm or look-up table discussed above);</p>
<p id="p-0070" num="0069">(c) a width or other dimension of the participant (e.g., as determined using an algorithm or look-up table similar to those discussed above);</p>
<p id="p-0071" num="0070">(d) the type of participant (e.g., person, animal, cart, etc.) identified by camera <b>101</b> (e.g., by comparing the participant contour with a number of pre-defined reference contours);</p>
<p id="p-0072" num="0071">(e) the orientation of the participant (e.g., whether a human participant <b>130</b> is facing camera <b>101</b> directly or sideways, which system <b>100</b> may determine automatically by comparing the participant contour with a number of pre-defined reference contours corresponding to various orientations); and/or</p>
<p id="p-0073" num="0072">(f) any other suitable factors.</p>
<p id="p-0074" num="0073">In other embodiments, the extrusion depth may be a standard pre-determined depth for all participants <b>130</b>.</p>
<p id="p-0075" num="0074">In some embodiments computer system <b>120</b> may apply certain assumptions in generating particular algorithms or look-up tables. Generally, the more assumptions used in generating a particular algorithm or look-up table, the less precise the algorithm or look-up table. However, certain assumptions may reduce the complexity of various calculations, resulting in faster processing, which may be desirable in certain applications or depending on the processing power of the relevant system <b>100</b>.</p>
<p id="h-0009" num="0000">Identifying a Participant Contour</p>
<p id="p-0076" num="0075">Once calibrated, system <b>100</b> may be used to capture camera images <b>200</b> of a participant <b>130</b>, identify a contour <b>201</b> of the participant <b>130</b>, determine location and/or dimensional data regarding the participant <b>130</b>, extrude the participant contour <b>201</b> to generate a virtual 3D representation of the participant <b>130</b> (i.e., virtual 3D participant <b>155</b>) in a virtual 3D scene <b>190</b>, and identify and manage interactions between the virtual 3D participant <b>155</b> and other virtual 3D objects in the virtual 3D scene <b>190</b>.</p>
<p id="p-0077" num="0076">A participant contour <b>201</b> (e.g., as shown in <figref idref="DRAWINGS">FIG. 2</figref>) may be a continuous curve or polygon(s) representing the outline of a participant <b>130</b>. Participant contour <b>201</b> may be used as a basis for developing a virtual 3D representation of participant <b>130</b> (i.e., virtual 3D participant <b>155</b>), as discussed below. Participant contour <b>201</b> may be determined by computer system <b>120</b> in any suitable manner. In one embodiment, participant contour <b>201</b> is determined using background subtraction. In this technique, an image of the background is captured before participant <b>130</b> enters the camera's field of view <b>103</b> (see <figref idref="DRAWINGS">FIG. 1</figref>). This captured image, referred to as background image <b>202</b>, may be stored in memory <b>122</b> (see <figref idref="DRAWINGS">FIG. 1</figref>) for later access. To determine participant contour <b>201</b>, image <b>200</b>&#x2014;which includes participant <b>130</b>&#x2014;may be compared pixel by pixel with background image <b>202</b>. For example, matching pixels may be identified as background pixels and grouped into contiguous clusters. In certain embodiments, a pixel match threshold may be set or adjusted to account for, e.g., variations in lighting conditions and to prevent shadows from being identified as primary objects. Computer system <b>120</b> may identify a cluster of non-matching pixels (between image <b>200</b> and background image <b>202</b>) as participant <b>130</b>.</p>
<p id="p-0078" num="0077">Each identified cluster may then be traced with a closed polygon line to form participant contour <b>201</b>. The number of vertices may vary depending on the level of resolution desired. Further, computer system <b>120</b> may apply line smoothing and/or contour adjustment, e.g., where there is a concern that some clothing elements may resemble the background (within the pixel matching threshold level) and cause the detected contour of the participant to have a jagged edge that inaccurately describes the participant's contour. Participant contour <b>201</b> may also be represented as a series of curves. Participant contour <b>104</b> can be determined using various other techniques including, e.g., the use of Sobel filters, Canny edge-detection, and color segmentation.</p>
<p id="p-0079" num="0078">Computer system <b>120</b> may also determine participant contours <b>201</b> for multiple participants <b>130</b> within field of view <b>103</b>. If image <b>200</b> includes multiple participants <b>131</b> in contact with each other or partially obscuring each another, computer system <b>120</b> may identify a single cluster corresponding to the multiple participants <b>131</b> and generate a single participant contour for the multiple participants <b>130</b>. In other embodiments, computer system <b>120</b> may recognize that there are multiple contacting/overlapping participants <b>130</b> in image <b>200</b> and apply a suitable algorithm for estimating a separate participant contour for each participant <b>130</b>. If image <b>200</b> includes multiple participants <b>130</b> that do not overlap each other, computer system <b>120</b> may identify each of participants <b>130</b> and generates a separate participant contour for each identified participant <b>130</b>.</p>
<p id="h-0010" num="0000">Determining the Participant's Location and/or Dimensions</p>
<p id="p-0080" num="0079">Computer system <b>120</b> may combine the participant contour <b>201</b> with location and/or dimension data regarding participant <b>130</b> to generate a virtual 3D representation of participant <b>130</b> (i.e., virtual 3D participant <b>155</b>) in virtual 3D scene <b>190</b>. Such participant location and/or dimension data may include, e.g., any one or more of the following: (a) a 3D location of participant <b>130</b>, (b) a height of participant <b>130</b>, (c) a width of participant <b>130</b>, (d) a depth of participant <b>130</b>, and (e) any other physical location, dimension, and/or shape of participant <b>130</b>.</p>
<p id="p-0081" num="0080">Computer system <b>120</b> may determine each type of participant location and/or dimension data in any suitable manner. For example, computer system <b>120</b> may determine various participant location and/or dimension data based on the location of one or more points on participant contour <b>201</b> in image <b>200</b>. Computer system <b>120</b> may apply any suitable algorithms and/or look-up tables for determining participant location and/or dimension data based on such point(s) on participant contour <b>201</b>, including, e.g., any one or more algorithms and/or look-up tables that may be generated by system <b>100</b> as discussed above in the &#x201c;System Calibration&#x201d; section. In particular, computer system <b>120</b> may access one or more algorithms and/or look-up tables stored in memory <b>122</b>, and apply such accessed algorithms and/or look-up tables to the relevant point(s) of participant contour <b>201</b> using processor <b>121</b>.</p>
<p id="p-0082" num="0081">Participant contour <b>201</b> may include one or more points used for determining a various location and/or dimension data of participant <b>130</b>. In this example, participant contour <b>201</b> includes a ground contact point <b>260</b> and a top contour point <b>261</b>. Ground contact point <b>260</b> represents the point where participant <b>130</b> touches the ground plane <b>250</b>, typically the lowest point on participant contour <b>201</b>. Ground contact point <b>260</b> may be used in determining various information, e.g., a 3D location of participant <b>130</b>, the distance between camera <b>101</b> and participant <b>130</b>, the height of participant <b>130</b>, etc. Top contour point <b>261</b> represents the topmost point on participant contour <b>201</b>, here the top of the participant's head. Top contour point <b>261</b> may be used along with ground contact point <b>260</b> to determine an approximate height of participant <b>130</b>. Example uses of ground contact point <b>260</b> and top contour point <b>261</b> for determining various location and/or dimension data of participant <b>130</b> (which may then be used by computer system <b>120</b> for generating virtual 3D participant <b>155</b> in virtual scene <b>190</b>) are described below in greater detail.</p>
<p id="p-0083" num="0082">Ground contact point <b>260</b> and/or top contour point <b>261</b> may be used in various ways for determining various location and/or dimension data of participant <b>130</b>, depending on the embodiment.</p>
<p id="h-0011" num="0000">Extruding the Participant Contour to Generate a Virtual 3D Participant</p>
<p id="p-0084" num="0083">System <b>100</b> may extrude participant contour <b>201</b> to generate a solid volume <b>306</b> for a virtual 3D representation of the participant <b>130</b> (i.e., virtual 3D participant <b>155</b>) in virtual 3D scene <b>190</b>.</p>
<p id="p-0085" num="0084">System <b>100</b> may extrude participant contour <b>201</b> by a uniform depth (e.g., as shown in <figref idref="DRAWINGS">FIGS. 3</figref>, <b>4</b>, and <b>6</b>) or non-uniform depth (e.g., as shown in <figref idref="DRAWINGS">FIGS. 6 and 7</figref>). The extrusion depth may be determined in a suitable manner. For example, system <b>100</b> may apply a standard extrusion depth for all participants <b>130</b>. Alternatively, system <b>100</b> may determine an extrusion depth for a particular participant <b>130</b> based on various factors, e.g.,</p>
<p id="p-0086" num="0085">(a) the physical location of the participant (e.g., as determined using a location algorithm or look-up table discussed above);</p>
<p id="p-0087" num="0086">(b) the height of the participant (e.g., as determined using a height algorithm or look-up table discussed above);</p>
<p id="p-0088" num="0087">(c) a width or other dimension of the participant (e.g., as determined using an algorithm or look-up table similar to those discussed above);</p>
<p id="p-0089" num="0088">(d) the type of participant (e.g., person, animal, cart, etc.) identified by camera <b>101</b> (e.g., by comparing the participant contour with a number of pre-defined reference contours);</p>
<p id="p-0090" num="0089">(e) the orientation of the participant (e.g., whether a human participant <b>130</b> is facing camera <b>101</b> directly or sideways, which system <b>100</b> may determine automatically by comparing the participant contour with a number of pre-defined reference contours corresponding to various orientations); and/or</p>
<p id="p-0091" num="0090">(f) any other suitable factors.</p>
<p id="p-0092" num="0091"><figref idref="DRAWINGS">FIG. 3</figref> illustrates a side view of an example extrusion of participant contour <b>201</b> by a uniform depth to generate a solid volume <b>306</b>, according to one embodiment of the present disclosure. The extruded volume <b>306</b> is illustrated as a trapezoid with a uniform extrusion depth of a bounded at the top and the bottom by the cone represented by lines <b>305</b><i>a </i>and <b>305</b><i>b. </i></p>
<p id="p-0093" num="0092">System <b>100</b> may attempt to select an extrusion depth that approximates the actual depth of participant <b>130</b>. However, the extrusion depth selected by system <b>100</b> may be greater than or less than the actual side view depth of participant <b>130</b>. The trapezoidal shape of extruded volume <b>306</b> results from an extrusion of the participant contour <b>201</b> orthogonal to the ground <b>250</b>. The plane of extrusion may also be orthogonal to ray <b>102</b>, which may in some instances reduce distortion when applying textures to the resulting 3D virtual 3D participant <b>155</b>.</p>
<p id="p-0094" num="0093"><figref idref="DRAWINGS">FIG. 4</figref> illustrates a 3D view of an example extrusion of participant contour <b>201</b> by a uniform depth to generate a solid volume <b>306</b>, according to one embodiment of the present disclosure. In this embodiment, participant contour <b>201</b> is extruded at the top along the upper cone boundary <b>305</b><i>b</i>, and at the bottom by ground plane <b>250</b>, rather than lower cone boundary <b>305</b><i>a </i>(in contrast to the extrusion shown in <figref idref="DRAWINGS">FIG. 3</figref>). Such extrusion may be desirable in certain applications, e.g., to prevent a physical interference between the bottom region of virtual 3D participant <b>155</b> and the virtual ground or floor.</p>
<p id="h-0012" num="0000">Generating 3D Virtual Scene <b>190</b> Including Virtual 3D Participant and Interactions</p>
<p id="p-0095" num="0094">Computer system <b>120</b> may generate a 3D virtual scene <b>190</b> including virtual participant <b>155</b> generated according to the techniques discussed herein in virtual 3D scene <b>190</b>, and determine and manage various interactions between virtual 3D participant <b>155</b> and other virtual elements, e.g., one or more external virtual 3D objects, one or more other virtual 3D participants <b>155</b> within the camera's field of view <b>103</b>, a background (reality-based or virtual), and/or any other virtual elements (e.g., light, fog, wind, etc.). Computer system <b>120</b> may incorporate a 3D graphics framework and real-time physics engine for generating 3D virtual scene <b>190</b> and identifying and managing interactions between virtual objects.</p>
<p id="p-0096" num="0095">&#x201c;External virtual 3D objects&#x201d; may refer to any virtual 3D objects generated independent from camera image <b>200</b>. For example, external virtual 3D objects may include pre-defined virtual 3D objects accessed from system memory <b>122</b> (e.g., generated by a system designer or operator). External virtual 3D objects may include objects that a participant <b>130</b> can only interact with virtually via virtual 3D scene <b>190</b>, e.g., a virtual ball, virtual bubbles, a cartoon character, a talking parrot, a birthday present, a product offered for sale, etc. External virtual 3D objects may be generated based on real, physical objects, or may be fictional objects. As an example of the former, virtual 3D objects may include a virtual 3D representation of a participant <b>130</b> (e.g., a live person) captured by a camera of a different system (i.e., other than system <b>100</b>).</p>
<p id="p-0097" num="0096">Computer system <b>120</b> may determine and manage various interactions between virtual objects including, for example:</p>
<p id="p-0098" num="0097">(a) a shadow cast by virtual participant <b>155</b> onto another virtual 3D object;</p>
<p id="p-0099" num="0098">(b) a shadow cast by a virtual 3D object onto virtual participant <b>155</b>;</p>
<p id="p-0100" num="0099">(c) a collision or contact between virtual participant <b>155</b> and another virtual 3D object;</p>
<p id="p-0101" num="0100">(d) a partial or total occlusion of virtual participant <b>155</b> by another virtual 3D object; and</p>
<p id="p-0102" num="0101">(e) a partial or total occlusion of a virtual 3D object by virtual participant <b>155</b>; and/or</p>
<p id="p-0103" num="0102">(f) any other type of interactions between virtual participant <b>155</b> and one or more other virtual 3D objects.</p>
<p id="p-0104" num="0103"><figref idref="DRAWINGS">FIG. 5</figref> illustrates an example virtual 3D scene <b>190</b> including a virtual 3D participant <b>155</b> generated according to the techniques discussed herein, and an external virtual ball object <b>156</b>, according to an example embodiment of the present disclosure. Virtual 3D scene <b>190</b> includes a view of 3D virtual participant <b>155</b> bound by participant contour <b>201</b>, and a view of virtual 3D ball <b>156</b>. Computer system <b>120</b> may import the portion of 2D camera image <b>200</b> bounded by participant contour <b>201</b> into the contour of virtual participant <b>155</b>. Alternatively, computer system <b>120</b> may apply a virtual image (e.g., an avatar) to the contour <b>210</b> of virtual 3D participant <b>155</b>, e.g., in an online gaming environment. Alternatively, computer system <b>120</b> may apply some combination of captured camera image data and virtual image data (e.g., a picture of participant <b>130</b> wearing a virtual costume) to the contour <b>210</b> of virtual 3D participant <b>155</b>.</p>
<p id="p-0105" num="0104">Computer system <b>120</b> may generate the view of virtual 3D scene <b>190</b> using various different techniques. In some implementations, computer system <b>120</b> may generate the view of virtual 3D scene <b>190</b> by (a) importing the captured camera image <b>200</b> (including participant <b>130</b> and background image <b>202</b>) and (b) introducing virtual 3D objects in the camera image <b>200</b>, e.g., by overlaying visible portions of such virtual 3D objects onto the camera image <b>200</b>. In other implementations, computer system <b>120</b> may generate the view of virtual 3D scene <b>190</b> by (a) creating a virtual image including all virtual elements, e.g., a virtual background and any virtual 3D objects (other than virtual 3D participant <b>155</b>), and (b) introducing virtual 3D participant <b>155</b> into the virtual image, e.g., by overlaying visible portions of virtual 3D participant <b>155</b> onto the virtual image. In other implementations, computer system <b>120</b> may generate the view of virtual 3D scene <b>190</b> by (a) combining reality-based objects (e.g., virtual 3D participant <b>155</b> and/or background scene <b>202</b>) with virtual elements (e.g., a virtual background and/or any virtual 3D objects, such as virtual ball <b>156</b>) in a 3D model, and (b) requesting a view of the 3D model from a particular vantage point, e.g., camera reference point <b>301</b>. In each of such example implementations, the view of virtual 3D participant <b>155</b> may appear identical and may appear to participant <b>130</b> as a mirror image of himself.</p>
<p id="p-0106" num="0105">Virtual ball <b>156</b> is shown as a non-limiting example of an external virtual 3D object generated by computer system <b>120</b> and introduced into virtual 3D scene <b>190</b>. A physics engine may control the position and/or movement of virtual ball <b>156</b>, including any interactions with virtual 3D participant <b>155</b>. For example, as virtual ball <b>156</b> rests on the participant's hand, physics engine <b>123</b> may apply an upward force on the ball from the participant's hand to counter the gravitational force on virtual ball <b>156</b>. Thus, if the participant moves his hand away from virtual ball <b>156</b>, physics engine <b>123</b> (still applying the gravitational force) may cause the ball to fall toward the ground. Furthermore, supposing computer system <b>120</b> introduces a virtual wind into virtual 3D scene <b>190</b>, physics engine <b>123</b> may apply the force of the wind to the ball, which may for example cause the ball to fly out of the participant's hand. Likewise, if another external 3D virtual object (e.g., another virtual ball or a virtual person) were to move across virtual 3D scene <b>190</b> and strike virtual ball <b>156</b>, physics engine <b>123</b> would control the collision and subsequent movement of virtual ball <b>156</b> accordingly to the rules of physics.</p>
<p id="h-0013" num="0000">Example Contour Extrusion Techniques</p>
<p id="p-0107" num="0106"><figref idref="DRAWINGS">FIGS. 6 and 7</figref> illustrate example extrusion techniques for extruding a participant contour <b>201</b>, according to certain embodiments of the present disclosure.</p>
<p id="p-0108" num="0107"><figref idref="DRAWINGS">FIG. 6</figref> illustrates a top-down view illustrating various extrusion techniques for generating a virtual 3D participant and their impact on interactions between the generated virtual 3D participant and other virtual objects in a virtual 3D scene, according to certain embodiments of the present disclosure. Camera reference point <b>301</b> is represented with camera field of view boundaries <b>103</b>.</p>
<p id="p-0109" num="0108">Three participant contours <b>601</b>, <b>602</b>, and <b>603</b> are illustrated as lines representing the top-down view of such 2D contours. A different extrusion technique is applied to each participant contour <b>601</b>, <b>602</b>, and <b>603</b>, though the illustrated techniques only represent examples and are not meant to be limiting. Participant contour <b>601</b> is extruded straight back away from camera reference point <b>301</b> to form a first virtual 3D participant <b>155</b><i>a </i>defined by 3D solid <b>631</b>. Participant contour <b>602</b> is extruded away from camera reference point <b>301</b>, but with beveled side <b>642</b> to form a second virtual 3D participant <b>155</b><i>b </i>defined by 3D solid <b>632</b>. And, participant contour <b>603</b> is extruded both toward and away from camera reference point <b>301</b> by a non-uniform depth to form a third virtual 3D participant <b>155</b><i>c </i>defined by 3D solids <b>634</b> and <b>633</b>.</p>
<p id="p-0110" num="0109"><figref idref="DRAWINGS">FIG. 6</figref> also illustrates the result of such different extrusions on interactions between the resulting virtual 3D participants <b>155</b><i>a</i>, <b>155</b><i>b</i>, and <b>155</b><i>c </i>and other virtual objects in a virtual 3D scene. To illustrate such different interactions, three virtual 3D objects <b>611</b>, <b>612</b>, and <b>613</b> are shown. Virtual 3D objects <b>611</b>, <b>612</b>, and <b>613</b> are moving along initial vectors V<sub>1</sub>, V<sub>2</sub>, and V<sub>3</sub>, respectively, toward virtual 3D participants <b>155</b><i>a</i>, <b>155</b><i>b</i>, and <b>155</b><i>c</i>, respectively. After colliding with virtual 3D participants <b>155</b><i>a</i>, <b>155</b><i>b</i>, and <b>155</b><i>c</i>, virtual 3D objects <b>611</b>, <b>612</b>, and <b>613</b> are directed along resultant vectors V<sub>1&#x2032;</sub>, V<sub>2&#x2032;</sub>, and V<sub>3&#x2032;</sub>, respectively, as discussed below. These interactions may be controlled by physics engine <b>123</b>.</p>
<p id="p-0111" num="0110">The extrusion of participant contour <b>601</b> into solid <b>631</b> is illustrated as a straight-back extrusion, but could also be a conical extrusion as illustrated in <figref idref="DRAWINGS">FIGS. 3 and 4</figref> with substantially similar results. This technique may provide the simplest approach to generating a 3D representation from a 2D contour. Additionally, computer system <b>120</b> may offset the extruded solid <b>631</b> (e.g., by placing solid <b>631</b> slightly closer to or further from camera reference point <b>301</b>) to account for various assumptions about a participant's center of mass relative to the participant's point of contact with the ground as viewed by the camera. If computer system <b>120</b> determines that the participant is relatively thin (e.g., by analyzing the participant's contour), system <b>120</b> may begin the extrusion at the contour and extrude away from the camera. If computer system <b>120</b> determines that the participant is relatively fat, system <b>120</b> may offset the extrusion some defined distance towards the camera and extrude further away from the camera than for a thin participant. In either case, this simplistic extrusion technique may result in a virtual 3D participant including square or nearly square edges. When virtual 3D object <b>611</b> (e.g., a ball) traveling along vector V<sub>1 </sub>comes into contact with 3D solid <b>631</b>, physics engine <b>123</b> detects the contact and redirects object <b>611</b> away along vector V<sub>1</sub>&#x2032; at an angle that is more severe than if 3D solid <b>631</b> embodied the actual 3D shape of the participant.</p>
<p id="p-0112" num="0111">The extrusion of participant contour <b>602</b> into solid <b>632</b> is illustrated as a straight-back extrusion, but with beveled rear edges <b>642</b>. As with the prior technique, the extrusion may be offset to account for the participant's estimated center of mass. This technique introduces some additional complexity, but provides a measured advance in realism in some instances. Here, virtual 3D object <b>612</b> traveling along vector V<sub>2 </sub>contacts solid <b>632</b> and deflects at a relatively slight angle illustrated by resulting vector V<sub>2</sub>&#x2032;, which may be more realistic (as compared to the non-beveled extrusion of solid <b>631</b>) given the rounded shape of an actual human body. While solid <b>632</b> is illustrated with only one bevel per side, system <b>120</b> may apply additional bevels as appropriate. The addition of multiple levels of beveling combined with bidirectional extrusion may approximate the next technique.</p>
<p id="p-0113" num="0112">The extrusion of participant contour <b>603</b> into solids <b>633</b> and <b>634</b> is illustrated as two curved surfaces more closely resembling the actual body shape of a person. While solid <b>634</b> is shown extruded toward the camera, the pair of solids <b>633</b> and <b>634</b> may be offset toward or away from the camera as discussed above. In this example, virtual 3D object <b>613</b> traveling along vector V<sub>3 </sub>collides with solid <b>634</b> and is redirected along new vector V<sub>3</sub>&#x2032; in front of solid <b>631</b>. In contrast, if extrusion <b>634</b> had been omitted, object <b>613</b> may be redirected on a course behind solid <b>631</b>. This technique may be approximated using discrete facets and/or bevels rather than implementing an actual curved surface.</p>
<p id="p-0114" num="0113"><figref idref="DRAWINGS">FIG. 7</figref> illustrates a side view of a curved, non-uniform depth, bidirectional extrusion of a participant contour, according to certain embodiments of the present disclosure. Camera <b>101</b> is illustrated to provide a reference point and is aimed along ray <b>102</b>. The camera's field of view is bounded by lines <b>103</b><i>a </i>and <b>103</b><i>b </i>originating at camera reference point <b>301</b>. A cone bounded by lines <b>305</b><i>a </i>and <b>305</b><i>b </i>intersects the top and bottom of participant contour <b>201</b> (shown in this side view as a vertical line). Extruded solid <b>306</b> is shown with a curved, non-uniform depth extending a first distance d<sub>1 </sub>from participant contour <b>201</b> away from camera reference point <b>301</b> and extending a second distance d<sub>2 </sub>from participant contour <b>201</b> toward camera reference point <b>301</b>.</p>
<p id="p-0115" num="0114">This extrusion technique may more accurately represent certain classes of participants. For example, if a participant is a beach ball or other inflatable object, extruded solid <b>306</b> may be adjusted to more closely resemble the actual shape than a solid extruded by a uniform depth. The extrusion in the amount of distances d<sub>1 </sub>and d<sub>2 </sub>may be straight towards or away from the camera (along ray <b>102</b>) or in any other direction. Because extruded solid <b>306</b> is curved, the concern of visual anomalies due to extrusion in a direction other than along the line of the camera perspective may be reduced or eliminated.</p>
<p id="h-0014" num="0000">Example Method of Operation of System <b>100</b></p>
<p id="p-0116" num="0115"><figref idref="DRAWINGS">FIG. 8</figref> illustrates an example method performed by system <b>100</b> for generating a virtual 3D representation of a participant (i.e., a virtual 3D participant) from a 2D camera image, generating a 3D virtual scene including the virtual 3D participant, and managing interactions between the virtual 3D participant and other virtual objects, in accordance with certain embodiments of the present disclosure.</p>
<p id="p-0117" num="0116">At step <b>802</b>, various aspects of system <b>100</b> are calibrated. Such calibration may include, for example, any one or more of the following:</p>
<p id="p-0118" num="0117">(a) determining camera configuration data, e.g., a physical location and/or orientation of the camera;</p>
<p id="p-0119" num="0118">(b) generating or accessing one or more formulas and/or look-up tables for determining a 3D physical location of a participant based on the location (e.g., pixel) of the participant within an image captured by the camera;</p>
<p id="p-0120" num="0119">(c) generating or accessing one or more formulas and/or look-up tables for determining a height of a participant based on (a) aspects of the participant's contour (e.g., the distance between the bottom and topmost points of the contour) and/or (b) the location of the participant's contour within the camera image;</p>
<p id="p-0121" num="0120">(d) generating or accessing one or more formulas and/or look-up tables for determining a depth for which to extrude a participant contour to generate a virtual 3D representation of the participant (i.e., a virtual 3D participant) for use in a virtual 3D scene; and/or</p>
<p id="p-0122" num="0121">(e) any other suitable calibrations for determining any location and/or dimension data regarding a participant or other aspect of the system.</p>
<p id="p-0123" num="0122">At step <b>804</b>, a camera captures an image and communicates data representing the image to a computer system for analysis. The captured image may be, for example, a full image or data from a video stream from which a full image may be reconstructed. The image may be color or monochrome and may represent visible light or other sensory data (e.g., infrared light or x-ray transmission). The camera or the computer system may reduce the resolution and/or color depth of the captured image to enhance aspects of the system's performance (e.g., processing speed).</p>
<p id="p-0124" num="0123">At step <b>806</b>, the computer system may identify a participant contour from the received camera image, e.g., according to any of the techniques described above with respect to <figref idref="DRAWINGS">FIG. 2</figref>. The computer system may store either (a) a representation of the contour outline or (b) a representation of the contour outline including the image data contained in the contour. The former representation may be more useful in the computational aspects of determining the participant's location, while the latter may be mapped to the surface of the extruded participant contour so that participants may see their own images displayed as augmented reality (at step <b>826</b>).</p>
<p id="p-0125" num="0124">In some instances, the computer system may identify multiple participants from the captured camera image. The computer system may process all identified participant contours, or select one or more particular participant contours for processing (e.g., by applying any suitable selection rules). In some embodiments, system operators may wear computer-system-recognizable markers to indicate to the computer system that they should be ignored. Alternatively, people wishing to participate with system <b>100</b> may be required to wear markers indicating that they should be included. In some embodiments, system operators may wear special clothing (e.g., solid blue coveralls) such that the computer system automatically ignores the system operators, e.g., using a suitable contour detection algorithm. This special clothing may allow the system operators to remain invisible on the 3D virtual display and incapable of interacting with any virtual objects in 3D virtual display.</p>
<p id="p-0126" num="0125">At step <b>808</b>, the computer system may identify one or more points of interest on the participant contour that may be used (e.g., at step <b>816</b>) for determining the location and/or dimensional aspects of the participant. For example, the computer system may identify the lowest point on the contour as a primary reference point defining where the participant is in contact with the ground. Alternatively, the primary reference point may be a point where the participant is touching a wall or some other predetermined contact point. For example, participants may be informed that to appear in the augmented reality experience, they must first touch a spot on the wall or on a railing. As another example, the computer system may identify the top point on the contour, which may be used (e.g., at step <b>816</b>) for determining a height of the participant.</p>
<p id="p-0127" num="0126">At step <b>810</b>, if camera configuration data needed for determining the location and/or dimensional aspects of the participant (at step <b>816</b>), the method may proceed to step <b>812</b>. If not, the method may proceed to step <b>816</b>.</p>
<p id="p-0128" num="0127">At step <b>812</b>, the computer system may determine whether the camera has physically moved since some prior time (e.g., a previous iteration of method <b>800</b>). If so, the computer system may recalibrate one or more aspects of the system at step <b>814</b>, including, e.g., determining updated camera configuration data (e.g., an updated physical location and/or orientation of the camera), and/or any algorithms or look-up tables that depend on the current camera configuration.</p>
<p id="p-0129" num="0128">Alternatively, if the computer system determines that the camera has not moved, the method may proceed to step <b>816</b>. At step <b>816</b>, the computer system determines the location and/or dimensional aspects (e.g., height, width, stance) of the participant based on (a) the point(s) of interest identified at step <b>808</b>, and/or (b) camera configuration determined at step <b>802</b> and/or <b>814</b>, and/or (c) any other relevant data. The computer system may use any of the techniques described herein, e.g., the various techniques described with reference to <figref idref="DRAWINGS">FIGS. 2 and 3</figref>.</p>
<p id="p-0130" num="0129">At step <b>818</b>, the computer system determines depth data for extruding the participant contour. The computer system may use any of the techniques described herein. For example, the system may apply a standard extrusion depth for all participants. Alternatively, system may determine a uniform or non-uniform extrusion depth for the participant based on one or more factors, for example:</p>
<p id="p-0131" num="0130">(a) the physical location of the participant (e.g., as determined at step <b>816</b>);</p>
<p id="p-0132" num="0131">(b) the height of the participant (e.g., as determined at step <b>816</b>);</p>
<p id="p-0133" num="0132">(c) a width or other dimension of the participant (e.g., as determined at step <b>816</b>);</p>
<p id="p-0134" num="0133">(d) the type of participant (e.g., person, animal, cart, etc.);</p>
<p id="p-0135" num="0134">(e) the orientation of the participant (e.g., whether the participant is facing camera directly or sideways); and/or</p>
<p id="p-0136" num="0135">(f) any other suitable factors.</p>
<p id="p-0137" num="0136">In some embodiments, depth data may be determined using a formula that incorporates multiple factors. For example, the system may apply a formula to compare the relative width and height of the participant contour to determine whether the participant is standing sideways or facing the camera. If the system determines that the participant is facing sideways, the system may select a greater depth than if the participant is determined to be facing the camera. In another example, the system may recognize (based on the size and/or shape of the contour) that the contour likely includes two or more people partially obscuring each other&#x2014;and thus appearing to the system as a single participant. In such situation, the system may assign the contour a depth corresponding to a single person. In some embodiments, the system may attempt to classify the participant and generate an extrusion depth accordingly. For example, the system may identify a participant as a ball, and accordingly assign a depth equal to the width of the ball.</p>
<p id="p-0138" num="0137">At step <b>820</b>, the computer system may extrude the participant contour to generate a 3D solid for a virtual 3D representation of the participant (i.e., a virtual 3D participant). The system may extrude the participant contour in a uniform or non-uniform manner based on the depth data determined at step <b>818</b>. Where multiple participants are identified from the camera image, the system may apply the same extrusion technique and/or depth data for all participant contours or may apply different extrusion techniques and/or depth data for different participant contours, based on some classification of each participant. Examples of extrusion techniques are described above, especially in reference to <figref idref="DRAWINGS">FIGS. 3</figref>, <b>4</b>, <b>6</b>, and <b>7</b>.</p>
<p id="p-0139" num="0138">At step <b>822</b>, the computer system may incorporate the virtual 3D participant generated at step <b>820</b> into a 3D virtual model. The 3D virtual model may include the virtual 3D participant and one or more other virtual elements, e.g., one or more external virtual 3D objects, one or more other virtual 3D participants within the camera's field of view <b>103</b>, a background (reality-based or virtual), and/or any other virtual elements (e.g., light, fog, wind, etc.). The computer system may utilize any suitable 3D rendering framework, e.g., OpenGL&#x2122; or DirectX&#x2122;.</p>
<p id="p-0140" num="0139">At step <b>824</b>, the computer system may determine interactions between the virtual 3D participant and one or more other virtual elements in the 3D virtual model. Example interactions may include:</p>
<p id="p-0141" num="0140">(a) a shadow cast by the virtual participant onto another virtual 3D object;</p>
<p id="p-0142" num="0141">(b) a shadow cast by a virtual 3D object onto the virtual participant;</p>
<p id="p-0143" num="0142">(c) a collision or contact between the virtual participant and another virtual 3D object;</p>
<p id="p-0144" num="0143">(d) a partial or total occlusion of the virtual participant by another virtual 3D object; and</p>
<p id="p-0145" num="0144">(e) a partial or total occlusion of a virtual 3D object by the virtual participant; and/or</p>
<p id="p-0146" num="0145">(f) any other type of interactions between the virtual participant and one or more other virtual 3D objects.</p>
<p id="p-0147" num="0146">Specific example interactions are described above with reference to <figref idref="DRAWINGS">FIGS. 5 and 9</figref> (e.g., casting shadows, colliding, deflecting, and obscuring).</p>
<p id="p-0148" num="0147">In some embodiments, computer system utilizes a 3D rendering framework (e.g., OpenGL&#x2122; or DirectX&#x2122;) to determine and visualize these interactions. In some embodiments, computer system utilizes a real-time physics engine (e.g., Open Physics Abstraction Layer, Bullet, or Simulation Open Framework Architecture) to control the movement and physical interactions of virtual 3D objects in the 3D virtual model. For example, the real-time physics engine may apply gravity forces to virtual objects, and control deflections, deformations, cleaving, sticking, or other interactions resulting from collisions between virtual 3D objects.</p>
<p id="p-0149" num="0148">At step <b>826</b>, the computer system may display the interactions between the virtual 3D participant and other virtual element(s) determined at step <b>824</b>. In some embodiments, the computer system may display a 3D virtual scene that displays the virtual 3D participant and other virtual 3D elements in the 3D virtual model, as well as interactions between such virtual 3D elements. In other embodiments, the 3D virtual scene may include only portions of the virtual 3D participant, or an indirect or abstracted representation of the virtual 3D participant. In other embodiments, the 3D virtual scene may display indirect or abstracted interactions between the virtual 3D participant and other virtual element(s). For example, suppose an implementation in which a participant can operate a hula hoop. The system may display the hula hoop but not the participant (i.e., the hula hoop may appear to be floating in space). A virtual 3D participant is generated for the participant. As the participant moves her hips, the virtual 3D participant interacts with the hula hoop, causing the hula hoop to move in circles. The interaction&#x2014;i.e., the movement of the hula hoop caused by the participant moving her hips&#x2014;is displayed to the participant.</p>
<p id="p-0150" num="0149">In some embodiments, the system may include a large screen or monitor for displaying the 3D virtual scene in view of the participant. This allows the participant to see the interactions and respond accordingly. For example, in the virtual bubble room scenario, a participant may want to catch or pop a virtual bubble, but can only see the virtual bubble in the 3D virtual scene displayed on the screen. In this embodiment, the 3D virtual scene may appear to the participant as a mirror augmented with virtual elements.</p>
<p id="p-0151" num="0150">The method may then return to step <b>804</b>. The method of steps <b>804</b>-<b>826</b> may be repeated any number of times and at any frequency. Steps <b>804</b>-<b>826</b> may be performed in real time or substantially in real time such that a participant may view her movements and/or interactions with virtual objects in a displayed 3D virtual scene in real time or substantially in real time.</p>
<p id="h-0015" num="0000">Example 3D Virtual Scenes with Virtual Interactions</p>
<p id="p-0152" num="0151"><figref idref="DRAWINGS">FIG. 9</figref> illustrates the display of an example 3D virtual scene <b>190</b> including various types of interactions between virtual 3D elements, according to certain embodiments of the present disclosure. Virtual scene <b>190</b> may be viewed on any suitable display <b>110</b>. In this example, the system provides a game allowing participants to shoot virtual basketballs into a virtual basketball goal. A virtual light source <b>1002</b> casts a bright light on various virtual 3D element including virtual 3D participants <b>1004</b> and <b>1006</b>, virtual basketballs <b>1008</b> and <b>1012</b>, and virtual basketball goal <b>1010</b>. Various example interactions are illustrated as follows. Virtual basketball <b>1008</b> rests on the hand of virtual 3D participant <b>1004</b> at <b>1022</b>. Basketball goal <b>1010</b> is obscured by the arm and foot of virtual 3D participant <b>1004</b> at locations <b>1046</b>. Virtual 3D participant <b>1004</b> casts a shadow on virtual 3D participant <b>1006</b> at <b>1040</b>; on virtual basketball <b>1012</b> at <b>1042</b>; and on the virtual ground at <b>1030</b>. Finally, virtual basketball <b>1012</b> obscures part of the arm of virtual 3D participant <b>1006</b> and may be deflected by the same at <b>1044</b>. These example interactions are discussed more fully as follows.</p>
<p id="p-0153" num="0152">Virtual basketball <b>1008</b> rests on the hand of virtual 3D participant <b>1004</b> at <b>1022</b>. In some embodiments, a real-time physics engine <b>123</b> applies a downward gravitational pull on purely virtual basketball <b>1008</b>. Here, a portion of the virtual 3D participant <b>1004</b> counters that force and prevents the ball from dropping. In some embodiments, the physics engine <b>123</b> may provide a measure of adhesive force (e.g., &#x201c;stickiness&#x201d;) to interactions between virtual basketballs <b>1008</b> and <b>1012</b> and the participants' hands to make it easier for the participants to hold the virtual basketballs (e.g., as it is difficult to actually determine whether a participant's hand is sufficiently large and level (or cupped) to prevent a virtual ball from rolling off).</p>
<p id="p-0154" num="0153">As mentioned above, virtual 3D participant <b>1004</b> casts a shadow on virtual 3D participant <b>1006</b> at <b>1040</b>; on purely virtual basketball <b>1012</b> at <b>1042</b>; and on the virtual ground at <b>1030</b>. The shadow may be formed by extending virtual rays of light from virtual light source <b>1002</b> across the virtual 3D scene. Where the rays of light intersect with a virtual 3D object, e.g. virtual 3D participant <b>1004</b>, the intersection point may have an increased brightness (possibly from complete darkness if virtual light source <b>1002</b> is the only light source in the virtual 3D scene). Portions of virtual 3D objects further from virtual light source <b>1002</b> along the same ray, which is blocked by virtual 3D participant <b>1004</b>, will accordingly not be brightened.</p>
<p id="p-0155" num="0154">Finally, virtual basketball <b>1012</b> obscures the arm of virtual 3D participant <b>1006</b> and may be deflected by the same at <b>1044</b>. This deflection may also cause a visible deformation of virtual basketball <b>1012</b>. If virtual basketball <b>1012</b> is translucent, it may not completely obscure the arm of virtual 3D participant <b>1006</b> at <b>1044</b>, but may instead alter the arm's appearance by blurring and/or altering its color.</p>
<p id="p-0156" num="0155"><figref idref="DRAWINGS">FIG. 10</figref> illustrates the display of an example 3D virtual scene including several virtual 3D participants interacting with one external virtual 3D object, according to certain embodiments of the present disclosure. Virtual scene <b>190</b> may include multiple participants <b>130</b><i>a </i>and participant <b>130</b><i>b </i>(5 total participants are illustrated in this example). The participants <b>130</b><i>a </i>and <b>130</b><i>b </i>are interacting with external virtual 3D object <b>156</b> (e.g., a virtual flag). Participants <b>130</b><i>a </i>may be people with their arms stretched upward to hold up virtual flag <b>156</b>. Participant <b>130</b><i>b </i>may be a person with arms stretched outward, rather than upward, with virtual flag <b>156</b> resting on participant <b>130</b><i>b </i>'s head. One corner of the flag (the upper right-hand corner of <figref idref="DRAWINGS">FIG. 10</figref>) is unsupported by any participant and may sag accordingly.</p>
<p id="p-0157" num="0156">In some embodiments, computer system <b>120</b> may use a physics engine to apply a gravitational force on virtual flag <b>156</b> forcing the flag downward until supported by a participant or the ground. In such an embodiment, the physics engine would cause the unsupported corner to fall to the ground and cause the corner supported by participant <b>130</b><i>b </i>to be lower than the portions of virtual flag <b>156</b> supported by participants <b>130</b><i>a </i>with upward stretched hands. The physics engine may employ a modified set of physics rules to prevent all of the edges of virtual flag <b>156</b> from hanging limply, as might be the case with a real, textile flag held up by five people in the same arrangement. Alternatively, virtual flag <b>156</b> may be modeled as having physical characteristics resembling a sturdier material such as poster board to achieve the same effect. In other embodiments, computer system <b>120</b> may plot a 3D contour based on the top points of participants <b>130</b><i>a </i>and <b>130</b><i>b </i>to visualize the shape of virtual flag <b>156</b>, thereby obviating the need for a physics engine.</p>
<p id="p-0158" num="0157"><figref idref="DRAWINGS">FIG. 11</figref> illustrates the display of an example 3D virtual scene including several virtual 3D participants interacting with one external virtual 3D object, according to certain embodiments of the present disclosure. Virtual scene <b>190</b> may include participant <b>130</b><i>a </i>and participants <b>130</b><i>b </i>(3 total participants are illustrated in this example). Participant <b>130</b><i>a </i>is a radio controlled car, e.g., one set into the field of view by a person and driven around on the ground. Participant <b>130</b><i>a </i>is drive around participants <b>130</b><i>b</i>, e.g., semi stationary barrier blocks made of foam or cardboard in a race course. The race course may be painted or marked with tape or may only appear on the display of the virtual 3D scene. External virtual 3D object <b>156</b> is a virtual race car driving through the same course.</p>
<p id="p-0159" num="0158">In some embodiments, computer system <b>120</b> may use a physics engine to apply inertial forces to virtual car <b>156</b> when virtual car <b>156</b> collides with remote controlled car <b>130</b><i>a </i>or barriers <b>130</b><i>b</i>. For example, if remote controlled car <b>130</b><i>a </i>attempts to overtake virtual car <b>156</b>, computer system <b>120</b> may cause virtual car <b>156</b> to lurch forward, upward and to the side before spinning out. While systems may exist to alter the position, speed, direction, level, and or other physical characteristics of remote controlled car <b>130</b><i>a </i>as a result of the same interaction, such a system is not part of this specific example. Furthermore, if remote controlled car <b>130</b><i>a </i>were to collide with barrier <b>130</b><i>b</i>, barrier <b>130</b><i>b </i>may move as a result (i.e., the car physically strikes and moves the barrier). Because barrier <b>130</b><i>b </i>is a participant, computer system <b>120</b> will recognize the new position of barrier <b>130</b><i>b </i>and reflect that position in the 3D virtual representation maintained internally and displayed. In other embodiments, a physics engine is not employed and collisions may be displayed as overlapping 3D virtual objects, much like old video games sometimes did.</p>
<p id="p-0160" num="0159">In some embodiments, barriers <b>130</b><i>b </i>may be specifically colored, shaded, or lighted. In such an embodiment, computer system <b>120</b> may distinguish between the contour of remote controlled car <b>130</b><i>a </i>and the contour of barrier <b>130</b><i>b </i>when remote controlled car <b>130</b><i>a </i>is between the camera and barrier <b>130</b><i>b </i>and is partially obscuring barrier <b>130</b><i>b</i>. In these embodiments, barriers <b>130</b><i>b </i>are still recognizable to computer system <b>120</b> and may interact with the 3D virtual representation of remote controlled car <b>130</b><i>a </i>and virtual car <b>156</b>.</p>
<p id="p-0161" num="0160">As stated above, although the disclosed embodiments are described in detail in the present disclosure, it should be understood that various changes, substitutions and alterations can be made to the embodiments without departing from their spirit and scope.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A computer implemented method for incorporating a representation of a participant into a virtual 3D environment, comprising:
<claim-text>receiving an image captured by a camera, the image including a participant;</claim-text>
<claim-text>automatically determining a contour of the participant;</claim-text>
<claim-text>automatically associating depth data with the contour of the participant;</claim-text>
<claim-text>automatically generating a first virtual 3D representation of the participant by extruding the contour based on the associated depth data;</claim-text>
<claim-text>determining a physics interaction between the first virtual 3D representation of the participant and a second virtual 3D representation of a second object based at least on the extruded contour, wherein the physics interaction comprises a force vector;</claim-text>
<claim-text>and automatically causing a display of the interaction between the first virtual 3D representation of the participant and the second virtual 3D representation of a second object based at least on the extruded contour, wherein the computer implemented method is performed substantially in real-time.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The computer implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein determining the physics interaction between the first virtual 3D representation of the participant and the second virtual 3D representation of the second object based at least on the extruded contour is performed at least in part by a real-time physics engine.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The computer implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the physics interaction between the first virtual 3D representation of the participant and the second virtual 3D representation of the second object comprises a collision or contact between the first virtual 3D representation of the participant and the second virtual 3D representation of the second object.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. A computing system comprising:
<claim-text>a processor;</claim-text>
<claim-text>a memory coupled to the processor; and</claim-text>
<claim-text>an interactive media subsystem enabled to:
<claim-text>receive an image captured by a camera, the image including a participant;</claim-text>
<claim-text>automatically determine a contour of the participant;</claim-text>
<claim-text>automatically associate depth data with the contour of the participant;</claim-text>
<claim-text>automatically generate a first virtual 3D representation of the participant by extruding the contour based on the associated depth data;</claim-text>
<claim-text>determine a physics interaction between the first virtual 3D representation of the participant and a second virtual 3D representation of a second object based at least on the extruded contour, wherein the physics interaction comprises a force vector; and</claim-text>
<claim-text>automatically cause a display of the interaction between the first virtual 3D representation of the participant and the second virtual 3D representation of a second object based at least on the extruded contour;</claim-text>
</claim-text>
<claim-text>wherein the interactive media subsystem operates substantially in real-time. </claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
