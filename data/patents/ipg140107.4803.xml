<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625896-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625896</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12418538</doc-number>
<date>20090403</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<us-term-of-grant>
<us-term-extension>1276</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>34</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382173</main-classification>
<further-classification>382164</further-classification>
<further-classification>382165</further-classification>
<further-classification>382260</further-classification>
<further-classification>348159</further-classification>
<further-classification>348586</further-classification>
</classification-national>
<invention-title id="d2e53">Image matting</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>6208351</doc-number>
<kind>B1</kind>
<name>Borg et al.</name>
<date>20010300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345600</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6480624</doc-number>
<kind>B1</kind>
<name>Horie et al.</name>
<date>20021100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382165</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>7420590</doc-number>
<kind>B2</kind>
<name>Matusik et al.</name>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348159</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>7599555</doc-number>
<kind>B2</kind>
<name>McGuire et al.</name>
<date>20091000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382173</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>7602990</doc-number>
<kind>B2</kind>
<name>Matusik et al.</name>
<date>20091000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382260</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>7633511</doc-number>
<kind>B2</kind>
<name>Shum et al.</name>
<date>20091200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345628</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>7636128</doc-number>
<kind>B2</kind>
<name>Sun et al.</name>
<date>20091200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348586</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2007/0070226</doc-number>
<kind>A1</kind>
<name>Matusik</name>
<date>20070300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2007/0247553</doc-number>
<kind>A1</kind>
<name>Matusik</name>
<date>20071000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00010">
<othercit>Yung-Yu Chuang et al., A Bayesian Approach to Digital Matting. In Proceedings of IEEE Computer Vision and Pattern Recognition (CVPR 2001), vol. II, 264-271, Dec. 2001.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00011">
<othercit>Y. Chuang, B. Curless, D. Salesin, and R. Szeliski. A Bayesian approach to digital matting. In CVPR, 2001.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00012">
<othercit>J. Sun, J. Jia, C.-K. Tang, and H.-Y. Shum. Poisson matting. ACM Trans. Graph., 23(3):315-321, 2004.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00013">
<othercit>Alvy Ray Smith et al., &#x201c;Blue Screen Matting,&#x201d; International Conference on Computer Graphics and Interactive Techniques, pp. 259-268, (1996 ).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00014">
<othercit>Paul Debevec et al., &#x201c;A Lighting Reproduction Approach to Live-Action Compositing,&#x201d; SIGGRAPH 2002, San Antonio, Jul. 21-26, 2002.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00015">
<othercit>Olivier Juan et al., &#x201c;Trimap Segmentation for Fast and User-Friendly Alpha Matting,&#x201d; Lecture Notes in Computer Science, No. 3752, Springer-Verlag, pp. 186-197 (2005).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00016">
<othercit>Richard J. Qian et al., &#x201c;Video Background Replacement without A Blue Screen,&#x201d; Image Processing, 1999. ICIP 99. Proceedings. 1999 International Conference on, vol. 4, Issue , 1999 pp. 143-144.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>21</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382173</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382224</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382130</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382190</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382260</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382284</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382300</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348584</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348586</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348159</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348275</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348589</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348597</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348590</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348578</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348722</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345630</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345640</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345592</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345431</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345433</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345435</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>4</number-of-drawing-sheets>
<number-of-figures>8</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20100254598</doc-number>
<kind>A1</kind>
<date>20101007</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Yang</last-name>
<first-name>Qingxiong</first-name>
<address>
<city>Urbana</city>
<state>IL</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Sobel</last-name>
<first-name>Irwin E.</first-name>
<address>
<city>Menlo Park</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Tan</last-name>
<first-name>Kar-Han</first-name>
<address>
<city>Santa Clara</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Yang</last-name>
<first-name>Qingxiong</first-name>
<address>
<city>Urbana</city>
<state>IL</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Sobel</last-name>
<first-name>Irwin E.</first-name>
<address>
<city>Menlo Park</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Tan</last-name>
<first-name>Kar-Han</first-name>
<address>
<city>Santa Clara</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Hewlett-Packard Development Comapany, L.P.</orgname>
<role>02</role>
<address>
<city>Houston</city>
<state>TX</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Bekele</last-name>
<first-name>Mekonen</first-name>
<department>2666</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">An alpha matte is generated from image forming elements of an image. For each of one or more of the image forming elements: a respective representative foreground value is determined from one or more of the image forming element values; the respective representative foreground value and the value of the image forming element are normalized with respect to a threshold level; and a respective value of the alpha matte is generated from an evaluation of the normalized image forming element value in relation to the normalized representative foreground value.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="95.93mm" wi="173.65mm" file="US08625896-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="239.69mm" wi="180.42mm" file="US08625896-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="222.08mm" wi="175.34mm" file="US08625896-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="239.18mm" wi="175.34mm" file="US08625896-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="157.73mm" wi="165.61mm" file="US08625896-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0002" num="0001">Matting is used in computer graphics and computer vision for a variety of purposes, including special effects in television shows and movies. Matting involves separating background and foreground regions in an image (e.g., a still image or a video image). Typically, this process is based on a model of an image I as a mixture of a foreground F and a background B in accordance with the following equation:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>I</i>(<i>x</i>)=&#x3b1;(<i>x</i>)<i>F</i>(<i>x</i>)+(1&#x2212;&#x3b1;(<i>x</i>))<i>B</i>(<i>x</i>)&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where x is a pixel location and &#x3b1;&#x3b5;[0, 1] is an alpha mask that quantifies the mixture. Matting involves solving for F, B, and &#x3b1; based on I. A common method of extracting alpha mattes from a video involves placing the foreground objects in front of uniformly colored background screens, e.g., blue or green backgrounds. Among the problems with such an approach are the constraints that the foreground object should not include any of the background colors and care should be taken to avoid adding a colorcast of the background onto the foreground.
</p>
<p id="p-0003" num="0002">What are needed are improved systems and methods of generating an alpha matte that does not involve the use of colored backgrounds.</p>
<heading id="h-0002" level="1">BRIEF SUMMARY OF THE INVENTION</heading>
<p id="p-0004" num="0003">In one aspect, the invention features a method of generating an alpha matte from image forming elements of an image. For each of one or more of the image forming elements: a respective representative foreground value is determined from one or more of the image forming element values; the respective representative foreground value and the value of the image forming element are normalized with respect to a threshold level; and a respective value of the alpha matte is generated from an evaluation of the normalized image forming element value in relation to the normalized representative foreground value.</p>
<p id="p-0005" num="0004">The invention also features apparatus operable to implement the inventive methods described above and computer-readable media storing computer-readable instructions causing a computer to implement the inventive methods described above.</p>
<p id="p-0006" num="0005">Other features and advantages of the invention will become apparent from the following description, including the drawings and the claims.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram of an embodiment of an image matting system.</p>
<p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. 2</figref> is a flow diagram of an embodiment of a method of generating an alpha matte.</p>
<p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. 3</figref> is a block diagram of an embodiment of the image matting system of <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. 4</figref> is a block diagram of an embodiment of the image matting system of <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 5A</figref> is a sectional view of an embodiment of a camera that includes a ring light.</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 5B</figref> is a front view of an embodiment of the ring light shown in <figref idref="DRAWINGS">FIG. 5A</figref>.</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 6</figref> is block diagram of an embodiment of a camera that incorporates an embodiment of the image matting system of <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 7</figref> is block diagram of an embodiment of a computer that incorporates an embodiment of the image matting system of <figref idref="DRAWINGS">FIG. 1</figref>.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION OF THE INVENTION</heading>
<p id="p-0015" num="0014">In the following description, like reference numbers are used to identify like elements. Furthermore, the drawings are intended to illustrate major features of exemplary embodiments in a diagrammatic manner. The drawings are not intended to depict every feature of actual embodiments nor relative dimensions of the depicted elements, and are not drawn to scale.</p>
<p id="h-0005" num="0000">I. Definition of Terms</p>
<p id="p-0016" num="0015">The term &#x201c;image forming element&#x201d; refers to an addressable region of an image. In some embodiments, the image forming elements correspond to pixels, which are the smallest addressable units of an image. Each image forming element has at least one respective value that is represented by one or more bits. For example, an image forming element in the RGB color space includes a respective value for each of the colors red, green, and blue, where each of the values may be represented by one or more bits.</p>
<p id="p-0017" num="0016">A &#x201c;mixed image forming element&#x201d; is an image forming element that is classified as having contributions from a foreground object and a background.</p>
<p id="p-0018" num="0017">A &#x201c;computer&#x201d; is a machine that processes data according to machine-readable instructions (e.g., software) that are stored on a machine-readable medium either temporarily or permanently. A set of such instructions that performs a particular task is referred to as a program or software program.</p>
<p id="p-0019" num="0018">The term &#x201c;machine-readable medium&#x201d; refers to any medium capable carrying information that is readable by a machine (e.g., a computer). Storage devices suitable for tangibly embodying these instructions and data include, but are not limited to, all forms of non-volatile computer-readable memory, including, for example, semiconductor memory devices, such as EPROM, EEPROM, and Flash memory devices, magnetic disks such as internal hard disks and removable hard disks, magneto-optical disks, DVD-ROM/RAM, and CD-ROM/RAM.</p>
<p id="p-0020" num="0019">As used herein, the term &#x201c;includes&#x201d; means includes but not limited to, the term &#x201c;including&#x201d; means including but not limited to. The term &#x201c;based on&#x201d; means based at least in part on.</p>
<p id="h-0006" num="0000">II. Introduction</p>
<p id="p-0021" num="0020">The embodiments that are described herein provide reliable, robust extraction of alpha mattes and foreground objects from images (e.g., still images and video images) without requiring the use of a colored background. In these embodiments, an alpha matte is generated based on evaluations of normalized image forming element values in relation to a normalized representative foreground value, where the image forming element values and the representative foreground value are normalized with respect to a specified threshold level that is set to segment the foreground object from the background in mixed image forming elements. In this way, these embodiments enable alpha mattes to be quickly and effectively generated without the complexities and concomitant problems associated with colored-background based approaches. The foreground objects readily can be determined from the alpha mattes and the corresponding images.</p>
<p id="h-0007" num="0000">III. Overview</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 1</figref> shows an embodiment of an image matting system <b>10</b> that includes an image sensor <b>12</b> and an image processing system <b>14</b>. The image sensor <b>12</b> has a field of view <b>15</b> of a scene <b>16</b> that includes a foreground object <b>18</b> (i.e., a dog). Light <b>20</b> illuminates a first side of the foreground object, and the image sensor <b>12</b> an image <b>22</b> from a second side of the foreground object opposite the first side. The image <b>22</b> is generated from the portion of the light <b>20</b> that is unobscured by and outlines the foreground object <b>18</b> in the scene <b>16</b>, as well as from light that is reflected from the foreground object <b>18</b> and other elements of the scene <b>16</b>. The image processing system <b>14</b> derives an alpha matte <b>24</b> and an image <b>26</b> of the foreground object <b>18</b> from the image <b>22</b>. In <figref idref="DRAWINGS">FIG. 1</figref>, the gray values of the background pixels represent alpha matte values of zero.</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 2</figref> shows an embodiment of a method by which the image matting system <b>14</b> generates the alpha matte <b>24</b> from the image <b>22</b>. In accordance with this method, the following processes are performed for each of one or more of the image forming elements of the image <b>22</b> (<figref idref="DRAWINGS">FIG. 2</figref>, block <b>30</b>). The image processing system <b>14</b> determines a respective representative foreground value from one or more of the image forming element values (<figref idref="DRAWINGS">FIG. 2</figref>, block <b>32</b>). The image processing system <b>14</b> normalizes the respective representative foreground value and the value of the image forming element with respect to a threshold level (<figref idref="DRAWINGS">FIG. 2</figref>, block <b>34</b>). The image processing system <b>14</b> generates a respective value of the alpha matte <b>24</b> from an evaluation of the normalized image forming element value in relation to the normalized representative foreground value (<figref idref="DRAWINGS">FIG. 2</figref>, block <b>36</b>).</p>
<p id="p-0024" num="0023">In some embodiments, the image <b>22</b> has multiple color components, and each of the image forming elements has a respective color component value for each of the color components. In these embodiments, a respective foreground color component value is determined for each of the color components. For each of the color components, the respective foreground color component value and the respective image forming element color component value are normalized. In addition, for each of the color components, a respective alpha matte value is generated from a respective evaluation of the respective normalized image forming element color component value in relation to the respective normalized foreground color component value.</p>
<p id="p-0025" num="0024">The elements of the method of <figref idref="DRAWINGS">FIG. 2</figref> are described detail below in the following section.</p>
<p id="h-0008" num="0000">IV. Image Matting</p>
<p id="p-0026" num="0025">A. Image Capture</p>
<p id="p-0027" num="0026">The image sensor <b>12</b> captures the image <b>22</b> of the scene <b>16</b>. This process typically involves shining the light <b>12</b> towards the foreground object <b>18</b> and capturing the light that outlines (or surrounds) the foreground object <b>18</b> in the scene <b>16</b>.</p>
<p id="p-0028" num="0027">In some embodiments, the threshold level that is used to normalize the representative foreground values and the image forming element values (see <figref idref="DRAWINGS">FIG. 2</figref>, block <b>34</b>) is a saturation level of the image forming element values. In these embodiments, the image sensor <b>12</b> typically is calibrated so that the intensity of the captured light <b>12</b> saturates the ones of the image forming elements of the image sensor <b>12</b> that depict the background to a saturation level of the image forming elements. In this process, a user typically adjusts the exposure and/or gain levels of the image sensor <b>12</b> until the background image forming elements are saturated while the foreground image forming elements are not saturated. In one exemplary calibration process, the user captures one or more images of a white reference (e.g., a white board) that is is placed in front of the target background, and adjusts the exposure and/or gain levels of the image sensor until the background image forming elements appearing in the calibration images are saturated and the foreground image forming elements are not saturated. After the calibration process, the image sensor is ready to capture the image <b>22</b>.</p>
<p id="p-0029" num="0028">The light <b>12</b> can be sourced by one or more light sources that are located on an opposite side of the foreground object <b>12</b> as the image sensor <b>12</b> or from the same side of the foreground object as the image sensor <b>12</b>.</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 3</figref> shows an embodiment <b>40</b> of the image matting system <b>10</b> that includes a light source <b>42</b> that sources the light <b>12</b> from the opposite side of the foreground object <b>18</b> as the image sensor <b>12</b>. The illuminating light that is generated by the light source <b>42</b> may be mono-colored or multi-colored. In some exemplary embodiments, the illuminating light is white, which avoids the addition of a colorcast to the foreground object <b>18</b>. In general, the light source <b>42</b> may be any type of light source that generates light with sufficient intensity to saturate the image forming elements of the image sensor <b>12</b>. In some embodiments, the light source <b>42</b> is implemented by one or more light emitting diodes.</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 4</figref> shows another embodiment <b>50</b> of the image matting system <b>10</b> that includes a light source <b>52</b> that is located on the same side of the foreground object <b>18</b> as the image sensor <b>12</b>. The illuminating light <b>53</b> that is generated by the light source <b>52</b> may be mono-colored or multi-colored. In some exemplary embodiments, the illuminating light is white, which avoids the addition of a colorcast to the foreground object <b>18</b>. In general, the light source <b>52</b> may be any type of light source that generates light with sufficient intensity to saturate the image forming elements of the image sensor <b>12</b>. In some embodiments, the light source <b>52</b> is implemented by one or more light emitting diodes.</p>
<p id="p-0032" num="0031">The image matting system <b>50</b> additionally includes a reflector <b>54</b> that is located on the opposite side of the foreground object <b>18</b> as the image sensor <b>12</b> and the light source <b>52</b>. The reflector <b>54</b> may be implemented by a wide variety of different types of reflectors that include one or more elements that reflect the light <b>12</b> with high efficiency. In some embodiments the reflector <b>54</b> is a retroreflector (e.g., a retroreflective curtain). In the embodiment of <figref idref="DRAWINGS">FIG. 4</figref>, some of the illuminating light <b>53</b> reflects off the illuminated side of the foreground object <b>18</b> towards the image sensor <b>12</b> and other portions of the light <b>53</b> are reflected by the reflector <b>54</b> as the light <b>20</b>, which is directed towards the image sensor <b>12</b>. The image sensor <b>12</b> captures the image <b>22</b> of at least a portion of the sourced light <b>53</b> that is reflected from the illuminated side of the foreground object <b>18</b>, as well as a portion <b>20</b> of the sourced light <b>53</b> that is reflected from the reflector <b>54</b>.</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIGS. 5A and 5B</figref> show an embodiment of a camera <b>56</b> that is suitable for use in the embodiment of <figref idref="DRAWINGS">FIG. 4</figref>. The camera <b>56</b> includes an embodiment <b>58</b> of the image sensor <b>12</b>, an optical system <b>60</b>, and a diaphragm <b>62</b> that are contained within a housing <b>64</b>. The image sensor <b>58</b> typically is implemented by a two-dimensional CCD or CMOS image sensor. The optical system <b>60</b> typically is implemented by one or more lenses that focus the incoming light <b>66</b> from the field of view <b>15</b> onto the active region of the image sensor <b>58</b>. The diaphragm <b>62</b> defines an aperture <b>68</b> that controls the amount of incoming light that is focused by the optical system <b>60</b>. In operation, the image sensor <b>58</b> produces image optical data from the incoming light <b>66</b> that is focused by the optical system <b>60</b> onto the active region. The camera <b>56</b> additionally includes an embodiment <b>70</b> of the light source <b>52</b>. The light source <b>70</b> includes a ring <b>72</b> of light emitting diodes <b>74</b> that emit white light (e.g., white light or mono-colored light).</p>
<p id="p-0034" num="0033">B. Image Processing</p>
<p id="p-0035" num="0034">1. Introduction</p>
<p id="p-0036" num="0035">The image processing system <b>14</b> processes the captured image <b>12</b> in order to generate the alpha matte <b>24</b> and the foreground image <b>26</b> (see <figref idref="DRAWINGS">FIG. 1</figref>).</p>
<p id="p-0037" num="0036">In some embodiments, before executing the processes described in blocks <b>32</b>-<b>36</b> of the method shown in <figref idref="DRAWINGS">FIG. 2</figref>, the image processing system <b>14</b> segments the image <b>22</b> into a trimap that segments the image forming elements of the image <b>22</b> into background, foreground, and mixed image forming elements. In this process, the image processing system <b>14</b> classifies the image forming elements of the image <b>22</b> based on their respective values. Image forming elements with values greater than or equal to a first threshold (I<sub>B</sub>) are classified as background image forming elements. Image forming elements with values less than or equal to a second threshold (I<sub>F</sub>) are classified as foreground image forming elements. The image forming elements with values between the first and second thresholds are classified as mixed image forming elements.</p>
<p id="p-0038" num="0037">In the alpha matte, the image forming elements that correspond to background image forming elements in the image <b>22</b> are assigned a minimal value (e.g., zero, corresponding to a black color); the image forming elements that correspond to foreground image forming elements in the image <b>22</b> are assigned a maximal value (e.g., 255 in an eight-bit color space model, corresponding to white); and the image forming elements that correspond to mixed image forming elements in the image <b>22</b> are assigned a respective grayscale value in accordance with the processes described in blocks <b>32</b>-<b>36</b> of <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0039" num="0038">2. Determining a Representative Foreground Value</p>
<p id="p-0040" num="0039">For each of the mixed image forming elements, the image processing system <b>14</b> determines a respective representative foreground value from one or more of the image forming element values (<figref idref="DRAWINGS">FIG. 2</figref>, block <b>32</b>).</p>
<p id="p-0041" num="0040">In some embodiments, the image processing system <b>14</b> determines the respective representative foreground value from one or more of the image forming element values corresponding to the foreground object. In some embodiments, for each mixed image forming element, the image processing system <b>14</b> sets the respective representative foreground value as the value of the nearest one of the foreground image forming elements in the image <b>22</b>. In this process, the image processing system <b>14</b> determines the respective Euclidean distance between the mixed image forming element and each of the foreground image forming elements. In some embodiments, the image processing system <b>14</b> sets the respective representative foreground value to a respective one of the image forming element values corresponding to a point on a foreground object in the image. In these embodiments, the image processing system <b>14</b> selects the value of the foreground image forming element that is associated with the smallest Euclidean distance as the respective representative foreground value for that mixed image forming element. In other embodiments, the representative foreground values may be determined from a combination (e.g., average) of multiple foreground image forming element values.</p>
<p id="p-0042" num="0041">3. Normalizing the Foreground Value and the Image Forming Element Values</p>
<p id="p-0043" num="0042">For each of the mixed image forming elements, the image processing system <b>14</b> normalizes the respective representative foreground value and the mixed image forming element value with respect to a threshold level (<figref idref="DRAWINGS">FIG. 2</figref>, block <b>34</b>).</p>
<p id="p-0044" num="0043">In some embodiments, the normalization process involves for each of the mixed image forming elements (i) determining the normalized representative foreground value from a difference between the representative foreground value and the threshold level, and (ii) determining the normalized image forming element value from a respective difference between the respective image forming element value and the threshold level. The threshold level typically is set to a value at or below the value of the background image forming elements in the image <b>22</b>. In these embodiments, the threshold level is set to the saturation value of the image forming elements (e.g., 255 in an eight-bit color space model). In other embodiments, the threshold level is set to a value below the saturation value of the image forming elements.</p>
<p id="p-0045" num="0044">4. Generating the Alpha Matte</p>
<p id="p-0046" num="0045">As explained above, the image forming elements in the alpha matte <b>24</b> that correspond to background image forming elements in the image <b>22</b> are assigned a minimal value (e.g., zero, corresponding to a black color), and the image forming elements in the alpha matte <b>24</b> that correspond to foreground image forming elements in the image <b>22</b> are assigned a maximal value (e.g., 255 in an eight-bit color space model, corresponding to white). The image forming elements in the alpha matte <b>24</b> that correspond to mixed image forming elements in the image <b>22</b> are assigned respective values based on the values of the corresponding mixed image forming elements in image <b>22</b>, the representative foreground values, and the threshold value.</p>
<p id="p-0047" num="0046">For each of the mixed image forming elements, the image processing system <b>14</b> generates a respective value of the alpha matte <b>24</b> from an evaluation of the normalized image forming element value in relation to the normalized representative foreground value (<figref idref="DRAWINGS">FIG. 2</figref>, block <b>36</b>).</p>
<p id="p-0048" num="0047">In some embodiments, the image processing system <b>14</b> determines values of the alpha matte from respective ratios between the normalized image forming element values and the normalized representative foreground values. In some of these embodiments, the image forming element values (x of the alpha matte a, are given in accordance with equation (2):</p>
<p id="p-0049" num="0048">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <msub>
          <mi>&#x3b1;</mi>
          <mi>p</mi>
        </msub>
        <mo>=</mo>
        <mfrac>
          <mrow>
            <msub>
              <mi>B</mi>
              <mi>p</mi>
            </msub>
            <mo>-</mo>
            <msub>
              <mi>I</mi>
              <mi>p</mi>
            </msub>
          </mrow>
          <mrow>
            <msub>
              <mi>B</mi>
              <mi>p</mi>
            </msub>
            <mo>-</mo>
            <msub>
              <mi>F</mi>
              <mi>p</mi>
            </msub>
          </mrow>
        </mfrac>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>2</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where I<sub>P </sub>is the intensity of image forming element p in the image <b>22</b>, F<sub>p </sub>is the respective representative foreground image forming element determined for pixel p, B<sub>p </sub>is the threshold value for pixel p. In some embodiments, B<sub>p </sub>is equal to a maximal intensity value (e.g., 255 in an eight-bit color space model, corresponding to white) for all pixels p. Here, B<sub>p</sub>-I<sub>p </sub>is the normalized image forming element value, and B<sub>p</sub>-F<sub>p </sub>is the normalized representative foreground value.
</p>
<p id="p-0050" num="0049">5. Generating the Foreground Object Image</p>
<p id="p-0051" num="0050">In some embodiments, the values of the background and foreground image forming elements in the foreground object image <b>26</b> are the same as the values of the background and foreground image forming elements in the image <b>22</b>. In these embodiments, the image forming elements (F<sub>p</sub>) in the foreground object image <b>26</b> that correspond to mixed image forming elements in the image <b>22</b> are assigned the representative foreground values determined above.</p>
<p id="h-0009" num="0000">V. Exemplary Operating Environments</p>
<p id="p-0052" num="0051">A. Introduction</p>
<p id="p-0053" num="0052">The image sensor <b>12</b> may be implemented by any type of imaging device that is capable of capturing one-dimensional or two-dimensional images of a scene. The image sensor <b>12</b> typically includes at least one image sensing component with a respective light sensing active area. Exemplary image sensing components include charge coupled devices (CCDs) and complementary metal-oxide-semiconductor (CMOS) devices. The image sensor <b>12</b> may include one or more optical elements for directing (e.g., shaping, focusing, or changing the propagation path of) the incoming light from the scene <b>16</b>.</p>
<p id="p-0054" num="0053">The image processing system <b>14</b> typically includes one or more discrete data processing components, each of which may be in the form of any one of various commercially available data processing chips. The image processing system <b>14</b> is not limited to a specific hardware or software configuration, but rather it may be implemented in any computing or processing environment, including in digital electronic circuitry or in computer hardware, firmware, device driver, or software. In some implementations, the image processing system <b>14</b> is embedded in the hardware of any one of a wide variety of digital and analog electronic devices, including desktop and workstation computers, digital still image cameras, digital video cameras, printers, scanners, and portable electronic devices (e.g., mobile phones, laptop and notebook computers, and personal digital assistants). In some embodiments, the image processing system <b>14</b> executes process instructions (e.g., machine-readable code, such as computer software) for implementing the methods that are executed by the embodiments of the image matting system <b>10</b>. These process instructions, as well as the data generated in the course of their execution, are stored in one or more computer-readable media. Storage devices suitable for tangibly embodying these instructions and data include all forms of non-volatile computer-readable memory, including, for example, semiconductor memory devices, such as EPROM, EEPROM, and flash memory devices, magnetic disks such as internal hard disks and removable hard disks magneto-optical disks, DVD-ROM/RAM, and CD-ROM/RAM.</p>
<p id="p-0055" num="0054">Embodiments of the image processing system <b>14</b> may be implemented by one or more discrete modules (or data processing components) that are not limited to any particular hardware, firmware, or software configuration. In some embodiments, the functionalities of the modules are combined into a single data processing component. In some embodiments, the respective functionalities of each of one or more of the modules are performed by a respective set of multiple data processing components. The various modules of the image processing system <b>14</b> may be co-located on a single apparatus or they may be distributed across multiple apparatus; if distributed across multiple apparatus, the modules may communicate with each other over local wired or wireless connections, or they may communicate over global network connections (e.g., communications over the internet).</p>
<p id="p-0056" num="0055">B. A First Exemplary Image Matting System Architecture and Application Environment</p>
<p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. 6</figref> shows an embodiment of a digital camera system <b>82</b> that incorporates an embodiment of the image processing system <b>14</b>. The digital camera system <b>82</b> may be configured to capture one or both of still images and video image frames. The digital camera system <b>82</b> includes an image sensor <b>84</b> (e.g., a charge coupled device (CCD) or a complementary metal-oxide-semiconductor (CMOS) image sensor), a sensor controller <b>86</b>, a memory <b>88</b>, a frame buffer <b>90</b>, a microprocessor <b>92</b>, an ASIC (application-specific integrated circuit) <b>94</b>, a DSP (digital signal processor) <b>96</b>, an I/O (input/output) adapter <b>98</b>, and a storage medium <b>100</b>. In general, the image processing system <b>14</b> may be implemented by one or more of hardware and firmware components. In the illustrated embodiment, the image processing system <b>60</b> is implemented in firmware, which is loaded into memory <b>88</b>. The storage medium <b>100</b> may be implemented by any type of image storage technology, including a compact flash memory card and a digital video tape cassette. The image data stored in the storage medium <b>100</b> may be transferred to a storage device (e.g., a hard disk drive, a floppy disk drive, a CD-ROM drive, or a non-volatile data storage device) of an external processing system (e.g., a computer or workstation) via the I/O subsystem <b>98</b>.</p>
<p id="p-0058" num="0057">The microprocessor <b>92</b> choreographs the operation of the digital camera system <b>82</b>. The image signal produced by the image sensor <b>84</b> is passed to the image processing system <b>14</b>, which produces the image <b>22</b>, the alpha matte <b>24</b>, and the foreground image <b>26</b> from the received data. In these embodiments, the image processing system <b>14</b> typically performs various operations on the image data, including one or more of the following operations: demosaicing; color correction; image compression; one or more storage operations; and one or more transmission operations.</p>
<p id="p-0059" num="0058">C. A Second Exemplary Image Matting System Architecture and Application Environment</p>
<p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. 7</figref> shows an embodiment of a computer system <b>120</b> that can implement any of the embodiments of the image processing system <b>14</b> that are described herein. The computer system <b>120</b> includes a processing unit <b>122</b> (CPU), a system memory <b>124</b>, and a system bus <b>126</b> that couples processing unit <b>122</b> to the various components of the computer system <b>120</b>. The processing unit <b>122</b> typically includes one or more processors, each of which may be in the form of any one of various commercially available processors. The system memory <b>124</b> typically includes a read only memory (ROM) that stores a basic input/output system (BIOS) that contains start-up routines for the computer system <b>120</b> and a random access memory (RAM). The system bus <b>126</b> may be a memory bus, a peripheral bus or a local bus, and may be compatible with any of a variety of bus protocols, including PCI, VESA, Microchannel, ISA, and EISA. The computer system <b>120</b> also includes a persistent storage memory <b>128</b> (e.g., a hard drive, a floppy drive, a CD ROM drive, magnetic tape drives, flash memory devices, and digital video disks) that is connected to the system bus <b>126</b> and contains one or more computer-readable media disks that provide non-volatile or persistent storage for data, data structures and computer-executable instructions.</p>
<p id="p-0061" num="0060">A user may interact (e.g., enter commands or data) with the computer <b>120</b> using one or more input devices <b>130</b> (e.g., a keyboard, a computer mouse, a microphone, joystick, and touch pad). Information may be presented through a user interface that is displayed to the user on a display monitor <b>160</b>, which is controlled by a display controller <b>150</b> (implemented by, e.g., a video graphics card). The computer system <b>120</b> also typically includes peripheral output devices, such as speakers and a printer. One or more remote computers may be connected to the computer system <b>120</b> through a network interface card (NIC) <b>136</b>.</p>
<p id="p-0062" num="0061">As shown in <figref idref="DRAWINGS">FIG. 7</figref>, the system memory <b>124</b> also stores the image processing system <b>14</b>, a graphics driver <b>138</b>, and processing information <b>140</b> that includes input data, processing data, and output data. In some embodiments, the image processing system <b>14</b> interfaces with the graphics driver <b>138</b> (e.g., via a DirectXe component of a MicrosoftWindows&#xae; operating system) to present a user interface on the display monitor <b>160</b> for managing and controlling the operation of the image processing system <b>10</b>.</p>
<p id="h-0010" num="0000">VI. Conclusion</p>
<p id="p-0063" num="0062">The embodiments that are described herein provide reliable, robust extraction of alpha mattes and foreground objects from images (e.g., still images and video images) without requiring the use of a colored background. In these embodiments, an alpha matte is generated based on evaluations of normalized image forming element values in relation to a normalized representative foreground value, where the image forming element values and the representative foreground value are normalized with respect to a specified threshold level that is set to segment the foreground object from the background in mixed image forming elements. In this way, these embodiments enable alpha mattes to be quickly and effectively generated without the complexities and concomitant problems associated with colored-background based approaches. The foreground objects readily can be determined from the alpha mattes and the corresponding images.</p>
<p id="p-0064" num="0063">Other embodiments are within the scope of the claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08625896-20140107-M00001.NB">
<img id="EMI-M00001" he="7.03mm" wi="76.20mm" file="US08625896-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method of generating an alpha matte from an image comprising image forming elements each of which has a respective value, the method comprising operating a physical processor to perform operations comprising for each of one or more of the image forming elements:
<claim-text>determining a respective representative foreground value from one or more of the image forming element values;</claim-text>
<claim-text>normalizing the respective representative foreground value and the value of the image forming element with respect to a threshold level; and</claim-text>
<claim-text>generating a respective value of the alpha matte from an evaluation of the normalized image forming element value in relation to the normalized representative foreground value.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising capturing the image of a scene comprising a foreground object and a background, and the determining comprises determining the representative foreground value from one or more of the image forming element values corresponding to the foreground object.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising shining light towards the foreground object, and wherein the capturing comprises capturing the light that outlines the foreground object in the scene, and the captured portion of the light has an intensity that saturates ones of the image forming elements depicting the background to the saturation level.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the shining comprises sourcing the light from a first side of the foreground object, and the capturing comprises capturing the image from a second side of the foreground object opposite the first side.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the shining comprises sourcing the light from an illumination side of the foreground object, and the capturing comprises capturing the image of the sourced light reflected from the illuminated side of the foreground object and the sourced light reflected from a reflector located on a second side of the object opposite the illumination side.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the shined light is white light.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determining comprises setting the respective representative foreground value to a respective one of the image forming element values corresponding to a point on a foreground object in the image.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the determining comprises identifying ones of the image forming elements that corresponding to the foreground object in the image, ascertaining a respective one of the identified image forming elements that is nearest to the image forming element for which the respective representative foreground value is being determined, and setting the respective representative foreground value to the value of the ascertained one of the image forming elements corresponding to the foreground object in the image.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the normalizing comprises (i) determining the normalized representative foreground value from a difference between the representative foreground value and the threshold level, and (ii) determining the normalized image forming element value from a respective difference between the respective image forming element value and the threshold level.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the threshold level is a saturation level of the image forming element values.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the generating comprises determining the respective value of the alpha matte from a respective ratio between the normalized image forming element value and the normalized representative foreground value.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:
<claim-text>the image comprises multiple color components;</claim-text>
<claim-text>each of the image forming elements has a respective color component value for each of the color components;</claim-text>
<claim-text>the determining comprises determining a respective foreground color component value for each of the color components;</claim-text>
<claim-text>the normalizing comprises for each of the color components normalizing the respective foreground color component value and the respective image forming element color component value; and</claim-text>
<claim-text>the generating comprises for each of the color components generating a respective alpha matte value from a respective evaluation of the respective normalized image forming element color component value in relation to the respective normalized foreground color component value.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. At least one non-transitory computer-readable medium having computer-readable program code embodied therein, the computer-readable program code adapted to be executed by a computer to implement a method of generating an alpha matte from an image comprising image forming elements each of which has a respective value, the method comprising:
<claim-text>determining a respective representative foreground value from one or more of the image forming element values;</claim-text>
<claim-text>normalizing the respective representative foreground value and the value of the image forming element with respect to a threshold level; and</claim-text>
<claim-text>generating a respective value of the alpha matte from an evaluation of the normalized image forming element value in relation to the normalized representative foreground value.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The at least one computer-readable medium of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein in the determining the program code causes the computer to perform operations comprising setting the respective representative foreground value to a respective one of the image forming element values corresponding to a point on a foreground object in the image.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The at least one computer-readable medium of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein in the normalizing the program code causes the computer to perform operations comprising (i) determining the normalized representative foreground value from a difference between the representative foreground value and the threshold level, and (ii) determining the normalized image forming element value from a respective difference between the respective image forming element value and the threshold level.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The at least one computer-readable medium of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein in the generating the program code causes the computer to perform operations comprising determining the respective value of the alpha matte from a respective ratio between the normalized image forming element value and the normalized representative foreground value.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. Apparatus for generating an alpha matte from an image comprising image forming elements each of which has a respective value, comprising:
<claim-text>a computer-readable memory storing computer-readable instructions; and</claim-text>
<claim-text>a data processing unit coupled to the memory, operable to execute the instructions, and based at least in part on the execution of the instructions operable to perform operations comprising</claim-text>
<claim-text>determining a respective representative foreground value from one or more of the image forming element values;</claim-text>
<claim-text>normalizing the respective representative foreground value and the value of the image forming element with respect to a threshold level; and</claim-text>
<claim-text>generating a respective value of the alpha matte from an evaluation of the normalized image forming element value in relation to the normalized representative foreground value.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The apparatus of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein in the determining the data processing unit is operable to perform operations comprising setting the respective representative foreground value to a respective one of the image forming element values corresponding to a point on a foreground object in the image.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The apparatus of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein in the normalizing the data processing unit is operable to perform operations comprising (i) determining the normalized representative foreground value from a difference between the representative foreground value and the threshold level, and (ii) determining the normalized image forming element value from a respective difference between the respective image forming element value and the threshold level.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The apparatus of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein in the generating the data processing unit is operable to perform operations comprising determining the respective value of the alpha matte from a respective ratio between the normalized image forming element value and the normalized representative foreground value.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising classifying ones of the image forming elements of the image as foreground image forming elements and classifying other ones of the image forming elements of the image as mixed image forming elements; and
<claim-text>wherein for each of respective ones of the mixed image forming elements, the determining comprises setting the respective representative foreground value to a value of a respective one of the foreground image forming elements nearest the respective mixed image forming element. </claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
