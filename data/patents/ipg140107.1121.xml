<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08622302-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08622302</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12725912</doc-number>
<date>20100317</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<us-term-of-grant>
<us-term-extension>339</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>19</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>23546201</main-classification>
<further-classification>235435</further-classification>
<further-classification>235439</further-classification>
<further-classification>235451</further-classification>
</classification-national>
<invention-title id="d2e53">Systems and methods for compensating for fixed pattern noise</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>3568151</doc-number>
<kind>A</kind>
<name>Majima</name>
<date>19710300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>3747066</doc-number>
<kind>A</kind>
<name>Vernot et al.</name>
<date>19730700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>3900832</doc-number>
<kind>A</kind>
<name>Hanchett</name>
<date>19750800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>3986000</doc-number>
<kind>A</kind>
<name>McJohnson</name>
<date>19761000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>4000397</doc-number>
<kind>A</kind>
<name>Hebert et al.</name>
<date>19761200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>5350908</doc-number>
<kind>A</kind>
<name>Bechtel</name>
<date>19940900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>5637853</doc-number>
<kind>A</kind>
<name>Joseph</name>
<date>19970600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>5729288</doc-number>
<kind>A</kind>
<name>Saito</name>
<date>19980300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6042012</doc-number>
<kind>A</kind>
<name>Olmstead et al.</name>
<date>20000300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6061092</doc-number>
<kind>A</kind>
<name>Bakhle et al.</name>
<date>20000500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6271785</doc-number>
<kind>B1</kind>
<name>Martin et al.</name>
<date>20010800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6871785</doc-number>
<kind>B2</kind>
<name>Piva et al.</name>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>7110465</doc-number>
<kind>B2</kind>
<name>Kaku et al.</name>
<date>20060900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>7280141</doc-number>
<kind>B1</kind>
<name>Frank et al.</name>
<date>20071000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348243</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>7443431</doc-number>
<kind>B2</kind>
<name>Kelly et al.</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2002/0064234</doc-number>
<kind>A1</kind>
<name>Kaku et al.</name>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>2005/0134714</doc-number>
<kind>A1</kind>
<name>Carlson et al.</name>
<date>20050600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>2006/0180670</doc-number>
<kind>A1</kind>
<name>Acosta et al.</name>
<date>20060800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>23546231</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>2006/0256890</doc-number>
<kind>A1</kind>
<name>Kaku et al.</name>
<date>20061100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>2007/0222870</doc-number>
<kind>A1</kind>
<name>Itoh</name>
<date>20070900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348243</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>2008/0062161</doc-number>
<kind>A1</kind>
<name>Brown et al.</name>
<date>20080300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345207</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>2008/0100736</doc-number>
<kind>A1</kind>
<name>Ise</name>
<date>20080500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>2008/0298649</doc-number>
<kind>A1</kind>
<name>Ennis et al.</name>
<date>20081200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382125</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>2009/0059316</doc-number>
<kind>A1</kind>
<name>Irwin et al.</name>
<date>20090300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>358474</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>2009/0101798</doc-number>
<kind>A1</kind>
<name>Yadid-Pacht et al.</name>
<date>20090400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>WO</country>
<doc-number>WO 2005/106768</doc-number>
<date>20051100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00027">
<othercit>Shawn Barnett and Dave Etchells; Canon EOS 30D; http://www.imaging-resource.com/PRODS/E30D/E30DA7.HTM; Apr. 14, 2006.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00028">
<othercit>Chris Limone; Dark Frame Subtraction Using Adobe Photoshop, http://takegreatpictures.com/digital%20photography/9503; Dec. 23, 2008.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>21</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>235435</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>235439</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>235454</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>235462</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>5</number-of-drawing-sheets>
<number-of-figures>5</number-of-figures>
</figures>
<us-related-documents>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>61161005</doc-number>
<date>20090317</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20100237149</doc-number>
<kind>A1</kind>
<date>20100923</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Olmstead</last-name>
<first-name>Bryan L.</first-name>
<address>
<city>Eugene</city>
<state>OR</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Olmstead</last-name>
<first-name>Bryan L.</first-name>
<address>
<city>Eugene</city>
<state>OR</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Stoel Rives LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Datalogic ADC, Inc.</orgname>
<role>02</role>
<address>
<city>Eugene</city>
<state>OR</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Lee</last-name>
<first-name>Michael G</first-name>
<department>2876</department>
</primary-examiner>
<assistant-examiner>
<last-name>Mikels</last-name>
<first-name>Matthew</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">This disclosure relates generally to systems and methods for estimating and, at least partially, compensating for fixed pattern noise (FPN) in an image sensor. In one configuration, an estimate of the FPN of an image sensor may be obtained by capturing a dark image (either a linear or an area image, depending on the sensor type) using a first exposure time, an illuminated image may be captured using a second exposure time, the second exposure time is greater than the first exposure time, and the dark image may be subtracted from the illuminated image to compensate, at least partially, for FPN. Certain configurations may utilize virtual scan lines. Two or more dark images may also be utilized to estimate FPN in an image sensor.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="231.31mm" wi="155.87mm" file="US08622302-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="227.33mm" wi="158.83mm" file="US08622302-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="227.33mm" wi="163.75mm" orientation="landscape" file="US08622302-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="226.82mm" wi="162.81mm" file="US08622302-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="232.33mm" wi="185.17mm" file="US08622302-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="206.93mm" wi="170.69mm" file="US08622302-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">COPYRIGHT NOTICE</heading>
<p id="p-0002" num="0001">&#xa9; 2008 Datalogic Scanning, Inc. A portion of the disclosure of this patent document contains material that is subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document or the patent disclosure, as it appears in the Patent and Trademark Office patent file or records, but otherwise reserves all copyright rights whatsoever. 37 CFR &#xa7;1.71(d).</p>
<heading id="h-0002" level="1">TECHNICAL FIELD</heading>
<p id="p-0003" num="0002">The field of the present disclosure relates generally, but not exclusively, to optical data readers and, in a preferred application, to systems and methods for reducing fixed pattern noise in optical data readers.</p>
<heading id="h-0003" level="1">BACKGROUND</heading>
<p id="p-0004" num="0003">Optical reading of data or encoded symbols, such as barcode labels, has been used in many applications. Typically, a barcode consists of a series of parallel light and dark rectangle areas of varying widths. Different widths of bars and spaces define different characters in a particular barcode symbology. A barcode label may be read by an optical code reader, which detects reflected and/or refracted light from the bars and spaces comprising the characters. A detector generates an electrical signal that has an amplitude determined by the intensity of the collected light. It should be understood that the principles described herein are applicable to various systems and methods for reading many types of optical codes, including 1-D, 2-D, Maxicode, PDF-417, and others.</p>
<p id="p-0005" num="0004">An optical code reader may utilize solid state image circuitry, such as charge coupled devices (CCDs) and complementary metal oxide semiconductor (CMOS) imagers to read an optical code. An optical code to be read may be illuminated with a light source, and reflected light may be collected by the CCD or CMOS imager. An electrical signal may be generated having an amplitude determined by the intensity of the collected light. As the image is read out, positive and negative transitions in the signal occur, signifying transitions between the bars and spaces.</p>
<p id="p-0006" num="0005">Optical code readers may be implemented using either a one-dimensional (a linear imager) or two-dimensional imaging array (an area sensor) of photosensors (or pixels) to capture the optical code. One-dimensional CCD readers capture a linear cross section of the optical code and may produce an analog waveform whose amplitude represents the relative darkness and lightness of the optical code. Two-dimensional CCD or CMOS imagers capture a two-dimensional image.</p>
<p id="p-0007" num="0006">Various factors influence the performance of optical code readers, including noise in the signal. CCDs and CMOS imagers may exhibit noise from a variety of sources, including fixed pattern noise (FPN).</p>
<p id="p-0008" num="0007">The present inventor has recognized that advantages may be realized by removing or reducing FPN in an optical code reader during normal operation. Such advantages may include improved performance in low light conditions, faster exposure times, and allowing the use of less expensive components. The present inventor has therefore determined that it would be desirable to remove or reduce FPN in an optical code reader so as to realize these and other advantages.</p>
<heading id="h-0004" level="1">SUMMARY</heading>
<p id="p-0009" num="0008">Disclosed are systems and methods for estimating and, at least partially, compensating for fixed pattern noise (FPN) in an image sensor of an optical data reader. In one configuration, an estimate of the FPN of an image sensor may be obtained by capturing a dark image (either a linear or an area image, depending on the sensor type) using a first exposure time. An illuminated image may be captured using a second exposure time. The second exposure time is greater than the first exposure time. The dark image may be subtracted from the illuminated image to compensate, at least partially, for FPN.</p>
<p id="p-0010" num="0009">In another preferred example, virtual scan lines may be utilized. A dark virtual scan line is generated from a dark two dimensional image. Similarly, an illuminated virtual scan line is generated from an illuminated two dimensional image. The dark virtual scan line may then be subtracted from the illuminated virtual scan line to compensate, at least partially, for FPN.</p>
<p id="p-0011" num="0010">In yet another example, two or more dark images may be captured and used to estimate FPN in an image sensor. The two or more dark images may be used to estimate different components of FPN, including dark current variations and transistor threshold mismatch. The estimated components of FPN may then be subtracted from an illuminated image to compensate, at least partially, for FPN.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0012" num="0011">Understanding that drawings depict only certain preferred embodiments and are not therefore to be considered to be limiting in nature, the preferred embodiments will be described and explained with additional specificity and detail through the use of the accompanying drawings, in which:</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 1</figref> is a flow chart of a method for compensating for FPN in an image sensor, according to a preferred embodiment.</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 2</figref> is a graph of sample results of the method of <figref idref="DRAWINGS">FIG. 1</figref> for compensating for FPN using dark image subtraction.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 3</figref> is a flow chart of a method for compensating for FPN in a two dimensional image sensor utilizing virtual scan lines, according to one embodiment.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 4</figref> is a flow chart of a method for compensating for FPN including both threshold mismatch dark signal non-uniformity and dark current dark signal non-uniformity.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 5</figref> is a block diagram of a portable data reader configured to estimate and to compensate for FPN.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0006" level="1">DETAILED DESCRIPTION OF PREFERRED EMBODIMENTS</heading>
<p id="p-0018" num="0017">In the following description, numerous specific details are provided for a thorough understanding of specific preferred embodiments. However, those skilled in the art will recognize that embodiments can be practiced without one or more of the specific details, or with other methods, components, materials, etc., following the descriptions herein.</p>
<p id="p-0019" num="0018">In some cases, well-known structures, materials, or operations are not shown or described in detail in order to avoid obscuring aspects of the preferred embodiments. Furthermore, the described features, structures, or characteristics may be combined in any suitable manner in one or more embodiments.</p>
<p id="p-0020" num="0019">FPN may include dark FPN, also known as dark signal non-uniformity (DSNU), and pixel response non-uniformity (PRNU). DSNU may include two different types of noise, namely threshold mismatch DSNU and dark current DSNU. Threshold mismatch DSNU may be caused by offset differences between pixels in a CMOS or CCD imager. The offset differences may be attributable to manufacturing process-related threshold voltage variations in the readout transistors coupled to each pixel. The output stage in some CMOS or CCD imagers consists of a single transistor in an open-loop source-follower configuration, leading to threshold mismatch DSNU.</p>
<p id="p-0021" num="0020">DSNU in CCD or CMOS imagers may also be caused by manufacturing process-related variations in dark current DSNU between neighboring pixels. Dark current is a relatively small electric current that flows through a photosensitive device, including photodiodes and/or charge-coupled devices, even when no photons are entering the device. Pixel response non-uniformity (PRNU) may also introduce FPN into the signal. PRNU refers to gain variations between pixels in response to a uniformly illuminated image. Such variations may be attributable, at least in part, to sensitivity variations of the photodiode and to differences in photodiode area between pixels.</p>
<p id="p-0022" num="0021">FPN may be a significant noise source, especially in low light conditions (e.g., less than 100 lux). For example, a model RPLIS-2048 linear CMOS imager, Rev H, available from Panavision Imaging, LLC, Horner, N.Y., has (according to its data sheet) FPN of 5% peak-to-peak full scale, or approximately 110 mV when peak-to-peak voltage is 2.2 V. The random noise cited in the data sheet for this same part is 1 mV rms (or about 5 mV peak-to-peak). In this example, the DSNU is 22 times larger than the random noise floor. Accordingly, by removing or correcting the FPN, a significant improvement in the signal to noise ratio could be achieved.</p>
<p id="p-0023" num="0022">The effects of FPN may be reduced by various techniques, such as increasing the level of illumination, increasing the exposure time, or selecting components with low FPN. Increasing the level of illumination may result in an increased signal strength and an improved signal-to-noise ratio; however, high intensity illumination may not always be available or convenient for a user. Longer exposure times may also reduce FPN in a given signal; however, longer exposure times may introduce other noise sources into the system and may reduce the performance of an imaging system (such as reducing the tolerance of the imaging system to object motion). Finally, components may be selected that are less prone to noise; however, such components may be more expensive and may increase the cost of the system.</p>
<p id="p-0024" num="0023">Other techniques for compensating for FPN may include calibration in non-operating conditions to obtain estimates of FPN for a given exposure time and temperature. Such calibration, however, may require a system to retain a variety of estimates of FPN, thus requiring additional resources. Further, since FPN may be unique to an individual CCD or CMOS imager, individual calibration of each CCD or CMOS imager may be required. Calibration of each individual CCD or CMOS imager would be costly and time consuming.</p>
<p id="p-0025" num="0024">As described above, FPN may include at least three types of noise: (1) DSNU due to transistor threshold mismatch, (2) DSNU due to dark current variation, and (3) PRNU. Noise from dark current DSNU may vary according to temperature and exposure time, with longer exposure times and/or higher temperatures resulting in greater noise from dark current DSNU. Further, low light conditions (e.g., less than 100 lux) may cause dark current DSNU to be a significant noise source. Certain embodiments disclosed herein compensate, at least partially, for dark current DSNU in an image sensor of an optical data reader. In certain applications, however, one or more sources of noise may not have a significant impact on the application. For example, it may be determined that when an exposure time is less than a particular threshold (e.g., less than 500 microseconds), the dark current DSNU component of FPN is small enough that it may be ignored.</p>
<p id="p-0026" num="0025">Similarly, it may be determined that the impact of PRNU on a particular application is tolerable when subtle differences in pixel gain do not adversely affect the performance of an imager. For example, when an optical code is imaged by an optical code reader, the image is analyzed for the stark contrast between bars and spaces (typically black bars and white spaces). In such an application, the high contrast between the black bars and the white background makes the impact of PRNU in the signal less important.</p>
<p id="p-0027" num="0026">In certain applications and with certain CMOS imagers, FPN may be approximated as simply DSNU due to transistor threshold mismatch because dark current DSNU and PRNU are either negligible or are unimportant for the particular application. An estimate of threshold mismatch may be acquired using a single dark exposure having a short exposure time because threshold mismatch is relatively unaffected by exposure time. Further, since the exposure time is short, the dark image will contain only negligible scene information, and thus, the same dark image may be subtracted from a plurality of illuminated images in order to compensate for FPN. This process is preferably done during normal operation of the system because the threshold voltage varies with temperature. The illuminated images and dark images will have the same threshold voltage DSNU characteristics if captured within a reasonable time frame, since the temperature would not vary significantly between measurements.</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 1</figref> illustrates an exemplary method <b>100</b> for compensating for FPN. In the method <b>100</b>, FPN is approximated using a single dark image. Alternatively, several dark images could be captured and averaged in order to make a more accurate measurement of FPN, if the readout noise of the system is not negligible compared to the magnitude of the FPN. By averaging several dark images, the effect of readout noise is reduced, since the readout noise is uncorrelated between the images. By averaging a suitable number of dark images, the effect of readout noise on measurement of FPN can also be reduced. At step <b>105</b>, a dark image is captured using a short exposure time. The captured image may be referred to as &#x201c;dark&#x201d; because the period of the exposure may be brief. In this brief exposure period the number of photons striking the image sensor will be low. Accordingly, the resulting dark image will contain only negligible scene information, but may provide an estimate of the threshold mismatch of the image sensor according to the then existing conditions. Assuming that dark current DSNU and PRNU are negligible, a dark image may be captured, stored, and subtracted from one, or a plurality of illuminated images, each of which may have a different exposure time, in order to compensate for FPN.</p>
<p id="p-0029" num="0028">A dark image has an exposure time that is short (i.e., the influence of the illumination of the scene is negligible). For example, if an exposure time is T seconds, an exposure of T/256 seconds would have 1/256 of the scene intensity. In such a system, any error attributable to scene information would be less than 1 bit in an 8 bit system. A suitable exposure time for a dark image may vary depending on the imager and the conditions of operation. A preferred range of an exposure time for a dark image is between approximately 1 microsecond and 1,000 microseconds. In an alternate embodiment, the range is between 5 and 30 microseconds. Such a range may effectively capture an estimate of FPN while minimizing the amount of scene information contained in the dark image. The optical reading systems and methods of the preferred embodiments are not limited to the preferred range, and exposure times less than 1 microsecond or more than 1,000 microseconds may also be used.</p>
<p id="p-0030" num="0029">At step <b>110</b>, the dark image is digitized, and at step <b>120</b> the digitized image is stored. The digitization may be accomplished using a microprocessor (such as a DSP) and an analog-to-digital converter (ADC) or any other suitable processing system. At step <b>120</b>, the digitized dark image is stored in a computer-readable storage medium. The computer-readable storage medium may be embodied as RAM, a hard disk drive, flash memory, or other suitable type of digital storage device. Similarly, at step <b>140</b>, an illuminated image is digitized, and in certain embodiments the illuminated image may be stored in the computer-readable storage medium.</p>
<p id="p-0031" num="0030">At step <b>130</b>, an illuminated image is captured using a typical exposure time. An illuminated image, as the term is used herein, is the result of an exposure time based on the conditions that are likely to allow the optical code reader to obtain a suitable image of the desired scene information. Illumination may be from ambient light, or from self generated light, such as from LEDs, or a combination thereof. In certain embodiments, an illuminated exposure time may be a constant value, while in other embodiments, the exposure time may be based on measurements of the ambient light or measurements of the returned signal. Various exposure control systems may be utilized to realize a suitable range of exposure times.</p>
<p id="p-0032" num="0031">In one preferred embodiment, the exposure time of the illuminated image is between 100 and 1,000 times longer than the exposure time of the dark image. In an alternate embodiment, the exposure time of the illuminated image is between 200 and 300 times longer than the exposure time of the dark image. In still a more preferred embodiment, the exposure time of the illuminated image is approximately 256 times longer than the exposure time of the dark image. Exposure times less than 100 times or more than 1,000 times longer than the exposure time of the dark image may also be used. For example, certain embodiments may be configured for short illuminated exposure times, and accordingly, may have an illuminated exposure time of only 50 times longer than the exposure time of the dark image.</p>
<p id="p-0033" num="0032">The illuminated image includes both desired data about the optical code (e.g., scene information) to be read and FPN that is approximated by the dark image stored at step <b>120</b>. Accordingly, at step <b>150</b>, a compensated image is generated by subtracting the stored dark image from the illuminated image. At step <b>160</b>, the compensated image may be outputted for further processing, such as edge detection and reading of the optical code.</p>
<p id="p-0034" num="0033">An updated dark image may be acquired periodically during use, or may be captured as required by conditions, such as changes in temperature. Various components may be included in the optical code reader to determine when reacquisition of the dark image is required. Since dark current DSNU is influenced by temperature, in one embodiment the optical code reader may periodically determine the temperature, and when a change in temperature is detected, the dark image may be reacquired. From step <b>170</b> the process returns to step <b>105</b>, if the dark image is to be reacquired. If the dark image does not need to be reacquired, the process returns to step <b>130</b>.</p>
<p id="p-0035" num="0034">The method <b>100</b> may be implemented in an optical code reader such that a dark image may be acquired during normal operation of the optical code reader. In other words, special equipment (e.g., a calibration box) and special conditions (e.g., particular lighting conditions and particular temperatures) are not required in order to obtain an estimate of FPN using a dark image. For example, in certain embodiments using a trigger pull scanner, a new dark image may be obtained each time the trigger is pulled. In various embodiments, the system may capture a dark image each time the system is activated, or may recapture a dark image according to an established schedule or as environmental conditions require. Further, the method <b>100</b> may be implemented such that an optical code reader compensates for FPN during normal operation. For example, in certain embodiments using a trigger pull scanner, method <b>100</b> may be performed each time the trigger is pulled. Method <b>100</b> is applicable, in various embodiments, to compensate for fixed pattern noise in both linear and two dimensional imagers.</p>
<p id="p-0036" num="0035">In one embodiment, a microprocessor implements the method <b>100</b>, as illustrated in <figref idref="DRAWINGS">FIG. 1</figref>, using a linear imager and the following pseudocode description of the method. The following pseudocode is copyright 2008 Datalogic Scanning, Inc.:</p>
<p id="p-0037" num="0036">1. Capture a dark linear image using exposure time T<b>1</b>.</p>
<p id="p-0038" num="0037">2. Digitize and store pixels from dark linear image into array D(i)=ADC(i), where i=0 to number of pixels in linear array, and ADC(i) is the digitized result from an analog to digital converter of pixel i.</p>
<p id="p-0039" num="0038">3. Capture an illuminated linear image using exposure time T<b>2</b>.</p>
<p id="p-0040" num="0039">4. Digitize and store a fixed pattern noise compensated image C(i)=ADC(i)&#x2212;D(i), where ADC(i) is the digitized illuminated linear image and D(i) is the stored dark linear image.</p>
<p id="p-0041" num="0040">Code for one embodiment of the method <b>100</b> of <figref idref="DRAWINGS">FIG. 1</figref> adapted for use on the Analog Devices model DSP2802 processor is set forth below. If the variable bGotDark is false, it is assumed that the data being captured was from a dark linear image, and the input data is captured into the array pInputPixels. If the variable bGotDark is true, then it is assumed that the dark image was captured and the illuminated image is captured. Accordingly, the dark image is subtracted from the illuminated image. The subtraction occurs at lines <b>17</b>-<b>22</b>. The output array is an 8 bit data type, while the input data is 16 bits. The following code is copyright 2008 Datalogic Scanning, Inc.:</p>
<p id="p-0042" num="0041">
<tables id="TABLE-US-00001" num="00001">
<table frame="none" colsep="0" rowsep="0" pgwide="1">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="14pt" align="left"/>
<colspec colname="2" colwidth="308pt" align="left"/>
<thead>
<row>
<entry namest="1" nameend="2" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry>01</entry>
<entry>/*</entry>
</row>
<row>
<entry>02</entry>
<entry>&#x2009;* ======== hwi_adc_task ========</entry>
</row>
<row>
<entry>03</entry>
<entry>&#x2009;* adc hardware interrupt (int1)</entry>
</row>
<row>
<entry>04</entry>
<entry>&#x2009;* this interrupt is triggered by an adc sequence</entry>
</row>
<row>
<entry>05</entry>
<entry>&#x2009;* copy the adc register into the pixel buffer</entry>
</row>
<row>
<entry>06</entry>
<entry>&#x2009;*</entry>
</row>
<row>
<entry>07</entry>
<entry>&#x2009;*/</entry>
</row>
<row>
<entry>08</entry>
<entry>interrupt void hwi_adc(void)</entry>
</row>
<row>
<entry>09</entry>
<entry>{</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="28pt" align="left"/>
<colspec colname="2" colwidth="294pt" align="left"/>
<tbody valign="top">
<row>
<entry>10</entry>
<entry>Uint16 nVal = AdcRegs.ADCRESULT0;</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="322pt" align="left"/>
<tbody valign="top">
<row>
<entry>11</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="28pt" align="left"/>
<colspec colname="2" colwidth="294pt" align="left"/>
<tbody valign="top">
<row>
<entry>12</entry>
<entry>// Data is converted from 16 bit data type to 8 bit data type</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="322pt" align="left"/>
<tbody valign="top">
<row>
<entry>13</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="28pt" align="left"/>
<colspec colname="2" colwidth="294pt" align="left"/>
<tbody valign="top">
<row>
<entry>14</entry>
<entry>// if we have darkPixels then subtract dark from current adcreg</entry>
</row>
<row>
<entry>15</entry>
<entry>if(bGotDark)</entry>
</row>
<row>
<entry>16</entry>
<entry>{</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="42pt" align="left"/>
<colspec colname="2" colwidth="280pt" align="left"/>
<tbody valign="top">
<row>
<entry>17</entry>
<entry>if (bHi)</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="56pt" align="left"/>
<colspec colname="2" colwidth="266pt" align="left"/>
<tbody valign="top">
<row>
<entry>18</entry>
<entry>plnputPixels[*pNdxInput] = (nVal - darkPixels[*pNdxInput*2]) &#x3c;&#x3c; 8;</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="42pt" align="left"/>
<colspec colname="2" colwidth="280pt" align="left"/>
<tbody valign="top">
<row>
<entry>19</entry>
<entry>else</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="56pt" align="left"/>
<colspec colname="2" colwidth="266pt" align="left"/>
<tbody valign="top">
<row>
<entry>20</entry>
<entry>pInputPixels[*pNdxInput++] |= (nVal - darkPixels[*pNdxInput*2]) &#x26; 0xff;</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="322pt" align="left"/>
<tbody valign="top">
<row>
<entry>21</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="42pt" align="left"/>
<colspec colname="2" colwidth="280pt" align="left"/>
<tbody valign="top">
<row>
<entry>22</entry>
<entry>bHi = !bHi;</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="28pt" align="left"/>
<colspec colname="2" colwidth="294pt" align="left"/>
<tbody valign="top">
<row>
<entry>23</entry>
<entry>}</entry>
</row>
<row>
<entry>24</entry>
<entry>else</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="42pt" align="left"/>
<colspec colname="2" colwidth="280pt" align="left"/>
<tbody valign="top">
<row>
<entry>25</entry>
<entry>pInputPixels[*pNdxInput++] = nVal; // 16bit valued dark pixels</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="322pt" align="left"/>
<tbody valign="top">
<row>
<entry>26</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="28pt" align="left"/>
<colspec colname="2" colwidth="294pt" align="left"/>
<tbody valign="top">
<row>
<entry>27</entry>
<entry>// if pixel buffer is full then turn off interrupt</entry>
</row>
<row>
<entry>28</entry>
<entry>if ((bGotDark &#x26;&#x26; (*pNdxInput &#x3e;= BUFFSIZE/2)) || (!bGotDark &#x26;&#x26; (*pNdxInput &#x3e;= BUFFSIZE)))</entry>
</row>
<row>
<entry>29</entry>
<entry>{</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="42pt" align="left"/>
<colspec colname="2" colwidth="280pt" align="left"/>
<tbody valign="top">
<row>
<entry>30</entry>
<entry>EPwm1 Regs.ETSEL.bit.SOCAEN = 0x0; // disable soca</entry>
</row>
<row>
<entry>31</entry>
<entry>AdcRegs.ADCTRL2.bit.INT_ENA_SEQ1 = 0x0; // disable hwi(int1)</entry>
</row>
<row>
<entry>32</entry>
<entry>bFrameDone = 1;</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="28pt" align="left"/>
<colspec colname="2" colwidth="294pt" align="left"/>
<tbody valign="top">
<row>
<entry>33</entry>
<entry>}</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="322pt" align="left"/>
<tbody valign="top">
<row>
<entry>34</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="28pt" align="left"/>
<colspec colname="2" colwidth="294pt" align="left"/>
<tbody valign="top">
<row>
<entry>35</entry>
<entry>// Reinitialize for next ADC sequence</entry>
</row>
<row>
<entry>36</entry>
<entry>AdcRegs.ADCTRL2.bit.RST SEQ1 = 1;&#x2003;// Reset SEQ1</entry>
</row>
<row>
<entry>37</entry>
<entry>AdcRegs.ADCST.bit.INT_SEQ1_CLR = 1;&#x2003;// Clear INT SEQ1 bit</entry>
</row>
<row>
<entry>38</entry>
<entry>PieCtrlRegs.PIEACK.all = PIEACK_GROUP1; //Acknowledge interrupt to PIE</entry>
</row>
<row>
<entry>39</entry>
<entry>}</entry>
</row>
<row>
<entry namest="1" nameend="2" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. 2</figref> is a graph of sample results of the method of <figref idref="DRAWINGS">FIG. 1</figref> for compensating for FPN using dark image subtraction. The signals shown in <figref idref="DRAWINGS">FIG. 2</figref> were acquired using a model RPLIS-2048 linear CMOS imager available from Panavision Imaging, LLC, Horner, N.Y. Signal <b>200</b> is a digitized representation of a signal corresponding to a dark linear image having an exposure time of 20 microseconds. Signal <b>210</b> is a digitized representation of a signal corresponding to an illuminated image of an optical code having an exposure time of 2 milliseconds. Signal <b>220</b> is a representation of a signal corresponding to a compensated image, where signal <b>200</b> has been subtracted from signal <b>210</b>. The optical code information contained in signal <b>210</b> is obscured by noise; however, the optical code information is made clear by subtracting signal <b>200</b> from signal <b>210</b>.</p>
<p id="p-0044" num="0043">Another example of a method <b>300</b>, according to the principles disclosed herein, is shown in <figref idref="DRAWINGS">FIG. 3</figref>. In the method <b>300</b>, an array of virtual scan lines are utilized to compensate for FPN. The creation and use of virtual scan lines in optical code readers is described in U.S. Pat. No. 7,398,927, U.S. Publication No. 2008/0169347, and U.S. Publication No. 2006/0081712, which are incorporated herein by reference. A virtual scan line may be selected from a two dimensional image of an optical code symbol having information bearing characteristics in predominantly a single dimension. In some embodiments incorporating virtual scan lines, only a select portion of the data may be stored and processed. In other embodiments, pixels may be summed or averaged from a portion of the image to generate virtual scan lines to improve the signal-to-noise ratio of the resulting virtual scan lines. Pixel summing or averaging of an imaging array may be performed in a direction orthogonal to the virtual scan line direction, in order to increase the signal to noise ratio of the virtual scan line signal over what can be achieved with virtual scan lines created using a single image pixel for each virtual scan line pixel.</p>
<p id="p-0045" num="0044">At step <b>305</b>, a two dimensional dark image (D) is acquired using an area image sensor. At step <b>310</b>, N dark virtual scan lines of length L are generated and a dark virtual scan line array (VD) is created. The location of a particular pixel within the dark virtual scan line array may be specified as VD(n,i), where n is a number between 0 and N&#x2212;1, and i is a number between 0 and L&#x2212;1. At step <b>320</b>, VD(n,i) is stored. At step <b>330</b>, a two dimensional illuminated image (E) is captured using an appropriate exposure time. At step <b>340</b>, an illuminated virtual scan line array VE(n,i) containing N virtual scan lines is created from image E. At step <b>350</b>, a compensated virtual scan line array (VC) is created, where VC(n,i)=VE(n,i)&#x2212;VD(n,i). In certain embodiments, a single dark virtual scan line and a single illuminated virtual scan line may be used in place of the arrays of virtual scan lines shown in <figref idref="DRAWINGS">FIG. 3</figref>. At step <b>360</b>, VC(n,i) is outputed for processing.</p>
<p id="p-0046" num="0045">Utilizing virtual scan lines may reduce the computational load that would be required to capture, digitize, and store an entire dark image, and to subtract an entire dark image from an entire illuminated image. Thus, embodiments utilizing virtual scan lines may require fewer resources than would otherwise be required, and accordingly may be less expensive. The methods of compensating for FPN disclosed herein may be applied to full images, particularly in embodiments having sufficient computational power.</p>
<p id="p-0047" num="0046">Another example of a method <b>400</b>, according to the principles disclosed herein, is shown in <figref idref="DRAWINGS">FIG. 4</figref>. The method <b>400</b> may be utilized to obtain an estimate of both threshold mismatch DSNU and dark current DSNU of a particular image sensor. At step <b>410</b>, a first dark image (D<b>1</b>) is captured using a first exposure time (T<b>1</b>). D<b>1</b> and T<b>1</b> are stored at step <b>415</b>. At step <b>420</b>, a second dark image (D<b>2</b>) is captured using a second exposure time (T<b>2</b>), where T<b>2</b> is greater than T<b>1</b>. At step <b>425</b>, D<b>2</b> and T<b>2</b> are stored. At step <b>430</b>, an illuminated image (I<b>1</b>) is captured using an exposure time (T<b>3</b>), where T<b>3</b> is greater than T<b>2</b>. At step <b>435</b>, <b>11</b> and T<b>3</b> are stored.</p>
<p id="p-0048" num="0047">In one preferred embodiment of method <b>400</b>, T<b>3</b> is between 300 and 700 times longer than T<b>1</b>, and T<b>3</b> is between 100 and 300 times longer than T<b>2</b>. In yet a more preferred embodiment of method <b>400</b>, T<b>3</b> is approximately 512 times longer than T<b>1</b>, and T<b>3</b> is approximately 256 times longer than T<b>2</b>.</p>
<p id="p-0049" num="0048">At step <b>440</b>, an estimate of threshold mismatch DSNU (TMDSNU) is created based on D<b>1</b>, T<b>1</b>, T<b>2</b>, and D<b>2</b>. The dark images D<b>1</b> and D<b>2</b> are composed of both TMDSNU and a dark current signal that is a product of the exposure time T<b>1</b> or T<b>2</b> and the dark current DSNU (DCDSNU), as shown in Eq. 1 and Eq. 2.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>D</i>1<i>=TMDSNU+DCDSNU*T</i>1&#x2003;&#x2003;Eq. 1<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>D</i>2<i>=TMDSNU+DCDSNU*T</i>2&#x2003;&#x2003;Eq. 2<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
The threshold mismatch, may therefore be expressed according to Eq. 3.
</p>
<p id="p-0050" num="0049">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mi>TMDSNU</mi>
        <mo>=</mo>
        <mrow>
          <mrow>
            <mi>T</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mn>2</mn>
            <mo>*</mo>
            <mfrac>
              <mrow>
                <mi>D</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mn>1</mn>
              </mrow>
              <mrow>
                <mrow>
                  <mi>T</mi>
                  <mo>&#x2062;</mo>
                  <mstyle>
                    <mspace width="0.3em" height="0.3ex"/>
                  </mstyle>
                  <mo>&#x2062;</mo>
                  <mn>2</mn>
                </mrow>
                <mo>-</mo>
                <mrow>
                  <mi>T</mi>
                  <mo>&#x2062;</mo>
                  <mstyle>
                    <mspace width="0.3em" height="0.3ex"/>
                  </mstyle>
                  <mo>&#x2062;</mo>
                  <mn>1</mn>
                </mrow>
              </mrow>
            </mfrac>
          </mrow>
          <mo>-</mo>
          <mrow>
            <mi>T</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mn>1</mn>
            <mo>*</mo>
            <mfrac>
              <mrow>
                <mi>D</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mn>2</mn>
              </mrow>
              <mrow>
                <mrow>
                  <mi>T</mi>
                  <mo>&#x2062;</mo>
                  <mstyle>
                    <mspace width="0.3em" height="0.3ex"/>
                  </mstyle>
                  <mo>&#x2062;</mo>
                  <mn>2</mn>
                </mrow>
                <mo>-</mo>
                <mrow>
                  <mi>T</mi>
                  <mo>&#x2062;</mo>
                  <mstyle>
                    <mspace width="0.3em" height="0.3ex"/>
                  </mstyle>
                  <mo>&#x2062;</mo>
                  <mn>1</mn>
                </mrow>
              </mrow>
            </mfrac>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mi>Eq</mi>
        <mo>.</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mn>3</mn>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
If T<b>1</b> is much shorter than T<b>2</b>, then the calculation simplifies to TMDSNU=D<b>1</b>. At step <b>445</b>, TMDSNU, is stored.
</p>
<p id="p-0051" num="0050">At step <b>450</b>, an estimate of the dark current DSNU (DCDSNU) and a scaling factor may be created using D<b>1</b>, T<b>1</b>, D<b>2</b>, T<b>2</b>, and T<b>3</b>. As discussed above, dark current DSNU is a function of time and temperature. Accordingly, certain embodiments may also utilize a temperature measurement in estimating dark current DSNU. In one embodiment, subtracting D<b>1</b> from D<b>2</b> provides an estimate of DSNU due to dark current, because threshold mismatch DSNU is common to both measurements. D<b>1</b> is subtracted from D<b>2</b> to eliminate an estimate of the threshold mismatch DSNU and to retain the DCDSNU that accumulated from the period of time between T<b>2</b> and T<b>1</b>. Accordingly, DCDSNU is equal to the difference between D<b>2</b> and D<b>1</b>, as shown in Eq. 4.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>DCDSNU=D</i>2<i>&#x2212;D</i>1&#x2003;&#x2003;Eq. 4<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
The time difference between T<b>1</b> and T<b>2</b> may then be compared to T<b>3</b> in order to create a scaling factor, S, that may appropriately scale the estimate of dark current DSNU based on the exposure time T<b>3</b>. In certain embodiments, Eq. 5 expresses the relationship between T<b>1</b>, T<b>2</b>, T<b>3</b>, and S.
</p>
<p id="p-0052" num="0051">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mi>S</mi>
        <mo>=</mo>
        <mfrac>
          <mrow>
            <mi>T</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mn>3</mn>
          </mrow>
          <mrow>
            <mrow>
              <mi>T</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mn>2</mn>
            </mrow>
            <mo>-</mo>
            <mrow>
              <mi>T</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mn>1</mn>
            </mrow>
          </mrow>
        </mfrac>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mi>Eq</mi>
        <mo>.</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mn>5</mn>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0053" num="0052">The scaling factor S may then be multiplied by DCDSNU (the estimate of dark current DSNU obtained by subtracting D<b>1</b> from D<b>2</b>, as shown in Eq. 4). At step <b>455</b>, the estimate of the dark current DSNU and the scaling factor may be stored. At step <b>460</b>, a compensated image CI<b>1</b> may be generated, according to the relationship shown in Eq. 6, and C<b>11</b> may be outputted at <b>470</b>.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>CI</i>1<i>=I</i>1&#x2212;(<i>TMDSNU+S*DCDSNU</i>)&#x2003;&#x2003;Eq. 6<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0054" num="0053">In some embodiments, more than two dark images may be acquired, and mathematical modeling techniques may be utilized to refine the estimate of DCDSNU as a function of time. Such mathematical modeling techniques may include logarithmic, exponential, and polynomial curve fitting techniques or other suitable techniques.</p>
<p id="p-0055" num="0054">A variety of systems are disclosed herein for compensating for FPN in an image sensor. In one system, the methods for compensating for FPN as described herein may be performed by a microprocessor in an optical code reader. In another system, the methods for compensating for FPN may be performed by a general purpose computer connected to an optical code reader utilizing a software program which implements the methods for compensating for FPN disclosed herein.</p>
<p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. 5</figref> is a block diagram of one embodiment of an optical data reader <b>500</b>, such as a barcode scanner. While a bus-based architecture, based on a bus <b>532</b>, is illustrated in <figref idref="DRAWINGS">FIG. 5</figref>, other architectures are also suitable. For example, one or more of the components may be directly coupled to each other. The optical data reader <b>500</b> may incorporate an image reading engine <b>502</b> to read optically encoded symbols. Image reading engine <b>502</b> may encompass a CCD-based or CMOS-based imager.</p>
<p id="p-0057" num="0056">The portable data reader <b>500</b> may further include a processor <b>504</b>. The processor <b>504</b> may include any commercially available processor or other logic device capable of executing instructions, such as a general-purpose microprocessor, microcontroller, digital signal processor, application specific integrated circuit (ASIC), programmable logic array (PLA), field programmable gate array (FPGA), or the like. The processor <b>504</b> executes one or more programs to control the operation of other components of the portable data reader <b>500</b> (i.e., the transfer of data between the other components, the association of data from various components, the performance of calculations, and the presentation of results to the user). The processor <b>504</b> may, for example, be configured to perform the subtraction of a dark image from an illuminated image to generate an image that is, at least partially, compensated for fixed pattern noise.</p>
<p id="p-0058" num="0057">Likewise, the portable data reader <b>500</b> may include an input controller <b>506</b> to receive user input from a keypad <b>508</b> or other input device, such as a pointing device, touch screen, microphone, temperature sensor, or other wired/wireless input devices. While various input devices may be integrated into the portable data reader <b>500</b> and coupled to the processor <b>504</b> via the input controller <b>506</b>, input devices may also connect via other interfaces, such as a port <b>592</b>. The port <b>592</b> may include one or more data interfaces, bus interfaces, wired or wireless network adapters, or modems for transmitting and receiving data. Accordingly, the input controller <b>506</b> may include one or more of hardware, software, and firmware to implement one or more protocols, such as stacked protocols along with corresponding layers. Thus, the port <b>592</b> may be embodied as one or more of a serial port (e.g., RS-232), a Universal Serial Bus (USB) port, an IEEE 1394 port, an IR interface, and the like. The input controller <b>506</b> may also support various wired, wireless, optical, and other communication standards.</p>
<p id="p-0059" num="0058">A display controller <b>510</b> may drive a display <b>512</b>. Display <b>512</b> may present data, menus, and prompts, and otherwise may be used to communicate with the user. Display <b>512</b> may be embodied as a transmissive or reflective liquid crystal display (LCD), cathode ray tube (CRT) display, touch screen, or other suitable display and/or input device.</p>
<p id="p-0060" num="0059">The portable data reader <b>500</b> may also include a network interface <b>514</b> to communicate with an external network. Network interface <b>514</b> or port <b>592</b> may be utilized to communicate with one or more hosts <b>597</b> or other devices (e.g., a computer or a point-of-sale terminal). For example, data gathered by, or decoded by, the portable data reader <b>500</b> may be passed along to the host computer <b>597</b>. The host computer <b>597</b> may present data, prompts, and otherwise communicate with the user via the display <b>512</b>. For example, the computer may present decoded data to the user via the display <b>512</b>, such as the object type (e.g., product type) corresponding to the scanned barcode and data associated with the object type (e.g., a price of the product). The data associated with the object type may be encoded in the barcode or accessed from a local or remote database based upon the object type. By way of another example, the host computer <b>597</b> may cause decoded data to be recorded on a tangible medium. For example, the host computer <b>597</b> may instruct a printer (not shown) to print the object type and data corresponding to the object type (e.g., print the product type and associated price on a receipt). The host computer <b>597</b> may be any machine that manipulates data according to a list of instructions (e.g., a point-of-sale terminal or any hardware/software used where a transaction takes place, such as a checkout counter in a shop). For example, the host computer <b>597</b> may comprise a mobile device, server, personal computer, or embedded computer. The network interface <b>514</b> may facilitate wired or wireless communication with other devices over a short distance (e.g., Bluetooth&#x2122;) or long distances (e.g., the Internet). In the case of a wired connection, a data bus may be provided using any protocol, such as IEEE 802.3 (Ethernet), advanced technology attachment (ATA), personal computer memory card international association (PCMCIA), and USB. A wireless connection may use low or high powered electromagnetic waves to transmit data using any wireless protocol, such as Bluetooth&#x2122;, IEEE 802.11a/b/g/n (or other WiFi standards), infrared data association (IrDa), and radiofrequency identification (RFID).</p>
<p id="p-0061" num="0060">The portable data reader <b>500</b> further includes a memory <b>516</b>, which may be implemented using one or more standard memory devices. The memory devices may include RAM, ROM, and/or EEPROM devices, and may also include magnetic and/or optical storage devices, such as hard disk drives, CD-ROM drives, DVD-ROM drives, etc.</p>
<p id="p-0062" num="0061">The memory <b>516</b> may store various functional modules, settings, and user data. For instance, the memory <b>516</b> may include an operating system (OS) 518. Any suitable operating system <b>518</b> may be employed.</p>
<p id="p-0063" num="0062">As illustrated, the memory <b>516</b> may further include an FPN compensation module <b>528</b>. The FPN compensation module <b>528</b> may contain instructions executable on the processor <b>504</b> that allow the portable data reader to capture one or more dark images, which may be stored in a read data module <b>524</b>. Further, the FPN compensation module <b>528</b> may contain instructions for creating an estimate of threshold mismatch dark signal non-uniformity and/or dark current dark signal non-uniformity. In various embodiments, the captured dark images may be linear or two dimensional. The FPN compensation module <b>528</b> may also contain instructions for subtracting the dark image, or the estimates of the threshold mismatch dark signal non-uniformity and the dark current dark signal non-uniformity, from an illuminated image. All of the foregoing may be stored within, or indexed by, a file system <b>530</b>, which is typically managed by the OS 518. In various embodiments, the function performed by the FPN compensation module in conjunction with the processor <b>504</b> may be performed by an FPN estimation subsystem.</p>
<p id="p-0064" num="0063">The memory <b>516</b> may also include various applications <b>520</b>, which may be operable to perform a variety of functions in conjunction with the processor <b>504</b>. One such application may include instructions executable on the processor <b>504</b> for generating virtual scan lines, as disclosed in U.S. Pat. No. 7,398,927, U.S. Publication No. 2008/0169347, and U.S. Publication No. 2006/0081712, which are already incorporated herein by reference. In various embodiments, the function performed by the application for generating virtual scan lines in conjunction with the processor <b>504</b> may be performed by a virtual scan line subsystem.</p>
<p id="p-0065" num="0064">In one embodiment, the memory <b>516</b> may further store a number of settings <b>526</b> for the portable data reader <b>500</b>. The settings <b>526</b> may include various symbology settings, device (e.g., user-interface) settings, and network settings.</p>
<p id="p-0066" num="0065">Although the embodiment illustrated in <figref idref="DRAWINGS">FIG. 5</figref> illustrates various software modules located in the memory <b>516</b>, in other embodiments, the functions associated with the various software modules may be performed in other ways. For example, various subsystems may be employed that utilize application specific integrated circuits or other hardware implementations to perform the described functions. In another example, a combination of hardware and software is employed to perform the functionality of the various modules. While the illustrated embodiment depicts one possible configuration of a portable data reader <b>500</b>, a wide variety of hardware and software configurations may be provided.</p>
<p id="p-0067" num="0066">As described above, dark current DSNU may vary according to temperature. Accordingly, certain embodiments may also include a temperature sensor <b>522</b>. Temperature sensor <b>522</b> may be utilized, for example, to detect changes in temperature, and based upon detected changes in temperature, to cause the portable data reader <b>500</b> to reacquire the dark image. In other embodiments, information from the temperature sensor <b>522</b> may be utilized in calculating an estimate of dark current DSNU.</p>
<p id="p-0068" num="0067">It will be apparent to those having skill in the art that many changes may be made to the details of the above-described embodiments without departing from the underlying principles of the present disclosure.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08622302-20140107-M00001.NB">
<img id="EMI-M00001" he="6.35mm" wi="76.20mm" file="US08622302-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US08622302-20140107-M00002.NB">
<img id="EMI-M00002" he="6.35mm" wi="76.20mm" file="US08622302-20140107-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>What is claimed:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method of compensating for fixed pattern noise in an image sensor of an optical data reader, the method comprising:
<claim-text>capturing a dark image using the image sensor, the dark image comprising a negligible amount of scene information to thereby provide an estimate of fixed pattern noise in the image sensor, the dark image having a corresponding first exposure time;</claim-text>
<claim-text>storing the dark image;</claim-text>
<claim-text>capturing an illuminated image comprising optical data from a read area using the image sensor, the illuminated image having a corresponding second exposure time, wherein the second exposure time exceeds the first exposure time;</claim-text>
<claim-text>subtracting the dark image from the illuminated image to generate a compensated image comprising optical data that is, at least partially, compensated for fixed pattern noise in the image sensor;</claim-text>
<claim-text>during normal operation of the optical data reader, detecting a change in a condition that affects the fixed pattern noise;</claim-text>
<claim-text>based upon detecting the change in the condition, capturing a new dark image and replacing the dark image with the new dark image; and</claim-text>
<claim-text>subtracting the new dark image from a subsequently captured illuminated image to generate a subsequent compensated image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the second exposure time is between 100 and 1,000 times longer than the first exposure time.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the second exposure time is between 200 and 300 times longer than the first exposure time.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first exposure time is between 1 microsecond and 1,000 microseconds.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first exposure time is between 5 microsecond and 30 microseconds.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the optical data reader comprises an optical code reader and wherein capturing the dark image and storing the dark image are performed each time the optical code reader is activated.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein subtracting the dark image from the illuminated image occurs prior to acquisition of a subsequent illuminated image.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>capturing a plurality of illuminated images, the plurality of illuminated images having a corresponding plurality of different exposure times;</claim-text>
<claim-text>subtracting the dark image from each of the plurality of illuminated images to generate a plurality of images, at least partially, compensated for fixed pattern noise.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>capturing a plurality of preliminary dark images using the image sensor; and</claim-text>
<claim-text>averaging the plurality of preliminary dark images in order to generate the dark image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein
<claim-text>the condition for which a change is detected is one of temperature, voltage, exposure time, and lighting.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. An optical data reader, comprising:
<claim-text>an image sensor to capture a dark image comprising a negligible amount of scene information to thereby provide an estimate of fixed pattern noise in the image sensor, the dark image having a corresponding first exposure time, and to capture an illuminated image comprising optical data from a read area, the illuminated image having a corresponding second exposure time, and wherein the second exposure time exceeds the first exposure time;</claim-text>
<claim-text>a non-transitory computer-readable storage medium to store the dark image; and</claim-text>
<claim-text>a fixed pattern noise compensation subsystem to subtract the dark image from the illuminated image to generate a compensated image comprising optical data that is, at least partially, compensated for fixed pattern noise in the image sensor, wherein during normal operation the fixed pattern noise compensation subsystem detects a change in a condition that affects the fixed pattern noise in the image sensor and, based upon detecting the change in the condition, directs the image sensor to capture a new dark image and replaces the dark image with the new dark image in the non-transitory computer-readable storage medium, wherein the new dark image is subtracted from a subsequent illuminated image to generate a subsequent compensated image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The optical data reader of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the fixed pattern noise compensation subsystem comprises a processor; and
<claim-text>wherein the computer-readable storage medium further stores a fixed pattern noise compensation module executable on the processor.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The optical data reader of <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising:
<claim-text>a temperature sensor to detect a change of temperature of the image sensor, wherein the condition for which the change is detected is temperature, and wherein, based upon a detection of a change of temperature measured by the temperature sensor, the image sensor recaptures the dark image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The optical data reader of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the image sensor comprises a linear CMOS imager.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The optical data reader of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the image sensor comprises a two dimensional CMOS imager, and wherein the optical data reader further comprises:
<claim-text>a virtual scan line generation subsystem to generate a virtual scan line array from a two dimensional image of an optical code symbol; and</claim-text>
<claim-text>wherein at least one of the dark image and the illuminated image comprise a virtual scan line array.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The optical data reader of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the optical data reader reads at least one of a barcode, a signature, a fingerprint, a handprint.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. A method of compensating for fixed pattern noise in an image sensor of an optical data reader, the method comprising:
<claim-text>capturing a two dimensional dark image using the image sensor, the dark image comprising a negligible amount of scene information to thereby provide an estimate of fixed pattern noise in the image sensor, the dark image having a corresponding first exposure time;</claim-text>
<claim-text>storing a portion of the two dimensional dark image along a pattern of virtual scan lines to produce a dark virtual scan line array;</claim-text>
<claim-text>capturing a two dimensional illuminated image comprising optical data from a read area, the illuminated image having a corresponding second exposure time;</claim-text>
<claim-text>generating from the two dimensional illuminated image an illuminated virtual scan line array along a pattern of virtual scan lines;</claim-text>
<claim-text>subtracting the dark virtual scan line array from the illuminated virtual scan line array to generate a two dimensional compensated image comprising optical data that is, at least partially, compensated for fixed pattern noise in the image sensor;</claim-text>
<claim-text>during normal operation of the optical data reader, detecting a change in a condition that affects the fixed pattern noise in the image sensor;</claim-text>
<claim-text>based upon detecting the change in the condition, capturing a new dark image and replacing the dark image with the new dark image in the pattern of virtual scan lines to produce a new dark virtual scan line array; and</claim-text>
<claim-text>subtracting the new dark virtual scan line array from a subsequently generated illuminated virtual scan line array to generate a subsequent compensated image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. A method of compensating for fixed pattern noise in an image sensor in an optical data reader, wherein the fixed pattern noise comprises threshold mismatch dark signal non-uniformity and dark current dark signal non-uniformity, the method comprising:
<claim-text>capturing a first dark image using the image sensor, the first dark image comprising a negligible amount of scene information, the first dark image having a first exposure time;</claim-text>
<claim-text>capturing a second dark image, the second dark image having a second exposure time, and wherein the second exposure time is longer than the first exposure time;</claim-text>
<claim-text>capturing an illuminated image comprising optical data from a read area using the image sensor, the illuminated image having a third exposure time;</claim-text>
<claim-text>creating an estimate of threshold mismatch dark signal non-uniformity using the first dark image;</claim-text>
<claim-text>creating an estimate of dark current dark signal non-uniformity by subtracting the first dark image from the second dark image;</claim-text>
<claim-text>creating a scaled estimate of the dark current dark signal non-uniformity by multiplying the estimate of the dark current dark signal non-uniformity by a scaling factor based on the first exposure time, the second exposure time, and the third exposure time; and</claim-text>
<claim-text>subtracting the estimate of the threshold mismatch dark signal non-uniformity and the scaled estimate of the dark current dark signal non-uniformity from the illuminated image to generate a compensated image that is, at least partially, compensated for fixed pattern noise in the image sensor.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The method of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the third exposure time is between 300 and 700 times longer than the first exposure time.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The method of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the third exposure time is between 100 and 300 times longer than the second exposure time.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The method of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the scaling factor comprises a ratio of the third exposure time and the difference between the second exposure time and first exposure time. </claim-text>
</claim>
</claims>
</us-patent-grant>
