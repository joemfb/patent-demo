<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08622742-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08622742</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12619584</doc-number>
<date>20091116</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<us-term-of-grant>
<us-term-extension>820</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>09</class>
<subclass>B</subclass>
<main-group>19</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>3</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>434118</main-classification>
<further-classification>715709</further-classification>
</classification-national>
<invention-title id="d2e53">Teaching gestures with offset contact silhouettes</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>6307544</doc-number>
<kind>B1</kind>
<name>Harding</name>
<date>20011000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715709</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6802717</doc-number>
<kind>B2</kind>
<name>Castro</name>
<date>20041000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>434169</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6984208</doc-number>
<kind>B2</kind>
<name>Zheng</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>7249950</doc-number>
<kind>B2</kind>
<name>Freeman et al.</name>
<date>20070700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>434155</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>7340077</doc-number>
<kind>B2</kind>
<name>Gokturk et al.</name>
<date>20080300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>7565295</doc-number>
<kind>B1</kind>
<name>Hernandez-Rebollar</name>
<date>20090700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>7598942</doc-number>
<kind>B2</kind>
<name>Underkoffler et al.</name>
<date>20091000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>8147248</doc-number>
<kind>B2</kind>
<name>Rimas-Ribikauskas et al.</name>
<date>20120400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2006/0210958</doc-number>
<kind>A1</kind>
<name>Rimas-Ribikauskas et al.</name>
<date>20060900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>434362</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2007/0177804</doc-number>
<kind>A1</kind>
<name>Elias et al.</name>
<date>20070800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382188</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2007/0201863</doc-number>
<kind>A1</kind>
<name>Wilson et al.</name>
<date>20070800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2007/0262964</doc-number>
<kind>A1</kind>
<name>Zotov et al.</name>
<date>20071100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2008/0052643</doc-number>
<kind>A1</kind>
<name>Ike et al.</name>
<date>20080200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2008/0122803</doc-number>
<kind>A1</kind>
<name>Izadi et al.</name>
<date>20080500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2008/0163130</doc-number>
<kind>A1</kind>
<name>Westerman</name>
<date>20080700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2008/0178126</doc-number>
<kind>A1</kind>
<name>Beeck et al.</name>
<date>20080700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715863</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>2008/0191864</doc-number>
<kind>A1</kind>
<name>Wolfson</name>
<date>20080800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>340524</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>2008/0192005</doc-number>
<kind>A1</kind>
<name>Elgoyhen et al.</name>
<date>20080800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>2008/0244468</doc-number>
<kind>A1</kind>
<name>Nishihara et al.</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715863</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>2009/0051648</doc-number>
<kind>A1</kind>
<name>Shamaie et al.</name>
<date>20090200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>2009/0143141</doc-number>
<kind>A1</kind>
<name>Wells et al.</name>
<date>20090600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>2009/0153289</doc-number>
<kind>A1</kind>
<name>Hope et al.</name>
<date>20090600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>2009/0178011</doc-number>
<kind>A1</kind>
<name>Ording et al.</name>
<date>20090700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715863</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>2010/0104134</doc-number>
<kind>A1</kind>
<name>Wang et al.</name>
<date>20100400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>2011/0320496</doc-number>
<kind>A1</kind>
<name>Reid et al.</name>
<date>20111200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00026">
<othercit>Agarwal, et al. , &#x201c;High Precision Multi-Touch Sensing on Surfaces using Overhead Cameras &#x201d;, Retrieved at &#x3c;&#x3c;http://research.microsoft.com/en-us/um/people/shahrami/papers/cslate2.pdf&#x3e;&#x3e;, Second Annual IEEE International Workshop on Horizontal Interactive Human-Computer System, 2007, pp. 197-200.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00027">
<othercit>Hodges, et al. , &#x201c;ThinSight: Versatile Multi-Touch Sensing for Thin Form-Factor Displays&#x201d;, Retrieved at &#x3c;&#x3c;http://delivery.acm.org/10.1145/1300000/1294258/p259-hodges.pdf?key1=1294258&#x26;key2=4336984521&#x26;coll=GUIDE&#x26;dl=GUIDE&#x26;CFID=56496022&#x26;CFTOKEN=78691673&#x3e;&#x3e;, UIST'07, Oct. 7-10, 2007, pp. 259-268.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00028">
<othercit>Izadi, et al. , &#x201c;Going beyond the Display: A Surface Technology with an Electronically Switchable Diffuser&#x201d;, Retrieved at &#x3c;&#x3c;http://delivery.acm.org/10.1145/1450000/1449760/p269-izadi.pdf?key1=1449760&#x26;key2=8969094521&#x26;coll=GUIDE&#x26;dl=GUIDE&#x26;CFID=56524950&#x26;CFTOKEN=32131660&#x3e;&#x3e;, UIST'08, Oct. 19-22, 2008, pp. 269-278.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00029">
<othercit>Freeman, et al. , &#x201c;ShadowGuides: Visualizations for In-Situ Learning of Multi-Touch and Whole-Hand Gestures&#x201d;, Retrieved at &#x3c;&#x3c;http://research.microsoft.com/en-us/um/people/benko/publications/2009/ShadowGuides<sub>&#x2014;</sub>paper188.pdf&#x3e;&#x3e;, ITS '09, Oct. 7, 2009, pp. 8.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00030">
<othercit>Bau, et al. , &#x201c;A Dynamic Guide for Learning Gesture-Based Command Sets&#x201d;, Retrieved at &#x3c;&#x3c;http://insitu.lri.fr/&#x2dc;bau/files/uist2008op.pdf&#x3e;&#x3e;, UIST'08, Oct. 19-22, 2008, pp. 10.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00031">
<othercit>Grossman, et al. , &#x201c;Strategies for Accelerating On-Line Learning of Hotkeys&#x201d;, Retrieved at &#x3e;&#x3e;http://delivery.acm.org/10.1145/1250000/1240865/p1591-grossman.pdf?key1=1240865&#x26;key2=8401194521&#x26;coll=GUIDE&#x26;dl=GUIDE&#x26;CFID=55313597&#x26;CFTOKEN=92994945&#x3e;&#x3e;, CHI 2007, Apr. 28-May 3, 2007, pp. 1591-1600.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00032">
<othercit>Tapia, et al. , &#x201c;Some Design Refinements and Principles on the Appearance and Behavior of Marking Menus&#x201d;, Retrieved at &#x3c;&#x3c;http://delivery.acm.org/10.1145/1300000/1294258/p259-hodges.pdf?key1=1294258&#x26;key2=4336984521&#x26;coll=GUIDE&#x26;dl=GUIDE&#x26;CFID=56496022&#x26;CFTOKEN=78691673&#x3e;&#x3e;, UIST '95, Nov. 14-17, 1995, pp. 189-195.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00033">
<othercit>&#x201c;GestureBar: Improving the Approachability of Gesture-based Interfaces&#x201d;, Retrieved at &#x3c;&#x3c;http://www.cs.brown.edu/people/acb/papers/p2269-bragdon.pdf, CHI 2009, Apr. 4-9, 2009, pp. 2269-2278.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00034">
<othercit>Vogel, et al. , &#x201c;Shift: A Technique for Operating Pen-Based Interfaces Using Touch&#x201d;, Retrieved at &#x3c;&#x3c;http://www.patrickbaudisch.com/publications/2007-Vogel-CHI07-Shift.pdf&#x3e;&#x3e;, CHI 2007, Apr. 28-May 3, 2007, pp. 10.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00035">
<othercit>Carenzi, et al., &#x201c;Using Generic Neural Networks in the Control and Prediction of Grasp Postures&#x201d;, retrieved at &#x3c;&#x3c;http://www.dice.ucl.ac.be/Proceedings/esann/esannpdf/es2005-33.pdf, Proceedings&#x2014;European Symposium on Artificial Neural Networks, Apr. 27-29, 2005, pp. 61-66.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00036">
<othercit>Puranam, Muthukumar B., &#x201c;Towards Full-Body Gesture Analysis and Recognition&#x201d;, retrieved at &#x3c;&#x3c;http://archive.uky.edu/bitstream/10225/221/finalreport.pdf&#x3e;&#x3e;, Jan. 2005, 75 Pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00037">
<othercit>Malerczyk, Cornelius, &#x201c;Dynamic Gestural Interaction with Immersive Environments&#x201d;, retrieved at &#x3c;&#x3c;http://wscg.zcu.cz/WSCG2008/Papers<sub>&#x2014;</sub>2008/short/E02-full.pdf&#x3e;&#x3e;, Feb. 2008, 6 Pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>16</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>434118</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>434163</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>434173</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>434362</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>715709</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>715863</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>7</number-of-drawing-sheets>
<number-of-figures>8</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20110117535</doc-number>
<kind>A1</kind>
<date>20110519</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Benko</last-name>
<first-name>Hrvoje</first-name>
<address>
<city>Seattle</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Wigdor</last-name>
<first-name>Daniel J.</first-name>
<address>
<city>Seattle</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Freeman</last-name>
<first-name>Dustin</first-name>
<address>
<city>Toronto</city>
<country>CA</country>
</address>
</addressbook>
<residence>
<country>CA</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Benko</last-name>
<first-name>Hrvoje</first-name>
<address>
<city>Seattle</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Wigdor</last-name>
<first-name>Daniel J.</first-name>
<address>
<city>Seattle</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Freeman</last-name>
<first-name>Dustin</first-name>
<address>
<city>Toronto</city>
<country>CA</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Alleman Hall McCoy Russell &#x26; Tuttle LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Microsoft Corporation</orgname>
<role>02</role>
<address>
<city>Redmond</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Egloff</last-name>
<first-name>Peter</first-name>
<department>3715</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A method for providing multi-touch input training on a display surface is disclosed. A touch input is detected at one or more regions of the display surface. A visualization of the touch input is displayed at a location of the display surface offset from the touch input. One or more annotations are displayed at a location of the display surface offset from the touch input and proximate to the visualization, where each annotation shows a different legal continuation of the touch input.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="122.60mm" wi="176.53mm" file="US08622742-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="222.08mm" wi="175.51mm" file="US08622742-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="217.85mm" wi="136.91mm" file="US08622742-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="180.76mm" wi="134.54mm" file="US08622742-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="152.74mm" wi="187.03mm" file="US08622742-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="138.35mm" wi="192.62mm" file="US08622742-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="231.90mm" wi="194.06mm" file="US08622742-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="232.92mm" wi="122.60mm" orientation="landscape" file="US08622742-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND</heading>
<p id="p-0002" num="0001">Multi-touch input on display surfaces can be used in a variety of different applications. For example, computing systems with interactive display surfaces can be configured to utilize touch inputs as forms of user input to control system operation.</p>
<heading id="h-0002" level="1">SUMMARY</heading>
<p id="p-0003" num="0002">The present disclosure describes touch input training on a display surface. A touch input is detected at one or more regions of the display surface, and a visualization of the touch input is displayed at a location of the display surface offset from the touch input. One or more annotations are displayed at a location of the display surface offset from the touch input and proximate to the visualization, where each annotation shows a different legal continuation of the touch input.</p>
<p id="p-0004" num="0003">This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used to limit the scope of the claimed subject matter. Furthermore, the claimed subject matter is not limited to implementations that solve any or all disadvantages noted in any part of this disclosure.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0005" num="0004"><figref idref="DRAWINGS">FIG. 1</figref> shows an example of multi-touch user input on a display surface with a visualization of the input offset from the touch input.</p>
<p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. 2</figref> shows another example of multi-touch user input on a display surface with a visualization of the input offset from the touch input.</p>
<p id="p-0007" num="0006"><figref idref="DRAWINGS">FIGS. 3A and 3B</figref> show examples of annotations proximate to an offset visualization of a touch input.</p>
<p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. 4</figref> shows examples of annotations proximate to an offset visualization of a touch input.</p>
<p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. 5</figref> shows an example of a plurality of annotations proximate to an offset visualization of a touch input.</p>
<p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. 6</figref> shows an example method for providing touch input training on a display surface.</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 7</figref> schematically shows an example embodiment of a computing device including a touch input sensing display configured to detect multi-touch user input on a display surface.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0012" num="0011">Computing systems with display surfaces can be configured to detect contact-shape touch input on the display surfaces. Contact-shape touch input may include one or more regions of one or more objects in contact with the display surface. For example, contact-shape touch input may include one or more regions of one or more hands in contact with the display surface. Such computing systems can be controlled by one or more users at least in part by touch input on the display surface. For example, a user may touch the display surface with one or both hands and complete a hand gesture while maintaining contact with the surface to move or resize an object displayed on the surface. As another example, a user may tap one or more fingers on the display surface while performing a hand gesture in contact with the surface to carry out various computing system actions associated with the hand gesture. For example, a user may resize an object by sliding two fingers in contact with the surface together.</p>
<p id="p-0013" num="0012">In such computing systems, the mapping of contact-shape touch input to system actions may be complex or unfamiliar to inexperienced or infrequent users. For example, there may be many different multi-touch hand gestures for a user to learn in order to effectively interact with such a system. Multi-touch computing system input may be difficult for a user to learn and may prevent the user from effectively using such a system.</p>
<p id="p-0014" num="0013">By providing a contact-shape touch input training system that displays a guide at a location of the display surface offset from the touch input and within the context of a user's current action, the transition in user skill level from novice to expert use may be eased while providing system usability to users at all skill levels.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 1</figref> shows an example display surface <b>100</b> on which a contact-shape touch input <b>102</b> is executed. Display surface <b>100</b> can be configured to detect contact-shape touch input, e.g., touch input by one or more regions of one or more user's hands. The example of contact-shape touch input shown in <figref idref="DRAWINGS">FIG. 1</figref> includes regions of a user's fingers and thumbs in contact with the display surface <b>100</b>. An example visualization <b>104</b> of the contact-shape touch input <b>102</b> is shown as black regions at a location on the display surface offset from the touch input. The visualization <b>104</b> is provided as feedback of the regions of the user's hands in contact with the surface, e.g., silhouettes of the tips of the fingers and thumbs in contact with the surface offset from the contact-shape touch input <b>102</b>. The visualization is not blocked from view by the hands of the user making the contact because the visualization <b>104</b> is displayed at a location of the display surface offset from the contact.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 2</figref> shows another example of a contact-shape touch input <b>202</b> on a display surface <b>200</b>. In this example, the contact-shape touch input includes multiple regions of a user's hand, e.g., regions of the fingers and regions of the palm of the user's hand. A visualization <b>204</b> of the contact regions of the user's hands is displayed at a location of the display surface offset from the touch input.</p>
<p id="p-0017" num="0016">The offset <b>206</b> is shown in <figref idref="DRAWINGS">FIG. 2</figref> as a distance and direction from the contact-shape touch input <b>202</b> to the visualization <b>204</b>. The distance and/or direction of the offset may depend on a variety of operating conditions of the computing system and/or display surface. For example, the offset distance and/or direction may be fixed relative to the touch input. In another example, the offset distance and/or direction may change depending on a location of the touch input on the display surface or depending on locations of one or more objects displayed on the surface. For example, if the touch input is adjacent to an edge of the display surface, the visualization may be displayed at a location of the display surface offset from the touch input toward an opposing edge of the surface. In another example, the visualization may be displayed in a dedicated visualization region of the display surface so as to not interfere with other objects displayed on the surface.</p>
<p id="p-0018" num="0017">Some computing systems may be able to detect hover input. In other words, user input may be detected before the input actually touches the display surface. For example, vision based touch systems that use infrared or visible light cameras may be able to view a user hand that is a short distance above the display surface. It is to be understood that the visualizations and annotations described herein with reference to touch input also apply to hover input.</p>
<p id="p-0019" num="0018">By providing a visualization of the touch or hover input at a location on the display surface offset from the touch or hover input, occlusion of the visualization by the touch or hover input may be avoided. Furthermore, the offset visualization provides direct feedback to the user on how the contact-shape touch or hover input is observed by the system. For example, a user may be provided with feedback as to what regions of the hands are perceived by the system as being in contact with or hovering above the surface. For example, users may be made aware of fingers or parts of their hands unintentionally touching the surface.</p>
<p id="p-0020" num="0019">In addition to offset visualizations of a user's touch or hover input, the computing system may provide real-time assistance in the form of annotations proximate to the visualizations. In other words, visualizations of the user's current hand pose as interpreted by the system (feedback) are combined with annotations including hand pose completion paths for completing the gesture (feedforward).</p>
<p id="p-0021" num="0020">Annotations proximate to the visualizations provide information describing different legal continuations associated with a given touch or hover input. Legal continuations of a touch or hover input are possible continuations of a touch or hover input that are recognizable and/or interpretable by the computing system, e.g., as system actions. For example, annotations may include information indicating a direction and/or path a continuation of the touch or hover input should move to execute a legal continuation of the touch or hover input. As such, the annotations may include arrows or paths to guide the user. In another example, the annotations may include one or more images simulating a touch or hover input at a corresponding one or more successive time frames to indicate a legal continuation of the touch or hover input. For example, one or more snapshots of a hand posture may be displayed at different locations of the display surface proximate to the visualization in order to guide a user to perform a hand gesture. Such arrows, paths, images, or other visual elements may be spatially related to the offset visualization of the touch contact points.</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIGS. 3-5</figref>, described below, show various nonlimiting examples of annotated visualizations. These examples are provided to demonstrate concepts that may be applied to a variety of different annotations.</p>
<p id="p-0023" num="0022">In <figref idref="DRAWINGS">FIG. 3A</figref>, an example touch input continuation guided by annotations is shown at successive times t<b>1</b>, t<b>2</b>, t<b>3</b>, and t<b>4</b>. In some examples, each legal touch input continuation may be mapped to a specific system action. Thus, different touch input continuations may result in different resulting system actions depending on which touch input continuations are performed by the user.</p>
<p id="p-0024" num="0023">At t<b>1</b>, a user touch input <b>302</b> is performed on display surface <b>300</b> and an offset visualization <b>304</b> is displayed. User touch input <b>302</b> may be an initial touch of the surface, e.g., a user may initiate a touch input by touching the display surface, or touch input <b>302</b> may be a continuation of a previously started touch input. Offset visualization <b>304</b> may be displayed immediately upon initiation of touch input <b>302</b> or after touch input <b>302</b> fails to change by a predetermined rate, e.g., offset visualization <b>304</b> may be displayed in response to a pause or hesitation in movement of the touch input. Such pauses or hesitations in movement of a user's touch input may indicate that the user is uncertain of the possible legal continuations available from the current touch input. Thus, following a hesitation or pause in movement of the touch input, annotations may be displayed to guide the user. For example, the annotations may be displayed after the touch input fails to change at a predetermined rate.</p>
<p id="p-0025" num="0024">At t<b>2</b>, a nonlimiting example of an annotation <b>311</b> is displayed. In this example, annotation <b>311</b> provides the user with a visual clue indicating that sliding a finger to the right across the display surface is a legal continuation of the touch input <b>302</b>. In particular, annotation <b>311</b> includes an arrow <b>306</b> indicating a direction of the legal continuation of the touch input <b>302</b>, a circle <b>308</b> indicating a destination of the legal continuation of the touch input <b>302</b>, and a progress bar <b>312</b> indicating a level of completion of the continuation of the touch input <b>302</b>. As shown, annotation <b>311</b> is displayed with a spatial connection to offset visualization <b>304</b>. Because offset visualization <b>304</b> effectively mirrors touch input <b>302</b>, annotation <b>311</b> shows the user how to move touch input <b>302</b> to execute the legal continuation of the touch input.</p>
<p id="p-0026" num="0025">At t<b>3</b>, the progress bar <b>312</b> changes to visually indicate a relative level of completion of the legal continuation of the touch input as the user slides a finger across display surface <b>300</b> in accordance with annotation <b>311</b>. The destination marker, circle <b>308</b>, remains visible in order to continue guiding the user.</p>
<p id="p-0027" num="0026">At t<b>4</b>, the user has moved offset visualization <b>304</b> to the location previously occupied by the destination marker, circle <b>308</b>. The progress bar <b>312</b> is completely shaded, thus indicating that the legal continuation has been completed. In some examples, some or all of the annotations may be hidden upon completion of the associated touch input continuations. For example, arrow <b>306</b>, circle <b>308</b>, and/or progress bar <b>312</b> may be hidden upon completion of the legal continuation shown in FIG. <b>3</b>A. In the illustrated scenario, arrow <b>306</b> and circle <b>308</b> are hidden, while progress bar <b>312</b> remains visible.</p>
<p id="p-0028" num="0027">Upon completion of the legal touch input, the computing system may respond by performing the system action that is mapped to that touch input. In this way, the offset visualization <b>304</b> and annotation <b>311</b> may be used to teach a user how to use touch input to effectuate a particular system action. Different annotations may be presented in response to different touch inputs, so that different legal continuations and resulting system actions may be taught to the user.</p>
<p id="p-0029" num="0028">As an example, <figref idref="DRAWINGS">FIG. 3B</figref> shows another example in which annotations are used with offset visualizations of touch contact points to teach a user a legal continuation of a touch input. In contrast to the example shown in <figref idref="DRAWINGS">FIG. 3A</figref>, where the legal continuation of a one-finger touch gesture was a finger slide, the legal continuation of the one-finger touch gesture of <figref idref="DRAWINGS">FIG. 3B</figref> includes touching a second finger against the display surface. It is to be understood that the examples of <figref idref="DRAWINGS">FIG. 3A</figref> and <figref idref="DRAWINGS">FIG. 3B</figref> are nonlimiting.</p>
<p id="p-0030" num="0029">At t<b>1</b> of <figref idref="DRAWINGS">FIG. 3B</figref>, a user touch input <b>316</b> is displayed on a display surface <b>314</b> and an offset visualization <b>318</b> is displayed.</p>
<p id="p-0031" num="0030">At t<b>2</b>, an annotation <b>320</b> is displayed to guide the user to place a second finger on the display surface next to the first finger. Annotation <b>320</b> further includes textual information <b>322</b> describing how the legal continuation may be executed&#x2014;in this case by placing a second finger down on the display surface. Such textual information may be incorporated into virtually any annotation.</p>
<p id="p-0032" num="0031">At t<b>3</b>, a second finger is placed on the surface, and annotations <b>320</b> are hidden.</p>
<p id="p-0033" num="0032">The examples provided in <figref idref="DRAWINGS">FIG. 3A</figref> and <figref idref="DRAWINGS">FIG. 3B</figref> show how different annotations may be used to teach a user different touch gestures. While not shown, it is to be understood that the computing system may perform one or more system actions responsive to the successful completion of such a touch gesture. For example, touch input <b>302</b> of <figref idref="DRAWINGS">FIG. 3A</figref> may be used to move a virtual object to the right, and touch input <b>316</b> of <figref idref="DRAWINGS">FIG. 3B</figref> may be used to select a virtual object. System actions may be mapped to touch gestures in any suitable way.</p>
<p id="p-0034" num="0033">In addition to teaching a user how to complete a legal continuation of a touch input, annotations may optionally include visual information indicating the outcome of performing the legal continuation. For example, an annotation may include an image of a sliding virtual object or a textual description stating, &#x201c;use two fingers to select.&#x201d;</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 4</figref> shows how annotations may be used to teach a user how to perform touch gestures that include changing hand postures. At t<b>1</b>, a user touches a display surface <b>400</b> with a first hand posture <b>402</b>, and a contact silhouette <b>404</b> is displayed to provide the user with visual feedback as to which portions of the user hand are in contact with display surface <b>400</b>. The contact silhouette is offset from the touch input, thus avoiding occlusion by the hand performing the touch gesture.</p>
<p id="p-0036" num="0035">At t<b>2</b><i>a </i>and t<b>2</b><i>b, </i><figref idref="DRAWINGS">FIG. 4</figref> shows different examples of annotations that may be used to guide the user through different legal continuations of touch gestures including first hand posture <b>402</b>. In particular, the example annotations show the user how the hand posture is to be changed throughout the touch gesture. The example annotations include images simulating the touch gesture throughout successive time frames. In other words, the annotations indicate legal continuations of the touch gesture by showing the user how hand posture is to be changed.</p>
<p id="p-0037" num="0036">At t<b>2</b><i>a, </i>example annotation <b>411</b> includes image <b>406</b>, image <b>408</b>, and image <b>410</b>, each of which shows a different hand posture to be performed at successive time frames. In this example, not only is the hand posture to change, as indicated by the shapes of the images, but the overall position of the hand is to move, as indicated by the positions of the images. In particular, annotation <b>411</b> teaches that a legal continuation of the touch gesture is performed by sliding the hand to the right while opening the fingers of the hand. A progress bar <b>412</b> is displayed to indicate a level of completion of the gesture. Progress bar <b>412</b> may also provide directional information to indicate the direction a continuation of the touch input is to move. For example, progress bar <b>412</b> is oriented in the direction that the touch input should move and indicates a path to complete the legal continuation.</p>
<p id="p-0038" num="0037">At t<b>2</b><i>b, </i>example annotation <b>413</b> includes image <b>406</b>, image <b>408</b>, and image <b>410</b>, which partially overlap contact silhouette <b>404</b> to indicate that a legal continuation of the touch gesture does not include moving the hand across the touch display. Instead, annotation <b>413</b> teaches that a legal continuation of the touch gesture is performed by opening the fingers of the hand while the hand remains in place. In such cases, where two or more images at least partially overlap, one or more of the images may be at least partially transparent.</p>
<p id="p-0039" num="0038"><figref idref="DRAWINGS">FIGS. 3A</figref>, <b>3</b>B, and <b>4</b> each show an example in which an annotation corresponding to a single continuation is displayed exclusive of other annotations corresponding to other continuations. In some embodiments, the computing system may display the annotation corresponding to the continuation that is believed to be the most likely continuation desired by the user. The likelihood that a continuation will be desired may be determined by the context of the use environment, usage heuristics, or any other suitable technique. In some embodiments, different annotations may be displayed one after another. For example, annotation <b>311</b> from <figref idref="DRAWINGS">FIG. 3A</figref> may be displayed first, and if a user does not perform the annotated continuation, annotation <b>320</b> from <figref idref="DRAWINGS">FIG. 3B</figref> may replace annotation <b>311</b>.</p>
<p id="p-0040" num="0039">In some embodiments, two or more annotations corresponding to two or more different continuations may be displayed at the same time. <figref idref="DRAWINGS">FIG. 5</figref> shows an example where a plurality of different annotations (e.g., annotation <b>506</b>, annotation <b>508</b>, and annotation <b>510</b>) are displayed at the same time. Each of the annotations corresponds to a different legal continuation of the touch input <b>502</b>.</p>
<p id="p-0041" num="0040">The touch input <b>502</b> shown in the example in <figref idref="DRAWINGS">FIG. 5</figref> includes contact-shape touch input of multiple regions of a user's fingers, thumb and palm on the display surface. The visualization <b>504</b> is shown in black at a location of the surface offset from the touch input <b>502</b>.</p>
<p id="p-0042" num="0041">Each of the different annotations may be a different color in order to help the user distinguish between the different annotations.</p>
<p id="p-0043" num="0042">In the example of <figref idref="DRAWINGS">FIG. 5</figref>, annotation <b>506</b> guides the user to bring the contact regions of the fingers and thumb toward a center portion of the hand while maintaining contact with the surface. The annotation <b>508</b> guides the user to rotate the contact regions of the fingers and thumb in a counterclockwise direction while maintaining contact with the surface. The annotation <b>510</b> guides the user to slide the contact regions of the fingers and thumb in a common direction, e.g., towards a bottom portion of the surface, while maintaining contact with the surface.</p>
<p id="p-0044" num="0043">In the example of <figref idref="DRAWINGS">FIG. 5</figref> the annotations are displayed as paths to guide the user. The paths further include progress information as indicated by shaded regions of the paths. The shaded regions may provide directional information as well as indicating a level of completion of the particular gesture. In this example, the user may choose one or any combination of the gestures to complete a legal continuation of the touch input <b>502</b>. For example, the user may rotate the contact regions of the fingers and thumb in a counterclockwise direction only. In another example, the user may perform all three movements simultaneously, e.g., the user may rotate the contact regions of the fingers and thumb while closing the fingers and thumb and sliding the fingers and thumb toward a bottom portion of the display surface. In this example, all the annotations may remain visible to the user throughout a continuation of the touch input since each legal continuation may be performed in combination with one or more of the other legal continuations.</p>
<p id="p-0045" num="0044">It is to be understood that the examples provided above are not limiting. Furthermore, the individual aspects described in each example may be combined. For example, the textual information <b>322</b> from <figref idref="DRAWINGS">FIG. 3B</figref> may be incorporated with the temporally overlapping display of two or more annotations from <figref idref="DRAWINGS">FIG. 5</figref>.</p>
<p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. 6</figref> shows an example method <b>600</b> for providing touch input training on a display surface by displaying annotations proximate to an offset visualization of a touch input to guide a user to perform legal continuations of the touch input.</p>
<p id="p-0047" num="0046">At <b>602</b>, method <b>600</b> includes determining if a touch input is detected. For example, a user may initiate a touch input by touching the display surface. In another example, a touch input may be detected at any time during a continuation of a touch input following an initial touch of the surface.</p>
<p id="p-0048" num="0047">If the answer at <b>602</b> is no, flow moves to <b>603</b>, where it is determined if the method should continue. If the answer at <b>603</b> is yes, flow moves back to <b>602</b>. If the answer at <b>603</b> is no, the method ends.</p>
<p id="p-0049" num="0048">If the answer at <b>602</b> is yes, flow moves to <b>604</b>. At <b>604</b>, method <b>600</b> includes determining whether the touch input identified at step <b>602</b> fails to change by a predetermined rate. The predetermined rate may be stored in a data-holding subsystem of the computing system with the display device. In some embodiments, the predetermined rate may be modified by a user.</p>
<p id="p-0050" num="0049">If the answer at <b>604</b> is no, flow moves to <b>605</b>. If the answer at <b>604</b> is yes, flow moves to <b>606</b>, where touch input training is initiated. For example, touch input training may be initiated in response to a hesitation or pause in the touch input. In this way, an experienced user may complete a hand gesture without initiating touch input training. In another example, a novice or infrequent user may initiate a touch input and then pause to initiate the touch input training. In still another example, an intermediate user may be unfamiliar with a particular hand gesture and pause while performing the touch input to initiate the touch input training.</p>
<p id="p-0051" num="0050">At <b>605</b>, method <b>600</b> includes determining whether the changing touch input is a legal continuation of the touch input. If the answer at <b>605</b> is yes, flow moves to <b>603</b>. If the answer at <b>605</b> is no, flow moves to <b>606</b>, where touch input training is initiated. For example, a user may perform an illegal continuation of a touch input at which point the touch input training may be initiated.</p>
<p id="p-0052" num="0051">At <b>606</b>, upon initiation of touch input training, method <b>600</b> displays a visualization of the touch input at a location of the display surface offset from the touch input. As described above, the visualization may be a contact silhouette of one or more regions of one or more hands (or other objects) in contact with the display surface. The visualization of the touch input may include a contact silhouette showing which portions of an input object performing the touch input are in contact with the display surface. The visualization may be offset from the contact point of the contact-shape touch input so as to avoid occlusion by the touch input. In some examples, the offset visualization may include a schematic representation of regions of a touch input in contact with the display surface. In another example, the offset visualization may include a silhouette or shadow replicating the actual contact regions of a touch input as perceived by the system.</p>
<p id="p-0053" num="0052">At <b>608</b>, method <b>600</b> displays annotations proximate to the offset visualization, where each annotation shows a legal continuation of the touch input. The annotations may include information indicating a direction a continuation of the touch input should move to execute a legal continuation of the touch input, e.g., the information may include an arrow or path. The annotations may include a progress bar indicating a level of completion of a continuation of the touch input. The annotations may be color coded, e.g., a first annotation may be displayed with a first color and a second annotation displayed with a second color different from the first color, the first annotation indicating a first legal continuation of the touch input, and the second annotation indicating a second legal continuation of the touch input, different from the first legal continuation of the touch input. The annotations may include one or more images simulating a touch input at a corresponding one or more successive time frames to indicate a legal continuation of the touch input. The annotations may include textual information describing a legal continuation of the touch input and/or information describing the outcome of performing continuation.</p>
<p id="p-0054" num="0053">At <b>610</b>, method <b>600</b> determines if a continuation of touch input is complete. If the answer at <b>610</b> is no, flow may move back to <b>608</b>, where an offset visualization with annotations to guide the user to complete a legal continuation of the touch input may continue to be displayed on the display surface. However, the displayed annotations may change depending on a level of completion of a legal continuation of the touch input. For example, a first set of annotations may be displayed based on a first touch input and a second set of annotations different from the first set of annotations may be displayed based on a second subsequent touch input. The method <b>600</b> may include continually updating the displayed annotations during a performance of a continuation of the touch input.</p>
<p id="p-0055" num="0054">If the answer at <b>610</b> is yes, flow moves to <b>612</b>. At <b>612</b>, upon completion of the touch input, method <b>600</b> includes hiding the visualization and/or annotation. Method <b>600</b> may optionally include notifying the user to discontinue the touch input. For example, textual information, symbols, and/or icons may be displayed on the surface instructing the user to remove the touch input. Flow then moves to <b>603</b> where it is determined if the method is to be continued.</p>
<p id="p-0056" num="0055">The above described methods and processes may be tied to a computing device. <figref idref="DRAWINGS">FIG. 7</figref> shows a schematic depiction of an example computing device <b>700</b> including a touch input sensing display <b>702</b> configured to visually present images to a user and detect contact-shape touch input on the display surface <b>704</b>. The touch input sensing display <b>702</b> may be any suitable touch display, nonlimiting examples of which include touch-sensitive liquid crystal displays, touch-sensitive organic light emitting diode (OLED) displays, and rear projection displays with infrared, vision-based, touch detection cameras. The touch input sensing display <b>702</b> may be configured to detect user-input of various types. For example, contact-shape touch input by one or more users via one or more objects contacting display surface <b>704</b>. Examples include, hand contact input, stylus contact input, etc.</p>
<p id="p-0057" num="0056">The computing device <b>700</b> may further include a touch input trainer <b>706</b> operatively connected to touch input sensing display <b>702</b>. The touch input trainer may be configured to display a visualization of a touch input at a location of the display surface <b>704</b> offset from the touch input and display one or more annotations at a location of the display surface offset from the touch input and proximate to the visualization, as described above. In this way, the touch input trainer <b>706</b> may guide a user of computing device <b>700</b> to complete a legal continuation of the touch input.</p>
<p id="p-0058" num="0057">Computing device <b>700</b> includes a logic subsystem <b>708</b> and a data-holding subsystem <b>710</b>. Logic subsystem <b>708</b> may include one or more physical devices configured to execute one or more instructions. For example, the logic subsystem may be configured to execute one or more instructions that are part of one or more programs, routines, objects, components, data structures, or other logical constructs. Such instructions may be implemented to perform a task, implement a data type, transform the state of one or more devices, or otherwise arrive at a desired result. The logic subsystem may include one or more processors that are configured to execute software instructions. Additionally or alternatively, the logic subsystem may include one or more hardware or firmware logic machines configured to execute hardware or firmware instructions. The logic subsystem <b>708</b> may optionally include individual components that are distributed throughout two or more devices, which may be remotely located in some embodiments. Furthermore the logic subsystem <b>708</b> may be in operative communication with the touch input sensing display <b>702</b> and the touch input trainer <b>706</b>.</p>
<p id="p-0059" num="0058">Data-holding subsystem <b>710</b> may include one or more physical devices configured to hold data and/or instructions executable by the logic subsystem to implement the herein described methods and processes. When such methods and processes are implemented, the state of data-holding subsystem <b>710</b> may be transformed (e.g., to hold different data). Data-holding subsystem <b>710</b> may include removable media and/or built-in devices. Data-holding subsystem <b>710</b> may include optical memory devices, semiconductor memory devices, and/or magnetic memory devices, among others. Data-holding subsystem <b>710</b> may include devices with one or more of the following characteristics: volatile, nonvolatile, dynamic, static, read/write, read-only, random access, sequential access, location addressable, file addressable, and content addressable. In some embodiments, logic subsystem <b>708</b> and data-holding subsystem <b>710</b> may be integrated into one or more common devices, such as an application specific integrated circuit or a system on a chip.</p>
<p id="p-0060" num="0059">It is to be understood that the configurations and/or approaches described herein are exemplary in nature, and that these specific embodiments or examples are not to be considered in a limiting sense, because numerous variations are possible. The specific routines or methods described herein may represent one or more of any number of processing strategies. As such, various acts illustrated may be performed in the sequence illustrated, in other sequences, in parallel, or in some cases omitted. Likewise, the order of the above-described processes may be changed.</p>
<p id="p-0061" num="0060">The subject matter of the present disclosure includes all novel and nonobvious combinations and subcombinations of the various processes, systems and configurations, and other features, functions, acts, and/or properties disclosed herein, as well as any and all equivalents thereof.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A computer-implemented method for providing hand gesture training on a display surface, comprising:
<claim-text>detecting a hand gesture on the display surface;</claim-text>
<claim-text>displaying a contact silhouette showing, at a location of the display surface offset from the hand gesture, only regions of a hand performing the hand gesture that are in contact with the display surface while not showing other regions of the hand that are not in contact with the display surface;</claim-text>
<claim-text>responsive to displaying the contact silhouette, determining with a logic subsystem one or more possible legal hand gesture continuations based on the detected hand gesture; and</claim-text>
<claim-text>displaying one or more annotations corresponding to the one or more possible legal hand gesture continuations proximate to the contact silhouette, each annotation showing a different legal hand gesture continuation, where the contact silhouette and the one or more annotations proximate to the contact silhouette are displayed after the hand gesture fails to change at a predetermined rate as recognized by the logic subsystem.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising hiding the contact silhouette and the one or more annotations proximate to the contact silhouette upon completion of the hand gesture.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, where the one or more annotations includes a first annotation displayed with a first color and a second annotation displayed with a second color different from the first color, the first annotation indicating a first legal continuation of the hand gesture, and the second annotation indicating a second legal continuation of the hand gesture, different from the first legal continuation of the hand gesture.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, where the one or more annotations includes one or more images simulating a hand gesture at a corresponding one or more successive time frames to indicate a legal continuation of the hand gesture.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, where the one or more annotations includes a progress bar indicating a level of completion of a continuation of the hand gesture.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein displaying one or more annotations proximate to the contact silhouette comprises:
<claim-text>displaying a first annotation proximate to the contact silhouette showing a first legal hand gesture continuation; and</claim-text>
<claim-text>if a subsequent hand gesture corresponding to the first legal hand gesture continuation is not detected within a threshold amount of time, then removing the first annotation and displaying a second annotation proximate the contact silhouette showing a second legal hand gesture continuation, different than the first legal hand gesture continuation.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. A computer-implemented method for providing hand gesture training on a display surface, comprising:
<claim-text>detecting a hand gesture on the display surface performed by a hand of a user;</claim-text>
<claim-text>displaying a contact silhouette showing, at a location of the display surface offset from the hand gesture, only regions of the hand performing the hand gesture that are in contact with the display surface while not showing other regions of the hand that are not in contact with the display surface;</claim-text>
<claim-text>responsive to displaying the contact silhouette, determining with a logic subsystem one or more possible legal hand gesture continuations based on the detected hand gesture;</claim-text>
<claim-text>responsive to the hand gesture failing to change at a predetermined rate as recognized by the logic subsystem, displaying one or more annotations corresponding to the one or more possible legal hand gesture continuations proximate to the contact silhouette, each annotation showing a different legal hand gesture continuation; and</claim-text>
<claim-text>hiding the contact silhouette upon completion of a legal continuation of the hand gesture as recognized by the logic subsystem.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the one or more annotations includes information indicating a direction a continuation of the hand gesture should move to execute a legal continuation of the hand gesture.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the information indicating a direction a continuation of the hand gesture should move to execute a legal continuation of the hand gesture includes an arrow.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the information indicating a direction a continuation of the hand gesture should move to execute a legal continuation of the hand gesture includes a path.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the one or more annotations includes a first annotation displayed with a first color and a second annotation displayed with a second color different from the first color, the first annotation indicating a first legal continuation of the hand gesture, and the second annotation indicating a second legal continuation of the hand gesture, different from the first legal continuation of the hand gesture.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the one or more annotations includes one or more images simulating a hand gesture at a corresponding one or more successive time frames to indicate a legal continuation of the hand gesture.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the one or more annotations includes textual information describing a legal continuation of the hand gesture.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein one or more of the contact silhouette and the one or more annotations includes a progress bar indicating a level of completion of a continuation of the hand gesture.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising providing a notification to discontinue the hand gesture upon completion of a continuation of the hand gesture.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. A computing system, comprising:
<claim-text>a display surface configured to receive touch input;</claim-text>
<claim-text>a logic subsystem operatively connected to the display surface; and</claim-text>
<claim-text>a data-holding subsystem holding instructions executable by the logic subsystem to:
<claim-text>detect a hand gesture on the display surface;</claim-text>
<claim-text>display a contact silhouette showing, at a location of the display surface offset from the hand gesture, only regions of a hand performing the hand gesture that are in contact with the display surface while not showing other regions of the hand that are not in contact with the display surface; and</claim-text>
<claim-text>display a first annotation proximate to the contact silhouette showing a first legal hand gesture continuation; and</claim-text>
<claim-text>if a subsequent hand gesture corresponding to the first legal hand gesture continuation is not detected within a threshold amount of time, then removing the first annotation and displaying a second annotation proximate the contact silhouette showing a second legal hand gesture continuation, different than the first legal hand gesture continuation. </claim-text>
</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
