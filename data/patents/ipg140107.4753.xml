<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625846-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625846</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12933393</doc-number>
<date>20090318</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>GB</country>
<doc-number>0805053.6</doc-number>
<date>20080318</date>
</priority-claim>
<priority-claim sequence="02" kind="national">
<country>GB</country>
<doc-number>0805145.0</doc-number>
<date>20080319</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>372</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382103</main-classification>
<further-classification>348169</further-classification>
</classification-national>
<invention-title id="d2e90">Object and movement detection</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5645077</doc-number>
<kind>A</kind>
<name>Foxlin</name>
<date>19970700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>600587</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6745389</doc-number>
<kind>B1</kind>
<name>Hostyn et al.</name>
<date>20040600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>719315</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>7308286</doc-number>
<kind>B2</kind>
<name>Felter</name>
<date>20071200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>4555621</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>7536029</doc-number>
<kind>B2</kind>
<name>Choi et al.</name>
<date>20090500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>2006/0161871</doc-number>
<kind>A1</kind>
<name>Hotelling et al.</name>
<date>20060700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2006/0238490</doc-number>
<kind>A1</kind>
<name>Stanley et al.</name>
<date>20061000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345156</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2007/0121097</doc-number>
<kind>A1</kind>
<name>Boillot</name>
<date>20070500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2008/0281523</doc-number>
<kind>A1</kind>
<name>Dahl et al.</name>
<date>20081100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>702  5</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>WO</country>
<doc-number>WO 2004/102301</doc-number>
<kind>A2</kind>
<date>20041100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>WO</country>
<doc-number>WO 2004102301</doc-number>
<kind>A2</kind>
<date>20041100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>WO</country>
<doc-number>WO 2006/067436</doc-number>
<kind>A1</kind>
<date>20060600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00012">
<othercit>International Search Report and Written Opinion dated Aug. 3, 2009 from International Patent Application No. PCT/GB2009/000725, filed on Mar. 18, 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>35</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382100</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382103</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382106</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382107</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348169-172</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>5</number-of-drawing-sheets>
<number-of-figures>13</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20110096954</doc-number>
<kind>A1</kind>
<date>20110428</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Dahl</last-name>
<first-name>Tobias</first-name>
<address>
<city>Oslo</city>
<country>NO</country>
</address>
</addressbook>
<residence>
<country>NO</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Dahl</last-name>
<first-name>Tobias</first-name>
<address>
<city>Oslo</city>
<country>NO</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Knobbe Martens Olson &#x26; Bear LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Elliptic Laboratories AS</orgname>
<role>03</role>
<address>
<city>Olso</city>
<country>NO</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Bhatnagar</last-name>
<first-name>Anand</first-name>
<department>2668</department>
</primary-examiner>
</examiners>
<pct-or-regional-filing-data>
<document-id>
<country>WO</country>
<doc-number>PCT/GB2009/000725</doc-number>
<kind>00</kind>
<date>20090318</date>
</document-id>
<us-371c124-date>
<date>20101214</date>
</us-371c124-date>
</pct-or-regional-filing-data>
<pct-or-regional-publishing-data>
<document-id>
<country>WO</country>
<doc-number>WO2009/115799</doc-number>
<kind>A </kind>
<date>20090924</date>
</document-id>
</pct-or-regional-publishing-data>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Motions, positions or configurations off, for example a human hand can be recognized by transmitting a plurality of transmit signals in respective time frames; receiving a plurality of receive signals; determining a plurality of channel impulse responses using the transmit and receive signals; defining a matrix of impulse responses, with impulse responses for adjacent time frames adjacent each other; and analyzing the matrix for patterns (<b>22</b>) corresponding to the motion position or configuration.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="138.35mm" wi="155.28mm" file="US08625846-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="164.76mm" wi="169.59mm" file="US08625846-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="223.27mm" wi="173.23mm" file="US08625846-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="238.76mm" wi="172.13mm" file="US08625846-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="250.36mm" wi="161.21mm" file="US08625846-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="247.57mm" wi="173.40mm" file="US08625846-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application is a National Phase of PCT/GB2009/000725, filed Mar. 18, 2009, which was published in English and designated the U.S., and claims priority to GB 0805053.6, filed Mar. 18, 2008 and to GB 0805145.0, filed Mar. 19, 2008, each of which is incorporated herein by reference in its entirety.</p>
<p id="p-0003" num="0002">This invention relates to the detection, location and/or tracking of one or more objects particularly, although not exclusively, using ultrasound.</p>
<p id="p-0004" num="0003">It is known that it is possible to track one or more objects with an ultrasonic transmitter and an array of receivers using time-of-flight measurements combined with triangulation or other geometrical intersection techniques. As with most imaging technologies, the resolution increases with the number of sensors-used. In particular to give sufficient resolution to enable the separation of signals coming from one object from signals from another requires a large number of sensors. With a large enough array of sensors therefore it can be seen that it would be possible to track multiple objects, such as the fingers on a hand, for interaction with a personal computer. Indeed various proposals for such tracking have been made&#x2014;e.g. in US patent application US 2006/0161871 by Apple or US 2007/0121097 by Navisense. However both of these proposals have shortcomings. In particular using a large number of sensors to give accurate tracking of multiple objects gives rise to a high degree of system complexity and significant costs.</p>
<p id="p-0005" num="0004">The present invention aims to take a different approach.</p>
<p id="p-0006" num="0005">When viewed from a first aspect the invention provides a method of recognising motion of a human hand comprising:
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0006">transmitting a plurality of transmit signals in respective time frames;</li>
        <li id="ul0002-0002" num="0007">receiving a plurality of receive signals, at least some of said receive signals being reflected from said hand;</li>
        <li id="ul0002-0003" num="0008">determining a plurality of channel impulse responses using said transmit and receive signals;</li>
        <li id="ul0002-0004" num="0009">defining a matrix of impulse responses, with impulse responses for adjacent time frames adjacent each other; and</li>
        <li id="ul0002-0005" num="0010">analysing said matrix for patterns corresponding to movement of said hand.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0007" num="0011">The invention extends to apparatus for recognising motion of a human hand comprising:
<ul id="ul0003" list-style="none">
    <li id="ul0003-0001" num="0000">
    <ul id="ul0004" list-style="none">
        <li id="ul0004-0001" num="0012">transmission means arranged to transmit a plurality of transmit signals in respective time frames;</li>
        <li id="ul0004-0002" num="0013">receiving means arranged to receive a plurality of receive signals at least some of said receive signals being reflected from said hand; and</li>
        <li id="ul0004-0003" num="0014">processing means arranged to determine a plurality of channel impulse responses using said transmit and receive signals, define a matrix of impulse responses, with impulse responses for adjacent time frames adjacent each other, and analyse said matrix for patterns corresponding to movement of said hand.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0008" num="0015">The invention also extends to a computer software product, and to a carrier bearing the same, configured, when run on a computer, to recognise motion of a human hand, the software having inputs for a plurality of transmit signals in respective time frames and a plurality of receive signals and further comprising logic arranged to: determine a plurality of channel impulse responses using said transmit and receive signals; define a matrix of impulse responses, with impulse responses for adjacent time frames adjacent each other; and analyse said matrix for patterns corresponding to one or more objects.</p>
<p id="p-0009" num="0016">Thus it will be seen by those skilled in the art that in accordance with the invention a matrix is constructed in which movement of the hand is represented by patterns corresponding to the impulse responses arising from reflections from the hand in successive timeframes. This allows, in various embodiments of the invention, recognition of the movement of a human hand to allow that movement to be used in a control interface without any physical contact being required&#x2014;i.e. a touchless interface. The movement recognition could be for example: to track the motion of a finger; to look for a gesture executed by the whole hand; to look for a change in the shape, orientation or configuration of the hand; or indeed any combination of these things. The significant benefit which many of the aspects and features disclosed herein help to achieve is that the motion recognition mentioned above can be achieved without high imaging resolution; tolerance to the noise and inherent ambiguities which arise from much lower resolutions can be successfully accommodated.</p>
<p id="p-0010" num="0017">The inventive concept also encompasses recognising movement of an inanimate object controlled by the hand, or of objects other than a human hand, e.g. another human body part or part of an animal's body. It also encompasses recognising specific positions, shapes, and configurations rather than movements. Thus when viewed from a second aspect the invention provides a method of determining the state of one or more objects in an imaging field comprising:
<ul id="ul0005" list-style="none">
    <li id="ul0005-0001" num="0000">
    <ul id="ul0006" list-style="none">
        <li id="ul0006-0001" num="0018">transmitting a plurality of transmit signals in respective time frames;</li>
        <li id="ul0006-0002" num="0019">receiving a plurality of receive signals;</li>
        <li id="ul0006-0003" num="0020">determining a plurality of channel impulse responses using said transmit and receive signals;</li>
        <li id="ul0006-0004" num="0021">defining a matrix of impulse responses, with impulse responses for adjacent time frames adjacent each other; and</li>
        <li id="ul0006-0005" num="0022">analysing said matrix for patterns corresponding to one or more objects.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0011" num="0023">The invention thus extends to apparatus for determining the state of one or more objects in an imaging field comprising:
<ul id="ul0007" list-style="none">
    <li id="ul0007-0001" num="0000">
    <ul id="ul0008" list-style="none">
        <li id="ul0008-0001" num="0024">transmission means arranged to transmit a plurality of transmit signals in respective time frames;</li>
        <li id="ul0008-0002" num="0025">receiving means arranged to receive a plurality of receive signals; and</li>
        <li id="ul0008-0003" num="0026">processing means arranged to determine a plurality of channel impulse responses using said transmit and receive signals, define a matrix of impulse responses, with impulse responses for adjacent time frames adjacent each other, and analyse said matrix for patterns corresponding to one or more objects.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0012" num="0027">The invention similarly also extends to a computer software product, and to a carrier bearing the same, configured, when run on a computer, to determine the state of one or more objects in an imaging field, the software having inputs for a plurality of transmit signals in respective time frames and a plurality of receive signals and further comprising logic arranged to: determine a plurality of channel impulse responses using said transmit and receive signals; define a matrix of impulse responses, with impulse responses for adjacent time frames adjacent each other; and analyse said matrix for patterns corresponding to one or more objects.</p>
<p id="p-0013" num="0028">In preferred embodiments of the invention the method and software are used to control an electronic device which, to give some non-limiting examples could be a computer, PDA, cell phone, display equipment, audio-visual equipment or sound reproduction equipment. The invention thus extends to an electronic device controlled by the methods and software disclosed herein.</p>
<p id="p-0014" num="0029">The nature of the transmit signals can be selected as appropriate. In a simple embodiment they could comprise a single impulse or spike, i.e. approximating a Dirac delta function within the limitations of the available bandwidth. This has some advantages in terms of requiring little, if any, processing of the &#x2018;raw signal&#x2019; to calculate impulse responses (in the theoretical case of a pure impulse, no calculation is required) but gives a poor signal-to-noise ratio because of the deliberately short transmission. In, other embodiments the transmit signals could be composed of a series or train of pulses. This gives a better signal-to-noise ratio than a single pulse without greatly increasing the computation required. In other embodiments the transmit signals comprise one or more chirps&#x2014;i.e. a signal with rising or falling frequency. These give a good signal-to-noise ratio and are reasonable for calculating the impulse responses using a corresponding de-chirp function applied to the &#x2018;raw&#x2019; received signal.</p>
<p id="p-0015" num="0030">The matrix could be a logical construct comprising data recorded in a memory or other storage medium of a computing device; equally it can be seen as an image, with the values of the matrix corresponding to brightness levels in the image. In either representation, conveniently the impulse responses for respective time frames are aligned so that the responses for stationary objects are represented by horizontal lines. However this is not essential; for example the sampling scheme and so matrix need not be quadratic or rectangular; it could be a different shape such as hexagonal. In general the matrix is made up of samples of the impulse response at different times, where the impulse response is itself a function of time.</p>
<p id="p-0016" num="0031">The Applicant has realised that by organising the impulse responses such that they can be represented as an image (whether or not they in fact are so represented), powerful analysis of the image can be employed to deduce useful information about the object(s). When such analysis is discussed hereinbelow reference will be made to impulse response images; however it should be understood by those skilled in the art that such analysis can equally be carried out on data in a matrix, whether it is stored as such or whether it is a purely logical construct, and that nothing herein should be construed as inherently limiting the invention to requiring production of an actual image or any other representation.</p>
<p id="p-0017" num="0032">Where reference is made herein to impulse responses and impulse response images, these terms are to be understood to include simple linear transformations of the received impulse responses, such as inverted or linearly scaled transformations thereof. The terms impulse response and impulse response image in the following should therefore be construed to encompass all such equivalents.</p>
<p id="p-0018" num="0033">In some known systems object tracking is carried out by comparing consecutive pairs of reflected signals (e.g. by comparing their phase) This can be thought of as the &#x201c;raw signal&#x201d; domain. The Applicant has appreciated however that by carrying out tracking in the impulse response image domain in accordance with the invention, significant advantages can be realised. For example there is less of a requirement to separate a large number of small pulses or to find leading edges of waves than there is when operating in the &#x2018;raw signal&#x2019; domain. The preferred embodiments of the invention allow an overview of a &#x2018;scene&#x2019; to be taken which then allows better qualified estimates to be made of the identity of parts of the image with the particular objects and their movement, as opposed to simply carrying out a &#x2018;search&#x2019; operation at any given time.</p>
<p id="p-0019" num="0034">The patterns in the impulse response &#x2018;images&#x2019; corresponding to objects and their movement will typically be made up of the impulse responses from a number of consecutive time intervals. By contrast with simple maxima, minima, zero phase points or column-pair phase or time delays, the use of patterns in accordance with the invention allows more accurate and reliable analysis of trends within the image, which in turn allows reliable identification, location and/or tracking of objects. In some embodiments of the invention the impulse response images allow multi-frame motion estimation to be conducted&#x2014;that is motion estimation where motion is not simply computed from frame-to-frame and then possibly averaged over the frames, but rather wherein the motion is computed using several frames right from the start. This can be seen effectively as performing &#x2018;synthesis before extraction&#x2019; and is fundamentally different from previous techniques in which data was extracted &#x2018;on the fly&#x2019; and then filtered, averaged or otherwise smoothed.</p>
<p id="p-0020" num="0035">However this is not essential in some aspects of the invention. The impulse response images also allow motion estimation where possible movements are computed for multiple objects and/or multiple pixels within a single object, whether it is computed for pairs of impulse response or for multiples: multi-pixel/multi-candidate motion estimation. Such motion estimation may use impulse response samples from two or more consecutive times. In practice, our research has shown that different points on the surface of the object can have different motion patterns due to the size and orientation of the object. If, for instance, a hand is moving along a straight line with constant speed closely in front of a receiver/transmitter setup, a reflective point or partial object on one side of the hand does not have the same series of distances to the receiver/transceiver setup as a point or a partial object on the other side does.</p>
<p id="p-0021" num="0036">By contrast, some prior art techniques are based on computing the phase delay of a signal from one frame to the next. This assumes that there is a single, &#x201c;average&#x201d; phase delay, or phase delay &#x201c;of the centroid&#x201d;, which is representative of the object as a whole. However the Applicant has found that this is often not an accurate assumption, depending on the shape of the object being tracked. Ambiguities can result, such as unwanted artefacts on the tracking curves, which can lead to confusion about the motion of an object position and hence to incorrect position estimation, particularly when 2D or 3D positions are computed.</p>
<p id="p-0022" num="0037">Hence, at least some embodiments of the invention can provide motion information for multiple reflective points or parts of objects in the impulse response image, the motion of these points being related to the movement of particular locally &#x2018;bright&#x2019; or &#x2018;dark&#x2019; pixels from one column of the impulse response matrix to the next. This &#x201c;motion of the pixel&#x201d; in the impulse responses can also be likened to an optical flow field.</p>
<p id="p-0023" num="0038">The impulse response image can, in accordance with preferred embodiments, therefore use multiple frames to track a single (effectively point) object, or use two or more frames to track multiple objects or multiple parts of an object of finite size; or indeed any combination of these.</p>
<p id="p-0024" num="0039">This approach can be contrasted with some prior art tracking approaches which effectively just try to locate an object at one time point and then to track its movement by looking for the change in location by the next time point, e.g. using phase-delay techniques. The phase delay is computed for pairs of impulse responses, then this is averaged over time. However, since each such pair-wise comparison is sensitive to noise and errors (particularly if there are bursts of noise causing noise impulse responses), the variance of this process tends to grow over time (even though it might have zero mean). The result is a tendency for drift in the tracked times-of-flight on each channel to occur, which could lead to even more severe artefacts when an attempt is made to combine these positions to determine a two-dimensional or three-dimensional position in space. Although there are several ways in which such drift can be corrected for, the embodiments of the present invention are beneficial in avoiding such drift and therefore obviating the need to apply such corrections.</p>
<p id="p-0025" num="0040">Furthermore the prior art approaches cannot cope adequately with the tracked object passing too close to another object in the field. Moreover, they cannot cope adequately when two objects are at similar time-of-flight distances as each other from a transmitter-receiver pairing, even if the objects are not close to each other in space. The present invention, at least in its preferred embodiments employing multi-frame motion estimation, has an advantage over the error-prone state-of-the-art pair-wise comparison of echoes or impulse responses, by instead using several echoic impulse responses together.</p>
<p id="p-0026" num="0041">In preferred embodiments the patterns comprise intensity contours. Clearly the fine structure of such contours will be dependent on the nature of the transmit signal; and they might not be static as interference between overlapping lines might give rise to flicker. However the contours can, preferably, be approximated by lines. Such lines could be straight or curved and in some preferred embodiments the lines themselves are approximated by a plurality of line segments.</p>
<p id="p-0027" num="0042">In a simple application, embodiments of the invention could be used for presence detection&#x2014;e.g. by looking for sufficiently prominent lines in a predetermined region of the image to indicate an object. The nature of the lines&#x2014;e.g. their spread or pattern&#x2014;could be used for determining a property of the object&#x2014;e.g. its size, shape or reflectivity compared to surrounding objects. In general when there is an object present in the imaging field the impulse response image will contain features with both energy and a definite direction (e.g. horizontal for stationary objects and angled for moving objects); whereas noise tends to have energy but no discernible direction.</p>
<p id="p-0028" num="0043">If, as is preferred, stationary objects are represented as horizontal lines in the impulse response image, moving objects will be represented by non-horizontal lines. Movements that extend or shorten at a constant rate the time-of-flight from the transmitter to an object and back to the receiver will be represented as straight lines at an angle to the horizontal, whereas other motion will be represented by curves. The Applicant has realised as a consequence that analysis of these lines can be used to determine the motion of objects. Moreover it has further realised that by applying some simple rules derived from the physical principles of the objects' movement, the movement of multiple objects can be tracked, even with insufficient resolution to be able to distinguish objects at any given instant.</p>
<p id="p-0029" num="0044">The motion of the objects can be seen as a key principle in allowing them to be separated without what would normally be considered sufficient resolution to enable them to be separately imaged, indeed even if the objects are overlapping. This stems from the observation that the process of echo location is linear: the echoes from one object are superposed on top of those of another, making the total image a sum of &#x201c;transparent&#x201d; impulse response images stemming from the motions of the various reflectors in the scene. Thus the preferred embodiments of the invention use multi-frame transparent motion estimation. A simple way of visualizing this is to consider different text printed onto transparent sheets which are laid on top of each other. If the sheets are static relative to one another it is likely to be difficult or impossible to read any of the text. However if they have relative movement, they are much easier to read.</p>
<p id="p-0030" num="0045">The concept of using motion to separate closely-spaced or overlapping objects can be seen as enhancing poor resolution in the spatial domain by exploiting higher resolution in the time domain.</p>
<p id="p-0031" num="0046">When viewed from a further aspect the invention provides a method of tracking an object in the presence, of one or more other objects in an imaging field comprising transmitting energy into said imaging field, receiving reflected energy from the imaging field, calculating a series of impulse responses at successive time intervals and selecting a subset of said impulse responses corresponding to a continuous motion. The invention extends to apparatus configured to carry out such a method and to a computer software product configured to carry out the aforementioned calculating and selecting steps.</p>
<p id="p-0032" num="0047">The transmitted energy may be intermittent (regularly or irregularly), e.g. in the form of individual pulses, pulse trains, chirps, etc.; or it may be continuous.</p>
<p id="p-0033" num="0048">What this means is that, at least in its preferred embodiments, the invention can allow the movement of multiple objects to be tracked with fewer sensors than would otherwise have been the case. Techniques in accordance with the invention effectively allow detection of motion, even of overlapping objects, without full imaging being necessary. A pattern in the impulse response image corresponding to motion of the first object can be separately identified and isolated as a subset (e.g. a line) of the impulse response image. This is effectively deconstructing the composite image into separate layers corresponding to the separate motions. Of course where the patterns for the objects overlap, part of the image &#x2018;subset&#x2019; referred to above will also belong to a corresponding &#x2018;subset&#x2019; for the motion of the other object.</p>
<p id="p-0034" num="0049">To take an example of the application of physical principles to impulse response images, if say the objects being tracked are fingers on a hand, intelligent estimates can be made about movements by considering only continuous movements, below a certain speed and/or acceleration. This can be translated into properties of the lines in the impulse response image: lines which are continuous; which have a gradient below a threshold; and possibly which have a curvature below a threshold. This last criterion could be applied as a maximum change in gradient for best-fit linear sections. Of course, different gradients and different thresholds may apply for each of the different &#x2018;channels&#x2019; in a system having more than one transmitter-receiver pair.</p>
<p id="p-0035" num="0050">There are many ways in which the impulse response images can be analysed to determine motion of objects. In some circumstances a relatively crude analysis might be sufficient. For example the system might be configured to detect an aggregate change in the impulse response, or a predetermined portion thereof, above a threshold, without needing to determine the precise nature of the change or the movement that caused it. To take just one example of this, it might be determined that if an impulse response changes by more than say 10% over a given period, it is to be interpreted as a movement.</p>
<p id="p-0036" num="0051">If more discriminating analysis is required, there are many different approaches that can be used. For example any set of filters or linear or non-linear transforms can be used, including but not limited to: curvilinear filters, Hough-transforms, radon transforms, pi-tau transforms, and Fourier or fractional Fourier transforms, applied to the whole impulse response image or parts thereof. In another alternative, projection approaches can be employed by matching sub-blocks or sliding sub-blocks of the images to a set of basis image blocks.</p>
<p id="p-0037" num="0052">In another alternative an adaptive filter or any other suitable mathematical technique can be applied in order to &#x2018;deconstruct&#x2019; the two or more layers corresponding to the motions of two or more objects.</p>
<p id="p-0038" num="0053">In a further alternative the images can be represented using functional data analysis. In yet another alternative the filters can be realized in the Fourier transfer domain, fractional Fourier transform domain, or time domain, involving shifted and moving averages of the impulse response columns. In some embodiments, the impulse response image may be transformed by an up-, down- or re-sampling filter. In certain embodiments, a filter may be applied to the received signal (i.e. the &#x2018;raw&#x2019; signal domain) before conversion to the impulse response domain.</p>
<p id="p-0039" num="0054">Several such analyses may be combined for a single impulse response image such that, for example, one or more filters is applied to the impulse response image; the resulting filtered image is transformed by, say, a wavelet transform; one or more further filters is applied to the transformed image; the inverse transform is applied; and one or more further filters is applied in the impulse response image domain.</p>
<p id="p-0040" num="0055">The Applicant has appreciated, however, that performing certain analyses in the impulse response image (rather than in a transformed domain) is advantageous, for example in terms of computational simplicity, since stasis and motion of objects are more readily apparent from the impulse response image.</p>
<p id="p-0041" num="0056">One set of preferred embodiments of the method comprise applying one or more line filters to the image. A line filter is an algorithm which enhances parts of the image which lie on a predetermined line relative to the rest of the image. This can be achieved by enhancing those parts from their original intensity, reducing the intensity of other parts, or a combination of the two. A further advantage of line filters is that they are effective for reducing the effect of spurious, temporary bursts of channel noise such as would give rise to &#x2018;bad columns&#x2019; in the matrix and so which could confuse conventional time-of flight tracking systems.</p>
<p id="p-0042" num="0057">In a preferred set of embodiments straight line filters are employed. These have the advantage of giving relatively simpler computations but can nonetheless be used to approximate non-linear motion through sequential curve-fitting should this be necessary. A single filter could be used&#x2014;e.g. for a system looking for a particular movement only&#x2014;but preferably a plurality of different filters is applied. The results achieved with the respective filters can then be compared to determine one or more &#x2018;winners&#x2019;&#x2014;i.e. those that give the strongest result overall or within a given area. A threshold result strength could be applied e.g. to avoid false matches if no object motion of interest is present.</p>
<p id="p-0043" num="0058">Preferably the line filter masks are configured with a roll-off at the edges thereof; that is to say, rather than a sharp boundary, there is a decreasing differential enhancement away from the centre of the line, preferably both in a direction normal to the line (i.e. above and below the line if horizontal) and away from the ends of the line.</p>
<p id="p-0044" num="0059">A simple line filter can be seen as a linear operation on an image, involving a sum or an average. However, one can envision simple or complex non-linear modifications to a filter, such as enhancing its performance by having an adaptive outlier-filter that removes columns, rows or pixels having a high probability of noise prior to the summing or averaging, to avoid the smearing of noise pixels onto other pixels.</p>
<p id="p-0045" num="0060">Additionally or alternatively, edge filters are employed. In contrast to line filters, which match anywhere within the thickness of a linear band, an edge filter only matches one or both of the two boundaries of such a band. An edge filter will thus generally require the proximity of a high magnitude region and a low magnitude (i.e. close-to-zero) region (the magnitudes being the absolute values of the impulse responses). The edge filter may be horizontal, but is preferably at a predetermined angle. It will thus typically be useful in the early determination of motion of a predetermined type, since it will strongly match only the edge of a line within the impulse response image. This can correspond to detection of the front or back of an object and may be used effectively to locate, for example, a human finger.</p>
<p id="p-0046" num="0061">Additionally or alternatively the columns of the filter mask could be convolved with a sine signal representative of the bandwidth of the transmit signal. This allows information from neighbouring lines to be combined, thereby allowing a more visible line to be extracted.</p>
<p id="p-0047" num="0062">Additionally or alternatively a complex filter could be employed which effectively captures both phase, direction and amplitude of object motion.</p>
<p id="p-0048" num="0063">In some circumstances it might be appropriate for a filter mask to combine a plurality of lines in the impulse response image to give a fewer number of lines e.g. a single line. For example if an object of interest is moving near to a strong reflector, say a computer screen, the impulse response image would contain similar lines corresponding to signals reflected directly off the object; and those that had also been reflected via the screen respectively.</p>
<p id="p-0049" num="0064">The Applicant has further appreciated that it is not always necessary to compute two-dimensional filter masks, which in some applications might require heavy computational resources; alternatives for example would be to apply rolling averages, infinite impulse response filters, or filters based on sub-band coding. It will be appreciate that such filters can still act on information from two or more successive impulse responses.</p>
<p id="p-0050" num="0065">It is furthermore not essential to employ straight line filters. For example filters corresponding to specific curved lines, i.e. curvi-linear filters, could be applied instead or in addition. In another alternative, more complex masks could be used to look for specific movements, particularly in the case of multiple moving objects&#x2014;e.g. in gesture recognition. An example of this would be a sinusoidal filter to detect a reciprocating object movement&#x2014;e.g. waving of a hand.</p>
<p id="p-0051" num="0066">However the Applicant has appreciated that instead of using a relatively complex shaped mask, patterns in the impulse response image can be determined by recording the order in which simple filters are matched to the impulse responses. This allows specific types of movement, such as gestures, to be detected by their corresponding patterns in the impulse response image, regardless of the particular position, orientation or speed of the gesture. Thus a set of preferred embodiments comprises recording a sequence of filters matched to respective portions of the impulse response image, and comparing said sequence to one or more predetermined sequences to identify a characteristic pattern in the impulse responses, e.g. corresponding to a particular gesture. The predetermined sequence could comprise specific predetermined filters, but preferably comprises a sequence of sets of filters where any filter from the correct set can satisfy the sequence.</p>
<p id="p-0052" num="0067">In one example, if positive filter matches by a given series of filters comprising an upwardly sloping filter followed by a horizontal filter and then followed by a downwardly sloping filter, a regular motion such as oscillation may be deduced. This technique may be used to detect, for example, a gesture such as one or more human digits being wiggled rhythmically e.g. in a waving or beckoning gesture. Detecting such a specific gesture may enable a connected electronic system to perform a specific action in response to the gesture. This approach of identifying sequences of relatively simple filters amongst those matched would typically be computationally cheaper than applying one complex filter, such as a sinusoid line filter, particularly if this is done in addition to applying line filters which may be necessary for simply tracing the curve. It is also more versatile since, for example, a sinusoidal line filter will only match a narrow range of frequency of variation in the trace. This approach can be applied with line filters, edge filters, other filters capturing directional information about an object, or a combination of some or all of these.</p>
<p id="p-0053" num="0068">In general the shape of a filter mask can be varied in a number of ways. For example as well as, or instead of, enhancing a particular line, it may be used to suppress, or suppress more strongly, a particular region of the impulse response image&#x2014;e.g. that which corresponds to a known or previously-identified source of noise, interference or movement not of interest.</p>
<p id="p-0054" num="0069">In its simplest embodiments, the invention can be used to give useful information regarding the presence or movement of single or multiple bodies using only a single transmitter and a single sensor/receiver. Preferably however a plurality of transmitters and/or a plurality of receivers is provided.</p>
<p id="p-0055" num="0070">Some embodiments of the invention can use just a single transmitter and receiver pair. These could be physically separate transducers or could be provided by a single transducer. In at least some embodiments however a plurality of receivers is provided. Preferably an impulse response image is constructed for each sensor. One or more &#x2018;winning&#x2019; lines could be determined from each of these images and the lines combined in some way to provide an overall line in 2D or 3D space. Preferably however filters, e.g. line filters, which are adapted from a common velocity conjecture for each specific sensor channel are applied. In other words the system preferably guesses a velocity for an object consistent with the previous trajectory and physical rules and converts this velocity conjecture into a set of line (or other) filters for each sensor channel. Each sensor channel requires a different filter for a given trajectory since each will give a slightly different change between one column of the impulse response image and the next depending on the different positions of the sensor relative to the object(s).</p>
<p id="p-0056" num="0071">In at least some preferred embodiments multiple transmitters are used. In one set of preferred embodiments different delays are applied to the transmit signals for each sensor. With appropriately timed delays for each sensor depending on its location relative to the others for example, even omni-directional transmitters can together transmit a narrow &#x2018;beam&#x2019; of transmit signals so that only objects in the path of the beam are detected or tracked. This is advantageous in allowing less overall transmitter power to be used, which is significant in the context of mobile, battery-powered devices. The reason for this is that the beam-forming gives greater selectivity and so less overall power is required to achieve a given minimum signal-to-noise ratio.</p>
<p id="p-0057" num="0072">A similar concept can be applied to multiple sensors. Thus in some preferred embodiments comprising a plurality of sensors, an artificial delay is added to the impulse responses of at least some of the sensors so that the system is only sensitive to echoes emanating from within a corresponding beam defined by the locations of the sensors and the respective time delays. The direction of the beam can of course be changed adaptively by altering the pattern of delays applied. Its focusing properties can also be altered by applying suitable weightings. The direction of the beam is preferably determined based on a predicted motion of an object being tracked. As previously discussed such a prediction can be based on the previous trajectory and simple physical rules based on continuity of movement and maximum acceleration. As in the case of the multiple transmitters, such beam-forming effectively allows the suppression of signals that come from direction which are not of interest and so allow more accurate tracking of the object of interest.</p>
<p id="p-0058" num="0073">In some embodiments, e.g. those where a relatively lower resolution is sufficient, the transmit signals in accordance with the invention could comprise discrete impulses interspersed with periods of non-transmission. In other embodiments, e.g. where greater resolution is required, continuous transmission is used. This could be regularly repeating, either on the period of the timeslots, a multiple thereof, or on another period. It should be understood that where continuous transmission is employed the timeslots could be a purely abstract construct with no relationship to the physical signals being transmitted. In such cases the &#x201c;timeslots&#x201d; are simply labels relating the impulse responses to particular sampling intervals when the impulse response image is constructed.</p>
<p id="p-0059" num="0074">The impulse responses can be calculated using any suitable technique. For instance if discrete pulses are transmitted there may be sufficient time between them for echoes either not to interfere, or to interfere only to an extent that the signals can be separated by known techniques. Where the pulses are more closely spaced or continuous transmission is used, other methods can be used such as cross-correlation or the continuous inversion technique disclosed in WO 2006/067436.</p>
<p id="p-0060" num="0075">The use of impulse response images for use in multi-frame, or multi-point, motion estimation is considered novel and inventive in its own right and thus when viewed from a further aspect the invention provides a method of determining the movement of one or more objects in an imaging field comprising:
<ul id="ul0009" list-style="none">
    <li id="ul0009-0001" num="0000">
    <ul id="ul0010" list-style="none">
        <li id="ul0010-0001" num="0076">transmitting a continuous or discontinuous transmit signal; receiving a plurality of receive signals;</li>
        <li id="ul0010-0002" num="0077">determining a plurality of channel impulse responses using said transmit and receive signals;</li>
        <li id="ul0010-0003" num="0078">defining a matrix of impulse responses, with impulse responses for respective time intervals adjacent one another; and</li>
        <li id="ul0010-0004" num="0079">isolating from said matrix a pattern corresponding to a motion of one of said objects.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0061" num="0080">The invention extends to apparatus for determining the state of one or more objects in an imaging field comprising:
<ul id="ul0011" list-style="none">
    <li id="ul0011-0001" num="0000">
    <ul id="ul0012" list-style="none">
        <li id="ul0012-0001" num="0081">transmission means arranged to transmit a continuous or discontinuous transmit signal;</li>
        <li id="ul0012-0002" num="0082">receiving means arranged to receive a plurality of receive signals; and</li>
        <li id="ul0012-0003" num="0083">processing means arranged to determine a plurality of channel impulse responses using said transmit and receive signals, define a matrix of impulse responses, with impulse responses for adjacent time frames adjacent each other, and isolate from said matrix a pattern corresponding to a motion of one of said objects.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0062" num="0084">The invention also extends to a computer software product, and a carrier bearing the same, configured, when run on a computer, to determine the state of one or more objects in an imaging field, the software having inputs for a continuous or discontinuous transmit signal and a plurality of receive signals and further comprising logic arranged to: determine a plurality of channel impulse responses using said transmit and receive signals; define a matrix of impulse responses, with impulse responses for adjacent time frames adjacent each other; and isolate from said matrix a pattern corresponding to a motion of one of said objects.</p>
<p id="p-0063" num="0085">In accordance with all foregoing aspects of the invention the impulse response information can be analysed just in the time domain as already described. However additionally or alternatively in some preferred embodiments of the invention the impulse responses are analysed in the frequency domain. Preferably this is achieved by performing a Fourier transform, preferably a fast Fourier transform, on at least some of the impulse responses. The Applicant has realised that additional information can be obtained from such frequency domain analysis. For example a distinction can be made between rotational and translational movement of an object since rotational movement will give rise to a shift in the zero points of the amplitude spectrum (caused by destructive interference) whereas translational movement does not. Thus preferred embodiments comprise analysing the amplitude spectrum in the frequency domain.</p>
<p id="p-0064" num="0086">Furthermore it has been realised that for a single object moving translationally the shape of the frequency spectrum (the relative amounts of energy at each frequency) will remain constant, albeit that the absolute energy levels will depend on distance to the transmitter/receiver. However if there are multiple objects moving the shape of the frequency spectrum changes over time. This information could be used, for example, to determine how to analyse movement in the time domain more accurately or more efficiently. Preferably therefore changes in the frequency spectrum over time are determined.</p>
<p id="p-0065" num="0087">It has also been appreciated that if an object in the imaging field alters in size or shape during the time window of observation its frequency spectrum will change. For example a change in size might give rise to a shift in frequencies. This could be used for example in a simple algorithm to detect the unclenching of a hand from a closed to an open configuration without having to perform any detailed tracking of the positions of individual fingers.</p>
<p id="p-0066" num="0088">Similarly if an object changes its shape, its corresponding frequency representation in the corresponding part of the impulse response image will also change. An application of the use of frequency information using a Fourier transform could simply be to detect that the object is or has been changing its shape. If the object changes its shape, such as a thumb being stuck out or pulled in, this will change the frequency spectrum and phase of the corresponding part of the impulse response. This in turn, can be taken as a command for a computer to perform a specific action, for example to treat the action as a mouse click. This is the corollary in the frequency domain of the previously discussed analysis in the time domain whereby a threshold level of change in the impulse response is detected, or whereby a sequence of &#x2018;hits&#x2019; using appropriate filters is detected.</p>
<p id="p-0067" num="0089">As is the case generally with time domain analysis, specific types of motion can be identified and/or objects tracked without knowing the size, shape or absolute position of the object using frequency domain analysis. This can be seen as a &#x2018;probabilistic&#x2019; tracking approach in contrast with a point-by-point &#x2018;deterministic&#x2019; tracking approach.</p>
<p id="p-0068" num="0090">Frequency domain analysis is considered novel and inventive in its own right and thus when viewed from another aspect the invention provides a method of identifying a predetermined motion of an object comprising transmitting a transmit signal, receiving during a first time interval a first receive signal, determining a first frequency composition of said first receive signal, receiving during a second time interval a second receive signal, determining a second frequency composition of said second receive signal and determining whether a predetermined difference exists between said first and second frequency compositions.</p>
<p id="p-0069" num="0091">The invention extends to apparatus for identifying a predetermined motion of an object comprising:
<ul id="ul0013" list-style="none">
    <li id="ul0013-0001" num="0000">
    <ul id="ul0014" list-style="none">
        <li id="ul0014-0001" num="0092">transmission means arranged to transmit a transmit signal;</li>
        <li id="ul0014-0002" num="0093">receiving means arranged to receive during a first time interval a first receive signal and receive during a second time interval a second receive signal; and</li>
        <li id="ul0014-0003" num="0094">processing means arranged to: determine a first frequency composition of said first receive signal; determine a second frequency composition of said second receive signal; and determine whether a predetermined difference exists between said first and second frequency compositions.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0070" num="0095">The invention also extends to a computer software product, and a carrier bearing the same, configured, when run on a computer, to identify a predetermined motion of an object, the software having inputs for a first receive signal during a first time interval and a second receive signal during a second time interval; the software further comprising logic arranged to: determine a first frequency composition of said first receive signal; determine a second frequency composition of said second receive signal; and determine whether a predetermined difference exists between said first and second frequency compositions.</p>
<p id="p-0071" num="0096">The concept above could be extended to cover multiple signals, or multiple impulse responses, which are collected and analysed together. Just as in the time domain, the frequency domain representations of the impulse signals could be aligned, e.g. in a matrix, so that the analysis techniques described herein can be used; e.g. line filters could be used to extract the motion of minima or maxima in the spectrum pattern. Other, intermediate domains between time and space, such as fractional Fourier domains or wavelet domains may be used for analysis, which can provide additional information about time or frequency aspects of the objects motion. Thus, from a further aspect, the invention provides a method for identifying a predetermined motion of an object comprising:
<ul id="ul0015" list-style="none">
    <li id="ul0015-0001" num="0000">
    <ul id="ul0016" list-style="none">
        <li id="ul0016-0001" num="0097">transmitting a transmit signal;</li>
        <li id="ul0016-0002" num="0098">receiving during a first time interval a first receive signal and receiving during a second time interval a second receive signal;</li>
        <li id="ul0016-0003" num="0099">transforming said first receive signal into a predetermined domain;</li>
        <li id="ul0016-0004" num="0100">transforming said second receive signal into said domain; and</li>
        <li id="ul0016-0005" num="0101">determining whether a predetermined difference exists between said first and second signals within said domain.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0072" num="0102">The invention extends to apparatus configured to carry out such a method and to a computer software product, and a carrier bearing the same, arranged to carry out the transforming and determining steps.</p>
<p id="p-0073" num="0103">The domain may be the frequency domain as previously described, but may alternatively be a fractional Fourier domain, a wavelet domain, or any other suitable domain.</p>
<p id="p-0074" num="0104">In a set of preferred embodiments the methods described herein are used to track the motion of a human hand or part thereof. Preferably this is used to control an electronic device, including but not limited to a device with a visual display, such as a computing device, mobile device, cell phone, PDA, laptop computer, desktop computer, television, music system, video camera, or any device, static or portable performing any combination of these or other functions. One particular, non-limiting example is tracking the movement of a user's finger to control a cursor on a computing device or other device with a graphical user interface (GUI).</p>
<p id="p-0075" num="0105">In one set of particularly preferred embodiments a method in accordance with the invention comprises the step of identifying a hand gesture. There are many possibilities for these gestures&#x2014;a few examples of which will be given below. It will be apparent however to those skilled in the art that there are many possible alternatives which the principles and advantages that can be achieved in accordance with embodiments of the invention make convenient or possible.</p>
<p id="p-0076" num="0106">In one example the system is arranged to detect two fingers moving together or apart (i.e. converging or separating). Of course the mapping of particular gestures to the control of functions can be decided according to the particular application. However this gesture could conveniently be used for example to indicate that a screen object is to be zoomed out or in respectively. In a preferred example the aforementioned gestures are identified from different direction movements in respective halves or portions of an impulse response image. It will be appreciated that in accordance with this embodiment of the invention, such identification can be successfully carried out since the impulse response image allows motion identification with a relatively crude level of spatial resolution. Moreover, it is not necessary to determine the exact positions of any of the fingers of the hand. Instead, only the relative separating or converging need be determined using a &#x2018;probalistic tracking&#x2019; approach (in contrast to precise &#x2018;deterministic tracking&#x2019;). Some pre-filtering of the image could be used&#x2014;e.g. by identifying areas of high reflection energy likely to correspond to finger tip movement.</p>
<p id="p-0077" num="0107">In a set of embodiments, a change of shape of an object is detected in the time domain. In some embodiments this is achieved by monitoring for a predetermined deformation in the impulse responses. A deformation occurs when individual parts of an impulse response in a given timeframe deviate in an adjacent timeframe. This means that the impulse response from one timeframe cannot be matched to that from the next by a simple shift in the time direction. When viewed in an impulse response image, this appears as individual lines diverging or converging. By contrast lines representing a moving object which is not changing in shape maintain a constant separation between them. This technique is powerful as it does not rely on a deterministic approach in which individual parts of an object are tracked and then relative movement calculated; rather a generalised change of shape (including expansion or contraction) can be detected, regardless of position, orientation or even starting-shape. This makes it useful for detecting gestures without requiring the computational resource necessary for deterministic finger tracking.</p>
<p id="p-0078" num="0108">Another way in which the ability to detect the configuration of a hand can be exploited, is to use it to determine when movement should be tracked and when it should not. There is a general problem with touchless control systems in ensuring that the system responds only to deliberate commands and not to inadvertent movements near its zone of sensitivity. Several ways round this have been proposed. One is to use a physical structure or indication to a user as to where the boundaries of the sensitive zone lie, and to ignore movements detected as being outside the zone. Another approach is to define such a zone &#x2018;virtually&#x2019;, i.e. without physical structure delineating it, but to provide some sort of feedback when movement within the zone is detected. In accordance with some envisaged embodiments of the present invention however, the configuration of a hand can be used to determine whether a movement or gesture is to be interpreted by the system. To take one non-limiting example, the system could be configured to distinguish between an open hand and a closed hand with a single projecting finger, with only gestures or movement made with the hand in the latter configuration being tracked or interpreted. Such a concept significantly reduces the chance of inadvertent movement interpretation in a highly user-friendly manner.</p>
<p id="p-0079" num="0109">When viewed from a further aspect the invention provides a touchless control system configured to be responsive to movements of a hand in a first configuration; but to ignore movements of the hand when the hand is in a second configuration.</p>
<p id="p-0080" num="0110">In another example rotation of the hand is detected. This could be used for example in the control of a computer game. The impulse response image is likely to contain more overlapping lines in this instance, but in accordance with preferred embodiments such a movement can nonetheless be distinguished&#x2014;e.g. with appropriately designed filters and/or using frequency domain analysis.</p>
<p id="p-0081" num="0111">In another example movement of thumb is detected. This could be used for example to emulate the click of a computer mouse button. In the impulse response image such a movement would appear as a differential gradient amongst some lines (corresponding to the thumb) compared to the rest of a group (corresponding to the rest of the hand). This is an example of the feature described above whereby a predetermined deformation in the impulse responses is detected to determine a change in shape of an object. It demonstrates that methods in accordance with the invention can be used to track or detect movements in objects which have significant dimensions&#x2014;in other words which cannot be approximated as point objects.</p>
<p id="p-0082" num="0112">More generally the Applicant has appreciated that using motion of the thumb to prompt an action, such as that equivalent to a mouse click, in a computer interface is advantageous over previous proposals to use tapping of a pointing finger. The movement of the pointing finger during tapping can give rise to an unintended movement in the detected lateral location of the finger so causing a corresponding unintended movement of the cursor which might mean that the intended action is not carried out. This could be highly frustrating for a user.</p>
<p id="p-0083" num="0113">When viewed from a further aspect the present invention provides apparatus for controlling a device comprising means for detecting movement of a first digit for controlling a first action of the device and means for detecting movement of a second digit relative to the first digit for controlling a second action of the device.</p>
<p id="p-0084" num="0114">Preferably said first action comprises moving a selector between a plurality of alternative positions&#x2014;this could for example be a cursor or a menu. Preferably said second action comprises an action corresponding to the position of said selector.</p>
<p id="p-0085" num="0115">Preferably the first digit is an index finger. Preferably the second digit is a thumb. It has been found that an index finger can be used naturally for moving and pointing whilst a thumb is easy and natural to move independently of the fingers in a &#x2018;click&#x2019; action. This is particularly natural when the hand is resting on a surface. In some preferred embodiments said second action is only carried out if it is determined that the first digit is static.</p>
<p id="p-0086" num="0116">It can be seen that the means for detecting movement of a second digit relative to the first digit can use a probabilistic approach to detect the relative motion. By this is meant that it preferably does not detect relative motion by independently determining the absolute positions of the first and second digit and comparing these.</p>
<p id="p-0087" num="0117">In general the foregoing demonstrates that in accordance with preferred embodiments of the invention certain types of movement such as: translation, or rotation; or relative movement between different parts of an object can be detected without having to know or measure the precise shape or size of the object. This has clear computational advantages but also means that a wide range of objects can be recognised with minimal, or no &#x2018;learning&#x2019;.</p>
<p id="p-0088" num="0118">As explained above, the technique of constructing impulse response images and analysing lines and shapes in these gives rise in the preferred embodiments of the invention to significant advantages. However one of the key principles used to carry out such analysis&#x2014;using motion conforming to physical rules to separate overlapping or unresolved objects&#x2014;can also be applied in other circumstances. For example in an imaging system &#x2018;unresolved&#x2019; or overlapping objects can be identified or tracked. In such systems rather than impulse response images necessarily being constructed, the impulse responses across a number of channels (e.g. from a number of sensors) can be combined to produce, at each time interval, a two- or three-dimensional image of the objects' locations, or a two- or three-dimensional map of the reflectivity of all the reflective points in the scene. There are several beam-forming or imaging techniques which can be used in this context, but in some preferred embodiments delay-and-sum imaging is used.</p>
<p id="p-0089" num="0119">The changes in such images can be analysed over time using the motion separation techniques described earlier. In other words the techniques can be applied to maps of the physical location of the objects as they can to impulse response images.</p>
<p id="p-0090" num="0120">Thus when viewed from a further aspect the invention provides a method of identifying or tracking an object comprising continually or continuously transmitting transmit signals, receiving signals reflected from said object at a sensor at a plurality of time intervals, constructing an image based on the receive signals from said plurality of time intervals and analysing said image for one or more patterns corresponding a predetermined movement of said object.</p>
<p id="p-0091" num="0121">The invention extends to apparatus for identifying or tracking an object comprising means for continually or continuously transmitting transmit signals, means for receiving signals reflected from said object at a sensor at a plurality of time intervals, and processing means configured to construct an image based on the receive signals from said plurality of time intervals and analyse said image for one or more patterns corresponding a predetermined movement of said object.</p>
<p id="p-0092" num="0122">The invention also extends to a computer software product, and a carrier bearing the same, configured, when run on a computer, to identify or track an object, the software having an input for signals reflected from said object at a sensor at a plurality of time intervals and logic configured to construct an image based on the receive signals from said plurality of time intervals and analyse said image for one or more patterns corresponding a predetermined movement of said object.</p>
<p id="p-0093" num="0123">As in the earlier aspects of the invention, the images can be compared using a line filter&#x2014;i.e. selecting only those parts of the images which are consistent with the predetermined movement. Of course the analysis will typically use the results from more than two time intervals and this will of course enhance the accuracy with which an object can be tracked. Preferably a rolling window of time intervals is used.</p>
<p id="p-0094" num="0124">One advantage of the physical mappings of reflectors is that the mapping process automatically deals with &#x2018;unphysical&#x2019; or rogue data&#x2014;e.g. from one channel&#x2014;that is not consistent with that from the other channels (since they are receiving reflections from the same physical object). This effectively gives an element of pre-filtering of the data.</p>
<p id="p-0095" num="0125">Also as in the earlier aspect of the invention the physical mappings of reflectors need not be visually represented but could comprise data from which a visual representation could be derived.</p>
<p id="p-0096" num="0126">Any of the other features described herein as preferred or possible features of the earlier aspects of the invention can equally be applied to the physical images described above.</p>
<p id="p-0097" num="0127">In all of the methods herein, the results are preferably stored in a volatile or non-volatile memory. Additionally or alternatively they are displayed on a display device. They thus preferably comprise the step of providing a display signal for a display device. Additionally or alternatively the methods are used to control an external device. They thus preferably comprise the step of providing a control signal for an external device.</p>
<p id="p-0098" num="0128">The methods of the invention are preferably carried out using computing means, computing machines, data processing apparatus or any other device capable of carrying out stored instructions. Such a device may be static; although the invention can equally be used with mobile devices. Indeed the advantages achievable in accordance with at least some embodiments of the invention of intolerance to absolute orientation or position and the ability to separate different motions make it particularly suitable for use in mobile devices. When viewed from another aspect therefore the invention provides a mobile device comprising an ultrasonic transmitter and separate/integrated ultrasonic receiver, said device being configured so that at least one operation thereof is controlled by determination of the movement of a user's hand in accordance with any of the method set out hereinabove.</p>
<p id="p-0099" num="0129">Whilst reference is made herein to arrangements in which signals are transmitted, reflected by an object and then received, the principles apply equally to &#x2018;active objects&#x2019; i.e. where the object being detected or tracked itself includes a transmitter so that the signal originates from the object rather than being reflected by it.</p>
<p id="p-0100" num="0130">In all aspects of the invention the signals are preferably ultrasonic signals. This means acoustic signals having a frequency higher than the normal human hearing range; typically this means the signals have a frequency, or base or median frequency greater than 20 kHz, e.g. between 30 and 50 kHz.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<p id="p-0101" num="0131">Certain preferred embodiments of the inventions will now be described, by way of example only, with reference to the accompanying drawings in which:</p>
<p id="p-0102" num="0132"><figref idref="DRAWINGS">FIG. 1</figref> is a schematic diagram showing the principal parts of a an ultrasonic multi-object tracking system;</p>
<p id="p-0103" num="0133"><figref idref="DRAWINGS">FIG. 2</figref> is a schematic representation of an impulse response image representing a single static object;</p>
<p id="p-0104" num="0134"><figref idref="DRAWINGS">FIG. 3</figref> shows how the impulse response image changes when the object is moving during the sampling window;</p>
<p id="p-0105" num="0135"><figref idref="DRAWINGS">FIG. 4</figref><i>a </i>is a schematic illustration of how a particular movement can be identified across a number of frames;</p>
<p id="p-0106" num="0136"><figref idref="DRAWINGS">FIG. 4</figref><i>b </i>shows the energy distribution for a certain movement;</p>
<p id="p-0107" num="0137"><figref idref="DRAWINGS">FIG. 4</figref><i>c </i>shows the energy distribution for a variation of the movement;</p>
<p id="p-0108" num="0138"><figref idref="DRAWINGS">FIG. 5</figref> is a schematic representation of the impulse response image corresponding to two objects moving across one another</p>
<p id="p-0109" num="0139"><figref idref="DRAWINGS">FIG. 6</figref><i>a </i>shows the impulse response of <figref idref="DRAWINGS">FIG. 5</figref> after a line filter has been applied;</p>
<p id="p-0110" num="0140"><figref idref="DRAWINGS">FIG. 6</figref><i>b </i>shows the impulse response of <figref idref="DRAWINGS">FIG. 5</figref> after an edge filter has been applied;</p>
<p id="p-0111" num="0141"><figref idref="DRAWINGS">FIG. 7</figref> is a schematic representation of the impulse response image corresponding to a &#x2018;thumb click&#x2019;;</p>
<p id="p-0112" num="0142"><figref idref="DRAWINGS">FIG. 8</figref><i>a </i>is a schematic representation of the impulse response image corresponding to a &#x2018;zoom action&#x2019;;</p>
<p id="p-0113" num="0143"><figref idref="DRAWINGS">FIG. 8</figref><i>b </i>shows the impulse response image of <figref idref="DRAWINGS">FIG. 8</figref><i>a </i>after applying line filters; and</p>
<p id="p-0114" num="0144"><figref idref="DRAWINGS">FIG. 8</figref><i>c </i>shows the impulse response image of <figref idref="DRAWINGS">FIG. 8</figref><i>b </i>after applying further line filters.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<p id="p-0115" num="0145">Turning first to <figref idref="DRAWINGS">FIG. 1</figref> an exemplary implementation of the principles of the invention to the touchless control of a graphical user interface of a computer, will be described. A signal generator <b>2</b> generates signals at ultrasonic frequencies which are converted to ultrasonic waves by an ultrasonic transmitter <b>4</b>. These waves bounce off an object <b>6</b> to be tracked, such as a hand, as well as bouncing off any other obstacles in the vicinity. The reflected energy is received by one or more ultrasound receivers <b>8</b> which convert the energy back into analogue electrical signals which are passed to a processor <b>10</b>. As will be explained in greater detail below, the processor <b>10</b> computes impulse responses, carries out filtering, combines impulse responses images to become 2D or 3D images etc, so as ultimately to determine the motion of the object <b>6</b>. The information about the presence and position of the object <b>6</b> is passed to a display <b>12</b> for controlling the movement of a cursor <b>14</b>. The display could be a separate system or indeed part of the computer on which the processor <b>10</b> is provided. The cursor <b>14</b> reproduces the motion of the object on the screen.</p>
<p id="p-0116" num="0146">The processor <b>10</b> is coupled to the signal generator <b>2</b> in order for the processor to control the precise transmission times, signal codes or other patterns, should that be necessary.</p>
<p id="p-0117" num="0147">The analogue signals output by the ultrasonic receiver <b>8</b> are used to calculate &#x2018;impulse responses&#x2019; for the &#x2018;channel&#x2019; comprising: the ultrasonic transmitter <b>4</b>, the imaging field containing the object of interest <b>6</b> and the ultrasonic receiver.</p>
<p id="p-0118" num="0148">One way of estimating the channel impulse response is to drive a short impulse into the signal and listen to received echoes and direct path energy being received. However, as it might be difficult to transmit such a signal without giving the transmitter a &#x2018;shock&#x2019;, containing frequency components outside the band in which the impulse response estimate is sought. Other techniques might be more appropriate, such as cross-correlation or pulse compression techniques.</p>
<p id="p-0119" num="0149">It may first be assumed that the transmission of a signal over a channel follows the following model;
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>y</i>(<i>t</i>)=<i>h</i>(<i>t</i>)*<i>x</i>(<i>t</i>)+<i>n</i>(<i>t</i>)&#x2003;&#x2003;Eq(1)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where:
<br/>
x(t) is the signal transmitted
<br/>
y(t) is the received signal
<br/>
h(t) is the channel's impulse response
<br/>
n(t) is an environmental noise term
<br/>
*denotes a convolution operator
</p>
<p id="p-0120" num="0150">The transmitted signal is represented as a time series&#x2014;that is to say a series of discrete signal values at regular time intervals. The received signal is also represented as a time series since it will be a sampled signal. The impulse response h(t) is what is being sought to be measured. It is assumed that the channel h(t) is constant or changing very slowly relative to the changes in x(t) and y(t), at least within any actual time-window used. That is not to say that time-varying channels cannot be measured, indeed in at least its preferred embodiments the invention is concerned with how a channel varies over time; it is just that the channel variation should be slow compared with the signal variation.</p>
<p id="p-0121" num="0151">The channel can be represented as a Q-tap finite impulse response (FIR) filter. As is well known in the art this is a construction in which the channel h(t) is seen as a series of weights to be applied to the previous Q time samples of the input signal. Assuming a signal x(t) has been transmitted through a loudspeaker, it is received through a microphone again as y(t). The received signal y(t) is taken to relate to the transmitted one x(t) as follows:</p>
<p id="p-0122" num="0152">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>y</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mi>l</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mrow>
            <mrow>
              <mo>[</mo>
              <mrow>
                <mrow>
                  <mi>x</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mi>t</mi>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>*</mo>
                <mrow>
                  <mi>h</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mi>t</mi>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
              <mo>]</mo>
            </mrow>
            <mo>&#x2062;</mo>
            <mrow>
              <mo>(</mo>
              <mi>l</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>=</mo>
          <mrow>
            <munderover>
              <mo>&#x2211;</mo>
              <mrow>
                <mi>k</mi>
                <mo>=</mo>
                <mn>0</mn>
              </mrow>
              <mrow>
                <mi>K</mi>
                <mo>-</mo>
                <mn>1</mn>
              </mrow>
            </munderover>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mrow>
              <mrow>
                <mi>x</mi>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>l</mi>
                    <mo>-</mo>
                    <mi>k</mi>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mo>&#x2062;</mo>
              <mrow>
                <mi>h</mi>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>(</mo>
                  <mi>k</mi>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mi>Eq</mi>
        <mo>&#x2062;</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mrow>
          <mo>(</mo>
          <mi>A</mi>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0123" num="0153">That is, a sample of y(t) is a linear combination of the K last samples of x(t) where the linear weights are given in the &#x201c;filter coefficients&#x201d; h(0), . . . h(K&#x2212;1). To estimate the channel, it is necessary to estimate these filter coefficients. In this technique the assumption is made that the signal x(t) is, for all t from &#x2212;infinity to plus infinity, &#x201c;white&#x201d;. In other words it is assumed that the signal is uncorrelated with itself for all non-zero shifts. Expressing this in an equation:</p>
<p id="p-0124" num="0154">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mrow>
            <mo>[</mo>
            <mrow>
              <mrow>
                <mi>x</mi>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>(</mo>
                  <mi>t</mi>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mo>*</mo>
              <mrow>
                <mi>x</mi>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mo>-</mo>
                    <mi>t</mi>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mrow>
            <mo>]</mo>
          </mrow>
          <mo>&#x2062;</mo>
          <mrow>
            <mo>(</mo>
            <mi>l</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mrow>
            <munderover>
              <mo>&#x2211;</mo>
              <mrow>
                <mi>k</mi>
                <mo>=</mo>
                <mrow>
                  <mo>-</mo>
                  <mi>&#x221e;</mi>
                </mrow>
              </mrow>
              <mi>&#x221e;</mi>
            </munderover>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mrow>
              <mrow>
                <mi>x</mi>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>l</mi>
                    <mo>-</mo>
                    <mi>k</mi>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mo>&#x2062;</mo>
              <mrow>
                <mi>x</mi>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mo>-</mo>
                    <mi>k</mi>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
          <mo>=</mo>
          <mrow>
            <mrow>
              <munderover>
                <mo>&#x2211;</mo>
                <mrow>
                  <mi>k</mi>
                  <mo>=</mo>
                  <mrow>
                    <mo>-</mo>
                    <mi>&#x221e;</mi>
                  </mrow>
                </mrow>
                <mi>&#x221e;</mi>
              </munderover>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mrow>
                <mrow>
                  <mi>x</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mi>l</mi>
                      <mo>+</mo>
                      <mi>k</mi>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>&#x2062;</mo>
                <mrow>
                  <mi>x</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mi>k</mi>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
            </mrow>
            <mo>=</mo>
            <mrow>
              <mo>{</mo>
              <mtable>
                <mtr>
                  <mtd>
                    <mi>P</mi>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mrow>
                        <mi>if</mi>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.8em" height="0.8ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <mi>l</mi>
                      </mrow>
                      <mo>=</mo>
                      <mn>0</mn>
                    </mrow>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mi>otherwise</mi>
                  </mtd>
                </mtr>
              </mtable>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mi>Eq</mi>
        <mo>&#x2062;</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mrow>
          <mo>(</mo>
          <mi>B</mi>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where P is a real, positive number, assuming that x(t) has finite support, i.e. it is zero for large +/&#x2212; values of t, and has finite values elsewhere. Convolving a signal with its own time-reverse is the same as correlation with the signal itself, i.e. computing the auto-correlation of the signal. So assuming that x(t) is indeed white, correlating x(t) with itself yields a positive value P for a time lag of 0 and zero everywhere else. Another way of writing this is:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>x</i>(<i>t</i>)*<i>x</i>(&#x2212;<i>t</i>)=<i>P</i>&#xb7;&#x2202;(<i>t</i>)&#x2003;&#x2003;(C)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where &#x2202;(t) is the Dirac delta function. It is now assumed that Eq(B) also holds approximately around a point t<sub>0 </sub>in time, with a length of N+1 samples and a time-window of x(t), so that:
</p>
<p id="p-0125" num="0155">
<maths id="MATH-US-00003" num="00003">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <msub>
            <mrow>
              <mo>[</mo>
              <mrow>
                <mrow>
                  <mi>x</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mi>t</mi>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>*</mo>
                <mrow>
                  <mi>x</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mo>-</mo>
                      <mi>t</mi>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
              <mo>]</mo>
            </mrow>
            <msub>
              <mi>t</mi>
              <mn>0</mn>
            </msub>
          </msub>
          <mo>&#x2062;</mo>
          <mrow>
            <mo>(</mo>
            <mi>l</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mrow>
            <munderover>
              <mo>&#x2211;</mo>
              <mrow>
                <mi>k</mi>
                <mo>=</mo>
                <mrow>
                  <mrow>
                    <mo>-</mo>
                    <mi>N</mi>
                  </mrow>
                  <mo>/</mo>
                  <mn>2</mn>
                </mrow>
              </mrow>
              <mrow>
                <mi>N</mi>
                <mo>/</mo>
                <mn>2</mn>
              </mrow>
            </munderover>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mrow>
              <mrow>
                <mi>x</mi>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>l</mi>
                    <mo>+</mo>
                    <msub>
                      <mi>t</mi>
                      <mn>0</mn>
                    </msub>
                    <mo>-</mo>
                    <mi>k</mi>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mo>&#x2062;</mo>
              <mrow>
                <mi>x</mi>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <msub>
                      <mi>t</mi>
                      <mn>0</mn>
                    </msub>
                    <mo>-</mo>
                    <mi>k</mi>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
          <mo>=</mo>
          <mrow>
            <mrow>
              <munderover>
                <mo>&#x2211;</mo>
                <mrow>
                  <mi>k</mi>
                  <mo>=</mo>
                  <mrow>
                    <mrow>
                      <mo>-</mo>
                      <mi>N</mi>
                    </mrow>
                    <mo>/</mo>
                    <mn>2</mn>
                  </mrow>
                </mrow>
                <mrow>
                  <mi>N</mi>
                  <mo>/</mo>
                  <mn>2</mn>
                </mrow>
              </munderover>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mrow>
                <mrow>
                  <mi>x</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mi>l</mi>
                      <mo>+</mo>
                      <mi>k</mi>
                      <mo>+</mo>
                      <msub>
                        <mi>t</mi>
                        <mn>0</mn>
                      </msub>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>&#x2062;</mo>
                <mrow>
                  <mi>x</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mi>k</mi>
                      <mo>+</mo>
                      <msub>
                        <mi>t</mi>
                        <mn>0</mn>
                      </msub>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
            </mrow>
            <mo>&#x2248;</mo>
            <mrow>
              <mo>{</mo>
              <mtable>
                <mtr>
                  <mtd>
                    <mi>P</mi>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mrow>
                        <mi>if</mi>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.8em" height="0.8ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <mi>l</mi>
                      </mrow>
                      <mo>=</mo>
                      <mn>0</mn>
                    </mrow>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mi>otherwise</mi>
                  </mtd>
                </mtr>
              </mtable>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mi>Eq</mi>
        <mo>&#x2062;</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mrow>
          <mo>(</mo>
          <mi>D</mi>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
Convolving y(t) with x(&#x2212;t) around t<sub>0 </sub>gives:
</p>
<p id="p-0126" num="0156">
<maths id="MATH-US-00004" num="00004">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <msub>
            <mrow>
              <mo>[</mo>
              <mrow>
                <mrow>
                  <mi>y</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mi>t</mi>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>*</mo>
                <mrow>
                  <mi>x</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mo>-</mo>
                      <mi>t</mi>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
              <mo>]</mo>
            </mrow>
            <msub>
              <mi>t</mi>
              <mn>0</mn>
            </msub>
          </msub>
          <mo>&#x2062;</mo>
          <mrow>
            <mo>(</mo>
            <mi>l</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <munderover>
            <mo>&#x2211;</mo>
            <mrow>
              <mi>k</mi>
              <mo>=</mo>
              <mrow>
                <mrow>
                  <mo>-</mo>
                  <mi>N</mi>
                </mrow>
                <mo>/</mo>
                <mn>2</mn>
              </mrow>
            </mrow>
            <mrow>
              <mi>N</mi>
              <mo>/</mo>
              <mn>2</mn>
            </mrow>
          </munderover>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mrow>
            <mrow>
              <mi>y</mi>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>l</mi>
                  <mo>+</mo>
                  <msub>
                    <mi>t</mi>
                    <mn>0</mn>
                  </msub>
                  <mo>-</mo>
                  <mi>k</mi>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>&#x2062;</mo>
            <mrow>
              <mi>x</mi>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <msub>
                    <mi>t</mi>
                    <mn>0</mn>
                  </msub>
                  <mo>-</mo>
                  <mi>k</mi>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mi>Eq</mi>
        <mo>&#x2062;</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mrow>
          <mo>(</mo>
          <mi>E</mi>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
and calculating the inner term in the sum gives:
</p>
<p id="p-0127" num="0157">
<maths id="MATH-US-00005" num="00005">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>y</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mi>l</mi>
              <mo>+</mo>
              <msub>
                <mi>t</mi>
                <mn>0</mn>
              </msub>
              <mo>-</mo>
              <mi>k</mi>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <munderover>
            <mo>&#x2211;</mo>
            <mrow>
              <mi>i</mi>
              <mo>=</mo>
              <mn>0</mn>
            </mrow>
            <mrow>
              <mi>K</mi>
              <mo>-</mo>
              <mn>1</mn>
            </mrow>
          </munderover>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mrow>
            <mrow>
              <mi>x</mi>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>l</mi>
                  <mo>+</mo>
                  <msub>
                    <mi>t</mi>
                    <mn>0</mn>
                  </msub>
                  <mo>-</mo>
                  <mi>k</mi>
                  <mo>-</mo>
                  <mi>i</mi>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>&#x2062;</mo>
            <mrow>
              <mi>h</mi>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mi>i</mi>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mi>Eq</mi>
        <mo>&#x2062;</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mrow>
          <mo>(</mo>
          <mi>F</mi>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
and hence:
</p>
<p id="p-0128" num="0158">
<maths id="MATH-US-00006" num="00006">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <msub>
            <mrow>
              <mo>[</mo>
              <mrow>
                <mrow>
                  <mi>y</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mi>t</mi>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>*</mo>
                <mrow>
                  <mi>x</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mo>-</mo>
                      <mi>t</mi>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
              <mo>]</mo>
            </mrow>
            <msub>
              <mi>t</mi>
              <mn>0</mn>
            </msub>
          </msub>
          <mo>&#x2062;</mo>
          <mrow>
            <mo>(</mo>
            <mi>l</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mrow>
            <munderover>
              <mo>&#x2211;</mo>
              <mrow>
                <mi>k</mi>
                <mo>=</mo>
                <mrow>
                  <mrow>
                    <mo>-</mo>
                    <mi>N</mi>
                  </mrow>
                  <mo>/</mo>
                  <mn>2</mn>
                </mrow>
              </mrow>
              <mrow>
                <mi>N</mi>
                <mo>/</mo>
                <mn>2</mn>
              </mrow>
            </munderover>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mrow>
              <munderover>
                <mo>&#x2211;</mo>
                <mrow>
                  <mi>i</mi>
                  <mo>=</mo>
                  <mn>0</mn>
                </mrow>
                <mrow>
                  <mi>K</mi>
                  <mo>-</mo>
                  <mn>1</mn>
                </mrow>
              </munderover>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mrow>
                <mrow>
                  <mi>x</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mi>l</mi>
                      <mo>+</mo>
                      <msub>
                        <mi>t</mi>
                        <mn>0</mn>
                      </msub>
                      <mo>-</mo>
                      <mi>k</mi>
                      <mo>-</mo>
                      <mi>i</mi>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>&#x2062;</mo>
                <mrow>
                  <mi>h</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mi>i</mi>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>&#x2062;</mo>
                <mrow>
                  <mi>x</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <msub>
                        <mi>t</mi>
                        <mn>0</mn>
                      </msub>
                      <mo>-</mo>
                      <mi>k</mi>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
          <mo>=</mo>
          <mrow>
            <mrow>
              <munderover>
                <mo>&#x2211;</mo>
                <mrow>
                  <mi>i</mi>
                  <mo>=</mo>
                  <mn>0</mn>
                </mrow>
                <mrow>
                  <mi>K</mi>
                  <mo>-</mo>
                  <mn>1</mn>
                </mrow>
              </munderover>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mrow>
                <mrow>
                  <mo>[</mo>
                  <mrow>
                    <munderover>
                      <mo>&#x2211;</mo>
                      <mrow>
                        <mi>k</mi>
                        <mo>-</mo>
                        <mrow>
                          <mi>N</mi>
                          <mo>/</mo>
                          <mn>2</mn>
                        </mrow>
                      </mrow>
                      <mrow>
                        <mi>N</mi>
                        <mo>/</mo>
                        <mn>2</mn>
                      </mrow>
                    </munderover>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.3em" height="0.3ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mrow>
                      <mrow>
                        <mi>x</mi>
                        <mo>&#x2061;</mo>
                        <mrow>
                          <mo>(</mo>
                          <mrow>
                            <mi>l</mi>
                            <mo>+</mo>
                            <msub>
                              <mi>t</mi>
                              <mn>0</mn>
                            </msub>
                            <mo>-</mo>
                            <mi>k</mi>
                            <mo>-</mo>
                            <mi>i</mi>
                          </mrow>
                          <mo>)</mo>
                        </mrow>
                      </mrow>
                      <mo>&#x2062;</mo>
                      <mrow>
                        <mi>x</mi>
                        <mo>&#x2061;</mo>
                        <mrow>
                          <mo>(</mo>
                          <mrow>
                            <msub>
                              <mi>t</mi>
                              <mn>0</mn>
                            </msub>
                            <mo>-</mo>
                            <mi>k</mi>
                          </mrow>
                          <mo>)</mo>
                        </mrow>
                      </mrow>
                    </mrow>
                  </mrow>
                  <mo>]</mo>
                </mrow>
                <mo>&#x2062;</mo>
                <mrow>
                  <mi>h</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mi>i</mi>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
            </mrow>
            <mo>&#x2248;</mo>
            <mrow>
              <mi>P</mi>
              <mo>*</mo>
              <mrow>
                <mi>h</mi>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>(</mo>
                  <mi>l</mi>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mi>Eq</mi>
        <mo>&#x2062;</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mrow>
          <mo>(</mo>
          <mi>G</mi>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0129" num="0159">As can be seen from Eq(D) above, the term in the brackets in Eq(G) is P (approximately) if and only if 1=i and (approximately) 0 otherwise. Hence by choosing l=1, the outcome of the convolution [y(t)*x(&#x2212;t)]<sub>t</sub><sub><sub2>0</sub2></sub>(l) will be P times h(1), if l=2, then it is P times h(2) etc. In this way, the filter coefficients h(.) can be estimated by convolving windows of x(t) with windows of y(t) around a certain &#x201c;centre&#x201d; time sample t<sub>0</sub>.</p>
<p id="p-0130" num="0160">In situations where there are a lot of echoes, and where a frequent update of the impulse response is desired, the problem arises of echoes from a past output signal x(t) leaking into the received signals y(t) in the next frame. A way to overcome this problem is to use a driving signal x(t) where each subsection is as orthogonal as possible, i.e. has a zero cross-correlation function, with the other ones. When estimating the impulse response at a given time-step, correlation is only done with that part of x(t) which was output during the timespan of interest. In practice, it might be difficult to construct signals x(t) for which the windowed snapshots are orthogonal. In WO 2006/067436, a method was suggested for overcoming this problem, by using a different model for estimating the channel impulse response continuously, without the need for perfectly piecewise orthogonal signal sections of x(t). The present invention encompasses using this method, or any other method for estimating a sequence of impulse responses and subsequently collecting them in an impulse response image and using this for tracking, motion estimation, detection, or motion feature extraction.</p>
<p id="p-0131" num="0161">The impulse responses, e.g. calculated as above, for a series of adjacent time-slots can then placed adjacent one another to form an impulse response image. A visual representation of a simple impulse response image is shown in <figref idref="DRAWINGS">FIG. 2</figref>. In a practical system such a visual representation would probably not actually be needed since the relevant analysis can be carried out on the calculated impulse responses without displaying them. Each vertical column of pixels in the image represents one sampling window or time-slot. Thus the vertical scale is filter tap number. The horizontal scale is sample number. The impulse response image can therefore be thought of as the result of chopping up a continuous time line of impulse responses in lengths equal to the length of the sample window, and placing the lengths next to each other.</p>
<p id="p-0132" num="0162">In <figref idref="DRAWINGS">FIG. 2</figref> shows schematically the impulse response image obtained (after a threshold filter has been applied to give enhanced clarity) with a single static object in the imaging field. The lower set of lines <b>16</b> are the impulse responses for the object, e,g, a hand. Since the object is static the impulse response is the same in each time slot and so the lines <b>16</b> are straight and horizontal.</p>
<p id="p-0133" num="0163">The upper set of lines <b>18</b> correspond to the direct path signal, i.e. the signal going straight from the transmitter to the receiver. This contribution <b>18</b> of the direct path, and possibly also background reflections from e.g. parts of the apparatus, to the impulse response image can be conveniently removed in a number of ways. A simple way is to calculate a &#x2018;representative&#x2019; column of impulse responses from a set of columns&#x2014;which could for example be the mean, median or maximum likelihood column&#x2014;and to subtract this from each of the columns. This is an improvement over known arrangements in which a blanking period is used between transmitting and receiving&#x2014;i.e. impulse responses are not calculated for the first few filter taps in a given time-slot. The known arrangement is not ideal because there are situations in which overlap can occur between reflections from an object of interest and either the direct path or background reflections. Of course more advanced techniques could be used to separate in situations where the contribution of the direct path to the image overlaps with that of the echoes from a moving object.</p>
<p id="p-0134" num="0164"><figref idref="DRAWINGS">FIG. 3</figref> shows how the schematic impulse response image of <figref idref="DRAWINGS">FIG. 2</figref> is altered when the direct path signal is removed (as explained above) and the hand is waved in front of the transducer setup. The lines <b>20</b> remain continuous, but now have a corresponding profile. The positive gradients in the lines <b>20</b> represent movement towards the receiver and the negative gradients represent movement away.</p>
<p id="p-0135" num="0165">There are many different ways in which the impulse response images can be analysed automatically to determine the presence or motion of an object in the imaging field. An explanation of the use of line filters for doing this will now be given.</p>
<p id="p-0136" num="0166">A line filter is an algorithm which is applied to the impulse response images and which suppresses parts of the image which do not lie on a straight line of given gradient, i.e. it enhances parts of the image which do lie along the line relative to the rest of the image. The line filter has a roll-off at its edges rather than a sharp boundary. Furthermore each column of the filter mask is convolved with a sin c function representing the bandwidth of the transmit signal. This combines information from neighbouring lines to extract a more visible line.</p>
<p id="p-0137" num="0167"><figref idref="DRAWINGS">FIGS. 4</figref><i>a </i>to <b>4</b><i>c </i>show one way in which the image of <figref idref="DRAWINGS">FIG. 3</figref> might be analysed. The impulse response image <b>22</b> is sent through a filter bank <b>24</b> to produce a set of line-filtered images <b>26</b>, one for each filter applied. Of course each of the filtered images is different as each filter picks out different parts of the original image.</p>
<p id="p-0138" num="0168">For a given time-step t, that is to say at a given horizontal position along each of the filtered images <b>26</b>, a &#x201c;slab&#x201d; cutting through the whole set of images is taken out and collected in a matrix <b>28</b> referred to below as Z(t). Z(t) is arranged so that the leftmost columns correspond to line filtered impulse response images <b>26</b> for which the line in the line filters tilt sharply upwards (the uppermost line in the filter bank <b>24</b>), whereas the rightmost columns correspond to line filters for which the lines tilt sharply downwards (lowermost line on the filter bank <b>24</b>). The intermediate columns are representative of intermediate line filters applied to the impulse response image <b>22</b>.</p>
<p id="p-0139" num="0169">This representation is useful for detecting specific motional tendencies. If, for instance, the matrix Z(t) has a broad layout like that indicated in <figref idref="DRAWINGS">FIG. 4</figref><i>b</i>, it indicates that in the upper part of the scene, i.e. representative of objects which are close to the transmitter/receiver (TX/RX) setup, the motion of these objects is towards the TX/RX setup. In the lower part of the image, representative of objects far away from the TX/RX, setup the objects are moving away from: the TX/RX. The situation could also be more localized, as illustrated by <figref idref="DRAWINGS">FIG. 4</figref><i>c</i>. Here, there is not much going on very close to or very far away from the TX/RX setup. In the middle however, there is an object moving closer to the TX/RX setup and one slightly further away which is moving further away. This can be detected using a filter F on the matrix Z(t), denoted by
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>w</i>(<i>t</i>)=<i>Z</i>(<i>t</i>)*<i>F</i>&#x2003;&#x2003;(Eq H)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0140" num="0170">Where F is a two-dimensional mask, typically being identical with the centre part of the layout shown in <figref idref="DRAWINGS">FIG. 4</figref><i>c </i>except with its columns and rows flipped. This technique is known as matched filtering. Typically, F will have the same number of columns as Z(t), thereby making the legal part of the convolution output a single column vector. Each element in this column vector contains a score, indicating the degree of presence of the expected gesture. Clearly, a full bank of filters could be used to detect variations of the same gesture, or to separate between more gestures. Furthermore since Z(t) is a result of the (raw) filtering of an impulse response image filtered with various line filters, it might be advantageous to compute the envelope of this matrix, as the values themselves will tend to fluctuate quickly between positive and negative values. Such envelope extraction can be accomplished by computing
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>w</i>(<i>t</i>)=(|<i>Z</i>(<i>t</i>)|*<i>B</i>)*<i>F</i>&#x2003;&#x2003;(Eq I)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0141" num="0171">Where |.| is the operator extracting the absolute values in the matrix and arranging them in a corresponding matrix, i.e. the element-by-element absolute value operator, and B is a two- or one-dimensional mask, which could be Gaussian. If B is a two-dimensional mask, it will blur impulse response images filtered by similar line filters, thereby indicating a lack of discrimination between similar lines, which might be better for some applications. If B is a one-dimensional vertical mask, this does not happen. Other filtering means, such as envelope extraction by means of Laplace transformation could also equally well be applied.</p>
<p id="p-0142" num="0172">The above-described &#x2018;motion filter&#x2019; approach extends beyond their use upon matrices Z(t) stemming from line-filtered impulse response images. If the background data was not impulse response images, but instead two-dimensional or three-dimensional images, created by inversion or otherwise, the filter masks B and F would be 3-way or 4-way filter masks, representing filtering in 2 or 3 spatial dimension and one temporal dimension. This variant might be useful for detecting situations which cannot readily be identified by considering impulse responses from a single TX/RX channel, such as the direction of rotation of an object in 3-dimensional space, or detecting the exact axis along which a finger-separating motion occurs. Clearly also, the filter mask F does not have to be a linear filter, and the convolution operator &#x2018;*&#x2019; isn't limited to denoting a linear filter operation. Non-linear filters, or combinations of filters could be used, such as max/min or mean filters. Various local combinations of filters, such as extracting the maximum value in the upper part of a window and the minimum value in the lower, could be used. Furthermore, the filtering operation could be carried out using any linear or non-linear extraction of the underlying data material; the element-by-element absolute value |.| operator being just one example.</p>
<p id="p-0143" num="0173">In addition to or instead of the line filters, edge filters may be included in filter bank <b>24</b>. An edge filter is an algorithm which is applied to the impulse response images and which suppresses parts of the image which do not lie on the edge of a straight line of given gradient, i.e. it enhances the edges of lines within the image that are parallel with the filter. This may be implemented as threshold rate of intensity change along a gradient perpendicular to that of the edge filter's gradient. This can be useful for tracking the periphery of an object; in particular the leading or trailing edge or surface of an object as it approaches or recedes from the transmitters/receivers. In some situations, it may be helpful to apply non-directional edge or line filters, which pass edges or lines of any orientation.</p>
<p id="p-0144" num="0174"><figref idref="DRAWINGS">FIG. 5</figref> is a schematic representation of the impulse response image corresponding to two objects moving apart, swapping sides and then swapping back to their original configuration. It can be seen therefore that the set of parallel lines at the left-hand edge of the image diverge at point A as the objects start to move apart in the &#x2018;time-of-flight&#x2019; direction. A given time of flight and the (usually physically separated) transmitter and receiver between them define an ellipse with the transmitter and receiver at the two focuses. The movement that is detected is the component of actual movement along a line joining the object and the centre of the ellipse. This can be thought of as the time-of-flight direction.</p>
<p id="p-0145" num="0175">The two objects then move together again and swap sides so that they are separated again at point B. Thereafter they are swapped over again to their original positions (point C) where they remain stationary and so the lines are once more horizontal.</p>
<p id="p-0146" num="0176"><figref idref="DRAWINGS">FIG. 6</figref><i>a </i>shows the impulse response image of <figref idref="DRAWINGS">FIG. 5</figref> after a line filter has been applied. As can be appreciated the resulting image is much easier for software to decipher. The patterns of energy highlighted by the filter (and similar filters with different slopes that might be applied) can be interpreted by an algorithm that looks for energy in the expected places.</p>
<p id="p-0147" num="0177"><figref idref="DRAWINGS">FIG. 6</figref><i>b </i>shows the impulse response image of <figref idref="DRAWINGS">FIG. 5</figref> after an edge filter has been applied. As with the line filter, the patterns of energy highlighted by the edge filter can be interpreted by an algorithm that looks for energy in the expected places.</p>
<p id="p-0148" num="0178"><figref idref="DRAWINGS">FIG. 7</figref> shows the impulse response image corresponding to a &#x2018;thumb-click&#x2019;: that is a hand with one index finger touching the table slightly in front of the others, moving on the table. In the parts where the upper part of the image is flat <b>30</b>, <b>32</b>, the front finger stays still but the motion of the thumb <b>34</b>, <b>36</b> relative to the rest of the hand can be seen.</p>
<p id="p-0149" num="0179"><figref idref="DRAWINGS">FIG. 8</figref><i>a </i>shows an illustration of two fingers moving apart. <b>38</b> shows the curve of the front-most finger, <b>40</b> points to situations where one can hardly see another finger moving in the other direction. <b>42</b> shows some heavy'lines which are continually present in the picture&#x2014;they represent peak values in the impulse response relating to the &#x201c;rest of the hand&#x201d;, which is more or less standing still while the two fingers move. The separation and the detection thereof could be taken as a signal to the computer to zoom in an image, or to grab an object (pinching together) or releasing an object (separating the fingers).</p>
<p id="p-0150" num="0180"><figref idref="DRAWINGS">FIG. 8</figref><i>b </i>shows the impulse response image of <figref idref="DRAWINGS">FIG. 8</figref><i>a </i>but with most of the horizontal lines (corresponding to stationary elements) having been removed by a suitable filter. Now both the up and down movement are visible.</p>
<p id="p-0151" num="0181"><figref idref="DRAWINGS">FIG. 8</figref><i>c </i>shows the impulse response image after applying suitable line filters. The motions of interest are clearly visible, having been extracted from the very confused original impulse response image (<figref idref="DRAWINGS">FIG. 8</figref><i>a</i>).</p>
<p id="p-0152" num="0182">These simple examples show that embodiments of the invention allow relatively complex gestures to be automatically interpreted using a simple energy pattern-matching algorithm. The approach described above is not analytically tracking the individual fingers (and so has a much lower requirement for resolution and computing power than such tracking would do) and it has a high degree of tolerance to factors like the distance of the hand from the transducers, the speed of movement, the shape and orientation of the hand, and even the presence of other objects in the imaging field. It should be appreciated however that should it be necessary or desirable the principles of the invention can also be applied to separating the movement of individual objects such as fingers so that they can be tracked.</p>
<p id="p-0153" num="0183">Considering the line-filter approach set out above more analytically, line filters can be used generally to determine the presence of a reflector at position x(t) moving in a spatial direction &#x394;x(t).</p>
<p id="p-0154" num="0184">If it is postulated that there exists a reflector at point x(t) moving in space with direction &#x3b1;x(t), the next point in space will be:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>x</i>(<i>t+</i>1)=<i>x</i>(<i>t</i>)+&#x394;<i>x</i>(<i>t</i>)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
assuming a locally linear motion. Both x(t) and x(t+1) give rise to distance-of-flights {d<sub>k</sub>(t)} and {d<sub>k</sub>(t+1)} for each transmitter/receiver pair, i.e. for each channel, k. The change in time delay from d<sub>k</sub>(t) to d<sub>k </sub>(t+1) for a specific channel k aligns with a line in the impulse response image, having the angle
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>&#x3b1;<sub>k</sub>=tan<sup>&#x2212;l</sup><i>{d</i><sub>k</sub>(<i>t+</i>1)&#x2212;<i>d</i><sub>k</sub>(<i>t</i>)}<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0155" num="0185">Hence, a way to test whether the reflector located in x(t) is actually moving in the direction &#x394;x(t), is to check each of the impulse response images, Z<sub>1</sub>, Z<sub>2</sub>, . . . Z<sub>K</sub>, each image Z<sub>k </sub>being filtered with a line filter having a line with inclination angle &#x3b1;<sub>k</sub>, to see whether it has high energy in the column corresponding to the time index t.</p>
<p id="p-0156" num="0186">Letting:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>W</i><sub>k</sub><i>=F</i>(<i>Z</i><sub>k</sub>,&#x3b1;<sub>k</sub>)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
denote the filtering of the image Z<sub>k </sub>with a line filter having an angle of inclination equal to &#x3b1;<sub>k</sub>, it must be checked whether the images W<sub>1</sub>, W<sub>2</sub>, . . . , W<sub>K </sub>have high energy in the column corresponding to the time index t. To be precise, letting W<sub>k </sub>(t,d) denote the value of the filtered image W<sub>k </sub>corresponding with the t'th time frame and the travel time d (in samples) between the transmitter, the possibly reflective point and the receiver. Then, a test observer for the test could be
</p>
<p id="p-0157" num="0187">
<maths id="MATH-US-00007" num="00007">
<math overflow="scroll">
<mrow>
  <mrow>
    <mi>z</mi>
    <mo>&#x2062;</mo>
    <mrow>
      <mo>{</mo>
      <mrow>
        <mrow>
          <mi>x</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mi>t</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>,</mo>
        <mrow>
          <mi>&#x394;</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mrow>
            <mi>x</mi>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mi>t</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
      <mo>}</mo>
    </mrow>
  </mrow>
  <mo>=</mo>
  <mrow>
    <munderover>
      <mo>&#x2211;</mo>
      <mrow>
        <mi>i</mi>
        <mo>=</mo>
        <mn>1</mn>
      </mrow>
      <mi>K</mi>
    </munderover>
    <mo>&#x2062;</mo>
    <mstyle>
      <mspace width="0.3em" height="0.3ex"/>
    </mstyle>
    <mo>&#x2062;</mo>
    <mrow>
      <msub>
        <mi>W</mi>
        <mi>k</mi>
      </msub>
      <mo>&#x2061;</mo>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>t</mi>
          <mo>,</mo>
          <mrow>
            <msub>
              <mi>d</mi>
              <mi>k</mi>
            </msub>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mi>t</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
</p>
<p id="p-0158" num="0188">If this score is high, then there is a high probability that there is a reflective object in the position x(t), moving with the direction &#x394;x(t). To find the underlying distribution for the test observer, the distribution of z{x(t), &#x394;x(t)} could be sampled in &#x2018;empty areas&#x2019; of the acoustic scene.</p>
<p id="p-0159" num="0189">Clearly, once an initial position x(t<sub>0</sub>) has been found, this test could be used to track an object throughout the scene, since a test can be conducted as to where the point has moved from one frame to the next. To improve tracking quality, one could at each stage try out a limited number of different &#x394;x(t)'s, i.e. only the ones that correspond with a reasonable guess at a possible physical motion. Clearly, the acceleration cannot be infinite from time-step to time-step.</p>
<p id="p-0160" num="0190">The tracking process could also be applied to a neighbourhood of points {x<sub>i </sub>(t)}, thereby improving the robustness of the tracking process, particularly for reflectors which are not represented as point reflectors within the frequency band of the system. The possible set of reflective points {x<sub>i </sub>(t)} could be related to prior knowledge of the object being tracked, e.g. they could represent the model of a hand or a set of fingers say by specifying that fingers can only bend in certain directions. Such a model does not need to be static, it could be a flexible model such as an active shape model, a statistical shape model, an active appearance model, a snake or an &#x2018;active blob&#x2019;. Such motion model tracking in the impulse response domain allows more accurate tracking of particular objects. It could also be applied in 2D or 3D imaging domains.</p>
<p id="p-0161" num="0191">An advantage of using the aforementioned approaches over the state-of-the art, is that multiple reflective points could be tracked in the scene simultaneously&#x2014;even if these give rise to the same times-of-flight or channel delays in one or more of the channels. This follows since the corresponding overlapping curves are &#x2018;resolved&#x2019; by using the line filters. In particular, since when tracking one object, only a narrow range of &#x3b1;x(t)'s are tested out for the motion of a particular object, two objects can more often than not &#x2018;cross&#x2019; each other's path without causing confusion as to which object belongs to which sequence of times-of-flights. This is an advantage when tracking multiple hands, fingers, styluses or objects within the scene.</p>
<p id="p-0162" num="0192">A further advantage of the use of line filters, is that effectively they allow separation of the impulse response image energy of an object that has a physical &#x2018;direction&#x2019; in space from that of general noise. When detecting the presence and the initial position of an object in the acoustic scene, this is important, since without using this motion information filtering, a tracking system could easily give a false-positive indication of the presence and the position of a reflector in the scene. In particular, one might want to track a finger standing out from a hand, i.e. being closer to the TX/RX setup than the rest of the hand. As the hand, with the finger pointing out, moves into the acoustic scene however, the motion of the finger will not normally differ a lot from that of the hand. Therefore, the motion information could effectively be used for the purpose of segmentation, i.e. to identify (the whole of) an object moving in a specific direction and velocity. The detection of an object of a certain size, identified as a moving segment, could be taken as a cue to start tracking. This could, for instance, make it possible for the system to discern between a hand, a finger, or a smaller or a bigger object being inserted into the acoustic scene. Another benefit is that it could help locating the exteriors of the object being moved into the scene, such as the finger tip. The finger tip could then be defined as the front-most part of the segment, the segment again being defined as the collection of points moving with a specific direction and speed.</p>
<p id="p-0163" num="0193">In some embodiments of some aspects of the invention the techniques described earlier for motion separation can be applied to three-dimensional images of the scene formed e.g. by delay-and-sum imaging. The principle behind this technique is to estimate the reflectivities of points in a 3-dimensional grid. These grid points then effectively become a representation of the 3-dimensional scene, so that the grid points are related to &#x2018;voxels&#x2019; (3-dimensional equivalents of pixels in 2D-imaging) of the 3-dimensional image.</p>
<p id="p-0164" num="0194">To explain how delay-and-sum imaging works, it is first assumed that there are N spherical receivers, located in positions r<sub>1</sub>, r<sub>2</sub>, . . . r<sub>N </sub>and a single transmitter located at the origin (clearly the concept can be extended to cover the possibility of using more than one transmitter). The objective is to compute &#x2018;voxel&#x2019; values at various points in space x<sub>1</sub>, x<sub>2</sub>, . . . x<sub>Q</sub>, i.e. values indicating the reflectivities at these points. The impulse responses are recorded/computed at the respective receivers, either through a pulse emission by the transmitter, or computed by other means, as described previously; these are defined as i<sub>1</sub>(s), i<sub>2</sub>(s), . . . i<sub>N </sub>(s).</p>
<p id="p-0165" num="0195">Now, a check can be made whether or not, and to what extent, there is a reflective point located at a position x<sub>k</sub>.</p>
<p id="p-0166" num="0196">The distance that would be travelled by an acoustic signal from the transmitter at the origin to the postulated reflective point x<sub>k </sub>and then to the receiver r<sub>1 </sub>is given by:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>d</i><sub>jk</sub><i>=&#x2225;x</i><sub>k</sub><i>&#x2225;+&#x2225;x</i><sub>k</sub><i>&#x2212;r</i><sub>j</sub>&#x2225;<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0167" num="0197">If there is indeed a reflector at the chosen point, this distance should then match with a &#x2018;peak&#x2019; in the impulse response for any given receiver j. The working hypothesis for the presence of a reflector is that &#x201c;if (a) a pulse was sent at time zero, and (b) it bounced off a reflector located in x<sub>k</sub>, then, at given times after the sound hit the reflector, each of the receivers would detect a burst of energy&#x201d;. The distances can be converted to timings (measured in numbers of samples) by multiplying by f/c where f is the sampling frequency and c the speed of sound.</p>
<p id="p-0168" num="0198">In other words the value of the expression</p>
<p id="p-0169" num="0199">
<maths id="MATH-US-00008" num="00008">
<math overflow="scroll">
<mrow>
  <msub>
    <mi>i</mi>
    <mi>j</mi>
  </msub>
  <mo>&#x2061;</mo>
  <mrow>
    <mo>(</mo>
    <mrow>
      <mfrac>
        <mi>f</mi>
        <mi>c</mi>
      </mfrac>
      <mo>&#xd7;</mo>
      <msub>
        <mi>d</mi>
        <mi>jk</mi>
      </msub>
    </mrow>
    <mo>)</mo>
  </mrow>
</mrow>
</math>
</maths>
<br/>
should be large value for any j, since the impulse response simply measures how much energy was received at any given sampling point in time. The multiplication by f/c simply converts the travel distance d into a sample index in the impulse response. Hence, an estimate of the reflective capacity in the point x<sub>k </sub>would be
</p>
<p id="p-0170" num="0200">
<maths id="MATH-US-00009" num="00009">
<math overflow="scroll">
<mrow>
  <msub>
    <mi>E</mi>
    <mi>k</mi>
  </msub>
  <mo>=</mo>
  <mrow>
    <munderover>
      <mo>&#x2211;</mo>
      <mrow>
        <mi>j</mi>
        <mo>=</mo>
        <mn>1</mn>
      </mrow>
      <mi>N</mi>
    </munderover>
    <mo>&#x2062;</mo>
    <mstyle>
      <mspace width="0.3em" height="0.3ex"/>
    </mstyle>
    <mo>&#x2062;</mo>
    <mrow>
      <msub>
        <mi>i</mi>
        <mi>j</mi>
      </msub>
      <mo>&#x2061;</mo>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mfrac>
            <mi>f</mi>
            <mi>c</mi>
          </mfrac>
          <mo>&#xd7;</mo>
          <msub>
            <mi>d</mi>
            <mi>jk</mi>
          </msub>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
</p>
<p id="p-0171" num="0201">Having computed the reflective capacity values or &#x2018;voxel&#x2019; intensity values E<sub>1</sub>, E<sub>2</sub>, . . . E<sub>Q </sub>corresponding with the points x<sub>1</sub>, x<sub>2</sub>, . . . x<sub>Q</sub>, the delay-and-sum imaging is complete. Clearly, the choice of sampling points will be important for the quality of the imaging, as will the positioning of the respective receivers (and/or transmitters). The signal bandwidth is also important in order to minimize the effect of sidelobes' and &#x2018;grating lobes&#x2019; which could be problematic if an under-sampled array were to be used.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08625846-20140107-M00001.NB">
<img id="EMI-M00001" he="9.14mm" wi="76.20mm" file="US08625846-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US08625846-20140107-M00002.NB">
<img id="EMI-M00002" he="12.70mm" wi="76.20mm" file="US08625846-20140107-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00003" nb-file="US08625846-20140107-M00003.NB">
<img id="EMI-M00003" he="19.39mm" wi="76.20mm" file="US08625846-20140107-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00004" nb-file="US08625846-20140107-M00004.NB">
<img id="EMI-M00004" he="9.14mm" wi="76.20mm" file="US08625846-20140107-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00005" nb-file="US08625846-20140107-M00005.NB">
<img id="EMI-M00005" he="8.81mm" wi="76.20mm" file="US08625846-20140107-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00006" nb-file="US08625846-20140107-M00006.NB">
<img id="EMI-M00006" he="21.17mm" wi="76.20mm" file="US08625846-20140107-M00006.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00007" nb-file="US08625846-20140107-M00007.NB">
<img id="EMI-M00007" he="8.81mm" wi="76.20mm" file="US08625846-20140107-M00007.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00008" nb-file="US08625846-20140107-M00008.NB">
<img id="EMI-M00008" he="6.35mm" wi="76.20mm" file="US08625846-20140107-M00008.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00009" nb-file="US08625846-20140107-M00009.NB">
<img id="EMI-M00009" he="9.14mm" wi="76.20mm" file="US08625846-20140107-M00009.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method of recognizing motion of a human hand, the method comprising:
<claim-text>transmitting a plurality of transmit signals in respective time frames;</claim-text>
<claim-text>receiving a plurality of receive signals, at least some of the receive signals being reflected from the hand;</claim-text>
<claim-text>calculating a plurality of channel impulse responses using the transmit and receive signals, wherein the receive signals are convolutions of the transmit signals and the channel impulse responses;</claim-text>
<claim-text>defining a matrix of impulse responses, with impulse responses for adjacent time frames adjacent to each other; and</claim-text>
<claim-text>analysing the matrix for patterns corresponding to movement of the hand.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, comprising recording the matrix as data in a storage medium of a computing device.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> comprising determining motion information for multiple reflective points or parts of objects from the impulse response matrix.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> comprising tracking the motion of a plurality of objects by analysing said matrix.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> comprising determining the motion of an object by detecting an aggregate change in the channel impulse responses, or a predetermined portion thereof, above a threshold.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising determining a size, shape or reflectivity of said one or more objects.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> comprising applying to the matrix or a part of the matrix one or more functions selected from the group consisting of: a curvilinear filter, a line filter, a straight-line filter, a sinusoidal filter, an edge filter, an up-sampling filter, a down-sampling filter, a re-sampling filter, a Hough transform, a wavelet transform, a radon transform, a pi-tau transform, a Fourier transform, a fractional Fourier transforms, a column-shift, a moving average of columns, a rolling average, an infinite impulse response filters, and a filter based on sub-band coding.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> comprising applying to the matrix or a part of the matrix a non-horizontal averaging filter.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> comprising applying a plurality of filters to the matrix and comparing their outputs to determine a filter that gives a strongest result overall or within a given area of the matrix.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> comprising determining whether the output of a filter applied to the matrix exceeds a threshold.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> comprising applying to the matrix a line filter having a roll-off at the edges of the line.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> comprising applying to the matrix an adaptive outlier-filter that removes columns, rows or entries having a probability of noise above a threshold probability.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> comprising matching sub-blocks or sliding sub-blocks of the matrix to a set of basis blocks.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the step of analysing said matrix for patterns comprises recording a sequence of filters matched to respective portions of the matrix, and comparing said sequence to one or more predetermined sequences to identify a characteristic pattern in the matrix.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref> wherein said characteristic pattern corresponds to a particular gesture.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref> wherein the predetermined sequence comprises a sequence of sets of filters where any filter from the correct set can satisfy the sequence.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref> wherein the sequence of filters consists of a sequence of line filters at predetermined angles.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> comprising receiving a plurality of receive signals at each of a plurality of receivers, defining a matrix of impulse responses for each receiver, and applying filters to each of said matrices, the filters applied to each matrix being adapted from a common velocity conjecture for said one or more objects.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein said signals are ultrasonic signals.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> comprising using the motion of the human hand to control at least one operation of an electronic device.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising storing results of said analysis in a memory.</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising providing a display signal for a display device.</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. An apparatus for recognising motion of a human hand comprising:
<claim-text>one or more transmitters arranged to transmit a plurality of transmit signals in respective time frames;</claim-text>
<claim-text>one or more receivers arranged to receive a plurality of receive signals, at least some of said receive signals being reflected from said hand; and</claim-text>
<claim-text>processing logic arranged to calculate a plurality of channel impulse responses using said transmit and receive signals wherein the receive signals are convolutions of the transmit signals and the channel impulse responses, define a matrix of impulse responses, with impulse responses for adjacent time frames adjacent each other, and analyse said matrix for patterns corresponding to movement of said hand.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. The apparatus as claimed in <claim-ref idref="CLM-00023">claim 23</claim-ref> wherein at least one of said one or more transmitters is an ultrasound transmitter.</claim-text>
</claim>
<claim id="CLM-00025" num="00025">
<claim-text>25. A computer program product comprising a non-transitory computer readable medium encoded with an information processing program for use in an information processing device, the program recognises motion of a human hand when executed, the program having inputs for a plurality of transmit signals in respective time frames and a plurality of receive signals when executed and further performs the operations comprising: calculating a plurality of channel impulse responses using said transmit and receive signals, wherein the receive signals are convolutions of the transmit signals and the channel impulse responses; defining a matrix of impulse responses, with impulse responses for adjacent time frames adjacent each other; and analysing said matrix for patterns corresponding to one or more objects.</claim-text>
</claim>
<claim id="CLM-00026" num="00026">
<claim-text>26. An electronic device comprising an apparatus for controlling at least one operation of the electronic device by recognising motion of a human hand, the apparatus comprising:
<claim-text>one or more transmitters arranged to transmit a plurality of transmit signals in respective time frames;</claim-text>
<claim-text>one or more receivers arranged to receive a plurality of receive signals, at least some of said receive signals being reflected from said hand; and</claim-text>
<claim-text>processing logic arranged to determine calculate a plurality of channel impulse responses using said transmit and receive signals, wherein the receive signals are convolutions of the transmit signals and the channel impulse response, define a matrix of impulse responses, with impulse responses for adjacent time frames adjacent each other, and analyse said matrix for patterns corresponding to movement of said hand.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00027" num="00027">
<claim-text>27. The electronic device of <claim-ref idref="CLM-00026">claim 26</claim-ref> wherein the device is a mobile device.</claim-text>
</claim>
<claim id="CLM-00028" num="00028">
<claim-text>28. A method of determining the state of one or more objects in an imaging field comprising:
<claim-text>transmitting a plurality of transmit signals in respective time frames;</claim-text>
<claim-text>receiving a plurality of receive signals;</claim-text>
<claim-text>calculating a plurality of channel impulse responses using said transmit and receive signals, wherein the receive signals are convolutions of the transmit signals and the channel impulse responses;</claim-text>
<claim-text>defining a matrix of impulse responses, with impulse responses for adjacent time frames adjacent each other; and</claim-text>
<claim-text>analysing said matrix for patterns corresponding to one or more objects.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00029" num="00029">
<claim-text>29. The method of <claim-ref idref="CLM-00028">claim 28</claim-ref> comprising using said analysis to control an electronic device.</claim-text>
</claim>
<claim id="CLM-00030" num="00030">
<claim-text>30. The method of <claim-ref idref="CLM-00028">claim 28</claim-ref> wherein the one or more objects includes a human hand or part thereof.</claim-text>
</claim>
<claim id="CLM-00031" num="00031">
<claim-text>31. The method of <claim-ref idref="CLM-00028">claim 28</claim-ref> comprising tracking an object and controlling an electronic device in response to said tracking.</claim-text>
</claim>
<claim id="CLM-00032" num="00032">
<claim-text>32. The method of <claim-ref idref="CLM-00028">claim 28</claim-ref> further comprising storing results of said analysis in a memory.</claim-text>
</claim>
<claim id="CLM-00033" num="00033">
<claim-text>33. The method of <claim-ref idref="CLM-00028">claim 28</claim-ref> further comprising providing a display signal for a display device.</claim-text>
</claim>
<claim id="CLM-00034" num="00034">
<claim-text>34. An apparatus for determining the state of one or more objects in an imaging field comprising:
<claim-text>one or more transmitters arranged to transmit a plurality of transmit signals in respective time frames;</claim-text>
<claim-text>one or more receivers arranged to receive a plurality of receive signals; and</claim-text>
<claim-text>processing logic arranged to calculate a plurality of channel impulse responses using said transmit and receive signals, wherein the receive signals are convolutions of the transmit signals and the channel impulse responses, define a matrix of impulse responses, with impulse responses for adjacent time frames adjacent each other, and analyse said matrix for patterns corresponding to one or more objects.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00035" num="00035">
<claim-text>35. A computer program product comprising a non-transitory computer readable medium encoded with an information processing program for use in an information processing device, the program when executed determines the state of one or more objects in an imaging field, the program having inputs for a plurality of transmit signals in respective time frames and a plurality of receive signals and when executed further performs the operations comprising: calculating a plurality of channel impulse responses using said transmit and receive signals, wherein the receive signals are convolutions of the transmit signals and the channel impulse responses; defining a matrix of impulse responses, with impulse responses for adjacent time frames adjacent each other; and analysing said matrix for patterns corresponding to one or more objects. </claim-text>
</claim>
</claims>
</us-patent-grant>
