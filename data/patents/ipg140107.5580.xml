<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626680-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626680</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12960131</doc-number>
<date>20101203</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<us-term-of-grant>
<us-term-extension>345</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>15</main-group>
<subgroup>18</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>706 12</main-classification>
</classification-national>
<invention-title id="d2e53">Group variable selection in spatiotemporal modeling</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>8145745</doc-number>
<kind>B1</kind>
<name>Ge et al.</name>
<date>20120300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>2007/0214097</doc-number>
<kind>A1</kind>
<name>Parsons et al.</name>
<date>20070900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>2008/0228446</doc-number>
<kind>A1</kind>
<name>Baraniuk et al.</name>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>2010/0205138</doc-number>
<kind>A1</kind>
<name>Zhang et al.</name>
<date>20100800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>2011/0028827</doc-number>
<kind>A1</kind>
<name>Sitaram et al.</name>
<date>20110200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2011/0055132</doc-number>
<kind>A1</kind>
<name>Mahdian et al.</name>
<date>20110300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2011/0112998</doc-number>
<kind>A1</kind>
<name>Abe et al.</name>
<date>20110500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2011/0131162</doc-number>
<kind>A1</kind>
<name>Kaushal et al.</name>
<date>20110600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2011/0167031</doc-number>
<kind>A1</kind>
<name>Kleinberg et al.</name>
<date>20110700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2012/0117059</doc-number>
<kind>A1</kind>
<name>Bailey et al.</name>
<date>20120500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2012/0262165</doc-number>
<kind>A1</kind>
<name>Griswold et al.</name>
<date>20121000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2012/0262166</doc-number>
<kind>A1</kind>
<name>Griswold et al.</name>
<date>20121000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00013">
<othercit>Spatial-temporal Causal Modeling for Climate Change Attribution A. Lozano, H. Li, A. Niculescu-Mizil, Y. Liu, C. Perlich, J. Hosking, and N. Abe IBM T. J. Watson Research Center Yorktown Heights, NY 10598.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00014">
<othercit>Lozano, A. et al., &#x201c;Block Variable Selection in Multivariate Regression and High-dimensional Causal Inference&#x201d;, Advanced Neural Information Processing Systems, pp. 1-9.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00015">
<othercit>Lozano, A. et al., &#x201c;Grouped Graphical Granger Modeling Methods for Temporal Causal Modeling&#x201d;, KDD'09, Jun. 28-Jul. 1, 2009, Paris, France, pp. 577-585.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00016">
<othercit>Arnold A. et al., &#x201c;Temporal Causal Modeling with Graphical Granger Methods&#x201d;, DDD'07 Aug. 12-15, 2007, San Jose, California, USA, pp. 66-75.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00017">
<othercit>Agarwal, N. et al., &#x201c;Identifying the Influential Bloggers in a Community&#x201d; WSDM'08, Feb. 11-12, 2008, pp. 207-217.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00018">
<othercit>Deshpande, G. et al., &#x201c;Multivariate Granger Causality Analysis of fMRI Data&#x201d;, Human Brain Mapping, 30, pp. 1361-1373, 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00019">
<othercit>Eiter, T. et al., &#x201c;Complexity results for structure-based causality&#x201d;, Artificial Intelligence, 142, 2002, pp. 53-89.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00020">
<othercit>Office Action dated Jan. 14, 2013 received in a related U.S. Appl. No. 12/960,106.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00021">
<othercit>Argyriou et al., &#x201c;Convex multi-task feature learning&#x201d;, Springer Science+Business Media, LLC (2008), pp. 243-272, Jan. 9, 2008.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00022">
<othercit>Bedrick et al., &#x201c;Model Selection for Multivariate Regression in Small Samples&#x201d;, Biometrics, Mar. 1994, p. 226.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00023">
<othercit>Breiman et al., &#x201c;Predicting Multivariate Responses in Multiple Linear Regression&#x201d;, Journal of the Royal Statistical Society, Series B (Methodological), vol. 59, No. 1 (1997), pp. 3-54; http://www.jstor.org/Tue May 24 10:43:34 2005.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00024">
<othercit>Enders, &#x201c;Applied Econometric Time Series&#x201d;, John Wiley &#x26; Sons, 2003.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00025">
<othercit>Fletcher et al., &#x201c;Orthogonal Matching Pursuit from Noisy Measurements: A New Analysis&#x201d;, NIPS, 2009, pp. 1-9.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00026">
<othercit>Frank et al., &#x201c;A Statistical View of Some Chemometrics Regression Tools&#x201d;, Technometrics, vol. 35, No. 2. p. 109, May 1993.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00027">
<othercit>Friedman et al., &#x201c;Sparse inverse covariance estimation with the graphical lasso&#x201d;, Biostatistics (2008), 9, 3, pp. 432-441, Advance Access publication on Dec. 12, 2007.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00028">
<othercit>Fujikoshi et al., &#x201c;Modified AIC and Cp in multivariate linear regression&#x201d;, Oxford Journals, Mathematics &#x26; Physical Sciences, Biometrika, vol. 84, Issue 3, pp. 707-716, Sep. 1, 1996.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00029">
<othercit>Granger, &#x201c;Testing for Causality&#x201d;, Journal of Economic Dynamics and Control 2 (1980) pp. 329-352, Jun. 1980.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00030">
<othercit>Harville, &#x201c;Matrix Algebra From a Statistician's Perspective&#x201d;, Springer, 1997.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00031">
<othercit>Huang et al., &#x201c;Learning with Structured Sparsity&#x201d;, Proceedings of the 26th Annual International Conference on Machine Learning, 2009, pp. 417-424.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00032">
<othercit>Joachims, &#x201c;Structured Output Prediction with Support Vector Machines&#x201d;, Springer-Verlag Berlin Heidelberg 2006, LNCS 4109, pp. 1-7.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00033">
<othercit>Kleinberg, &#x201c;Authoritative Sources in a Hyperlinked Environment&#x201d;, Proceedings of teh ACM-SIAM Symposium on Discrete Algorithms, 1998, and as IBM Research Report RJ 10076, May 1997.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00034">
<othercit>Lozano et al., &#x201c;Grouped graphical Granger modeling for gene expression regulatory networks discovery&#x201d;, vol. 25 ISMB 2009, pp. i110-i118, doi:10.1093/bioinformatics/btp199.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00035">
<othercit>Lozano et al., &#x201c;Group Orthogonal Matching Pursuit for Variable Selection and Prediction&#x201d;, Advances in Neural Information Processing Systems 22, 2009, pp. 1-9.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00036">
<othercit>Mallat et al., &#x201c;Matching Pursuit With time-Frequency Dictionaries&#x201d;, IEEE Transactions of Signal Processing, Dec. 1993, pp. 1-43.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00037">
<othercit>Meinshausen et al., &#x201c;High Dimensional Graphs and Variable Selection with the Lasso&#x201d;, The Annals of Statistics, AMS 2000, pp. 1-32.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00038">
<othercit>Melville et al., &#x201c;Social Media Analytics: Channeling the Power of Blogosphere for Marketing Insight&#x201d;, WIN 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00039">
<othercit>Micchelli et al., &#x201c;Kernels for Multi-task Learning&#x201d;, In NIPS, 2004.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00040">
<othercit>Obozinski et al., &#x201c;Mutt-task feature selection&#x201d;, Technical Report, Jun. 2006, Department of Statistics, University of California, Berkeley, pp. 1-15.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00041">
<othercit>Page et al., &#x201c;The PageRank Citation Ranking: Bringing Order to the Web&#x201d;, Technical Report, Stanford Digital Libraries, Jan. 29, 1998, pp. 1-17.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00042">
<othercit>Reinsel et al., &#x201c;Lecture Notes in Statistics&#x201d;, Springer, Published Sep. 1998.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00043">
<othercit>Ricci et al., &#x201c;Magic Moments for Structured Output Prediction&#x201d;, Journal of Machine Learning Research, 9 (2008) Dec. 2008, pp. 2801-2846.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00044">
<othercit>Tibshirani, :Regression Shrinkage and Selection via the Lasso, Journal of the Royal Statistical Society, Series B (Methodological), vol. 58, Issue 1 (1996), pp. 267-288., http://www.jstor.org/Sat Feb. 15 17:27:37 2003.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00045">
<othercit>Tropp et al., &#x201c;Algorithms for Simultaneous Sparse Approximation Part I: Greedy Pursuit&#x201d;, Sig. Proc., 86(3), pp. 572-588, 2006, Date: Typeset on Mar. 17, 2005.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00046">
<othercit>Tsochantaridis et al., &#x201c;Support Vector Machine Learning for Interdependent and Structured Output Spaces&#x201d;, Proceedings of the 21st International Conference on Machine Learning (ICML), pp. 104-112, 2004.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00047">
<othercit>Turlach et al., &#x201c;Simultaneous Variable Selection&#x201d;, School of Mathematics and Statistics (M019), University of Western Australia, CSIRO Mathematical and Information Sciences, Australia, Sep. 16, 2004.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00048">
<othercit>Wasserman et al., &#x201c;Social Network Analysis Methods and Applications&#x201d;, Cambridge Univ. Press, 1994.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00049">
<othercit>Yuan et al., &#x201c;Dimension reduction and coefficient estimation in multivariate linear regression&#x201d;, Journal of the Royal Statistical Society Series B, 60, Part 3, pp. 320-346, (2007).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00050">
<othercit>Yuan et al., &#x201c;Model selection and estimation in regression with grouped variables&#x201d;, Journal of the Royal Statistical Society Series B, 68, Part 1, pp. 49-67, (2006).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00051">
<othercit>Zhang, &#x201c;On the Consistency of Feature Selection using Greedy Least Squares Regression&#x201d;, Journal of Machine Learning Research 10 (2009) pp. 555-568, Mar. 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00052">
<othercit>Zhao et al., &#x201c;Grouped and Hierarchical Model Selection through Composite Absolute Penalties&#x201d;, Department of Statistics, Univ. of California Berkeley, Apr. 17, 2006, pp. 1-36.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00053">
<othercit>&#x201c;Advection&#x201d;, Wikipedia, http://en.wikipedia.org/wiki/Advection, Nov. 30, 2010, pp. 1-4.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00054">
<othercit>Arg max, http://en.wikipedia.org/wiki/Arg<sub>&#x2014;</sub>max, Nov. 22, 2010, pp. 1-2.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00055">
<othercit>&#x201c;Causality&#x201d;, http://en.wikipedia.org/wiki/Causality, Nov. 22, 2010, pp. 1-17.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00056">
<othercit>&#x201c;Datasets&#x201d;, http://www.cs.cornell.edu/PROJECTS/KDDCUP/DATASETS.HTML, Sep. 4, 2003, pp. 1-2.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00057">
<othercit>&#x201c;Divergence theorem&#x201d;, http://en.wikipedia.org/wiki/Divergence<sub>&#x2014;</sub>theorem, Nov. 22, 2010, pp. 1-5.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00058">
<othercit>&#x201c;Finite volume method&#x201d;, http://en.wikipedia.org/wiki/Finite<sub>&#x2014;</sub>volume<sub>&#x2014;</sub>method, Nov. 30, 2010, pp. 1-4.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00059">
<othercit>&#x201c;Granger causality&#x201d;, http://en.wikipedia.org/wiki/Granger<sub>&#x2014;</sub>causality, Nov. 30, 2010, pp. 1-2.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00060">
<othercit>&#x201c;Greedy algorithm&#x201d;, http://en.wikipedia.org/wiki/Greedy<sub>&#x2014;</sub>algorithm, Dec. 2, 2010, pp. 1-3.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00061">
<othercit>&#x201c;Lexical analysis&#x201d;, http://en.wikipedia.org/wiki/Lexical<sub>&#x2014;</sub>analysis, Dec. 2, 2010, pp. 1-5.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00062">
<othercit>&#x201c;Likelihood function&#x201d;, http://en.wikipedia.org/wiki/Likelihood<sub>&#x2014;</sub>function, Dec. 6, 2010, pp. 1-9.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00063">
<othercit>&#x201c;Linear least squares&#x201d;, http://en.wikipedia.org/wiki/Linear<sub>&#x2014;</sub>least<sub>&#x2014;</sub>squares, Dec. 2, 2010, pp. 1-13.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00064">
<othercit>&#x201c;Matrix norm&#x201d;, http://en.wikipedia.org/wiki/Matrix<sub>&#x2014;</sub>norm, Dec. 2, 2010, pp. 1-5.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00065">
<othercit>&#x201c;Moore-Penrose pseudoinverse&#x201d;, http://en.wikipedia.org/wiki/Moore-Penrose<sub>&#x2014;</sub>pseudoninverse, Dec. 2, 2010, pp. 1-8.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00066">
<othercit>&#x201c;Multivariate adaptive regression splines&#x201d;, http://en.wikipedia.org/wiki/Multivariate<sub>&#x2014;</sub>adaptive<sub>&#x2014;</sub>regression<sub>&#x2014;</sub>splines, Dec. 2, 2010, pp. 1-8.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00067">
<othercit>&#x201c;Multivariate probit&#x201d;, http://en.wikipedia.org/wiki/Multivariate<sub>&#x2014;</sub>probit, Dec. 2, 2010, p. 1.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00068">
<othercit>&#x201c;Non-negative matrix factorization&#x201d;, http://en.wikipedia.org/wiki/Non-negative<sub>&#x2014;</sub>matrix<sub>&#x2014;</sub>factorization, Dec. 2, 2010, pp. 1-6.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00069">
<othercit>Phenomenology (science), http://en.wikipedia.org/wiki/Phenomenology<sub>&#x2014;</sub>(science), Nov. 22, 2010, pp. 1-3.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00070">
<othercit>&#x201c;Regression analysis&#x201d;, http://en.wikipedia.org/wiki/Regression<sub>&#x2014;</sub>analysis, Dec. 2, 2010, pp. 1-9.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00071">
<othercit>Restriction (mathematics), http://en.wikipedia.org/wiki/Restrition<sub>&#x2014;</sub>(mathematics), Dec. 6, 2010, p. 1.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00072">
<othercit>&#x201c;Source-sink dynamics&#x201d;, http://en.wikipedia.org/wiki/Source-sink<sub>&#x2014;</sub>dynamics, Dec. 2, 2010, pp. 1-6.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00073">
<othercit>&#x201c;Sparse matrix&#x201d;, http://en.wikipedia.org/wiki/Sparse<sub>&#x2014;</sub>matrix, Dec. 6, 2010, pp. 1-5.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00074">
<othercit>&#x201c;Stemming&#x201d;, http://en.wikipedia.org/wiki/Stemming, Dec. 2, 2010, pp. 1-9.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00075">
<othercit>&#x201c;Stop words&#x201d;, http://en.wikipedia.org/wiki/Stop<sub>&#x2014;</sub>words, Dec. 2, 2010, p. 1.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00076">
<othercit>&#x201c;Taxicab geometry&#x201d;, http://en.wikipedia.org/wiki/Taxicab<sub>&#x2014;</sub>geometry, Dec. 2, 2010, pp. 1-3.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00077">
<othercit>Trace (linear algebra), http://en.wikipedia.org/wiki/Trace<sub>&#x2014;</sub>(linear<sub>&#x2014;</sub>algebra), Dec. 6, 2010, pp. 1-8.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00078">
<othercit>&#x201c;Vector autoregression&#x201d;, http://en.wikipedia.org/wiki/Vector<sub>&#x2014;</sub>autoregression, Dec. 2, 2010, pp. 1-6.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00079">
<othercit>&#x201c;Web 2.0&#x201d;, http://en.wikipedia.org/wiki/Web<sub>&#x2014;</sub>2.0, Dec. 2, 2010, pp. 1-10.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00080">
<othercit>&#x201c;Web crawler&#x201d;, http://en.wikipedia.org/wiki/Web<sub>&#x2014;</sub>crawler, Dec. 2, 2010, pp. 1-12.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>25</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>706 12</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>4</number-of-drawing-sheets>
<number-of-figures>9</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120143796</doc-number>
<kind>A1</kind>
<date>20120607</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Lozano</last-name>
<first-name>Aurelie C.</first-name>
<address>
<city>Scarsdale</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Sindhwani</last-name>
<first-name>Vikas</first-name>
<address>
<city>Hawthorne</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Lozano</last-name>
<first-name>Aurelie C.</first-name>
<address>
<city>Scarsdale</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Sindhwani</last-name>
<first-name>Vikas</first-name>
<address>
<city>Hawthorne</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Scully, Scott, Murphy &#x26; Presser, P.C.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
<agent sequence="02" rep-type="attorney">
<addressbook>
<last-name>Morris, Esq.</last-name>
<first-name>Daniel P.</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>International Business Machines Corporation</orgname>
<role>02</role>
<address>
<city>Armonk</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Chaki</last-name>
<first-name>Kakali</first-name>
<department>2122</department>
</primary-examiner>
<assistant-examiner>
<last-name>Seck</last-name>
<first-name>Ababacar</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">In response to issues of high dimensionality and sparsity in machine learning, it is proposed to use a multiple output regression modeling module that takes into account information on groups of related predictor features and groups of related regressions, both given as input, and outputs a regression model with selected feature groups. Optionally, the method can be employed as a component in methods of causal influence detection, which are applied on a time series training data set representing the time-evolving content generated by community members, output a model of causal relationships and a ranking of the members according to their influence.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="146.98mm" wi="176.28mm" file="US08626680-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="238.51mm" wi="189.40mm" file="US08626680-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="222.93mm" wi="191.01mm" file="US08626680-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="173.06mm" wi="139.36mm" orientation="landscape" file="US08626680-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="231.65mm" wi="161.97mm" file="US08626680-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">RELATED APPLICATION</heading>
<p id="p-0002" num="0001">The following application is filed concurrently with U.S. patent application Ser. No. 12/960,160; &#x201c;INFERRING INFLUENCE AND AUTHORITY,&#x201d; Inventors A. C. Lozano &#x26; V. Sindhwani</p>
<heading id="h-0002" level="1">BACKGROUND</heading>
<p id="p-0003" num="0002">The present disclosure generally relates to the use of regression in machine learning and data mining, and particularly to &#x201c;variable selection&#x201d; and in which constructs are based upon a training data set.</p>
<p id="p-0004" num="0003">The broad goal of supervised learning is to effectively learn unknown functional dependencies between a set of input variables and a set of output variables, given a collection of training examples. The present application recognizes a potential synergism between two topics that arise in this context. The mention of these two topics here is made for the purpose of US law relating to information disclosure and does not constitute an admission that combination of any documents listed herein was known or obvious prior to such recognition by Applicants.</p>
<p id="p-0005" num="0004">The first topic is Multivariate Regression Ildiko E. Frank and Jerome H. Friedman, &#x201c;A statistical view of some chemometrics regression tools,&#x201d; Technometrics, 35(2):109-135, 1993; Leo Breiman and Jerome H Friedman, &#x201c;Predicting multivariate responses in multiple linear regression,&#x201d;Journal of the Royal Statistical Society: Series B, (1):1369-7412, 1997. Ming Yuan and Ali Ekici and Zhaosong Lu and Renato Monteiro, &#x201c;Dimension reduction and coefficient estimation in multivariate linear regression&#x201d;, Journal Of The Royal Statistical Society Series B, 2007 which generalizes basic single-output regression to settings involving multiple output variables with potentially significant correlations between them. Applications of multivariate regression models include chemometrics, econometrics and computational biology.</p>
<p id="p-0006" num="0005">Multivariate Regression may be viewed as a basis for many techniques in machine learning such as multi-task learning Charles A. Micchelli and Massimiliano Pontil, &#x201c;Kernels for multi-task learning,&#x201d; NIPS, 2004; Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil, &#x201c;Convex multi-task feature learning,&#x201d; Machine Learning, 73(3):243-272, 2008 and structured output prediction Elisa Ricci, Tijl De Bie, and Nello Cristianini, &#x201c;Magic moments for structured output prediction, &#x201c;Journal of Machine Learning Research, 9:2803-2846, December 2008; T. Joachims, &#x201c;Structured output prediction with support vector machines.&#x201d; Joint IAPR International Workshops on Structural and Syntactic Pattern Recognition (SSPR) and Statistical Techniques in Pattern Recognition (SPR), pages 1-7, 2006; I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. &#x201c;Support vector machine learning for interdependent and structured output spaces,&#x201d; International Conference on Machine Learning (ICML), pages 104-112, 2004.</p>
<p id="p-0007" num="0006">These techniques are output-centric in the sense that they attempt to exploit dependencies between output variables to jointly learn models that generalize better than those learned by treating outputs independently.</p>
<p id="p-0008" num="0007">The second topic includes topics such as sparsity, variable selection and the broader notion of regularization. The view here is input-centric in the following specific sense. In very high dimensional problems, where the number of input variables may exceed the number of examples, the only hope for avoiding overfitting is via some form of &#x201c;capacity control&#x201d; over the family of dependencies being explored by the learning algorithm. This capacity control may be implemented in various ways, e.g., via dimensionality reduction, input variable selection or regularized risk minimization. Estimation of sparse models that are supported on a small set of input variables is a strand of research in machine learning. It encompasses 1-1 regularization (e.g., the lasso algorithm of R. Tibshirani, &#x201c;Regression shrinkage and selection via the lasso,&#x201d; Journal of the Royal Statistical Society, Series B, 58:267-288, 1994 and matching pursuit techniques; S. Mallat and Z. Zhang, &#x201c;Matching pursuits with time-frequency dictionaries,&#x201d; IEEE Transactions on Signal Processing, 1993 that come with theoretical guarantees on the recovery of the exact support under certain conditions. Particularly pertinent to this invention is the notion of structured sparsity. In many problems involving very high-dimensional datasets, the prior knowledge that the support of the model should be a union over domain-specific groups of features is enforced. Several methods have been recently proposed for this setting. For instance, M. Yuan and Y. Lin, &#x201c;Model selection and estimation in regression with grouped variables,&#x201d; Journal of the Royal Statistical Society, Series B, 68:49-67, 2006 P. Zhao and G. Rocha and B. Yu, &#x201c;Grouped and hierarchical model selection through composite absolute penalties&#x201d; Technical report, 2006 extend the Lasso formulation to this context, while the methods of A. C. Lozano, G. Swirszcz, and N. Abe, &#x201c;Grouped orthogonal matching pursuit for variable selection and prediction,&#x201d; Advances in Neural Information Processing Systems 22, 2009; J. Huang, T. Zhang, and D. Metaxas, &#x201c;Learning with structured sparsity,&#x201d; Proceedings of the 26th Annual International Conference on Machine Learning, 2009 extend matching pursuit techniques.</p>
<p id="p-0009" num="0008">The present disclosure treats very high dimensional problems involving a large number of output variables. It is desirable to address sparsity via input variable selection in multivariate linear models with regularization, since the number of parameters grows not only with the data dimensionality but also the number of outputs.</p>
<heading id="h-0003" level="1">SUMMARY</heading>
<p id="p-0010" num="0009">The approach herein is guided by the following desiderata: (a) performing variable selection for each output in isolation may be highly suboptimal since the input variables which are relevant to (a subset of) the outputs may only exhibit weak correlation with each individual output. It is also desirable to leverage information on the relatedness between outputs, so as to guide the decision on the relevance of a certain input variable to a certain output, using additional evidence based on the relevance to related outputs; (b) It is desirable to take into account any grouping structure that may exist between input and output variables. Also, in the presence of noisy data, inclusion decisions made at the group level may be more robust than those at the individual variable level.</p>
<p id="p-0011" num="0010">In one embodiment, a computer method includes carrying out operations on at least one data processing device, the operations including:
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0011">maintaining a machine readable embodiment on a medium of data, the data being organized into at least one matrix data structure having a plurality of dimensions;</li>
        <li id="ul0002-0002" num="0012">imposing a group structure on the data, which group structure defines input and output blocks, at least one of the input and output blocks encompassing at least first and second data points in each of at least first and second dimensions;</li>
        <li id="ul0002-0003" num="0013">performing multivariate regression in accordance with a coefficient matrix that is organized into coefficient blocks, the coefficient blocks linking the input and output blocks, the multivariate regression comprising, iteratively
        <ul id="ul0003" list-style="none">
            <li id="ul0003-0001" num="0014">refining the coefficient blocks and</li>
            <li id="ul0003-0002" num="0015">selecting pertinent ones of the input and output blocks, responsive to results of such iteration;</li>
        </ul>
        </li>
        <li id="ul0002-0004" num="0016">presenting at least one user accessible machine representation of at least one conclusion about at least one relationship between the pertinent ones of the input and output blocks, responsive to the coefficient matrix.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0012" num="0017">In another embodiment, a system includes:
<ul id="ul0004" list-style="none">
    <li id="ul0004-0001" num="0000">
    <ul id="ul0005" list-style="none">
        <li id="ul0005-0001" num="0018">at least one storage device for embodying data and/or program code in a machine usable form;</li>
        <li id="ul0005-0002" num="0019">at least one processor for performing operations in conjunction with the storage device, the operations being in accordance with the above method.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0013" num="0020">In another embodiment, a computer program product for identifying influence includes a storage medium readable by a processing circuit and storing instructions to be run by the processing circuit for performing the above method.</p>
<p id="p-0014" num="0021">Objects and advantages will become apparent throughout the present disclosure and claims.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0015" num="0022">Embodiments will now be described by way of non-limiting example with reference to the following figures:</p>
<p id="p-0016" num="0023"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram of a system in which the invention may be implemented.</p>
<p id="p-0017" num="0024"><figref idref="DRAWINGS">FIG. 2</figref> depicts the block structure defined by the specification of input variable groups and output variable groups, namely the regression groups.</p>
<p id="p-0018" num="0025"><figref idref="DRAWINGS">FIG. 3</figref> is a conceptual diagram giving an intuitive illustration of block selection in a simplified, two dimensional example.</p>
<p id="p-0019" num="0026"><figref idref="DRAWINGS">FIG. 4</figref> is a conceptual diagram giving an intuitive illustration of selection of a first block for a solution matrix A in multivariate regression in a simplified, two dimensional example.</p>
<p id="p-0020" num="0027"><figref idref="DRAWINGS">FIG. 5</figref> is a conceptual diagram giving an intuitive illustration of selection of a second block for a solution matrix A in multivariate regression in a simplified, two dimensional example.</p>
<p id="p-0021" num="0028">In general, a data structure for use in group selection should meet certain requirements. In particular, the structure should allow for data having a given number of instances. For example, each instance might correspond to a different observation, e.g., a different point in time. Moreover, the data should be composed of a certain number of features to allow definition of a grouping structure over them.</p>
<p id="p-0022" num="0029"><figref idref="DRAWINGS">FIG. 6</figref> is an algorithmic style presentation of a sub-procedure relating to identification of solution matrix A.</p>
<p id="p-0023" num="0030"><figref idref="DRAWINGS">FIG. 7</figref> is a matrix visualization showing a simplified example of a sparse matrix A including the blocks identified in <figref idref="DRAWINGS">FIGS. 5 and 6</figref>.</p>
<p id="p-0024" num="0031"><figref idref="DRAWINGS">FIG. 8</figref> shows rearranging blocks to make them contiguous.</p>
<p id="p-0025" num="0032"><figref idref="DRAWINGS">FIG. 9</figref> shows more examples of variable groupings.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0026" num="0033">As will be appreciated by one skilled in the art, aspects of the present invention may be embodied as a system, method or computer program product. Accordingly, aspects of the present invention may take the foam of an entirely hardware embodiment, an entirely software embodiment (including firmware, resident software, micro-code, etc.) or an embodiment combining software and hardware aspects that may all generally be referred to herein as a &#x201c;circuit,&#x201d; &#x201c;module&#x201d; or &#x201c;system.&#x201d; Furthermore, aspects of the present invention may take the form of a computer program product embodied in one or more computer readable medium(s) having computer readable program code embodied thereon.</p>
<p id="p-0027" num="0034">Any combination of one or more computer readable media may be utilized. The computer readable medium may be a computer readable signal medium or a computer readable storage medium. A computer readable storage medium may be, for example, but not limited to, an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system, apparatus, or device, or any suitable combination of the foregoing. More specific examples (a non-exhaustive list) of the computer readable storage medium would include the following: an electrical connection having one or more wires, a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory), an optical fiber, a portable compact disc read-only memory (CD-ROM), an optical storage device, a magnetic storage device, or any suitable combination of the foregoing. In the context of this document, a computer readable storage medium may be any tangible medium that can contain, or store a program for use by or in connection with an instruction execution system, apparatus, or device.</p>
<p id="p-0028" num="0035">A computer readable signal medium may include a propagated data signal with computer readable program code embodied therein, for example, in baseband or as part of a carrier wave. Such a propagated signal may take any of a variety of forms, including, but not limited to, electro-magnetic, optical, or any suitable combination thereof. A computer readable signal medium may be any computer readable medium that is not a computer readable storage medium and that can communicate, propagate, or transport a program for use by or in connection with an instruction execution system, apparatus, or device.</p>
<p id="p-0029" num="0036">Program code embodied on a computer readable medium may be transmitted using any appropriate medium, including but not limited to wireless, wireline, optical fiber cable, RF, etc., or any suitable combination of the foregoing.</p>
<p id="p-0030" num="0037">Computer program code for carrying out operations for aspects of the present invention may be written in any combination of one or more programming languages, including an object oriented programming language such as Java, Smalltalk, C++ or the like and conventional procedural programming languages, such as the &#x201c;C&#x201d; programming language or similar programming languages. The program code may execute entirely on the user's computer, partly on the user's computer, as a stand-alone software package, partly on the user's computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario, the remote computer may be connected to the user's computer through any type of network, including a local area network (LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider).</p>
<p id="p-0031" num="0038">Aspects of the present invention are described below with reference to flowchart illustrations and/or block diagrams of methods, apparatus (systems) and computer program products according to embodiments of the invention. It will be understood that each block of the flowchart illustrations and/or block diagrams, and combinations of blocks in the flowchart illustrations and/or block diagrams, can be implemented by computer program instructions. These computer program instructions may be provided to a processor of a general purpose computer, special purpose computer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute via the processor of the computer or other programmable data processing apparatus, create means for implementing the functions/acts specified in the flowchart and/or block diagram block or blocks. These computer program instructions may also be stored in a computer readable medium that can direct a computer, other programmable data processing apparatus, or other devices to function in a particular manner, such that the instructions stored in the computer readable medium produce an article of manufacture including instructions which implement the function/act specified in the flowchart and/or block diagram block or blocks.</p>
<p id="p-0032" num="0039">The computer program instructions may also be loaded onto a computer, other programmable data processing apparatus, or other devices to cause a series of operational steps to be performed on the computer, other programmable apparatus or other devices to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide processes for implementing the functions/acts specified in the flowchart and/or block diagram block or blocks.</p>
<p id="p-0033" num="0040">Herein, the terms &#x201c;features&#x201d; and &#x201c;input variables&#x201d; are synonymous and are interchangeable. Similarly, the terminology &#x201c;multivariate regression,&#x201d; &#x201c;multiple output regression,&#x201d; and &#x201c;multiple response regression&#x201d; are synonymous and may be used interchangeably. Also, the terms &#x201c;output variable groups&#x201d;, &#x201c;target groups&#x201d;, and &#x201c;regression groups&#x201d; mean the same and are interchangeable.</p>
<p id="p-0034" num="0041">The term &#x201c;spatiotemporal&#x201d; is used to refer to a set of data that can be organized into a plurality of dimensions of a space, according to features of the data, and which varies over time.</p>
<p id="p-0035" num="0042">The term &#x201c;causal&#x201d; herein means something less than &#x201c;cause.&#x201d; Something may be &#x201c;causal&#x201d; if it is suspected of being a cause or behaves like a cause, even if it is not in fact the cause.</p>
<p id="p-0036" num="0043">The term &#x201c;originating community member&#x201d; will be used herein to mean an entity associated with a set of content. For instance, a blogger or author can be an originating community member with respect to words or an element of a phenomenological system can be an originating community member with respect to gathered data. An originating community member can be considered causal with respect to another community member. When the originating community member is a causal person, that person can be regarded as an &#x201c;authority&#x201d;.</p>
<p id="p-0037" num="0044">The term &#x201c;simultaneous&#x201d; as used herein need not mean literally simultaneously. It means that a group of regressions are modeled together, using the same set of input variable groups as predictors for the group of regressions, and where the decision of whether to include or exclude input variable groups in the aforementioned set is made jointly for the whole regression group.</p>
<p id="p-0038" num="0045">The term &#x201c;jointly&#x201d; as used herein means that the influence of all the bloggers on each other is assessed based on a joint consideration of all bloggers into a joint model, rather than performing isolated pair-wise tests between pairs of bloggers disregarding the presence and impact of other bloggers.</p>
<p id="p-0039" num="0046">Methods and systems of variable selection in multivariate regression are set forth herein. These are particularly relevant for predictive and causal influence modeling applied to a chosen data set, e.g. in the area of Internet blogs or academic publications. Other fields of application include phenomenological systems, such as biological systems, where genes, population features, or ecosystem features can be viewed as authorities governing behavior of aspects of the system. The methods are given information on the grouping structure between input variables and on the grouping structure between the regressions, and use this information to guide the variable selection by imposing a sparsity structure on the estimated regression coefficients reflecting the grouping information. The method is also able to make use of additional information provided by an estimate of the error covariance matrix to further improve modeling accuracy.</p>
<p id="p-0040" num="0047">The method may comprise a) &#x201c;forward iterative variable block selection,&#x201d; namely iteratively selecting one of the input variable groups of interest and one of the regression groups of interest in each iteration based on by how much their introduction into the current model can reduce the residual loss; and b) in subsequent iterations making the variable block selection with respect to the reduction in residual loss of the estimated model when the input variable group and corresponding regression group in question are included into the model in addition to those groups that have been selected in earlier iterations; c) in which an arbitrary component regression method is allowed for performing the regression in each iteration.</p>
<p id="p-0041" num="0048">In another aspect, the method may additionally include &#x201c;iterative re-estimation&#x201d; of the estimated coefficients of the current model, i.e., upon selection of the variable groups in each iteration the model coefficients are re-estimated, which may be performed, for example, using the least squares method.</p>
<p id="p-0042" num="0049">A program storage device readable by a machine, tangibly embodying a program of instructions executable by the machine to perform above described methods may be also provided.</p>
<p id="p-0043" num="0050">Further features as well as the structure and operation of various embodiments are described in detail below with reference to the accompanying drawings. In the drawings, like reference numbers indicate identical or functionally similar elements.</p>
<p id="p-0044" num="0051"><figref idref="DRAWINGS">FIG. 1</figref> shows a system in which the invention may be implemented. The system includes a user and or network interface <b>101</b>, a processor <b>102</b>, and a storage device <b>103</b>. Elements depicted in this figure and elements depicted as distinct might be merged. The interface <b>101</b> inputs and outputs data and may be of any suitable type including wired and wireless connections, graphical user interfaces, keyboards, pointer devices, printers, etc. The processor <b>102</b> again may be of any suitable type including single and parallel processors and of diverse capabilities. The storage device <b>103</b> might be of any suitable type including magnetic, optical, solid state, read only, and read/write and may provide a physical embodiment of data and/or computer program code. Data on the storage device will include a set of observations on several features. Features might be words of a blog or, in phenomenological systems they might represent other measured quantities such as temperature or pressure are 2 features.</p>
<p id="p-0045" num="0052">The present disclosure addresses the problem of variable selection for multivariate regression, where a natural grouping structure exists within the explanatory variables, a natural grouping structure also exists within the regressions, and it is desired to select the correct set of variable groups, rather than the individual variables. In this respect, the definitions of &#x201c;simultaneous&#x201d; and &#x201c;joint,&#x201d; from the co-pending disclosure incorporated by reference above, will be pertinent.</p>
<p id="p-0046" num="0053">The approach described herein will most likely be useful to very high dimensional problems involving a large number of output variables. Sparsity may be addressed via input variable selection in multivariate linear models. In such models regularization is desirable, since the number of parameters grows not only with the data dimensionality but also the number of outputs.</p>
<p id="p-0047" num="0054">The present disclosure presents a greedy style algorithm in the category of matching pursuit procedures, which we call Multivariate Group Orthogonal Matching Pursuit} (MG-OMP), for enforcing arbitrary block sparsity patterns in multivariate regression coefficients. These patterns are specified by groups defined over both input and output variables.</p>
<p id="p-0048" num="0055">In particular, MG-OMP can handle cases where the set of relevant features may differ from one response (group) to another, and is thus more general than simultaneous variable selection procedures (e.g. S-OMP of J. A. Tropp, A. C. Gilbert, and M. J. Strauss, &#x201c;Algorithms for simultaneous sparse approximation: part i: Greedy pursuit,&#x201d; Sig. Proc., 86(3):572-588, 2006 as simultaneity of the selection is enforced within groups of related output variables rather than for the entire output. MG-OMP is more general than the Group-OMP (G-OMP) algorithm of A. C. Lozano, G. Swirszcz, and N. Abe, &#x201c;Grouped orthogonal matching pursuit for variable selection and prediction,&#x201d; Advances in Neural Information Processing Systems 22, 2009 and applies to the case of multivariate regression.</p>
<p id="p-0049" num="0056"><figref idref="DRAWINGS">FIG. 2</figref> shows a functional diagram of MG-OMP. More about the meaning of individual blocks in this diagram will be discussed below with reference to a more specific algorithmic presentation. While functions listed in this figure are presented in terms of modules, in fact the functions need not be so confined. Functions may be spread across more or less modules in accordance with the design parameters of the skilled artisan. Illustrated in <figref idref="DRAWINGS">FIG. 2</figref> are: data storage module <b>6</b> (<b>201</b>); variable block selection top control module <b>1</b> (<b>202</b>); regression with variable block selection module <b>2</b> (<b>203</b>); current residuals and selected input-output groups update module <b>4</b> (<b>204</b>); current residual and selected input-output groups storage module <b>5</b> (<b>205</b>); and model output module <b>3</b> (<b>206</b>).</p>
<p id="p-0050" num="0057">The following is the general form of the multivariate regression model,
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>Y=X&#x100;+E, </i><?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where Y&#x3b5;R<sup>n&#xd7;K </sup>is the output matrix fanned by n training examples on K output variables, X&#x3b5;R<sup>n&#xd7;p </sup>is the data matrix whose rows are p-dimensional feature vectors for the n training examples, &#x100; is the p&#xd7;K matrix formed by the true regression coefficients, and E is the n&#xd7;K error matrix. The row vectors of the error matrix E are assumed to be independently sampled from a multivariate Gaussian distribution N(0, &#x3a3;) where &#x3a3; is the K&#xd7;K error covariance matrix. For simplicity of notation we assume without loss of generality that the columns of X and Y have been centered so it is not necessary to deal with intercept terms.
</p>
<p id="p-0051" num="0058">Centered means that the sample mean has been subtracted out of X. i.e.</p>
<p id="p-0052" num="0059">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mrow>
  <msub>
    <mi>X</mi>
    <mi>ij</mi>
  </msub>
  <mo>&#x2190;</mo>
  <mfrac>
    <mrow>
      <msub>
        <mi>X</mi>
        <mi>ij</mi>
      </msub>
      <mo>-</mo>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mfrac>
            <mn>1</mn>
            <mi>n</mi>
          </mfrac>
          <mo>&#x2062;</mo>
          <mrow>
            <munderover>
              <mo>&#x2211;</mo>
              <mrow>
                <mi>k</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mi>n</mi>
            </munderover>
            <mo>&#x2062;</mo>
            <msub>
              <mi>X</mi>
              <mi>kj</mi>
            </msub>
          </mrow>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mrow>
    <msup>
      <mrow>
        <mo>(</mo>
        <mrow>
          <munderover>
            <mo>&#x2211;</mo>
            <mrow>
              <mi>l</mi>
              <mo>=</mo>
              <mn>1</mn>
            </mrow>
            <mi>n</mi>
          </munderover>
          <mo>&#x2062;</mo>
          <msup>
            <mrow>
              <mo>(</mo>
              <mrow>
                <msub>
                  <mi>X</mi>
                  <mi>ij</mi>
                </msub>
                <mo>-</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mfrac>
                      <mn>1</mn>
                      <mi>n</mi>
                    </mfrac>
                    <mo>&#x2062;</mo>
                    <mrow>
                      <munderover>
                        <mo>&#x2211;</mo>
                        <mrow>
                          <mi>k</mi>
                          <mo>=</mo>
                          <mn>1</mn>
                        </mrow>
                        <mi>n</mi>
                      </munderover>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>X</mi>
                        <mi>kj</mi>
                      </msub>
                    </mrow>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mo>)</mo>
            </mrow>
            <mn>2</mn>
          </msup>
        </mrow>
        <mo>)</mo>
      </mrow>
      <mrow>
        <mn>1</mn>
        <mo>/</mo>
        <mn>2</mn>
      </mrow>
    </msup>
  </mfrac>
</mrow>
</math>
</maths>
<br/>
If X and Y are centered we model Y=XBeta. Otherwise, intercept tetras are added to the model, namely consider an additional vector Beta<b>0</b>, such that Y=Beta<b>0</b>+XBeta
</p>
<p id="p-0053" num="0060">The negative log-likelihood function (up to a constant) corresponding to the aforementioned model can be expressed as
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>&#x2212;<i>l</i>(<i>A</i>,&#x3a3;)=<i>tr</i>((<i>Y&#x2212;XA</i>)<sup>T</sup>(<i>Y&#x2212;XA</i>)&#x3a3;<sup>&#x2212;1</sup>)&#x2212;<i>n </i>log|&#x3a3;<sup>&#x2212;1</sup>|<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where, A is any estimate of the true coefficient matrix &#x100; |&#xb7;| denotes the determinant of a matrix and tr(&#xb7;) denotes the trace function.
</p>
<p id="p-0054" num="0061">The maximum likelihood estimator is a matrix A that minimized the negative likelihood expression above. The maximizer of the likelihood is also the minimizer of the negative log likelihood. One possible maximum likelihood estimator of the coefficient matrix is the Ordinary Least Squares (OLS) estimator &#xc2;<sup>OLS</sup>=(X<sup>T</sup>X)<sup>&#x2212;1</sup>X<sup>T</sup>Y, which is equivalent to the concatenation of the OLS estimates for each of the K outputs taken separately, and thus does not depend on the covariance matrix &#x3a3;. This suggests that the OLS estimator may be suboptimal since it does not make use of the relatedness of the responses. In addition, it is well known that the OLS estimator performs poorly in the case of high dimensional predictor vectors and/or when the predictors are highly correlated. To alleviate these issues, it is desirable to reduce the number of parameters to be estimated. This can be accomplished via variable selection. Clearly, however, variable selection in multiple output regression is particularly challenging in the presence of high dimensional feature vectors as well as possibly a large number of responses.</p>
<p id="p-0055" num="0062">In many applications, including high-dimensional time series analysis and causal modeling settings discussed below, it is possible to provide domain specific guidance for variable selection by imposing a sparsity structure on A. Let I={I<sub>1</sub>, . . . I<sub>L</sub>} denote the set formed by L (possibly overlapping) groups of input variables where I<sub>k</sub>&#x2282;{1, . . . p}, k=1, . . . , L. Let O={O<sub>1</sub>, . . . O<sub>M</sub>} denote the set formed by M (possibly overlapping) groups of output variables where O<sub>k</sub>&#x2282;{1, . . . K}, k=1, . . . , M. If certain variables do not belong to any group, they may be treated as groups of size 1. These group definitions specify a &#x201c;block sparsity/support pattern&#x201d; on A. Such blocks are illustrated in <figref idref="DRAWINGS">FIGS. 4 and 5</figref> for the case of non-overlapping groups and where. Column indices may be permuted so that groups go over contiguous indices. An example of this is shown in <figref idref="DRAWINGS">FIG. 8</figref> where blocks denoted l<sub>1</sub>, l<sub>2</sub>, l<sub>3</sub>, and l<sub>4 </sub>are rearranged to be contiguous.</p>
<p id="p-0056" num="0063">A novel algorithm will now be outlined, &#x201c;Multivariate Group Orthogonal Matching Pursuit&#x201d;, that attempts to minimize the negative log-likelihood associated with the multivariate regression model subject to the constraint that the support (set of non-zeros) of the regression coefficient matrix, A, is a union of blocks formed by input and output variable groupings. The algorithm can also deal with the more general case where there may be a different grouping structure for each output group, namely for each O<sub>k</sub>, there could be a different set I<sub>Ok</sub>, of input variable groups.</p>
<p id="p-0057" num="0064">A more general view of variable grouping can be considered. One could envision that the output groups are individual bloggers, while the input groups could be defined over &#x201c;cliques&#x201d; of bloggers, and the clique configurations could be different depending on the output group. For instance, blogger B<b>1</b> could be influenced by himself or bloggers B<b>2</b>&#x26;B<b>3</b> or bloggers B<b>4</b>&#x26;B<b>5</b>, while blogger B<b>2</b> could be influenced by bloggers B<b>1</b>&#x26;B<b>4</b> or bloggers B<b>3</b>&#x26;B<b>5</b> or himself . . . Etc.</p>
<p id="p-0058" num="0065">In <figref idref="DRAWINGS">FIG. 4</figref> &#x26; <figref idref="DRAWINGS">FIG. 9</figref>, the output groups are defined over columns (separated by vertical lines). The input groups are defined over rows. In <figref idref="DRAWINGS">FIG. 4</figref> the vertical tiling is the same across the entire matrix (reflecting the fact that the input groups are the same across all the output groups). In contrast, in <figref idref="DRAWINGS">FIG. 9</figref>, the vertical tiling depends on the output groups, so the composition for the input groups might differ from one output group to the next</p>
<p id="p-0059" num="0066">The Multivariate Group Orthogonal Matching Pursuit (MG-OMP) procedure performs greedy pursuit with respect to the loss function
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>L</i><sub>C</sub>(<i>A</i>,&#x3a3;)=<i>tr</i>((<i>Y&#x2212;XA</i>)<sup>T</sup>(<i>Y&#x2212;XA</i>)<i>C</i>)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where C is an estimate of the precision matrix &#x3a3;<sup>&#x2212;1 </sup>given as input. Possible estimates include the sample estimate using residual error obtained from running univariate Group-OMP for each response individually. In addition to leveraging the grouping information via block sparsity constraints, the Multivariate Group Orthogonal Matching Pursuit procedure is able to incorporate additional information on the relatedness of the output variables which is implicitly encoded in the error covariance matrix, noting that the latter is also the covariance matrix of the response Y conditioned on the predictor matrix X. Existing variable selection methods often ignore this information and deal instead with (regularized versions of) the simplified objective tr((Y&#x2212;XA)<sup>T</sup>(Y&#x2212;XA)) thereby implicitly assuming that &#x3a3; is the identity matrix.
</p>
<p id="p-0060" num="0067">Some additional notation will now be set forth. For any set of output variables O&#x2282;{1, . . . K}, C<sub>O </sub>will denote the restriction of the K&#xd7;K precision matrix C to columns corresponding to the output variables in O, while C<sub>O,O </sub>denotes similar restriction to both columns and rows. For any set of input variables I&#x2282;{1, . . . p}, X<sub>I </sub>will denote the restriction of X to columns corresponding to the input variables in I. Furthermore, to simplify the exposition, it will be assumed in the remainder that, for each group of input variables I<sub>s </sub>in I, X<sub>Is </sub>is orthonormalized, i.e., X<sub>I</sub><sub><sub2>s</sub2></sub><sup>T</sup>X<sub>I</sub><sub><sub2>s</sub2></sub>=Identity. In the mth iteration of the procedure, A<sup>(m&#x2212;1) </sup>will denote the latest estimate of the regression coefficient matrix and R<sup>(m&#x2212;1) </sup>the corresponding matrix of residuals, i.e., R<sup>(m&#x2212;1)</sup>=Y&#x2212;XA<sup>(m&#x2212;1)</sup>.</p>
<p id="p-0061" num="0068">The MG-OMP algorithm is shown schematically in <figref idref="DRAWINGS">FIG. 2</figref> and textually below. <figref idref="DRAWINGS">FIGS. 3-5</figref> also illustrate points with respect to the algorithm below</p>
<p id="p-0062" num="0069">INPUT (<b>201</b>)
<ul id="ul0006" list-style="none">
    <li id="ul0006-0001" num="0000">
    <ul id="ul0007" list-style="none">
        <li id="ul0007-0001" num="0070">The data X&#x3b5;<img id="CUSTOM-CHARACTER-00001" he="3.13mm" wi="3.13mm" file="US08626680-20140107-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/><sup>n&#xd7;p </sup>(<b>301</b>)</li>
        <li id="ul0007-0002" num="0071">The response matrix Y&#x3b5;<img id="CUSTOM-CHARACTER-00002" he="3.13mm" wi="3.13mm" file="US08626680-20140107-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/><sup>K&#xd7;n</sup>. (<b>302</b>)</li>
        <li id="ul0007-0003" num="0072">The input and output group structures <img id="CUSTOM-CHARACTER-00003" he="3.13mm" wi="2.46mm" file="US08626680-20140107-P00002.TIF" alt="custom character" img-content="character" img-format="tif"/>={I<sub>r</sub>, r=1, . . . , L} and <img id="CUSTOM-CHARACTER-00004" he="3.13mm" wi="2.79mm" file="US08626680-20140107-P00003.TIF" alt="custom character" img-content="character" img-format="tif"/>={O<sub>s</sub>, s=1, . . . , M}. (<b>303</b>, <b>304</b>, respectively)</li>
        <li id="ul0007-0004" num="0073">An estimate of the K&#xd7;K error precision matrix C={circumflex over (&#x3a3;)}<sup>&#x2212;1 </sup></li>
        <li id="ul0007-0005" num="0074">Precision &#x3b5;&#x3e;0 for the stopping criterion</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0063" num="0075">INITIALIZATION (<b>202</b>, VARIABLE BLOCK SELECTION)
<ul id="ul0008" list-style="none">
    <li id="ul0008-0001" num="0000">
    <ul id="ul0009" list-style="none">
        <li id="ul0009-0001" num="0076">Initialization: <img id="CUSTOM-CHARACTER-00005" he="3.13mm" wi="3.89mm" file="US08626680-20140107-P00004.TIF" alt="custom character" img-content="character" img-format="tif"/><sup>(0)</sup>=&#x2205;, A<sup>(0)</sup>=0.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0064" num="0077">FOR m=1
<ul id="ul0010" list-style="none">
    <li id="ul0010-0001" num="0000">
    <ul id="ul0011" list-style="none">
        <li id="ul0011-0001" num="0078">Compute current residual error matrix
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>R<sup>(0)</sup>=Y<?in-line-formulae description="In-line Formulae" end="tail"?>
</li>
        <li id="ul0011-0002" num="0079">Pick the block that helps most in term of reducing the residual error (<b>203</b>, Regression with variable block selection; <b>401</b> of <figref idref="DRAWINGS">FIG. 4</figref>)
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>Let (<i>r</i><sup>(m)</sup><i>,s</i><sup>(m)</sup>)=arg max<sub>r,s</sub><i>tr</i>((<i>X</i><sub>I</sub><sub><sub2>r</sub2></sub><sup>T</sup><img id="CUSTOM-CHARACTER-00006" he="3.13mm" wi="3.13mm" file="US08626680-20140107-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/><sup>(m&#x2212;1)</sup><i>C</i><sub>O</sub><sub><sub2>s</sub2></sub>)<sup>T</sup>(<i>X</i><sub>I</sub><sub><sub2>r</sub2></sub><sup>T</sup><img id="CUSTOM-CHARACTER-00007" he="3.13mm" wi="3.13mm" file="US08626680-20140107-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/><sup>(m&#x2212;1)</sup><i>C</i><sub>O</sub><sub><sub2>s</sub2></sub>)(<i>C</i><sub>O</sub><sub><sub2>s</sub2></sub><sub>,O</sub><sub><sub2>s</sub2></sub><sup>&#x2212;1</sup>)).<?in-line-formulae description="In-line Formulae" end="tail"?>
</li>
        <li id="ul0011-0003" num="0080">If the benefit gained by incorporating this block is too small, stop
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>If <i>tr</i>((<i>X</i><sub>I</sub><sub><sub2>r</sub2></sub><sup>T</sup><img id="CUSTOM-CHARACTER-00008" he="3.13mm" wi="3.13mm" file="US08626680-20140107-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/><sup>(m&#x2212;1)</sup><i>C</i><sub>O</sub><sub><sub2>s</sub2></sub>)<sup>T</sup>(<i>X</i><sub>I</sub><sub><sub2>r</sub2></sub><sup>T</sup><img id="CUSTOM-CHARACTER-00009" he="3.13mm" wi="3.13mm" file="US08626680-20140107-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/><sup>(m&#x2212;1)</sup><i>C</i><sub>O</sub><sub><sub2>s</sub2></sub>)(<i>C</i><sub>O</sub><sub><sub2>s</sub2></sub><sub>,O</sub><sub><sub2>s</sub2></sub><sup>&#x2212;1</sup>))&#x2266;&#x3b5;) break<?in-line-formulae description="In-line Formulae" end="tail"?>
</li>
        <li id="ul0011-0004" num="0081">Else: include this block into the set of selected blocks (colored blocks of A) (storage of prior generation of M is at <b>204</b> and <b>205</b>. Current Residuals and selected input-output groups Update; output of current generation of M is at <b>206</b> Model Output)
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>Set <img id="CUSTOM-CHARACTER-00010" he="3.13mm" wi="3.89mm" file="US08626680-20140107-P00004.TIF" alt="custom character" img-content="character" img-format="tif"/><sup>(m)</sup>=<img id="CUSTOM-CHARACTER-00011" he="3.13mm" wi="3.89mm" file="US08626680-20140107-P00004.TIF" alt="custom character" img-content="character" img-format="tif"/><sup>(m&#x2212;1)</sup>&#x222a;(<i>I</i><sub>r</sub>(<i>m</i>),<i>O</i><sub>k</sub>(<i>m</i>))<?in-line-formulae description="In-line Formulae" end="tail"?>
</li>
        <li id="ul0011-0005" num="0082">and compute the least square solution using all the updated set of selected bocks (Model output <b>206</b>) A<sup>(m)</sup>=&#xc2;<sub>X</sub>(<img id="CUSTOM-CHARACTER-00012" he="3.13mm" wi="3.89mm" file="US08626680-20140107-P00004.TIF" alt="custom character" img-content="character" img-format="tif"/><sup>(m)</sup>, Y).</li>
        <li id="ul0011-0006" num="0083">&#x2003;where &#xc2;<sub>X</sub>(<img id="CUSTOM-CHARACTER-00013" he="3.13mm" wi="3.89mm" file="US08626680-20140107-P00004.TIF" alt="custom character" img-content="character" img-format="tif"/><sup>(m), Y) </sup>is returned by the subprocedure &#x201c;SUBPROC1&#x201d; called with X,Y,C and <img id="CUSTOM-CHARACTER-00014" he="3.13mm" wi="3.89mm" file="US08626680-20140107-P00004.TIF" alt="custom character" img-content="character" img-format="tif"/><sup>(m) </sup>as inputs per <figref idref="DRAWINGS">FIG. 6</figref></li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0065" num="0084">For m=2
<ul id="ul0012" list-style="none">
    <li id="ul0012-0001" num="0000">
    <ul id="ul0013" list-style="none">
        <li id="ul0013-0001" num="0085">Compute current residual error matrix:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><img id="CUSTOM-CHARACTER-00015" he="3.13mm" wi="3.13mm" file="US08626680-20140107-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/><sup>(m&#x2212;1)</sup><i>=Y&#x2212;XA</i><sup>(m&#x2212;1) </sup><?in-line-formulae description="In-line Formulae" end="tail"?>
</li>
        <li id="ul0013-0002" num="0086">Pick the block that helps most in term of reducing the residual error (<b>501</b>)
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>Let (<i>r</i><sup>(m)</sup><i>,s</i><sup>(m)</sup>)=arg max<sub>r,s </sub><i>tr</i>((<i>X</i><sub>I</sub><sub><sub2>r</sub2></sub><sup>T</sup><img id="CUSTOM-CHARACTER-00016" he="3.13mm" wi="3.13mm" file="US08626680-20140107-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/><sup>(m&#x2212;1)</sup><i>C</i><sub>O</sub><sub><sub2>s</sub2></sub>)<sup>T</sup>(<i>X</i><sub>I</sub><sub><sub2>r</sub2></sub><sup>T</sup><img id="CUSTOM-CHARACTER-00017" he="3.13mm" wi="3.13mm" file="US08626680-20140107-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/><sup>(m&#x2212;1)</sup><i>C</i><sub>O</sub><sub><sub2>s</sub2></sub>)(<i>C</i><sub>O</sub><sub><sub2>s</sub2></sub><sub>,O</sub><sub><sub2>s</sub2></sub><sup>&#x2212;1</sup>)).<?in-line-formulae description="In-line Formulae" end="tail"?>
</li>
        <li id="ul0013-0003" num="0087">If benefit gained by incorporating this block is too small stop:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>If <i>tr</i>((<i>X</i><sub>I</sub><sub><sub2>r</sub2></sub><sup>T</sup><img id="CUSTOM-CHARACTER-00018" he="3.13mm" wi="3.13mm" file="US08626680-20140107-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/><sup>(m&#x2212;1)</sup><i>C</i><sub>O</sub><sub><sub2>s</sub2></sub>)<sup>T</sup>(<i>X</i><sub>I</sub><sub><sub2>r</sub2></sub><sup>T</sup><img id="CUSTOM-CHARACTER-00019" he="3.13mm" wi="3.13mm" file="US08626680-20140107-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/><sup>(m&#x2212;1)</sup><i>C</i><sub>O</sub><sub><sub2>s</sub2></sub>)(<i>C</i><sub>O</sub><sub><sub2>s</sub2></sub><sub>,O</sub><sub><sub2>s</sub2></sub><sup>&#x2212;1</sup>))&#x2266;&#x3b5;) break<?in-line-formulae description="In-line Formulae" end="tail"?>
</li>
        <li id="ul0013-0004" num="0088">Else: include this block in the set of selected blocks (<b>401</b>)
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>Set <img id="CUSTOM-CHARACTER-00020" he="3.13mm" wi="3.89mm" file="US08626680-20140107-P00004.TIF" alt="custom character" img-content="character" img-format="tif"/><sup>(m)</sup>=<img id="CUSTOM-CHARACTER-00021" he="3.13mm" wi="3.89mm" file="US08626680-20140107-P00004.TIF" alt="custom character" img-content="character" img-format="tif"/><sup>(m&#x2212;1)</sup>&#x222a;(<i>I</i><sub>r</sub>(<i>m</i>), <i>O</i><sub>k</sub>(<i>m</i>))<?in-line-formulae description="In-line Formulae" end="tail"?>
</li>
        <li id="ul0013-0005" num="0089">and compute the least square solution using all the updated set of selected bocks
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>A</i><sup>(m)</sup><i>=&#xc2;</i><sub>X</sub>(<img id="CUSTOM-CHARACTER-00022" he="3.13mm" wi="3.89mm" file="US08626680-20140107-P00004.TIF" alt="custom character" img-content="character" img-format="tif"/><sup>(m)</sup><i>,Y</i>).<?in-line-formulae description="In-line Formulae" end="tail"?>
</li>
        <li id="ul0013-0006" num="0090">where &#xc2;<sub>X</sub>(<img id="CUSTOM-CHARACTER-00023" he="3.13mm" wi="3.89mm" file="US08626680-20140107-P00004.TIF" alt="custom character" img-content="character" img-format="tif"/><sup>(m)</sup>, Y) is returned by the subprocedure &#x201c;SUBPROC1&#x201d; called with X,Y,C and <img id="CUSTOM-CHARACTER-00024" he="3.13mm" wi="3.89mm" file="US08626680-20140107-P00004.TIF" alt="custom character" img-content="character" img-format="tif"/><sup>(m) </sup>as inputs per <figref idref="DRAWINGS">FIG. 6</figref></li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0066" num="0091">Output: The selected group pairs <img id="CUSTOM-CHARACTER-00025" he="3.13mm" wi="3.89mm" file="US08626680-20140107-P00004.TIF" alt="custom character" img-content="character" img-format="tif"/><sup>(m) </sup>and the regression coefficient matrix A<sup>(m)</sup>&#x3b5;<img id="CUSTOM-CHARACTER-00026" he="3.13mm" wi="3.13mm" file="US08626680-20140107-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/><sup>n&#xd7;K</sup>. For instance, with respect to <figref idref="DRAWINGS">FIGS. 3-5</figref>, one might get the output <img id="CUSTOM-CHARACTER-00027" he="3.13mm" wi="3.89mm" file="US08626680-20140107-P00004.TIF" alt="custom character" img-content="character" img-format="tif"/><sup>(2)</sup>={(I<sub>2</sub>, O<sub>3</sub>),(I<sub>1</sub>,O<sub>1</sub>)}and A as shown in <figref idref="DRAWINGS">FIG. 7</figref>.</p>
<p id="p-0067" num="0092">Script and Italic manuscript letters &#x201c;M&#x201d; are used interchangeably herein. The regression coefficient matrix is then re-estimated as</p>
<p id="p-0068" num="0093">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mrow>
            <msup>
              <mi>A</mi>
              <mrow>
                <mo>(</mo>
                <mi>m</mi>
                <mo>)</mo>
              </mrow>
            </msup>
            <mo>=</mo>
            <mrow>
              <msub>
                <mover>
                  <mi>A</mi>
                  <mo>^</mo>
                </mover>
                <mi>X</mi>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <msup>
                    <mi>&#x2133;</mi>
                    <mrow>
                      <mo>(</mo>
                      <mi>m</mi>
                      <mo>)</mo>
                    </mrow>
                  </msup>
                  <mo>,</mo>
                  <mi>Y</mi>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
          <mo>,</mo>
          <mi>where</mi>
        </mrow>
        <mo>&#x2062;</mo>
        <mstyle>
          <mtext>
</mtext>
        </mstyle>
        <mo>&#x2062;</mo>
        <mrow>
          <mrow>
            <msub>
              <mover>
                <mi>A</mi>
                <mo>^</mo>
              </mover>
              <mi>X</mi>
            </msub>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <msup>
                  <mi>&#x2133;</mi>
                  <mrow>
                    <mo>(</mo>
                    <mi>m</mi>
                    <mo>)</mo>
                  </mrow>
                </msup>
                <mo>,</mo>
                <mi>Y</mi>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>=</mo>
          <mrow>
            <mi>arg</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mrow>
              <munder>
                <mi>min</mi>
                <mrow>
                  <mi>A</mi>
                  <mo>&#x2208;</mo>
                  <msup>
                    <mi>R</mi>
                    <mrow>
                      <mi>p</mi>
                      <mo>&#xd7;</mo>
                      <mi>K</mi>
                    </mrow>
                  </msup>
                </mrow>
              </munder>
              <mo>&#x2062;</mo>
              <mrow>
                <msub>
                  <mi>L</mi>
                  <mi>C</mi>
                </msub>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>(</mo>
                  <mi>A</mi>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
        <mo>&#x2062;</mo>
        <mstyle>
          <mtext>
</mtext>
        </mstyle>
        <mo>&#x2062;</mo>
        <mrow>
          <mrow>
            <mi>subject</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.8em" height="0.8ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>to</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.8em" height="0.8ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mrow>
              <mi>supp</mi>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mi>A</mi>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
          <mo>&#x22d0;</mo>
          <mrow>
            <mrow>
              <mi>&#x2133;</mi>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mi>m</mi>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>.</mo>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mi>Eq</mi>
        <mo>.</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mn>1</mn>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0069" num="0094">More detail about the operation of the subprocedure of <figref idref="DRAWINGS">FIG. 6</figref> will now be presented. A closed form solution for the equation Eq1 will now be given. Since there are features which are only relevant to a subset of the responses, the covariance matrix C comes into play and the problem can not be decoupled, in contrast to the standard OLS solution. However, a closed form solution can be derived by recalling the following matrix identities (see D. Harville, &#x201c;Matrix Algebra from a Statistician's Perspective,&#x201d; Springer, 1997.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>tr</i>(<i>M</i><sub>1</sub><sup>T</sup><i>M</i><sub>2</sub><i>M</i><sub>3</sub><i>M</i><sub>4</sub><sup>T</sup>)=vec(<i>M</i><sub>1</sub>)<sup>T</sup>(<i>M</i><sub>4</sub><img id="CUSTOM-CHARACTER-00028" he="3.13mm" wi="2.46mm" file="US08626680-20140107-P00005.TIF" alt="custom character" img-content="character" img-format="tif"/><i>M</i><sub>2</sub>)vec(<i>M</i><sub>3</sub>),&#x2003;&#x2003;Eq.2<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>vec(<i>M</i><sub>1</sub><i>M</i><sub>2</sub>)=(<i>I</i><img id="CUSTOM-CHARACTER-00029" he="3.13mm" wi="2.46mm" file="US08626680-20140107-P00005.TIF" alt="custom character" img-content="character" img-format="tif"/><i>M</i><sub>1</sub>)vec(<i>M</i><sub>2</sub>),&#x2003;&#x2003;Eq.3<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where vec denotes the matrix vectorization, <img id="CUSTOM-CHARACTER-00030" he="3.13mm" wi="2.46mm" file="US08626680-20140107-P00005.TIF" alt="custom character" img-content="character" img-format="tif"/> the Kronecker product, and the dimensions of the identity matrix I equal the number of rows of M<sub>2</sub>. From equation Eq.2, we have
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>tr</i>((<i>Y&#x2212;XA</i>)<sup>T</sup>(<i>Y&#x2212;XA</i>)<i>C</i>)=(vec(<i>Y&#x2212;XA</i>))<sup>T</sup>(<i>C</i><img id="CUSTOM-CHARACTER-00031" he="3.13mm" wi="2.46mm" file="US08626680-20140107-P00005.TIF" alt="custom character" img-content="character" img-format="tif"/><i>I</i><sub>n</sub>)(vec(<i>Y&#x2212;XA</i>)). &#x2003;&#x2003;Eq.4<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0070" num="0095">For a set of selected blocks, say M, let O<sub>M </sub>be the union of the output groups present in M i.e., O<sub>M</sub>=&#x222a;<sub>s:&#x2203;r suchthat(I</sub><sub><sub2>r</sub2></sub><sub>,O</sub><sub><sub2>s</sub2></sub><sub>)&#x3b5;M</sub>O<sub>s</sub>. Then, let {tilde over (C)}=C<sub>O</sub><sub><sub2>M</sub2></sub><sub>,O</sub><sub><sub2>M</sub2></sub><img id="CUSTOM-CHARACTER-00032" he="3.13mm" wi="2.46mm" file="US08626680-20140107-P00005.TIF" alt="custom character" img-content="character" img-format="tif"/>I<sub>n </sub>and {tilde over (Y)}=vec(Y<sub>O</sub><sub><sub2>M</sub2></sub>). For each output group O<sub>s </sub>present in M, let I<sub>O</sub><sub><sub2>s</sub2></sub>=&#x222a;<sub>r:(I</sub><sub><sub2>r</sub2></sub><sub>,O</sub><sub><sub2>s</sub2></sub><sub>)&#x3b5;M</sub>I<sub>r</sub>. Finally define {tilde over (X)} such that {tilde over (X)}=diag{I<sub>|O</sub><sub><sub2>8</sub2></sub><sub>|</sub><img id="CUSTOM-CHARACTER-00033" he="3.13mm" wi="2.46mm" file="US08626680-20140107-P00005.TIF" alt="custom character" img-content="character" img-format="tif"/>X<sub>I</sub><sub><sub2>O8</sub2></sub>, O<sub>s</sub>&#x2282;<img id="CUSTOM-CHARACTER-00034" he="3.13mm" wi="5.25mm" file="US08626680-20140107-P00006.TIF" alt="custom character" img-content="character" img-format="tif"/>}.</p>
<p id="p-0071" num="0096">Using equations Eq.4 and Eq.3 one can show that the non-zero entries of vec(&#xc2;<sub>X</sub>(<img id="CUSTOM-CHARACTER-00035" he="3.13mm" wi="3.89mm" file="US08626680-20140107-P00004.TIF" alt="custom character" img-content="character" img-format="tif"/>, Y)), i.e., those corresponding to the support induced by M, are given by:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>{circumflex over (&#x3b1;)}=(<i>{tilde over (X)}</i><sup>T</sup><i>{tilde over (C)}{tilde over (X)}</i>)<sup>&#x2212;1</sup>(<i>{tilde over (X)}</i><sup>T</sup><i>{tilde over (C)}</i>)<i>{tilde over (Y)}, </i><?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
which thus provides a closed-form formula for the coefficient re-estimation step.
</p>
<p id="p-0072" num="0097">To conclude the description of the MG-OMP algorithm, one could also consider performing alternate optimization of the original negative log-likelihood objective over A and &#x3a3;, using MG-OMP to optimize over A for a fixed estimate of &#x3a3;, and using a covariance estimation algorithm (e.g. Graphical Lasso J. Friedman, T. Hastie, and R. Tibshirani, &#x201c;Sparse inverse covariance estimation with the graphical lasso,&#x201d; Biostatistics, 9(3):432-441, July 2008), to estimate &#x3a3; with fixed A.</p>
<p id="p-0073" num="0098">In the context of blogs, the input and output groups usually coincide; but they need not. For instance, one could consider focusing on how bloggers within a certain community influence bloggers from another community. Say the set of blogger in the imput are physicists and the set of blogger in the output are biologists.</p>
<p id="h-0006" num="0000">Discussion of Related Work</p>
<p id="p-0074" num="0099">Several methods have been proposed recently to address the variable group selection problem in univariate regression, based on the minimization of an empirical error penalized by a regularization term. These methods extend the Lasso formulation (see R. Tibshirani, &#x201c;Regression shrinkage and selection via the lasso,&#x201d; Journal of the Royal Statistical Society, Series B, 58:267-288, 1994.) by modifying the 1-1 penalty to account for the group structure. Specifically, M. Yuan and Y. Lin, &#x201c;Model selection and estimation in regression with grouped variables,&#x201d; Journal of the Royal Statistical Society, Series B, 68:49-67, 2006 proposed the Group Lasso, which employs the so-called 1-1/1-2 penalty, i.e., the sum of the 1-2 norms of the model restrictions to each variable group, so as to enforce sparsity at the variable group level. Group lasso methods have been applied to simultaneous individual variable selection in the context of multivariate regression B A. Turlach and W. N. Venables and S. J. Wright,&#x201c;Simultaneous variable selection&#x201d;, Technometrics, 2005 In view of the results by T. Zhang, &#x201c;On the consistency of feature selection using greedy least squares regression,&#x201d; Journal of Machine Learning Research, 2008; A. Fletcher and S. Rangan, &#x201c;Orthogonal Matching Pursuit From Noisy Random Measurements: A New Analysis&#x201d;, NIPS, 2009. A. C. Lozano, G. Swirszcz, and N. Abe, &#x201c;Grouped orthogonal matching pursuit for variable selection and prediction,&#x201d; Advances in Neural Information Processing Systems 22, 2009, OMP-based methods compare favorably to Lasso-based methods both in term of variable selection and prediction accuracy.</p>
<p id="p-0075" num="0100">Furthermore, the proposed MG-OMP procedure is much more flexible than existing methods, in terms of handling various sparsity patterns with respect to both input and output variables. For instance it is more general than simultaneous variable selection procedures (e.g. S-OMP of J. A. Tropp, A. C. Gilbert, and M. J. Strauss, &#x201c;Algorithms for simultaneous sparse approximation: part i: Greedy pursuit,&#x201d; Sig. Proc., 86(3):572-588, 2006), while generalizing variable group selection methods J. Huang, T. Zhang, and D. Metaxas D., &#x201c;Learning with structured sparsity,&#x201d; Proceedings of the 26th Annual International Conference on Machine Learning, 2009; A. C. Lozano, G. Swirszcz, and N. Abe, &#x201c;Grouped orthogonal matching pursuit for variable selection and prediction&#x201d;. Advances in Neural Information Processing Systems 22, 2009 to multivariate regression.</p>
<p id="p-0076" num="0101">In addition, MG-OMP is especially attractive due to its computational efficiency, and also since the algorithm can be parallelized in a very efficient manner. Such points are crucial when dealing with large datasets, which is increasingly the case in many applications of machine learning and data mining.</p>
<p id="p-0077" num="0102">Other approaches have been proposed for multivariate regression that differ from variable selection. G. Reinsel and R. Velu, &#x201c;Multivariate Reduced-rank Regression: Theory and Applications,&#x201d;, Springer New York, 1998 enforce dimension reduction by constraining the rank of the regression coefficient matrix. Y. Fujikoshi and K. Satoh,&#x201c;Modified AIC and Cp in multivariate linear regression&#x201d;, Biometrika, 1997; E. Bedrick and C. Tsai, &#x201c;Model selection for multivariate regression in small samples&#x201d;, Biometrics, 1994; consider criterion-based model selection. The FES method of Ming Yuan and Ali Ekici and Zhaosong Lu and Renato Monteiro, &#x201c;Dimension reduction and coefficient estimation in multivariate linear regression&#x201d;, Journal Of The Royal Statistical Society Series B, 2007 solves</p>
<p id="p-0078" num="0103">
<maths id="MATH-US-00003" num="00003">
<math overflow="scroll">
  <mrow>
    <mi>arg</mi>
    <mo>&#x2062;</mo>
    <mstyle>
      <mspace width="0.3em" height="0.3ex"/>
    </mstyle>
    <mo>&#x2062;</mo>
    <mi>min</mi>
    <mo>&#x2062;</mo>
    <mstyle>
      <mspace width="0.3em" height="0.3ex"/>
    </mstyle>
    <mo>&#x2062;</mo>
    <mrow>
      <mi>tr</mi>
      <mo>&#x2061;</mo>
      <mrow>
        <mo>(</mo>
        <mrow>
          <msup>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mi>Y</mi>
                <mo>-</mo>
                <mi>XA</mi>
              </mrow>
              <mo>)</mo>
            </mrow>
            <mi>T</mi>
          </msup>
          <mo>&#x2062;</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mi>Y</mi>
              <mo>-</mo>
              <mi>XA</mi>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mrow>
  </mrow>
</math>
</maths>
<maths id="MATH-US-00003-2" num="00003.2">
<math overflow="scroll">
  <mrow>
    <mrow>
      <mrow>
        <mi>subject</mi>
        <mo>&#x2062;</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mi>to</mi>
        <mo>&#x2062;</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mrow>
          <munderover>
            <mo>&#x2211;</mo>
            <mrow>
              <mi>i</mi>
              <mo>=</mo>
              <mn>1</mn>
            </mrow>
            <mrow>
              <mi>min</mi>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>p</mi>
                  <mo>,</mo>
                  <mi>K</mi>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
          </munderover>
          <mo>&#x2062;</mo>
          <mrow>
            <msub>
              <mi>&#x3c3;</mi>
              <mi>i</mi>
            </msub>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mi>A</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
      <mo>&#x2264;</mo>
      <mi>t</mi>
    </mrow>
    <mo>,</mo>
  </mrow>
</math>
</maths>
<br/>
where &#x3c3;<sub>i</sub>(A) is the ith largest singular value of A, and ignoring the error covariance matrix. This approach also leads to rank-reduction.
<br/>
Experimental Evaluation
</p>
<p id="p-0079" num="0104">An empirical evaluation of the method proposed herein was undertaken in comparison with representative variable selection methods, in terms of accuracy of prediction and variable (group) selection. The F<b>1</b> measure of variable selection accuracy will be used, this being defined as F<b>1</b>={2PR}/{P+R}, where P denotes the precision and R denotes the recall. As measure of prediction accuracy, either the model error defined as ME(&#xc2;,A)=tr((&#xc2;&#x2212;A)<sup>T</sup>E(X<sup>T</sup>X)(&#xc2;&#x2212;A)), when available, or otherwise the average squared error on a test set.</p>
<p id="p-0080" num="0105">For all the greedy pursuit methods, &#x201c;holdout validated&#x201d; estimates are applied, namely the iteration number (i.e. stopping point) that minimize the average squared error on a validation set is selected. For univariate methods, two cases: are considered (i) individual selection of the iteration number for each univariate regression that minimize the average squared error w.r.t to the univariate regression considered, (ii) joint selection of the same iteration number for each univariate regression that minimize the sum of the averaged squared error over the K univariate regressions. For each setting, there were 50 runs, each with 50 observations for training, 50 for validation and 50 for testing.</p>
<p id="p-0081" num="0106">Experiment 1: Feature/Response groups of size one. The purpose of this experiment was to evaluate the benefits of using the method described herein, as the strength of the correlation between the responses is varied, in the simple case where both response groups and feature groups are also of size one (i.e, each feature/response is a group). There should be independent sparsity patterns for each individual regression. there is an n&#xd7;p predictor matrix X, where the rows are generated independently according to a multivariate Gaussian distribution N<sub>p</sub>(0,S), with S<sub>i,j</sub>=0.7<sup>|i&#x2212;j|</sup>. The n&#xd7;K error matrix E is generated according to N<sub>K</sub>(0, &#x3a3;), with &#x3a3;<sub>i,j</sub>=&#x3c1;<sup>|i&#x2212;j|</sup>, where &#x3c1;&#x3b5;{0,0.5,0.7,0.9}. The true coefficient matrix A was generated as follows. First the sparsity pattern was generated, namely the entries of A were first set to be zero or non-zero by drawing independent samples from a Bernoulli distribution with probability P<sub>s</sub>. Then for each non-zero entry of A, independently, the value was set according to the normal distribution N(0,1). Various combinations for the values of (p, K, &#x3c1;, P<sub>s</sub>) were considered.</p>
<p id="p-0082" num="0107">MG-OMP was compared with C estimated from univariate OMP fits (denoted as &#x201c;MG-OMP (C)&#x201d;); MG-OMP with C set to identity (denoted as &#x201c;MG-OMP (Id)&#x201d;); OMP with individual and joint iteration number selection (&#x201c;OMP (ind)&#x201d; and &#x201c;OMP (joint)&#x201d;); and OLS. The results are presented in Table 1 below.</p>
<p id="p-0083" num="0108">
<tables id="TABLE-US-00001" num="00001">
<table frame="none" colsep="0" rowsep="0" pgwide="1">
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="329pt" align="center"/>
<thead>
<row>
<entry namest="1" nameend="1" rowsep="1">TABLE 1</entry>
</row>
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
<row>
<entry>Average F<sub>1 </sub>score (top) and model error (bottom) for the models output by variants of</entry>
</row>
<row>
<entry>MG-OMP, Univariate OMP and Ordinary Least Squares, under the setting of Experiment 1.</entry>
</row>
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="9">
<colspec colname="1" colwidth="14pt" align="center"/>
<colspec colname="2" colwidth="14pt" align="center"/>
<colspec colname="3" colwidth="14pt" align="center"/>
<colspec colname="4" colwidth="14pt" align="center"/>
<colspec colname="5" colwidth="56pt" align="center"/>
<colspec colname="6" colwidth="56pt" align="center"/>
<colspec colname="7" colwidth="56pt" align="center"/>
<colspec colname="8" colwidth="49pt" align="center"/>
<colspec colname="9" colwidth="56pt" align="center"/>
<tbody valign="top">
<row>
<entry>p</entry>
<entry>K</entry>
<entry>&#x3c1;</entry>
<entry>P<sub>s</sub></entry>
<entry>MG-OMP (C)</entry>
<entry>MG-OMP (Id)</entry>
<entry>OMP (ind)</entry>
<entry>OMP (joint)</entry>
<entry>OLS</entry>
</row>
<row>
<entry namest="1" nameend="9" align="center" rowsep="1"/>
</row>
<row>
<entry>20</entry>
<entry>20</entry>
<entry>0.9</entry>
<entry>0.1</entry>
<entry>0.827 &#xb1; 0.009</entry>
<entry>0.726 &#xb1; 0.008</entry>
<entry>0.689 &#xb1; 0.009</entry>
<entry>0.611 &#xb1; 0.007</entry>
<entry>0.2611 &#xb1; 0.003&#x2002;</entry>
</row>
<row>
<entry>20</entry>
<entry>20</entry>
<entry>0.7</entry>
<entry>0.1</entry>
<entry>0.775 &#xb1; 0.006</entry>
<entry>0.723 &#xb1; 0.007</entry>
<entry>0.678 &#xb1; 0.007</entry>
<entry>0.607 &#xb1; 0.007</entry>
<entry>0.258 &#xb1; 0.003</entry>
</row>
<row>
<entry>20</entry>
<entry>20</entry>
<entry>0.5</entry>
<entry>0.1</entry>
<entry>0.744 &#xb1; 0.007</entry>
<entry>0.715 &#xb1; 0.007</entry>
<entry>0.679 &#xb1; 0.007</entry>
<entry>0.609 &#xb1; 0.007</entry>
<entry>0.257 &#xb1; 0.003</entry>
</row>
<row>
<entry>20</entry>
<entry>20</entry>
<entry>0</entry>
<entry>0.1</entry>
<entry>0.710 &#xb1; 0.006</entry>
<entry>0.717 &#xb1; 0.008</entry>
<entry>&#x2002;0.683 &#xb1; 0.0073</entry>
<entry>0.605 &#xb1; 0.007</entry>
<entry>0.260 &#xb1; 0.003</entry>
</row>
<row>
<entry>20</entry>
<entry>20</entry>
<entry>0.9</entry>
<entry>0.5</entry>
<entry>0.804 &#xb1; 0.003</entry>
<entry>0.764 &#xb1; 0.005</entry>
<entry>0.767 &#xb1; 0.003</entry>
<entry>0.746 &#xb1; 0.003</entry>
<entry>0.706 &#xb1; 0.003</entry>
</row>
<row>
<entry>20</entry>
<entry>20</entry>
<entry>0.7</entry>
<entry>0.5</entry>
<entry>0.786 &#xb1; 0.003</entry>
<entry>0.768 &#xb1; 0.003</entry>
<entry>0.770 &#xb1; 0.003</entry>
<entry>0.748 &#xb1; 0.003</entry>
<entry>0.713 &#xb1; 0.002</entry>
</row>
<row>
<entry>20</entry>
<entry>20</entry>
<entry>0.5</entry>
<entry>0.5</entry>
<entry>0.771 &#xb1; 0.003</entry>
<entry>0.768 &#xb1; 0.003</entry>
<entry>0.769 &#xb1; 0.003</entry>
<entry>0.747 &#xb1; 0.003</entry>
<entry>0.708 &#xb1; 0.002</entry>
</row>
<row>
<entry>20</entry>
<entry>20</entry>
<entry>0</entry>
<entry>0.5</entry>
<entry>0.758 &#xb1; 0.003</entry>
<entry>0.760 &#xb1; 0.003</entry>
<entry>0.768 &#xb1; 0.003</entry>
<entry>0.746 &#xb1; 0.003</entry>
<entry>0.706 &#xb1; 0.003</entry>
</row>
<row>
<entry>20</entry>
<entry>60</entry>
<entry>0.9</entry>
<entry>0.1</entry>
<entry>0.833 &#xb1; 0.004</entry>
<entry>0.717 &#xb1; 0.004</entry>
<entry>0.687 &#xb1; 0.005</entry>
<entry>0.609 &#xb1; 0.004</entry>
<entry>0.260 &#xb1; 0.002</entry>
</row>
<row>
<entry>20</entry>
<entry>60</entry>
<entry>0.7</entry>
<entry>0.1</entry>
<entry>0.782 &#xb1; 0.003</entry>
<entry>0.716 &#xb1; 0.004</entry>
<entry>0.688 &#xb1; 0.004</entry>
<entry>0.609 &#xb1; 0.004</entry>
<entry>0.261 &#xb1; 0.002</entry>
</row>
<row>
<entry>20</entry>
<entry>60</entry>
<entry>0.5</entry>
<entry>0.1</entry>
<entry>0.751 &#xb1; 0.004</entry>
<entry>0.713 &#xb1; 0.006</entry>
<entry>0.695 &#xb1; 0.004</entry>
<entry>0.605 &#xb1; 0.005</entry>
<entry>0.263 &#xb1; 0.002</entry>
</row>
<row>
<entry>20</entry>
<entry>60</entry>
<entry>0</entry>
<entry>0.1</entry>
<entry>0.714 &#xb1; 0.004</entry>
<entry>0.712 &#xb1; 0.004</entry>
<entry>0.684 &#xb1; 0.003</entry>
<entry>0.601 &#xb1; 0.004</entry>
<entry>0.260 &#xb1; 0.002</entry>
</row>
<row>
<entry>60</entry>
<entry>20</entry>
<entry>0.9</entry>
<entry>0.1</entry>
<entry>0.777 &#xb1; 0.006</entry>
<entry>0.647 &#xb1; 0.007</entry>
<entry>0.619 &#xb1; 0.005</entry>
<entry>0.593 &#xb1; 0.005</entry>
<entry>NA</entry>
</row>
<row>
<entry>60</entry>
<entry>20</entry>
<entry>0.7</entry>
<entry>0.1</entry>
<entry>0.705 &#xb1; 0.005</entry>
<entry>0.643 &#xb1; 0.005</entry>
<entry>0.615 &#xb1; 0.005</entry>
<entry>0.591 &#xb1; 0.006</entry>
<entry>NA</entry>
</row>
<row>
<entry>60</entry>
<entry>20</entry>
<entry>0.5</entry>
<entry>0.1</entry>
<entry>0.670 &#xb1; 0.004</entry>
<entry>0.658 &#xb1; 0.005</entry>
<entry>0.625 &#xb1; 0.004</entry>
<entry>0.601 &#xb1; 0.004</entry>
<entry>NA</entry>
</row>
<row>
<entry>60</entry>
<entry>20</entry>
<entry>0</entry>
<entry>0.1</entry>
<entry>0.648 &#xb1; 0.006</entry>
<entry>0.647 &#xb1; 0.005</entry>
<entry>0.619 &#xb1; 0.004</entry>
<entry>0.592 &#xb1; 0.005</entry>
<entry>NA</entry>
</row>
<row>
<entry namest="1" nameend="9" align="center" rowsep="1"/>
</row>
<row>
<entry>p</entry>
<entry>K</entry>
<entry>&#x3c1;</entry>
<entry>P<sub>s</sub></entry>
<entry>MG-OMP</entry>
<entry>MG-OMP (Id)</entry>
<entry>OMP (ind)</entry>
<entry>OMP (joint)</entry>
<entry>OLS</entry>
</row>
<row>
<entry namest="1" nameend="9" align="center" rowsep="1"/>
</row>
<row>
<entry>20</entry>
<entry>20</entry>
<entry>0.9</entry>
<entry>0.1</entry>
<entry>0.626 &#xb1; 0.043</entry>
<entry>&#x2002;3.305 &#xb1; 0.134</entry>
<entry>&#x2002;3.470 &#xb1; 0.173</entry>
<entry>&#x2002;5.105 &#xb1; 0.198</entry>
<entry>14.241 &#xb1; 0.511</entry>
</row>
<row>
<entry>20</entry>
<entry>20</entry>
<entry>0.7</entry>
<entry>0.1</entry>
<entry>1.655 &#xb1; 0.082</entry>
<entry>&#x2002;3.331 &#xb1; 0.133</entry>
<entry>&#x2002;3.450 &#xb1; 0.113</entry>
<entry>&#x2002;5.087 &#xb1; 0.192</entry>
<entry>14.299 &#xb1; 0.320</entry>
</row>
<row>
<entry>20</entry>
<entry>20</entry>
<entry>0.5</entry>
<entry>0.1</entry>
<entry>2.529 &#xb1; 0.109</entry>
<entry>&#x2002;3.401 &#xb1; 0.158</entry>
<entry>&#x2002;3.507 &#xb1; 0.116</entry>
<entry>&#x2002;5.119 &#xb1; 0.165</entry>
<entry>14.032 &#xb1; 0.246</entry>
</row>
<row>
<entry>20</entry>
<entry>20</entry>
<entry>0</entry>
<entry>0.1</entry>
<entry>3.460 &#xb1; 0.119</entry>
<entry>&#x2002;3.292 &#xb1; 0.123</entry>
<entry>&#x2002;3.402 &#xb1; 0.089</entry>
<entry>&#x2002;5.063 &#xb1; 0.125</entry>
<entry>13.731 &#xb1; 0.203</entry>
</row>
<row>
<entry>20</entry>
<entry>20</entry>
<entry>0.9</entry>
<entry>0.5</entry>
<entry>4.660 &#xb1; 0.166</entry>
<entry>&#x2002;9.906 &#xb1; 0.305</entry>
<entry>&#x2002;8.655 &#xb1; 0.279</entry>
<entry>11.195 &#xb1; 0.388</entry>
<entry>13.132 &#xb1; 0.546</entry>
</row>
<row>
<entry>20</entry>
<entry>20</entry>
<entry>0.7</entry>
<entry>0.5</entry>
<entry>8.101 &#xb1; 0.195</entry>
<entry>10.551 &#xb1; 0.254</entry>
<entry>&#x2002;9.151 &#xb1; 0.253</entry>
<entry>12.165 &#xb1; 0.328</entry>
<entry>13.893 &#xb1; 0.385</entry>
</row>
<row>
<entry>20</entry>
<entry>20</entry>
<entry>0.5</entry>
<entry>0.5</entry>
<entry>9.407 &#xb1; 0.201</entry>
<entry>10.812 &#xb1; 0.242</entry>
<entry>&#x2002;8.992 &#xb1; 0.201</entry>
<entry>11.946 &#xb1; 0.255</entry>
<entry>13.722 &#xb1; 0.240</entry>
</row>
<row>
<entry>20</entry>
<entry>20</entry>
<entry>0</entry>
<entry>0.5</entry>
<entry>10.932 &#xb1; 0.208&#x2002;</entry>
<entry>10.856 &#xb1; 0.215</entry>
<entry>&#x2002;8.884 &#xb1; 0.150</entry>
<entry>11.635 &#xb1; 0.227</entry>
<entry>14.021 &#xb1; 0.249</entry>
</row>
<row>
<entry>20</entry>
<entry>60</entry>
<entry>0.9</entry>
<entry>0.1</entry>
<entry>3.343 &#xb1; 0.402</entry>
<entry>10.709 &#xb1; 0.302</entry>
<entry>10.378 &#xb1; 0.263</entry>
<entry>15.723 &#xb1; 0.368</entry>
<entry>41.525 &#xb1; 0.731</entry>
</row>
<row>
<entry>20</entry>
<entry>60</entry>
<entry>0.7</entry>
<entry>0.1</entry>
<entry>5.268 &#xb1; 0.200</entry>
<entry>11.001 &#xb1; 0.279</entry>
<entry>10.298 &#xb1; 0.262</entry>
<entry>15.181 &#xb1; 0.358</entry>
<entry>39.946 &#xb1; 0.568</entry>
</row>
<row>
<entry>20</entry>
<entry>60</entry>
<entry>0.5</entry>
<entry>0.1</entry>
<entry>7.987 &#xb1; 0.215</entry>
<entry>10.768 &#xb1; 0.277</entry>
<entry>10.484 &#xb1; 0.205</entry>
<entry>16.163 &#xb1; 0.276</entry>
<entry>41.285 &#xb1; 0.578</entry>
</row>
<row>
<entry>20</entry>
<entry>60</entry>
<entry>0</entry>
<entry>0.1</entry>
<entry>10.516 &#xb1; 0.203&#x2002;</entry>
<entry>10.732 &#xb1; 0.251</entry>
<entry>10.396 &#xb1; 0.173</entry>
<entry>15.802 &#xb1; 0.290</entry>
<entry>40.991 &#xb1; 0.619</entry>
</row>
<row>
<entry>60</entry>
<entry>20</entry>
<entry>0.9</entry>
<entry>0.1</entry>
<entry>3.059 &#xb1; 0.276</entry>
<entry>13.365 &#xb1; 0.556</entry>
<entry>17.720 &#xb1; 0.796</entry>
<entry>19.209 &#xb1; 0.789</entry>
<entry>NA</entry>
</row>
<row>
<entry>60</entry>
<entry>20</entry>
<entry>0.7</entry>
<entry>0.1</entry>
<entry>7.552 &#xb1; 0.405</entry>
<entry>14.151 &#xb1; 0.457</entry>
<entry>19.222 &#xb1; 0.782</entry>
<entry>20.752 &#xb1; 0.769</entry>
<entry>NA</entry>
</row>
<row>
<entry>60</entry>
<entry>20</entry>
<entry>0.5</entry>
<entry>0.1</entry>
<entry>10.619 &#xb1; 0.407&#x2002;</entry>
<entry>12.853 &#xb1; 0.370</entry>
<entry>17.965 &#xb1; 0.681</entry>
<entry>19.623 &#xb1; 0.668</entry>
<entry>NA</entry>
</row>
<row>
<entry>60</entry>
<entry>20</entry>
<entry>0</entry>
<entry>0.1</entry>
<entry>13.581 &#xb1; 0.450&#x2002;</entry>
<entry>12.8686 &#xb1; 0.470&#x2002;</entry>
<entry>18.661 &#xb1; 0.899</entry>
<entry>20.590 &#xb1; 0.888</entry>
<entry>NA</entry>
</row>
<row>
<entry namest="1" nameend="9" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0084" num="0109">Experiment 2: Grouping structure among the features, response groups of size one. The goal of this experiment was to evaluate the method in the case where the response groups are still of size one, but where a grouping structure exists among the features. A model with 3rd order polynomial expansion: Y=XA<sub>1</sub>+X<sup>2</sup>A<sub>2</sub>+X<sup>3</sup>A<sub>3</sub>+E, was considered, with notational irregularity such that X<sup>q </sup>denotes the matrix such that X<sub>i,j</sub><sup>q</sup>=(X<sub>i,j</sub>)<sup>q</sup>. X and E were generated as in Experiment 1. A<sub>1</sub>, A<sub>2</sub>, A<sub>3 </sub>had a common sparsity pattern, setup as in Experiment 1 with P<sub>s</sub>=0.1. Then for each non-zero entry of the A<sub>i</sub>'s, the value was set independently according to N(0,1). The number of features for X, i.e., p, was set to 20. Hence we consider 60 variables grouped into 20 groups corresponding the 3rd degree polynomial expansion.</p>
<p id="p-0085" num="0110">Comparison was made between (i) MG-OMP with correct feature groups and C estimated from univariate OMP fits (denoted as &#x201c;MG-OMP(C)&#x201d;); (ii) MG-OMP with correct feature groups and C set to identity (denoted as &#x201c;MG-OMP}(Id)&#x201d;); (iii) MG-OMP with feature groups set to be the individual variables and C estimated from univariate OMP fits (denoted as &#x201c;M-OMP (C)&#x201d;); (iv) G-OMP with individual and joint iteration number selection; and (v) OMP with individual and joint iteration number selection. The results are presented in Table 2 below.</p>
<p id="p-0086" num="0111">
<tables id="TABLE-US-00002" num="00002">
<table frame="none" colsep="0" rowsep="0" pgwide="1">
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="378pt" align="center"/>
<thead>
<row>
<entry namest="1" nameend="1" rowsep="1">TABLE 2</entry>
</row>
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
<row>
<entry>Average F<sub>1 </sub>score (top) and average test set squared error (bottom) for the models</entry>
</row>
<row>
<entry>output by variants of MG-OMP, Group-OMP and OMP under the setting of Experiment 2.</entry>
</row>
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="8">
<colspec colname="1" colwidth="14pt" align="center"/>
<colspec colname="2" colwidth="49pt" align="center"/>
<colspec colname="3" colwidth="49pt" align="center"/>
<colspec colname="4" colwidth="49pt" align="center"/>
<colspec colname="5" colwidth="49pt" align="center"/>
<colspec colname="6" colwidth="49pt" align="center"/>
<colspec colname="7" colwidth="56pt" align="center"/>
<colspec colname="8" colwidth="63pt" align="center"/>
<tbody valign="top">
<row>
<entry>&#x3c1;</entry>
<entry>MG-OMP (C)</entry>
<entry>MG-OMP (Id)</entry>
<entry>M-OMP (C)</entry>
<entry>G-OMP (ind)</entry>
<entry>G-OMP (joint)</entry>
<entry>OMP (ind) (g F1)</entry>
<entry>OMP (joint) (g F1)</entry>
</row>
<row>
<entry namest="1" nameend="8" align="center" rowsep="1"/>
</row>
<row>
<entry>0.9</entry>
<entry>0.949 &#xb1; 0.006</entry>
<entry>0.903 &#xb1; 0.006</entry>
<entry>0.723 &#xb1; 0.007</entry>
<entry>0.892 &#xb1; 0.005</entry>
<entry>0.710 &#xb1; 0.011</entry>
<entry>0.595 &#xb1; 0.005</entry>
<entry>0.476 &#xb1; 0.012</entry>
</row>
<row>
<entry>0.7</entry>
<entry>0.931 &#xb1; 0.009</entry>
<entry>0.895 &#xb1; 0.007</entry>
<entry>0.723 &#xb1; 0.006</entry>
<entry>0.873 &#xb1; 0.007</entry>
<entry>0.686 &#xb1; 0.011</entry>
<entry>0.594 &#xb1; 0.005</entry>
<entry>0.492 &#xb1; 0.013</entry>
</row>
<row>
<entry>0.5</entry>
<entry>0.943 &#xb1; 0.004</entry>
<entry>0.886 &#xb1; 0.007</entry>
<entry>0.726 &#xb1; 0.009</entry>
<entry>0.873 &#xb1; 0.007</entry>
<entry>0.688 &#xb1; 0.013</entry>
<entry>0.596 &#xb1; 0.005</entry>
<entry>0.516 &#xb1; 0.015</entry>
</row>
<row>
<entry>0</entry>
<entry>0.928 &#xb1; 0.008</entry>
<entry>0.803 &#xb1; 0.006</entry>
<entry>0.675 &#xb1; 0.013</entry>
<entry>0.890 &#xb1; 0.006</entry>
<entry>0.722 &#xb1; 0.009</entry>
<entry>0.594 &#xb1; 0.005</entry>
<entry>0.503 &#xb1; 0.013</entry>
</row>
<row>
<entry namest="1" nameend="8" align="center" rowsep="1"/>
</row>
<row>
<entry>&#x3c1;</entry>
<entry>MG-OMP (C)</entry>
<entry>MG-OMP (Id)</entry>
<entry>M-OMP (C)</entry>
<entry>G-OMP (ind)</entry>
<entry>G-OMP (joint)</entry>
<entry>OMP (ind) (g F1)</entry>
<entry>OMP (joint) (g F1)</entry>
</row>
<row>
<entry namest="1" nameend="8" align="center" rowsep="1"/>
</row>
<row>
<entry>0.9</entry>
<entry>1.217 &#xb1; 0.063</entry>
<entry>1.654 &#xb1; 0.075</entry>
<entry>4.312 &#xb1; 0.406</entry>
<entry>1.793 &#xb1; 0.082</entry>
<entry>3.185 &#xb1; 0.171</entry>
<entry>3.486 &#xb1; 0.232</entry>
<entry>5.001 &#xb1; 0.335</entry>
</row>
<row>
<entry>0.7</entry>
<entry>1.672 &#xb1; 0.169</entry>
<entry>1.797 &#xb1; 0.131</entry>
<entry>4.958 &#xb1; 0.344</entry>
<entry>1.803 &#xb1; 0.066</entry>
<entry>3.439 &#xb1; 0.208</entry>
<entry>3.224 &#xb1; 0.169</entry>
<entry>4.693 &#xb1; 0.235</entry>
</row>
<row>
<entry>0.5</entry>
<entry>1.728 &#xb1; 0.089</entry>
<entry>1.801 &#xb1; 0.092</entry>
<entry>4.881 &#xb1; 0.340</entry>
<entry>1.858 &#xb1; 0.081</entry>
<entry>3.393 &#xb1; 0.225</entry>
<entry>3.379 &#xb1; 0.184</entry>
<entry>4.864 &#xb1; 0.262</entry>
</row>
<row>
<entry>0</entry>
<entry>2.092 &#xb1; 0.142</entry>
<entry>1.080 &#xb1; 0.132</entry>
<entry>4.453 &#xb1; 0.229</entry>
<entry>1.764 &#xb1; 0.064</entry>
<entry>3.252 &#xb1; 0.176</entry>
<entry>3.591 &#xb1; 0.196</entry>
<entry>5.179 &#xb1; 0.278</entry>
</row>
<row>
<entry namest="1" nameend="8" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0087" num="0112">Experiment 3: Grouping structure among features, a single response group. The goal of this experiment was to evaluate our method in the case where all the responses are grouped into a single group (i.e. simultaneous selection is desired across all the regressions). The model was the same as in Experiment 2, except for the sparsity pattern, which was row-wise as follows. A<sub>1</sub>, A<sub>2</sub>, A<sub>3 </sub>have the same sparsity pattern, which is set by setting each row of the concatenated matrix [A<sub>1</sub>, A<sub>2</sub>, A<sub>3</sub>] to be zero or non-zero according to Bernoulli draws with success probability P<sub>r</sub>=0.2. Then for each non-zero entry, independently, its value was set according to N(0,1). The number of features for X, i.e., p, was set to 20, hence 60 variables grouped into 20 groups corresponding the 3rd degree polynomial expansion were considered.</p>
<p id="p-0088" num="0113">MG-OMP was compared with correct feature groups and with feature groups set to be the individual variables, with C set to identity for both (note that in this special case using C estimated from univariate OMP fits has negligible impact on performance and is thus omitted for the sake of conciseness); G-OMP and OMP with individual and joint iteration number selection. The results are presented in Table 3 below.</p>
<p id="p-0089" num="0114">
<tables id="TABLE-US-00003" num="00003">
<table frame="none" colsep="0" rowsep="0" pgwide="1">
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="329pt" align="center"/>
<thead>
<row>
<entry namest="1" nameend="1" rowsep="1">TABLE 3</entry>
</row>
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
<row>
<entry>Average F<sub>1 </sub>score (top) and average test set squared error (bottom) for the models</entry>
</row>
<row>
<entry>output by variants of MG-OMP, Group-OMP mid OMP under the setting of Experiment 3.</entry>
</row>
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="7">
<colspec colname="1" colwidth="14pt" align="center"/>
<colspec colname="2" colwidth="49pt" align="center"/>
<colspec colname="3" colwidth="49pt" align="center"/>
<colspec colname="4" colwidth="49pt" align="center"/>
<colspec colname="5" colwidth="49pt" align="center"/>
<colspec colname="6" colwidth="56pt" align="center"/>
<colspec colname="7" colwidth="63pt" align="center"/>
<tbody valign="top">
<row>
<entry>&#x3c1;</entry>
<entry>MG-OMP</entry>
<entry>M-OMP</entry>
<entry>G-OMP (ind)</entry>
<entry>G-OMP (joint)</entry>
<entry>OMP (ind) (g F1)</entry>
<entry>OMP (Joint) (g F1)</entry>
</row>
<row>
<entry namest="1" nameend="7" align="center" rowsep="1"/>
</row>
<row>
<entry>0.9</entry>
<entry>0.971 &#xb1; 0.009</entry>
<entry>0.735 &#xb1; 0.021</entry>
<entry>0.896 &#xb1; 0.010</entry>
<entry>0.883 &#xb1; 0.014</entry>
<entry>0.599 &#xb1; 0.007</entry>
<entry>0.556 &#xb1; 0.012</entry>
</row>
<row>
<entry>0.7</entry>
<entry>0.998 &#xb1; 0.002</entry>
<entry>0.727 &#xb1; 0.020</entry>
<entry>0.890 &#xb1; 0.010</entry>
<entry>0.852 &#xb1; 0.015</entry>
<entry>0.603 &#xb1; 0.007</entry>
<entry>0.592 &#xb1; 0.011</entry>
</row>
<row>
<entry>0.5</entry>
<entry>0.981 &#xb1; 0.009</entry>
<entry>0.727 &#xb1; 0.022</entry>
<entry>0.908 &#xb1; 0.007</entry>
<entry>0.870 &#xb1; 0.013</entry>
<entry>0.605 &#xb1; 0.006</entry>
<entry>0.600 &#xb1; 0.015</entry>
</row>
<row>
<entry>0</entry>
<entry>0.998 &#xb1; 0.002</entry>
<entry>0.704 &#xb1; 0.023</entry>
<entry>0.900 &#xb1; 0.007</entry>
<entry>0.870 &#xb1; 0.012</entry>
<entry>0.587 &#xb1; 0.008</entry>
<entry>0.572 &#xb1; 0.014</entry>
</row>
<row>
<entry namest="1" nameend="7" align="center" rowsep="1"/>
</row>
<row>
<entry>&#x3c1;</entry>
<entry>MG-OMP</entry>
<entry>M-OMP</entry>
<entry>G-OMP (ind)</entry>
<entry>G-OMP (joint)</entry>
<entry>OMP (ind)</entry>
<entry>OMP (joint)</entry>
</row>
<row>
<entry namest="1" nameend="7" align="center" rowsep="1"/>
</row>
<row>
<entry>0.9</entry>
<entry>1.634 &#xb1; 0.058</entry>
<entry>4.323 &#xb1; 0.686</entry>
<entry>2.237 &#xb1; 0.181</entry>
<entry>3.310 &#xb1; 0.326</entry>
<entry>4.699 &#xb1; 0.495</entry>
<entry>6.901 &#xb1; 0.776</entry>
</row>
<row>
<entry>0.7</entry>
<entry>1.611 &#xb1; 0.071</entry>
<entry>3.165 &#xb1; 0.311</entry>
<entry>1.974 &#xb1; 0.122</entry>
<entry>3.294 &#xb1; 0.337</entry>
<entry>3.879 &#xb1; 0.300</entry>
<entry>5.211 &#xb1; 0.359</entry>
</row>
<row>
<entry>0.5</entry>
<entry>1.877 &#xb1; 0.169</entry>
<entry>3.056 &#xb1; 0.257</entry>
<entry>2.003 &#xb1; 0.092</entry>
<entry>2.930 &#xb1; 0.178</entry>
<entry>4.093 &#xb1; 0.293</entry>
<entry>5.526 &#xb1; 0.357</entry>
</row>
<row>
<entry>0</entry>
<entry>1.802 &#xb1; 0.137</entry>
<entry>4.972 &#xb1; 0.684</entry>
<entry>2.252 &#xb1; 0.221</entry>
<entry>3.398 &#xb1; 0.415</entry>
<entry>4.796 &#xb1; 0.395</entry>
<entry>6.634 &#xb1; 0.553</entry>
</row>
<row>
<entry namest="1" nameend="7" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0090" num="0115">Experiment 4: Grouping structure among features and responses. The goal of this experiment was to evaluate the general setting for MG-OMP, where grouping structures exist among the features as well as among the targets.</p>
<p id="p-0091" num="0116">A model with 3rd order polynomial expansion was considered: [Y<sub>T1</sub>, . . . , Y<sub>TM</sub>]=X[A<sub>1,T1</sub>, . . . , A<sub>1,TM</sub>]+X<sup>2</sup>[A<sub>2,T1</sub>, . . . A<sub>2,TM</sub>]+X<sup>3</sup>[A<sub>3,T1</sub>, . . . , A<sub>3,TM</sub>]+E, where the T<sub>k</sub>'s are the target groups, X and E are generated according to the same distributions as in Experiment 2. For each k, each row of the concatenated matrix [A<sub>1,T1</sub>, . . . , A<sub>3,TM</sub>] was either all non-zero or all zero, according to Bernoulli draws with success probability P<sub>r</sub>=0.1. Then for each non-zero entry of A<sub>1,Tk</sub>, independently, its value was set according to N(0,1). The number of features for X is set to 20, hence the experiment considered 60 variables grouped into 20 groups corresponding the 3rd degree polynomial expansion. The number of regressions was set to 60, with 20 regression groups (T<sub>1</sub>, . . . , T<sub>20</sub>), each of size 3.</p>
<p id="p-0092" num="0117">MG-OMP with correct feature and regression groups was compared, both for C estimated from univariate OMP fits and set to identity; running MG-OMP separately for each regression groups (denoted by &#x201c;Parallel-MG-OMP&#x201d;), namely, considering parallel runs, each with a single regression group, with C set to identity (Using C estimated from univariate OMP fits has negligible impact on performance, and hence is omitted for the sake of conciseness); G-OMP and OMP were with individual and joint iteration number selection. The results are presented in Table 4 below.</p>
<p id="p-0093" num="0118">
<tables id="TABLE-US-00004" num="00004">
<table frame="none" colsep="0" rowsep="0" pgwide="1">
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="273pt" align="center"/>
<thead>
<row>
<entry namest="1" nameend="1" rowsep="1">TABLE 4</entry>
</row>
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
<row>
<entry>Average F<sub>1 </sub>score (top) and average test set squared error (bottom) for the models</entry>
</row>
<row>
<entry>output by variants of MG-OMP, Group-OMP and OMP under the setting of Experiment 4.</entry>
</row>
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="6">
<colspec colname="1" colwidth="14pt" align="center"/>
<colspec colname="2" colwidth="49pt" align="center"/>
<colspec colname="3" colwidth="49pt" align="center"/>
<colspec colname="4" colwidth="63pt" align="center"/>
<colspec colname="5" colwidth="49pt" align="center"/>
<colspec colname="6" colwidth="49pt" align="center"/>
<tbody valign="top">
<row>
<entry>&#x3c1;</entry>
<entry>MG-OMP (C)</entry>
<entry>MG-OMP (Id)</entry>
<entry>Parallel-MG-OMP</entry>
<entry>G-OMP (ind)</entry>
<entry>OMP (ind)</entry>
</row>
<row>
<entry namest="1" nameend="6" align="center" rowsep="1"/>
</row>
<row>
<entry>0.9</entry>
<entry>0.863 &#xb1; 0.003</entry>
<entry>0.818 &#xb1; 0.003</entry>
<entry>0.762 &#xb1; 0.003</entry>
<entry>0.646 &#xb1; 0.007</entry>
<entry>0.517 &#xb1; 0.006</entry>
</row>
<row>
<entry>0.7</entry>
<entry>0.850 &#xb1; 0.002</entry>
<entry>0.806 &#xb1; 0.003</entry>
<entry>0.757 &#xb1; 0.003</entry>
<entry>0.631 &#xb1; 0.008</entry>
<entry>0.517 &#xb1; 0.007</entry>
</row>
<row>
<entry>0.5</entry>
<entry>0.850 &#xb1; 0.003</entry>
<entry>0.802 &#xb1; 0.004</entry>
<entry>0.766 &#xb1; 0.004</entry>
<entry>0.641 &#xb1; 0.006</entry>
<entry>0.525 &#xb1; 0.007</entry>
</row>
<row>
<entry>0</entry>
<entry>0.847 &#xb1; 0.004</entry>
<entry>0.848 &#xb1; 0.004</entry>
<entry>0.783 &#xb1; 0.004</entry>
<entry>0.651 &#xb1; 0.007</entry>
<entry>0.525 &#xb1; 0.007</entry>
</row>
<row>
<entry namest="1" nameend="6" align="center" rowsep="1"/>
</row>
<row>
<entry>&#x3c1;</entry>
<entry>MG-OMP (C)</entry>
<entry>MG-OMP (Id)</entry>
<entry>Parallel-MG-OMP</entry>
<entry>G-OMP (ind)</entry>
<entry>OMP (ind)</entry>
</row>
<row>
<entry namest="1" nameend="6" align="center" rowsep="1"/>
</row>
<row>
<entry>0.9</entry>
<entry>3.009 &#xb1; 0.234</entry>
<entry>3.324 &#xb1; 0.273</entry>
<entry>4.086 &#xb1; 0.169</entry>
<entry>6.165 &#xb1; 0.317</entry>
<entry>6.978 &#xb1; 0.206</entry>
</row>
<row>
<entry>0.7</entry>
<entry>3.114 &#xb1; 0.252</entry>
<entry>3.555 &#xb1; 0.287</entry>
<entry>4.461 &#xb1; 0.159</entry>
<entry>8.170 &#xb1; 0.328</entry>
<entry>&#x2002;8.14 &#xb1; 0.390</entry>
</row>
<row>
<entry>0.5</entry>
<entry>3.117 &#xb1; 0.234</entry>
<entry>3.630 &#xb1; 0.281</entry>
<entry>4.499 &#xb1; 0.288</entry>
<entry>7.305 &#xb1; 0.331</entry>
<entry>8.098 &#xb1; 0.323</entry>
</row>
<row>
<entry>0</entry>
<entry>3.124 &#xb1; 0.256</entry>
<entry>3.123 &#xb1; 0.262</entry>
<entry>3.852 &#xb1; 0.185</entry>
<entry>6.137 &#xb1; 0.330</entry>
<entry>7.414 &#xb1; 0.331</entry>
</row>
<row>
<entry namest="1" nameend="6" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0094" num="0119">Overall, in all the settings with considered, MG-OMP was superior both in terms of prediction and variable selection accuracy, and more so when the correlation between responses increases. Note that even MG-OMP even with an imperfect estimate for C such as the identity was quite beneficial, and one of its merits came from the fact that model selection (i.e. namely stopping the procedure at a certain point) seemed to be more stable when stopping was performed once at a global level for all the regressions, rather than K times, once for each response.</p>
<p id="h-0007" num="0000">Application to Vector Autoregressive Models and Causal Influence Detection</p>
<p id="p-0095" num="0120">One area of application of MG-OMP is the area of causal influence detection, as disclosed in YOR920100188US1, incorporated by reference above. As discussed in that application, MG-OMP is one embodiment.</p>
<p id="p-0096" num="0121">Various aspects of the present disclosure may be embodied as a program, software, or computer instructions embodied in a computer or machine usable or readable medium, which causes the computer or machine to perform the steps of the method when executed on the computer, processor, and/or machine.</p>
<p id="p-0097" num="0122">The system and method of the present disclosure may be implemented and run on a general-purpose computer or special-purpose computer system. The computer system may be any type of known or will be known systems and may typically include a processor, memory device, a storage device, input/output devices, internal buses, and/or a communications interface for communicating with other computer systems in conjunction with communication hardware and software, etc.</p>
<p id="p-0098" num="0123">The terms &#x201c;computer system&#x201d; and &#x201c;computer network&#x201d; as may be used in the present application may include a variety of combinations of fixed and/or portable computer hardware, software, peripherals, and storage devices. The computer system may include a plurality of individual components that are networked or otherwise linked to perform collaboratively, or may include one or more stand-alone components. The hardware and software components of the computer system of the present application may include and may be included within fixed and portable devices such as desktop, laptop, server. A module may be a component of a device, software, program, or system that implements some &#x201c;functionality&#x201d;, which can be embodied as software, hardware, firmware, electronic circuitry, or etc.</p>
<p id="p-0099" num="0124">The flowchart and block diagrams in the Figures illustrate the architecture, functionality, and operation of possible implementations of systems, methods and computer program products according to various embodiments of the present invention. In this regard, each block in the flowchart or block diagrams may represent a module, segment, or portion of code, which comprises one or more executable instructions for implementing the specified logical function(s). It should also be noted that, in some alternative implementations, the functions noted in the block may occur out of the order noted in the figures. For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or the blocks may sometimes be executed in the reverse order, depending upon the functionality involved. It will also be noted that each block of the block diagrams and/or flowchart illustration, and combinations of blocks in the block diagrams and/or flowchart illustration, can be implemented by special purpose hardware-based systems that perform the specified functions or acts, or combinations of special purpose hardware and computer instructions.</p>
<p id="p-0100" num="0125">From reading the present disclosure, other modifications will be apparent to persons skilled in the art. Such modifications may involve other features which are already known in the field and which may be used instead of or in addition to features already described herein. Although claims have been formulated in this application to particular combinations of features, it should be understood that the scope of the disclosure of the present application also includes any novel feature or novel combination of features disclosed herein either explicitly or implicitly or any generalization thereof, whether or not it mitigates any or all of the same technical problems as does the present invention. The applicants hereby give notice that new claims may be formulated to such features during the prosecution of the present application or any further application derived therefrom.</p>
<p id="p-0101" num="0126">The word &#x201c;comprising&#x201d;, &#x201c;comprise&#x201d;, or &#x201c;comprises&#x201d; as used herein should not be viewed as excluding additional elements. The singular article &#x201c;a&#x201d; or &#x201c;an&#x201d; as used herein should not be viewed as excluding a plurality of elements. Unless the word &#x201c;or&#x201d; is expressly limited to mean only a single item exclusive from other items in reference to a list of at least two items, then the use of &#x201c;or&#x201d; in such a list is to be interpreted as including (a) any single item in the list, (b) all of the items in the list, or (c) any combination of the items in the list. Use of ordinal numbers, such as &#x201c;first&#x201d; or &#x201c;second,&#x201d; is for distinguishing otherwise identical terminology, and is not intended to imply that operations or steps must occur in any particular order, unless otherwise indicated.</p>
<p id="p-0102" num="0127">Where software or algorithms are disclosed, anthropomorphic or thought-like language may be used herein. There is, nevertheless, no intention to claim human thought or manual operations, unless otherwise indicated. All claimed operations are intended to be carried out automatically by hardware or software.</p>
<p id="p-0103" num="0128">Where software or hardware is disclosed, it may be drawn with boxes in a drawing. These boxes may in some cases be conceptual. They are not intended to imply that functions described with respect to them could not be distributed to multiple operating entities; nor are they intended to imply that functions could not be combined into one module or entity unless otherwise indicated.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08626680-20140107-M00001.NB">
<img id="EMI-M00001" he="19.05mm" wi="76.20mm" file="US08626680-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US08626680-20140107-M00002.NB">
<img id="EMI-M00002" he="14.82mm" wi="76.20mm" file="US08626680-20140107-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00003 MATH-US-00003-2" nb-file="US08626680-20140107-M00003.NB">
<img id="EMI-M00003" he="13.38mm" wi="76.20mm" file="US08626680-20140107-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>We claim:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A computer method comprising carrying out operations on at least one data processing device, the operations comprising:
<claim-text>maintaining a machine readable embodiment on a medium of data, the data being organized into at least one matrix data structure having a plurality of dimensions;</claim-text>
<claim-text>imposing a group structure on the data, which group structure defines input and output blocks, at least one of the input and output blocks encompassing at least first and second data points in each of at least first and second dimensions;</claim-text>
<claim-text>performing multivariate regression in accordance with a coefficient matrix that is organized into coefficient blocks, the coefficient blocks linking the input and output blocks, the multivariate regression comprising, iteratively
<claim-text>refining the coefficient blocks and</claim-text>
<claim-text>selecting pertinent ones of the input and output blocks,</claim-text>
</claim-text>
<claim-text>responsive to results of such iteration;</claim-text>
<claim-text>presenting at least one user accessible machine representation of at least one conclusion about at least one relationship between the pertinent ones of the input and output blocks, responsive to the coefficient matrix.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein refining coefficient blocks comprises imposing a sparsity structure on the coefficient matrix.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein input groups are groups of input features and output groups are groups of regressions.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein
<claim-text>the data comprises results of activities of a set of originating community members; and</claim-text>
<claim-text>an output of the regression indicates causal influence by at least one community member on another community member.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the output comprises a ranking of community members.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>:
<claim-text>i) wherein refining comprises, not necessarily in the following order:
<claim-text>a) first selecting at least one of the input blocks</claim-text>
<claim-text>b) second selecting at least one of the output blocks; and</claim-text>
<claim-text>c) revising the first and second selecting to reduce residual loss; and</claim-text>
</claim-text>
<claim-text>ii) performing multivariate regression further comprises, for each iteration, calculating an updated residual loss responsive to a prior multivariate regression for the input and output blocks.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein minimizing residual loss comprises comparing results from more than one iteration.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein an arbitrary component regression method is allowed for performing the regression in each iteration.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the operations further comprise taking into account additional information provided by an estimate of the error covariance matrix by incorporating this estimate into a loss function to be minimized.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein regression coefficients are re-estimated in each iteration.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. A computer method comprising:
<claim-text>maintaining embodiments of at least first and second groups of content on a medium readable by the computer, such that each group is a representation of manifestations of activity of at least respective first and second originating community members;</claim-text>
<claim-text>performing spatiotemporal modeling with respect to all members of the first and second groups simultaneously;
<claim-text>wherein a group of regressions are modeled together, using a same set of input variable groups as predictors for the group of regressions, said modeling comprising:</claim-text>
<claim-text>jointly determining, for a whole regression group, whether to include or exclude input variable groups in a set of input variable groups, said joint determining including receiving information on a grouping structure between input variables and on a grouping structure between the regressions, and imposing, based on said information, a sparsity structure on estimated regression coefficients based on the grouping structure information to guide a variable selection;</claim-text>
<claim-text>determining an estimate of an error covariance matrix; and</claim-text>
<claim-text>performing an iterative variable block selection comprising: selecting an input variable group of interest and a regression group of interest in each iteration for inclusion in said model based on an estimated reduction of residual loss in the model from said error matrix; and</claim-text>
<claim-text>in subsequent iterations, selecting a variable block based on the estimated residual loss reduction of the model when the input variable group and corresponding regression group are included in the model in addition to those input variable and corresponding regression groups selected in earlier iterations, and</claim-text>
</claim-text>
<claim-text>inferring a relationship between the first and second community members responsive to the modeling.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. A system comprising:
<claim-text>at least one storage device for embodying data and/or program code in a machine usable form;</claim-text>
<claim-text>at least one processor for performing operations in conjunction with the storage device, the operations comprising:
<claim-text>maintaining a machine readable embodiment on a medium of data, the data being organized into at least one matrix data structure having a plurality of dimensions;</claim-text>
<claim-text>imposing a group structure on the data, which group structure defines input and output blocks, at least one of the input and output blocks encompassing at least first and second data points in each of at least first and second dimensions;</claim-text>
<claim-text>performing multivariate regression in accordance with a coefficient matrix that is organized into coefficient blocks, the coefficient blocks linking the input and output blocks, the multivariate regression comprising, iteratively
<claim-text>refining the coefficient blocks and</claim-text>
<claim-text>selecting pertinent ones of the input and output blocks,</claim-text>
</claim-text>
<claim-text>responsive to results of such iteration;</claim-text>
<claim-text>presenting at least one user accessible machine representation of at least one conclusion about at least one relationship between the pertinent ones of the input and output blocks, responsive to the coefficient matrix.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein refining coefficient blocks comprises imposing a sparsity structure on the coefficient matrix.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein input groups are groups of input features and output groups are groups of regressions.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein
<claim-text>the data comprises results of activities of a set of originating community members; and</claim-text>
<claim-text>an output of the regression indicates causal influence by at least one community member on another community member.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the output comprises a ranking of community members.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. A computer program product for identifying influence, the computer program product comprising a non-transitory storage medium readable by a processing circuit and storing instructions to be run by the processing circuit for performing a method comprising
<claim-text>maintaining a machine readable embodiment on a medium of data, the data being organized into at least one matrix data structure having a plurality of dimensions;</claim-text>
<claim-text>imposing a group structure on the data, which group structure defines input and output blocks, at least one of the input and output blocks encompassing at least first and second data points in each of at least first and second dimensions;</claim-text>
<claim-text>performing multivariate regression in accordance with a coefficient matrix that is organized into coefficient blocks, the coefficient blocks linking the input and output blocks, the multivariate regression comprising, iteratively
<claim-text>refining the coefficient blocks and</claim-text>
<claim-text>selecting pertinent ones of the input and output blocks,</claim-text>
</claim-text>
<claim-text>responsive to results of such iteration;</claim-text>
<claim-text>presenting at least one user accessible machine representation of at least one conclusion about at least one relationship between the pertinent ones of the input and output blocks, responsive to the coefficient matrix.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The program product of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein refining coefficient blocks comprises imposing a sparsity structure on the coefficient matrix.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The program product of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein input groups are groups of input features and output groups are groups of regressions.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The program product of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein
<claim-text>the data comprises results of activities of a set of originating community members; and</claim-text>
<claim-text>an output of the regression indicates causal influence by at least one community member on another community member.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The program product of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the output comprises a ranking of community members.</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The program product of <claim-ref idref="CLM-00017">claim 17</claim-ref>,
<claim-text>i) wherein refining comprises, not necessarily in the following order:
<claim-text>a) first selecting at least one of the input blocks</claim-text>
<claim-text>b) second selecting at least one of the output blocks; and</claim-text>
<claim-text>c) revising the first and second selecting to reduce residual loss; and</claim-text>
</claim-text>
<claim-text>ii) performing multivariate regression further comprises, for each iteration, calculating an updated residual loss responsive to a prior multivariate regression for the input and output blocks.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The program product of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein minimizing residual loss comprises comparing results from more than one iteration.</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. The program product of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein an arbitrary component regression method is allowed for performing the regression in each iteration.</claim-text>
</claim>
<claim id="CLM-00025" num="00025">
<claim-text>25. The program product of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein the operations further comprise taking into account additional information provided by an estimate of the error covariance matrix by incorporating this estimate into a loss function to be minimized.</claim-text>
</claim>
</claims>
</us-patent-grant>
