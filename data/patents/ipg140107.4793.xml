<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625886-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625886</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13022877</doc-number>
<date>20110208</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<us-term-of-grant>
<us-term-extension>201</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>62</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382159</main-classification>
<further-classification>382155</further-classification>
<further-classification>382321</further-classification>
<further-classification>382305</further-classification>
</classification-national>
<invention-title id="d2e53">Finding repeated structure for data extraction from document images</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>2007/0280530</doc-number>
<kind>A1</kind>
<name>Fung et al.</name>
<date>20071200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382159</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>2010/0253967</doc-number>
<kind>A1</kind>
<name>Privault et al.</name>
<date>20101000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>358  115</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>2011/0078191</doc-number>
<kind>A1</kind>
<name>Ragnet et al.</name>
<date>20110300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>707780</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>2012/0203721</doc-number>
<kind>A1</kind>
<name>Bart et al.</name>
<date>20120800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>706 13</main-classification></classification-national>
</us-citation>
<us-citation>
<nplcit num="00005">
<othercit>Information extraction&#x2014;structure., Sarkar et al., ACM, 978-1-60558-773-8, Jun. 9-11 2010, pp. 1-8.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00006">
<othercit>Information extraction&#x2014;structure., Sarkar et al., ACM, 978-1-60558-773-8, Jun 9-11 2010, pp. 1-8.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00007">
<othercit>Design&#x2014;tables., Silva et al., IJODA&#x26;R, 10.1007/s10032-005-0001-x, 2006, pp. 144-171.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00008">
<othercit>Y. Bela&#xef;d and A. Bela&#xef;d. Morphological tagging approach in document analysis of invoices. In ICPR, 2004.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00009">
<othercit>C. Chow and C. Liu. Approximating discrete probability distributions with dependence trees. Information Theory, 14(11):462-467, 1968.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00010">
<othercit>R. Fergus, P. Perona, and A. Zisserman. Object class recognition by unsupervised scale-invariant learning. In CVPR, Jun. 2003.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00011">
<othercit>N. Friedman, D. Geiger, and M. Goldszmidt. Bayesian network classifiers. Machine Learning, 29:131-163, 1997.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00012">
<othercit>H. Hamza, Y. Bela&#xef;d, and A. Bela&#xef;d. Case-based reasoning for invoice analysis and recognition. In Proceedings of the 7th International conference on Case-Based Reasoning, 2007.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00013">
<othercit>T. Hassan. User-guided wrapping of pdf documents using graph matching techniques. In ICDAR, 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00014">
<othercit>B. Klein, S. Agne, and A. Dengel. Results of a study on invoice-reading systems in Germany. In DAS, 2004.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00015">
<othercit>B. Klein, S. Gokkus, T. Kieninger, and A. Dengel. Three approaches to &#x201c;industrial&#x201d; table spotting. In ICDAR, 2001.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00016">
<othercit>P. Sarkar and E. Saund. Perceptual organization in semantic role labeling. In Proceedings of 2005 Symposium on Document Image Understanding Technology, 2005.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00017">
<othercit>P. Sarkar and E. Saund. On the reading of tables of contents. In DAS, 2008.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00018">
<othercit>M. Weber, M. Welling, and P. Perona. Towards automatic discovery of object categories. In CVPR, 2000.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>19</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382155</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382159</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382321</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382305</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>5</number-of-drawing-sheets>
<number-of-figures>5</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120201457</doc-number>
<kind>A1</kind>
<date>20120809</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Bart</last-name>
<first-name>Evgeniy</first-name>
<address>
<city>Sunnyvale</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Sarkar</last-name>
<first-name>Prateek</first-name>
<address>
<city>Sunnyvale</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Saund</last-name>
<first-name>Eric</first-name>
<address>
<city>San Carlos</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Bart</last-name>
<first-name>Evgeniy</first-name>
<address>
<city>Sunnyvale</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Sarkar</last-name>
<first-name>Prateek</first-name>
<address>
<city>Sunnyvale</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Saund</last-name>
<first-name>Eric</first-name>
<address>
<city>San Carlos</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Fay Sharpe LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Palo Alto Research Center Incorporated</orgname>
<role>02</role>
<address>
<city>Palo Alto</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Patel</last-name>
<first-name>Jayesh A</first-name>
<department>2669</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Methods and system employing the same for finding repeated structure for data extraction from document images are provided. A reference record and one or more reference fields thereof are identified from a document image. One or more candidate fields are generated for each of the reference fields. One or more best candidate records from the candidate fields are selected using a probabilistic model and an optimal record set is determined from the best candidate records.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="159.34mm" wi="144.02mm" file="US08625886-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="174.07mm" wi="156.80mm" file="US08625886-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="176.61mm" wi="150.45mm" file="US08625886-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="209.30mm" wi="168.32mm" file="US08625886-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="113.79mm" wi="123.02mm" file="US08625886-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="196.51mm" wi="168.99mm" orientation="landscape" file="US08625886-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND</heading>
<p id="p-0002" num="0001">The present exemplary embodiments relate generally to document processing. They find particular application in conjunction with finding repeated structure within document images for data extraction, and will be described with particular reference thereto. However, it is to be appreciated that the present exemplary embodiments are also amenable to other like applications.</p>
<p id="p-0003" num="0002">Many documents contain repeated structure. Forms, templates, and letterheads are examples of structure repeated across multiple documents. Layouts of headings, body text, and captions are examples of structure repeated across pages within a single document. Bulleted lists and tabular data are examples of structure repeated within a single page of a document. Prominent examples of the latter are invoices, receipts, and many healthcare forms. In these cases, instances of repeated structure of interest are called records. For example, in an invoice each record corresponds to a single product or service purchased. Records are composed of individual fields (such as &#x2018;unit price&#x2019; or &#x2018;quantity&#x2019;). These fields are laid out in a consistent (but unknown in advance) spatial structure.</p>
<p id="p-0004" num="0003">Repeated structure conveys vital perceptual and semantic cues to a human reader. The relationships among the elements are encoded implicitly via the spatial structure. Identifying and extracting repeated structure is useful in a variety of applications. For example, product names extracted from an invoice can be matched to an database to verify receipt before remitting payment. As another example, in the healthcare domain, blood test results often describe each test performed. The results of these individual tests can be extracted, accumulated, and plotted as a function of time to display trends.</p>
<p id="p-0005" num="0004">Despite its recognized value in business workflows, data extraction tasks suffer from inadequate or unreliable levels of automation. Consequently, data extraction is still largely done manually. However, the cost of manual data extraction can be quite high. For example, manually processing a single invoice can cost up to 9 Euro. For large businesses which can process tens of thousands of invoices per day, manual processing can dramatically increase the cost of operations. Accordingly, there is a strong need for a reliable, automatic approach to data extraction.</p>
<p id="p-0006" num="0005">Automatic extraction of repeated structure from documents is a challenging task for a number of reasons. Variations in the content of individual fields induce significant variability in the structure. This variability includes changes in the field's visual appearance, as well as width and height. These changes, in turn, induce variations in the relative placement of other fields and in the presence and appearance of field separators. Many cues typically used for data extraction become difficult to exploit in these circumstances. For example, while whitespace gaps form a useful cue for field boundaries, they may be absent in some documents due to overlaps between different fields (interlacing). As another example, if different records occupy a different number of text lines, the periodicity structure will be disrupted as well, and the relative positions of different fields will not be consistent across items. When dealing with repeated structure across multiple documents, variations in the generation of these documents present an additional problem. For example, while the layout of a company's invoices may be centrally specified, local branches may deviate from this layout in different ways. Similarly, although standards governing the layout of medical claims forms exist, different hospitals deviate from this layout in unpredictable ways. In addition, variations and problems in the scanning process (such as paper slipping or other distortions) are also unpredictable, vary from document to document, and introduce an additional source of layout variability. All these difficulties make data extraction difficult to approach with shrink-wrapped automated solutions.</p>
<p id="p-0007" num="0006">One approach to automatic data extraction is called &#x2018;wrapping&#x2019;. In wrapping, one instance of the structure to be extracted is marked by a user. Subsequently, the approach &#x2018;wraps&#x2019; (finds and extracts) additional instances of this structure. This wrapping is based on subgraph matching. The biggest drawback of this approach is that it does not learn from the annotation specified by the user. Instead, the user manually specifies the conditions for subgraphs to match. This requires significant effort and technical expertise. In addition, the specific set of conditions used in the wrapping approach may not be powerful or flexible enough for some problems.</p>
<p id="p-0008" num="0007">Another approach, which is specific to extracting information from invoices, extracts consecutive lines that share similar token structure. Similarity is measured by token content (numerical/alphabetic/alphanumeric) as well as by token alignment. In this approach, &#x2018;main lines&#x2019; are identified as text lines containing a real number (which usually corresponds to the price). Projection profiles on the main lines are then used to find columns. Periodicity structure of the main lines is used to assign the invoice to one of several predefined types.</p>
<p id="p-0009" num="0008">In yet another approach, which is again specific to extracting information from invoices, similar ideas as the previous approach are used. According to this approach, a database of known invoice structures is used to extract structure from invoices of familiar types and token alignment is used as a cue to identify repeated structure in unfamiliar invoices.</p>
<p id="p-0010" num="0009">Although some of the approaches described above are used in practice, it is desirable to extend their range of applicability. Namely, most of the current approaches assume fields are organized in widely separated columns and that no interlacing is present. Further, most current approaches assume records are periodic (i.e., the heights of line items are equal). As a result, when these assumptions fail, documents cannot be processed.</p>
<p id="p-0011" num="0010">One reason for these limitations is that the cues used for making the decisions are relatively weak. For example, while the alignment of different fields may indicate a match, it is by no means a guarantee of a match (because non-matching fields are often aligned by accident, while matching fields may occasionally be misaligned). Another reason is that the decision function used to integrate the cues is often ad-hoc, relying on a series of thresholds to select a set of matches. Using hard thresholds is particularly problematic since, as mentioned above, any single cue might fail for a particular document.</p>
<p id="p-0012" num="0011">The present disclosure contemplates new and improved systems and/or methods for document image analysis, including systems and/or methods that remedy these and other problems.</p>
<heading id="h-0002" level="1">INCORPORATION BY REFERENCE</heading>
<p id="p-0013" num="0012">U.S. patent application Ser. No. 13/022,952 for &#x201c;System and Method for Efficient Interpretation of Natural Images and Document Images in Terms of Objects and Their Parts,&#x201d; by Bart et al., filed on even date herewith, is hereby incorporated herein by reference in its entirety.</p>
<heading id="h-0003" level="1">BRIEF DESCRIPTION</heading>
<p id="p-0014" num="0013">Various details of the present disclosure are hereinafter summarized to provide a basic understanding. This summary is not an extensive overview of the disclosure and is intended neither to identify certain elements of the disclosure, nor to delineate the scope thereof. Rather, the primary purpose of the summary is to present certain concepts of the disclosure in a simplified form prior to the more detailed description that is presented hereinafter. Most of the discussion will focus on the case of invoice parsing, wherein the structure repeats within a single page of the document. It is to be appreciated that the proposed methods are equally applicable to other document types, including those where the structure repeats over multiple pages.</p>
<p id="p-0015" num="0014">According to one aspect of the present disclosure, a method for finding repeated structure for data extraction from document images is provided. A reference record and one or more reference fields thereof are identified from a document image. One or more candidate fields are generated for each of the reference fields and one or more candidate records are generated from the candidate fields. Note that candidate record generation is not exhaustive; only as many records as necessary to find the optima are generated. One or more best candidate records from the candidate records are selected using a probabilistic model and an optimal record set is determined from the best candidate records.</p>
<p id="p-0016" num="0015">According to another aspect of the present disclosure, a document processing system for finding repeated structure for data extraction from document images is provided. The system includes an imaging device and/or a document conversion system and a data extraction system. The imaging device and/or the document conversion system convert documents, paper, electronic or otherwise, into document images, and the data extraction system extracts repeated structure from document images using a probabilistic model for cue integration.</p>
<p id="p-0017" num="0016">According to another aspect of the present disclosure, a system for finding repeated structure for data extraction from document images is provided. The system includes a non-transient computer readable medium having computer executable instructions. These instructions identify a reference record and one or more reference fields thereof, from a document image; generate one or more candidate fields for each of the reference fields; find one or more best candidate records from the candidate fields using a probabilistic model; and determine an optimal record set from the best candidate records. The system further includes one or more processors that execute the computer executable instructions to find repeated structure in document images.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0018" num="0017">The following description and drawings set forth certain illustrative implementations of the disclosure in detail, which are indicative of several exemplary ways in which the various principles of the disclosure may be carried out. The illustrative examples, however, are not exhaustive of the many possible embodiments of the disclosure. Other objects, advantages and novel features of the disclosure will be set forth in the following detailed description of the disclosure when considered in conjunction with the drawings, in which:</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 1</figref> illustrates a method of extracting repeated structure within a page of a document according to the present disclosure;</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 2</figref> illustrates a display having a document image of an invoice;</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 3</figref> illustrates a method of extracting repeated structure within and/or across pages of one and/or multiple documents according to the present disclosure;</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 4</figref> illustrates a data extraction system according to the present disclosure; and,</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 5</figref> illustrates a document processing system suitably employing a data extraction system of the present disclosure.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0024" num="0023">One or more embodiments or implementations are hereinafter described in conjunction with the drawings, where like reference numerals are used to refer to like elements throughout, and where the various features are not necessarily drawn to scale. It is to be appreciated that while the descriptions center on the case of finding repeated structure within one page of a document, the same methods apply equally to finding repeated structure within multiple pages of a document and within and/or across multiple documents.</p>
<p id="h-0006" num="0000">1.0 Overview</p>
<p id="p-0025" num="0024">Repetition of structure is prevalent in documents. In document design, such repetition conveys the underlying logical and function structure of the data. For example, in invoices, the names, unit prices, quantities and other descriptors of every line item are arranged in consistent spatial structure. Disclosed herein, methods and systems employing the same provide a general approach to extracting such repeated structure from documents. The structural similarity may be measured by a wide variety of perceptually motivated cues, such as alignment and saliency. These cues may then be combined in a probabilistic model, and repeated structure instances are found by applying a novel algorithm for exact probabilistic inference in this model.</p>
<p id="h-0007" num="0000">2.0 The Basic Approach</p>
<p id="p-0026" num="0025">With reference to <figref idref="DRAWINGS">FIG. 1</figref>, a method <b>100</b> of extracting repeated structure within a page of a document is illustrated. The document may be an invoice, a receipt, a healthcare document, or other documents or sets of documents having repeated structure therein. The method <b>100</b> may include generating <b>102</b> a document image, identifying <b>104</b> a reference record and one or more reference fields thereof, from the document image, generating <b>106</b> one or more candidate fields (or candidate matches) for each of the reference fields, selecting <b>110</b> one or more best candidate records from the candidate records (using a routine for generating candidate records from the candidate fields <b>108</b>), determining <b>112</b> an optimal record set from the best candidate records, and extracting <b>114</b> data from the optimal record set. Note that the probabilistic model used in step <b>110</b> needs to be pre-trained for this process. This pre-training is described below.</p>
<p id="p-0027" num="0026">The generation <b>102</b> of the document image may include converting the original incoming document representation into image form. If the original documents are hard (paper) copies, this image form can be generated using one or more of a camera, scanner, imaging device, and the like, as needed. In case the original documents are electronic (for example, in PDF format), the image form may be generated by converting it suitably to an image format. In this case, some information from the original document may be extracted and preserved for future use, in addition to the image itself. This may include, for example, the text content (if available electronically), which may subsequently be used directly instead of performing OCR on the image. The generation <b>102</b> of the document image may further include initially processing the document image. The initial processing may include deskewing, thresholding, and the like.</p>
<p id="p-0028" num="0027">The identification <b>104</b> of a reference record and one or more reference fields thereof, from the document image, may include automatically identifying the reference record and the one or more reference fields. However, suitably the identification <b>104</b> includes receiving annotations from a user of the method <b>100</b>. Also note that the record and field identification may happen in a batch mode in a separate step. In this case, the user performs this identification ahead of time, and at run time the system simply retrieves the previously annotated record, for example, from a disk drive or from a network. Even more, it is to be appreciated that although most of the discussions below consider only one example of each field being identified (called the &#x2018;one-shot&#x2019; case), using additional examples is readily possible, using standard techniques from computer vision and machine learning, and may be advantageous in certain applications.</p>
<p id="p-0029" num="0028">For user-based identification, the user may be presented with the document image via a display. The display may, for example, include one or more of a computer display, a projector, a LCD, a plasma display, and the like. Upon being presented with the document image, the user suitably identifies a record and its corresponding fields via the user input device. However, in other embodiments, the user may only identify a record via the user input device. In such embodiments, the identification <b>104</b> may further include identifying the individual fields of the identified record automatically. The user input device may include one or more of a mouse, keyboard, button, a dial, a touch-screen, and the like.</p>
<p id="p-0030" num="0029">To identify the record and/or its corresponding fields, the user may draw one or more bounding boxes around the record and/or the fields using the user input device. In some of such embodiments, the bounding boxes may automatically snap to tokens to avoid the need for the user to precisely control the drawing. Alternatively, or additionally, the user may select tokens belonging to each field instead of drawing bounding boxes via the user input device. In embodiments involving tokens, the identification <b>104</b> may further include extracting tokens using optical character recognition (OCR) or using tokens extracted directly from the original electronic document, if available. Tokens usually correspond to individual words and/or characters.</p>
<p id="p-0031" num="0030">With reference to <figref idref="DRAWINGS">FIG. 2</figref>, a display <b>200</b> displaying a document image <b>202</b> of an invoice is provided. The document image <b>202</b> includes a plurality of records <b>204</b>, where the records includes a plurality of fields <b>206</b>. As shown, a user annotated the document image <b>202</b> by drawing bounding boxes <b>208</b> around one of the records <b>204</b><i>a </i>and its corresponding fields. Note that the record bounding box <b>208</b><i>b </i>is not necessary in this case since it can be inferred automatically from the field bounding boxes.</p>
<p id="p-0032" num="0031">Referring back to <figref idref="DRAWINGS">FIG. 1</figref>, in certain embodiments, the identification <b>104</b> may further include attempting to identify a document type of the document image before the user annotates the document image. In such embodiments, if the document type is identified, then annotations from the user may not be needed. In this manner, if many similar document images with identical layouts need to be processed only one of these document images may need to be annotated.</p>
<p id="p-0033" num="0032">The generation <b>106</b> of one or more candidate fields (or candidate matches) for each of the reference fields may include extracting tokens from the document image and generating all contiguous blocks of the tokens. As noted above, the tokens may be extracted by OCR (which also gives their text content) or from the original electronic document, and usually correspond to individual words or characters. It is advantageous to generate all contiguous blocks of the tokens because fields in document images generally consist of multiple words and/or span multiple text lines, whereby good matches for a reference field generally consist of multiple tokens. These contiguous blocks may then be used as an initial pool of candidate fields for each of the reference fields.</p>
<p id="p-0034" num="0033">While this initial pool could be used as the set of candidate fields for each of the reference fields, there are many (sometimes tens of thousands) candidate fields in this pool and considering all of them would be time-consuming. Therefore, in certain embodiments, the generation <b>106</b> may further include, for each reference field, discarding all candidate fields that are unlikely to participate in a good match with the corresponding reference field. This filtering process produces a much shorter list of candidate fields (e.g., between 20 and 200) for each reference field, which, as should be appreciated, will generally be different for each reference field.</p>
<p id="p-0035" num="0034">So as to perform this filtering, a probabilistic model, described below, may be employed to evaluate the likelihood of potential matches. However, it is to be appreciated that other heuristics (for example, filtering candidate fields by size) may be also used as appropriate. The probabilistic model may include features that measure how likely a given candidate field B is to match the corresponding reference field R. The values of these features (denoted f<sub>s</sub>(B, R)) may be computed for each candidate field B and candidate fields with scores less than a threshold may be removed from the candidate list.</p>
<p id="p-0036" num="0035">The thresholds may be learned in a training stage and depend on f<sub>s</sub>(B, R), as well as on f<sub>s</sub>(R, R) (the corresponding feature value computed for the reference field R itself). The latter dependency is useful for features that measure a field's perceptual coherence. For example, most fields do not contain large whitespace gaps, whereby a candidate field with such a gap is unlikely to be a good match and can be discarded. However, if the reference field itself contains a large gap, then it becomes plausible that a candidate field might contain a gap as well. Candidate fields with large gaps are therefore no longer considered unlikely and need not be removed.</p>
<p id="p-0037" num="0036">The generation <b>108</b> of one or more candidate records from the candidate fields suitably includes generating each possible combination of the candidate fields discussed above, in an order suitable for the selection <b>110</b>. Put another way, the generation <b>108</b> suitably includes generating records that can be derived from the candidate fields previously determined. Note again that this generation is not exhaustive; rather, candidate records are generated on demand and in an order suitable for the selection <b>110</b>.</p>
<p id="p-0038" num="0037">The selection <b>110</b> of the one or more best candidate records from the candidate records may include finding candidate records having a best match quality with the reference record. Suitably, match quality is assessed using a probabilistic model, but other means of assessing match quality are equally amenable.</p>
<p id="p-0039" num="0038">To assess a match quality of a candidate record using a probabilistic model, denote the n reference fields of a reference record R by the sequence {R<sub>i</sub>}<sub>i=1</sub><sup>n</sup>, and denote the n candidate fields (one candidate field for each reference field) of a candidate record B by the sequence {B<sub>i</sub>}<sub>i=1</sub><sup>n</sup>, where a candidate field for a reference field R<sub>i </sub>is denoted B. The match quality of a candidate record may then be evaluated by modeling the class conditional probability of the match,
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>p</i>({<i>B</i><sub>i</sub>}<sub>i=1</sub><sup>n</sup><i>,{R</i><sub>i</sub>}<sub>i=1</sub><sup>n</sup><i>|C</i>&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
The class variable C may take on two values: &#x2018;match&#x2019;, or M, and &#x2018;no match&#x2019;, or <o ostyle="single">M</o>.
</p>
<p id="p-0040" num="0039">For efficiency, the following specialized form of the model may be employed:</p>
<p id="p-0041" num="0040">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>p</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <msubsup>
                <mrow>
                  <mo>{</mo>
                  <msub>
                    <mi>B</mi>
                    <mi>i</mi>
                  </msub>
                  <mo>}</mo>
                </mrow>
                <mrow>
                  <mi>i</mi>
                  <mo>=</mo>
                  <mn>1</mn>
                </mrow>
                <mi>n</mi>
              </msubsup>
              <mo>,</mo>
              <mrow>
                <msubsup>
                  <mrow>
                    <mo>{</mo>
                    <msub>
                      <mi>R</mi>
                      <mi>i</mi>
                    </msub>
                    <mo>}</mo>
                  </mrow>
                  <mrow>
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                  </mrow>
                  <mi>n</mi>
                </msubsup>
                <mo>&#x2758;</mo>
                <mi>C</mi>
              </mrow>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <munderover>
            <mo>&#x220f;</mo>
            <mrow>
              <mi>i</mi>
              <mo>&#x2208;</mo>
              <mrow>
                <mo>[</mo>
                <mrow>
                  <mn>1</mn>
                  <mo>&#x2062;</mo>
                  <mi>&#x2026;</mi>
                  <mo>&#x2062;</mo>
                  <mstyle>
                    <mspace width="0.3em" height="0.3ex"/>
                  </mstyle>
                  <mo>&#x2062;</mo>
                  <mi>n</mi>
                </mrow>
                <mo>]</mo>
              </mrow>
            </mrow>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
          </munderover>
          <mo>&#x2062;</mo>
          <mrow>
            <mrow>
              <mi>s</mi>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <msub>
                    <mi>B</mi>
                    <mi>i</mi>
                  </msub>
                  <mo>,</mo>
                  <mrow>
                    <msub>
                      <mi>R</mi>
                      <mi>i</mi>
                    </msub>
                    <mo>&#x2758;</mo>
                    <mi>C</mi>
                  </mrow>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>*</mo>
            <mrow>
              <munderover>
                <mo>&#x220f;</mo>
                <munder>
                  <mrow>
                    <mi>i</mi>
                    <mo>,</mo>
                    <mrow>
                      <mi>j</mi>
                      <mo>&#x2208;</mo>
                      <mrow>
                        <mo>[</mo>
                        <mrow>
                          <mn>1</mn>
                          <mo>&#x2062;</mo>
                          <mstyle>
                            <mspace width="0.8em" height="0.8ex"/>
                          </mstyle>
                          <mo>&#x2062;</mo>
                          <mi>&#x2026;</mi>
                          <mo>&#x2062;</mo>
                          <mstyle>
                            <mspace width="1.1em" height="1.1ex"/>
                          </mstyle>
                          <mo>&#x2062;</mo>
                          <mi>n</mi>
                        </mrow>
                        <mo>]</mo>
                      </mrow>
                    </mrow>
                  </mrow>
                  <mrow>
                    <mi>i</mi>
                    <mo>&#x2260;</mo>
                    <mi>j</mi>
                  </mrow>
                </munder>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
              </munderover>
              <mo>&#x2062;</mo>
              <mrow>
                <mrow>
                  <mi>d</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <msub>
                        <mi>B</mi>
                        <mi>i</mi>
                      </msub>
                      <mo>,</mo>
                      <msub>
                        <mi>B</mi>
                        <mi>j</mi>
                      </msub>
                      <mo>,</mo>
                      <msub>
                        <mi>R</mi>
                        <mi>i</mi>
                      </msub>
                      <mo>,</mo>
                      <mrow>
                        <msub>
                          <mi>R</mi>
                          <mi>j</mi>
                        </msub>
                        <mo>&#x2758;</mo>
                        <mi>C</mi>
                      </mrow>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>*</mo>
                <mrow>
                  <mrow>
                    <mi>a</mi>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <msubsup>
                          <mrow>
                            <mo>{</mo>
                            <msub>
                              <mi>B</mi>
                              <mi>i</mi>
                            </msub>
                            <mo>}</mo>
                          </mrow>
                          <mrow>
                            <mi>i</mi>
                            <mo>=</mo>
                            <mn>1</mn>
                          </mrow>
                          <mi>n</mi>
                        </msubsup>
                        <mo>,</mo>
                        <mrow>
                          <msubsup>
                            <mrow>
                              <mo>{</mo>
                              <msub>
                                <mi>R</mi>
                                <mi>i</mi>
                              </msub>
                              <mo>}</mo>
                            </mrow>
                            <mrow>
                              <mi>i</mi>
                              <mo>=</mo>
                              <mn>1</mn>
                            </mrow>
                            <mi>n</mi>
                          </msubsup>
                          <mo>&#x2758;</mo>
                          <mi>C</mi>
                        </mrow>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mo>.</mo>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>2</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
However, in other embodiments, additional terms may be incorporated. For example, terms that involve three candidate fields may be useful to incorporate relations such as in between'.
</p>
<p id="p-0042" num="0041">s( ) models the probability that a candidate field B<sub>i </sub>matches a corresponding reference field R<sub>i </sub>(s stands for &#x2018;single-field criterion&#x2019;); d( ) models the probability that a pair of candidate fields B<sub>i</sub>, B<sub>j </sub>match a corresponding pair of reference fields R<sub>i</sub>, R<sub>j </sub>(d stands for &#x2018;double-field criterion&#x2019;); and a( ) models the overall match quality not modeled by the previous terms (a stands for &#x2018;all-field criterion&#x2019;).</p>
<p id="p-0043" num="0042">The terms s( ), d( ), and a( ) may be modeled using the naive Bayes assumption, where features indicating presence or absence of a match are extracted and modeled as conditionally independent given the class. Although the naive Bayes assumption usually does not hold in practice, it often gives good results. Nonetheless, more sophisticated methods that incorporate dependencies may alternatively be employed.</p>
<p id="p-0044" num="0043">Different kinds of features may be extracted for different terms of the model above according to the purpose of the terms. As will be seen, many features in the embodiments described herein are motivated by human perception. Namely, since documents are intended for interpretation by humans, it is natural to use perceptual cues to assess the match quality and coherence of candidate fields. But note that other types of features may be used as well.</p>
<p id="p-0045" num="0044">The s( ) terms in the embodiments described herein embodiment measure the perceptual coherence of candidate fields and assess how well each candidate field B<sub>i </sub>matches the corresponding reference field R<sub>i</sub>. To compute s(B<sub>i</sub>, R<sub>i</sub>|C), one or more of the following features may be extracted.</p>
<p id="p-0046" num="0045">Basic Similarity: Standard features, such as alignment, absolute position on the page, height/width similarity, and the like, may be computed. Alignment can be computed as the difference (in pixels) in the position of the left, right, or the middle of the reference field and the candidate field. This difference may be normalized to account for scale, for example, by dividing it by the average text height in pixels. Height and width features refer to differences in height and width, respectively, between the reference and candidate fields, possibly also normalized. Additional features may be used as needed by each application and may include text similarity and content type (e.g. character/numeric/alphanumeric).</p>
<p id="p-0047" num="0046">Separation: The distance from the left edge of B<sub>i </sub>to the nearest horizontally overlapping token to the left may be computed. The purpose of this feature is to measure how perceptually salient B<sub>i </sub>is as a separate entity. If this distance is small, then B, is not perceptually salient, because it almost blends with another token. Since documents are designed for easy human perception, they are less likely to contain such a field, and its quality as a potential match may therefore be reduced. Similarly, features for separation to the right, top, and bottom may be computed. Again, these features may be normalized for scale.</p>
<p id="p-0048" num="0047">Gaps: The size of the largest horizontal gap between neighboring tokens on the same text line within the candidate field may be computed. The idea is perceptual coherence. Normally, words are placed relatively close to each other on the same text line. A large gap might suggest that B<sub>i </sub>is in fact not a single field, but a union of several fields, and therefore a poor match. If more than one text line is present, the largest vertical gap may also be computed. Other features measuring compliance with standard typographic conventions may be used as well.</p>
<p id="p-0049" num="0048">Line Breaks: For each text line except the last, the distance from the right edge of the rightmost token on that line to the right edge of B<sub>i </sub>may be computed. The idea is to measure coherence of B<sub>i</sub>. Normally, as much text as possible without overflowing is fitted on each text line, and line breaks are avoided unless the next word does not fit on the current line. Therefore, a large gap suggests that B<sub>i </sub>is not very coherent and may consist of several fields. Alternatively, the trailing whitespace width in a given line may be compared to the width of the first word on the subsequent line. If that word could fit on the previous line, a penalty could be incurred.</p>
<p id="p-0050" num="0049">Note also that in the exemplary embodiments described here, the same features are computed for all fields; in other words, the same function s(B<sub>i</sub>, R<sub>i</sub>|C) is used for all fields i. In other embodiments, a set of functions s<sub>i</sub>(B<sub>i</sub>, R<sub>i</sub>|C) could be used instead. Each function s<sub>i </sub>could use a different set of features, as needed. This could be useful if different features are important for different fields. For example, if certain fields are known to be aligned on the left, while others are known to be aligned on the right, this method could allow assignment of high priority to left alignment and low priority to right alignment to the first set of fields, and a low priority to left alignment and a high priority to right alignment to the second set of fields. This approach has been tested as well and achieves good performance with suitable data.</p>
<p id="p-0051" num="0050">The d( ) terms in the embodiments described herein are used to determine whether the spatial relations between two candidate fields B<sub>i </sub>and B<sub>j </sub>match the spatial relations between the corresponding reference fields R<sub>i </sub>and R<sub>j</sub>. In other applications, additional features may be used. For example, suppose it is known that a certain field represents a date. Dates can occur in multiple formats, for example, &#x201c;mm/dd/yy&#x201d; or &#x201c;mm.dd.yyyy&#x201d;. All of these formats are possible in general, but within a single document, a single consistent format is usually used. In this case, d( ) could include terms that measure consistency of the format. In the current embodiment, one or more of the following features may be extracted to compute d(B<sub>i</sub>, B<sub>j</sub>, R<sub>i</sub>, R<sub>j</sub>).</p>
<p id="p-0052" num="0051">Offsets: The difference of the distances between the left edges of B<sub>i </sub>and B<sub>j </sub>and the left edges of R<sub>i </sub>and R<sub>j </sub>may be computed. The distance between the left edges of two fields measures the horizontal offset of one field with respect to another. This feature therefore measures how different the offset between B<sub>i </sub>and B<sub>j </sub>is from the offset between R<sub>i </sub>and R<sub>j</sub>. Similarly, the offsets of the centers and right edges may be computed, as well as vertical offsets of top, middle and bottom lines. These features may be normalized to scale as above.</p>
<p id="p-0053" num="0052">Unexplained Text: Let R be the minimal bounding box that includes R<sub>i </sub>and R<sub>j</sub>, and B be the minimal bounding box that includes B<sub>i </sub>and B<sub>j</sub>. If R contains text tokens beyond those included in R<sub>i </sub>and R<sub>j</sub>, then this feature has a value of zero. Otherwise (if all text in R is either in R<sub>i </sub>or in R<sub>j</sub>), it is expected that B contains no additional text except the text in B<sub>i </sub>and B<sub>j</sub>. The value of this feature is then the area (in pixels) of such text contained in B. This feature may be normalized to scale by dividing by the square of the average text height in the document.</p>
<p id="p-0054" num="0053">Note that as with the s( ) terms, the d( ) terms could also be pair-specific. In other words, instead of using the same function d( ) for every pair of fields (i, j), a pair-specific function d<sub>ij</sub>( ) could be used for each pair.</p>
<p id="p-0055" num="0054">Finally, the a( ) term provides the overall assessment of the match quality between the candidate record and the reference record. One or more of the following features may be used to compute a( ):</p>
<p id="p-0056" num="0055">Intersection: A candidate record where some fields intersect is penalized.</p>
<p id="p-0057" num="0056">Unexplained Text: The total area (in pixels) of unexplained text in the bounding box B, which is the minimal bounding box that includes all B<sub>i</sub>'s. Unexplained text refers to text tokens inside B that do not belong to any B<sub>i</sub>. This feature may be normalized to scale as above.</p>
<p id="p-0058" num="0057">Note that unexplained text features appear in the d( ) terms as well as in the a( ) term. The a( ) version of this feature directly measures the total amount of unexplained text contained within a candidate record. The d( ) version is therefore redundant. Nevertheless, it is used to help optimization, as described below.</p>
<p id="p-0059" num="0058">It is to be appreciated that additional features may be used in one or more of the terms described above. In particular, incorporating more perceptual cues may be used. In addition, a wide array of features that characterize appearance of each field may be used. These may include font characteristics (e.g., face, size, weight, slant, etc.), color, texture measures, and the like.</p>
<p id="p-0060" num="0059">As mentioned above, the terms s( ), d( ), and a( ) may be modeled using the naive Bayes assumption. The log-likelihood ratio can in this case be expressed as</p>
<p id="p-0061" num="0060">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mtable>
        <mtr>
          <mtd>
            <mrow>
              <mrow>
                <mi>L</mi>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>(</mo>
                  <msubsup>
                    <mrow>
                      <mo>{</mo>
                      <mi>B</mi>
                      <mo>}</mo>
                    </mrow>
                    <mrow>
                      <mi>i</mi>
                      <mo>=</mo>
                      <mn>1</mn>
                    </mrow>
                    <mi>n</mi>
                  </msubsup>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mo>=</mo>
              <mi/>
              <mo>&#x2062;</mo>
              <mrow>
                <mi>log</mi>
                <mo>&#x2062;</mo>
                <mfrac>
                  <mrow>
                    <mi>p</mi>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <msubsup>
                          <mrow>
                            <mo>{</mo>
                            <msub>
                              <mi>B</mi>
                              <mi>i</mi>
                            </msub>
                            <mo>}</mo>
                          </mrow>
                          <mrow>
                            <mi>i</mi>
                            <mo>=</mo>
                            <mn>1</mn>
                          </mrow>
                          <mi>n</mi>
                        </msubsup>
                        <mo>,</mo>
                        <mrow>
                          <msubsup>
                            <mrow>
                              <mo>{</mo>
                              <msub>
                                <mi>R</mi>
                                <mi>i</mi>
                              </msub>
                              <mo>}</mo>
                            </mrow>
                            <mrow>
                              <mi>i</mi>
                              <mo>=</mo>
                              <mn>1</mn>
                            </mrow>
                            <mi>n</mi>
                          </msubsup>
                          <mo>&#x2758;</mo>
                          <mi>M</mi>
                        </mrow>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mrow>
                    <mi>p</mi>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <msubsup>
                          <mrow>
                            <mo>{</mo>
                            <msub>
                              <mi>B</mi>
                              <mi>i</mi>
                            </msub>
                            <mo>}</mo>
                          </mrow>
                          <mrow>
                            <mi>i</mi>
                            <mo>=</mo>
                            <mn>1</mn>
                          </mrow>
                          <mi>n</mi>
                        </msubsup>
                        <mo>,</mo>
                        <mrow>
                          <msubsup>
                            <mrow>
                              <mo>{</mo>
                              <msub>
                                <mi>R</mi>
                                <mi>i</mi>
                              </msub>
                              <mo>}</mo>
                            </mrow>
                            <mrow>
                              <mi>i</mi>
                              <mo>=</mo>
                              <mn>1</mn>
                            </mrow>
                            <mi>n</mi>
                          </msubsup>
                          <mo>&#x2758;</mo>
                          <mover>
                            <mi>M</mi>
                            <mi>_</mi>
                          </mover>
                        </mrow>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                </mfrac>
              </mrow>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mrow>
              <mo>=</mo>
              <mi/>
              <mo>&#x2062;</mo>
              <mrow>
                <mrow>
                  <munderover>
                    <mo>&#x2211;</mo>
                    <mrow>
                      <mi>i</mi>
                      <mo>&#x2208;</mo>
                      <mrow>
                        <mo>[</mo>
                        <mrow>
                          <mn>1</mn>
                          <mo>&#x2062;</mo>
                          <mi>&#x2026;</mi>
                          <mo>&#x2062;</mo>
                          <mstyle>
                            <mspace width="0.3em" height="0.3ex"/>
                          </mstyle>
                          <mo>&#x2062;</mo>
                          <mi>n</mi>
                        </mrow>
                        <mo>]</mo>
                      </mrow>
                    </mrow>
                    <mstyle>
                      <mspace width="0.3em" height="0.3ex"/>
                    </mstyle>
                  </munderover>
                  <mo>&#x2062;</mo>
                  <mrow>
                    <munderover>
                      <mo>&#x2211;</mo>
                      <mi>k</mi>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                    </munderover>
                    <mo>&#x2062;</mo>
                    <mrow>
                      <msubsup>
                        <mi>w</mi>
                        <mi>k</mi>
                        <mi>s</mi>
                      </msubsup>
                      <mo>&#x2061;</mo>
                      <mrow>
                        <mo>[</mo>
                        <mrow>
                          <msubsup>
                            <mi>f</mi>
                            <mi>k</mi>
                            <mi>s</mi>
                          </msubsup>
                          <mo>&#x2061;</mo>
                          <mrow>
                            <mo>(</mo>
                            <mrow>
                              <msub>
                                <mi>B</mi>
                                <mi>i</mi>
                              </msub>
                              <mo>,</mo>
                              <msub>
                                <mi>R</mi>
                                <mi>i</mi>
                              </msub>
                            </mrow>
                            <mo>)</mo>
                          </mrow>
                        </mrow>
                        <mo>]</mo>
                      </mrow>
                    </mrow>
                  </mrow>
                </mrow>
                <mo>+</mo>
              </mrow>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mrow>
              <mi/>
              <mo>&#x2062;</mo>
              <mrow>
                <mrow>
                  <munderover>
                    <mo>&#x2211;</mo>
                    <munder>
                      <mrow>
                        <mi>i</mi>
                        <mo>,</mo>
                        <mrow>
                          <mi>j</mi>
                          <mo>&#x2208;</mo>
                          <mrow>
                            <mo>[</mo>
                            <mrow>
                              <mn>1</mn>
                              <mo>&#x2062;</mo>
                              <mi>&#x2026;</mi>
                              <mo>&#x2062;</mo>
                              <mstyle>
                                <mspace width="0.3em" height="0.3ex"/>
                              </mstyle>
                              <mo>&#x2062;</mo>
                              <mi>n</mi>
                            </mrow>
                            <mo>]</mo>
                          </mrow>
                        </mrow>
                      </mrow>
                      <mrow>
                        <mi>i</mi>
                        <mo>&#x2260;</mo>
                        <mi>j</mi>
                      </mrow>
                    </munder>
                    <mstyle>
                      <mspace width="0.3em" height="0.3ex"/>
                    </mstyle>
                  </munderover>
                  <mo>&#x2062;</mo>
                  <mrow>
                    <munderover>
                      <mo>&#x2211;</mo>
                      <mi>k</mi>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                    </munderover>
                    <mo>&#x2062;</mo>
                    <mrow>
                      <msubsup>
                        <mi>w</mi>
                        <mi>k</mi>
                        <mi>d</mi>
                      </msubsup>
                      <mo>&#x2061;</mo>
                      <mrow>
                        <mo>[</mo>
                        <mrow>
                          <msubsup>
                            <mi>f</mi>
                            <mi>k</mi>
                            <mi>d</mi>
                          </msubsup>
                          <mo>&#x2061;</mo>
                          <mrow>
                            <mo>(</mo>
                            <mrow>
                              <msub>
                                <mi>B</mi>
                                <mi>i</mi>
                              </msub>
                              <mo>,</mo>
                              <msub>
                                <mi>B</mi>
                                <mi>j</mi>
                              </msub>
                              <mo>,</mo>
                              <msub>
                                <mi>R</mi>
                                <mi>i</mi>
                              </msub>
                              <mo>,</mo>
                              <msub>
                                <mi>R</mi>
                                <mi>j</mi>
                              </msub>
                            </mrow>
                            <mo>)</mo>
                          </mrow>
                        </mrow>
                        <mo>]</mo>
                      </mrow>
                    </mrow>
                  </mrow>
                </mrow>
                <mo>+</mo>
              </mrow>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mrow>
              <mi/>
              <mo>&#x2062;</mo>
              <mrow>
                <mrow>
                  <munderover>
                    <mo>&#x2211;</mo>
                    <mi>k</mi>
                    <mstyle>
                      <mspace width="0.3em" height="0.3ex"/>
                    </mstyle>
                  </munderover>
                  <mo>&#x2062;</mo>
                  <mrow>
                    <msubsup>
                      <mi>w</mi>
                      <mi>k</mi>
                      <mi>a</mi>
                    </msubsup>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>[</mo>
                      <mrow>
                        <msubsup>
                          <mi>f</mi>
                          <mi>k</mi>
                          <mi>a</mi>
                        </msubsup>
                        <mo>&#x2061;</mo>
                        <mrow>
                          <mo>(</mo>
                          <mrow>
                            <msubsup>
                              <mrow>
                                <mo>{</mo>
                                <msub>
                                  <mi>B</mi>
                                  <mi>i</mi>
                                </msub>
                                <mo>}</mo>
                              </mrow>
                              <mrow>
                                <mi>i</mi>
                                <mo>=</mo>
                                <mn>1</mn>
                              </mrow>
                              <mi>n</mi>
                            </msubsup>
                            <mo>,</mo>
                            <msubsup>
                              <mrow>
                                <mo>{</mo>
                                <msub>
                                  <mi>R</mi>
                                  <mi>i</mi>
                                </msub>
                                <mo>}</mo>
                              </mrow>
                              <mrow>
                                <mi>i</mi>
                                <mo>=</mo>
                                <mn>1</mn>
                              </mrow>
                              <mi>n</mi>
                            </msubsup>
                          </mrow>
                          <mo>)</mo>
                        </mrow>
                      </mrow>
                      <mo>]</mo>
                    </mrow>
                  </mrow>
                </mrow>
                <mo>,</mo>
              </mrow>
            </mrow>
          </mtd>
        </mtr>
      </mtable>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>3</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where w<sub>k</sub><sup>s</sup>, is the weight of the s( ) feature k, and w<sub>k</sub><sup>d </sup>and w<sub>k</sub><sup>a </sup>are similar weights for the d( ) and a( ) features, respectively. These weights are learned from training data, as described below.
</p>
<p id="p-0062" num="0061">To find candidate records having a best match quality, a search algorithm called best-first leaf search (BFLS) is suitably employed to find candidate records that maximize an objective function, such as the objective function L described above in connection with the probabilistic model. However, other search algorithms such as A* and Gibbs sampling may be employed.</p>
<p id="p-0063" num="0062">BFLS uses an upper bound on the objective function and examines candidate records in the order of decreasing upper bounds. Once a candidate record whose match quality is above the next-best upper bound is found, the search can safely terminate since the remaining candidate records are guaranteed to have worse upper bounds and therefore also worse match qualities. The main requirement of BFLS is that the upper bound be factorizable into terms, each of which only involves one candidate field.</p>
<p id="p-0064" num="0063">Applying BFLS to the probabilistic model described above, an example of a factorizable upper bound is</p>
<p id="p-0065" num="0064">
<maths id="MATH-US-00003" num="00003">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>U</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <msubsup>
              <mrow>
                <mo>{</mo>
                <msub>
                  <mi>B</mi>
                  <mi>i</mi>
                </msub>
                <mo>}</mo>
              </mrow>
              <mrow>
                <mi>i</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mi>n</mi>
            </msubsup>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <munderover>
            <mo>&#x2211;</mo>
            <mrow>
              <mi>i</mi>
              <mo>&#x2208;</mo>
              <mrow>
                <mo>[</mo>
                <mrow>
                  <mn>1</mn>
                  <mo>&#x2062;</mo>
                  <mi>&#x2026;</mi>
                  <mo>&#x2062;</mo>
                  <mstyle>
                    <mspace width="0.3em" height="0.3ex"/>
                  </mstyle>
                  <mo>&#x2062;</mo>
                  <mi>n</mi>
                </mrow>
                <mo>]</mo>
              </mrow>
            </mrow>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
          </munderover>
          <mo>&#x2062;</mo>
          <mrow>
            <mrow>
              <msubsup>
                <mi>w</mi>
                <mi>k</mi>
                <mi>s</mi>
              </msubsup>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>[</mo>
                <mrow>
                  <msubsup>
                    <mi>f</mi>
                    <mi>k</mi>
                    <mi>s</mi>
                  </msubsup>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <msub>
                        <mi>B</mi>
                        <mi>i</mi>
                      </msub>
                      <mo>,</mo>
                      <msub>
                        <mi>R</mi>
                        <mi>i</mi>
                      </msub>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>]</mo>
              </mrow>
            </mrow>
            <mo>.</mo>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>4</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
This upper bound is a proper upper bound only if the weights w<sub>k</sub><sup>d </sup>and w<sub>k</sub><sup>a </sup>are non-positive. Namely, most features used for d( ) and a( ) measure discrepancies between an &#x2018;ideal&#x2019; field and the candidate field. Large feature values then correspond to large discrepancies and are therefore expected to be much more common in poorly matching candidate fields, as compared to good candidate fields. This implies negative weights for at least large feature values. However, in practice, some weights may be positive, especially for small feature values.
</p>
<p id="p-0066" num="0065">To compensate for positive weights, the largest weight for each feature may be subtracted from all weights for that feature, making all weights non-positive. This is possible since all features have finitely many possible values (e.g., 100). Subtracting the maximum weight from each feature only changes L by a constant, which is irrelevant since it is only used to find a maximum.</p>
<p id="p-0067" num="0066">While the upper bound in equation (4) may be employed, it is not very tight. The reason is that it involves only the factors matching appearance of a single candidate field to a single reference field, using features such as alignment. A typical document contains many candidate fields that are aligned with any reference field. Most records composed of these fields will not form a coherent match due to, for example, violation of relative locations structure. But since relative locations do not influence the upper bound, many poor candidates will have a promising (high) value of U and therefore will be considered by the algorithm. This leads to inefficient search.</p>
<p id="p-0068" num="0067">To overcome this inefficiency, BFLS may alternatively be applied to the probabilistic model as follows. First, assume that a candidate field for one of the reference fields (say R<sub>p</sub>) is fixed to be B<sub>p</sub>. This reference field is hereafter referred to as a pivot field p. A new upper bound U<sup>p </sup>may then be defined as</p>
<p id="p-0069" num="0068">
<maths id="MATH-US-00004" num="00004">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <msup>
            <mi>U</mi>
            <mi>p</mi>
          </msup>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <msubsup>
              <mrow>
                <mo>{</mo>
                <msub>
                  <mi>B</mi>
                  <mi>i</mi>
                </msub>
                <mo>}</mo>
              </mrow>
              <mrow>
                <mi>i</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mi>n</mi>
            </msubsup>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mrow>
            <munderover>
              <mo>&#x2211;</mo>
              <mrow>
                <mi>i</mi>
                <mo>&#x2260;</mo>
                <mi>p</mi>
              </mrow>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
            </munderover>
            <mo>&#x2062;</mo>
            <mrow>
              <munderover>
                <mo>&#x2211;</mo>
                <mi>k</mi>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
              </munderover>
              <mo>&#x2062;</mo>
              <mrow>
                <msubsup>
                  <mi>w</mi>
                  <mi>k</mi>
                  <mi>s</mi>
                </msubsup>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>[</mo>
                  <mrow>
                    <msubsup>
                      <mi>f</mi>
                      <mi>k</mi>
                      <mi>s</mi>
                    </msubsup>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <msub>
                          <mi>B</mi>
                          <mi>i</mi>
                        </msub>
                        <mo>,</mo>
                        <msub>
                          <mi>R</mi>
                          <mi>i</mi>
                        </msub>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mo>]</mo>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
          <mo>+</mo>
          <mrow>
            <munderover>
              <mo>&#x2211;</mo>
              <mrow>
                <mi>i</mi>
                <mo>&#x2260;</mo>
                <mi>p</mi>
              </mrow>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
            </munderover>
            <mo>&#x2062;</mo>
            <mrow>
              <munderover>
                <mo>&#x2211;</mo>
                <mi>k</mi>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
              </munderover>
              <mo>&#x2062;</mo>
              <mrow>
                <msubsup>
                  <mi>w</mi>
                  <mi>k</mi>
                  <mi>d</mi>
                </msubsup>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>[</mo>
                  <mrow>
                    <msubsup>
                      <mi>f</mi>
                      <mi>k</mi>
                      <mi>d</mi>
                    </msubsup>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <msub>
                          <mi>B</mi>
                          <mi>i</mi>
                        </msub>
                        <mo>,</mo>
                        <msub>
                          <mi>B</mi>
                          <mi>p</mi>
                        </msub>
                        <mo>,</mo>
                        <msub>
                          <mi>R</mi>
                          <mi>i</mi>
                        </msub>
                        <mo>,</mo>
                        <msub>
                          <mi>R</mi>
                          <mi>p</mi>
                        </msub>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mo>]</mo>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
          <mo>+</mo>
          <mrow>
            <munderover>
              <mo>&#x2211;</mo>
              <mrow>
                <mi>j</mi>
                <mo>&#x2260;</mo>
                <mi>p</mi>
              </mrow>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
            </munderover>
            <mo>&#x2062;</mo>
            <mrow>
              <munderover>
                <mo>&#x2211;</mo>
                <mi>k</mi>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
              </munderover>
              <mo>&#x2062;</mo>
              <mrow>
                <mrow>
                  <msubsup>
                    <mi>w</mi>
                    <mi>k</mi>
                    <mi>d</mi>
                  </msubsup>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>[</mo>
                    <mrow>
                      <msubsup>
                        <mi>f</mi>
                        <mi>k</mi>
                        <mi>d</mi>
                      </msubsup>
                      <mo>&#x2061;</mo>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <msub>
                            <mi>B</mi>
                            <mi>p</mi>
                          </msub>
                          <mo>,</mo>
                          <msub>
                            <mi>B</mi>
                            <mi>j</mi>
                          </msub>
                          <mo>,</mo>
                          <msub>
                            <mi>R</mi>
                            <mi>p</mi>
                          </msub>
                          <mo>,</mo>
                          <msub>
                            <mi>R</mi>
                            <mi>j</mi>
                          </msub>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                    <mo>]</mo>
                  </mrow>
                </mrow>
                <mo>.</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>5</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
Since B<sub>p </sub>is fixed, each of the w<sub>k</sub><sup>d </sup>terms depends only on one of the candidate fields and this upper bound is factorizable.
</p>
<p id="p-0070" num="0069">U<sup>p </sup>is tighter than the bound in equation (4), since U<sup>p </sup>includes terms that measure the quality of the spatial relations (and other features in d( )) involving the pivot field p. It is therefore expected that the search problem with the pivot field p held fixed can be solved more efficiently. Further, since U<sup>p </sup>involves the d( ) terms, the solutions which contain unexplained text will be penalized by it, even though the a( ) terms are not included in U<sup>p</sup>. This also improves the efficiency of search.</p>
<p id="p-0071" num="0070">In view of U<sup>p</sup>, the following modified search algorithm may be employed. One of the reference fields may be selected as the pivot field. Each candidate field for that reference field may then be considered in turn as a possible fixed match. For each fixed match, the reduced problem is constructed and optimized using BFLS with the modified upper bound U<sup>p</sup>. Since an optimization problem needs to be solved for every candidate match of the pivot field, it is natural to select as the pivot field the reference field with the fewest candidate fields. This modification of BFLS is called pBFLS, for &#x2018;pivoted BFLS&#x2019;.</p>
<p id="p-0072" num="0071">Using pBFLS, a predetermined number, such as N=10, of the best candidate records may be selected for each candidate field of the pivot field. If there are P candidate fields of the pivot field, the final set of best candidate records has a size of NP.</p>
<p id="p-0073" num="0072">The determination <b>112</b> of the optimal record set from the best candidate records may include generating one or more candidate record sets from the best candidate records and selecting the best candidate record set from the candidate record sets. A document, such as an invoice, may contain multiple instances of the repeated structure. All such instances may need to be identified. The set of all instances of repeated structure forms a &#x2018;record set&#x2019;. In the current embodiment, the optimal record set may not include overlapping records and suitably accounts for more tokens than any other candidate record set. Further, of all candidate record sets that account for the same number of tokens, the set which has the highest overall quality of individual records (as measured by the sum of all the L scores) may be selected. In other embodiments, the optimality criteria may be suitably different to account for the target application.</p>
<p id="p-0074" num="0073">To generate the candidate record sets, every combination of the best candidate records may be enumerated. However, in embodiments employing pBFLS, the candidate record sets may be generated by enumerating every combination of the best candidate records having zero or one best candidate records for each of the fixed matches of the pivot field. Suitably, this prevents some candidate record sets having intersection from being considered, since two candidate records with the same fixed pivot match will intersect at least at that fixed pivot match. Note that other intersections may still be possible even in this case, and should be checked for separately.</p>
<p id="p-0075" num="0074">To select the optimal record set, a search of the candidate record sets may be performed. Suitably, this search is a brute-force search, but other means of searching may be employed. While searching the candidate record sets, candidate record sets having any intersections (i.e., fields that belong to multiple candidate records) are ignored. Further, the search suitably seeks the candidate record set explaining more tokens than any other candidate record set. To compute how much text is explained by a candidate record set, the area (in pixels) of all the tokens which do not belong to any field of any record in the candidate record set is computed. This text is called &#x2018;unexplained,&#x2019; and the candidate record set which minimizes the area of unexplained text is selected. Further, the &#x2018;quality&#x2019; of the record set is computed as the sum of the L values of all the records in the set. Of all candidate record sets with the minimal amount of unexplained text, the set with the best quality may suitably be selected.</p>
<p id="p-0076" num="0075">The number of possible solution sets that need to be searched is the product of the numbers of candidate records for each pivot match. This number can become prohibitive, especially for large document images. Therefore, in certain embodiments, the determination <b>112</b> of the optimal record set may further include filtering the best candidate records to retain only the most promising candidate records before generating the candidate record sets and searching the candidate record sets.</p>
<p id="p-0077" num="0076">A natural way to perform this filtering is by setting a threshold on the match quality, measured by the log-likelihood function L above. However, the number of terms in L depends on the number of reference fields. The range of variation of L is therefore different for problems with different numbers of reference fields: what is considered a high (good) value of L for a problem with ten reference fields may be a low (poor) value for a problem with only two reference fields. This makes selecting a single problem-independent threshold difficult.</p>
<p id="p-0078" num="0077">To alleviate this problem, L can be modified to define L&#x2032;. The idea is to include some of the information available in L, but to make the number of terms in L&#x2032; independent of the number of reference fields. To achieve this, note that L involves three types of terms: the f<sup>s</sup>( ) terms (those involving a single B<sub>i</sub>), the f<sup>d</sup>( ) terms (those involving two B<sub>i</sub>'s), and the f<sup>a</sup>( ) terms (those involving all B<sub>i</sub>'s). Consider, for example, the f<sup>d</sup>( ) terms. There are roughly n<sup>2 </sup>combinations of two fields (recall that n is the number of reference fields). Therefore, there are roughly n<sup>2 </sup>terms per feature. These terms provide information about how much the spatial relations of a candidate record are similar to the spatial relations of the reference record. Denote the vector of all values of a feature f<sub>k</sub><sup>d </sup>(for all pairs B<sub>i</sub>, B<sub>j</sub>, i&#x2260;j) by F<sub>k</sub><sup>d</sup>. In L, all values in this vector are used, and since the length of F<sub>k</sub><sup>d </sup>depends on n, so does L. For L&#x2032;, therefore, F<sub>k</sub><sup>d </sup>may be characterized by a fixed number of values.</p>
<p id="p-0079" num="0078">In certain embodiments, five evenly spaced percentile values may be employed to characterize F<sub>k</sub><sup>d</sup>. Namely, the minimum, maximum, median, 25'th percentile and the 75'th percentile values may be used. These values provide rough information about how many spatial relations match well and how many match poorly, but without specific details about how well each particular spatial relation is matched. These features therefore provide less information than the full vector F<sub>k</sub><sup>d</sup>. However, on the other hand, the number of these values is independent of n, and therefore they can be used consistently regardless of how many reference fields there are. Similarly, the vectors F<sub>k</sub><sup>s </sup>and F<sub>k</sub><sup>a </sup>may be created. If the objective function involves additional terms (say, relations among three fields), similar vectors can be created for each term. Again, the naive Bayes assumption is used to compute a modified log-likelihood ratio L&#x2032;({B<sub>i</sub>}<sub>i=1</sub><sup>n</sup>) based on these percentile features.</p>
<p id="p-0080" num="0079">The naive Bayes assumption is that the features are independent given the class value. This assumption is clearly violated for the percentiles of F<sub>k</sub><sup>a</sup>. The reason is that F<sub>k</sub><sup>a </sup>has length one, since there is only one way to choose n fields out of n. Therefore, all percentiles have the same value, and clearly aren't independent, whereby in practice we only use one percentile value for F<sub>k</sub><sup>a</sup>.</p>
<p id="p-0081" num="0080">The percentiles of and F<sub>k</sub><sup>s </sup>and F<sub>k</sub><sup>d </sup>are not independent as well. For example, the maximum is never smaller than the minimum. Therefore, it is desirable to use a learning method which can cope with dependent features (such as SVM) to perform this final filtering. However, in the embodiments described herein the Na&#xef;ve Bayes method is used and works well for our tasks.</p>
<p id="p-0082" num="0081">In view of the foregoing, the filtering may include computing the value of L&#x2032; for each candidate record and removing the candidate records whose L&#x2032; value is less than a pre-determined threshold. Thereafter, the candidate records sets can be enumerated and searched, as described above.</p>
<p id="p-0083" num="0082">To further speed up the search, the selection of the optimal record set may be done progressively. This works, for example, if there is some natural ordering of records. For example, in the case of invoices, the records correspond to line items and are typically ordered from top to bottom on the page. When pBFLS is used, this ordering is indicated by the relative y positions of the candidate pivot matches. In this case, the selection may proceed as follows. The topmost record is picked, and several (two in the embodiments described herein) additional records below it are added for context. An optimal configuration of this subset of records is selected, for example, using brute force with filtering, as above. Then the optimal candidate for the topmost record is retained, and the process repeats with the next set of records (i.e. records 2 through 4). Note that it is important to include overlap in this search. The reason is that although there is enough context to pick the optimal value for the top record reliably, there may not be enough context to pick the optimal values for the subsequent two records in the same batch.</p>
<p id="p-0084" num="0083">The extraction <b>114</b> of data from the optimal record set is suitably accomplished by assigning each labeled field a semantic role and by extending this semantic role to other matching fields. In such embodiments, the records of the candidate record set may be extracted to a database and/or a spreadsheet, thereby allowing efficient storage and searching thereof. It should be appreciated that in some cases OCR need not be employed. For example, images of the records may be extracted from the document images.</p>
<p id="p-0085" num="0084">Prior to applying the procedure outlined above, the models should be trained. The training of the probabilistic models with one or more annotated document images seeks to learn the parameters of the probabilistic models specified above. Specifically, the weights w used in L and L&#x2032; need to be learned. In certain embodiments, the standard learning procedure for naive Bayes models may be used. In such embodiments, a training set of document images may be employed. In each document image, one or more records need to be annotated, as described above. Suitably, if other models are used, their parameters should be trained as well.</p>
<p id="p-0086" num="0085">For the Na&#xef;ve Bayes case, the weight w for a feature f may be learned as follows. The values of f are discretized and clipped at 0 and 99. For each discretized value f<sub>0</sub>, the probabilities p(f=f<sub>0</sub>|M) and =f<sub>0</sub>| <o ostyle="single">M</o>) are estimated from the training data. The weight is then computed as</p>
<p id="p-0087" num="0086">
<maths id="MATH-US-00005" num="00005">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>w</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>[</mo>
            <msub>
              <mi>f</mi>
              <mn>0</mn>
            </msub>
            <mo>]</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mi>log</mi>
          <mo>&#x2062;</mo>
          <mrow>
            <mfrac>
              <mrow>
                <mi>p</mi>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>f</mi>
                    <mo>=</mo>
                    <mrow>
                      <msub>
                        <mi>f</mi>
                        <mi>o</mi>
                      </msub>
                      <mo>&#x2758;</mo>
                      <mi>M</mi>
                    </mrow>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mrow>
                <mi>p</mi>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>f</mi>
                    <mo>=</mo>
                    <mrow>
                      <msub>
                        <mi>f</mi>
                        <mi>o</mi>
                      </msub>
                      <mo>&#x2758;</mo>
                      <mover>
                        <mi>M</mi>
                        <mi>_</mi>
                      </mover>
                    </mrow>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mfrac>
            <mo>.</mo>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>6</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0088" num="0087">In certain embodiments, the training may automatically deal with irrelevant features. Suitably, this is accomplished by assigning low weights and will not influence the methods disclosed herein. It is therefore possible to specify a wide variety of features without having to verify whether these features are useful for match finding.</p>
<p id="p-0089" num="0088">With reference to <figref idref="DRAWINGS">FIG. 3</figref>, a method <b>300</b> of extracting repeated structure within and/or across one or more pages of one or more documents is illustrated. The documents may include one or more invoices, receipts, healthcare documents, and other documents or sets of documents having repeated structure therein (within and/or across pages of one and/or multiple documents). Further, each document includes one or more pages.</p>
<p id="p-0090" num="0089">The method <b>300</b> includes generating <b>302</b> a document image for each page of the documents, as described above in connection with Action <b>102</b> of <figref idref="DRAWINGS">FIG. 1</figref>, and then selecting <b>304</b> one of the document images as a reference. Typically, it does not matter which of the document images is selected. A reference record and one or more reference fields thereof are then identified <b>306</b> within the reference, as described above in connection with Action <b>104</b> of <figref idref="DRAWINGS">FIG. 1</figref>. Upon identifying <b>306</b> the reference fields and the reference record, a determination <b>308</b> is made as to whether there are document images that need to be checked for repeated structure. Typically, this initially includes all of the document images, including the reference. However, insofar as the documents are known to only include a single record per page, the reference may be excluded from the check since all the records therein are known. If there are not document images that need to be checked (i.e., they have all been checked and/or the records in each of the document images are known), the method <b>300</b> terminates <b>310</b>. If there are document images that need to be checked, one of these document images is selected <b>312</b>.</p>
<p id="p-0091" num="0090">One or more candidate fields for each of the reference fields are generated <b>314</b> for the selected document image. One or more best candidate records from the candidate records are then selected <b>316</b> using a routine for generating <b>318</b> candidate records from the candidate fields. An optimal record set is determined <b>320</b> from the best candidate records of the selected document and data is extracted <b>322</b> from the optimal record set of the selected document. Actions <b>308</b> through <b>322</b> are then repeated until all the document images are checked. Actions <b>314</b> through <b>322</b> are as described in connection with Actions <b>106</b> through <b>114</b> of <figref idref="DRAWINGS">FIG. 1</figref>, respectively.</p>
<p id="h-0008" num="0000">3.0 System Implementation</p>
<p id="p-0092" num="0091">With reference to <figref idref="DRAWINGS">FIG. 4</figref>, a data extraction system <b>400</b> is provided. One or more computers or other digital/electronic processing devices <b>402</b>, each including storage and a digital/electronic processor, such as a microprocessor, microcontroller, graphic processing unit (GPU), etc., suitably embody the system <b>400</b>. However, in other embodiments, the system <b>400</b> may be embodied by one or more servers, each including a digital processor and each including or having access to digital data storage, such servers being suitably accessed via the Internet or a local area network, or by a personal data assistant (PDA) including a digital processor and digital data storage, or so forth.</p>
<p id="p-0093" num="0092">The computers or other digital/electronic processing devices <b>402</b> suitably include or are operatively connected with one or more user input devices <b>404</b>, such as a keyboard, mouse, touch screen, etc., for receiving user input to control the system <b>400</b>. Further, the computers or other digital/electronic processing devices <b>402</b> suitably include or are operatively connected with one or more display devices <b>406</b>, such as an LCD, a plasma display, a projector, etc., for displaying output generated by the system. In other embodiments, the input for controlling the system <b>400</b> is received from programs running on the computers or other digital/electronic processing devices <b>402</b>, or from a network connection, or so forth. Similarly, in other embodiments the output may serve as input to a program running on the computers or other digital/electronic processing devices <b>402</b>, or may be transmitted via a network connection, or so forth.</p>
<p id="p-0094" num="0093">The system <b>400</b> suitably includes a data extraction module <b>408</b> that implements one or more aspects of the methods and/or algorithms disclosed herein. In certain embodiments, the module <b>408</b> receives one or more document images from a source external to the module <b>408</b> and detects repeated structure within the document images. In some of such embodiments, the module <b>408</b> further receives user annotations of one of the document images from a user of the system <b>402</b>. Suitably, this is accomplished via the user input devices <b>404</b> or an external source, such as the network.</p>
<p id="p-0095" num="0094">In some embodiments, the data extraction module <b>408</b> is embodied by a storage medium storing instructions executable by the computers or other digital/electronic processing devices <b>402</b>. The storage medium may include, for example: a magnetic disk or other magnetic storage medium; an optical disk or other optical storage medium; a random access memory (RAM), read-only memory (ROM), or other electronic memory device or chip or set of operatively interconnected chips; an Internet server from which the stored instructions may be retrieved via the Internet or a local area network; or so forth.</p>
<p id="p-0096" num="0095">With reference to <figref idref="DRAWINGS">FIG. 5</figref>, a document processing system <b>500</b> employing a data extraction system is illustrated. The document processing system <b>500</b> may include an imaging device <b>502</b>, a document conversion system <b>504</b>, a data extraction system <b>506</b>, such as the data extraction system <b>400</b> of <figref idref="DRAWINGS">FIG. 4</figref>, and a database <b>508</b>. Notably, however, the document processing system <b>500</b> may only include one of the imaging device <b>502</b> and the document conversion system <b>504</b>.</p>
<p id="p-0097" num="0096">The imaging device <b>502</b> converts one or more paper documents <b>510</b> into document images <b>512</b>. The imaging device <b>502</b> may be one or more of a camera, a scanner, and the like. In certain embodiments, the imaging device <b>502</b> may receive the paper documents <b>510</b> via a conveyor path extending from a feed tray. However, other means of receiving the paper documents <b>510</b> are equally amenable. For example, in certain embodiments, an operator of the document processing system <b>500</b> may feed the paper documents <b>510</b> to the imaging device <b>502</b>.</p>
<p id="p-0098" num="0097">The document conversion system <b>504</b> converts one or more electronic documents <b>514</b> into document images <b>516</b>. Optionally, the document images <b>516</b> may include auxiliary data, such as the text content of the document <b>514</b>. Put another way, the document conversion system <b>504</b> converts the electronic documents <b>514</b> in various electronic formats (such as PDF, XML, Word, etc.) into a standardized image format, possibly with auxiliary data. The electronic documents <b>514</b> may be loaded from a magnetic or optical media or a network.</p>
<p id="p-0099" num="0098">The data extraction system <b>506</b> processes the document images <b>512</b>, <b>516</b> to extract data contained therein according to methods and/or approach of the present disclosure. Suitably, the data extraction system <b>506</b> may receive the document images <b>512</b>, <b>516</b> via a communications network, such as the Internet, a local area network, a wireless network, and the like. However, in other embodiments, the data extraction system <b>506</b> may receive the document images <b>512</b>, <b>516</b> via a data bus, such as USB, Firewire, etc., a storage medium, such as a CD, a thumb drive, etc., and the like.</p>
<p id="p-0100" num="0099">The database <b>508</b> stores records extracted from the document images <b>512</b>, <b>516</b>. Suitably, the database <b>508</b> receives the records from the data extraction system <b>506</b> via a communications network, such as the Internet, a local area network, a wireless network, and the like. In certain embodiments, the database <b>508</b> may be distributed across a plurality of computer servers interconnected by a communications network. The database <b>508</b> suitably stores the records in a table structure having fields corresponding to the fields of the records.</p>
<p id="h-0009" num="0000">4.0 Experiments</p>
<p id="p-0101" num="0100">The proposed systems and methods were trained on 10 synthetic invoice images. After that, they were tested on a dataset of 15 invoice types. This dataset consisted of scanned invoices from 15 different vendors with different invoice styles. Two to six invoices for each vendor were used, for a total of 51 invoices. Each invoice contained between 2 and 21 records (each record corresponds to a description of one product). The total number of records in all the invoices was 409. The number of fields per record varied between 4 and 11. One record per invoice was used for annotation. The remaining 358 records were used for testing the system.</p>
<p id="p-0102" num="0101">Of these, 330 records (92%) were found successfully. A match was declared successful when all fields of a record were identified correctly. On the level of complete documents, a document was declared as processed successfully if all records were found successfully. In test dataset described above, 32 out of 51 invoices (63%) were processed successfully. In a production setting, these invoices would not need any further manual processing.</p>
<p id="p-0103" num="0102">The most frequent mode of failure was the tendency of the specific embodiments disclosed herein to include spurious text within a match, even at the expense of match quality. The reason for this behavior is the requirement to explain as much of the document as possible, and the strong penalty for unexplained text. In other embodiments, this could be addressed by training on an appropriate training set which includes appropriately labeled examples including spurious text.</p>
<p id="p-0104" num="0103">Some of the remaining errors are attributed to OCR. Most of these occur when OCR fails to recognize a token. Since candidate fields disclosed herein are constructed from tokens found by OCR, such an omission is not recoverable. Moreover, since the proposed methods cannot deal with missing fields, the entire record which contains the omitted field will not be found. It may be possible to remedy this by using connected sets of black pixels as an additional source of tokens.</p>
<p id="p-0105" num="0104">Two additional experiments were performed to show the generality of the proposed methods. In one experiment, a set of photographed consumer receipts was used. In another, a set of scanned blood test result reports was used. No additional training was needed. The methods trained on the 10 synthetic invoices generalized well to these additional domains.</p>
<p id="p-0106" num="0105">Further tests were performed with a variety of document types, including online store query results, medical claims forms, and others. Tests in which the repeated structure was found across multiple pages of the same document, as well as across multiple documents, were also performed. The performance of the proposed methods on these datasets was similar to the performance with invoices.</p>
<p id="h-0010" num="0000">5.0 Conclusion</p>
<p id="p-0107" num="0106">The present disclosure provides methods, and systems employing the same, for finding repeated structure in document images. The method may include generating a document image, identifying a reference record and one or more reference fields thereof, generating one or more candidate fields, generating one or more candidate records, selecting one or more best candidate records, determining an optimal record set, extracting data from the optimal record set, and training the probabilistic model.</p>
<p id="p-0108" num="0107">A novel achievement of the methods is the ability to solve a wider range of problems than previously possible. Namely, the methods, after a brief training, can solve problems with document images from several different categories (such as invoices, receipts, and blood test forms). Previously, separate training was needed for each task. Additionally, the methods cover a broader range of documents in each category than previously possible. For example, they can handle cases when different columns overlap (leading to interlacing), when there is no clear separation between columns, and when different instances of repeated structure can occupy a different number of text lines.</p>
<p id="p-0109" num="0108">One novel aspect of the methods is the use of a fully probabilistic formulation (as opposed to a set of ad-hoc rules). Namely, a single principled objective function is formulated and optimized. This provides greater flexibility and allows the methods to cope with more variability in the structure than previously possible. This also allows the methods to grade possible solutions continuously (as opposed to making hard decisions), which improves performance since more information is considered before irreversible decisions are made.</p>
<p id="p-0110" num="0109">Another novel aspect is the use of a broader range of cues compared to previously described models. Especially novel is the use of perceptual cues and encouraging the methods to explain as much of a document image as possible. Perceptual cues used in previous approaches were limited to alignment and proximity, whereas the methods incorporate additional important cues, such as the presence of gaps, amount of whitespace, etc. These are especially helpful when the methods need to adapt to structures that occupy varying number of text lines.</p>
<p id="p-0111" num="0110">Another important novelty is that the methods avoid using column structure as cue. This allows the methods to deal with document images where individual columns overlap (leading to interlacing). This is in contrast to most other invoice parsing methods which assume column structure. The only method which doesn't explicitly model column structure is the wrapping method. However, wrapping does not allow for automatic parsing of documents, as conditions for match in wrapping must be specified explicitly by the user.</p>
<p id="p-0112" num="0111">Other novel aspects are that the methods use very elaborate spatial relations between fields and a novel optimization algorithm for efficiently searching the space of candidate matches. In contrast, previous approaches only used crude spatial relations data such as &#x2018;left/right/above/below&#x2019;.</p>
<p id="p-0113" num="0112">The performance of the methods was illustrated on examples from several different domains (including invoices, receipts, and medical documents). The methods cover a broader range of documents in each of these categories than was possible previously. In addition, the method adapts well to other repeated structure finding tasks, such as finding repeated structure in tables of contents or in search results.</p>
<p id="p-0114" num="0113">The general need for human input is one of the main limitations of the methods. Human supervision is required at two stages: during training, when user-labeled training examples need to be supplied, and during processing, when the user annotates an example of the structure to be found. Nonetheless, the amount of training data needed by the methods is already quite small.</p>
<p id="p-0115" num="0114">During experimentation, training on ten invoice images allowed the methods to successfully generalize novel invoices (from vendors not represented in the training set), as well as to receipts and medical forms. Further, in an experiment attempting to generalize an annotation from one invoice to additional invoices from the same vendor the performance was comparable to performance with annotations specifically made for each invoice. Since many companies receive most of their invoices from a small subset of vendors, the methods save significant annotation effort.</p>
<p id="p-0116" num="0115">The disclosure has been made with reference to preferred embodiments. Obviously, modifications and alterations will occur to others upon reading and understanding the preceding detailed description. It is intended that the preferred embodiments be construed as including all such modifications and alterations insofar as they come within the scope of the appended claims or the equivalents thereof.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08625886-20140107-M00001.NB">
<img id="EMI-M00001" he="21.51mm" wi="76.20mm" file="US08625886-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US08625886-20140107-M00002.NB">
<img id="EMI-M00002" he="34.54mm" wi="76.20mm" file="US08625886-20140107-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00003" nb-file="US08625886-20140107-M00003.NB">
<img id="EMI-M00003" he="8.13mm" wi="76.20mm" file="US08625886-20140107-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00004" nb-file="US08625886-20140107-M00004.NB">
<img id="EMI-M00004" he="16.93mm" wi="76.20mm" file="US08625886-20140107-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00005" nb-file="US08625886-20140107-M00005.NB">
<img id="EMI-M00005" he="7.45mm" wi="76.20mm" file="US08625886-20140107-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method for finding repeated structure for data extraction from document images, said method comprising:
<claim-text>identifying one or more reference records and one or more reference fields thereof, from one or more document images;</claim-text>
<claim-text>generating one or more candidate fields for each of the reference fields;</claim-text>
<claim-text>finding one or more best candidate records from the candidate fields using a probabilistic model, where the search is performed by one or more processors; and,</claim-text>
<claim-text>determining an optimal record set from the best candidate records.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the identification of the reference record and the reference fields includes receiving user annotations of the document image, wherein the user annotations identify the reference record and/or the reference fields.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>for each candidate field, measuring a likelihood of matching an associated reference field using a probabilistic model; and,</claim-text>
<claim-text>discarding candidate fields having a likelihood of matching an associated reference field below a threshold.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the generation of the candidate fields includes:
<claim-text>for each candidate field, measuring a likelihood of matching an associated reference field using a set of heuristics, and,</claim-text>
<claim-text>discarding candidate fields having a likelihood of matching an associated reference field below a threshold.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, where the match qualities of the records are determined using the probabilistic model, wherein the probabilistic model includes terms assessing a probability that a group of one or more fields of a candidate record match a corresponding group of one or more reference fields and/or a term assessing overall match quality of a candidate record to a reference record.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the probabilistic model includes features that encourage explaining as much of the document image as possible.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the probabilistic model includes features that measure perceptual coherence of fields.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the features include one or more of separation, gaps, line breaks, and offsets.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the features include one or more of similarity, unexplained text, and intersections.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein criteria for the determination of the optimal record set include selecting a set of records from the best candidate records that explain as much of the document image as possible.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determination of the optimal record set uses a second probabilistic model independent of the number of the reference fields to asses match quality of the best candidate records to the reference record.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said method assumes no column structure in the document images.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>training the probabilistic model with one or more annotated document images.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>extracting data from the optimal record set, wherein the extracted data is stored in a database.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. A document processing system for finding repeated structure for data extraction from document images, said system comprising:
<claim-text>an imaging device and/or a document conversion system that converts documents into document images; and,</claim-text>
<claim-text>a data extraction system that extracts repeated structure from document images using a probabilistic model for cue integration, wherein the data extraction system including one or more processors executing computer executable instructions stored on a non-transient computer readable medium that:</claim-text>
<claim-text>identify a reference record and one or more reference fields thereof, from a document image;</claim-text>
<claim-text>generate one or more candidate fields for each of the reference fields:</claim-text>
<claim-text>find one or more best candidate records from the candidate fields using the probabilistic model; and,</claim-text>
<claim-text>determine an optimal record set from the best candidate records.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The document processing system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, further comprising:
<claim-text>a database, wherein said database stores data extracted by the data extraction system.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. A system for finding repeated structure for data extraction from document images, said system comprising:
<claim-text>one or more non-transient computer readable mediums having computer executable instructions that:
<claim-text>identify a reference record and one or more reference fields thereof, from a document image;</claim-text>
<claim-text>generate one or more candidate fields for each of the reference fields;</claim-text>
<claim-text>find one or more best candidate records from the candidate fields using a probabilistic model; and,</claim-text>
<claim-text>determine an optimal record set from the best candidate records; and,</claim-text>
</claim-text>
<claim-text>one or more processors that execute the computer executable instructions to find repeated structure in document images.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, said system further comprising:
<claim-text>a user input device, wherein the identification of the reference record and the reference fields includes receiving user annotations of the document images from the user input device.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the probabilistic model includes features that measure perceptual coherence of fields.</claim-text>
</claim>
</claims>
</us-patent-grant>
