<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08624910-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08624910</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12868586</doc-number>
<date>20100825</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<us-term-of-grant>
<us-term-extension>585</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>09</class>
<subclass>G</subclass>
<main-group>5</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>13</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20110101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>T</subclass>
<main-group>15</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>T</subclass>
<main-group>11</main-group>
<subgroup>40</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>40</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>36</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>54</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>15</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>9</main-group>
<subgroup>30</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>9</main-group>
<subgroup>40</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>345582</main-classification>
<further-classification>345503</further-classification>
<further-classification>345522</further-classification>
<further-classification>345536</further-classification>
<further-classification>345552</further-classification>
<further-classification>382305</further-classification>
<further-classification>382260</further-classification>
<further-classification>382274</further-classification>
<further-classification>382276</further-classification>
<further-classification>382303</further-classification>
<further-classification>711100</further-classification>
<further-classification>711118</further-classification>
<further-classification>711119</further-classification>
<further-classification>712 21</further-classification>
<further-classification>712203</further-classification>
<further-classification>712205</further-classification>
<further-classification>712220</further-classification>
<further-classification>718107</further-classification>
</classification-national>
<invention-title id="d2e53">Register indexed sampler for texture opcodes</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>6269391</doc-number>
<kind>B1</kind>
<name>Gillespie</name>
<date>20010700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>718100</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>7256796</doc-number>
<kind>B1</kind>
<name>Bastos et al.</name>
<date>20070800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345614</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>7411592</doc-number>
<kind>B1</kind>
<name>Dunn</name>
<date>20080800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345581</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>7773092</doc-number>
<kind>B1</kind>
<name>Heckbert</name>
<date>20100800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345582</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>7948495</doc-number>
<kind>B1</kind>
<name>Nordquist et al.</name>
<date>20110500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345522</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>8189009</doc-number>
<kind>B1</kind>
<name>Brown et al.</name>
<date>20120500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345582</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2006/0212681</doc-number>
<kind>A1</kind>
<name>Codrescu et al.</name>
<date>20060900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>712214</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2008/0109795</doc-number>
<kind>A1</kind>
<name>Buck et al.</name>
<date>20080500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>717137</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2008/0198169</doc-number>
<kind>A1</kind>
<name>Boyd et al.</name>
<date>20080800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345522</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2008/0246773</doc-number>
<kind>A1</kind>
<name>Jiao et al.</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345522</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2009/0147017</doc-number>
<kind>A1</kind>
<name>Jiao</name>
<date>20090600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345582</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2010/0002000</doc-number>
<kind>A1</kind>
<name>Everitt et al.</name>
<date>20100100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345426</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2010/0122042</doc-number>
<kind>A1</kind>
<name>Lin</name>
<date>20100500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711154</main-classification></classification-national>
</us-citation>
</us-references-cited>
<number-of-claims>18</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>345418</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345581-582</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345589</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345614</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345618</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345501-503</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345506</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345520</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345522</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345530</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345536-538</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345548-552</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345553</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382254</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382260-269</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382274</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382276</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382285</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382300</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382303-305</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382307</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>711  5</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>711100</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>711113</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>711118-120</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>711119</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>711125</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>711168-170</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>711213</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>711215</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>712  1</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>712  9- 10</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>712 19- 21</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>712 25</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>712 31</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>712 34</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>712200</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>712203</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>712205-206</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>712220</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>712224</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>712235</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>718100-101</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>718106-107</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>8</number-of-drawing-sheets>
<number-of-figures>8</number-of-figures>
</figures>
<us-related-documents>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>61245104</doc-number>
<date>20090923</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20110069076</doc-number>
<kind>A1</kind>
<date>20110324</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Lindholm</last-name>
<first-name>John Erik</first-name>
<address>
<city>Saratoga</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Tang</last-name>
<first-name>Yan Yan</first-name>
<address>
<city>San Jose</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Lindholm</last-name>
<first-name>John Erik</first-name>
<address>
<city>Saratoga</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Tang</last-name>
<first-name>Yan Yan</first-name>
<address>
<city>San Jose</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Patterson &#x26; Sheridan, L.L.P.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Nvidia Corporation</orgname>
<role>02</role>
<address>
<city>Santa Clara</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Sajous</last-name>
<first-name>Wesner</first-name>
<department>2677</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">One embodiment of the present invention sets forth a technique for dynamically specifying a texture header and texture sampler using an index. The index corresponds to a particular register value that may be static or computed during execution of a shader program. Any texture operation instruction may specify an index value for each of the texture header and the texture sampler.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="205.32mm" wi="159.85mm" file="US08624910-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="158.75mm" wi="110.74mm" file="US08624910-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="226.99mm" wi="160.95mm" file="US08624910-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="178.22mm" wi="156.38mm" file="US08624910-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="173.74mm" wi="134.62mm" file="US08624910-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="207.94mm" wi="152.91mm" file="US08624910-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="220.13mm" wi="158.92mm" file="US08624910-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="157.31mm" wi="130.81mm" file="US08624910-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="234.44mm" wi="182.29mm" file="US08624910-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application claims priority benefit to U.S. provisional patent application titled, &#x201c;Register Indexed Header/Sampler ID for Texture Opcodes,&#x201d; filed on Sep. 23, 2009 and having Ser. No. 61/245,104.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">1. Field of the Invention</p>
<p id="p-0004" num="0003">The present invention generally relates to specifying texture header and texture sampler values for texture map operations.</p>
<p id="p-0005" num="0004">2. Description of the Related Art</p>
<p id="p-0006" num="0005">Execution of a texture fetch operation requires the specification of a texture header and a texture sampler which together describe the texture data (format and particular texture map) to be sampled and the filter to be applied to the sampled texture map to produce filtered texels for a pixel. Traditionally these have been specified by immediate values, meaning that the identity of the particular texture header and the particular texture sampler is static specified for the instruction at the time that a shader program including the texture fetch operation is compiled for execution by a processor.</p>
<p id="p-0007" num="0006">When a graphics API such as Direct X <b>10</b> is used, a texture fetch instruction may not be used within an if-else statement. Therefore, different texture header and/or sampler may not be specified as inputs to a texture fetch operation in the &#x201c;if&#x201d; clause compared with a texture fetch operation in the &#x201c;else&#x201d; clause. Dynamic selection of different texture headers and/or samplers during execution of a shader program may not be possible.</p>
<p id="p-0008" num="0007">Accordingly, what is needed in the art is a system and method for dynamically specifying a texture sampler and texture header for texture operations during execution of a shader program.</p>
<heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0009" num="0008">A system and method for dynamically specifying a texture header and texture sampler using an index. The index corresponds to a particular register value that may store static value or a value that is computed during execution of a shader program. Any texture operation instruction may specify an index for each of the texture header and the texture sampler. A single index may specify a unique texture header or texture sampler for each pixel that is processed by a thread in a thread group when the shader program is executed in a multi-threaded processor.</p>
<p id="p-0010" num="0009">Various embodiments of a method of the invention for dynamically determining a texture sampler for a texture operation instruction include receiving a texture operation instruction and determining that the texture operation instruction specifies a first index associated with the texture sampler. A texture sampler is read from a first register using the first index. Texture data is sampled according to a sampling pattern corresponding to the texture sampler. The sampled texture data is filtered to produce filtered texture data for a pixel.</p>
<p id="p-0011" num="0010">Various embodiments of the invention include a system for dynamically determining a texture sampler for a texture operation instruction.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0012" num="0011">So that the manner in which the above recited features of the present invention can be understood in detail, a more particular description of the invention, briefly summarized above, may be had by reference to embodiments, some of which are illustrated in the appended drawings. It is to be noted, however, that the appended drawings illustrate only typical embodiments of this invention and are therefore not to be considered limiting of its scope, for the invention may admit to other equally effective embodiments.</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram illustrating a computer system configured to implement one or more aspects of the present invention;</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram of a parallel processing subsystem for the computer system of <figref idref="DRAWINGS">FIG. 1</figref>, according to one embodiment of the present invention;</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 3A</figref> is a block diagram of a GPC within one of the PPUs of <figref idref="DRAWINGS">FIG. 2</figref>, according to one embodiment of the present invention;</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 3B</figref> is a block diagram of a partition unit within one of the PPUs of <figref idref="DRAWINGS">FIG. 2</figref>, according to one embodiment of the present invention;</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 4</figref> is a conceptual diagram of a graphics processing pipeline that one or more of the PPUs of <figref idref="DRAWINGS">FIG. 2</figref> can be configured to implement, according to one embodiment of the present invention;</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 5A</figref> is a block diagram of the texture unit of <figref idref="DRAWINGS">FIG. 3A</figref>, according to one embodiment of the present invention;</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 5B</figref> is a block diagram of the texture sampler and header unit of <figref idref="DRAWINGS">FIG. 5A</figref>, according to one embodiment of the present invention; and</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 6</figref> is a flow diagram of method steps for determining the texture sampler and header specified for a texture operation, according to one embodiment of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0021" num="0020">In the following description, numerous specific details are set forth to provide a more thorough understanding of the present invention. However, it will be apparent to one of skill in the art that the present invention may be practiced without one or more of these specific details. In other instances, well-known features have not been described in order to avoid obscuring the present invention.</p>
<heading id="h-0006" level="1">System Overview</heading>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram illustrating a computer system <b>100</b> configured to implement one or more aspects of the present invention. Computer system <b>100</b> includes a central processing unit (CPU) <b>102</b> and a system memory <b>104</b> communicating via a bus path through a memory bridge <b>105</b>. Memory bridge <b>105</b> may be integrated into CPU <b>102</b> as shown in <figref idref="DRAWINGS">FIG. 1</figref>. Alternatively, memory bridge <b>105</b>, may be a conventional device, e.g., a Northbridge chip, that is connected via a bus to CPU <b>102</b>. Memory bridge <b>105</b> is connected via communication path <b>106</b> (e.g., a HyperTransport link) to an I/O (input/output) bridge <b>107</b>. I/O bridge <b>107</b>, which may be, e.g., a Southbridge chip, receives user input from one or more user input devices <b>108</b> (e.g., keyboard, mouse) and forwards the input to CPU <b>102</b> via path <b>106</b> and memory bridge <b>105</b>. A parallel processing subsystem <b>112</b> is coupled to memory bridge <b>105</b> via a bus or other communication path <b>113</b> (e.g., a PCI Express, Accelerated Graphics Port, or HyperTransport link); in one embodiment parallel processing subsystem <b>112</b> is a graphics subsystem that delivers pixels to a display device <b>110</b> (e.g., a conventional CRT or LCD based monitor). A system disk <b>114</b> is also connected to I/O bridge <b>107</b>. A switch <b>116</b> provides connections between I/O bridge <b>107</b> and other components such as a network adapter <b>118</b> and various add-in cards <b>120</b> and <b>121</b>. Other components (not explicitly shown), including USB or other port connections, CD drives, DVD drives, film recording devices, and the like, may also be connected to I/O bridge <b>107</b>. Communication paths interconnecting the various components in <figref idref="DRAWINGS">FIG. 1</figref> may be implemented using any suitable protocols, such as PCI (Peripheral Component Interconnect), PCI-Express (PCI-E), AGP (Accelerated Graphics Port), HyperTransport, or any other bus or point-to-point communication protocol(s), and connections between different devices may use different protocols as is known in the art.</p>
<p id="p-0023" num="0022">In one embodiment, the parallel processing subsystem <b>112</b> incorporates circuitry optimized for graphics and video processing, including, for example, video output circuitry, and constitutes a graphics processing unit (GPU). In another embodiment, the parallel processing subsystem <b>112</b> incorporates circuitry optimized for general purpose processing, while preserving the underlying computational architecture, described in greater detail herein. In yet another embodiment, the parallel processing subsystem <b>112</b> may be integrated with one or more other system elements, such as the memory bridge <b>105</b>, CPU <b>102</b>, and I/O bridge <b>107</b> to form a system on chip (SoC).</p>
<p id="p-0024" num="0023">It will be appreciated that the system shown herein is illustrative and that variations and modifications are possible. The connection topology, including the number and arrangement of bridges, may be modified as desired. For instance, in some embodiments, system memory <b>104</b> is connected to CPU <b>102</b> directly rather than through a bridge, and other devices communicate with system memory <b>104</b> via memory bridge <b>105</b> and CPU <b>102</b>. In other alternative topologies, parallel processing subsystem <b>112</b> is connected to I/O bridge <b>107</b> or directly to CPU <b>102</b>, rather than to memory bridge <b>105</b>. In still other embodiments, one or more of CPU <b>102</b>, I/O bridge <b>107</b>, parallel processing subsystem <b>112</b>, and memory bridge <b>105</b> may be integrated into one or more chips. The particular components shown herein are optional; for instance, any number of add-in cards or peripheral devices might be supported. In some embodiments, switch <b>116</b> is eliminated, and network adapter <b>118</b> and add-in cards <b>120</b>, <b>121</b> connect directly to I/O bridge <b>107</b>.</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 2</figref> illustrates a parallel processing subsystem <b>112</b>, according to one embodiment of the present invention. As shown, parallel processing subsystem <b>112</b> includes one or more parallel processing units (PPUs) <b>202</b>, each of which is coupled to a local parallel processing (PP) memory <b>204</b>. In general, a parallel processing subsystem includes a number U of PPUs, where U&#x2267;1. (Herein, multiple instances of like objects are denoted with reference numbers identifying the object and parenthetical numbers identifying the instance where needed.) PPUs <b>202</b> and parallel processing memories <b>204</b> may be implemented using one or more integrated circuit devices, such as programmable processors, application specific integrated circuits (ASICs), or memory devices, or in any other technically feasible fashion.</p>
<p id="p-0026" num="0025">Referring again to <figref idref="DRAWINGS">FIG. 1</figref>, in some embodiments, some or all of PPUs <b>202</b> in parallel processing subsystem <b>112</b> are graphics processors with rendering pipelines that can be configured to perform various tasks related to generating pixel data from graphics data supplied by CPU <b>102</b> and/or system memory <b>104</b>, interacting with local parallel processing memory <b>204</b> (which can be used as graphics memory including, e.g., a conventional frame buffer) to store and update pixel data, delivering pixel data to display device <b>110</b>, and the like. In some embodiments, parallel processing subsystem <b>112</b> may include one or more PPUs <b>202</b> that operate as graphics processors and one or more other PPUs <b>202</b> that are used for general-purpose computations. The PPUs may be identical or different, and each PPU may have its own dedicated parallel processing memory device(s) or no dedicated parallel processing memory device(s). One or more PPUs <b>202</b> may output data to display device <b>110</b> or each PPU <b>202</b> may output data to one or more display devices <b>110</b>.</p>
<p id="p-0027" num="0026">In operation, CPU <b>102</b> is the master processor of computer system <b>100</b>, controlling and coordinating operations of other system components. In particular, CPU <b>102</b> issues commands that control the operation of PPUs <b>202</b>. In some embodiments, CPU <b>102</b> writes a stream of commands for each PPU <b>202</b> to a command buffer (not explicitly shown in either <figref idref="DRAWINGS">FIG. 1</figref> or <figref idref="DRAWINGS">FIG. 2</figref>) that may be located in system memory <b>104</b>, parallel processing memory <b>204</b>, or another storage location accessible to both CPU <b>102</b> and PPU <b>202</b>. PPU <b>202</b> reads the command stream from the command buffer and then executes commands asynchronously relative to the operation of CPU <b>102</b>. CPU <b>102</b> may also create data buffers that PPUs <b>202</b> may read in response to commands in the command buffer. Each command and data buffer may be read by each of PPUs <b>202</b>.</p>
<p id="p-0028" num="0027">Referring back now to <figref idref="DRAWINGS">FIG. 2</figref>, each PPU <b>202</b> includes an I/O (input/output) unit <b>205</b> that communicates with the rest of computer system <b>100</b> via communication path <b>113</b>, which connects to memory bridge <b>105</b> (or, in one alternative embodiment, directly to CPU <b>102</b>). The connection of PPU <b>202</b> to the rest of computer system <b>100</b> may also be varied. In some embodiments, parallel processing subsystem <b>112</b> is implemented as an add-in card that can be inserted into an expansion slot of computer system <b>100</b>. In other embodiments, a PPU <b>202</b> can be integrated on a single chip with a bus bridge, such as memory bridge <b>105</b> or I/O bridge <b>107</b>. In still other embodiments, some or all elements of PPU <b>202</b> may be integrated on a single chip with CPU <b>102</b>.</p>
<p id="p-0029" num="0028">In one embodiment, communication path <b>113</b> is a PCI-Express link, in which dedicated lanes are allocated to each PPU <b>202</b>, as is known in the art. Other communication paths may also be used. An I/O unit <b>205</b> generates packets (or other signals) for transmission on communication path <b>113</b> and also receives all incoming packets (or other signals) from communication path <b>113</b>, directing the incoming packets to appropriate components of PPU <b>202</b>. For example, commands related to processing tasks may be directed to a host interface <b>206</b>, while commands related to memory operations (e.g., reading from or writing to parallel processing memory <b>204</b>) may be directed to a memory crossbar unit <b>210</b>. Host interface <b>206</b> reads each command buffer and outputs the work specified by the command buffer to a front end <b>212</b>.</p>
<p id="p-0030" num="0029">Each PPU <b>202</b> advantageously implements a highly parallel processing architecture. As shown in detail, PPU <b>202</b>(<b>0</b>) includes a processing cluster array <b>230</b> that includes a number C of general processing clusters (GPCs) <b>208</b>, where C&#x2267;1. Each GPC <b>208</b> is capable of executing a large number (e.g., hundreds or thousands) of threads concurrently, where each thread is an instance of a program. In various applications, different GPCs <b>208</b> may be allocated for processing different types of programs or for performing different types of computations. For example, in a graphics application, a first set of GPCs <b>208</b> may be allocated to perform tessellation operations and to produce primitive topologies for patches, and a second set of GPCs <b>208</b> may be allocated to perform tessellation shading to evaluate patch parameters for the primitive topologies and to determine vertex positions and other per-vertex attributes. The allocation of GPCs <b>208</b> may vary depending on the workload arising for each type of program or computation. Alternatively, GPCs <b>208</b> may be allocated to perform processing tasks using a time-slice scheme to switch between different processing tasks.</p>
<p id="p-0031" num="0030">GPCs <b>208</b> receive processing tasks to be executed via a work distribution unit <b>200</b>, which receives commands defining processing tasks from front end unit <b>212</b>. Processing tasks include pointers to data to be processed, e.g., surface (patch) data, primitive data, vertex data, and/or pixel data, as well as state parameters and commands defining how the data is to be processed (e.g., what program is to be executed). Work distribution unit <b>200</b> may be configured to fetch the pointers corresponding to the processing tasks, may receive the pointers from front end <b>212</b>, or may receive the data directly from front end <b>212</b>. In some embodiments, indices specify the location of the data in an array. Front end <b>212</b> ensures that GPCs <b>208</b> are configured to a valid state before the processing specified by the command buffers is initiated.</p>
<p id="p-0032" num="0031">When PPU <b>202</b> is used for graphics processing, for example, the processing workload for each patch is divided into approximately equal sized tasks to enable distribution of the tessellation processing to multiple GPCs <b>208</b>. A work distribution unit <b>200</b> may be configured to output tasks at a frequency capable of providing tasks to multiple GPCs <b>208</b> for processing. In some embodiments of the present invention, portions of GPCs <b>208</b> are configured to perform different types of processing. For example a first portion may be configured to perform vertex shading and topology generation, a second portion may be configured to perform tessellation and geometry shading, and a third portion may be configured to perform pixel shading in screen space to produce a rendered image. The ability to allocate portions of GPCs <b>208</b> for performing different types of processing tasks efficiently accommodates any expansion and contraction of data produced by those different types of processing tasks. Intermediate data produced by GPCs <b>208</b> may be buffered to allow the intermediate data to be transmitted between GPCs <b>208</b> with minimal stalling in cases where the rate at which data is accepted by a downstream GPC <b>208</b> lags the rate at which data is produced by an upstream GPC <b>208</b>.</p>
<p id="p-0033" num="0032">Memory interface <b>214</b> may be partitioned into a number D of memory partition units that are each coupled to a portion of parallel processing memory <b>204</b>, where D&#x2267;1. Each portion of parallel processing memory <b>204</b> generally includes one or more memory devices (e.g DRAM <b>220</b>). Persons skilled in the art will appreciate that DRAM <b>220</b> may be replaced with other suitable storage devices and can be of generally conventional design. A detailed description is therefore omitted. Render targets, such as frame buffers or texture maps may be stored across DRAMs <b>220</b>, allowing partition units <b>215</b> to write portions of each render target in parallel to efficiently use the available bandwidth of parallel processing memory <b>204</b>.</p>
<p id="p-0034" num="0033">Any one of GPCs <b>208</b> may process data to be written to any of the DRAMs <b>220</b> within parallel processing memory <b>204</b>. Crossbar unit <b>210</b> is configured to route the output of each GPC <b>208</b> to the input of any partition unit <b>215</b> or to another GPC <b>208</b> for further processing. GPCs <b>208</b> communicate with memory interface <b>214</b> through crossbar unit <b>210</b> to read from or write to various external memory devices. In one embodiment, crossbar unit <b>210</b> has a connection to memory interface <b>214</b> to communicate with I/O unit <b>205</b>, as well as a connection to local parallel processing memory <b>204</b>, thereby enabling the processing cores within the different GPCs <b>208</b> to communicate with system memory <b>104</b> or other memory that is not local to PPU <b>202</b>. Crossbar unit <b>210</b> may use virtual channels to separate traffic streams between the GPCs <b>208</b> and partition units <b>215</b>.</p>
<p id="p-0035" num="0034">Again, GPCs <b>208</b> can be programmed to execute processing tasks relating to a wide variety of applications, including but not limited to, linear and nonlinear data transforms, filtering of video and/or audio data, modeling operations (e.g., applying laws of physics to determine position, velocity and other attributes of objects), image rendering operations (e.g., tessellation shader, vertex shader, geometry shader, and/or pixel shader programs), and so on. PPUs <b>202</b> may transfer data from system memory <b>104</b> and/or local parallel processing memories <b>204</b> into internal (on-chip) memory, process the data, and write result data back to system memory <b>104</b> and/or local parallel processing memories <b>204</b>, where such data can be accessed by other system components, including CPU <b>102</b> or another parallel processing subsystem <b>112</b>.</p>
<p id="p-0036" num="0035">A PPU <b>202</b> may be provided with any amount of local parallel processing memory <b>204</b>, including no local memory, and may use local memory and system memory in any combination. For instance, a PPU <b>202</b> can be a graphics processor in a unified memory architecture (UMA) embodiment. In such embodiments, little or no dedicated graphics (parallel processing) memory would be provided, and PPU <b>202</b> would use system memory exclusively or almost exclusively. In UMA embodiments, a PPU <b>202</b> may be integrated into a bridge chip or processor chip or provided as a discrete chip with a high-speed link (e.g., PCI-Express) connecting the PPU <b>202</b> to system memory via a bridge chip or other communication means.</p>
<p id="p-0037" num="0036">As noted above, any number of PPUs <b>202</b> can be included in a parallel processing subsystem <b>112</b>. For instance, multiple PPUs <b>202</b> can be provided on a single add-in card, or multiple add-in cards can be connected to communication path <b>113</b>, or one or more PPUs <b>202</b> can be integrated into a bridge chip. PPUs <b>202</b> in a multi-PPU system may be identical to or different from one another. For instance, different PPUs <b>202</b> might have different numbers of processing cores, different amounts of local parallel processing memory, and so on. Where multiple PPUs <b>202</b> are present, those PPUs may be operated in parallel to process data at a higher throughput than is possible with a single PPU <b>202</b>. Systems incorporating one or more PPUs <b>202</b> may be implemented in a variety of configurations and form factors, including desktop, laptop, or handheld personal computers, servers, workstations, game consoles, embedded systems, and the like.</p>
<heading id="h-0007" level="1">Processing Cluster Array Overview</heading>
<p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. 3A</figref> is a block diagram of a GPC <b>208</b> within one of the PPUs <b>202</b> of <figref idref="DRAWINGS">FIG. 2</figref>, according to one embodiment of the present invention. Each GPC <b>208</b> may be configured to execute a large number of threads in parallel, where the term &#x201c;thread&#x201d; refers to an instance of a particular program executing on a particular set of input data. In some embodiments, single-instruction, multiple-data (SIMD) instruction issue techniques are used to support parallel execution of a large number of threads without providing multiple independent instruction units. In other embodiments, single-instruction, multiple-thread (SIMT) techniques are used to support parallel execution of a large number of generally synchronized threads, using a common instruction unit configured to issue instructions to a set of processing engines within each one of the GPCs <b>208</b>. Unlike a SIMD execution regime, where all processing engines typically execute identical instructions, SIMT execution allows different threads to more readily follow divergent execution paths through a given thread program. Persons skilled in the art will understand that a SIMD processing regime represents a functional subset of a SIMT processing regime.</p>
<p id="p-0039" num="0038">In graphics applications, a GPC <b>208</b> may be configured to implement a primitive engine for performing screen space graphics processing functions that may include, but are not limited to primitive setup, rasterization, and z culling. The primitive engine receives a processing task from work distribution unit <b>200</b>, and when the processing task does not require the operations performed by primitive engine, the processing task is passed through the primitive engine to a pipeline manager <b>305</b>. Operation of GPC <b>208</b> is advantageously controlled via a pipeline manager <b>305</b> that distributes processing tasks to streaming multiprocessors (SPMs) <b>310</b>. Pipeline manager <b>305</b> may also be configured to control a work distribution crossbar <b>330</b> by specifying destinations for processed data output by SPMs <b>310</b>.</p>
<p id="p-0040" num="0039">In one embodiment, each GPC <b>208</b> includes a number M of SPMs <b>310</b>, where M&#x2267;1, each SPM <b>310</b> configured to process one or more thread groups. The series of instructions transmitted to a particular GPC <b>208</b> constitutes a thread, as previously defined herein, and the collection of a certain number of concurrently executing threads across the parallel processing engines (not shown) within an SPM <b>310</b> is referred to herein as a &#x201c;thread group.&#x201d; As used herein, a &#x201c;thread group&#x201d; refers to a group of threads (also referred to as a warp) concurrently executing the same program on different input data, with each thread of the group being assigned to a different processing engine within an SPM <b>310</b>. A thread group may include fewer threads than the number of processing engines within the SPM <b>310</b>, in which case some processing engines will be idle during cycles when that thread group is being processed. A thread group may also include more threads than the number of processing engines within the SPM <b>310</b>, in which case processing will take place over multiple clock cycles. Since each SPM <b>310</b> can support up to G thread groups concurrently, it follows that up to G&#xd7;M thread groups can be executing in GPC <b>208</b> at any given time.</p>
<p id="p-0041" num="0040">Additionally, a plurality of related thread groups may be active (in different phases of execution) at the same time within an SPM <b>310</b>. This collection of thread groups is referred to herein as a &#x201c;cooperative thread array&#x201d; (&#x201c;CTA&#x201d;). The size of a particular CTA is equal to m*k, where k is the number of concurrently executing threads in a thread group and is typically an integer multiple of the number of parallel processing engines within the SPM <b>310</b>, and m is the number of thread groups simultaneously active within the SPM <b>310</b>. The size of a CTA is generally determined by the programmer and the amount of hardware resources, such as memory or registers, available to the CTA.</p>
<p id="p-0042" num="0041">An exclusive local address space is available to each thread, and a shared per-CTA address space is used to pass data between threads within a CTA. Data stored in the per-thread local address space and per-CTA address space is stored in L1 cache <b>320</b>, and an eviction policy may be used to favor keeping the data in L1 cache <b>320</b>. Each SPM <b>310</b> uses space in a corresponding L1 cache <b>320</b> that is used to perform load and store operations. Each SPM <b>310</b> also has access to L2 caches within the partition units <b>215</b> that are shared among all GPCs <b>208</b> and may be used to transfer data between threads. Finally, SPMs <b>310</b> also have access to off-chip &#x201c;global&#x201d; memory, which can include, e.g., parallel processing memory <b>204</b> and/or system memory <b>104</b>. An L2 cache may be used to store data that is written to and read from global memory. It is to be understood that any memory external to PPU <b>202</b> may be used as global memory. Additionally, an L1.5 cache <b>335</b> may be included within the GPC <b>208</b>, configured to receive and hold data fetched from memory via memory interface <b>214</b> requested by SPM <b>310</b>, including instructions, uniform data, and constant data, and provide the requested data to SPM <b>310</b>. Embodiments having multiple SPMs <b>310</b> in GPC <b>208</b> beneficially share common instructions and data cached in L1.5 cache <b>335</b>.</p>
<p id="p-0043" num="0042">Also, each SPM <b>310</b> advantageously includes an identical set of functional units (e.g., arithmetic logic units, etc.) that may be pipelined, allowing a new instruction to be issued before a previous instruction has finished, as is known in the art. Any combination of functional units may be provided. In one embodiment, the functional units support a variety of operations including integer and floating point arithmetic (e.g., addition and multiplication), comparison operations, Boolean operations (AND, OR, XOR), bit-shifting, and computation of various algebraic functions (e.g., planar interpolation, trigonometric, exponential, and logarithmic functions, etc.); and the same functional-unit hardware can be leveraged to perform different operations.</p>
<p id="p-0044" num="0043">Each GPC <b>208</b> may include a memory management unit (MMU) <b>328</b> that is configured to map virtual addresses into physical addresses. In other embodiments, MMU(s) <b>328</b> may reside within the memory interface <b>214</b>. The MMU <b>328</b> includes a set of page table entries (PTEs) used to map a virtual address to a physical address of a tile and optionally a cache line index. The physical address is processed to distribute surface data access locality to allow efficient request interleaving among partition units. The cache line index may be used to determine whether of not a request for a cache line is a hit or miss.</p>
<p id="p-0045" num="0044">In graphics applications, a GPC <b>208</b> may be configured such that each SPM <b>310</b> is coupled to a texture unit <b>315</b> for performing texture mapping operations, e.g., determining texture sample positions, reading texture data, and filtering the texture data. Texture data is read via memory interface <b>214</b> and is fetched from an L2 cache, parallel processing memory <b>204</b>, or system memory <b>104</b>, as needed. Texture unit <b>315</b> may be configured to store the texture data in an internal cache. In some embodiments, texture unit <b>315</b> is coupled to L1 cache <b>320</b>, and texture data is stored in L1 cache <b>320</b>. Each SPM <b>310</b> outputs processed tasks to work distribution crossbar <b>330</b> in order to provide the processed task to another GPC <b>208</b> for further processing or to store the processed task in an L2 cache, parallel processing memory <b>204</b>, or system memory <b>104</b> via crossbar unit <b>210</b>. A preROP (pre-raster operations) <b>325</b> is configured to receive data from SPM <b>310</b>, direct data to ROP units within partition units <b>215</b>, and perform optimizations for color blending, organize pixel color data, and perform address translations.</p>
<p id="p-0046" num="0045">It will be appreciated that the core architecture described herein is illustrative and that variations and modifications are possible. Any number of processing engines, e.g., primitive engines, SPMs <b>310</b>, texture units <b>315</b>, or preROPs <b>325</b> may be included within a GPC <b>208</b>. Further, while only one GPC <b>208</b> is shown, a PPU <b>202</b> may include any number of GPCs <b>208</b> that are advantageously functionally similar to one another so that execution behavior does not depend on which GPC <b>208</b> receives a particular processing task. Further, each GPC <b>208</b> advantageously operates independently of other GPCs <b>208</b> using separate and distinct processing engines, L1 caches <b>320</b>, and so on.</p>
<p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. 3B</figref> is a block diagram of a partition unit <b>215</b> within one of the PPUs <b>202</b> of <figref idref="DRAWINGS">FIG. 2</figref>, according to one embodiment of the present invention. As shown, partition unit <b>215</b> includes a L2 cache <b>350</b>, a frame buffer (FB) <b>355</b>, and a raster operations unit (ROP) <b>360</b>. L2 cache <b>350</b> is a read/write cache that is configured to perform load and store operations received from crossbar unit <b>210</b> and ROP <b>360</b>. Read misses and urgent writeback requests are output by L2 cache <b>350</b> to FB <b>355</b> for processing. Dirty updates are also sent to FB <b>355</b> for opportunistic processing. FB <b>355</b> interfaces directly with DRAM <b>220</b>, outputting read and write requests and receiving data read from DRAM <b>220</b>.</p>
<p id="p-0048" num="0047">In graphics applications, ROP <b>360</b> is a processing unit that performs raster operations, such as stencil, z test, blending, and the like, and outputs pixel data as processed graphics data for storage in graphics memory. In some embodiments of the present invention, ROP <b>360</b> is included within each GPC <b>208</b> instead of partition unit <b>215</b>, and pixel read and write requests are transmitted over crossbar unit <b>210</b> instead of pixel fragment data.</p>
<p id="p-0049" num="0048">The processed graphics data may be displayed on display device <b>110</b> or routed for further processing by CPU <b>102</b> or by one of the processing entities within parallel processing subsystem <b>112</b>. Each partition unit <b>215</b> includes a ROP <b>360</b> in order to distribute processing of the raster operations. In some embodiments, ROP <b>360</b> may be configured to compress z or color data that is written to memory and decompress z or color data that is read from memory.</p>
<p id="p-0050" num="0049">Persons skilled in the art will understand that the architecture described in <figref idref="DRAWINGS">FIGS. 1</figref>, <b>2</b>, <b>3</b>A and <b>3</b>B in no way limits the scope of the present invention and that the techniques taught herein may be implemented on any properly configured processing unit, including, without limitation, one or more CPUs, one or more multi-core CPUs, one or more PPUs <b>202</b>, one or more GPCs <b>208</b>, one or more graphics or special purpose processing units, or the like, without departing the scope of the present invention.</p>
<p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. 4</figref> is a conceptual diagram of a graphics processing pipeline <b>400</b>, that one or more of the PPUs <b>202</b> of <figref idref="DRAWINGS">FIG. 2</figref> can be configured to implement, according to one embodiment of the present invention. For example, one of the SPMs <b>310</b> may be configured to perform the functions of one or more of a vertex processing unit <b>415</b>, a geometry processing unit <b>425</b>, and a fragment processing unit <b>460</b>. The functions of data assembler <b>410</b>, primitive assembler <b>420</b>, rasterizer <b>455</b>, and raster operations unit <b>465</b> may also be performed by other processing engines within a GPC <b>208</b> and a corresponding partition unit <b>215</b>. Alternately, graphics processing pipeline <b>400</b> may be implemented using dedicated processing units for one or more functions.</p>
<p id="p-0052" num="0051">Data assembler <b>410</b> processing unit collects vertex data for high-order surfaces, primitives, and the like, and outputs the vertex data, including the vertex attributes, to vertex processing unit <b>415</b>. Vertex processing unit <b>415</b> is a programmable execution unit that is configured to execute vertex shader programs, lighting and transforming vertex data as specified by the vertex shader programs. For example, vertex processing unit <b>415</b> may be programmed to transform the vertex data from an object-based coordinate representation (object space) to an alternatively based coordinate system such as world space or normalized device coordinates (NDC) space. Vertex processing unit <b>415</b> may read data that is stored in L1 cache <b>320</b>, parallel processing memory <b>204</b>, or system memory <b>104</b> by data assembler <b>410</b> for use in processing the vertex data.</p>
<p id="p-0053" num="0052">Primitive assembler <b>420</b> receives vertex attributes from vertex processing unit <b>415</b>, reading stored vertex attributes, as needed, and constructs graphics primitives for processing by geometry processing unit <b>425</b>. Graphics primitives include triangles, line segments, points, and the like. Geometry processing unit <b>425</b> is a programmable execution unit that is configured to execute geometry shader programs, transforming graphics primitives received from primitive assembler <b>420</b> as specified by the geometry shader programs. For example, geometry processing unit <b>425</b> may be programmed to subdivide the graphics primitives into one or more new graphics primitives and calculate parameters, such as plane equation coefficients, that are used to rasterize the new graphics primitives.</p>
<p id="p-0054" num="0053">In some embodiments, geometry processing unit <b>425</b> may also add or delete elements in the geometry stream. Geometry processing unit <b>425</b> outputs the parameters and vertices specifying new graphics primitives to a viewport scale, cull, and clip unit <b>450</b>. Geometry processing unit <b>425</b> may read data that is stored in parallel processing memory <b>204</b> or system memory <b>104</b> for use in processing the geometry data. Viewport scale, cull, and clip unit <b>450</b> performs clipping, culling, and viewport scaling and outputs processed graphics primitives to a rasterizer <b>455</b>.</p>
<p id="p-0055" num="0054">Rasterizer <b>455</b> scan converts the new graphics primitives and outputs fragments and coverage data to fragment processing unit <b>460</b>. Additionally, rasterizer <b>455</b> may be configured to perform z culling and other z-based optimizations.</p>
<p id="p-0056" num="0055">Fragment processing unit <b>460</b> is a programmable execution unit that is configured to execute fragment shader programs, transforming fragments received from rasterizer <b>455</b>, as specified by the fragment shader programs. For example, fragment processing unit <b>460</b> may be programmed to perform operations such as perspective correction, texture mapping, shading, blending, and the like, to produce shaded fragments that are output to raster operations unit <b>465</b>. Fragment processing unit <b>460</b> may read data that is stored in parallel processing memory <b>204</b> or system memory <b>104</b> for use in processing the fragment data. Fragments may be shaded at pixel, sample, or other granularity, depending on the programmed sampling rate.</p>
<p id="p-0057" num="0056">Raster operations unit <b>465</b> is a processing unit that performs raster operations, such as stencil, z test, blending, and the like, and outputs pixel data as processed graphics data for storage in graphics memory. The processed graphics data may be stored in graphics memory, e.g., parallel processing memory <b>204</b>, and/or system memory <b>104</b>, for display on display device <b>110</b> or for further processing by CPU <b>102</b> or parallel processing subsystem <b>112</b>. In some embodiments of the present invention, raster operations unit <b>465</b> is configured to compress z or color data that is written to memory and decompress z or color data that is read from memory.</p>
<heading id="h-0008" level="1">Indexed Texture Headers and Samplers</heading>
<p id="p-0058" num="0057">Any texture operation instruction may specify an index value for each of the texture header and the texture sampler. Therefore, the texture header and texture sampler may be specified dynamically during execution of the texture operation instruction. A texture fetch operation instruction, TEX, includes a .I specifier that has either a TRUE or FALSE value. When the .I specifier is FALSE, the operands are used directly as immediate values of the texture sampler and texture header. When the .I specifier is TRUE, the operands are used as indices for the texture sampler and texture header. The code shown in TABLE 1 illustrates an embodiment of a format for the texture fetch operation, where #tidU08 and #smpU05 are operands for an immediate (8 bit) texture header and an immediate (5 bit) texture sampler, respectively. When the .I identifier is TRUE the #tidU08 and #smpU05 operands are ignored and the texture header index and texture sampler index are specified by the Ra operand. Additional specifiers and operands may be included in the texture fetch operation instruction. Other embodiments may use a different format for specifying the texture header index and texture sampler index.</p>
<p id="p-0059" num="0058">
<tables id="TABLE-US-00001" num="00001">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="center"/>
<thead>
<row>
<entry namest="1" nameend="1" rowsep="1">TABLE 1</entry>
</row>
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
<row>
<entry>Texture Fetch Operation Instruction Format</entry>
</row>
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="14pt" align="left"/>
<colspec colname="1" colwidth="203pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>TEX{.I} Rd, Ra, #tidU08, #smpU05;</entry>
</row>
<row>
<entry/>
<entry>.I: Indexed texture header and sampler supplied</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="28pt" align="left"/>
<colspec colname="1" colwidth="189pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>Header (U09) and sampler (U07) are packed in Ra as:</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="56pt" align="left"/>
<colspec colname="1" colwidth="161pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>header[31:23] | sampler[22:16]</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="14pt" align="left"/>
<colspec colname="1" colwidth="203pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>#tidU08: texture header id</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="42pt" align="left"/>
<colspec colname="1" colwidth="175pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>The texture target is given by tidU08: The low 7-bits</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="14pt" align="left"/>
<colspec colname="1" colwidth="203pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>specify texture target (0-127).</entry>
</row>
<row>
<entry/>
<entry>#smpU05: texture sampler id</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="42pt" align="left"/>
<colspec colname="1" colwidth="175pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>The sampler to be used is smpU05: The low 4 bits</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="14pt" align="left"/>
<colspec colname="1" colwidth="203pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>specify the sampler (0-15).</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
<br/>
The texture coordinates (s, t, and r) are stored in vector registers Ra and Rb and the result of the texture fetch operation is written back starting at register Rd. When .I is TRUE, the texture header and texture sampler indices are also stored in Ra (the header is stored in bits [31:23] and the sampler is stored in bits [22:16]). An array index may also be stored in Ra when .I is TRUE. In one embodiment Rb stores a level-of-detail mipmap bias, and a programmable texel offset.
</p>
<p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. 5A</figref> is a block diagram of the texture unit <b>315</b> of <figref idref="DRAWINGS">FIG. 3A</figref>, according to one embodiment of the present invention. The texture unit <b>315</b> includes an operation translation unit <b>505</b>, a read request arbiter <b>510</b>, a texture transpose buffer <b>515</b>, a texture sampler and header unit <b>500</b>, a data packet logic unit <b>525</b>, a texture read unit <b>530</b>, and a texture filter unit <b>535</b>. The operation translation unit <b>505</b> is responsible for formatting texture operation instructions that are received from SPM <b>310</b>. When the operation translation unit <b>505</b> receives an instruction that has the .I specifier set to TRUE, the operation translation unit <b>505</b> issues a read request with the indices to the read request arbiter <b>510</b> to read the texture header and texture sampler values that are stored in the texture transpose buffer <b>515</b>. The texture header and texture sampler values may be written to the texture transpose buffer <b>515</b> by the SPM <b>310</b> at any point during execution of a shader program before the texture sampler and texture header values are needed by a texture operation instruction. In addition to storing texture header and texture sampler values, the texture transpose buffer <b>515</b> also stores texture coordinates. After all of the texture coordinates for a particular texture operation instruction are stored in the texture transpose buffer <b>515</b>, the SPM <b>310</b> issues the actual texture operation instruction to the operation translation unit <b>505</b>.</p>
<p id="p-0061" num="0060">The read request arbiter <b>510</b> arbitrates between read requests for texture headers and samplers that are received from the operation translation unit <b>505</b> and read requests for texture coordinates received from the data packet logic unit <b>525</b>. The read requests for indexed texture headers and texture samplers use a common address for all of the banks within the texture transpose buffer <b>515</b> since no transposing is required to read the indexed texture headers and texture samplers. The read requests generated by the data packet logic unit <b>525</b> for texture coordinates may use a different read address for each bank within the texture transpose buffer <b>515</b>. Therefore, the data packet logic unit <b>525</b> also generates a control for configuring read crossbar multiplexors in the texture transpose buffer <b>515</b>. A single quad of texture coordinates is output by the texture transpose buffer <b>515</b> per clock cycle since the texture operation instructions is executed to a quad of pixels each cycle. A quad of pixels is a 2&#xd7;2, 1&#xd7;4, or 4&#xd7;1 set of pixels.</p>
<p id="p-0062" num="0061">For non-indexed texture operation instructions, since the texture header and texture sampler is specified via immediate in the instruction, all threads within the same thread group will share the same texture header and texture sampler value. However, when the texture operation instruction specifies a texture sampler index and texture header index, each thread (processing a pixel) can have a different texture sampler value and a different texture index value stored in the texture transpose buffer <b>515</b>. When two or more active threads that process a pixel quad have different texture sampler values or different texture header values, the threads diverge and the texture sampler and header unit <b>500</b> serializes output of the pixel-specific texture sampler and header values for the pixel quad. The texture sampler and header unit <b>500</b> receives the indexed texture samplers and texture headers for a thread group read from the texture transpose buffer <b>515</b> and receives the immediate texture sampler and texture header for a thread group from the operation translation unit <b>505</b>. The texture sampler and header unit <b>500</b> outputs a per-quad (or a per-pixel when the threads for a pixel quad diverge) texture sampler and texture header to the data packet logic unit <b>525</b> each clock cycle. The details of the texture sampler and header unit <b>500</b> are described in conjunction with <figref idref="DRAWINGS">FIG. 5B</figref>.</p>
<p id="p-0063" num="0062">The data packet logic <b>525</b> receives the texture coordinates read from the texture transpose buffer <b>515</b> and the texture sampler and texture header from the operation translation unit <b>505</b>. The data packet logic <b>525</b> constructs texture addresses that is used to sample (read) texture data from the texture map specified by the texture identifier included in the texture header. The texture sampler specifies a sampling pattern that is used to sample the texture data from the texture map and filter the texture data to produce filtered texels. The texture read unit <b>530</b> receives the texture addresses and reads the texture data. The texture data is received by the texture filter unit <b>535</b> and filtered according to the texture sampler to produce filtered texels.</p>
<p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. 5B</figref> is a block diagram of the texture sampler and header unit <b>500</b> of <figref idref="DRAWINGS">FIG. 5A</figref>, according to one embodiment of the present invention. When the texture sampler and header unit <b>500</b> receives non-indexed texture sampler and header values from the operation translation unit <b>505</b>, the texture header value and the texture sampler value are stored in the sampler/header storage <b>555</b> and output for each quad in the thread group that has at least one active thread. Since there can be variable number of transactions (active quads or divergent pixels) for each thread group of texture operation instruction, a &#x201c;last&#x201d; bit in is used to indicate that the current transaction is the last transaction of the whole thread group. The last bit may be output from the texture sampler and header unit <b>500</b> to the operation translation unit <b>505</b> with the texture sampler and texture header values for the last quad in the thread group if there is no divergence within the quad, or sent with the last pixel within the quad if there is divergence between the pixels in the quad.</p>
<p id="p-0065" num="0064">A thread group register <b>540</b> and a quad multiplexer <b>545</b> receive the indexed texture samplers and texture headers for a thread group that are output by the texture transpose buffer <b>515</b>. The thread group register <b>540</b> stores all but one of the per-quad texture samplers and texture headers for the thread group since the one per-quad texture samplers and texture headers may be selected by the quad multiplexor <b>545</b> for output during the first clock cycle. Each clock cycle that the divergence check unit <b>550</b> will accept new texture samplers and texture headers, the quad multiplexor <b>545</b> outputs four texture samplers and texture headers (one texture sampler and one texture header for each pixel) when at least one thread processing a pixel in the quad is active.</p>
<p id="p-0066" num="0065">Since each transaction output to the operation translation unit <b>505</b> can only contain one texture header value and one texture sample value, in the case of indexed texture headers and samplers, the divergence check unit <b>550</b> needs to check whether either of the one texture header value and one texture sampler value within a pixel quad is &#x201c;divergent,&#x201d; meaning that a texture header value or a texture sampler value for an active pixel differs from the texture header value or texture sampler value for another active pixel within the same pixel quad. The lowest number pixel within the pixel quad that is processed by an active thread is selected by the divergence check unit <b>550</b>. The texture header value and texture sampler value for the selected pixel is compared against the texture header values and texture sampler values for the other pixels in the pixel quad that are processed by active threads. If all pixels in a pixel quad &#x201c;point to&#x201d; the same texture header and texture sampler, only one lookup is needed.</p>
<p id="p-0067" num="0066">One possible approach to handle a diverging quad is that any pixels within the pixel quad with matching texture header and texture sampler values can be issued together with the lowest number active pixel in the same transaction. The texture header and texture sampler values for any diverging pixels are issued in subsequent transactions. Alternatively, when any pixel diverges each pixel within the pixel quad that is processed by an active thread is output in sequence. As a result, a single pixel quad can generate up to four transactions, each with only one pixel active. When the texture operation instruction is non-indexed, all pixels within the each pixel quad and within the thread group are guaranteed to have the same texture header value and texture sampler value, and hence no divergence will occur. The transactions that include a single texture header and single texture sampler are buffered in the sampler header storage <b>550</b> for output to the operation translation unit <b>505</b>.</p>
<p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. 6</figref> is a flow diagram of method steps for determining the texture sampler and header specified for a texture operation instruction, according to one embodiment of the present invention. At step <b>600</b> a texture request is received from the SPM <b>310</b>. A texture request is produced by the SPM <b>310</b> after the texture coordinates for a texture operation instruction have been stored in the texture transpose buffer <b>515</b>. At step <b>605</b>, the operation translation unit <b>505</b> determines if the texture operation instruction specifies an index associated with the texture sampler, i.e., if the .I specifier is TRUE. If the texture operation instruction does not specify an index, then at step <b>610</b> the operation translation unit <b>505</b> outputs the immediate texture sampler and texture header to the texture sampler and texture header unit <b>500</b> and receives (from the texture sampler and header unit <b>500</b>) a copy of the texture sampler and texture header for each quad in the thread group that has at least one active thread.</p>
<p id="p-0069" num="0068">If, at step <b>605</b>, the operation translation unit <b>505</b> determines that the texture operation instruction does specifies an index, then at step <b>615</b> the operation translation unit <b>505</b> outputs a read request with the texture sampler index and the texture header index and the texture transpose buffer <b>515</b> reads the texture samplers and texture headers for the thread group. At step <b>616</b> the texture sampler and header unit <b>500</b> determines if at least one of the threads in the thread group that process a first pixel quad are active, and, if not, then at step <b>618</b> the texture samplers and texture headers for the first pixel quad are discarded and step <b>616</b> is repeated for the next quad. Note that at least one thread in the thread group is active in order for the texture unit <b>315</b> to receive the texture operation instruction for execution to process the thread group.</p>
<p id="p-0070" num="0069">If at step <b>616</b> the texture sampler and header unit <b>500</b> determines that at least one of the threads in the thread group that process the quad is active, then at step <b>620</b> the texture sampler and header unit <b>500</b> determines if the texture samplers and texture headers read for the pixel quad are divergent. If, the texture samplers and texture headers for the pixel quad are not divergent, then at step <b>635</b> the texture header and texture sampler for the quad are output by the texture sampler and header unit <b>500</b> and the texture sampler and header unit <b>500</b> proceeds directly to step <b>640</b>. Otherwise, at step <b>625</b> the texture sampler and header unit <b>500</b> outputs the texture sampler and texture header for the first pixel and other pixels in the pixel quad that are non-divergent (matching pixels). At step <b>630</b> the texture sampler and header unit <b>500</b> determines if the matching pixels include the last active pixel for the pixel quad, and, if not, then steps <b>625</b> and <b>630</b> are repeated for the next active pixel that did not match a previously matched pixel for the pixel quad. If, at step <b>630</b> the matching pixels did include the last active pixel, then the texture sampler and header unit <b>500</b> proceeds directly to step <b>640</b>.</p>
<p id="p-0071" num="0070">At step <b>640</b> the texture sampler and header unit <b>500</b> determines if the texture sampler and the texture header for the last active pixel quad was output in step <b>625</b> or step <b>635</b>, and, if not, then the texture sampler and header unit <b>500</b> returns to step <b>620</b> to process the next pixel quad. Otherwise, at step <b>650</b> the texture sampler and header unit <b>500</b> outputs the last output flag with the last texture sampler and texture header that was output. At step <b>655</b> the texture read unit <b>530</b> samples the texture data based on the texture sampler and texture header. At step <b>660</b> the texture filter unit <b>535</b> filters the sampled texture data to produce filtered texture data for a pixel.</p>
<p id="p-0072" num="0071">The texture unit <b>315</b> is configured to support texture operation instructions that specify an index for a texture sampler and an index for a texture header. The index for the texture sampler and the index for the texture header may be determined dynamically during execution of a shader program. Using an index allows for greater flexibility and control in performing texture mapping operations.</p>
<p id="p-0073" num="0072">One embodiment of the invention may be implemented as a program product for use with a computer system. The program(s) of the program product define functions of the embodiments (including the methods described herein) and can be contained on a variety of computer-readable storage media. Illustrative computer-readable storage media include, but are not limited to: (i) non-writable storage media (e.g., read-only memory devices within a computer such as CD-ROM disks readable by a CD-ROM drive, flash memory, ROM chips or any type of solid-state non-volatile semiconductor memory) on which information is permanently stored; and (ii) writable storage media (e.g., floppy disks within a diskette drive or hard-disk drive or any type of solid-state random-access semiconductor memory) on which alterable information is stored.</p>
<p id="p-0074" num="0073">The invention has been described above with reference to specific embodiments. Persons skilled in the art, however, will understand that various modifications and changes may be made thereto without departing from the broader spirit and scope of the invention as set forth in the appended claims. The foregoing description and drawings are, accordingly, to be regarded in an illustrative rather than a restrictive sense.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method for dynamically determining a texture sampler for a texture operation instruction, comprising:
<claim-text>receiving the texture operation instruction;</claim-text>
<claim-text>determining that the texture operation instruction specifies a first register included in a multi-threaded processor, the first register storing a first index associated with the texture sampler;</claim-text>
<claim-text>reading the texture sampler from a second register included in the multi-threaded processor using the first index;</claim-text>
<claim-text>sampling texture data according to a sampling pattern corresponding to the texture sampler; and</claim-text>
<claim-text>filtering the sampled texture data to produce filtered texture data for a pixel.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising the steps of:
<claim-text>determining that the texture operation instruction specifies a second index associated with a texture header;</claim-text>
<claim-text>reading the texture header from a third register using the second index; and</claim-text>
<claim-text>sampling the texture data from a texture map corresponding to the texture header.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the texture operation instruction is configured for multi-threaded execution to process a group of pixel quads including multiple pixels and the texture header includes a separate texture header for each one of the pixels in the pixel quads.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, further comprising the steps of:
<claim-text>determining that a first texture header for a first pixel in a pixel quad and a second texture header for a second pixel in the pixel quad are different;</claim-text>
<claim-text>sampling a first portion of the texture data from a first texture map corresponding to the first texture header; and</claim-text>
<claim-text>sampling a second portion of the texture data from a second texture map corresponding to the second texture header.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising, prior to the step of receiving the texture operation instruction, writing the texture header into the third register.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the texture operation instruction is configured for multi-threaded execution to process a group of pixel quads including multiple pixels and the texture sampler includes a separate texture sampler for each one of the pixels in the pixel quads.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising the steps of:
<claim-text>determining that a first texture sampler for a first pixel in a pixel quad and a second texture sampler for a second pixel in the pixel quad are different;</claim-text>
<claim-text>sampling a first portion of the texture data according to a sampling pattern corresponding to the first texture sampler; and</claim-text>
<claim-text>sampling a second portion of the texture data according to a sampling pattern corresponding to the second texture sampler.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising, prior to the step of receiving the texture operation instruction, writing the texture sampler into the second register.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. A computer-readable storage medium storing instructions that, when executed by a processor, cause the processor to dynamically determine a texture sampler for a texture operation instruction, by performing the steps of:
<claim-text>receiving the texture operation instruction;</claim-text>
<claim-text>determining that the texture operation instruction specifies a first register included in a multi-threaded processor, the first register storing a first index associated with the texture sampler;</claim-text>
<claim-text>reading the texture sampler from a second register included in the multi-threaded processor using the first index;</claim-text>
<claim-text>sampling texture data according to a sampling pattern corresponding to the texture sampler; and</claim-text>
<claim-text>filtering the sampled texture data to produce filtered texture data for a pixel.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. A system for dynamically determining a texture sampler for a texture operation instruction, the system comprising:
<claim-text>a multi-threaded processor including a texture unit configured to:
<claim-text>receive the texture operation instruction;</claim-text>
<claim-text>determine that the texture operation instruction specifies a first register included in the multi-threaded processor, the first register storing a first index associated with the texture sampler;</claim-text>
<claim-text>read the texture sampler from a second register included in the multi-threaded processor using the first index;</claim-text>
<claim-text>sample texture data according to a sampling pattern corresponding to the texture sampler; and</claim-text>
<claim-text>filter the sampled texture data to produce filtered texture data for a pixel.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, further comprising a memory storing instructions that, when executed by the multi-threaded processor, configures the multi-threaded processor to:
<claim-text>receive the texture operation instruction;</claim-text>
<claim-text>determine that the texture operation instruction specifies the first index;</claim-text>
<claim-text>read the texture sampler;</claim-text>
<claim-text>sample the texture data; and</claim-text>
<claim-text>filter the sampled texture data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the texture unit is further configured to:
<claim-text>determine that the texture operation instruction specifies a second index associated with a texture header;</claim-text>
<claim-text>read the texture header from a third register using the second index; and</claim-text>
<claim-text>sample the texture data from a texture map corresponding to the texture header.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the texture operation instruction is configured for multi-threaded execution to process a group of pixel quads including multiple pixels and the texture header includes a separate texture header for each one of the pixels in the pixel quads.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the texture unit is further configured to:
<claim-text>determine that a first texture header for a first pixel in a pixel quad and a second texture header for a second pixel in the pixel quad are different;</claim-text>
<claim-text>sample a first portion of the texture data from a first texture map corresponding to the first texture header; and</claim-text>
<claim-text>sample a second portion of the texture data from a second texture map corresponding to the second texture header.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the texture unit is further configured to write the texture header into the third register prior to receiving the texture operation instruction.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the texture operation instruction is configured for multi-threaded execution to process a group of pixel quads including multiple pixels and the texture sampler includes a separate texture sampler for each one of the pixels in the pixel quads.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the texture unit is further configured to:
<claim-text>determine that a first texture sampler for a first pixel in a pixel quad and a second texture sampler for a second pixel in the pixel quad are different;</claim-text>
<claim-text>sample a first portion of the texture data according to a sampling pattern corresponding to the first texture sampler; and</claim-text>
<claim-text>sample a second portion of the texture data according to a sampling pattern corresponding to the second texture sampler.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the texture unit is further configured to write the texture sampler into the second register prior to receiving the texture operation instruction. </claim-text>
</claim>
</claims>
</us-patent-grant>
