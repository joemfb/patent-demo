<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625810-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625810</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12278568</doc-number>
<date>20070207</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<us-term-of-grant>
<us-term-extension>940</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>R</subclass>
<main-group>5</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>381 21</main-classification>
<further-classification>381 22</further-classification>
<further-classification>381 23</further-classification>
</classification-national>
<invention-title id="d2e53">Apparatus and method for encoding/decoding signal</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5166685</doc-number>
<kind>A</kind>
<name>Campbell et al.</name>
<date>19921100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5524054</doc-number>
<kind>A</kind>
<name>Spille et al.</name>
<date>19960600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5561736</doc-number>
<kind>A</kind>
<name>Moore et al.</name>
<date>19961000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5579396</doc-number>
<kind>A</kind>
<name>Iida et al.</name>
<date>19961100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5632005</doc-number>
<kind>A</kind>
<name>Davis et al.</name>
<date>19970500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>5668924</doc-number>
<kind>A</kind>
<name>Takahashi</name>
<date>19970900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>5703584</doc-number>
<kind>A</kind>
<name>Hill et al.</name>
<date>19971200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>5862227</doc-number>
<kind>A</kind>
<name>Orduna-Bustamante et al.</name>
<date>19990100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>5890125</doc-number>
<kind>A</kind>
<name>Davis et al.</name>
<date>19990300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6072877</doc-number>
<kind>A</kind>
<name>Abel</name>
<date>20000600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6081783</doc-number>
<kind>A</kind>
<name>Divine et al.</name>
<date>20000600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6118875</doc-number>
<kind>A</kind>
<name>Moller</name>
<date>20000900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>6226616</doc-number>
<kind>B1</kind>
<name>You et al.</name>
<date>20010500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>6307941</doc-number>
<kind>B1</kind>
<name>Tanner et al.</name>
<date>20011000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>6466913</doc-number>
<kind>B1</kind>
<name>Yasuda et al.</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>6504496</doc-number>
<kind>B1</kind>
<name>Mesarovic et al.</name>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>6574339</doc-number>
<kind>B1</kind>
<name>Kim</name>
<date>20030600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>6611212</doc-number>
<kind>B1</kind>
<name>Craven et al.</name>
<date>20030800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>6633648</doc-number>
<kind>B1</kind>
<name>Bauck</name>
<date>20031000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>6711266</doc-number>
<kind>B1</kind>
<name>Aylward et al.</name>
<date>20040300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>6721425</doc-number>
<kind>B1</kind>
<name>Aylward</name>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>6795556</doc-number>
<kind>B1</kind>
<name>Sibbald et al.</name>
<date>20040900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>6973130</doc-number>
<kind>B1</kind>
<name>Wee et al.</name>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>7085393</doc-number>
<kind>B1</kind>
<name>Chen</name>
<date>20060800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>7177431</doc-number>
<kind>B2</kind>
<name>Davis et al.</name>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>7180964</doc-number>
<kind>B2</kind>
<name>Borowski et al.</name>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>375329</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>7260540</doc-number>
<kind>B2</kind>
<name>Miyasaka et al.</name>
<date>20070800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>7302068</doc-number>
<kind>B2</kind>
<name>Longbottom et al.</name>
<date>20071100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>7391877</doc-number>
<kind>B1</kind>
<name>Brungart</name>
<date>20080600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>7519530</doc-number>
<kind>B2</kind>
<name>Kaajas et al.</name>
<date>20090400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>US</country>
<doc-number>7519538</doc-number>
<kind>B2</kind>
<name>Villemoes et al.</name>
<date>20090400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>US</country>
<doc-number>7536021</doc-number>
<kind>B2</kind>
<name>Dickins et al.</name>
<date>20090500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00033">
<document-id>
<country>US</country>
<doc-number>7555434</doc-number>
<kind>B2</kind>
<name>Nomura et al.</name>
<date>20090600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00034">
<document-id>
<country>US</country>
<doc-number>7613306</doc-number>
<kind>B2</kind>
<name>Miyasaka et al.</name>
<date>20091100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00035">
<document-id>
<country>US</country>
<doc-number>7720230</doc-number>
<kind>B2</kind>
<name>Allamanche et al.</name>
<date>20100500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00036">
<document-id>
<country>US</country>
<doc-number>7761304</doc-number>
<kind>B2</kind>
<name>Faller</name>
<date>20100700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00037">
<document-id>
<country>US</country>
<doc-number>7773756</doc-number>
<kind>B2</kind>
<name>Beard</name>
<date>20100800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00038">
<document-id>
<country>US</country>
<doc-number>7787631</doc-number>
<kind>B2</kind>
<name>Faller</name>
<date>20100800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>381 20</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00039">
<document-id>
<country>US</country>
<doc-number>7797163</doc-number>
<kind>B2</kind>
<name>Pang et al.</name>
<date>20100900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00040">
<document-id>
<country>US</country>
<doc-number>7880748</doc-number>
<kind>B1</kind>
<name>Sevigny</name>
<date>20110200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00041">
<document-id>
<country>US</country>
<doc-number>7916873</doc-number>
<kind>B2</kind>
<name>Villemoes et al.</name>
<date>20110300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00042">
<document-id>
<country>US</country>
<doc-number>7961889</doc-number>
<kind>B2</kind>
<name>Kim et al.</name>
<date>20110600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00043">
<document-id>
<country>US</country>
<doc-number>7979282</doc-number>
<kind>B2</kind>
<name>Kim et al.</name>
<date>20110700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00044">
<document-id>
<country>US</country>
<doc-number>7987096</doc-number>
<kind>B2</kind>
<name>Kim et al.</name>
<date>20110700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00045">
<document-id>
<country>US</country>
<doc-number>8081762</doc-number>
<kind>B2</kind>
<name>Ojala et al.</name>
<date>20111200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00046">
<document-id>
<country>US</country>
<doc-number>8081764</doc-number>
<kind>B2</kind>
<name>Takagi et al.</name>
<date>20111200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>381 20</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00047">
<document-id>
<country>US</country>
<doc-number>8108220</doc-number>
<kind>B2</kind>
<name>Saunders et al.</name>
<date>20120100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00048">
<document-id>
<country>US</country>
<doc-number>8116459</doc-number>
<kind>B2</kind>
<name>Disch et al.</name>
<date>20120200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>381 22</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00049">
<document-id>
<country>US</country>
<doc-number>8150042</doc-number>
<kind>B2</kind>
<name>Van Loon et al.</name>
<date>20120400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00050">
<document-id>
<country>US</country>
<doc-number>8150066</doc-number>
<kind>B2</kind>
<name>Kubo</name>
<date>20120400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00051">
<document-id>
<country>US</country>
<doc-number>8185403</doc-number>
<kind>B2</kind>
<name>Pang et al.</name>
<date>20120500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00052">
<document-id>
<country>US</country>
<doc-number>8189682</doc-number>
<kind>B2</kind>
<name>Yamasaki</name>
<date>20120500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524025</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00053">
<document-id>
<country>US</country>
<doc-number>8255211</doc-number>
<kind>B2</kind>
<name>Vinton et al.</name>
<date>20120800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00054">
<document-id>
<country>US</country>
<doc-number>2001/0031062</doc-number>
<kind>A1</kind>
<name>Terai et al.</name>
<date>20011000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00055">
<document-id>
<country>US</country>
<doc-number>2003/0007648</doc-number>
<kind>A1</kind>
<name>Currell</name>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00056">
<document-id>
<country>US</country>
<doc-number>2003/0035553</doc-number>
<kind>A1</kind>
<name>Baumgarte et al.</name>
<date>20030200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00057">
<document-id>
<country>US</country>
<doc-number>2003/0182423</doc-number>
<kind>A1</kind>
<name>Shafir et al.</name>
<date>20030900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00058">
<document-id>
<country>US</country>
<doc-number>2003/0236583</doc-number>
<kind>A1</kind>
<name>Baumgarte et al.</name>
<date>20031200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00059">
<document-id>
<country>US</country>
<doc-number>2004/0032960</doc-number>
<kind>A1</kind>
<name>Griesinger</name>
<date>20040200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00060">
<document-id>
<country>US</country>
<doc-number>2004/0049379</doc-number>
<kind>A1</kind>
<name>Thumpudi et al.</name>
<date>20040300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00061">
<document-id>
<country>US</country>
<doc-number>2004/0071445</doc-number>
<kind>A1</kind>
<name>Tarnoff et al.</name>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00062">
<document-id>
<country>US</country>
<doc-number>2004/0111171</doc-number>
<kind>A1</kind>
<name>Jang et al.</name>
<date>20040600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00063">
<document-id>
<country>US</country>
<doc-number>2004/0118195</doc-number>
<kind>A1</kind>
<name>Nespo et al.</name>
<date>20040600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00064">
<document-id>
<country>US</country>
<doc-number>2004/0138874</doc-number>
<kind>A1</kind>
<name>Kaajas et al.</name>
<date>20040700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00065">
<document-id>
<country>US</country>
<doc-number>2004/0196770</doc-number>
<kind>A1</kind>
<name>Touyama et al.</name>
<date>20041000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00066">
<document-id>
<country>US</country>
<doc-number>2004/0196982</doc-number>
<kind>A1</kind>
<name>Aylward et al.</name>
<date>20041000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00067">
<document-id>
<country>US</country>
<doc-number>2005/0061808</doc-number>
<kind>A1</kind>
<name>Cole et al.</name>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00068">
<document-id>
<country>US</country>
<doc-number>2005/0063613</doc-number>
<kind>A1</kind>
<name>Casey et al.</name>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00069">
<document-id>
<country>US</country>
<doc-number>2005/0074127</doc-number>
<kind>A1</kind>
<name>Herre et al.</name>
<date>20050400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00070">
<document-id>
<country>US</country>
<doc-number>2005/0089181</doc-number>
<kind>A1</kind>
<name>Polk, Jr.</name>
<date>20050400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00071">
<document-id>
<country>US</country>
<doc-number>2005/0117762</doc-number>
<kind>A1</kind>
<name>Sakurai et al.</name>
<date>20050600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00072">
<document-id>
<country>US</country>
<doc-number>2005/0135643</doc-number>
<kind>A1</kind>
<name>Lee et al.</name>
<date>20050600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00073">
<document-id>
<country>US</country>
<doc-number>2005/0157883</doc-number>
<kind>A1</kind>
<name>Herre et al.</name>
<date>20050700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00074">
<document-id>
<country>US</country>
<doc-number>2005/0179701</doc-number>
<kind>A1</kind>
<name>Jahnke</name>
<date>20050800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00075">
<document-id>
<country>US</country>
<doc-number>2005/0180579</doc-number>
<kind>A1</kind>
<name>Baumgarte</name>
<date>20050800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00076">
<document-id>
<country>US</country>
<doc-number>2005/0195981</doc-number>
<kind>A1</kind>
<name>Faller et al.</name>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00077">
<document-id>
<country>US</country>
<doc-number>2005/0271367</doc-number>
<kind>A1</kind>
<name>Lee et al.</name>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00078">
<document-id>
<country>US</country>
<doc-number>2005/0273322</doc-number>
<kind>A1</kind>
<name>Lee et al.</name>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00079">
<document-id>
<country>US</country>
<doc-number>2005/0273324</doc-number>
<kind>A1</kind>
<name>Yi</name>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00080">
<document-id>
<country>US</country>
<doc-number>2005/0276430</doc-number>
<kind>A1</kind>
<name>He et al.</name>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00081">
<document-id>
<country>US</country>
<doc-number>2006/0002572</doc-number>
<kind>A1</kind>
<name>Smithers et al.</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00082">
<document-id>
<country>US</country>
<doc-number>2006/0004583</doc-number>
<kind>A1</kind>
<name>Herre et al.</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00083">
<document-id>
<country>US</country>
<doc-number>2006/0008091</doc-number>
<kind>A1</kind>
<name>Kim et al.</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00084">
<document-id>
<country>US</country>
<doc-number>2006/0008094</doc-number>
<kind>A1</kind>
<name>Huang et al.</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00085">
<document-id>
<country>US</country>
<doc-number>2006/0009225</doc-number>
<kind>A1</kind>
<name>Herre et al.</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00086">
<document-id>
<country>US</country>
<doc-number>2006/0050909</doc-number>
<kind>A1</kind>
<name>Kim et al.</name>
<date>20060300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00087">
<document-id>
<country>US</country>
<doc-number>2006/0072764</doc-number>
<kind>A1</kind>
<name>Mertens et al.</name>
<date>20060400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00088">
<document-id>
<country>US</country>
<doc-number>2006/0083394</doc-number>
<kind>A1</kind>
<name>McGrath</name>
<date>20060400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00089">
<document-id>
<country>US</country>
<doc-number>2006/0115100</doc-number>
<kind>A1</kind>
<name>Faller et al.</name>
<date>20060600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00090">
<document-id>
<country>US</country>
<doc-number>2006/0126851</doc-number>
<kind>A1</kind>
<name>Yuen et al.</name>
<date>20060600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00091">
<document-id>
<country>US</country>
<doc-number>2006/0133618</doc-number>
<kind>A1</kind>
<name>Villemoes et al.</name>
<date>20060600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00092">
<document-id>
<country>US</country>
<doc-number>2006/0153408</doc-number>
<kind>A1</kind>
<name>Faller et al.</name>
<date>20060700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00093">
<document-id>
<country>US</country>
<doc-number>2006/0190247</doc-number>
<kind>A1</kind>
<name>Lindblom</name>
<date>20060800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00094">
<document-id>
<country>US</country>
<doc-number>2006/0198527</doc-number>
<kind>A1</kind>
<name>Chun</name>
<date>20060900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00095">
<document-id>
<country>US</country>
<doc-number>2006/0233379</doc-number>
<kind>A1</kind>
<name>Villemoes et al.</name>
<date>20061000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>381 23</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00096">
<document-id>
<country>US</country>
<doc-number>2006/0233380</doc-number>
<kind>A1</kind>
<name>Holzer et al.</name>
<date>20061000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00097">
<document-id>
<country>US</country>
<doc-number>2006/0239473</doc-number>
<kind>A1</kind>
<name>Kjorling et al.</name>
<date>20061000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00098">
<document-id>
<country>US</country>
<doc-number>2006/0251276</doc-number>
<kind>A1</kind>
<name>Chen</name>
<date>20061100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00099">
<document-id>
<country>US</country>
<doc-number>2007/0133831</doc-number>
<kind>A1</kind>
<name>Kim et al.</name>
<date>20070600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00100">
<document-id>
<country>US</country>
<doc-number>2007/0160218</doc-number>
<kind>A1</kind>
<name>Jakka et al.</name>
<date>20070700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00101">
<document-id>
<country>US</country>
<doc-number>2007/0160219</doc-number>
<kind>A1</kind>
<name>Jakka et al.</name>
<date>20070700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00102">
<document-id>
<country>US</country>
<doc-number>2007/0162278</doc-number>
<kind>A1</kind>
<name>Miyasaka et al.</name>
<date>20070700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00103">
<document-id>
<country>US</country>
<doc-number>2007/0165886</doc-number>
<kind>A1</kind>
<name>Topliss et al.</name>
<date>20070700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00104">
<document-id>
<country>US</country>
<doc-number>2007/0172071</doc-number>
<kind>A1</kind>
<name>Mehrotra et al.</name>
<date>20070700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00105">
<document-id>
<country>US</country>
<doc-number>2007/0183603</doc-number>
<kind>A1</kind>
<name>Jin et al.</name>
<date>20070800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00106">
<document-id>
<country>US</country>
<doc-number>2007/0203697</doc-number>
<kind>A1</kind>
<name>Pang et al.</name>
<date>20070800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00107">
<document-id>
<country>US</country>
<doc-number>2007/0219808</doc-number>
<kind>A1</kind>
<name>Herre et al.</name>
<date>20070900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00108">
<document-id>
<country>US</country>
<doc-number>2007/0223708</doc-number>
<kind>A1</kind>
<name>Villemoes et al.</name>
<date>20070900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00109">
<document-id>
<country>US</country>
<doc-number>2007/0223709</doc-number>
<kind>A1</kind>
<name>Kim et al.</name>
<date>20070900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00110">
<document-id>
<country>US</country>
<doc-number>2007/0233296</doc-number>
<kind>A1</kind>
<name>Kim et al.</name>
<date>20071000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00111">
<document-id>
<country>US</country>
<doc-number>2007/0258607</doc-number>
<kind>A1</kind>
<name>Purnhagen et al.</name>
<date>20071100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00112">
<document-id>
<country>US</country>
<doc-number>2007/0280485</doc-number>
<kind>A1</kind>
<name>Villemoes</name>
<date>20071200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00113">
<document-id>
<country>US</country>
<doc-number>2007/0291950</doc-number>
<kind>A1</kind>
<name>Kimura et al.</name>
<date>20071200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00114">
<document-id>
<country>US</country>
<doc-number>2008/0002842</doc-number>
<kind>A1</kind>
<name>Neusinger et al.</name>
<date>20080100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00115">
<document-id>
<country>US</country>
<doc-number>2008/0008327</doc-number>
<kind>A1</kind>
<name>Ojala et al.</name>
<date>20080100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00116">
<document-id>
<country>US</country>
<doc-number>2008/0033732</doc-number>
<kind>A1</kind>
<name>Seefeldt et al.</name>
<date>20080200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00117">
<document-id>
<country>US</country>
<doc-number>2008/0052089</doc-number>
<kind>A1</kind>
<name>Takagi</name>
<date>20080200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00118">
<document-id>
<country>US</country>
<doc-number>2008/0097750</doc-number>
<kind>A1</kind>
<name>Seefeldt et al.</name>
<date>20080400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00119">
<document-id>
<country>US</country>
<doc-number>2008/0130904</doc-number>
<kind>A1</kind>
<name>Faller</name>
<date>20080600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00120">
<document-id>
<country>US</country>
<doc-number>2008/0192941</doc-number>
<kind>A1</kind>
<name>Oh et al.</name>
<date>20080800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00121">
<document-id>
<country>US</country>
<doc-number>2008/0195397</doc-number>
<kind>A1</kind>
<name>Myburg et al.</name>
<date>20080800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00122">
<document-id>
<country>US</country>
<doc-number>2008/0199026</doc-number>
<kind>A1</kind>
<name>Oh et al.</name>
<date>20080800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00123">
<document-id>
<country>US</country>
<doc-number>2008/0304670</doc-number>
<kind>A1</kind>
<name>Breebaart</name>
<date>20081200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>381 17</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00124">
<document-id>
<country>US</country>
<doc-number>2009/0041265</doc-number>
<kind>A1</kind>
<name>Kubo</name>
<date>20090200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00125">
<document-id>
<country>US</country>
<doc-number>2009/0110203</doc-number>
<kind>A1</kind>
<name>Taleb</name>
<date>20090400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00126">
<document-id>
<country>US</country>
<doc-number>2009/0129601</doc-number>
<kind>A1</kind>
<name>Ojala et al.</name>
<date>20090500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00127">
<document-id>
<country>CN</country>
<doc-number>1223064</doc-number>
<date>19990700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00128">
<document-id>
<country>CN</country>
<doc-number>1253464</doc-number>
<date>20000500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00129">
<document-id>
<country>CN</country>
<doc-number>1411679</doc-number>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00130">
<document-id>
<country>CN</country>
<doc-number>1495705</doc-number>
<date>20040500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00131">
<document-id>
<country>CN</country>
<doc-number>1655651</doc-number>
<date>20050800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00132">
<document-id>
<country>EP</country>
<doc-number>0 637 191</doc-number>
<date>19950200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00133">
<document-id>
<country>EP</country>
<doc-number>0857375</doc-number>
<date>19980800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00134">
<document-id>
<country>EP</country>
<doc-number>1211857</doc-number>
<date>20020600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00135">
<document-id>
<country>EP</country>
<doc-number>1 315 148</doc-number>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00136">
<document-id>
<country>EP</country>
<doc-number>1376538</doc-number>
<kind>A1</kind>
<date>20040100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00137">
<document-id>
<country>EP</country>
<doc-number>1455345</doc-number>
<date>20040900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00138">
<document-id>
<country>EP</country>
<doc-number>1 545 154</doc-number>
<date>20050600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00139">
<document-id>
<country>EP</country>
<doc-number>1 617 413</doc-number>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00140">
<document-id>
<country>JP</country>
<doc-number>7248255</doc-number>
<date>19950900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00141">
<document-id>
<country>JP</country>
<doc-number>08-079900</doc-number>
<date>19960300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00142">
<document-id>
<country>JP</country>
<doc-number>8-084400</doc-number>
<date>19960300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00143">
<document-id>
<country>JP</country>
<doc-number>9-074446</doc-number>
<date>19970300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00144">
<document-id>
<country>JP</country>
<doc-number>09-224300</doc-number>
<date>19970800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00145">
<document-id>
<country>JP</country>
<doc-number>9-261351</doc-number>
<date>19971000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00146">
<document-id>
<country>JP</country>
<doc-number>09-275544</doc-number>
<date>19971000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00147">
<document-id>
<country>JP</country>
<doc-number>10-304498</doc-number>
<date>19981100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00148">
<document-id>
<country>JP</country>
<doc-number>11-032400</doc-number>
<date>19990200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00149">
<document-id>
<country>JP</country>
<doc-number>11503882</doc-number>
<date>19990300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00150">
<document-id>
<country>JP</country>
<doc-number>2001028800</doc-number>
<date>20010100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00151">
<document-id>
<country>JP</country>
<doc-number>2001-188578</doc-number>
<date>20010700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00152">
<document-id>
<country>JP</country>
<doc-number>2001-516537</doc-number>
<date>20010900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00153">
<document-id>
<country>JP</country>
<doc-number>2001-359197</doc-number>
<date>20011200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00154">
<document-id>
<country>JP</country>
<doc-number>2002-049399</doc-number>
<date>20020200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00155">
<document-id>
<country>JP</country>
<doc-number>2003-009296</doc-number>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00156">
<document-id>
<country>JP</country>
<doc-number>2003-111198</doc-number>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00157">
<document-id>
<country>JP</country>
<doc-number>2004-078183</doc-number>
<date>20040300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00158">
<document-id>
<country>JP</country>
<doc-number>2004-535145</doc-number>
<date>20041100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00159">
<document-id>
<country>JP</country>
<doc-number>2005-063097</doc-number>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00160">
<document-id>
<country>JP</country>
<doc-number>2005-229612</doc-number>
<date>20050800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00161">
<document-id>
<country>JP</country>
<doc-number>2005-523624</doc-number>
<date>20050800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00162">
<document-id>
<country>JP</country>
<doc-number>2005-352396</doc-number>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00163">
<document-id>
<country>JP</country>
<doc-number>2006-014219</doc-number>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00164">
<document-id>
<country>JP</country>
<doc-number>2007-511140</doc-number>
<date>20070400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00165">
<document-id>
<country>JP</country>
<doc-number>2007-288900</doc-number>
<date>20071100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00166">
<document-id>
<country>JP</country>
<doc-number>2008-504578</doc-number>
<date>20080200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00167">
<document-id>
<country>JP</country>
<doc-number>08-065169</doc-number>
<date>20080300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00168">
<document-id>
<country>JP</country>
<doc-number>2008-511044</doc-number>
<date>20080400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00169">
<document-id>
<country>JP</country>
<doc-number>08-202397</doc-number>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00170">
<document-id>
<country>KR</country>
<doc-number>10-2001-0001993</doc-number>
<date>20010100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00171">
<document-id>
<country>KR</country>
<doc-number>10-2001-0009258</doc-number>
<date>20010200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00172">
<document-id>
<country>KR</country>
<doc-number>2004106321</doc-number>
<kind>A</kind>
<date>20041200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00173">
<document-id>
<country>KR</country>
<doc-number>2005061808</doc-number>
<kind>A</kind>
<date>20050600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00174">
<document-id>
<country>KR</country>
<doc-number>2005063613</doc-number>
<kind>A</kind>
<date>20050600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00175">
<document-id>
<country>RU</country>
<doc-number>2119259</doc-number>
<date>19980900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00176">
<document-id>
<country>RU</country>
<doc-number>2129336</doc-number>
<date>19990400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00177">
<document-id>
<country>RU</country>
<doc-number>2221329</doc-number>
<kind>C2</kind>
<date>20040100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00178">
<document-id>
<country>RU</country>
<doc-number>2004133032</doc-number>
<date>20050400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00179">
<document-id>
<country>RU</country>
<doc-number>2005103637</doc-number>
<kind>A</kind>
<date>20050700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00180">
<document-id>
<country>RU</country>
<doc-number>2005104123</doc-number>
<date>20050700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00181">
<document-id>
<country>TW</country>
<doc-number>263646</doc-number>
<date>19951100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00182">
<document-id>
<country>TW</country>
<doc-number>289885</doc-number>
<date>19961100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00183">
<document-id>
<country>TW</country>
<doc-number>503626</doc-number>
<date>20010900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00184">
<document-id>
<country>TW</country>
<doc-number>468182</doc-number>
<date>20011200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00185">
<document-id>
<country>TW</country>
<doc-number>550541</doc-number>
<date>20030900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00186">
<document-id>
<country>TW</country>
<doc-number>200304120</doc-number>
<date>20030900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00187">
<document-id>
<country>TW</country>
<doc-number>200405673</doc-number>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00188">
<document-id>
<country>TW</country>
<doc-number>594675</doc-number>
<date>20040600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00189">
<document-id>
<country>TW</country>
<doc-number>I230024</doc-number>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00190">
<document-id>
<country>TW</country>
<doc-number>200921644</doc-number>
<date>20050500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00191">
<document-id>
<country>TW</country>
<doc-number>2005334234</doc-number>
<date>20051000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00192">
<document-id>
<country>TW</country>
<doc-number>200537436</doc-number>
<kind>A</kind>
<date>20051100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00193">
<document-id>
<country>WO</country>
<doc-number>97/15983</doc-number>
<date>19970500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00194">
<document-id>
<country>WO</country>
<doc-number>WO 98/42162</doc-number>
<date>19980900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00195">
<document-id>
<country>WO</country>
<doc-number>99/49574</doc-number>
<date>19990900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00196">
<document-id>
<country>WO</country>
<doc-number>9949574</doc-number>
<date>19990900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00197">
<document-id>
<country>WO</country>
<doc-number>WO 03/007656</doc-number>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00198">
<document-id>
<country>WO</country>
<doc-number>WO 03-007656</doc-number>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00199">
<document-id>
<country>WO</country>
<doc-number>03/085643</doc-number>
<date>20031000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00200">
<document-id>
<country>WO</country>
<doc-number>03-090208</doc-number>
<date>20031000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00201">
<document-id>
<country>WO</country>
<doc-number>2004-008805</doc-number>
<date>20040100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00202">
<document-id>
<country>WO</country>
<doc-number>2004/008806</doc-number>
<date>20040100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00203">
<document-id>
<country>WO</country>
<doc-number>2004-019656</doc-number>
<date>20040300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00204">
<document-id>
<country>WO</country>
<doc-number>2004/028204</doc-number>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00205">
<document-id>
<country>WO</country>
<doc-number>2004-036549</doc-number>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00206">
<document-id>
<country>WO</country>
<doc-number>2004-036954</doc-number>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00207">
<document-id>
<country>WO</country>
<doc-number>2004-036955</doc-number>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00208">
<document-id>
<country>WO</country>
<doc-number>2004036548</doc-number>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00209">
<document-id>
<country>WO</country>
<doc-number>2005/036925</doc-number>
<date>20050400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00210">
<document-id>
<country>WO</country>
<doc-number>2005/043511</doc-number>
<date>20050500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00211">
<document-id>
<country>WO</country>
<doc-number>2005/069637</doc-number>
<date>20050700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00212">
<document-id>
<country>WO</country>
<doc-number>2005/069638</doc-number>
<date>20050700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00213">
<document-id>
<country>WO</country>
<doc-number>2005/081229</doc-number>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00214">
<document-id>
<country>WO</country>
<doc-number>2005/098826</doc-number>
<date>20051000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00215">
<document-id>
<country>WO</country>
<doc-number>2005/101371</doc-number>
<date>20051000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00216">
<document-id>
<country>WO</country>
<doc-number>WO2005101370</doc-number>
<kind>A1</kind>
<date>20051000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00217">
<document-id>
<country>WO</country>
<doc-number>2006/002748</doc-number>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00218">
<document-id>
<country>WO</country>
<doc-number>WO 2006-003813</doc-number>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00219">
<document-id>
<country>WO</country>
<doc-number>WO2007/010785</doc-number>
<date>20070100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<patcit num="00220">
<document-id>
<country>WO</country>
<doc-number>2007/080212</doc-number>
<date>20070700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00221">
<othercit>Korean Office Action for KR Application No. 10-2008-7016477, dated Mar. 26, 2010, 12 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00222">
<othercit>Korean Office Action for KR Application No. 10-2008-7016479, dated Mar. 26, 2010, 11 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00223">
<othercit>Taiwanese Office Action for TW Application No. 96104543, dated Mar. 30, 2010, 12, pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00224">
<othercit>Breebaart et al., &#x201c;MPEG Surround Binaural Coding Proposal Philips/CT/ThG/VAST Audio,&#x201d; ITU Study Group 16&#x2014;Video Coding Experts Group&#x2014;ISO/IEC MPEG &#x26; ITU-T VCEG (ISO/IEC JTC1/SC29/WG11 and ITU-T SG16 Q6), XX, XX, No. M13253, Mar. 29, 2006, 49 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00225">
<othercit>Office Action, U.S. Appl. No. 11/915,327, dated Apr. 8, 2011, 14 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00226">
<othercit>Search Report, European Appln. No. 07701033.8, dated Apr. 1, 2011, 7 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00227">
<othercit>Kj&#xf6;rling et al., &#x201c;MPEG Surround Amendment Work Item on Complexity Reductions of Binaural Filtering,&#x201d; ITU Study Group 16 Video Coding Experts Group&#x2014;ISO/IEC MPEG &#x26; ITU-T VCEG (ISO/IEC JTC1/SC29/WG11 and ITU-T SG16 Q6), XX, XX, No. M13672, Jul. 12, 2006, 5 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00228">
<othercit>Kok Seng et al., &#x201c;Core Experiment on Adding 3D Stereo Support to MPEG Surround,&#x201d; ITU Study Group 16 Video Coding Experts Group&#x2014;ISO/IEC MPEG &#x26; ITU-T VCEG (ISO/IEC JTC1/SC29/WG11 and ITU-T SG16 Q6), XX, XX, No. M12845, Jan. 11, 2006, 11 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00229">
<othercit>&#x201c;Text of ISO/IEC 14496-3:200X/PDAM 4, MPEG Surround,&#x201d; ITU Study Group 16 Video Coding Experts Group&#x2014;ISO/IEC MPEG &#x26; ITU-T VCEG (ISO/IEC JTC1/SC29/WG11 and ITU-T SG16 Q6), XX, XX, No. N7530, Oct. 21, 2005, 169 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00230">
<othercit>Chinese Patent Gazette, Chinese Appln. No. 200780001540.X, mailed Jun. 15, 2011, 2 pages with English abstract.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00231">
<othercit>Engdeg&#xe4;rd et al. &#x201c;Synthetic Ambience in Parametric Stereo Coding,&#x201d; Audio Engineering Society (AES) 116th Convention, Berlin, Germany, May 8-11, 2004, pp. 1-12.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00232">
<othercit>Search Report, European Appln. No. 07708534.8, dated Jul. 4, 2011, 7 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00233">
<othercit>Office Action, Canadian Application No. 2,636,494, mailed Aug. 4, 2010, 3 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00234">
<othercit>Office Action, Japanese Appln. No. 2008-513374, mailed Aug. 24, 2010, 8 pages with English translation.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00235">
<othercit>Faller, &#x201c;Coding of Spatial Audio Compatible with Different Playback Formats,&#x201d; Proceedings of the Audio Engineering Society Convention Paper, USA, Audio Engineering Society, Oct. 28, 2004, 117th Convention, pp. 1-12.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00236">
<othercit>Schuijers et al., &#x201c;Advances in Parametric Coding for High-Quality Audio,&#x201d; Proceedings of the Audio Engineering Society Convention Paper 5852, Audio Engineering Society, Mar. 22, 2003, 114th Convention, pp. 1-11.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00237">
<othercit>Office Action, Japanese Appln. No. 2008-551196, dated Dec. 21, 2010, 4 pages with English translation.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00238">
<othercit>Office Action, U.S. Appl. No. 12/161,560, dated Feb. 17, 2012, 13 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00239">
<othercit>Savioja, &#x201c;Modeling Techniques for Virtual Acoustics,&#x201d; Thesis, Aug. 24, 2000, 88 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00240">
<othercit>Office Action, U.S. Appl. No. 11/915,327, dated Dec. 10, 2010, 20 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00241">
<othercit>Japanese Office Action dated Nov. 9, 2010 from Japanese Application No. 2008-551199 with English translation, 11 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00242">
<othercit>Japanese Office Action dated Nov. 9, 2010 from Japanese Application No. 2008-551194 with English translation, 11 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00243">
<othercit>Japanese Office Action dated Nov. 9, 2010 from Japanese Application No. 2008-551193 with English translation, 11 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00244">
<othercit>Japanese Office Action dated Nov. 9, 2010 from Japanese Application No. 2008-551200 with English translation, 11 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00245">
<othercit>Korean Office Action dated Nov. 25, 2010 from Korean Application No. 10-2008-7016481 with English translation, 8 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00246">
<othercit>MPEG-2 Standard. ISO/IEC Document 13818-3:1994(E), Generic Coding of Moving Pictures and Associated Audio information, Part 3: Audio, Nov. 11, 1994, 4 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00247">
<othercit>Chang, &#x201c;Document Register for 75th meeting in Bangkok, Thailand&#x201d;, ISO/IEC JTC/SC29/WG11, MPEG2005/M12715, Bangkok, Thailand, Jan. 2006, 3 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00248">
<othercit>Donnelly et al., &#x201c;The Fast Fourier Transform for Experimentalists, Part II: Convolutions,&#x201d; Computing in Science &#x26; Engineering, IEEE, Aug. 1, 2005, vol. 7, No. 4, pp. 92-95.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00249">
<othercit>Office Action, U.S. Appl. No. 12/161,560, dated Oct. 27, 2011, 14 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00250">
<othercit>Office Action, U.S. Appl. No. 12/278,775, dated Dec. 9, 2011, 16 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00251">
<othercit>Office Action, European Appln. No. 07 701 033.8, 16 dated Dec. 2011, 4 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00252">
<othercit>Office Action, U.S. Appl. No. 12/278,569, dated Dec. 2, 2011, 10 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00253">
<othercit>Notice of Allowance, U.S. Appl. No. 12/278,572, dated Dec. 20, 2011, 12 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00254">
<othercit>Notice of Allowance, U.S. Appl. No. 12/161,334, dated Dec. 20, 2011, 11 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00255">
<othercit>Herre et al., &#x201c;MP3 Surround: Efficient and Compatible Coding of Multi-Channel Audio,&#x201d; Convention Paper of the Audio Engineering Society 116th Convention, Berlin, Germany, May 8, 2004, 6049, pp. 1-14.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00256">
<othercit>Office Action, Japanese Appln. No. 2008-554134, dated Nov. 15, 2011, 6 pages with English translation.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00257">
<othercit>Office Action, Japanese Appln. No. 2008-554141, dated Nov. 24, 2011, 8 pages with English translation.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00258">
<othercit>Office Action, Japanese Appln. No. 2008-554139, dated Nov. 16, 2011, 12 pages with English translation.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00259">
<othercit>Office Action, Japanese Appln. No. 2008-554138, dated Nov. 22, 2011, 7 pages with English translation.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00260">
<othercit>Quackenbush, &#x201c;Annex I&#x2014;Audio report&#x201d; ISO/IEC JTC1/SC29/WG11, MPEG, N7757, Moving Picture Experts Group, Bangkok, Thailand, Jan. 2006, pp. 168-196.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00261">
<othercit>&#x201c;Text of ISO/IEC 14496-3:2001/FPDAM 4, Audio Lossless Coding (ALS), New Audio Profiles and BSAC Extensions,&#x201d; International Organization for Standardization, ISO/IEC JTC1/SC29/WG11, No. N7016, Hong Kong, China, Jan. 2005, 65 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00262">
<othercit>Search Report, European Appln. No. 07708824.3, dated Dec. 15, 2010, 7 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00263">
<othercit>Faller, C. et al., &#x201c;Efficient Representation of Spatial Audio Using Perceptual Parametrization,&#x201d; Workshop on Applications of Signal Processing to Audio and Acoustics, Oct. 21-24, 2001, Piscataway, NJ, USA, <i>IEEE, </i>pp. 199-202.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00264">
<othercit>Office Action, Japanese Appln. No. 2008-551195, dated Dec. 21, 2010, 10 pages with English translation.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00265">
<othercit>Kulkarni et al., &#x201c;On the Minimum-Phase Approximation of Head-Related Transfer Functions,&#x201d; Applications of Signal Processing to Audio and Acoustics, IEEE ASSP Workshop on New Paltz, Oct. 15-18, 1995, 4 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00266">
<othercit>&#x201c;ISO/IEC 23003-1:2006/FCD, MPEG Surround,&#x201d; ITU Study Group 16, Video Coding Experts Group&#x2014;ISO/IEC MPEG &#x26; ITU-T VCEG (ISO/IEC/JTC1/SC29/WG11 and ITU-T SG16 Q6), XX, XX, No. N7947, Mar. 3, 2006, 186 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00267">
<othercit>Search Report, European Appln. No. 07701037.9, dated Jun. 15, 2011, 8 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00268">
<othercit>Office Action, U.S. Appl. No. 12/161,563, dated Jan. 18, 2012, 39 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00269">
<othercit>Office Action, U.S. Appl. No. 12/161,337, dated Jan. 9, 2012, 4 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00270">
<othercit>Office Action, U.S. Appl. No. 12/278,774, dated Jan. 20, 2012, 44 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00271">
<othercit>&#x201c;Text of ISO/IEC 23003-1:2006/FCD, MPEG Surround,&#x201d; International Organization for Standardization Organisation Internationale De Normalisation, ISO/IEC JTC 1/SC 29/WG 11 Coding of Moving Pictures and Audio, No. N7947, Audio sub-group, Jan. 2006, Bangkok, Thailand, pp. 1-178.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00272">
<othercit>Russian Notice of Allowance for Application No. 2008133995 dated Feb. 11, 2010, 11 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00273">
<othercit>Final Office Action, U.S. Appl. No. 11/915,329, dated Mar. 24, 2011, 14 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00274">
<othercit>Chinese Office Action issued in Appln No. 200780004505.3 on Mar. 2, 2011, 14 pages, including English translation.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00275">
<othercit>Hironori Tokuno. Et al. &#x2018;Inverse Filter of Sound Reproduction Systems Using Regularization&#x2019;, IEICE Trans. Fundamentals. vol. E80-A.No. 5.May 1997, pp. 809-820.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00276">
<othercit>Korean Office Action for Appln. No. 10-2008-7016477 dated Mar. 26, 2010, 4 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00277">
<othercit>Korean Office Action for Appln. No. 10-2008-7016478 dated Mar. 26, 2010, 4 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00278">
<othercit>Korean Office Action for Appln. No. 10-2008-7016479 dated Mar. 26, 2010, 4 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00279">
<othercit>Taiwanese Office Action for Appln. No. 096102406 dated Mar. 4, 2010, 7 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00280">
<othercit>European Search Report, EP Application No. 07 708 825.0, mailed May 26, 2010, 8 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00281">
<othercit>Schroeder, E. F. et al., &#x201c;Der MPEG-2-Standard: Generische Codierung f&#xfc;r Bewegtbilder und zugeh&#xf6;rige Audio-Information, Audio-Codierung (Teil 4),&#x201d; Fkt Fernseh Und Kinotechnik, Fachverlag Schiele &#x26; Schon Gmbh., Berlin, DE, vol. 47, No. 7-8, Aug. 30, 1994, pp. 364-368 and 370.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00282">
<othercit>Notice of Allowance (English language translation) from RU 2008136007 dated Jun. 8, 2010, 5 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00283">
<othercit>Japanese Office Action for Application No. 2008-513378, dated Dec. 14, 2009, 12 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00284">
<othercit>Taiwanese Office Action for Application No. 096102407, dated Dec. 10, 2009, 8 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00285">
<othercit>Taiwan Patent Office, Office Action in Taiwanese patent application 096102410, dated Jul. 2, 2009, 5 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00286">
<othercit>Chinese Gazette, Chinese Appln. No. 200680018245.0, dated Jul. 27, 2011, 3 pages with English abstract.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00287">
<othercit>Notice of Allowance, Japanese Appln. No. 2008-551193, dated Jul. 20, 2011, 6 pages with English translation.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00288">
<othercit>Russian Notice of Allowance for Application No. 2008114388, dated Aug. 24, 2009, 13 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00289">
<othercit>Taiwanese Office Action for Application No. 96104544, dated Oct. 9, 2009, 13 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00290">
<othercit>International Search Report for PCT Application No. PCT/KR2007/000342, dated Apr. 20, 2007, 3 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00291">
<othercit>European Search Report for Application No. 07 708 820.1 dated Apr. 9, 2010, 8 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00292">
<othercit>European Search Report for Application No. 07 708 818.5 dated Apr. 15, 2010, 7 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00293">
<othercit>U.S. Office Action dated Mar. 15, 2012 for U.S. Appl. No. 12/161,558, 4 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00294">
<othercit>U.S. Office Action dated Mar. 30, 2012 for U.S. Appl. No. 11/915,319, 12 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00295">
<othercit>European Office Action dated Apr. 2, 2012 for Application No. 06 747 458.5, 4 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00296">
<othercit>Beack S; et al.; &#x201c;An Efficient Representation Method for ICLD with Robustness to Spectral Distortion&#x201d;, IETRI Journal, vol. 27, No. 3, Jun. 2005, Electronics and Telecommunications Research Institute, KR, Jun. 1, 2005, XP003008889, 4 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00297">
<othercit>Pasi, Ojala, &#x201c;New use cases for spatial audio coding,&#x201d; ITU Study Group 16&#x2014;Video Coding Experts Group&#x2014;ISO/IEG MPEG &#x26; ITU-T VCEG (ISO/IEC JTC1/SC29/WG11 and ITU-T SG16 Q6), XX, XX, No. M12913; XP030041582 (Jan. 11, 2006).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00298">
<othercit>Pasi, Ojala et al., &#x201c;Further information on 1-26 Nokia binaural decoder,&#x201d; ITU Study Group 16&#x2014;Video Coding Experts Group&#x2014;ISO/IEC MPEG &#x26; ITU-T VCEG (ISO/IEC JTC1/SC29/WG11 and ITU-T SG16 Q6), XX, XX, No. M13231; XP030041900 (Mar. 29, 2006).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00299">
<othercit>Kristofer, Kjorling, &#x201c;Proposal for extended signaling in spatial audio,&#x201d; ITU Study Group 16&#x2014;Video Coding Experts Group&#x2014;ISO/IEC MPEG &#x26; ITU-T VCEG (ISO/IEC JTC1/SC29/WG11 and ITU-T SG16 Q6), XX, XX, No. M12361; XP030041045 (Jul. 20, 2005).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00300">
<othercit>WD 2 for MPEG Surround, ITU Study Group 16&#x2014;Video Coding Experts Group&#x2014;ISO/IEC MPEG &#x26; ITU-T VCEG (ISO/IEC JTC1/SC29/WG11 and ITU-T SG16 Q6), XX, XX, No. N7387; XP030013965 (Jul. 29, 2005).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00301">
<othercit>EPO Examiner, European Search Report for Application No. 06 747 458.5 dated Feb. 4, 2011.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00302">
<othercit>EPO Examiner, European Search Report for Application No. 06 747 459.3 dated Feb. 4, 2011.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00303">
<othercit>Office Action, U.S. Appl. No. 12/161,563, dated Apr. 16, 2012, 11 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00304">
<othercit>Office Action, U.S. Appl. No. 12/278,775, dated Jun. 11, 2012, 13 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00305">
<othercit>Office Action, U.S. Appl. No. 12/278,774, dated Jun. 18, 2012, 12 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00306">
<othercit>Quackenbush, MPEG Audio Subgroup, Panasonic Presentation, Annex 1&#x2014;Audio Report, 75<sup>th </sup>meeting, Bangkok, Thailand, Jan. 16-20, 2006, pp. 168-196.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00307">
<othercit>Notice of Allowance, U.S. Appl. No. 12/161,558, dated Aug. 10, 2012, 9 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00308">
<othercit>Breebaart, et al.: &#x201c;Multi-Channel Goes Mobile: MPEG Surround Binaural Rendering&#x201d; In: Audio Engineering Society the 29th International Conference, Seoul, Sep. 2-4, 2006, pp. 1-13. See the abstract, pp. 1-4, figures 5,6.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00309">
<othercit>Breebaart, J., et al.: &#x201c;MPEG Spatial Audio Coding/MPEG Surround: Overview and Current Status&#x201d; In: Audio Engineering Society the 119th Convention, New York, Oct. 7-10, 2005, pp. 1-17. See pp. 4-6.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00310">
<othercit>Faller, C., et al.: &#x201c;Binaural Cue Coding-Part II: Schemes and Applications&#x201d;, IEEE Transactions on Speech and Audio Processing, vol. 11, No. 6, 2003, 12 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00311">
<othercit>Faller, C.: &#x201c;Coding of Spatial Audio Compatible with Different Playback Formats&#x201d;, Audio Engineering Society Convention Paper, Presented at 117th Convention, Oct. 28-31, 2004, San Francisco, CA.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00312">
<othercit>Faller, C.: &#x201c;Parametric Coding of Spatial Audio&#x201d;, Proc. of the 7th Int. Conference on Digital Audio Effects, Naples, Italy, 2004, 6 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00313">
<othercit>Herre, J., et al.: &#x201c;Spatial Audio Coding: Next generation efficient and compatible coding of multi-channel audio&#x201d;, Audio Engineering Society Convention Paper, San Francisco, CA, 2004, 13 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00314">
<othercit>Herre, J., et al.: &#x201c;The Reference Model Architecture for MPEG Spatial Audio Coding&#x201d;, Audio Engineering Society Convention Paper 6447, 2005, Barcelona, Spain, 13 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00315">
<othercit>International Search Report in International Application No. PCT/KR2006/000345, dated Apr. 19, 2007, 1 page.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00316">
<othercit>International Search Report in International Application No. PCT/KR2006/000346, dated Apr. 18, 2007, 1 page.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00317">
<othercit>International Search Report in International Application No. PCT/KR2006/000347, dated Apr. 17, 2007, 1 page.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00318">
<othercit>International Search Report in International Application No. PCT/KR2006/000866, dated Apr. 30, 2007, 1 page.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00319">
<othercit>International Search Report in International Application No. PCT/KR2006/000867, dated Apr. 30, 2007, 1 page.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00320">
<othercit>International Search Report in International Application No. PCT/KR2006/000868, dated Apr. 30, 2007, 1 page.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00321">
<othercit>International Search Report in International Application No. PCT/KR2006/001987, dated Nov. 24, 2006, 2 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00322">
<othercit>International Search Report in International Application No. PCT/KR2006/002016, dated Oct. 16, 2006, 2 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00323">
<othercit>International Search Report in International Application No. PCT/KR2006/003659, dated Jan. 9, 2007, 1 page.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00324">
<othercit>International Search Report in International Application No. PCT/KR2006/003661, dated Jan. 11, 2007, 1 page.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00325">
<othercit>International Search Report in International Application No. PCT/KR2007/000340, dated May 4, 2007, 1 page.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00326">
<othercit>International Search Report in International Application No. PCT/KR2007/000668, dated Jun. 11, 2007, 2 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00327">
<othercit>International Search Report in International Application No. PCT/KR2007/000672, dated Jun. 11, 2007, 1 page.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00328">
<othercit>International Search Report in International Application No. PCT/KR2007/000675, dated Jun. 8, 2007, 1 page.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00329">
<othercit>International Search Report in International Application No. PCT/KR2007/000676, dated Jun. 8, 2007, 1 page.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00330">
<othercit>International Search Report in International Application No. PCT/KR2007/000730, dated Jun. 12, 2007, 1 page.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00331">
<othercit>International Search Report in International Application No. PCT/KR2007/001560, dated Jul. 20, 2007, 1 page.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00332">
<othercit>International Search Report in International Application No. PCT/KR2007/001602, dated Jul. 23, 2007, 1 page.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00333">
<othercit>Scheirer, E. D., et al.: &#x201c;AudioBIFS: Describing Audio Scenes with the MPEG-4 Multimedia Standard&#x201d;, IEEE Transactions on Multimedia, Sep. 1999, vol. 1, No. 3, pp. 237-250. See the abstract.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00334">
<othercit>Vannanen, R., et al.: &#x201c;Encoding and Rendering of Perceptual Sound Scenes in the Carrouso Project&#x201d;, AES 22nd International Conference on Virtual, Synthetic and Entertainment Audio, Paris, France, 9 pages, Jun. 2002.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00335">
<othercit>Vannanen, Riitta, &#x201c;User Interaction and Authoring of 3D Sound Scenes in the Carrouso EU project&#x201d;, Audio Engineering Society Convention Paper 5764, Amsterdam, The Netherlands, 2003, 9 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00336">
<othercit>Office Action in U.S. Appl. No. 11/915,329, dated Jan. 14, 2013, 11 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00337">
<othercit>Notice of Allowance in U.S. Appl. No. 12/161,563, dated Sep. 28, 2012, 10 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00338">
<othercit>U.S. Office Action in U.S. Appl. No. 11/915,327, dated Dec. 12, 2012, 16 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00339">
<othercit>U.S. Appl. No. 11/915,329, mailed Oct. 8, 2010, 13 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00340">
<othercit>Moon et al., &#x201c;A Multichannel Audio Compression Method with Virtual Source Location Information for MPEG-4 SAC,&#x201d; IEEE Trans. Consum. Electron., vol. 51, No. 4, Nov. 2005, pp. 1253-1259.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00341">
<othercit>Notice of Allowance in U.S. Appl. No. 11/915,327, mailed Apr. 17, 2013, 13 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00342">
<othercit>U.S. Office Action in U.S. Appl. No. 12/161,560, dated Oct. 3, 2013, 12 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>7</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>381 21- 23</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>8</number-of-drawing-sheets>
<number-of-figures>16</number-of-figures>
</figures>
<us-related-documents>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60765747</doc-number>
<date>20060207</date>
</document-id>
</us-provisional-application>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60771471</doc-number>
<date>20060209</date>
</document-id>
</us-provisional-application>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60773337</doc-number>
<date>20060215</date>
</document-id>
</us-provisional-application>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60775775</doc-number>
<date>20060223</date>
</document-id>
</us-provisional-application>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60781750</doc-number>
<date>20060314</date>
</document-id>
</us-provisional-application>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60782519</doc-number>
<date>20060316</date>
</document-id>
</us-provisional-application>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60792329</doc-number>
<date>20060417</date>
</document-id>
</us-provisional-application>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60793653</doc-number>
<date>20060421</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20090010440</doc-number>
<kind>A1</kind>
<date>20090108</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Jung</last-name>
<first-name>Yang Won</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
<residence>
<country>KR</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Pang</last-name>
<first-name>Hee Suk</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
<residence>
<country>KR</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Oh</last-name>
<first-name>Hyen O</first-name>
<address>
<city>Gyeonggi-do</city>
<country>KR</country>
</address>
</addressbook>
<residence>
<country>KR</country>
</residence>
</us-applicant>
<us-applicant sequence="004" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Kim</last-name>
<first-name>Dong Soo</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
<residence>
<country>KR</country>
</residence>
</us-applicant>
<us-applicant sequence="005" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Lim</last-name>
<first-name>Jae Hyun</first-name>
<address>
<city>Gwanak-gu</city>
<country>KR</country>
</address>
</addressbook>
<residence>
<country>KR</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Jung</last-name>
<first-name>Yang Won</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Pang</last-name>
<first-name>Hee Suk</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Oh</last-name>
<first-name>Hyen O</first-name>
<address>
<city>Gyeonggi-do</city>
<country>KR</country>
</address>
</addressbook>
</inventor>
<inventor sequence="004" designation="us-only">
<addressbook>
<last-name>Kim</last-name>
<first-name>Dong Soo</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
</inventor>
<inventor sequence="005" designation="us-only">
<addressbook>
<last-name>Lim</last-name>
<first-name>Jae Hyun</first-name>
<address>
<city>Gwanak-gu</city>
<country>KR</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Fish &#x26; Richardson P.C.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>LG Electronics, Inc.</orgname>
<role>03</role>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Clark</last-name>
<first-name>S. V.</first-name>
<department>2896</department>
</primary-examiner>
<assistant-examiner>
<last-name>Miyoshi</last-name>
<first-name>Jesse Y</first-name>
</assistant-examiner>
</examiners>
<pct-or-regional-filing-data>
<document-id>
<country>WO</country>
<doc-number>PCT/KR2007/000674</doc-number>
<kind>00</kind>
<date>20070207</date>
</document-id>
<us-371c124-date>
<date>20080806</date>
</us-371c124-date>
</pct-or-regional-filing-data>
<pct-or-regional-publishing-data>
<document-id>
<country>WO</country>
<doc-number>WO2007/091847</doc-number>
<kind>A </kind>
<date>20070816</date>
</document-id>
</pct-or-regional-publishing-data>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">An encoding method and apparatus and a decoding method and apparatus are provided. The decoding method includes extracting a down-mix signal and spatial information regarding a plurality of channels from an input bitstream, and generating a three-dimensional (3D) down-mix signal by performing a 3D rendering operation on the down-mix signal using the spatial information and a filter, wherein the sum of the number of valid signals of the down-mix signal, the number of valid signals of the spatial information, and the number of valid signals of co-efficients of the filter is less than the number of valid signals of the 3D down-mix signal. Accordingly, it is possible to efficiently encode multi-channel signals with 3D effects and to adaptively restore and reproduce audio signals with optimum sound quality according to the characteristics of an audio reproduction environment.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="150.03mm" wi="211.41mm" file="US08625810-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="219.46mm" wi="162.05mm" orientation="landscape" file="US08625810-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="224.62mm" wi="173.31mm" file="US08625810-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="157.23mm" wi="169.25mm" file="US08625810-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="256.03mm" wi="152.15mm" orientation="landscape" file="US08625810-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="81.03mm" wi="162.73mm" file="US08625810-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="217.42mm" wi="116.25mm" orientation="landscape" file="US08625810-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="242.40mm" wi="165.10mm" file="US08625810-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="165.10mm" wi="165.10mm" file="US08625810-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">TECHNICAL FIELD</heading>
<p id="p-0002" num="0001">The present invention relates to an encoding/decoding method and an encoding/decoding apparatus, and more particularly, to an encoding/decoding apparatus which can process an audio signal so that three dimensional (3D) sound effects can be created, and an encoding/decoding method using the encoding/decoding apparatus.</p>
<heading id="h-0002" level="1">BACKGROUND ART</heading>
<p id="p-0003" num="0002">An encoding apparatus down-mixes a multi-channel signal into a signal with fewer channels, and transmits the down-mixed signal to a decoding apparatus. Then, the decoding apparatus restores a multi-channel signal from the down-mixed signal and reproduces the restored multi-channel signal using three or more speakers, for example, 5.1-channel speakers.</p>
<p id="p-0004" num="0003">Multi-channel signals may be reproduced by 2-channel speakers such as headphones. In this case, in order to make a user feel as if sounds output by 2-channel speakers were reproduced from three or more sound sources, it is necessary to develop three-dimensional (3D) processing techniques capable of encoding or decoding multi-channel signals so that 3D effects can be created.</p>
<heading id="h-0003" level="1">DISCLOSURE OF INVENTION</heading>
<heading id="h-0004" level="1">Technical Problem</heading>
<p id="p-0005" num="0004">The present invention provides an encoding/decoding apparatus and an encoding/decoding method which can reproduce multi-channel signals in various reproduction environments by efficiently processing signals with 3D effects.</p>
<heading id="h-0005" level="1">Technical Solution</heading>
<p id="p-0006" num="0005">According to an aspect of the present invention, there is provided a decoding method of decoding a signal, the decoding method including extracting a down-mix signal and spatial information regarding a plurality of channels from an input bitstream, and generating a three-dimensional (3D) down-mix signal by performing a 3D rendering operation on the down-mix signal using the spatial information and a filter, wherein the sum of the number of valid signals of the down-mix signal, the number of valid signals of the spatial information, and the number of valid signals of coefficients of the filter is less than the number of valid signals of the 3D down-mix signal.</p>
<p id="p-0007" num="0006">According to another aspect of the present invention, there is provided a decoding method of decoding a signal, the decoding method including extracting a down-mix signal and a plurality of pieces of spatial information regarding a plurality of channels from an input bitstream, correcting one of the plurality of pieces of spatial information using a piece of spatial information adjacent thereto, and generating a multi-channel signal using the corrected spatial information and the down-mix signal.</p>
<p id="p-0008" num="0007">According to another aspect of the present invention, there is provided an encoding method of encoding a multi-channel signal with a plurality of channels, the encoding method including encoding the multi-channel signal into a down-mix signal with fewer channels, generating spatial information regarding the plurality of channels, and generating a 3D down-mix signal by performing a 3D rendering operation on the down-mix signal using the spatial information and a filter, wherein the sum of the number of valid signals of the down-mix signal, the number of valid signals of the spatial information, and the number of valid signals of coefficients of the filter is less than the number of valid signals of the 3D down-mix signal.</p>
<p id="p-0009" num="0008">According to another aspect of the present invention, there is provided a decoding apparatus for decoding a signal, the decoding apparatus including a bit unpacking unit which extracts a down-mix signal and spatial information regarding a plurality of channels from an input bitstream, and a 3D rendering unit which generates a 3D down-mix signal by performing a 3D rendering operation on the down-mix signal using the spatial information and a filter, wherein the sum of the number of valid signals of the down-mix signal, the number of valid signals of the spatial information, and the number of valid signals of coefficients of the filter is less than the number of valid signals of the 3D down-mix signal.</p>
<p id="p-0010" num="0009">According to another aspect of the present invention, there is provided a decoding apparatus for decoding a signal, the decoding apparatus including a bit unpacking unit which extracts a down-mix signal and a plurality of pieces of spatial information regarding a plurality of channels from an input bitstream, a spatial information correction unit which corrects one of the plurality of pieces of spatial information using a piece of spatial information adjacent to the piece of spatial information to be corrected, and a multi-channel decoder which generates a multi-channel signal using the corrected spatial information and the down-mix signal.</p>
<p id="p-0011" num="0010">According to another aspect of the present invention, there is provided an encoding apparatus for encoding a multi-channel signal with a plurality of channels, the encoding apparatus including a multi-channel encoder which encodes the multi-channel signal into a down-mix signal with fewer channels and generates spatial information regarding the plurality of channels, and a 3D rendering unit which generates a 3D down-mix signal by performing a 3D rendering operation on the down-mix signal using the spatial information and a filter, wherein the sum of the number of valid signals of the down-mix signal, the number of valid signals of the spatial information, and the number of valid signals of coefficients of the filter is less than the number of valid signals of the 3D down-mix signal.</p>
<p id="p-0012" num="0011">According to another aspect of the present invention, there is provided a computer-readable recording medium having a computer program for executing any one of the above-described decoding methods.</p>
<heading id="h-0006" level="1">Advantageous Effects</heading>
<p id="p-0013" num="0012">According to the present invention, it is possible to efficiently encode multi-channel signals with 3D effects and to adaptively restore and reproduce audio signals with optimum sound quality according to the characteristics of a reproduction environment.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0007" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram of an encoding/decoding apparatus according to an embodiment of the present invention;</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram of an encoding apparatus according to an embodiment of the present invention;</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 3</figref> is a block diagram of a decoding apparatus according to an embodiment of the present invention;</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 4</figref> is a block diagram of an encoding apparatus according to another embodiment of the present invention;</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 5</figref> is a block diagram of a decoding apparatus according to another embodiment of the present invention;</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 6</figref> is a block diagram of a decoding apparatus according to another embodiment of the present invention;</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 7</figref> is a block diagram of a three-dimensional (3D) rendering apparatus according to an embodiment of the present invention;</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIGS. 8 through 11</figref> illustrate bitstreams according to embodiments of the present invention;</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 12</figref> is a block diagram of an encoding/decoding apparatus for processing an arbitrary down-mix signal according to an embodiment of the present invention;</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 13</figref> is a block diagram of an arbitrary down-mix signal compensation/3D rendering unit according to an embodiment of the present invention;</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 14</figref> is a block diagram of a decoding apparatus for processing a compatible down-mix signal according to an embodiment of the present invention;</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 15</figref> is a block diagram of a down-mix compatibility processing/3D rendering unit according to an embodiment of the present invention; and</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 16</figref> is a block diagram of a decoding apparatus for canceling crosstalk according to an embodiment of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0008" level="1">BEST MODE FOR CARRYING OUT THE INVENTION</heading>
<p id="p-0027" num="0026">The present invention will hereinafter be described more fully with reference to the accompanying drawings, in which exemplary embodiments of the invention are shown.</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram of an encoding/decoding apparatus according to an embodiment of the present invention. Referring to <figref idref="DRAWINGS">FIG. 1</figref>, an encoding unit <b>100</b> includes a multi-channel encoder <b>110</b>, a three-dimensional (3D) rendering unit <b>120</b>, a down-mix encoder <b>130</b>, and a bit packing unit <b>140</b>.</p>
<p id="p-0029" num="0028">The multi-channel encoder <b>110</b> down-mixes a multi-channel signal with a plurality of channels into a down-mix signal such as a stereo signal or a mono signal and generates spatial information regarding the channels of the multi-channel signal. The spatial information is needed to restore a multi-channel signal from the down-mix signal.</p>
<p id="p-0030" num="0029">Examples of the spatial information include a channel level difference (CLD), which indicates the difference between the energy levels of a pair of channels, a channel prediction coefficient (CPC), which is a prediction coefficient used to generate a 3-channel signal based on a 2-channel signal, inter-channel correlation (ICC), which indicates the correlation between a pair of channels, and a channel time difference (CTD), which is the time interval between a pair of channels.</p>
<p id="p-0031" num="0030">The 3D rendering unit <b>120</b> generates a 3D down-mix signal based on the down-mix signal. The 3D down-mix signal may be a 2-channel signal with three or more directivities and can thus be reproduced by 2-channel speakers such as headphones with 3D effects. In other words, the 3D down-mix signal may be reproduced by 2-channel speakers so that a user can feel as if the 3D down-mix signal were reproduced from a sound source with three or more channels. The direction of a sound source may be determined based on at least one of the difference between the intensities of two sounds respectively input to both ears, the time interval between the two sounds, and the difference between the phases of the two sounds. Therefore, the 3D rendering unit <b>120</b> can convert the down-mix signal into the 3D down-mix signal based on how the humans can determine the 3D location of a sound source with their sense of hearing.</p>
<p id="p-0032" num="0031">The 3D rendering unit <b>120</b> may generate the 3D down-mix signal by filtering the down-mix signal using a filter. In this case, filter-related information, for example, a coefficient of the filter, may be input to the 3D rendering unit <b>120</b> by an external source. The 3D rendering unit <b>120</b> may use the spatial information provided by the multi-channel encoder <b>110</b> to generate the 3D down-mix signal based on the down-mix signal. More specifically, the 3D rendering unit <b>120</b> may convert the down-mix signal into the 3D down-mix signal by converting the down-mix signal into an imaginary multi-channel signal using the spatial information and filtering the imaginary multi-channel signal.</p>
<p id="p-0033" num="0032">The 3D rendering unit <b>120</b> may generate the 3D down-mix signal by filtering the down-mix signal using a head-related transfer function (HRTF) filter.</p>
<p id="p-0034" num="0033">A HRTF is a transfer function which describes the transmission of sound waves between a sound source at an arbitrary location and the eardrum, and returns a value that varies according to the direction and altitude of a sound source. If a signal with no directivity is filtered using the HRTF, the signal may be heard as if it were reproduced from a certain direction.</p>
<p id="p-0035" num="0034">The 3D rendering unit <b>120</b> may perform a 3D rendering operation in a frequency domain, for example, a discrete Fourier transform (DFT) domain or a fast Fourier transform (FFT) domain. In this case, the 3D rendering unit <b>120</b> may perform DFT or FFT before the 3D rendering operation or may perform inverse DFT (IDFT) or inverse FFT (IFFT) after the 3D rendering operation.</p>
<p id="p-0036" num="0035">The 3D rendering unit <b>120</b> may perform the 3D rendering operation in a quadrature mirror filter (QMF)/hybrid domain. In this case, the 3D rendering unit <b>120</b> may perform QMF/hybrid analysis and synthesis operations before or after the 3D rendering operation.</p>
<p id="p-0037" num="0036">The 3D rendering unit <b>120</b> may perform the 3D rendering operation in a time domain. The 3D rendering unit <b>120</b> may determine in which domain the 3D rendering operation is to be performed according to required sound quality and the operational capacity of the encoding/decoding apparatus.</p>
<p id="p-0038" num="0037">The down-mix encoder <b>130</b> encodes the down-mix signal output by the multi-channel encoder <b>110</b> or the 3D down-mix signal output by the 3D rendering unit <b>120</b>. The down-mix encoder <b>130</b> may encode the down-mix signal output by the multi-channel encoder <b>110</b> or the 3D down-mix signal output by the 3D rendering unit <b>120</b> using an audio encoding method such as an advanced audio coding (AAC) method, an MPEG layer 3 (MP3) method, or a bit sliced arithmetic coding (BSAC) method.</p>
<p id="p-0039" num="0038">The down-mix encoder <b>130</b> may encode a non-3D down-mix signal or a 3D down-mix signal. In this case, the encoded non-3D down-mix signal and the encoded 3D down-mix signal may both be included in a bitstream to be transmitted.</p>
<p id="p-0040" num="0039">The bit packing unit <b>140</b> generates a bitstream based on the spatial information and either the encoded non-3D down-mix signal or the encoded 3D down-mix signal.</p>
<p id="p-0041" num="0040">The bitstream generated by the bit packing unit <b>140</b> may include spatial information, down-mix identification information indicating whether a down-mix signal included in the bitstream is a non-3D down-mix signal or a 3D down-mix signal, and information identifying a filter used by the 3D rendering unit <b>120</b> (e.g., HRTF coefficient information).</p>
<p id="p-0042" num="0041">In other words, the bitstream generated by the bit packing unit <b>140</b> may include at least one of a non-3D down-mix signal which has not yet been 3D-processed and an encoder 3D down-mix signal which is obtained by a 3D processing operation performed by an encoding apparatus, and down-mix identification information identifying the type of down-mix signal included in the bitstream.</p>
<p id="p-0043" num="0042">It may be determined which of the non-3D down-mix signal and the encoder 3D down-mix signal is to be included in the bitstream generated by the bit packing unit <b>140</b> at the user's choice or according to the capabilities of the encoding/decoding apparatus illustrated in <figref idref="DRAWINGS">FIG. 1</figref> and the characteristics of a reproduction environment.</p>
<p id="p-0044" num="0043">The HRTF coefficient information may include coefficients of an inverse function of a HRTF used by the 3D rendering unit <b>120</b>. The HRTF coefficient information may only include brief information of coefficients of the HRTF used by the 3D rendering unit <b>120</b>, for example, envelope information of the HRTF coefficients. If a bitstream including the coefficients of the inverse function of the HRTF is transmitted to a decoding apparatus, the decoding apparatus does not need to perform an HRTF coefficient conversion operation, and thus, the amount of computation of the decoding apparatus may be reduced.</p>
<p id="p-0045" num="0044">The bitstream generated by the bit packing unit <b>140</b> may also include information regarding an energy variation in a signal caused by HRTF-based filtering, i.e., information regarding the difference between the energy of a signal to be filtered and the energy of a signal that has been filtered or the ratio of the energy of the signal to be filtered and the energy of the signal that has been filtered.</p>
<p id="p-0046" num="0045">The bitstream generated by the bit packing unit <b>140</b> may also include information indicating whether it includes HRTF coefficients. If HRTF coefficients are included in the bitstream generated by the bit packing unit <b>140</b>, the bitstream may also include information indicating whether it includes either the coefficients of the HRTF used by the 3D rendering unit <b>120</b> or the coefficients of the inverse function of the HRTF.</p>
<p id="p-0047" num="0046">Referring to <figref idref="DRAWINGS">FIG. 1</figref>, a first decoding unit <b>200</b> includes a bit unpacking unit <b>210</b>, a down-mix decoder <b>220</b>, a 3D rendering unit <b>230</b>, and a multi-channel decoder <b>240</b>.</p>
<p id="p-0048" num="0047">The bit unpacking unit <b>210</b> receives an input bitstream from the encoding unit <b>100</b> and extracts an encoded down-mix signal and spatial information from the input bitstream. The down-mix decoder <b>220</b> decodes the encoded down-mix signal. The down-mix decoder <b>220</b> may decode the encoded down-mix signal using an audio signal decoding method such as an AAC method, an MP3 method, or a BSAC method.</p>
<p id="p-0049" num="0048">As described above, the encoded down-mix signal extracted from the input bitstream may be an encoded non-3D down-mix signal or an encoded, encoder 3D down-mix signal. Information indicating whether the encoded down-mix signal extracted from the input bitstream is an encoded non-3D down-mix signal or an encoded, encoder 3D down-mix signal may be included in the input bitstream.</p>
<p id="p-0050" num="0049">If the encoded down-mix signal extracted from the input bitstream is an encoder 3D down-mix signal, the encoded down-mix signal may be readily reproduced after being decoded by the down-mix decoder <b>220</b>.</p>
<p id="p-0051" num="0050">On the other hand, if the encoded down-mix signal extracted from the input bitstream is a non-3D down-mix signal, the encoded down-mix signal may be decoded by the down-mix decoder <b>220</b>, and a down-mix signal obtained by the decoding may be converted into a decoder 3D down-mix signal by a 3D rendering operation performed by the third rendering unit <b>233</b>. The decoder 3D down-mix signal can be readily reproduced.</p>
<p id="p-0052" num="0051">The 3D rendering unit <b>230</b> includes a first renderer <b>231</b>, a second renderer <b>232</b>, and a third renderer <b>233</b>. The first renderer <b>231</b> generates a down-mix signal by performing a 3D rendering operation on an encoder 3D down-mix signal provided by the down-mix decoder <b>220</b>. For example, the first renderer <b>231</b> may generate a non-3D down-mix signal by removing 3D effects from the encoder 3D down-mix signal. The 3D effects of the encoder 3D down-mix signal may not be completely removed by the first renderer <b>231</b>. In this case, a down-mix signal output by the first renderer <b>231</b> may have some 3D effects.</p>
<p id="p-0053" num="0052">The first renderer <b>231</b> may convert the 3D down-mix signal provided by the down-mix decoder <b>220</b> into a down-mix signal with 3D effects removed therefrom using an inverse filter of the filter used by the 3D rendering unit <b>120</b> of the encoding unit <b>100</b>. Information regarding the filter used by the 3D rendering unit <b>120</b> or the inverse filter of the filter used by the 3D rendering unit <b>120</b> may be included in the input bitstream.</p>
<p id="p-0054" num="0053">The filter used by the 3D rendering unit <b>120</b> may be an HRTF filter. In this case, the coefficients of the HRTF used by the encoding unit <b>100</b> or the coefficients of the inverse function of the HRTF may also be included in the input bitstream. If the coefficients of the HRTF used by the encoding unit <b>100</b> are included in the input bitstream, the HRTF coefficients may be inversely converted, and the results of the inverse conversion may be used during the 3D rendering operation performed by the first renderer <b>231</b>. If the coefficients of the inverse function of the HRTF used by the encoding unit <b>100</b> are included in the input bitstream, they may be readily used during the 3D rendering operation performed by the first renderer <b>231</b> without being subjected to any inverse conversion operation. In this case, the amount of computation of the first decoding apparatus <b>100</b> may be reduced.</p>
<p id="p-0055" num="0054">The input bitstream may also include filter information (e.g., information indicating whether the coefficients of the HRTF used by the encoding unit <b>100</b> are included in the input bitstream) and information indicating whether the filter information has been inversely converted.</p>
<p id="p-0056" num="0055">The multi-channel decoder <b>240</b> generates a 3D multi-channel signal with three or more channels based on the down-mix signal with 3D effects removed therefrom and the spatial information extracted from the input bitstream.</p>
<p id="p-0057" num="0056">The second renderer <b>232</b> may generate a 3D down-mix signal with 3D effects by performing a 3D rendering operation on the down-mix signal with 3D effects removed therefrom. In other words, the first renderer <b>231</b> removes 3D effects from the encoder 3D down-mix signal provided by the down-mix decoder <b>220</b>. Thereafter, the second renderer <b>232</b> may generate a combined 3D down-mix signal with 3D effects desired by the first decoding apparatus <b>200</b> by performing a 3D rendering operation on a down-mix signal obtained by the removal performed by the first renderer <b>231</b>, using a filter of the first decoding apparatus <b>200</b>.</p>
<p id="p-0058" num="0057">The first decoding apparatus <b>200</b> may include a renderer in which two or more of the first, second, and third renderers <b>231</b>, <b>232</b>, and <b>233</b> that perform the same operations are integrated.</p>
<p id="p-0059" num="0058">A bitstream generated by the encoding unit <b>100</b> may be input to a second decoding apparatus <b>300</b> which has a different structure from the first decoding apparatus <b>200</b>. The second decoding apparatus <b>300</b> may generate a 3D down-mix signal based on a down-mix signal included in the bitstream input thereto.</p>
<p id="p-0060" num="0059">More specifically, the second decoding apparatus <b>300</b> includes a bit unpacking unit <b>310</b>, a down-mix decoder <b>320</b>, and a 3D rendering unit <b>330</b>. The bit unpacking unit <b>310</b> receives an input bitstream from the encoding unit <b>100</b> and extracts an encoded down-mix signal and spatial information from the input bitstream. The down-mix decoder <b>320</b> decodes the encoded down-mix signal. The 3D rendering unit <b>330</b> performs a 3D rendering operation on the decoded down-mix signal so that the decoded down-mix signal can be converted into a 3D down-mix signal.</p>
<p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram of an encoding apparatus according to an embodiment of the present invention. Referring to <figref idref="DRAWINGS">FIG. 2</figref>, the encoding apparatus includes rendering units <b>400</b> and <b>420</b> and a multi-channel encoder <b>410</b>. Detailed descriptions of the same encoding processes as those of the embodiment of <figref idref="DRAWINGS">FIG. 1</figref> will be omitted.</p>
<p id="p-0062" num="0061">Referring to <figref idref="DRAWINGS">FIG. 2</figref>, the 3D rendering units <b>400</b> and <b>420</b> may be respectively disposed in front of and behind the multi-channel encoder <b>410</b>. Thus, a multi-channel signal may be 3D-rendered by the 3D rendering unit <b>400</b>, and then, the 3D-rendered multi-channel signal may be encoded by the multi-channel encoder <b>410</b>, thereby generating a pre-processed, encoder 3D down-mix signal. Alternatively, the multi-channel signal may be down-mixed by the multi-channel encoder <b>410</b>, and then, the down-mixed signal may be 3D-rendered by the 3D rendering unit <b>420</b>, thereby generating a post-processed, encoder down-mix signal.</p>
<p id="p-0063" num="0062">Information indicating whether the multi-channel signal has been 3D-rendered before or after being down-mixed may be included in a bitstream to be transmitted.</p>
<p id="p-0064" num="0063">The 3D rendering units <b>400</b> and <b>420</b> may both be disposed in front of or behind the multi-channel encoder <b>410</b>.</p>
<p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. 3</figref> is a block diagram of a decoding apparatus according to an embodiment of the present invention. Referring to <figref idref="DRAWINGS">FIG. 3</figref>, the decoding apparatus includes 3D rendering units <b>430</b> and <b>450</b> and a multi-channel decoder <b>440</b>. Detailed descriptions of the same decoding processes as those of the embodiment of <figref idref="DRAWINGS">FIG. 1</figref> will be omitted.</p>
<p id="p-0066" num="0065">Referring to <figref idref="DRAWINGS">FIG. 3</figref>, the 3D rendering units <b>430</b> and <b>450</b> may be respectively disposed in front of and behind the multi-channel decoder <b>440</b>. The 3D rendering unit <b>430</b> may remove 3D effects from an encoder 3D down-mix signal and input a down-mix signal obtained by the removal to the multi-channel decoder <b>430</b>. Then, the multi-channel decoder <b>430</b> may decode the down-mix signal input thereto, thereby generating a pre-processed 3D multi-channel signal. Alternatively, the multi-channel decoder <b>430</b> may restore a multi-channel signal from an encoded 3D down-mix signal, and the 3D rendering unit <b>450</b> may remove 3D effects from the restored multi-channel signal, thereby generating a post-processed 3D multi-channel signal.</p>
<p id="p-0067" num="0066">If an encoder 3D down-mix signal provided by an encoding apparatus has been generated by performing a 3D rendering operation and then a down-mixing operation, the encoder 3D down-mix signal may be decoded by performing a multi-channel decoding operation and then a 3D rendering operation. On the other hand, if the encoder 3D down-mix signal has been generated by performing a down-mixing operation and then a 3D rendering operation, the encoder 3D down-mix signal may be decoded by performing a 3D rendering operation and then a multi-channel decoding operation.</p>
<p id="p-0068" num="0067">Information indicating whether an encoded 3D down-mix signal has been obtained by performing a 3D rendering operation before or after a down-mixing operation may be extracted from a bitstream transmitted by an encoding apparatus.</p>
<p id="p-0069" num="0068">The 3D rendering units <b>430</b> and <b>450</b> may both be disposed in front of or behind the multi-channel decoder <b>440</b>.</p>
<p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. 4</figref> is a block diagram of an encoding apparatus according to another embodiment of the present invention. Referring to <figref idref="DRAWINGS">FIG. 4</figref>, the encoding apparatus includes a multi-channel encoder <b>500</b>, a 3D rendering unit <b>510</b>, a down-mix encoder <b>520</b>, and a bit packing unit <b>530</b>. Detailed descriptions of the same encoding processes as those of the embodiment of <figref idref="DRAWINGS">FIG. 1</figref> will be omitted.</p>
<p id="p-0071" num="0070">Referring to <figref idref="DRAWINGS">FIG. 4</figref>, the multi-channel encoder <b>500</b> generates a down-mix signal and spatial information based on an input multi-channel signal. The 3D rendering unit <b>510</b> generates a 3D down-mix signal by performing a 3D rendering operation on the down-mix signal.</p>
<p id="p-0072" num="0071">It may be determined whether to perform a 3D rendering operation on the down-mix signal at a user's choice or according to the capabilities of the encoding apparatus, the characteristics of a reproduction environment, or required sound quality.</p>
<p id="p-0073" num="0072">The down-mix encoder <b>520</b> encodes the down-mix signal generated by the multi-channel encoder <b>500</b> or the 3D down-mix signal generated by the 3D rendering unit <b>510</b>.</p>
<p id="p-0074" num="0073">The bit packing unit <b>530</b> generates a bitstream based on the spatial information and either the encoded down-mix signal or an encoded, encoder 3D down-mix signal. The bitstream generated by the bit packing unit <b>530</b> may include down-mix identification information indicating whether an encoded down-mix signal included in the bitstream is a non-3D down-mix signal with no 3D effects or an encoder 3D down-mix signal with 3D effects. More specifically, the down-mix identification information may indicate whether the bitstream generated by the bit packing unit <b>530</b> includes a non-3D down-mix signal, an encoder 3D down-mix signal or both.</p>
<p id="p-0075" num="0074"><figref idref="DRAWINGS">FIG. 5</figref> is a block diagram of a decoding apparatus according to another embodiment of the present invention. Referring to <figref idref="DRAWINGS">FIG. 5</figref>, the decoding apparatus includes a bit unpacking unit <b>540</b>, a down-mix decoder <b>550</b>, and a 3D rendering unit <b>560</b>. Detailed descriptions of the same decoding processes as those of the embodiment of <figref idref="DRAWINGS">FIG. 1</figref> will be omitted.</p>
<p id="p-0076" num="0075">Referring to <figref idref="DRAWINGS">FIG. 5</figref>, the bit unpacking unit <b>540</b> extracts an encoded down-mix signal, spatial information, and down-mix identification information from an input bitstream. The down-mix identification information indicates whether the encoded down-mix signal is an encoded non-3D down-mix signal with no 3D effects or an encoded 3D down-mix signal with 3D effects.</p>
<p id="p-0077" num="0076">If the input bitstream includes both a non-3D down-mix signal and a 3D down-mix signal, only one of the non-3D down-mix signal and the 3D down-mix signal may be extracted from the input bitstream at a user's choice or according to the capabilities of the decoding apparatus, the characteristics of a reproduction environment or required sound quality.</p>
<p id="p-0078" num="0077">The down-mix decoder <b>550</b> decodes the encoded down-mix signal. If a down-mix signal obtained by the decoding performed by the down-mix decoder <b>550</b> is an encoder 3D down-mix signal obtained by performing a 3D rendering operation, the down-mix signal may be readily reproduced.</p>
<p id="p-0079" num="0078">On the other hand, if the down-mix signal obtained by the decoding performed by the down-mix decoder <b>550</b> is a down-mix signal with no 3D effects, the 3D rendering unit <b>560</b> may generate a decoder 3D down-mix signal by performing a 3D rendering operation on the down-mix signal obtained by the decoding performed by the down-mix decoder <b>550</b>.</p>
<p id="p-0080" num="0079"><figref idref="DRAWINGS">FIG. 6</figref> is a block diagram of a decoding apparatus according to another embodiment of the present invention. Referring to <figref idref="DRAWINGS">FIG. 6</figref>, the decoding apparatus includes a bit unpacking unit <b>600</b>, a down-mix decoder <b>610</b>, a first 3D rendering unit <b>620</b>, a second 3D rendering unit <b>630</b>, and a filter information storage unit <b>640</b>. Detailed descriptions of the same decoding processes as those of the embodiment of <figref idref="DRAWINGS">FIG. 1</figref> will be omitted.</p>
<p id="p-0081" num="0080">The bit unpacking unit <b>600</b> extracts an encoded, encoder 3D down-mix signal and spatial information from an input bitstream. The down-mix decoder <b>610</b> decodes the encoded, encoder 3D down-mix signal.</p>
<p id="p-0082" num="0081">The first 3D rendering unit <b>620</b> removes 3D effects from an encoder 3D down-mix signal obtained by the decoding performed by the down-mix decoder <b>610</b>, using an inverse filter of a filter of an encoding apparatus used for performing a 3D rendering operation. The second rendering unit <b>630</b> generates a combined 3D down-mix signal with 3D effects by performing a 3D rendering operation on a down-mix signal obtained by the removal performed by the first 3D rendering unit <b>620</b>, using a filter stored in the decoding apparatus.</p>
<p id="p-0083" num="0082">The second 3D rendering unit <b>630</b> may perform a 3D rendering operation using a filter having different characteristics from the filter of the encoding unit used to perform a 3D rendering operation. For example, the second 3D rendering unit <b>630</b> may perform a 3D rendering operation using an HRTF having different coefficients from those of an HRTF used by an encoding apparatus.</p>
<p id="p-0084" num="0083">The filter information storage unit <b>640</b> stores filter information regarding a filter used to perform a 3D rendering, for example, HRTF coefficient information. The second 3D rendering unit <b>630</b> may generate a combined 3D down-mix using the filter information stored in the filter information storage unit <b>640</b>.</p>
<p id="p-0085" num="0084">The filter information storage unit <b>640</b> may store a plurality of pieces of filter information respectively corresponding to a plurality of filters. In this case, one of the plurality of pieces of filter information may be selected at a user's choice or according to the capabilities of the decoding apparatus or required sound quality.</p>
<p id="p-0086" num="0085">People from different races may have different ear structures. Thus, HRTF coefficients optimized for different individuals may differ from one another. The decoding apparatus illustrated in <figref idref="DRAWINGS">FIG. 6</figref> can generate a 3D down-mix signal optimized for the user. In addition, the decoding apparatus illustrated in <figref idref="DRAWINGS">FIG. 6</figref> can generate a 3D down-mix signal with 3D effects corresponding to an HRTF filter desired by the user, regardless of the type of HRTF provided by a 3D down-mix signal provider.</p>
<p id="p-0087" num="0086"><figref idref="DRAWINGS">FIG. 7</figref> is a block diagram of a 3D rendering apparatus according to an embodiment of the present invention. Referring to <figref idref="DRAWINGS">FIG. 7</figref>, the 3D rendering apparatus includes first and second domain conversion units <b>700</b> and <b>720</b> and a 3D rendering unit <b>710</b>. In order to perform a 3D rendering operation in a predetermined domain, the first and second domain conversion units <b>700</b> and <b>720</b> may be respectively disposed in front of and behind the 3D rendering unit <b>710</b>.</p>
<p id="p-0088" num="0087">Referring to <figref idref="DRAWINGS">FIG. 7</figref>, an input down-mix signal is converted into a frequency-domain down-mix signal by the first domain conversion unit <b>700</b>. More specifically, the first domain conversion unit <b>700</b> may convert the input down-mix signal into a DFT-domain down-mix signal or a FFT-domain down-mix signal by performing DFT or FFT.</p>
<p id="p-0089" num="0088">The 3D rendering unit <b>710</b> generates a multi-channel signal by applying spatial information to the frequency-domain down-mix signal provided by the first domain conversion unit <b>700</b>. Thereafter, the 3D rendering unit <b>710</b> generates a 3D down-mix signal by filtering the multi-channel signal.</p>
<p id="p-0090" num="0089">The 3D down-mix signal generated by the 3D rendering unit <b>710</b> is converted into a time-domain 3D down-mix signal by the second domain conversion unit <b>720</b>. More specifically, the second domain conversion unit <b>720</b> may perform IDFT or IFFT on the 3D down-mix signal generated by the 3D rendering unit <b>710</b>.</p>
<p id="p-0091" num="0090">During the conversion of a frequency-domain 3D down-mix signal into a time-domain 3D down-mix signal, data loss or data distortion such as aliasing may occur.</p>
<p id="p-0092" num="0091">In order to generate a multi-channel signal and a 3D down-mix signal in a frequency domain, spatial information for each parameter band may be mapped to the frequency domain, and a number of filter coefficients may be converted to the frequency domain.</p>
<p id="p-0093" num="0092">The 3D rendering unit <b>710</b> may generate a 3D down-mix signal by multiplying the frequency-domain down-mix signal provided by the first domain conversion unit <b>700</b>, the spatial information, and the filter coefficients.</p>
<p id="p-0094" num="0093">A time-domain signal obtained by multiplying a down-mix signal, spatial information and a plurality of filter coefficients that are all represented in an M-point frequency domain has M valid signals. In order to represent the down-mix signal, the spatial information and the filter in the M-point frequency domain, M-point DFT or M-point FFT may be performed.</p>
<p id="p-0095" num="0094">Valid signals are signals that do not necessarily have a value of 0. For example, a total of x valid signals can be generated by obtaining x signals from an audio signal through sampling. Of the x valid signals, y valid signals may be zero-padded. Then, the number of valid signals is reduced to (x&#x2212;y). Thereafter, a signal with a valid signals and a signal with b valid signals are convoluted, thereby obtaining a total of (a+b&#x2212;1) valid signals.</p>
<p id="p-0096" num="0095">The multiplication of the down-mix signal, the spatial information, and the filter coefficients in the M-point frequency domain can provide the same effect as convoluting the down-mix signal, the spatial information, and the filter coefficients in a time-domain. A signal with (3*M&#x2212;2) valid signals can be generated by converting the down-mix signal, the spatial information and the filter coefficients in the M-point frequency domain to a time domain and convoluting the results of the conversion.</p>
<p id="p-0097" num="0096">Therefore, the number of valid signals of a signal obtained by multiplying a down-mix signal, spatial information, and filter coefficients in a frequency domain and converting the result of the multiplication to a time domain may differ from the number of valid signals of a signal obtained by convoluting the down-mix signal, the spatial information, and the filter coefficients in the time domain. As a result, aliasing may occur during the conversion of a 3D down-mix signal in a frequency domain into a time-domain signal.</p>
<p id="p-0098" num="0097">In order to prevent aliasing, the sum of the number of valid signals of a down-mix signal in a time domain, the number of valid signals of spatial information mapped to a frequency domain, and the number of filter coefficients must not be greater than M. The number of valid signals of spatial information mapped to a frequency domain may be determined by the number of points of the frequency domain. In other words, if spatial information represented for each parameter band is mapped to an N-point frequency domain, the number of valid signals of the spatial information may be N.</p>
<p id="p-0099" num="0098">Referring to <figref idref="DRAWINGS">FIG. 7</figref>, the first domain conversion unit <b>700</b> includes a first zero-padding unit <b>701</b> and a first frequency-domain conversion unit <b>702</b>. The third rendering unit <b>710</b> includes a mapping unit <b>711</b>, a time-domain conversion unit <b>712</b>, a second zero-padding unit <b>713</b>, a second frequency-domain conversion unit <b>714</b>, a multi-channel signal generation unit <b>715</b>, a third zero-padding unit <b>716</b>, a third frequency-domain conversion unit <b>717</b>, and a 3D down-mix signal generation unit <b>718</b>.</p>
<p id="p-0100" num="0099">The first zero-padding unit <b>701</b> performs a zero-padding operation on a down-mix signal with X samples in a time domain so that the number of samples of the down-mix signal can be increased from X to M. The first frequency-domain conversion unit <b>702</b> converts the zero-padded down-mix signal into an M-point frequency-domain signal. The zero-padded down-mix signal has M samples. Of the M samples of the zero-padded down-mix signal, only X samples are valid signals.</p>
<p id="p-0101" num="0100">The mapping unit <b>711</b> maps spatial information for each parameter band to an N-point frequency domain. The time-domain conversion unit <b>712</b> converts spatial information obtained by the mapping performed by the mapping unit <b>711</b> to a time domain. Spatial information obtained by the conversion performed by the time-domain conversion unit <b>712</b> has N samples.</p>
<p id="p-0102" num="0101">The second zero-padding unit <b>713</b> performs a zero-padding operation on the spatial information with N samples in the time domain so that the number of samples of the spatial information can be increased from N to M. The second frequency-domain conversion unit <b>714</b> converts the zero-padded spatial information into an M-point frequency-domain signal. The zero-padded spatial information has N samples. Of the N samples of the zero-padded spatial information, only N samples are valid.</p>
<p id="p-0103" num="0102">The multi-channel signal generation unit <b>715</b> generates a multi-channel signal by multiplying the down-mix signal provided by the first frequency-domain conversion unit <b>712</b> and spatial information provided by the second frequency-domain conversion unit <b>714</b>. The multi-channel signal generated by the multi-channel signal generation unit <b>715</b> has M valid signals. On the other hand, a multi-channel signal obtained by convoluting, in the time domain, the down-mix signal provided by the first frequency-domain conversion unit <b>712</b> and the spatial information provided by the second frequency-domain conversion unit <b>714</b> has (X+N&#x2212;1) valid signals.</p>
<p id="p-0104" num="0103">The third zero-padding unit <b>716</b> may perform a zero-padding operation on Y filter coefficients that are represented in the time domain so that the number of samples can be increased to M. The third frequency-domain conversion unit <b>717</b> converts the zero-padded filter coefficients to the M-point frequency domain. The zero-padded filter coefficients have M samples. Of the M samples, only Y samples are valid signals.</p>
<p id="p-0105" num="0104">The 3D down-mix signal generation unit <b>718</b> generates a 3D down-mix signal by multiplying the multi-channel signal generated by the multi-channel signal generation unit <b>715</b> and a plurality of filter coefficients provided by the third frequency-domain conversion unit <b>717</b>. The 3D down-mix signal generated by the 3D down-mix signal generation unit <b>718</b> has M valid signals. On the other hand, a 3D down-mix signal obtained by convoluting, in the time domain, the multi-channel signal generated by the multi-channel signal generation unit <b>715</b> and the filter coefficients provided by the third frequency-domain conversion unit <b>717</b> has (X+N+Y&#x2212;2) valid signals.</p>
<p id="p-0106" num="0105">It is possible to prevent aliasing by setting the M-point frequency domain used by the first, second, and third frequency-domain conversion units <b>702</b>, <b>714</b>, and <b>717</b> to satisfy the following equation: M&#x2267;(X+N+Y&#x2212;2). In other words, it is possible to prevent aliasing by enabling the first, second, and third frequency-domain conversion units <b>702</b>, <b>714</b>, and <b>717</b> to perform M-point DFT or M-point FFT that satisfies the following equation: M&#x2267;(X+N+Y&#x2212;2).</p>
<p id="p-0107" num="0106">The conversion to a frequency domain may be performed using a filter bank other than a DFT filter bank, an FFT filter bank, and QMF bank. The generation of a 3D down-mix signal may be performed using an HRTF filter.</p>
<p id="p-0108" num="0107">The number of valid signals of spatial information may be adjusted using a method other than the above-mentioned methods or may be adjusted using one of the above-mentioned methods that is most efficient and requires the least amount of computation.</p>
<p id="p-0109" num="0108">Aliasing may occur not only during the conversion of a signal, a coefficient or spatial information from a frequency domain to a time domain or vice versa but also during the conversion of a signal, a coefficient or spatial information from a QMF domain to a hybrid domain or vice versa. The above-mentioned methods of preventing aliasing may also be used to prevent aliasing from occurring during the conversion of a signal, a coefficient or spatial information from a QMF domain to a hybrid domain or vice versa.</p>
<p id="p-0110" num="0109">Spatial information used to generate a multi-channel signal or a 3D down-mix signal may vary. As a result of the variation of the spatial information, signal discontinuities may occur as noise in an output signal.</p>
<p id="p-0111" num="0110">Noise in an output signal may be reduced using a smoothing method by which spatial information can be prevented from rapidly varying.</p>
<p id="p-0112" num="0111">For example, when first spatial information applied to a first frame differs from second spatial information applied to a second frame when the first frame and the second frame are adjacent to each other, a discontinuity is highly likely to occur between the first and second frames.</p>
<p id="p-0113" num="0112">In this case, the second spatial information may be compensated for using the first spatial information or the first spatial information may be compensated for using the second spatial information so that the difference between the first spatial information and the second spatial information can be reduced, and that noise caused by the discontinuity between the first and second frames can be reduced. More specifically, at least one of the first spatial information and the second spatial information may be replaced with the average of the first spatial information and the second spatial information, thereby reducing noise.</p>
<p id="p-0114" num="0113">Noise is also likely to be generated due to a discontinuity between a pair of adjacent parameter bands. For example, when third spatial information corresponding to a first parameter band differs from fourth spatial information corresponding to a second parameter band when the first and second parameter bands are adjacent to each other, a discontinuity is likely to occur between the first and second parameter bands.</p>
<p id="p-0115" num="0114">In this case, the third spatial information may be compensated for using the fourth spatial information or the fourth spatial information may be compensated for using the third spatial information so that the difference between the third spatial information and the fourth spatial information can be reduced, and that noise caused by the discontinuity between the first and second parameter bands can be reduced. More specifically, at least one of the third spatial information and the fourth spatial information may be replaced with the average of the third spatial information and the fourth spatial information, thereby reducing noise.</p>
<p id="p-0116" num="0115">Noise caused by a discontinuity between a pair of adjacent frames or a pair of adjacent parameter bands may be reduced using methods other than the above-mentioned methods.</p>
<p id="p-0117" num="0116">More specifically, each frame may be multiplied by a window such as a Hanning window, and an &#x201c;overlap and add&#x201d; scheme may be applied to the results of the multiplication so that the variations between the frames can be reduced. Alternatively, an output signal to which a plurality of pieces of spatial information are applied may be smoothed so that variations between a plurality of frames of the output signal can be prevented.</p>
<p id="p-0118" num="0117">The decorrelation between channels in a DFT domain using spatial information, for example, ICC, may be adjusted as follows.</p>
<p id="p-0119" num="0118">The degree of decorrelation may be adjusted by multiplying a coefficient of a signal input to a one-to-two (OTT) or two-to-three (TTT) box by a predetermined value. The predetermined value can be defined by the following equation:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>(A+(1&#x2212;A*A)^0.5*i)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where A indicates an ICC value applied to a predetermined band of the OTT or TTT box and i indicates an imaginary part. The imaginary part may be positive or negative.
</p>
<p id="p-0120" num="0119">The predetermined value may accompany a weighting factor according to the characteristics of the signal, for example, the energy level of the signal, the energy characteristics of each frequency of the signal, or the type of box to which the ICC value A is applied. As a result of the introduction of the weighting factor, the degree of decorrelation may be further adjusted, and interframe smoothing or interpolation may be applied.</p>
<p id="p-0121" num="0120">As described above with reference to <figref idref="DRAWINGS">FIG. 7</figref>, a 3D down-mix signal may be generated in a frequency domain by using an HRTF or a head related impulse response (HRIR), which is converted to the frequency domain.</p>
<p id="p-0122" num="0121">Alternatively, a 3D down-mix signal may be generated by convoluting an HRIR and a down-mix signal in a time domain. A 3D down-mix signal generated in a frequency domain may be left in the frequency domain without being subjected to inverse domain transform.</p>
<p id="p-0123" num="0122">In order to convolute an HRIR and a down-mix signal in a time domain, a finite impulse response (FIR) filter or an infinite impulse response (IIR) filter may be used.</p>
<p id="p-0124" num="0123">As described above, an encoding apparatus or a decoding apparatus according to an embodiment of the present invention may generate a 3D down-mix signal using a first method that involves the use of an HRTF in a frequency domain or an HRIR converted to the frequency domain, a second method that involves convoluting an HRIR in a time domain, or the combination of the first and second methods.</p>
<p id="p-0125" num="0124"><figref idref="DRAWINGS">FIGS. 8 through 11</figref> illustrate bitstreams according to embodiments of the present invention.</p>
<p id="p-0126" num="0125">Referring to <figref idref="DRAWINGS">FIG. 8</figref>, a bitstream includes a multi-channel decoding information field which includes information necessary for generating a multi-channel signal, a 3D rendering information field which includes information necessary for generating a 3D down-mix signal, and a header field which includes header information necessary for using the information included in the multi-channel decoding information field and the information included in the 3D rendering information field. The bitstream may include only one or two of the multi-channel decoding information field, the 3D rendering information field, and the header field.</p>
<p id="p-0127" num="0126">Referring to <figref idref="DRAWINGS">FIG. 9</figref>, a bitstream, which contains side information necessary for a decoding operation, may include a specific configuration header field which includes header information of a whole encoded signal and a plurality of frame data fields which includes side information regarding a plurality of frames. More specifically, each of the frame data fields may include a frame header field which includes header information of a corresponding frame and a frame parameter data field which includes spatial information of the corresponding frame. Alternatively, each of the frame data fields may include a frame parameter data field only.</p>
<p id="p-0128" num="0127">Each of the frame parameter data fields may include a plurality of modules, each module including a flag and parameter data. The modules are data sets including parameter data such as spatial information and other data such as down-mix gain and smoothing data which is necessary for improving the sound quality of a signal.</p>
<p id="p-0129" num="0128">If module data regarding information specified by the frame header fields is received without any additional flag, if the information specified by the frame header fields is further classified, or if an additional flag and data are received in connection with information not specified by the frame header, module data may not include any flag.</p>
<p id="p-0130" num="0129">Side information regarding a 3D down-mix signal, for example, HRTF coefficient information, may be included in at least one of the specific configuration header field, the frame header fields, and the frame parameter data fields.</p>
<p id="p-0131" num="0130">Referring to <figref idref="DRAWINGS">FIG. 10</figref>, a bitstream may include a plurality of multi-channel decoding information fields which include information necessary for generating multi-channel signals and a plurality of 3D rendering information fields which include information necessary for generating 3D down-mix signals.</p>
<p id="p-0132" num="0131">When receiving the bitstream, a decoding apparatus may use either the multi-channel decoding information fields or the 3D rendering information field to perform a decoding operation and skip whichever of the multi-channel decoding information fields and the 3D rendering information fields are not used in the decoding operation. In this case, it may be determined which of the multi-channel decoding information fields and the 3D rendering information fields are to be used to perform a decoding operation according to the type of signals to be reproduced.</p>
<p id="p-0133" num="0132">In other words, in order to generate multi-channel signals, a decoding apparatus may skip the 3D rendering information fields, and read information included in the multi-channel decoding information fields. On the other hand, in order to generate 3D down-mix signals, a decoding apparatus may skip the multi-channel decoding information fields, and read information included in the 3D rendering information fields.</p>
<p id="p-0134" num="0133">Methods of skipping some of a plurality of fields in a bitstream are as follows.</p>
<p id="p-0135" num="0134">First, field length information regarding the size in bits of a field may be included in a bitstream. In this case, the field may be skipped by skipping a number of bits corresponding to the size in bits of the field. The field length information may be disposed at the beginning of the field.</p>
<p id="p-0136" num="0135">Second, a syncword may be disposed at the end or the beginning of a field. In this case, the field may be skipped by locating the field based on the location of the syncword.</p>
<p id="p-0137" num="0136">Third, if the length of a field is determined in advance and fixed, the field may be skipped by skipping an amount of data corresponding to the length of the field. Fixed field length information regarding the length of the field may be included in a bitstream or may be stored in a decoding apparatus.</p>
<p id="p-0138" num="0137">Fourth, one of a plurality of fields may be skipped using the combination of two or more of the above-mentioned field skipping methods.</p>
<p id="p-0139" num="0138">Field skip information, which is information necessary for skipping a field such as field length information, syncwords, or fixed field length information may be included in one of the specific configuration header field, the frame header fields, and the frame parameter data fields illustrated in <figref idref="DRAWINGS">FIG. 9</figref> or may be included in a field other than those illustrated in <figref idref="DRAWINGS">FIG. 9</figref>.</p>
<p id="p-0140" num="0139">For example, in order to generate multi-channel signals, a decoding apparatus may skip the 3D rendering information fields with reference to field length information, a syncword, or fixed field length information disposed at the beginning of each of the 3D rendering information fields, and read information included in the multi-channel decoding information fields.</p>
<p id="p-0141" num="0140">On the other hand, in order to generate 3D down-mix signals, a decoding apparatus may skip the multi-channel decoding information fields with reference to field length information, a syncword, or fixed field length information disposed at the beginning of each of the multi-channel decoding information fields, and read information included in the 3D rendering information fields.</p>
<p id="p-0142" num="0141">A bitstream may include information indicating whether data included in the bitstream is necessary for generating multi-channel signals or for generating 3D down-mix signals.</p>
<p id="p-0143" num="0142">However, even if a bitstream does not include any spatial information such as CLD but includes only data (e.g., HRTF filter coefficients) necessary for generating a 3D down-mix signal, a multi-channel signal can be reproduced through decoding using the data necessary for generating a 3D down-mix signal without a requirement of the spatial information.</p>
<p id="p-0144" num="0143">For example, a stereo parameter, which is spatial information regarding two channels, is obtained from a down-mix signal. Then, the stereo parameter is converted into spatial information regarding a plurality of channels to be reproduced, and a multi-channel signal is generated by applying the spatial information obtained by the conversion to the down-mix signal.</p>
<p id="p-0145" num="0144">On the other hand, even if a bitstream includes only data necessary for generating a multi-channel signal, a down-mix signal can be reproduced without a requirement of an additional decoding operation or a 3D down-mix signal can be reproduced by performing 3D processing on the down-mix signal using an additional HRTF filter.</p>
<p id="p-0146" num="0145">If a bitstream includes both data necessary for generating a multi-channel signal and data necessary for generating a 3D down-mix signal, a user may be allowed to decide whether to reproduce a multi-channel signal or a 3D down-mix signal.</p>
<p id="p-0147" num="0146">Methods of skipping data will hereinafter be described in detail with reference to respective corresponding syntaxes.</p>
<p id="p-0148" num="0147">Syntax 1 indicates a method of decoding an audio signal in units of frames.</p>
<p id="p-0149" num="0148">[Syntax 1]</p>
<p id="p-0150" num="0149">
<tables id="TABLE-US-00001" num="00001">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="49pt" align="left"/>
<colspec colname="1" colwidth="168pt" align="left"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry>SpatialFrame( )</entry>
</row>
<row>
<entry/>
<entry>{</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="63pt" align="left"/>
<colspec colname="1" colwidth="154pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>FramingInfo( );</entry>
</row>
<row>
<entry/>
<entry>bsIndependencyFlag;</entry>
</row>
<row>
<entry/>
<entry>OttData( );</entry>
</row>
<row>
<entry/>
<entry>TttData( );</entry>
</row>
<row>
<entry/>
<entry>SmgData( );</entry>
</row>
<row>
<entry/>
<entry>TempShapeData( );</entry>
</row>
<row>
<entry/>
<entry>if (bsArbitraryDownmix) {</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="77pt" align="left"/>
<colspec colname="1" colwidth="140pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>ArbitraryDownmixData( );</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="63pt" align="left"/>
<colspec colname="1" colwidth="154pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>}</entry>
</row>
<row>
<entry/>
<entry>if (bsResidualCoding) {</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="77pt" align="left"/>
<colspec colname="1" colwidth="140pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>ResidualData( );</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="63pt" align="left"/>
<colspec colname="1" colwidth="154pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>}</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="49pt" align="left"/>
<colspec colname="1" colwidth="168pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>}</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0151" num="0150">In Syntax 1, Ottdata( ) and TttData( ) are modules which represent parameters (such as spatial information including a CLD, ICC, and CPC) necessary for restoring a multi-channel signal from a down-mix signal, and 5 mgData( ), TempShapeData( ), Arbitrary-DownmixData( ), and ResidualData( ) are modules which represent information necessary for improving the quality of sound by correcting signal distortions that may have occurred during an encoding operation.</p>
<p id="p-0152" num="0151">For example, if a parameter such as a CLD, ICC or CPC and information included in the module ArbitraryDownmixData( ) are only used during a decoding operation, the modules 5 mgData( ) and TempShapeData( ), which are disposed between the modules TttData( ) and ArbitraryDownmixData( ), may be unnecessary. Thus, it is efficient to skip the modules 5 mgData( ) and TempShapeData( ).</p>
<p id="p-0153" num="0152">A method of skipping modules according to an embodiment of the present invention will hereinafter be described in detail with reference to Syntax 2 below.</p>
<p id="p-0154" num="0153">[Syntax 2]</p>
<p id="p-0155" num="0154">
<tables id="TABLE-US-00002" num="00002">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="84pt" align="left"/>
<colspec colname="1" colwidth="133pt" align="left"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry>:</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="56pt" align="left"/>
<colspec colname="1" colwidth="161pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>TttData( );</entry>
</row>
<row>
<entry/>
<entry>SkipData( ){</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="70pt" align="left"/>
<colspec colname="1" colwidth="147pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>bsSkipBits;</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="56pt" align="left"/>
<colspec colname="1" colwidth="161pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>}</entry>
</row>
<row>
<entry/>
<entry>SmgData( );</entry>
</row>
<row>
<entry/>
<entry>TempShapeData( );</entry>
</row>
<row>
<entry/>
<entry>if (bsArbitraryDownmix) {</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="70pt" align="left"/>
<colspec colname="1" colwidth="147pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>ArbitraryDownmixData( );</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="56pt" align="left"/>
<colspec colname="1" colwidth="161pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>}</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="84pt" align="left"/>
<colspec colname="1" colwidth="133pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>:</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0156" num="0155">Referring to Syntax 2, a module SkipData( ) may be disposed in front of a module to be skipped, and the size in bits of the module to be skipped is specified in the module SkipData( ) as bsSkipBits.</p>
<p id="p-0157" num="0156">In other words, assuming that modules 5 mgData( ) and TempShapeData( ) are to be skipped, and that the size in bits of the modules 5 mgData( ) and TempShapeData( ) combined is 150, the modules 5 mgData( ) and TempShapeData( ) can be skipped by setting bsSkipBits to 150.</p>
<p id="p-0158" num="0157">A method of skipping modules according to another embodiment of the present invention will hereinafter be described in detail with reference to Syntax 3.</p>
<p id="p-0159" num="0158">[Syntax 3]</p>
<p id="p-0160" num="0159">
<tables id="TABLE-US-00003" num="00003">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="91pt" align="left"/>
<colspec colname="1" colwidth="126pt" align="left"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry>:</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="63pt" align="left"/>
<colspec colname="1" colwidth="154pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>TttData( );</entry>
</row>
<row>
<entry/>
<entry>bsSkipSyncflag;</entry>
</row>
<row>
<entry/>
<entry>SmgData( );</entry>
</row>
<row>
<entry/>
<entry>TempShapeData( );</entry>
</row>
<row>
<entry/>
<entry>bsSkipSyncword;</entry>
</row>
<row>
<entry/>
<entry>if (bsArbitraryDownmix) {</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="77pt" align="left"/>
<colspec colname="1" colwidth="140pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>ArbitraryDownmixData( );</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="63pt" align="left"/>
<colspec colname="1" colwidth="154pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>}</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="49pt" align="left"/>
<colspec colname="1" colwidth="168pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>:</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0161" num="0160">Referring to Syntax 3, an unnecessary module may be skipped by using bsSkipSyncflag, which is a flag indicating whether to use a syncword, and bsSkipSyncword, which is a syncword that can be disposed at the end of a module to be skipped.</p>
<p id="p-0162" num="0161">More specifically, if the flag bsSkipSyncflag is set such that a syncword can be used, one or more modules between the flag bsSkipSyncflag and the syncword bsSkipSyncword, i.e., modules 5 mgData( ) and TempShapeData( ), may be skipped.</p>
<p id="p-0163" num="0162">Referring to <figref idref="DRAWINGS">FIG. 11</figref>, a bitstream may include a multi-channel header field which includes header information necessary for reproducing a multi-channel signal, a 3D rendering header field which includes header information necessary for reproducing a 3D down-mix signal, and a plurality of multi-channel decoding information fields, which include data necessary for reproducing a multi-channel signal.</p>
<p id="p-0164" num="0163">In order to reproduce a multi-channel signal, a decoding apparatus may skip the 3D rendering header field, and read data from the multi-channel header field and the multi-channel decoding information fields.</p>
<p id="p-0165" num="0164">A method of skipping the 3D rendering header field is the same as the field skipping methods described above with reference to <figref idref="DRAWINGS">FIG. 10</figref>, and thus, a detailed description thereof will be skipped.</p>
<p id="p-0166" num="0165">In order to reproduce a 3D down-mix signal, a decoding apparatus may read data from the multi-channel decoding information fields and the 3D rendering header field. For example, a decoding apparatus may generate a 3D down-mix signal using a dow n-mix signal included in the multi-channel decoding information field and HRTF coefficient information included in the 3D down-mix signal.</p>
<p id="p-0167" num="0166"><figref idref="DRAWINGS">FIG. 12</figref> is a block diagram of an encoding/decoding apparatus for processing an arbitrary down-mix signal according to an embodiment of the present invention. Referring to <figref idref="DRAWINGS">FIG. 12</figref>, an arbitrary down-mix signal is a down-mix signal other than a down-mix signal generated by a multi-channel encoder <b>801</b> included in an encoding apparatus <b>800</b>. Detailed descriptions of the same processes as those of the embodiment of <figref idref="DRAWINGS">FIG. 1</figref> will be omitted.</p>
<p id="p-0168" num="0167">Referring to <figref idref="DRAWINGS">FIG. 12</figref>, the encoding apparatus <b>800</b> includes the multi-channel encoder <b>801</b>, a spatial information synthesization unit <b>802</b>, and a comparison unit <b>803</b>.</p>
<p id="p-0169" num="0168">The multi-channel encoder <b>801</b> down-mixes an input multi-channel signal into a stereo or mono down-mix signal, and generates basic spatial information necessary for restoring a multi-channel signal from the down-mix signal.</p>
<p id="p-0170" num="0169">The comparison unit <b>803</b> compares the down-mix signal with an arbitrary down-mix signal, and generates compensation information based on the result of the comparison. The compensation information is necessary for compensating for the arbitrary down-mix signal so that the arbitrary down-mix signal can be converted to be approximate to the down-mix signal. A decoding apparatus may compensate for the arbitrary down-mix signal using the compensation information and restore a multi-channel signal using the compensated arbitrary down-mix signal. The restored multi-channel signal is more similar than a multi-channel signal restored from the arbitrary down-mix signal generated by the multi-channel encoder <b>801</b> to the original input multi-channel signal.</p>
<p id="p-0171" num="0170">The compensation information may be a difference between the down-mix signal and the arbitrary down-mix signal. A decoding apparatus may compensate for the arbitrary down-mix signal by adding, to the arbitrary down-mix signal, the difference between the down-mix signal and the arbitrary down-mix signal.</p>
<p id="p-0172" num="0171">The difference between the down-mix signal and the arbitrary down-mix signal may be down-mix gain which indicates the difference between the energy levels of the down-mix signal and the arbitrary down-mix signal.</p>
<p id="p-0173" num="0172">The down-mix gain may be determined for each frequency band, for each time/time slot, and/or for each channel. For example, one part of the down-mix gain may be determined for each frequency band, and another part of the down-mix gain may be determined for each time slot.</p>
<p id="p-0174" num="0173">The down-mix gain may be determined for each parameter band or for each frequency band optimized for the arbitrary down-mix signal. Parameter bands are frequency intervals to which parameter-type spatial information is applied.</p>
<p id="p-0175" num="0174">The difference between the energy levels of the down-mix signal and the arbitrary down-mix signal may be quantized. The resolution of quantization levels for quantizing the difference between the energy levels of the down-mix signal and the arbitrary down-mix signal may be the same as or different from the resolution of quantization levels for quantizing a CLD between the down-mix signal and the arbitrary down-mix signal. In addition, the quantization of the difference between the energy levels of the down-mix signal and the arbitrary down-mix signal may involve the use of all or some of the quantization levels for quantizing the CLD between the down-mix signal and the arbitrary down-mix signal.</p>
<p id="p-0176" num="0175">Since the resolution of the difference between the energy levels of the down-mix signal and the arbitrary down-mix signal is generally lower than the resolution of the CLD between the down-mix signal and the arbitrary down-mix signal, the resolution of the quantization levels for quantizing the difference between the energy levels of the down-mix signal and the arbitrary down-mix signal may have a minute value compared to the resolution of the quantization levels for quantizing the CLD between the down-mix signal and the arbitrary down-mix signal.</p>
<p id="p-0177" num="0176">The compensation information for compensating for the arbitrary down-mix signal may be extension information including residual information which specifies components of the input multi-channel signal that cannot be restored using the arbitrary down-mix signal or the down-mix gain. A decoding apparatus can restore components of the input multi-channel signal that cannot be restored using the arbitrary down-mix signal or the down-mix gain using the extension information, thereby restoring a signal almost indistinguishable from the original input multi-channel signal.</p>
<p id="p-0178" num="0177">Methods of generating the extension information are as follows.</p>
<p id="p-0179" num="0178">The multi-channel encoder <b>801</b> may generate information regarding components of the input multi-channel signal that are lacked by the down-mix signal as first extension information. A decoding apparatus may restore a signal almost indistinguishable from the original input multi-channel signal by applying the first extension information to the generation of a multi-channel signal using the down-mix signal and the basic spatial information.</p>
<p id="p-0180" num="0179">Alternatively, the multi-channel encoder <b>801</b> may restore a multi-channel signal using the down-mix signal and the basic spatial information, and generate the difference between the restored multi-channel signal and the original input multi-channel signal as the first extension information.</p>
<p id="p-0181" num="0180">The comparison unit <b>803</b> may generate, as second extension information, information regarding components of the down-mix signal that are lacked by the arbitrary down-mix signal, i.e., components of the down-mix signal that cannot be compensated for using the down-mix gain. A decoding apparatus may restore a signal almost indistinguishable from the down-mix signal using the arbitrary down-mix signal and the second extension information.</p>
<p id="p-0182" num="0181">The extension information may be generated using various residual coding methods other than the above-described method.</p>
<p id="p-0183" num="0182">The down-mix gain and the extension information may both be used as compensation information. More specifically, the down-mix gain and the extension information may both be obtained for an entire frequency band of the down-mix signal and may be used together as compensation information. Alternatively, the down-mix gain may be used as compensation information for one part of the frequency band of the down-mix signal, and the extension information may be used as compensation information for another part of the frequency band of the down-mix signal. For example, the extension information may be used as compensation information for a low frequency band of the down-mix signal, and the down-mix gain may be used as compensation information for a high frequency band of the down-mix signal.</p>
<p id="p-0184" num="0183">Extension information regarding portions of the down-mix signal, other than the low-frequency band of the down-mix signal, such as peaks or notches that may considerably affect the quality of sound may also be used as compensation information.</p>
<p id="p-0185" num="0184">The spatial information synthesization unit <b>802</b> synthesizes the basic spatial information (e.g., a CLD, CPC, ICC, and CTD) and the compensation information, thereby generating spatial information. In other words, the spatial information, which is transmitted to a decoding apparatus, may include the basic spatial information, the down-mix gain, and the first and second extension information.</p>
<p id="p-0186" num="0185">The spatial information may be included in a bitstream along with the arbitrary down-mix signal, and the bitstream may be transmitted to a decoding apparatus.</p>
<p id="p-0187" num="0186">The extension information and the arbitrary down-mix signal may be encoded using an audio encoding method such as an AAC method, a MP3 method, or a BSAC method. The extension information and the arbitrary down-mix signal may be encoded using the same audio encoding method or different audio encoding methods.</p>
<p id="p-0188" num="0187">If the extension information and the arbitrary down-mix signal are encoded using the same audio encoding method, a decoding apparatus may decode both the extension information and the arbitrary down-mix signal using a single audio decoding method. In this case, since the arbitrary down-mix signal can always be decoded, the extension information can also always be decoded. However, since the arbitrary down-mix signal is generally input to a decoding apparatus as a pulse code modulation (PCM) signal, the type of audio codec used to encode the arbitrary down-mix signal may not be readily identified, and thus, the type of audio codec used to encode the extension information may not also be readily identified.</p>
<p id="p-0189" num="0188">Therefore, audio codec information regarding the type of audio codec used to encode the arbitrary down-mix signal and the extension information may be inserted into a bitstream.</p>
<p id="p-0190" num="0189">More specifically, the audio codec information may be inserted into a specific configuration header field of a bitstream. In this case, a decoding apparatus may extract the audio codec information from the specific configuration header field of the bitstream and use the extracted audio codec information to decode the arbitrary down-mix signal and the extension information.</p>
<p id="p-0191" num="0190">On the other hand, if the arbitrary down-mix signal and the extension information are encoded using different audio encoding methods, the extension information may not be able to be decoded. In this case, since the end of the extension information cannot be identified, no further decoding operation can be performed.</p>
<p id="p-0192" num="0191">In order to address this problem, audio codec information regarding the types of audio codecs respectively used to encode the arbitrary down-mix signal and the extension information may be inserted into a specific configuration header field of a bitstream. Then, a decoding apparatus may read the audio codec information from the specific configuration header field of the bitstream and use the read information to decode the extension information. If the decoding apparatus does not include any decoding unit that can decode the extension information, the decoding of the extension information may not further proceed, and information next to the extension information may be read.</p>
<p id="p-0193" num="0192">Audio codec information regarding the type of audio codec used to encode the extension information may be represented by a syntax element included in a specific configuration header field of a bitstream. For example, the audio codec information may be represented by bsResidualCodecType, which is a 4-bit syntax element, as indicated in Table 1 below.</p>
<p id="p-0194" num="0193">
<tables id="TABLE-US-00004" num="00004">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="147pt" align="center"/>
<colspec colname="2" colwidth="70pt" align="left"/>
<thead>
<row>
<entry namest="1" nameend="2" rowsep="1">TABLE 1</entry>
</row>
<row>
<entry namest="1" nameend="2" align="center" rowsep="1"/>
</row>
<row>
<entry>bsResidualCodecType</entry>
<entry>Codec</entry>
</row>
<row>
<entry namest="1" nameend="2" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry>0</entry>
<entry>AAC</entry>
</row>
<row>
<entry>1</entry>
<entry>MP3</entry>
</row>
<row>
<entry>2</entry>
<entry>BSAC</entry>
</row>
<row>
<entry>3 . . . 15</entry>
<entry>Reserved</entry>
</row>
<row>
<entry namest="1" nameend="2" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0195" num="0194">The extension information may include not only the residual information but also channel expansion information. The channel expansion information is information necessary for expanding a multi-channel signal obtained through decoding using the spatial information into a multi-channel signal with more channels. For example, the channel expansion information may be information necessary for expanding a 5.1-channel signal or a 7.1-channel signal into a 9.1-channel signal.</p>
<p id="p-0196" num="0195">The extension information may be included in a bitstream, and the bitstream may be transmitted to a decoding apparatus. Then, the decoding apparatus may compensate for the down-mix signal or expand a multi-channel signal using the extension information. However, the decoding apparatus may skip the extension information, instead of extracting the extension information from the bitstream. For example, in the case of generating a multi-channel signal using a 3D down-mix signal included in the bitstream or generating a 3D down-mix signal using a down-mix signal included in the bitstream, the decoding apparatus may skip the extension information.</p>
<p id="p-0197" num="0196">A method of skipping the extension information included in a bitstream may be the same as one of the field skipping methods described above with reference to <figref idref="DRAWINGS">FIG. 10</figref>.</p>
<p id="p-0198" num="0197">For example, the extension information may be skipped using at least one of bit size information which is attached to the beginning of a bitstream including the extension information and indicates the size in bits of the extension information, a syncword which is attached to the beginning or the end of the field including the extension information, and fixed bit size information which indicates a fixed size in bits of the extension information. The bit size information, the syncword, and the fixed bit size information may all be included in a bitstream. The fixed bit size information may also be stored in a decoding apparatus.</p>
<p id="p-0199" num="0198">Referring to <figref idref="DRAWINGS">FIG. 12</figref>, a decoding unit <b>810</b> includes a down-mix compensation unit <b>811</b>, a 3D rendering unit <b>815</b>, and a multi-channel decoder <b>816</b>.</p>
<p id="p-0200" num="0199">The down-mix compensation unit <b>811</b> compensates for an arbitrary down-mix signal using compensation information included in spatial information, for example, using down-mix gain or extension information.</p>
<p id="p-0201" num="0200">The 3D rendering unit <b>815</b> generates a decoder 3D down-mix signal by performing a 3D rendering operation on the compensated down-mix signal. The multi-channel decoder <b>816</b> generates a 3D multi-channel signal using the compensated down-mix signal and basic spatial information, which is included in the spatial information.</p>
<p id="p-0202" num="0201">The down-mix compensation unit <b>811</b> may compensate for the arbitrary down-mix signal in the following manner.</p>
<p id="p-0203" num="0202">If the compensation information is down-mix gain, the down-mix compensation unit <b>811</b> compensates for the energy level of the arbitrary down-mix signal using the down-mix gain so that the arbitrary down-mix signal can be converted into a signal similar to a down-mix signal.</p>
<p id="p-0204" num="0203">If the compensation information is second extension information, the down-mix compensation unit <b>811</b> may compensate for components that are lacked by the arbitrary down-mix signal using the second extension information.</p>
<p id="p-0205" num="0204">The multi-channel decoder <b>816</b> may generate a multi-channel signal by sequentially applying pre-matrix M<b>1</b>, mix-matrix M<b>2</b> and post-matrix M<b>3</b> to a down-mix signal. In this case, the second extension information may be used to compensate for the down-mix signal during the application of mix-matrix M<b>2</b> to the down-mix signal. In other words, the second extension information may be used to compensate for a down-mix signal to which pre-matrix M<b>1</b> has already been applied.</p>
<p id="p-0206" num="0205">As described above, each of a plurality of channels may be selectively compensated for by applying the extension information to the generation of a multi-channel signal. For example, if the extension information is applied to a center channel of mix-matrix M<b>2</b>, left- and right-channel components of the down-mix signal may be compensated for by the extension information. If the extension information is applied to a left channel of mix-matrix M<b>2</b>, the left-channel component of the down-mix signal may be compensated for by the extension information.</p>
<p id="p-0207" num="0206">The down-mix gain and the extension information may both be used as the compensation information. For example, a low frequency band of the arbitrary down-mix signal may be compensated for using the extension information, and a high frequency band of the arbitrary down-mix signal may be compensated for using the down-mix gain. In addition, portions of the arbitrary down-mix signal, other than the low frequency band of the arbitrary down-mix signal, for example, peaks or notches that may considerably affect the quality of sound, may also be compensated for using the extension information. Information regarding portion to be compensated for by the extension information may be included in a bitstream. Information indicating whether a down-mix signal included in a bitstream is an arbitrary down-mix signal or not and information indicating whether the bitstream includes compensation information may be included in the bitstream.</p>
<p id="p-0208" num="0207">In order to prevent clipping of a down-mix signal generated by the encoding unit <b>800</b>, the down-mix signal may be divided by predetermined gain. The predetermined gain may have a static value or a dynamic value.</p>
<p id="p-0209" num="0208">The down-mix compensation unit <b>811</b> may restore the original down-mix signal by compensating for the down-mix signal, which is weakened in order to prevent clipping, using the predetermined gain.</p>
<p id="p-0210" num="0209">An arbitrary down-mix signal compensated for by the down-mix compensation unit <b>811</b> can be readily reproduced. Alternatively, an arbitrary down-mix signal yet to be compensated for may be input to the 3D rendering unit <b>815</b>, and may be converted into a decoder 3D down-mix signal by the 3D rendering unit <b>815</b>.</p>
<p id="p-0211" num="0210">Referring to <figref idref="DRAWINGS">FIG. 12</figref>, the down-mix compensation unit <b>811</b> includes a first domain converter <b>812</b>, a compensation processor <b>813</b>, and a second domain converter <b>814</b>.</p>
<p id="p-0212" num="0211">The first domain converter <b>812</b> converts the domain of an arbitrary down-mix signal into a predetermined domain. The compensation processor <b>813</b> compensates for the arbitrary down-mix signal in the predetermined domain, using compensation information, for example, down-mix gain or extension information.</p>
<p id="p-0213" num="0212">The compensation of the arbitrary down-mix signal may be performed in a QMF/hybrid domain. For this, the first domain converter <b>812</b> may perform QMF/hybrid analysis on the arbitrary down-mix signal. The first domain converter <b>812</b> may convert the domain of the arbitrary down-mix signal into a domain, other than a QMF/hybrid domain, for example, a frequency domain such as a DFT or FFT domain. The compensation of the arbitrary down-mix signal may also be performed in a domain, other than a QMF/hybrid domain, for example, a frequency domain or a time domain.</p>
<p id="p-0214" num="0213">The second domain converter <b>814</b> converts the domain of the compensated arbitrary down-mix signal into the same domain as the original arbitrary down-mix signal. More specifically, the second domain converter <b>814</b> converts the domain of the compensated arbitrary down-mix signal into the same domain as the original arbitrary down-mix signal by inversely performing a domain conversion operation performed by the first domain converter <b>812</b>.</p>
<p id="p-0215" num="0214">For example, the second domain converter <b>814</b> may convert the compensated arbitrary down-mix signal into a time-domain signal by performing QMF/hybrid synthesis on the compensated arbitrary down-mix signal. Also, the second domain converter <b>814</b> may perform IDFT or IFFT on the compensated arbitrary down-mix signal.</p>
<p id="p-0216" num="0215">The 3D rendering unit <b>815</b>, like the 3D rendering unit <b>710</b> illustrated in <figref idref="DRAWINGS">FIG. 7</figref>, may perform a 3D rendering operation on the compensated arbitrary down-mix signal in a frequency domain, a QMF/hybrid domain or a time domain. For this, the 3D rendering unit <b>815</b> may include a domain converter (not shown). The domain converter converts the domain of the compensated arbitrary down-mix signal into a domain in which a 3D rendering operation is to be performed or converts the domain of a signal obtained by the 3D rendering operation.</p>
<p id="p-0217" num="0216">The domain in which the compensation processor <b>813</b> compensates for the arbitrary down-mix signal may be the same as or different from the domain in which the 3D rendering unit <b>815</b> performs a 3D rendering operation on the compensated arbitrary down-mix signal.</p>
<p id="p-0218" num="0217"><figref idref="DRAWINGS">FIG. 13</figref> is a block diagram of a down-mix compensation/3D rendering unit <b>820</b> according to an embodiment of the present invention. Referring to <figref idref="DRAWINGS">FIG. 13</figref>, the down-mix compensation/3D rendering unit <b>820</b> includes a first domain converter <b>821</b>, a second domain converter <b>822</b>, a compensation/3D rendering processor <b>823</b>, and a third domain converter <b>824</b>.</p>
<p id="p-0219" num="0218">The down-mix compensation/3D rendering unit <b>820</b> may perform both a compensation operation and a 3D rendering operation on an arbitrary down-mix signal in a single domain, thereby reducing the amount of computation of a decoding apparatus.</p>
<p id="p-0220" num="0219">More specifically, the first domain converter <b>821</b> converts the domain of the arbitrary down-mix signal into a first domain in which a compensation operation and a 3D rendering operation are to be performed. The second domain converter <b>822</b> converts spatial information, including basic spatial information necessary for generating a multi-channel signal and compensation information necessary for compensating for the arbitrary down-mix signal, so that the spatial information can become applicable in the first domain. The compensation information may include at least one of down-mix gain and extension information.</p>
<p id="p-0221" num="0220">For example, the second domain converter <b>822</b> may map compensation information corresponding to a parameter band in a QMF/hybrid domain to a frequency band so that the compensation information can become readily applicable in a frequency domain.</p>
<p id="p-0222" num="0221">The first domain may be a frequency domain such as a DFT or FFT domain, a QMF/hybrid domain, or a time domain. Alternatively, the first domain may be a domain other than those set forth herein.</p>
<p id="p-0223" num="0222">During the conversion of the compensation information, a time delay may occur. In order to address this problem, the second domain converter <b>822</b> may perform a time delay compensation operation so that a time delay between the domain of the compensation information and the first domain can be compensated for.</p>
<p id="p-0224" num="0223">The compensation/3D rendering processor <b>823</b> performs a compensation operation on the arbitrary down-mix signal in the first domain using the converted spatial information and then performs a 3D rendering operation on a signal obtained by the compensation operation. The compensation/3D rendering processor <b>823</b> may perform a compensation operation and a 3D rendering operation in a different order from that set forth herein.</p>
<p id="p-0225" num="0224">The compensation/3D rendering processor <b>823</b> may perform a compensation operation and a 3D rendering operation on the arbitrary down-mix signal at the same time. For example, the compensation/3D rendering processor <b>823</b> may generate a compensated 3D down-mix signal by performing a 3D rendering operation on the arbitrary down-mix signal in the first domain using a new filter coefficient, which is the combination of the compensation information and an existing filter coefficient typically used in a 3D rendering operation.</p>
<p id="p-0226" num="0225">The third domain converter <b>824</b> converts the domain of the 3D down-mix signal generated by the compensation/3D rendering processor <b>823</b> into a frequency domain.</p>
<p id="p-0227" num="0226"><figref idref="DRAWINGS">FIG. 14</figref> is a block diagram of a decoding apparatus <b>900</b> for processing a compatible down-mix signal according to an embodiment of the present invention. Referring to <figref idref="DRAWINGS">FIG. 14</figref>, the decoding apparatus <b>900</b> includes a first multi-channel decoder <b>910</b>, a down-mix compatibility processing unit <b>920</b>, a second multi-channel decoder <b>930</b>, and a 3D rendering unit <b>940</b>. Detailed descriptions of the same decoding processes as those of the embodiment of <figref idref="DRAWINGS">FIG. 1</figref> will be omitted.</p>
<p id="p-0228" num="0227">A compatible down-mix signal is a down-mix signal that can be decoded by two or more multi-channel decoders. In other words, a compatible down-mix signal is a down-mix signal that is initially optimized for a predetermined multi-channel decoder and that can be converted afterwards into a signal optimized for a multi-channel decoder, other than the predetermined multi-channel decoder, through a compatibility processing operation.</p>
<p id="p-0229" num="0228">Referring to <figref idref="DRAWINGS">FIG. 14</figref>, assume that an input compatible down-mix signal is optimized for the first multi-channel decoder <b>910</b>. In order for the second multi-channel decoder <b>930</b> to decode the input compatible down-mix signal, the down-mix compatibility processing unit <b>920</b> may perform a compatibility processing operation on the input compatible down-mix signal so that the input compatible down-mix signal can be converted into a signal optimized for the second multi-channel decoder <b>930</b>. The first multi-channel decoder <b>910</b> generates a first multi-channel signal by decoding the input compatible down-mix signal. The first multi-channel decoder <b>910</b> can generate a multi-channel signal through decoding simply using the input compatible down-mix signal without a requirement of spatial information.</p>
<p id="p-0230" num="0229">The second multi-channel decoder <b>930</b> generates a second multi-channel signal using a down-mix signal obtained by the compatibility processing operation performed by the down-mix compatibility processing unit <b>920</b>. The 3D rendering unit <b>940</b> may generate a decoder 3D down-mix signal by performing a 3D rendering operation on the down-mix signal obtained by the compatibility processing operation performed by the down-mix compatibility processing unit <b>920</b>.</p>
<p id="p-0231" num="0230">A compatible down-mix signal optimized for a predetermined multi-channel decoder may be converted into a down-mix signal optimized for a multi-channel decoder, other than the predetermined multi-channel decoder, using compatibility information such as an inversion matrix. For example, when there are first and second multi-channel encoders using different encoding methods and first and second multi-channel decoders using different encoding/decoding methods, an encoding apparatus may apply a matrix to a down-mix signal generated by the first multi-channel encoder, thereby generating a compatible down-mix signal which is optimized for the second multi-channel decoder. Then, a decoding apparatus may apply an inversion matrix to the compatible down-mix signal generated by the encoding apparatus, thereby generating a compatible down-mix signal which is optimized for the first multi-channel decoder.</p>
<p id="p-0232" num="0231">Referring to <figref idref="DRAWINGS">FIG. 14</figref>, the down-mix compatibility processing unit <b>920</b> may perform a compatibility processing operation on the input compatible down-mix signal using an inversion matrix, thereby generating a down-mix signal which is optimized for the second multi-channel decoder <b>930</b>.</p>
<p id="p-0233" num="0232">Information regarding the inversion matrix used by the down-mix compatibility processing unit <b>920</b> may be stored in the decoding apparatus <b>900</b> in advance or may be included in an input bitstream transmitted by an encoding apparatus. In addition, information indicating whether a down-mix signal included in the input bitstream is an arbitrary down-mix signal or a compatible down-mix signal may be included in the input bitstream.</p>
<p id="p-0234" num="0233">Referring to <figref idref="DRAWINGS">FIG. 14</figref>, the down-mix compatibility processing unit <b>920</b> includes a first domain converter <b>921</b>, a compatibility processor <b>922</b>, and a second domain converter <b>923</b>.</p>
<p id="p-0235" num="0234">The first domain converter <b>921</b> converts the domain of the input compatible down-mix signal into a predetermined domain, and the compatibility processor <b>922</b> performs a compatibility processing operation using compatibility information such as an inversion matrix so that the input compatible down-mix signal in the predetermined domain can be converted into a signal optimized for the second multi-channel decoder <b>930</b>.</p>
<p id="p-0236" num="0235">The compatibility processor <b>922</b> may perform a compatibility processing operation in a QMF/hybrid domain. For this, the first domain converter <b>921</b> may perform QMF/hybrid analysis on the input compatible down-mix signal. Also, the first domain converter <b>921</b> may convert the domain of the input compatible down-mix signal into a domain, other than a QMF/hybrid domain, for example, a frequency domain such as a DFT or FFT domain, and the compatibility processor <b>922</b> may perform the compatibility processing operation in a domain, other than a QMF/hybrid domain, for example, a frequency domain or a time domain.</p>
<p id="p-0237" num="0236">The second domain converter <b>923</b> converts the domain of a compatible down-mix signal obtained by the compatibility processing operation. More specifically, the second domain converter <b>923</b> may convert the domain of the compatibility down-mix signal obtained by the compatibility processing operation into the same domain as the original input compatible down-mix signal by inversely performing a domain conversion operation performed by the first domain converter <b>921</b>.</p>
<p id="p-0238" num="0237">For example, the second domain converter <b>923</b> may convert the compatible down-mix signal obtained by the compatibility processing operation into a time-domain signal by performing QMF/hybrid synthesis on the compatible down-mix signal obtained by the compatibility processing operation. Alternatively, the second domain converter <b>923</b> may perform IDFT or IFFT on the compatible down-mix signal obtained by the compatibility processing operation.</p>
<p id="p-0239" num="0238">The 3D rendering unit <b>940</b> may perform a 3D rendering operation on the compatible down-mix signal obtained by the compatibility processing operation in a frequency domain, a QMF/hybrid domain or a time domain. For this, the 3D rendering unit <b>940</b> may include a domain converter (not shown). The domain converter converts the domain of the input compatible down-mix signal into a domain in which a 3D rendering operation is to be performed or converts the domain of a signal obtained by the 3D rendering operation.</p>
<p id="p-0240" num="0239">The domain in which the compatibility processor <b>922</b> performs a compatibility processing operation may be the same as or different from the domain in which the 3D rendering unit <b>940</b> performs a 3D rendering operation.</p>
<p id="p-0241" num="0240"><figref idref="DRAWINGS">FIG. 15</figref> is a block diagram of a down-mix compatibility processing/3D rendering unit <b>950</b> according to an embodiment of the present invention. Referring to <figref idref="DRAWINGS">FIG. 15</figref>, the down-mix compatibility processing/3D rendering unit <b>950</b> includes a first domain converter <b>951</b>, a second domain converter <b>952</b>, a compatibility/3D rendering processor <b>953</b>, and a third domain converter <b>954</b>.</p>
<p id="p-0242" num="0241">The down-mix compatibility processing/3D rendering unit <b>950</b> performs a compatibility processing operation and a 3D rendering operation in a single domain, thereby reducing the amount of computation of a decoding apparatus.</p>
<p id="p-0243" num="0242">The first domain converter <b>951</b> converts an input compatible down-mix signal into a first domain in which a compatibility processing operation and a 3D rendering operation are to be performed. The second domain converter <b>952</b> converts spatial information and compatibility information, for example, an inversion matrix, so that the spatial information and the compatibility information can become applicable in the first domain.</p>
<p id="p-0244" num="0243">For example, the second domain converter <b>952</b> maps an inversion matrix corresponding to a parameter band in a QMF/hybrid domain to a frequency domain so that the inversion matrix can become readily applicable in a frequency domain.</p>
<p id="p-0245" num="0244">The first domain may be a frequency domain such as a DFT or FFT domain, a QMF/hybrid domain, or a time domain. Alternatively, the first domain may be a domain other than those set forth herein.</p>
<p id="p-0246" num="0245">During the conversion of the spatial information and the compatibility information, a time delay may occur. In order to address this problem,</p>
<p id="p-0247" num="0246">In order to address this problem, the second domain converter <b>952</b> may perform a time delay compensation operation so that a time delay between the domain of the spatial information and the compensation information and the first domain can be compensated for.</p>
<p id="p-0248" num="0247">The compatibility/3D rendering processor <b>953</b> performs a compatibility processing operation on the input compatible down-mix signal in the first domain using the converted compatibility information and then performs a 3D rendering operation on a compatible down-mix signal obtained by the compatibility processing operation. The compatibility/3D rendering processor <b>953</b> may perform a compatibility processing operation and a 3D rendering operation in a different order from that set forth herein.</p>
<p id="p-0249" num="0248">The compatibility/3D rendering processor <b>953</b> may perform a compatibility processing operation and a 3D rendering operation on the input compatible down-mix signal at the same time. For example, the compatibility/3D rendering processor <b>953</b> may generate a 3D down-mix signal by performing a 3D rendering operation on the input compatible down-mix signal in the first domain using a new filter coefficient, which is the combination of the compatibility information and an existing filter coefficient typically used in a 3D rendering operation.</p>
<p id="p-0250" num="0249">The third domain converter <b>954</b> converts the domain of the 3D down-mix signal generated by the compatibility/3D rendering processor <b>953</b> into a frequency domain.</p>
<p id="p-0251" num="0250"><figref idref="DRAWINGS">FIG. 16</figref> is a block diagram of a decoding apparatus for canceling crosstalk according to an embodiment of the present invention. Referring to <figref idref="DRAWINGS">FIG. 16</figref>, the decoding apparatus includes a bit unpacking unit <b>960</b>, a down-mix decoder <b>970</b>, a 3D rendering unit <b>980</b>, and a crosstalk cancellation unit <b>990</b>. Detailed descriptions of the same decoding processes as those of the embodiment of <figref idref="DRAWINGS">FIG. 1</figref> will be omitted.</p>
<p id="p-0252" num="0251">A 3D down-mix signal output by the 3D rendering unit <b>980</b> may be reproduced by a headphone. However, when the 3D down-mix signal is reproduced by speakers that are distant apart from a user, inter-channel crosstalk is likely to occur.</p>
<p id="p-0253" num="0252">Therefore, the decoding apparatus may include the crosstalk cancellation unit <b>990</b> which performs a crosstalk cancellation operation on the 3D down-mix signal.</p>
<p id="p-0254" num="0253">The decoding apparatus may perform a sound field processing operation.</p>
<p id="p-0255" num="0254">Sound field information used in the sound field processing operation, i.e., information identifying a space in which the 3D down-mix signal is to be reproduced, may be included in an input bitstream transmitted by an encoding apparatus or may be selected by the decoding apparatus.</p>
<p id="p-0256" num="0255">The input bitstream may include reverberation time information. A filter used in the sound field processing operation may be controlled according to the reverberation time information.</p>
<p id="p-0257" num="0256">A sound field processing operation may be performed differently for an early part and a late reverberation part. For example, the early part may be processed using a FIR filter, and the late reverberation part may be processed using an IIR filter.</p>
<p id="p-0258" num="0257">More specifically, a sound field processing operation may be performed on the early part by performing a convolution operation in a time domain using an FIR filter or by performing a multiplication operation in a frequency domain and converting the result of the multiplication operation to a time domain. A sound field processing operation may be performed on the late reverberation part in a time domain.</p>
<p id="p-0259" num="0258">The present invention can be realized as computer-readable code written on a computer-readable recording medium. The computer-readable recording medium may be any type of recording device in which data is stored in a computer-readable manner. Examples of the computer-readable recording medium include a ROM, a RAM, a CD-ROM, a magnetic tape, a floppy disc, an optical data storage, and a carrier wave (e.g., data transmission through the Internet). The computer-readable recording medium can be distributed over a plurality of computer systems connected to a network so that computer-readable code is written thereto and executed therefrom in a decentralized manner. Functional programs, code, and code segments needed for realizing the present invention can be easily construed by one of ordinary skill in the art.</p>
<p id="p-0260" num="0259">As described above, according to the present invention, it is possible to efficiently encode multi-channel signals with 3D effects and to adaptively restore and reproduce audio signals with optimum sound quality according to the characteristics of a reproduction environment.</p>
<heading id="h-0009" level="1">INDUSTRIAL APPLICABILITY</heading>
<p id="p-0261" num="0260">Other implementations are within the scope of the following claims. For example, grouping, data coding, and entropy coding according to the present invention can be applied to various application fields and various products. Storage media storing data to which an aspect of the present invention is applied are within the scope of the present invention.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method of decoding a signal, comprising:
<claim-text>receiving a down-mix signal and a plurality of spatial information regarding a plurality of channels;</claim-text>
<claim-text>extracting down-mix identification information specifying whether the down-mix signal is a signal obtained by performing a three-dimensional (3D) rendering operation from the received plurality of spatial information;</claim-text>
<claim-text>performing a correction for at least one of the plurality of spatial information using spatial information adjacent to the at least one spatial information, wherein the correction comprises replacing at least one of first spatial information corresponding to a first parameter band and second spatial information corresponding to a second parameter band with the average of the first spatial information and the second spatial information for suppressing aliasing emerging in borders between the first and second parameter bands, the first and second parameter bands being adjacent to each other;</claim-text>
<claim-text>removing a 3D effect from the down-mix signal by performing an inverse 3D rendering operation based on the down-mix identification information; and,</claim-text>
<claim-text>generating a multi-channel signal using the at least one of the plurality of spatial information and the 3D effect removed down-mix signal.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the correction comprises replacing at least one of adjacent spatial information with the average of adjacent spatial information.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the correction comprises replacing at least one of first spatial information corresponding to a first frame and second spatial information corresponding to a second frame with the average of the first spatial information and the second spatial information, the first and second frames being adjacent to each other.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. A non-transitory computer-readable recording medium containing computer instructions stored therein for causing a computer processor to execute the decoding method of any one of <claim-ref idref="CLM-00001">claims 1</claim-ref> through <b>3</b>.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. An apparatus for decoding a signal, comprising:
<claim-text>a bit unpacking unit receiving a down-mix signal and a plurality of spatial information regarding a plurality of channels, and down-mix identification information specifying whether the down-mix signal is a signal obtained by performing a three-dimensional (3D) rendering operation;</claim-text>
<claim-text>a spatial information correction unit performing a correction for at least one of the plurality of spatial information using spatial information adjacent to the at least one spatial information, wherein the correction comprises replacing at least one of first spatial information corresponding to a first parameter band and second spatial information corresponding to a second parameter band with the average of the first spatial information and the second spatial information for suppressing aliasing emerging in borders between the first and second parameter bands, the first and second parameter bands being adjacent to each other;</claim-text>
<claim-text>an inverse 3D rendering unit configured to remove a 3D effect from the down-mix signal by performing an inverse 3D rendering operation based on the down-mix identification information; and</claim-text>
<claim-text>a multi-channel decoder generating a multi-channel signal using the at least one of the spatial information and the 3D effect removed down-mix signal.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The apparatus of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the spatial information correction unit replaces at least one of adjacent spatial information with the average of adjacent spatial information.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The apparatus of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the spatial information correction unit replaces at least one of first spatial information corresponding to a first frame and second spatial information corresponding to a second frame with the average of the first spatial information and the second spatial information, the first and second frames being adjacent to each other.</claim-text>
</claim>
</claims>
</us-patent-grant>
