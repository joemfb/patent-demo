<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626489-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626489</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12754180</doc-number>
<date>20100405</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>KR</country>
<doc-number>10-2009-0076556</doc-number>
<date>20090819</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>431</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>17</main-group>
<subgroup>27</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>704  9</main-classification>
<further-classification>704  7</further-classification>
<further-classification>704 10</further-classification>
<further-classification>704257</further-classification>
</classification-national>
<invention-title id="d2e71">Method and apparatus for processing data</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5676551</doc-number>
<kind>A</kind>
<name>Knight et al.</name>
<date>19971000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>434236</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5860064</doc-number>
<kind>A</kind>
<name>Henton</name>
<date>19990100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704260</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>2003/0028380</doc-number>
<kind>A1</kind>
<name>Freeland et al.</name>
<date>20030200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704260</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>2004/0199923</doc-number>
<kind>A1</kind>
<name>Russek</name>
<date>20041000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>719310</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>2004/0267816</doc-number>
<kind>A1</kind>
<name>Russek</name>
<date>20041200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>7071041</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2007/0087798</doc-number>
<kind>A1</kind>
<name>McGucken</name>
<date>20070400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>463  1</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2008/0306995</doc-number>
<kind>A1</kind>
<name>Newell et al.</name>
<date>20081200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>7071041</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2009/0248399</doc-number>
<kind>A1</kind>
<name>Au</name>
<date>20091000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704  9</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2010/0167819</doc-number>
<kind>A1</kind>
<name>Schell</name>
<date>20100700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>463 36</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>JP</country>
<doc-number>05-100692</doc-number>
<date>19930400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>JP</country>
<doc-number>07-104778</doc-number>
<date>19950400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>JP</country>
<doc-number>2003-233388</doc-number>
<date>20030800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>JP</country>
<doc-number>2003-271174</doc-number>
<date>20030900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>JP</country>
<doc-number>2003-302992</doc-number>
<date>20031000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>JP</country>
<doc-number>2005-181840</doc-number>
<date>20050700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>JP</country>
<doc-number>2006-010845</doc-number>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>JP</country>
<doc-number>2007-183421</doc-number>
<date>20070700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>KR</country>
<doc-number>10-2008-0060909</doc-number>
<date>20080700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>20</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>None</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>9</number-of-drawing-sheets>
<number-of-figures>9</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20110046943</doc-number>
<kind>A1</kind>
<date>20110224</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Lee</last-name>
<first-name>Dong Yeol</first-name>
<address>
<city>Suwon-si</city>
<country>KR</country>
</address>
</addressbook>
<residence>
<country>KR</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Park</last-name>
<first-name>Seung Seop</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
<residence>
<country>KR</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Ahn</last-name>
<first-name>Jae Hyun</first-name>
<address>
<city>Suwon-si</city>
<country>KR</country>
</address>
</addressbook>
<residence>
<country>KR</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Lee</last-name>
<first-name>Dong Yeol</first-name>
<address>
<city>Suwon-si</city>
<country>KR</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Park</last-name>
<first-name>Seung Seop</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Ahn</last-name>
<first-name>Jae Hyun</first-name>
<address>
<city>Suwon-si</city>
<country>KR</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>NSIP Law</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Samsung Electronics Co., Ltd.</orgname>
<role>03</role>
<address>
<city>Suwon-si</city>
<country>KR</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Saint Cyr</last-name>
<first-name>Leonard</first-name>
<department>2658</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A data processing method and apparatus that may set emotion based on development of a story are provided. The method and apparatus may set emotion without inputting emotion for each sentence of text data. Emotion setting information is generated based on development of the story and the like, and may be applied to the text data.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="128.10mm" wi="203.12mm" file="US08626489-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="108.88mm" wi="127.25mm" file="US08626489-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="140.72mm" wi="134.79mm" file="US08626489-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="128.78mm" wi="132.00mm" file="US08626489-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="215.90mm" wi="122.51mm" orientation="landscape" file="US08626489-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="202.78mm" wi="144.10mm" orientation="landscape" file="US08626489-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="131.57mm" wi="133.86mm" file="US08626489-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="133.86mm" wi="134.45mm" file="US08626489-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="144.53mm" wi="132.33mm" file="US08626489-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="215.73mm" wi="152.57mm" orientation="landscape" file="US08626489-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION(S)</heading>
<p id="p-0002" num="0001">This application claims the benefit under 35 U.S.C. &#xa7;119(a) of Korean Patent Application No. 10-2009-0076556, filed on Aug. 19, 2009, in the Korean Intellectual Property Office, the entire disclosure of which is incorporated herein by reference for all purposes.</p>
<heading id="h-0002" level="1">BACKGROUND</heading>
<p id="p-0003" num="0002">1. Field</p>
<p id="p-0004" num="0003">The following description relates to a data processing method and apparatus, and more particularly, to a text data processing method and apparatus that may be used for natural text-to-speech (TTS) expression.</p>
<p id="p-0005" num="0004">2. Description of Related Art</p>
<p id="p-0006" num="0005">Speech synthesis is commonly utilized in a variety of devices, for example, E-books, robots, animation user created contents (UCC), MPEG-1 Audio Layer 3 (MP3) players, and the like. Speech synthesis may be used in a variety of functions, for example, reading a title of a song or a name of a singer based on a human voice in a MP3 player, a function of speaking by a character, a human voice in a real time animation, a function of reading a book in the E-book, and the like.</p>
<p id="p-0007" num="0006">A device for providing speech synthesis may output a waveform by inputting a text to be spoken. A speech synthesis that expresses emotion may be performed by designating a parameter, such as a type of emotion to be expressed, a speed of reading, a pitch of reading, and the like. Based on these parameters, various emotions, such as delight, sadness, anger, and the like, may be expressible.</p>
<p id="p-0008" num="0007">Generally, expressing emotion for a plurality of successive sentences using speech synthesis may be possible when contents of the sentences are not excessive. However, when excessive contents are included, or when it is desired to promptly express emotion, a large amount of time may be expended to process text data. Also, when a text-to-speech is performed, it may be difficult to provide an appropriate expression for a situation based on only emotion information.</p>
<heading id="h-0003" level="1">SUMMARY</heading>
<p id="p-0009" num="0008">In one aspect, provided is a method for processing text data, the method including receiving text data including a story, generating a profile with respect to at least one of a character in the story and a narrator in the story, and generating emotion information corresponding to the story, the emotion information relating to at least one of a profile with respect to a character in the story, a profile with respect to a narrator in the story, emotion change information of a character in the story, and tension information of the story.</p>
<p id="p-0010" num="0009">The profile with respect to the character may include at least one of a character name, a characteristic of the character, a speech style, a relationship with another character, a speed of speech, an intensity of speech, and a pitch of speech, and the profile with respect to the narrator may include at least one of a narrator name, a speech style, a degree of applying a tension of the story, a speed of speech, an intensity of speech, and a pitch of speech.</p>
<p id="p-0011" num="0010">The generating of the emotion information may include dividing the story based on a classifying standard, generating speaker information with respect to the text data based on the divided story, and generating at least one of conversation background information corresponding to each division of the story, emotion change information of the character, and tension information of the story.</p>
<p id="p-0012" num="0011">The classifying standard may be chapter information included in the text data or a classifying standard inputted by a user.</p>
<p id="p-0013" num="0012">The generating of the speaker information may include analyzing a sentence included in the text data, and generating speaker information based on the analyzed sentence.</p>
<p id="p-0014" num="0013">The method may further include mapping emotion information corresponding to the story to the text data and storing the mapped information.</p>
<p id="p-0015" num="0014">In another aspect, provided is an apparatus for storing text data, the apparatus including a text data storing unit to store text data including a story, a profile storing unit to store a profile with respect to at least one of a character in the story and a narrator in the story, and an emotion information storing unit to store emotion information corresponding to the story, the emotion information relating to at least one of a profile with respect to a character in the story, a profile with respect to a narrator in the story, emotion change information of a character in the story, and tension information of the story.</p>
<p id="p-0016" num="0015">The profile with respect to the character may include at least one of a character name, a characteristic of the character, a speech style, a relationship with another character, a speed of speech, an intensity of speech, and a pitch of speech, and the profile with respect to the narrator may include at least one of a narrator name, a speech style, a degree of applying a tension of the story, a speed of speech, an intensity of speech, and a pitch of speech.</p>
<p id="p-0017" num="0016">The text data including the story may be divided based on a classifying standard, and the emotion information storing unit may store at least one of conversation background information corresponding to each division of the text data, emotion change information of a character, and tension information of the story.</p>
<p id="p-0018" num="0017">The classifying standard may be at least one of chapter information included in the text data, a change in a situation where the story is developed, a conversation background in the story, an emotion change of a character, a change in scene of the story, and depth information set to divide the story.</p>
<p id="p-0019" num="0018">In another aspect, provided is a method for processing text data, the method including loading text data including a story, reading emotion information corresponding to the story, and generating speech setting information to perform a text-to-speech (TTS) transformation based on the emotion information, the emotion information relating to at least one of a profile with respect to a character in the story, a profile with respect to a narrator in the story, emotion change information of a character in the story, tension information of the story, and conversation background information in the story.</p>
<p id="p-0020" num="0019">When the speaker is a narrator, the reading of the emotion information may include reading the emotion information based on a sentence unit of the text data or a speaker unit, and extracting, from the emotion information, tension information of a current point in time.</p>
<p id="p-0021" num="0020">When the character is a speaker, the reading of the emotion information may include reading the emotion information based on a sentence unit of the text data or a speaker unit, and extracting, from the emotion change information of the character, emotion information of the character of a current point in time.</p>
<p id="p-0022" num="0021">The generating of the speech setting information may include setting background music information corresponding to tension information of the story; and setting acoustic effect information corresponding to conversation background information in the story.</p>
<p id="p-0023" num="0022">The method may further include transforming the text data into speech based on the speech setting information.</p>
<p id="p-0024" num="0023">In another aspect, provided is an apparatus for processing text data, the apparatus including a text data loading unit to load text data including a story, an emotion information reading unit to read emotion information corresponding to the story, a speech setting information generating unit to generate speech setting information to perform a TTS transformation based on the emotion information, and a TTS transforming unit to transform the text data to speech based on the speech setting information.</p>
<p id="p-0025" num="0024">The emotion information may be at least one of a profile with respect to a character in the story, a profile with respect to a narrator in the story, emotion change information of a character in the story, tension information of the story, and conversation background information in the story.</p>
<p id="p-0026" num="0025">The profile with respect to the character may include at least one of a character name, a characteristic of the character, a speech style, a relationship of another character, a speed of speech, an intensity of speech, and a pitch of speech, and the profile with respect to the narrator may include at least one of a narrator name, a speech style, a degree of applying a tension of a story, a speed of speech, an intensity of speech, and a pitch of speech.</p>
<p id="p-0027" num="0026">The speech setting information may include at least one of background music information corresponding to the tension information of the story or acoustic effect information corresponding to conversation background information in the story.</p>
<p id="p-0028" num="0027">The speech setting information may include extracting tension information of a current point in time based on a sentence unit of the text data or a speaker unit, and generating speech setting information of the current point in time, from the tension information of the current point in time, wherein, when the tension information of the current point in time does not exist, generating speech setting information of the current point in time based on tension information of a past point in time or a future point in time.</p>
<p id="p-0029" num="0028">The speech setting information generating unit may generate speech setting information of a current point in time from emotion change information of the character.</p>
<p id="p-0030" num="0029">Other features and aspects will be apparent from the following description, the drawings, and the claims.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 1</figref> is a diagram illustrating an example of a text processing apparatus.</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 2</figref> is a flowchart illustrating an example of a text data processing method.</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 3</figref> is a flowchart illustrating an example of a method for generating emotion information.</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 4</figref> is a diagram illustrating an example of a process of dividing a story.</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 5</figref> is a diagram illustrating an example text data processing method.</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 6</figref> is a diagram illustrating an example of a storing apparatus that may be included in the text processing apparatus of <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. 7</figref> is a diagram illustrating an example of a text data processing apparatus that may be included in the text processing apparatus of <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. 8</figref> is a flowchart illustrating an example of a text data processing method.</p>
<p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. 9</figref> is a diagram illustrating an example of an operation for generating speech to setting information.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<p id="p-0040" num="0039">Throughout the drawings and the description, unless otherwise described, the same drawing reference numerals will be understood to refer to the same elements, features, and structures. The relative size and depiction of these elements may be exaggerated for clarity, illustration, and convenience.</p>
<heading id="h-0005" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0041" num="0040">The following description is provided to assist the reader in gaining a comprehensive understanding of the methods, apparatuses, and/or systems described herein. Accordingly, various changes, modifications, and equivalents of the methods, apparatuses, and/or systems described herein will be suggested to those of ordinary skill in the art. Also, descriptions of well-known functions and constructions may be omitted for increased clarity and conciseness.</p>
<p id="p-0042" num="0041">The following description may be applicable to a service of reading contents via a terminal capable of accessing the Internet, for example, an E-book, and MP3 player, a mobile terminal, an animation including a natural emotion expression, and the like.</p>
<p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. 1</figref> illustrates an example of a text processing apparatus. Referring to <figref idref="DRAWINGS">FIG. 1</figref>, the example text processing apparatus includes a text data processing apparatus <b>110</b>, a text data processing apparatus including a text-to-speech (TTS) system <b>120</b>, also referred to as TTS system <b>120</b>, and a storing apparatus <b>130</b>.</p>
<p id="p-0044" num="0043">The text data processing apparatus <b>110</b> receives text data. The text data may include various data, for example, a story. The text data processing apparatus <b>110</b> may generate emotion information corresponding to the story. The emotion information and the text data may be stored in a storing apparatus <b>130</b>.</p>
<p id="p-0045" num="0044">The TTS system <b>120</b> includes a text-to-speech (TTS) system to perform a TTS transformation of text data to which emotion information is set. For example, the TTS system <b>120</b> may perform TTS transformation of text data stored in the storing apparatus or may perform TTS transformation by receiving, from the text data processing apparatus <b>110</b>, the text data to which the emotion information is set. The TTS system <b>120</b> may be a program module or a hardware device that is capable of performing the TTS transformation through a TTS engine.</p>
<p id="p-0046" num="0045">The text data processing apparatus <b>110</b> may perform a function of a service, and the text data processing apparatus including the TTS system <b>120</b> may perform a function of a client or a device.</p>
<p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. 2</figref> illustrates a flowchart of an example text data processing method. The method may be performed, for example, by the text data processing apparatus <b>110</b> of <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0048" num="0047">In <b>210</b>, the text data processing apparatus <b>110</b> receives text data. The data may include, for example, a story. The text data processing apparatus <b>110</b> may include a processor to process text data in <b>210</b>. The text data including the story may be inputted by a user. In some embodiments, the text data may be stored in the storing apparatus <b>130</b>.</p>
<p id="p-0049" num="0048">In <b>220</b>, the text data processing apparatus <b>110</b> generates a profile, for example, a profile with respect to a character in the story and/or a narrator in the story. For example, the generation of the profile may be a transformation of information inputted by the user into a form recognized by a computer or the processor.</p>
<p id="p-0050" num="0049">The narrator profile may include, for example, basic information with respect to the narrator that may be used for telling and reading the story. The narrator profile may include, for example, at least one of a name of the narrator, a speech style, a degree of applying a tension of the story, a speed of speech, an intensity of speech, a pitch of speaking, and the like.</p>
<p id="p-0051" num="0050">The character profile may, include, for example, basic information with respect to a character which is needed for reading the story. When the story includes a plurality of characters, the character profile may be generated for one or more the characters, for example all of the characters. The character profile may, include, for example, at least one of a name of a character, a characteristic of the character, a speech style, a relationship with another character, a speed of speech, an intensity of speech, a pitch of speech, and the like.</p>
<p id="p-0052" num="0051">Table 1 illustrates an example of contents of an example narrator profile.</p>
<p id="p-0053" num="0052">
<tables id="TABLE-US-00001" num="00001">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="left"/>
<thead>
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry>Contents of narrator profile</entry>
</row>
<row>
<entry>name: a name of a narrator</entry>
</row>
<row>
<entry>speech style: being set by defining various styles, such as a descriptive</entry>
</row>
<row>
<entry>style, a conversation style, a documentary style, and the like, and being</entry>
</row>
<row>
<entry>changed based on a function of a TTS system</entry>
</row>
<row>
<entry>setting a degree of applying a tension: a degree of a change in a pitch of</entry>
</row>
<row>
<entry>the narrator is set to a level 1, a level 2, . . . and a level n, based on the</entry>
</row>
<row>
<entry>tension of contents of a story</entry>
</row>
<row>
<entry>speed of reading and intensity of reading: speed level 1, 2, and 3, for</entry>
</row>
<row>
<entry>example, strong tone, medium tone, and light tone</entry>
</row>
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0054" num="0053">Table 2 illustrates an example of contents of an example character profile.</p>
<p id="p-0055" num="0054">
<tables id="TABLE-US-00002" num="00002">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="left"/>
<thead>
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry>Contents of character profile</entry>
</row>
<row>
<entry>name: a name of a character</entry>
</row>
<row>
<entry>characteristic: a characteristic in a story is selected, and the selection is</entry>
</row>
<row>
<entry>based on emotion theory</entry>
</row>
<row>
<entry>a degree of reflecting emotion: a degree of expressing emotion by speech</entry>
</row>
<row>
<entry>is set, as emotion is changed</entry>
</row>
<row>
<entry>speech style: mainly a conversation style, being set by defining various</entry>
</row>
<row>
<entry>styles depending on the characteristic or tendency, and being changed</entry>
</row>
<row>
<entry>based on a function of a TTS system</entry>
</row>
<row>
<entry>intimacy or relationship between characters: setting a relationship with</entry>
</row>
<row>
<entry>another character to which a profile is set</entry>
</row>
<row>
<entry>tendency of treating people: setting a conversation style of a conversation</entry>
</row>
<row>
<entry>with a general character to which a relationship is not set</entry>
</row>
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0056" num="0055">In <b>230</b>, the text data processing apparatus <b>110</b> generates emotion information corresponding to the story. For example, the emotion information may relate to at least one of a profile with respect to the character, a profile with respect to the narrator, emotion change information of the character, tension information of the story, and the like.</p>
<p id="p-0057" num="0056">In <b>240</b>, the text data processing apparatus <b>110</b> maps the emotion information corresponding to the story to the text data and may store the mapped information.</p>
<p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. 3</figref> illustrates an example flowchart for generating emotion information, for example, the operation of generating emotion information shown in <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0059" num="0058">In <b>331</b>, the text data processing apparatus <b>110</b> may divide a story into divisions based on a classifying standard. For example, a division of the story may include at least one of a situation of a story, a background of the story, a scene of a story, a chapter of a story, and the like. In some embodiments, the division of the story may be based on a TTS transformation, and may occur when a TTS transformation is performed. The atmosphere and image of each scene may change as the story is developed. Accordingly, when the story is divided based on the situation of the story, emotion appropriate for each scene may be expressed.</p>
<p id="p-0060" num="0059">For example, the text data processing apparatus <b>110</b> may divide the story based on chapter information included in text data or a classifying standard inputted by a user. The text data processing apparatus <b>110</b> may automatically divide the story based on a chapter section of the text. The text data processing apparatus <b>110</b> may divide the story based on the classifying standard inputted by the user, and may generate identification information to classify each divided story. For example, the classifying standard may include, for example, at least one of a change in a situation where the story is developed, a conversation background in the story, a change in emotion of the character, depth information set for dividing the story, and the like. The identification information to classify a divided story may be generated to be corresponding to a described classifying standard.</p>
<p id="p-0061" num="0060">In <b>333</b>, the text data processing apparatus <b>110</b> may generate speaker information with respect to the text data. For example, the generation of the speaker information may be transformation of information inputted by the user into a form recognizable to a computer or a processor.</p>
<p id="p-0062" num="0061">The text data processing apparatus <b>110</b> may analyze a sentence included in the text data, and may generate speaker information corresponding to each sentence. For example, the text data processing apparatus <b>110</b> may designate a speaker with respect all text as narrator or a character. The text data processing apparatus <b>110</b> may designate a speaker with respect to some text as a narrator and a speaker with respect to other text as a narrator. In some embodiments, the text data processing apparatus may exclude a sentence discriminated by using quotation marks (&#x201c; &#x201d;).</p>
<p id="p-0063" num="0062">In <b>335</b>, the text data processing apparatus <b>110</b> may generate at least one of conversation background information corresponding to a divided story, emotion change information of the character, and tension information of the story. For example, the generation of the information may be transformation of information inputted by the user into a form recognizable to the computer or the processor.</p>
<p id="p-0064" num="0063">The conversation background information may be utilized for a natural speech when a TTS transformation is performed. For example, a TTS transformation system may express a conversation in a quiet library, a conversation in a theater, or a conversation in a noisy road, by referring to the conversation background information. The above described background information is provided merely for example, and is not meant to limit the different background information that may be utilized. In some embodiments, the conversation background information may give an effect on setting of a background acoustic when the TTS transformation is performed.</p>
<p id="p-0065" num="0064">The tension information may give an effect on speech of the narrator when the TTS transformation is performed. For example, when a tension level is set based on a development of a situation of the story, a speech style of the narrator or an intensity of the speech may be changed during the TTS transformation, based on the tension level set. The tension information may be applied to the entire story or to a portion of the story. For example, the tension information may be set with respect to a portion where a tension increases. The tension information may set a change of a tension over time, and may be designated with respect to contents of a particular text.</p>
<p id="p-0066" num="0065">The tension information may not need to be designated with respect to all text. For example, when an initial value and a last value of a particular section are designated, a value between the initial value and the last value does not need to be designated. Instead, a value may be generated based on an interpolated value. The interpolated value may be obtained, for example, by interpolating a tension value for text based upon the initial value and the last value.</p>
<p id="p-0067" num="0066">The emotion change information of a character is information about how to express speech of the character during the TTS transformation. The emotion change information of the character may be applied to a portion of text. In some embodiments, a character profile value may be applied to a portion of text that does not include the emotion change information of the character, during the TTS transformation.</p>
<p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. 4</figref> illustrates an example of a process of dividing a story. Referring to <figref idref="DRAWINGS">FIG. 4</figref>, an example of dividing a story based on one or more depths is shown. In this example, depth <b>1</b> may discriminate the story based on text data itself, such as a chapter.</p>
<p id="p-0069" num="0068">Depth <b>2</b> is an example of dividing the story based on a conversation background of the story. In this example, the text data processing apparatus <b>110</b> may divide the story based on conversation background information inputted by a user, and may generate an identifier to identify a conversation background of each divided story. For example, the identifier to identify the conversation background may be used for expressing a natural speech in the TTS system.</p>
<p id="p-0070" num="0069">Depth <b>3</b> is an example of dividing the text again in a specific conversation background. In this example, the text data processing apparatus <b>110</b> may divide the story based on a depth <b>3</b> classifying standard, and may generate an identifier to identify a depth <b>3</b> section of each division of the divided story. The identifier to identify the depth <b>3</b> section may be used for expressing a natural speech in the TTS system.</p>
<p id="p-0071" num="0070"><figref idref="DRAWINGS">FIG. 5</figref> illustrates an example of a text data processing method. The text data processing apparatus <b>110</b> may generate and store emotion information <b>510</b> of text data, speech setting information <b>520</b>, and sound data <b>530</b>. The speech setting information <b>520</b> and the sound data <b>530</b> may be generated and stored by the text data processing apparatus (including the TTS system) <b>120</b>.</p>
<p id="p-0072" num="0071">The emotion information <b>510</b> of the text data may include at least one of speaker information of the text data, conversation background information, emotion change information of the character, and tension information of the story. The emotion information <b>510</b> of the text data may be used as identifiers to discriminate each division of a story, a profile of the character, and a profile of the narrator. In some embodiments, the emotion information <b>510</b> of the text data may be corrected by the user.</p>
<p id="p-0073" num="0072">The speech setting information <b>520</b> may include at least one of background music setting data or speech setting data. The speech setting data may include, for example, speaker information matched to each sentence of the text data. Also, the speech setting data may include a parameter to apply the emotion information <b>510</b> of the text data to a TTS system <b>120</b>.</p>
<p id="p-0074" num="0073">The sound data <b>530</b> may be waveform data, and the speech setting information <b>520</b> may be outputted from the TTS system <b>120</b>.</p>
<p id="p-0075" num="0074"><figref idref="DRAWINGS">FIG. 6</figref> illustrates an example of a storing apparatus that may be used in the text processing apparatus of <figref idref="DRAWINGS">FIG. 1</figref>. The storing apparatus <b>130</b> may be a portable storage medium. In some embodiments, the storing apparatus <b>130</b> may be included in the text data processing apparatus <b>110</b> or the text data processing apparatus <b>120</b>. In the example shown in <figref idref="DRAWINGS">FIG. 6</figref>, the storing apparatus <b>130</b> includes a text data storing unit <b>631</b>, a profile storing unit <b>633</b>, emotion information storing unit <b>635</b>, and a data managing unit <b>637</b>.</p>
<p id="p-0076" num="0075">The text data storing unit <b>631</b> stores text data including a story. The text data may be stored in a divided condition based on a classifying standard. For example, the classifying standard may be at least one of chapter information included in the text data, a change in a situation where the story is developed, a conversation background of the story, an emotion change of the character, depth information set for dividing the story, a chapter, a scene, and the like.</p>
<p id="p-0077" num="0076">The profile storing unit <b>633</b> stores profile information, for example, a profile of at least one character in the story, a profile of at least one narrator in the story, or a combination thereof. The profile of the character may include, for example, at least one a character name, a characteristic of the character, a speech style, a relationship with another character, a speed of speech, an intensity of speech, a pitch of speech, and the like. The profile of the narrator may include, for example, at least one of a narrator name, a speech style, a degree of applying tension of the story, a speed of speech, an intensity of speech, a pitch of speech, and the like.</p>
<p id="p-0078" num="0077">The emotion information storing unit <b>635</b> stores emotion information corresponding to the story. The emotion information may relate to, for example, at least one of the profile of a character, the profile of a narrator, emotion change information of the character, tension information of the story, and the like. The emotion information storing unit <b>635</b> may store, for example, at least one of the conversation background information corresponding to the divided text data, the emotion change information of the character, the tension information of the story, and the like.</p>
<p id="p-0079" num="0078">The data managing unit <b>637</b> may be connected with an external device, and may perform a controlling operation to input/output data stored in the storing apparatus <b>130</b>.</p>
<p id="p-0080" num="0079"><figref idref="DRAWINGS">FIG. 7</figref> illustrates an example of a text data processing apparatus that may be used in the text processing apparatus of <figref idref="DRAWINGS">FIG. 1</figref>. The example text data processing apparatus <b>120</b> may include a text data loading unit <b>721</b>, a speech setting information generating unit <b>723</b>, an emotion information reading unit <b>725</b>, and a TTS transforming unit <b>727</b>. The TTS transforming unit <b>727</b> may be connected with a sound outputting apparatus, such as a speaker, and the like.</p>
<p id="p-0081" num="0080">The text data loading unit <b>721</b> loads text data including a story. The speech setting information generating unit <b>723</b> generates speech setting information for TTS transformation, based on emotion information. For example, the speech setting information may include at least one of background music information corresponding to the tension information of the story and acoustic effect information corresponding to the conversation background information of the story.</p>
<p id="p-0082" num="0081">The speech setting information generating unit <b>723</b> may extract tension information of a current point in a time based on a sentence unit of the text data or based on a speaker unit, and may generate speech setting information of the current point in time from the tension information of the current point in time. When the tension information of the current point in time does not exist, the speech setting information of the current point in time may be generated based on tension information of a past point in time and/or a future point in time. For example, the speech setting information generating unit <b>723</b> may generate the tension information of the current point in time as a level <b>3</b> when the tension information of the past point in time is level <b>2</b>. In this example, the &#x201c;current point in time&#x201d; indicates a sentence currently inputted to the speech setting information generating unit <b>723</b> or text data currently inputted to the speech setting information generating unit <b>723</b>.</p>
<p id="p-0083" num="0082">The speech setting information generating unit <b>723</b> may generate speech setting information of the current point in time from emotion change information of a character. For example, the emotion change information of the character may be classified into &#x201c;angry,&#x201d; &#x201c;grumpy,&#x201d; &#x201c;happy,&#x201d; &#x201c;normal,&#x201d; and the like, and may be classified based on a numeric value. When the emotion change information of the character does not exist, the speech setting information generating unit <b>723</b> may generate the speech setting information of the current point in time based on emotion change information of the character with respect to a past point in time and/or a future point in time.</p>
<p id="p-0084" num="0083">The emotion information reading unit <b>725</b> reads emotion information corresponding to the story. For example, the emotion information may include at least one of a profile with respect to the character in the story, a profile with respect to a narrator in the story, emotion change information of the character, tension information of the story, conversation background information of the story, and the like.</p>
<p id="p-0085" num="0084">TTS transforming unit <b>727</b> may transform the text data into speech based on speech setting information.</p>
<p id="p-0086" num="0085"><figref idref="DRAWINGS">FIG. 8</figref> illustrates an example of a text data processing method. The text data processing method of <figref idref="DRAWINGS">FIG. 8</figref> may be performed by a text data processing apparatus including a processor or a text data processing apparatus that includes a TTS system.</p>
<p id="p-0087" num="0086">In <b>810</b>, the text data processing apparatus performs loading text data including a story. In <b>810</b>, the text data processing apparatus may check an environment or a performance of the TTS system to determine, for example, a number of speakers that the TTS system is able to support, a degree of emotion expression that the TTS system is able to support, an acoustic effect that the TTS system is able to support, and the like. The text data processing apparatus may generate speech setting information based on the environment or performance of the TTS system. For example, when performance of the TTS system supports only one narrator, all narration may be spoken by the one narrator even though two or more narrator profiles exist.</p>
<p id="p-0088" num="0087">In some embodiments, a user of the text data processing apparatus may change or select a speaker as desired by the user. For example, when two characters are in the story, the user or the text data processing apparatus may designate a first character as a speaker A and may designate a second character as a speaker B. The number of characters in the story may be known by each character profile.</p>
<p id="p-0089" num="0088">In <b>820</b>, the text data processing apparatus may read emotion information corresponding to the story. For example, the text data processing apparatus may read the emotion information based on a sentence unit of text data or a speaker unit, and may extract tension information of a current point in time from the emotion information, when the speaker is the narrator.</p>
<p id="p-0090" num="0089">For example, the text data processing apparatus may extract emotion information of a character of the current point in time, from the emotion change information of the character, when the speaker is the character.</p>
<p id="p-0091" num="0090">In <b>830</b>, the text data processing apparatus may generate the speech setting information for TTS transformation based on the emotion information. The text data processing apparatus may interpolate prior tension information and posterior tension information to generate tension information with respect to a current text, when tension information set on the current text does not exist. The tension information may be used as the speech setting information to perform speech of the current text according to &#x201c;a degree of applying tension&#x201d; defined in the narrator profile.</p>
<p id="p-0092" num="0091">In this example, the text data processing apparatus may extract emotion information of a character with respect to a current point in time from the emotion change information of the character, when the speaker is the character. The text data processing apparatus may generate the speech setting information to perform speech of the current text using at least one of emotion of the character at the current point in time, a numeric value of emotion, conversation background information, and the like.</p>
<p id="p-0093" num="0092">When the current text does not include a set emotion, the text data processing apparatus may generate the speech setting information using a basic conversation tendency and/or an intimacy and/or relationship with another character set in the character profile. In some embodiments, the speech setting information may be generated using a currently set emotion and a numeric value, when the current text includes a set emotion. When the numeric value does not exist, a prior emotion value and a posterior emotion value may be interpolated and the speech setting information may be generated based on the interpolation. For example, the interpolation may be performed using a degree of a change in the emotion according to a situation, and a characteristic of the character.</p>
<p id="p-0094" num="0093">The text data processing apparatus may generate various speech setting information of the character according to the conversation background information of the current point in time. For example, when the conversation speech setting information of the current point in time is a library, the text data processing apparatus may generate the speech setting information to output speech of the character quietly.</p>
<p id="p-0095" num="0094">In <b>830</b>, the text data processing apparatus may set background music information corresponding to tension information of the story, and may set acoustic effect corresponding to conversation background information of the story.</p>
<p id="p-0096" num="0095">In <b>840</b>, the text data processing apparatus may transform text data into speech based on the speech setting information.</p>
<p id="p-0097" num="0096"><figref idref="DRAWINGS">FIG. 9</figref> illustrates an example of an operation of generating speech setting information. <figref idref="DRAWINGS">FIG. 9</figref> illustrates text data, a parameter indicating a speaker, a parameter indicating a conversation background, a parameter indicating a character emotion change, and a parameter indicating a tension of a story.</p>
<p id="p-0098" num="0097">Referring to <figref idref="DRAWINGS">FIG. 9</figref>, the parameter indicating the speaker is shown for each sentence. The parameter indicating the conversation background may be set to, for example, &#x201c;house&#x201d;. Accordingly, a text data processing apparatus may output acoustic effect corresponding to the &#x201c;house&#x201d;, as a sound.</p>
<p id="p-0099" num="0098">The emotion change of a character may include, for example, emotions that include &#x201c;alarmed,&#x201d; &#x201c;complaining,&#x201d; &#x201c;angry,&#x201d; &#x201c;normal,&#x201d; and the like. In this example, the emotion change of character A is changed from &#x201c;alarmed, level <b>2</b>&#x201d; to &#x201c;alarmed, level <b>3</b>.&#x201d; Accordingly, the text data processing apparatus may distinguish between &#x201c;alarmed, level <b>2</b>&#x201d; and &#x201c;alarmed, level <b>3</b>,&#x201d; and may output speech differently based upon the different level.</p>
<p id="p-0100" num="0099">In this example, the parameter indicating the tension is expressed with a numeric value. Accordingly, the text data processing apparatus may output speech of a narrator differently based on the numeric value of the parameter indicating the tension.</p>
<p id="p-0101" num="0100">Also, a user may correct a character profile or a narrator profile, and may operate a TTS system as desired by the user. That is, the user may change a degree of applying a tension of the narrator profile, and thus, may freely change a speech effect of the story. Also, the user may change emotion change information of the character, and thus, may change speech of the character as desired by the user.</p>
<p id="p-0102" num="0101">Emotion setting information may be generated based on the development of a story and the like, when emotion is applied to the text data, and thus, emotion may be easily and quickly applied to the text data.</p>
<p id="p-0103" num="0102">Text-to-speech (TTS) is naturally embodied based on various emotion information. A parameter for TTS transformation may be stored based on a background of a story, a change in an emotion of a character, a change in a tension of the story, and the like. Accordingly, a small amount of data may be stored compared with a case of storing a parameter for each sentence.</p>
<p id="p-0104" num="0103">The processes, functions, methods and/or software according to the above-described examples may be recorded, stored, or fixed in one or more computer-readable storage media that includes program instructions to be implemented by a computer to cause a processor to execute or perform the program instructions. The media may also include, alone or in combination with the program instructions, data files, data structures, and the like. The media and program instructions may be those specially designed and constructed, or they may be of the kind well-known and available to those having skill in the computer software arts. Examples of computer-readable storage media include magnetic media such as hard disks, floppy disks, and magnetic tape; optical media such as CD ROM disks and DVDs; magneto-optical media, such as optical disks; and hardware devices that are specially configured to store and perform program instructions, such as read-only memory (ROM), random access memory (RAM), flash memory, and the like. Examples of program instructions include both machine code, such as produced by a compiler, and files containing higher level code that may be executed by the computer using an interpreter. The described hardware devices may be configured to act as one or more software modules in order to perform the operations of the above-described example embodiments, or vice versa. In addition, a computer-readable storage medium may be distributed among computer systems connected through a network and computer-readable codes or program instructions may be stored and executed in a decentralized manner.</p>
<p id="p-0105" num="0104">A number of examples have been described above. Nevertheless, it will be understood that various modifications may be made. For example, suitable results may be achieved if the described techniques are performed in a different order and/or if components in a described system, architecture, device, or circuit are combined in a different manner and/or replaced or supplemented by other components or their equivalents. Accordingly, other implementations are within the scope of the following claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method for processing text data, the method comprising:
<claim-text>receiving, at a processor, text data including a story;</claim-text>
<claim-text>generating, at the processor, a character profile with respect to a character in the story;</claim-text>
<claim-text>generating, at the processor, a narrator profile with respect to a narrator in the story; and</claim-text>
<claim-text>generating, at the processor, emotion information corresponding to development of a situation of the story, the emotion information relating to the character profile, the narrator profile, emotion change information of a character in the story, and tension information of the story;</claim-text>
<claim-text>wherein the character profile includes a character name, a characteristic of the character, a speech style being changed based on a function of a text-to-speech (TTS) system, a relationship with another character, a speed of speech, an intensity of speech, and a pitch of speech, and</claim-text>
<claim-text>wherein the narrator profile includes a narrator name, a speech style being changed based on a function of a text-to-speech (TTS) system, a degree of applying a tension of the story, a speed of speech, an intensity of speech, and a pitch of speech.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the generating of the emotion information comprises:
<claim-text>dividing the story based on a classifying standard;</claim-text>
<claim-text>generating speaker information with respect to the text data based on the divided story; and</claim-text>
<claim-text>generating at least one of conversation background information corresponding to each division of the story, emotion change information of the character, and tension information of the story.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the classifying standard is chapter information included in the text data or a classifying standard inputted by a user.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the generating of the speaker information comprises:
<claim-text>analyzing a sentence included in the text data; and</claim-text>
<claim-text>generating speaker information based on the analyzed sentence.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>mapping emotion information corresponding to the story to the text data and storing the mapped information.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. An apparatus for storing text data, the apparatus comprising:
<claim-text>a text data storing unit configured to store text data including a story;</claim-text>
<claim-text>a character profile storing unit configured to store a character profile with respect to a character in the story and a narrator profile with respect to a narrator in the story; and</claim-text>
<claim-text>an emotion information storing unit configured to store emotion information corresponding to development of a situation of the story, the emotion information relating to the character profile, the narrator profile, emotion change information of a character in the story, and tension information of the story;</claim-text>
<claim-text>wherein the character profile includes a character name, a characteristic of the character, a speech style being changed based on a function of a text-to-speech (TTS) system, a relationship with another character, a speed of speech, an intensity of speech, and a pitch of speech, and</claim-text>
<claim-text>wherein the narrator profile includes a narrator name, a speech style being changed based on a function of a text-to-speech (TTS) system, a degree of applying a tension of the story, a speed of speech, an intensity of speech, and a pitch of speech.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The apparatus of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the text data including the story is divided based on a classifying standard, and the emotion information storing unit stores at least one of conversation background information corresponding to each division of the text data, emotion change information of a character, and tension information of the story.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The apparatus of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the classifying standard is at least one of chapter information included in the text data, a change in a situation where the story is developed, a conversation background in the story, an emotion change of a character, a change in scene of the story, and depth information set to divide the story.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. A method for processing text data, the method comprising:
<claim-text>loading, at a processor, text data including a story;</claim-text>
<claim-text>reading, at the processor, emotion information corresponding to development of a situation of the story; and</claim-text>
<claim-text>generating, at the processor, speech setting information to perform a text-to-speech (TTS) transformation based on the emotion information, the emotion information relating to a character profile with respect to a character in the story, a narrator profile with respect to a narrator in the story, emotion change information of a character in the story, tension information of the story, and conversation background information in the story;</claim-text>
<claim-text>wherein the character profile includes a character name, a characteristic of the character, a speech style being changed based on a function of a text-to-speech (TTS) system, a relationship with another character, a speed of speech, an intensity of speech, and a pitch of speech, and</claim-text>
<claim-text>wherein the narrator profile includes a narrator name, a speech style being changed based on a function of a text-to-speech (TTS) system, a degree of applying a tension of the story, a speed of speech, an intensity of speech, and a pitch of speech.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein, when the speaker is a narrator, the reading of the emotion information comprises:
<claim-text>reading the emotion information based on a sentence unit of the text data or a speaker unit; and</claim-text>
<claim-text>extracting, from the emotion information, tension information of a current point in time.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein, when the character is a speaker, the reading of the emotion information comprises:
<claim-text>reading the emotion information based on a sentence unit of the text data or a speaker unit; and</claim-text>
<claim-text>extracting, from the emotion change information of the character, emotion information of the character of a current point in time.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the generating of the speech setting information comprises:
<claim-text>setting background music information corresponding to tension information of the story; and</claim-text>
<claim-text>setting acoustic effect information corresponding to conversation background information in the story.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising:
<claim-text>transforming the text data into speech based on the speech setting information.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. An apparatus for processing text data, the apparatus comprising:
<claim-text>a text data loading unit configured to load text data including a story;</claim-text>
<claim-text>an emotion information reading unit configured to read emotion information corresponding to development of a situation of the story;</claim-text>
<claim-text>a speech setting information generating unit configured to generate speech setting information to perform a text-to-speech (TTS) transformation based on the emotion information; and</claim-text>
<claim-text>a TTS transforming unit configured to transform the text data to speech based on the speech setting information,</claim-text>
<claim-text>wherein the emotion information corresponding to a character profile with respect to a character in the story, a narrator profile with respect to a narrator in the story, emotion change information of a character in the story, tension information of the story, and conversation background information in the story;</claim-text>
<claim-text>wherein the character profile includes a character name, a characteristic of the character, a speech style being changed based on a function of a text-to-speech (TTS) system, a relationship with another character, a speed of speech, an intensity of speech, and a pitch of speech, and</claim-text>
<claim-text>wherein the narrator profile includes a narrator name, a speech style being changed based on a function of a text-to-speech (TTS) system, a degree of applying a tension of the story, a speed of speech, an intensity of speech, and a pitch of speech.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The apparatus of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the profile with respect to the character includes at least one of a character name, a characteristic of the character, a speech style, a relationship of another character, a speed of speech, an intensity of speech, and a pitch of speech, and the profile with respect to the narrator includes at least one of a narrator name, a speech style, a degree of applying a tension of a story, a speed of speech, an intensity of speech, and a pitch of speech.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The apparatus of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the speech setting information includes at least one of background music information corresponding to the tension information of the story or acoustic effect information corresponding to conversation background information in the story.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The apparatus of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the speech setting information comprises:
<claim-text>extracting tension information of a current point in time based on a sentence unit of the text data or a speaker unit; and</claim-text>
<claim-text>generating speech setting information of the current point in time, from the tension information of the current point in time,</claim-text>
<claim-text>wherein, when the tension information of the current point in time does not exist, generating speech setting information of the current point in time based on tension information of a past point in time or a future point in time.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The apparatus of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the speech setting information generating unit generates speech setting information of a current point in time from emotion change information of the character.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. A non-transitory computer program embodied on a computer readable storage medium, the computer program being configured to control a processor to perform:
<claim-text>receiving text data including a story;</claim-text>
<claim-text>generating a character profile with respect to a character in the story;</claim-text>
<claim-text>generating, at the processor, a narrator profile with respect to a narrator in the story; and</claim-text>
<claim-text>generating emotion information corresponding to the story, the emotion information relating to the character profile, the narrator profile, emotion change information of a character in the story, and tension information of the story;</claim-text>
<claim-text>wherein the character profile includes a character name, a characteristic of the character, a speech style being changed based on a function of a text-to-speech (TTS) system, a relationship with another character, a speed of speech, an intensity of speech, and a pitch of speech, and</claim-text>
<claim-text>wherein the narrator profile includes a narrator name, a speech style being changed based on a function of a text-to-speech (TTS) system, a degree of applying a tension of the story, a speed of speech, an intensity of speech, and a pitch of speech.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. A non-transitory computer program embodied on a computer readable storage medium, the computer program being configured to control a processor to perform:
<claim-text>loading text data including a story;</claim-text>
<claim-text>reading emotion information corresponding to the story; and</claim-text>
<claim-text>generating speech setting information to perform a text-to-speech (TTS) transformation based on the emotion information, the emotion information relating to a character profile with respect to a character in the story, a narrator profile with respect to a narrator in the story, emotion change information of a character in the story, tension information of the story, and conversation background information in the story;</claim-text>
<claim-text>wherein the character profile includes a character name, a characteristic of the character, a speech style being changed based on a function of a text-to-speech (TTS) system, a relationship with another character, a speed of speech, an intensity of speech, and a pitch of speech, and</claim-text>
<claim-text>wherein the narrator profile includes a narrator name, a speech style being changed based on a function of a text-to-speech (TTS) system, a degree of applying a tension of the story, a speed of speech, an intensity of speech, and a pitch of speech.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
