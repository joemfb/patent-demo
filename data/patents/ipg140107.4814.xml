<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625907-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625907</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12797778</doc-number>
<date>20100610</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<us-term-of-grant>
<us-term-extension>439</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>62</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>68</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>70</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>36</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382225</main-classification>
<further-classification>382226</further-classification>
<further-classification>382235</further-classification>
</classification-national>
<invention-title id="d2e53">Image clustering</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5983218</doc-number>
<kind>A</kind>
<name>Syeda-Mahmood</name>
<date>19991100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>8229219</doc-number>
<kind>B1</kind>
<name>Ioffe</name>
<date>20120700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382168</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>2007/0005556</doc-number>
<kind>A1</kind>
<name>Ganti</name>
<date>20070100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>2007/0174269</doc-number>
<kind>A1</kind>
<name>Jing</name>
<date>20070700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>2008/0292196</doc-number>
<kind>A1</kind>
<name>Jain</name>
<date>20081100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2009/0034791</doc-number>
<kind>A1</kind>
<name>Doretto et al.</name>
<date>20090200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2009/0161962</doc-number>
<kind>A1</kind>
<name>Gallagher et al.</name>
<date>20090600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382203</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2010/0268714</doc-number>
<kind>A1</kind>
<name>Moon et al.</name>
<date>20101000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>707737</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>WO</country>
<doc-number>WO2010021625</doc-number>
<kind>A1</kind>
<date>20100200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00010">
<othercit>Chum et al, &#x201c;Large-Scale Discovery of Spatially Related Images&#x201d;, 2009, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 32, No. 2, pp. 371-377.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00011">
<othercit>Sivic et al, &#x201c;Video Google: A Text Retrieval Approach to Object Matching in Videos,&#x201d; 2003, Proceedings of the Ninth IEEE International Conference on Computer Vision, pp. 1-8.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00012">
<othercit>Mukherjee et al, &#x201c;Systematic Temperature Sensor Allocation and Placement for Microprocessors,&#x201d; 2006, DAC 2006, pp. 1-6.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00013">
<othercit>Ondfej Chum et al, &#x201c;Near Duplicate Image Detection: min-Hash and tf-idf Weighting,&#x201d; 2008, BMVC, pp. 1-10.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00014">
<othercit>O. Chum, J. Philbin, J. Sivic, M. Isard, and A. Zisserman, &#x201c;Total Recall: Automatic Query Expansion with a Generative Feature Model for Object Retrieval,&#x201d; Proc. IEEE Int'l Conf. Computer Vision, 2007, pp. 1-8.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00015">
<othercit>Kennedy et al.; Generating Diverse and Representative Image Search Results for Landmarks; Published Date: 2008; In WWW '08: Proceeding of the 17th international conference on World Wide Web (2008), pp. 297-306.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00016">
<othercit>Chum et al.; Web Scale Image Clustering; Published Date: May 23, 2008.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00017">
<othercit>Moosmann et al.; Randomized Clustering Forests for Image Classification; Published Date: Sep. 2008; Pattern Analysis and Machine Intelligence, IEEE Transactions on Issue Date: Sep. 2008.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00018">
<othercit>Barnard et al.; Clustering Art; Retrieved Date: Mar. 29, 2010.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00019">
<othercit>Blei et al.; Latent dirichlet allocation; J. Mach. Learn. Res., 3:993-1022,2003.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00020">
<othercit>Chum et al.; Scalable near identical image and shot detection; In CIVR '07: Proceedings of the 6th ACM international conference on Image and video retrieval, pp. 549-556, New; York, NY, USA, 2007. ACM.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00021">
<othercit>Das et al.; Google news personalization: scalable online collaborative filtering; In WWW '07: Proceedings of the 16th international conference on World Wide Web, pp. 271-280, New York, NY; USA, 2007. ACM.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00022">
<othercit>Dean et al.; Mapreduce: Simplified data processing on large clusters; pp. 137-150, 2004.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00023">
<othercit>M. I. et al.; Dryad: Distributed data-parallel programs from sequential building blocks. 2007.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00024">
<othercit>T. Hofmann; Probabilistic latent semantic indexing; In SIGIR'99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in; information retrieval, pp. 50-57, New York, NY, USA, 1999. ACM.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00025">
<othercit>Joly et al.; Robust content-based video copy identification in a large reference database; Image and Video Retrieval, 9(2):511-516, Feb. 2003.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00026">
<othercit>Joly et al.; Content-based copy retrieval using distortion-based probabilistic similarity search; Multimedia, IEEE Transactions on, 9(2):293-306, Feb. 2007.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00027">
<othercit>Nister et al.; Scalable recognition with a vocabulary tree. In CVPR '06: Proceedings of the 2006 IEEE; Computer Society Conference on Computer Vision and Pattern; Recognition, pp. 2161-2168, Washington, DC,; USA, 2006. IEEE Computer Society.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00028">
<othercit>Quack et al.: a system for large-scale, content-based web image retrieval; In Multimedia '04: Proceedings of the 12th annual ACM international conference on Multimedia, pp. 508-511, New York, NY, USA, 2004. ACM.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00029">
<othercit>I. S. S. M. S. Agarwal et al.; Building rome in a day; ICCV 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00030">
<othercit>Sivic et al.; Video google: A text retrieval approach to object matching in videos; In ICCVV '03:;Proceedings of the Ninth IEEE International Conference on Computer Vision; IEEE Computer Society, 2003.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00031">
<othercit>Torralba et al.; Small codes and large image databases for recognition; In Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, pp. 1-8, Jun. 2008.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00032">
<othercit>Zhong et al.;Bundling features for large scale partial-duplicate web image search; In CVPR '09: Proceedings of the 2009 IEEE Computer Society Conference on Computer Vision and Pattern; Recognition, 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>20</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>None</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>6</number-of-drawing-sheets>
<number-of-figures>6</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20110305399</doc-number>
<kind>A1</kind>
<date>20111215</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Zitnick</last-name>
<first-name>Charles</first-name>
<address>
<city>Seattle</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Sagula</last-name>
<first-name>Rafael</first-name>
<address>
<city>Kirkland</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Chandrashekar</last-name>
<first-name>Ashok</first-name>
<address>
<city>Hanover</city>
<state>NH</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Zitnick</last-name>
<first-name>Charles</first-name>
<address>
<city>Seattle</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Sagula</last-name>
<first-name>Rafael</first-name>
<address>
<city>Kirkland</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Chandrashekar</last-name>
<first-name>Ashok</first-name>
<address>
<city>Hanover</city>
<state>NH</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Microsoft Corporation</orgname>
<role>02</role>
<address>
<city>Redmond</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Werner</last-name>
<first-name>Brian P</first-name>
<department>2665</department>
</primary-examiner>
<assistant-examiner>
<last-name>Dunphy</last-name>
<first-name>David F</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A database of images may be accessed. A feature set may be computed for each image, respectively. Each feature set includes feature integers quantized from interest points of a corresponding image. An initial set of clusters of the feature sets is found based on min hashes of the feature sets. Given the clusters of feature sets, descriptors for each of the clusters are computed, respectively, by selecting feature integers from among the feature sets in a cluster. The clusters are then refined by comparing at least some of the feature sets with at least some of the cluster descriptors, and based on such comparing adding some of the feature sets to clusters whose feature descriptors have similarity to the feature sets.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="231.73mm" wi="147.66mm" file="US08625907-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="218.52mm" wi="158.75mm" file="US08625907-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="247.65mm" wi="175.34mm" file="US08625907-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="87.29mm" wi="111.84mm" file="US08625907-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="249.68mm" wi="149.44mm" file="US08625907-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="243.50mm" wi="146.64mm" file="US08625907-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="122.09mm" wi="77.05mm" file="US08625907-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND</heading>
<p id="p-0002" num="0001">Large image collections may be used for many tasks such as computer vision, geometry reconstruction, data mining, and so on. Despite the effectiveness of some algorithms that may perform such tasks, improved results can be obtained by relying on the density of image data in such collections. The web on the whole has billions of images and is an ideal resource for such endeavors. Recent algorithms have performed similarity based retrieval on large image collections. While this is useful, further benefits can be derived by discovering the underlying structure of an image collection through computationally demanding tasks such as unsupervised appearance-based image clustering. Applications of clustering include improved image search relevance, facilitation of content filtering, and generating useful web image statistics. Also, by caching image clusters, significant runtime savings can be obtained for various applications. However, large scale clustering is challenging both in terms of accuracy and computational cost.</p>
<p id="p-0003" num="0002">Traditional algorithms for data clustering do not scale well for large image collections. In particular, iterative algorithms (e.g., k-means, hierarchical clustering) and probabilistic models exhibit poor scaling. Further, some traditional algorithms may need to determine the number of clusters, which is difficult in large collections. The scale of an image dataset may also lead to a preference for certain platforms. In particular, datacenter platforms and programming frameworks like Map-Reduce and Dryad Linq may be desirable, yet iterative algorithms may not adapt well to such platforms.</p>
<p id="p-0004" num="0003">Scalable techniques for image clustering are discussed below.</p>
<heading id="h-0002" level="1">SUMMARY</heading>
<p id="p-0005" num="0004">The following summary is included only to introduce some concepts discussed in the Detailed Description below. This summary is not comprehensive and is not intended to delineate the scope of the claimed subject matter, which is set forth by the claims presented at the end.</p>
<p id="p-0006" num="0005">A database of images may be accessed, and a feature set may be computed for each image, respectively. Each feature set includes feature integers quantized from interest points of a corresponding image. An initial set of clusters of the feature sets is found based on min hashes of the feature sets. Given the clusters of feature sets, descriptors for each of the clusters are computed, respectively, by selecting feature integers from among the feature sets in a cluster. The clusters are then refined by comparing at least some of the feature sets with at least some of the cluster descriptors, and based on such comparing adding some of the feature sets to clusters whose feature descriptors have similarity to the feature sets.</p>
<p id="p-0007" num="0006">Many of the attendant features will be explained below with reference to the following detailed description considered in connection with the accompanying drawings.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0008" num="0007">The present description will be better understood from the following detailed description read in light of the accompanying drawings, wherein like reference numerals are used to designate like parts in the accompanying description.</p>
<p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. 1</figref> shows a system for collecting images and computing features of the images.</p>
<p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. 2</figref> shows a process for forming image clusters.</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 3</figref> shows a process by which image features sets may be analyzed to form clusters.</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 4</figref> shows a first clustering stage process.</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 5</figref> shows a second stage of image clustering.</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 6</figref> shows an example computer which may be used to implement embodiments and features discussed herein.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION</heading>
<heading id="h-0005" level="1">Overview</heading>
<p id="p-0015" num="0014">Embodiments discussed below relate to clustering images. This description will begin with an overview of how a large scale image collection, and features of images therein, may be obtained. A general clustering approach will be described next, followed by detailed description of various clustering stages.</p>
<p id="h-0006" num="0000">Collection of Images and &#x201c;Bag of Features&#x201d; Analysis</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 1</figref> shows a system for collecting images and computing features of the images. While a collection (dataset) or database of images <b>100</b> may be obtained in any way, large collections may be readily obtained using available crawling technology. A web crawler <b>102</b> may crawl the Internet to collect images <b>104</b> from web pages, databases, from within documents or content of web pages, from databases, message feeds, and so on. A feature extractor <b>106</b> may then use any of a variety or combination of known feature analysis techniques to perform a feature extraction process <b>108</b>. Specifically, for each crawled image, the feature extractor <b>106</b> may find some number (for example, 80) of image features or points of interest <b>110</b> of an image <b>104</b>. For example, Harris corner detection, Laplacian peaks, SIFT (gradient orientation histogram), SPIN (rotation invariant histogram of intensities), RIFT (rotation invariant version of SIFT), and so on, may be used alone or in combination to find image features or points of interest <b>110</b>.</p>
<p id="p-0017" num="0016">Each such found point of interest <b>110</b> may then be represented by a descriptor, for example, an array of floating point numbers (e.g., from 32 to 144 floats) computed from the point of interest <b>110</b>. Each descriptor of the current image may then be quantized to a word of an image-vocabulary dictionary <b>111</b>, which may be on the order of 1,000,000 image-data &#x201c;words&#x201d; (features). Such dictionaries are often referred to as a &#x201c;bag of features&#x201d; or &#x201c;bag of words&#x201d;. Techniques for computing such a dictionary are described elsewhere. In any event, these quantized words or features of an image will be referred to, for discussion, as &#x201c;integer features&#x201d;. In one embodiment, numerous points of interest may be found (e.g., hundreds) for an image, but only the top N (e.g. 80) points may be used. The top N might be those that are closest to a word in the dictionary <b>111</b> or those with strongest interest point responses, etc.</p>
<p id="p-0018" num="0017">An image descriptor mentioned in the paragraph above may be quantized to an integer feature (a word in the dictionary) using a kd-tree, hierarchal k-means techniques, or other known techniques. That is, a data structure may be used to map the image descriptor to a word in the dictionary that is closest or most similar to the image descriptor.</p>
<p id="p-0019" num="0018">The collection of thus-derived integer features for an image form a final feature set <b>112</b> for the image (e.g., &#x2dc;80 integers each ranging from 1 to 1,000,000). Each such image <b>104</b> and its corresponding feature set <b>112</b> are stored in association with each other in the image database <b>100</b>. As will be explained, the feature sets <b>112</b> will be used for clustering the images <b>104</b>.</p>
<p id="h-0007" num="0000">General Clustering Approach</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 2</figref> shows a process for forming image clusters <b>128</b>. Each cluster <b>128</b> will include images that are computationally similar and which, with varying results, will have similar appearance. One or more computers (described below with reference to <figref idref="DRAWINGS">FIG. 6</figref>) will access <b>130</b> the images in the image database <b>100</b>. More particularly, the feature sets of the images will be accessed and used as proxies for the images themselves. The image feature sets <b>112</b> will be analyzed <b>132</b> to form image clusters <b>128</b>, and the image clusters <b>128</b> are stored or outputted <b>134</b> for practical use by search engines, imaging applications, geometry reconstruction, statistical analysis, and so on.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 3</figref> shows a process by which image features sets may be analyzed <b>132</b> to form clusters <b>128</b>. A first clustering stage <b>152</b> may form rough or imprecise image clusters (a first set of clusters) using any techniques that are sufficiently fast for the number of images in the image database <b>100</b>. Embodiments using Min Hash techniques will be described with reference to <figref idref="DRAWINGS">FIG. 4</figref>. A second clustering stage <b>154</b> may then be performed on the first set of clusters. This may involve boosting or improving recall of clusters in the first set of clusters by moving images into better matching clusters. As will be described in reference to <figref idref="DRAWINGS">FIG. 5</figref>, this second clustering stage may involve comparing each image feature set to the features of each cluster. That is, image feature sets may be compared against cluster feature sets (features from various of the image feature sets in a cluster).</p>
<p id="h-0008" num="0000">First Stage</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 4</figref> shows a first clustering stage process. Note that an image and its corresponding image feature set will be referred to interchangeably; a feature set represents its image. For each image in the image database <b>100</b> (or a queried set of images therefrom), a hash computation <b>170</b> is performed, using, for example, Min Hash techniques. This involves starting <b>172</b> with the feature set for the current image for which hash computation <b>170</b> is being performed. The integers in the current feature set are randomized <b>174</b>, that is, the integer features are mapped to an array of random integers (random words in the visual dictionary). Using small integers as examples (actual values may range to 1,000,000 in one embodiment) If the current feature set is {58, 10, 102, 34, . . . , F<sub>80</sub>}, then the Min Hash takes the 1,000,000 possible integers (the dictionary) and permutes them into a random order (while a 1 to 1 random mapping is desirable, randomized mapping is sufficient). For example, the randomization {110, 52, 64, 31, . . . } might be produced. The min hash value is found <b>176</b> by taking the minimum in the randomized set that maps to an integer in the current feature set, in this case, 31. In one embodiment, the forming of a randomized <b>174</b> set and finding <b>176</b> a minimum hash value thereof may be repeated (e.g., 3 times), and the results concatenated <b>178</b> to form an output hash value (often referred to as a sketch).</p>
<p id="p-0023" num="0022">By way of explanation, the probability of two min hashes being the same is the same as the Jaccard distance between the two sets of number (the two feature sets). In other words, the probability that two hash values are the same is approximately equal to the intersection of the corresponding features over the union of those features, and the probability of the hashes colliding is the probability of the original numbers being the same. However, a single hash may not be sufficiently unique; the probability that the two features sets will have a single feature in common may be too high. To make a hash more unique, a sketch is formed, which is a concatenation of several min hashes. If only one min hash value is used, then too many images may collide. With a sketch, there may be greater confidence that the corresponding images are the same or similar, as there will be a greater number of common features. If the probability of each one of these min hashes colliding is X, then the probability of a match is x<sup>K </sup>(where K is the number of hashes concatenated, e.g., 3) so the probability of a collision colliding is less. What may be done, then, is for every image, some predetermined number of sketches (e.g., 25) may be generated, each with a size 3 (3 concatenated min hashes).</p>
<p id="p-0024" num="0023">Images that share the same sketches can be efficiently found by sorting the images based on each of their computed sketches. Images that share a sketch will neighbor each other in the sorted list. The sketches for each image may be sorted, and the sorted values searched or compared <b>180</b> to determine if two hashes (and hence two images) are the same. If two sketches are the same, the corresponding two different images (feature sets) share at least 3 features, which indicates that they may be the same, and the feature sets are added to a potential cluster. When finished comparing 180 sketches, a first set of clusters is outputted <b>182</b>. These clusters will have been computed in a fast and scalable manner, however, initial clusters may have high precision but poor recall, and additional cluster refinement may be desirable. Additional refinement may be accomplished in the second stage, described in the next section. While a min hash function may be used, any locality sensitive hashing (LSH) technique may be used. For example, other LSH techniques have been designed for approximating L2 or L1 distances between features, as opposed to the hamming distance approximated by min hash.</p>
<p id="h-0009" num="0000">Second Stage</p>
<p id="p-0025" num="0024">In practice, the clusters in the first set output by the first stage may tend to be small with highly similar or identical images in clusters. That is, a given cluster may have mostly images that are very close or similar to each other. There may also be images that are nearby that should be part of the given cluster, but the images did not have matching hashes. A second stage may therefore be performed to refine the clusters and move images into clusters that are a closer fit for those images. That is, recall of the initial clusters may be boosted.</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 5</figref> shows a second stage of image clustering. Initially, a descriptor-computing step <b>190</b> is performed for each cluster outputted by the first stage. Step <b>190</b> computes a cluster descriptor (analogous to a phrase of words/features in the dictionary/vocabulary). Specifically, given a current cluster whose cluster descriptor is to be computed, the feature integers in each feature set that belongs to the current cluster are added to a temporary working set, with duplication counted. That is, for each feature integer in each image in the cluster, the number of times the feature integer occurs in a feature set of the cluster is counted in the temporary working set. The top N (e.g. 40) integer features (the 40 most common integer features in the cluster's feature sets) are found <b>194</b>. These top N feature integers are then saved <b>196</b> as the cluster descriptor for the current cluster. Note that any other technique may be used to form a cluster descriptor, though preferably feature integers will be drawn from among many of the feature sets in the cluster and will thus represent the cluster members as a whole. For example, some feature integers might be weighted based on information in the vocabulary, or they might be weighted based on information such as cluster size, the number of clusters, etc. In one embodiment, a histogram of feature integers may be computed for each cluster, and TF/IDF may be used to select the top N most relevant feature integers.</p>
<p id="p-0027" num="0026">Having found a cluster descriptor (set of feature integers) for each cluster, a comparing process <b>198</b> is then performed for each image; each image's feature set is tested for similarity to each cluster's cluster descriptor. For each image, the feature integers in the image's feature set are compared <b>200</b> to the feature integers in the current cluster's cluster descriptor. Any variety of means of comparison may be used. In one embodiment, an image may be added to or placed <b>202</b> in a cluster if it has 7 or more feature integers in common with the feature integers in the current cluster descriptor (in some embodiments, the placed <b>202</b> image may be removed from a prior cluster to which it belonged). In other embodiments, various known measures of distance or proximity may be used such as TF/IDF weighting. The resulting refined clusters are then outputted <b>204</b>.</p>
<p id="p-0028" num="0027">Matching every cluster descriptor against every feature set may be performed by initially creating an inverse lookup table that maps feature integers to cluster descriptors, thus, given a feature integer of an image, it may quickly be determined which clusters the feature integer is found in. If such a lookup process reveals that there is an image has 7 or more feature integers in a cluster, then the image may be added to that cluster. In some embodiments, an image may be included in only one cluster, for instance, the cluster whose cluster descriptor is closest to the image's feature set.</p>
<p id="p-0029" num="0028">Various refinements and improvements may be used. Some of the outputted <b>204</b> clusters may exhibit incoherency; some member images may have little semantic similarity. This may occur when images have large numbers of globally common features. That is, images that have features with high probability of occurrence across the entire image collection may not place well. However, these types of images may be identified and removed by, for each cluster, for a given cluster, determining a quality score as the sum of the TF/IDF scores of the features in the given cluster's cluster descriptor. Clusters with low quality scores may be eliminated.</p>
<p id="p-0030" num="0029">The second stage may be repeated in iterative fashion, but improvements may quickly become marginal. One pass provides satisfactory results for some input image databases, which is helpful for large image databases. Also, it should be noted that given clusters of feature sets, it is trivial to compute final clusters of images per se; a feature set of an image acts like an identifier for that image. In one embodiment, computations may be performed using addresses of or pointers to images and feature sets and only the addresses or pointers are manipulated to form clusters.</p>
<p id="p-0031" num="0030">The Map-Reduce framework for Min Hash has been described elsewhere and is well suited for data center platforms to implement embodiments described above. Algorithmic extensions to Min Hash discussed herein can be expressed in relational algebra, thus, any stage may be readily implemented in the Dryad framework and therefore executed on data center platforms.</p>
<heading id="h-0010" level="1">CONCLUSION</heading>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 6</figref> shows an example computer <b>220</b> which may be used to implement embodiments and features discussed above. The computer <b>220</b> may include storage <b>222</b> (volatile memory, non-volatile memory, and/or disk storage, etc.) and one or more processors <b>224</b>. One or more computers <b>220</b> may operate cooperatively as in the case of the data center mentioned above. One of ordinary skill will be able to write code based on the description above and therefore realize the embodiments in the form of information stored in the storage <b>222</b> or in external device readable media (e.g., disks, memory sticks, etc.). This is deemed to include at least media such as optical storage (e.g., CD-ROM), magnetic media, flash ROM, or any current or future means of storing digital information in a form readily and reliably usable by computer <b>220</b>. The stored information can be in the form of machine executable instructions (e.g., compiled executable binary code), source code, bytecode, or any other information that can be used to enable or configure computing devices to perform the various embodiments discussed above. This is also deemed to include at least volatile memory such as RAM and/or virtual memory storing information such as CPU instructions during execution of a program carrying out an embodiment, as well as non-volatile media storing information that allows a program or executable to be loaded and executed. The embodiments and features can be performed on any type of computing device, including portable devices, workstations, servers, mobile wireless devices, and so on.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method implemented by one or more computers to cluster images, the method comprising:
<claim-text>accessing a set of images, each image having a corresponding feature set, respectively, each feature set comprising feature integers representing features of the corresponding image;</claim-text>
<claim-text>perform a first clustering step comprising forming initial clusters of the feature sets according to the feature integers of the feature sets;</claim-text>
<claim-text>computing cluster descriptors for the initial clusters, respectively, where a descriptor for a given initial cluster is computed by selecting and including in the feature set feature integers from among the feature sets in the given initial cluster, each feature set in the given initial cluster corresponding to a different one of the images in the set of images, each cluster descriptor comprising a set of feature integers selected from at least two different feature sets in the corresponding initial cluster such that each cluster descriptor comprises feature integers of multiple of the images, whereby each initial cluster has a feature set comprised of feature integers from two or more of the feature sets in the initial cluster; and</claim-text>
<claim-text>refining the initial clusters by comparing at least some of the feature sets of the images with at least some of the cluster descriptors, and based on such comparing moving some of the features sets from corresponding initial clusters to other initial clusters.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a feature set of an image comprises a set of integers obtained by quantizing descriptors computed from features of the image.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the forming of the initial clusters comprises computing min hashes of the feature sets.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. A method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the forming of the initial clusters further comprises sorting the min hashes of the feature sets and forming clusters of the feature sets that have matching min hashes.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. A method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the min hash of a feature set comprises a concatenation of multiple different min hashes of the feature set.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein whether a feature set is moved to another cluster is determined based on the number of feature integers common to both the feature set and the cluster descriptor of the other cluster.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. A method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the number of feature integers is determined by using a reverse lookup table that maps feature integers to clusters.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. One or more computer-readable storage storing information to enable one or more computers to perform a process, the process comprising:
<claim-text>accessing a plurality of feature sets of respective images, each feature set of an image comprising a plurality of features of the image;</claim-text>
<claim-text>for each feature set, given a feature set, computing a plurality of sketches, each sketch of the feature set comprising a concatenation of hashes of the feature set;</claim-text>
<claim-text>placing the feature sets in clusters based on the respective sketches;</claim-text>
<claim-text>adding a set of the feature sets to a set of the clusters, where, for a given one of the features sets in the set of feature sets, and for a given one of the clusters in the set of clusters, a determination is made whether to add the given feature set to the given cluster by comparing the features of the given feature set with top N features selected from among the feature sets in the given cluster, the top N features selected by identifying the N most common features among the features of the feature sets in the given cluster, wherein the top N features includes features from different feature sets, of different respective images, in the given cluster; and</claim-text>
<claim-text>based on the added-to clusters of feature sets, storing information indicating which images belong to which clusters.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. One or more computer-readable storage according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the features of a feature set were computed based on image analysis of the corresponding image, the features representing features of subject matter in the image.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. One or more computer-readable storage according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein each feature set comprises a bag of features, and the features of each feature set comprise features from a feature dictionary.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. One or more computer-readable storage according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the features of a feature set are computed by mapping features of the images to nearest words in the feature dictionary and using those words as the features of the feature sets.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. One or more computer-readable storage according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the hashes of the sketches comprise min hashes, the min hashes having been computed based on random permutations of the feature sets.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. One or more computer-readable storage according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the hashes are computed from a locality sensitive hashing function.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. One or more computer-readable storage according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the locality sensitive hashing function comprises a Min Hash function.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. A computing device comprising:
<claim-text>memory storing a plurality of feature sets, each feature set comprising a plurality of features of a corresponding image;</claim-text>
<claim-text>a processor executing instructions that access the feature sets and store in the memory information indicating clusters of the feature sets, the clusters having been computed by the processor computing min hashes of the feature sets and determining which feature sets belong to which clusters based on which feature sets have min hashes with common values;</claim-text>
<claim-text>the processor computing and storing in the memory cluster descriptors for the clusters, respectively, a cluster descriptor comprising a plurality of features from the feature sets in the corresponding cluster including at least two features from two different feature sets; and</claim-text>
<claim-text>the processor altering the clusters stored in the memory by adding some of the feature sets to some of the clusters including moving some feature sets to other clusters, where the processor determines whether to add a given feature set to a given cluster based on the features in the given feature set and based on the features in the cluster descriptor for the given cluster, the features in the cluster descriptor for the given cluster including at least two features from two respective different feature sets in the given cluster.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. A computing device according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the features of a feature set for an image comprise quantizations of interest points in the image.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. A computing device according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein a min hash of a feature set comprises a concatenation of individual min hashes from different permutations of the feature set.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. A computing device according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein for each cluster the processor removes features from the cluster descriptors, where a feature is removed based on how commonly it occurs among the feature sets.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. A computing device according to <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein how commonly a feature occurs is computed using TF/IDF over the set of features in the feature sets.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. A computing device according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, determining scores for respective clusters based on features in respective cluster descriptors, and eliminating clusters based on the scores. </claim-text>
</claim>
</claims>
</us-patent-grant>
