<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625671-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625671</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12685464</doc-number>
<date>20100111</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<us-term-of-grant>
<us-term-extension>354</us-term-extension>
<disclaimer>
<text>This patent is subject to a terminal disclaimer.</text>
</disclaimer>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>9</main-group>
<subgroup>64</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>37524016</main-classification>
</classification-national>
<invention-title id="d2e55">Look-ahead system and method for pan and zoom detection in video sequences</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>6052414</doc-number>
<kind>A</kind>
<name>Lee et al.</name>
<date>20000400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524016</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6670963</doc-number>
<kind>B2</kind>
<name>Osberger</name>
<date>20031200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345629</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6735253</doc-number>
<kind>B1</kind>
<name>Chang et al.</name>
<date>20040500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>7027513</doc-number>
<kind>B2</kind>
<name>Zhang et al.</name>
<date>20060400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524016</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>2001/0017890</doc-number>
<kind>A1</kind>
<name>Rhee</name>
<date>20010800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>WO</country>
<doc-number>WO-97/08266</doc-number>
<date>19981100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>WO</country>
<doc-number>WO 98/52356</doc-number>
<date>19981100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>WO</country>
<doc-number>WO-98/52356</doc-number>
<kind>A1</kind>
<date>19981100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00009">
<othercit>Bouthemy, Patrick et al., &#x201c;A Unified Approach to Shot Change Detection and Camera Motion Characterization,&#x201d; IEEE Transactions on Circuits and Systems for Video Technology, vol. 9, No. 7, pp. 1030-1044, Oct. 1999.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00010">
<othercit>Dufaux, Frederick et al. &#x201c;Efficient, Robust, and Fast Global Motion Estimation for Video Coding,&#x201d; IEEE Transactions on Image Processing, vol. 9, No. 3, pp. 497-501, Mar. 2000.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00011">
<othercit>Guler, S., &#x201c;Scene and content analysis from multiple video streams,&#x201d; Applied Imagery Pattern Recognition Workshop, AIPR 2001 30th Oct. 10-12, 2001 pp. 119-125.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00012">
<othercit>Jozawa, H. et al., &#x201c;Two-Stage Motion Compensation Using Adaptive Global MC and Local Affine MC,&#x201d; IEEE Transactions on Circuits and Systems for Video Technology, vol. 7, No. 1, pp. 75-85, Feb. 1997.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00013">
<othercit>MPEG and ITU-T, &#x201c;Joint Final Committee Draft of Joint Video Specification ISO/IEC/JTC1/SC29/WG11 (MPEG) 14496-10 and ITU-T Rec. H.264,&#x201d; Geneva, 242 pages, Oct. 2002.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00014">
<othercit>Rath, Gagan et al., &#x201c;Iterative Least Squares and Compression Based Estimations for a Four-Parameter Linear Global Motion Model and Global Motion Compensation,&#x201d; IEEE Transactions on Circuits and Systems for Video Technology, vol. 9, No. 7, pp. 1075-1099, Oct. 1999.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00015">
<othercit>Tan, YP et al. &#x201c;Rapid Estimation of Camera Motion from Compressed Video with Application Video Annotation,&#x201d;IEEE Transactions on Circuits and Systems for Video Technology, vol. 10, No. 1, pp. 133-146, Feb. 2000.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00016">
<othercit>Tse, Y.T.; Baker, R.L., &#x201c;Global zoom/pan estimation and compensation for video compression,&#x201d; Acoustics, Speech, and Signal Processing, 1991, ICASSP-91, 1991 International Conference on Apr. 14-17, 1991, vol. 4, pp. 2725-2728.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>17</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>None</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>4</number-of-drawing-sheets>
<number-of-figures>4</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>10655564</doc-number>
<date>20030903</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>7646437</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>12685464</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20100110303</doc-number>
<kind>A1</kind>
<date>20100506</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Dumitras</last-name>
<first-name>Adriana</first-name>
<address>
<city>Sunnyvale</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Haskell</last-name>
<first-name>Barin G.</first-name>
<address>
<city>Mountain View</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Dumitras</last-name>
<first-name>Adriana</first-name>
<address>
<city>Sunnyvale</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Haskell</last-name>
<first-name>Barin G.</first-name>
<address>
<city>Mountain View</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Kenyon &#x26; Kenyon LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Apple Inc.</orgname>
<role>02</role>
<address>
<city>Cupertino</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Kelley</last-name>
<first-name>Christopher S</first-name>
<department>2482</department>
</primary-examiner>
<assistant-examiner>
<last-name>Findley</last-name>
<first-name>Christopher</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A look-ahead system and method for pan and zoom detection in video sequences is disclosed. The system and method use motion vectors in a reference coordinate system to identify pans and zooms in video sequences. The identification of pans and zooms enables parameter switching for improved encoding in various video standards (e.g., H.264) and improved video retrieval of documentary movies and other video sequences in video databases or other storage devices.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="253.58mm" wi="198.88mm" file="US08625671-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="244.26mm" wi="185.93mm" orientation="landscape" file="US08625671-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="243.76mm" wi="166.62mm" orientation="landscape" file="US08625671-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="255.19mm" wi="213.61mm" file="US08625671-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="243.33mm" wi="191.85mm" orientation="landscape" file="US08625671-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application is a continuation of U.S. patent application Ser. No. 10/655,564, filed on Sep. 3, 2003, which is incorporated herein by reference in its entirety.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">TECHNICAL FIELD</heading>
<p id="p-0003" num="0002">The invention relates generally to analysis of motion in video sequences and, more particularly, to identifying pan and zoom global motion in video sequences.</p>
<heading id="h-0003" level="1">BACKGROUND</heading>
<p id="p-0004" num="0003">The analysis of motion information in video sequences has typically addressed two largely non-overlapping applications: video retrieval and video coding. In video retrieval systems, the dominant motion, motion trajectories and tempo are computed to identify particular video clips or sequences that are similar in terms of motion characteristics or belong to a distinct class (e.g., commercials). In video coding systems, global motion parameters are estimated for global motion compensation and for constructing sprites. In both video retrieval and video coding systems, it is desirable to identify pan and zoom global motion. For video retrieval systems, pan and zoom detection enables classification of video sequences (e.g., documentary movies) for efficient retrieval from video databases. For video coding systems, pan and zoom detection enables the adaptive switching of coding parameters (e.g., the selection of temporal and spatial Direct Modes in H.264).</p>
<p id="p-0005" num="0004">Previous methods for detecting pan and zoom global motion in video sequences require estimating parameters of global motion, i.e., motion such that most of the image points are displaced in a uniform manner. Because the motion of many image points in a video frame is described by a small set of parameters related to camera parameters, estimating global motion parameters is a more constrained case than the estimation of motion parameters in all image points. The number of parameters obtained depends on the global motion model that is assumed to best describe the motion in the video sequence, for example, translational, affine, perspective, quadratic, etc., yielding 2, 6, 8 and 12 parameters, respectively. In particular, a perspective motion model yields the estimated coordinates {circumflex over (x)}, &#x177; using the old coordinates x<sub>i</sub>, y<sub>i </sub>and the equations:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>{circumflex over (x)}</i><sub>i</sub>=(<i>a</i><sub>0</sub><i>+a</i><sub>2</sub><i>x</i><sub>i</sub><i>+a</i><sub>3</sub><i>y</i><sub>i</sub>)/(<i>a</i><sub>6</sub><i>x</i><sub>i</sub><i>+a</i><sub>7</sub><i>y</i><sub>i</sub>+1)&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>&#x177;</i><sub>i</sub>=(<i>a</i><sub>1</sub><i>+a</i><sub>4</sub><i>x</i><sub>i</sub><i>+a</i><sub>5</sub><i>y</i><sub>i</sub>)/(<i>a</i><sub>6</sub><i>x</i><sub>i</sub><i>+a</i><sub>7</sub><i>y</i><sub>i</sub>+1)&#x2003;&#x2003;(2)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where a<sub>0 </sub>. . . a<sub>7 </sub>are the motion parameters. Other models can be obtained as particular cases of the perspective model. For example, if a<sub>6</sub>=a<sub>7</sub>=0, the affine model (six parameters) is obtained, if a<sub>2</sub>=a<sub>5</sub>, a<sub>3</sub>=a<sub>4</sub>=a<sub>6</sub>=a<sub>7</sub>=0, the translation-zoom model (three parameters) is obtained, and if a<sub>2</sub>=a<sub>5</sub>=1, a<sub>3</sub>=a<sub>4</sub>=a<sub>6</sub>=a<sub>7</sub>=0, the translational model (two parameters) is obtained.
</p>
<p id="p-0006" num="0005">Global motion estimation can be formulated as an optimization problem, where the error between a current frame and a motion compensated previous frame is minimized. Techniques such as gradient descent and second order optimization procedures have been applied iteratively to solve the optimization problem. In Hirohisa Jozawa, et al., &#x201c;Two-stage Motion Compensation Using Adaptive Global MC and Local Affine MC,&#x201d; <i>IEEE Trans. on Circuits and Systems for Video Tech</i>., Vol. 7, No. 1, pp. 75-82, February 1997, global motion parameters are estimated using a two-stage motion compensation process. In the first stage, global motion is estimated and a global motion compensated picture is obtained. In the second stage, the global motion compensated picture is used as a reference for local motion compensation. The local motion compensation is performed both for the global motion compensated reference image and for the image without global motion compensation using an affine motion model in the framework of the H.263 standard.</p>
<p id="p-0007" num="0006">Other techniques for estimating global motion in video sequences have also been proposed. A technique proposed in Frederic Dufaux et al., &#x201c;Efficient, Robust and Fast Global Motion Estimation for Video Coding,&#x201d; <i>IEEE Trans. on Image Processing</i>, Vol. 9, No. 3, pp. 497-510, March 2000, includes a three-stage process. In a first stage, a low pass image pyramid is constructed by successive decompositions of the original picture. In a second stage, an initial estimation is performed, followed by a refining of the initial estimate, using gradient descent-based in a third stage. A perspective model with eight parameters has been used in this technique to model camera motion.</p>
<p id="p-0008" num="0007">In Gagan B. Rath, et al., &#x201c;Iterative Least Squares and Compression Based Estimation for a Four-Parameter Linear Global Motion Model and Global Motion Compensation,&#x201d; <i>IEEE Trans. on Circuits and Systems for Video Tech</i>., Vol. 9, No. 7, pp. 1075-1099, October 1999, a four-parameter model for global motion is employed for pan and zoom motion estimation. This technique uses iterative least squares estimation to accurately estimate parameters.</p>
<p id="p-0009" num="0008">In Patrick Bouthemy, et al., &#x201c;A Unified Approach to Shot Change Detection and Camera Motion Characterization,&#x201d; <i>IEEE Trans. on Circuits and Systems for Video Tech</i>., Vol. 9, No. 7, pp. 1030-1040, October 1999, a unified approach to shot change detection and camera motion characterization is proposed. By using an affine motion model, global motion parameters are estimated and at the same time, the evolution of scene cuts and transitions is evaluated.</p>
<p id="p-0010" num="0009">In Yap-Peng, et al., &#x201c;Rapid Estimation of Camera Motion from Compressed Video With Application to Video Annotation,&#x201d; <i>IEEE Trans. on Circuits and Systems for Video Tech</i>., Vol. 10, No. 1, pp. 133-146, February 2000, camera motion parameters are estimated from compressed video, where macroblocks from P frames are used to estimate the unknown parameters of a global motion model.</p>
<p id="p-0011" num="0010">All of the conventional methods described above require estimating global motion parameters to identify a specific type of global motion (e.g., pan, zoom or other). To estimate global motion, however, these conventional methods employ a generic motion model having global motion parameters that must be estimated. These global motion parameters are not necessary, however, for retrieving video sequences from databases. Nor are these global motion parameters necessary for parameter switching in video coding systems. Therefore, the conventional methods described above for estimating global motion increase unnecessarily the computational complexity of the application systems that employ such techniques.</p>
<p id="p-0012" num="0011">Video retrieval systems can benefit from pan and zoom detection, which would allow identification of documentary movies and other sequences in video databases. Documentary movies include, for example, long panning clips that have a typical length of at least 10 seconds (i.e., 240 frames for a frame rate of 23.976 fps). These long panning clips are often preceded or followed by zooms on scenes or objects of interest. Pan and zoom clips are also present in numerous other types of sequences, from cartoons and sports games to home videos. It is therefore of interest to retrieve video clips and sequences having common pan or zoom characteristics.</p>
<p id="p-0013" num="0012">Pan and zoom detection in video sequences can also enhance the capabilities of an encoder in a standards compliant system. It is well-known that encoders that are compliant with the MPEG and ITU standards may be unconstrained in terms of analysis methods and parameter values selections, as well as various coding scenarios for given applications, as long as the resulting compressed bit streams are standards-compliant (i.e., can be decoded by any corresponding standardized decoder). The objective of performing various enhancements at the encoder side is bit rate reduction of the compressed streams while maintaining high visual quality in the decoded pictures. An example of such enhancement is the selection of temporal and spatial Direct Modes described in the H.264 video coding standard.</p>
<p id="p-0014" num="0013">In H.264, each frame of a video sequence is divided into pixel blocks having varying size (e.g., 4&#xd7;4, 8&#xd7;8, 16&#xd7;16). These pixel blocks are coded using motion compensated predictive coding. A predicted pixel block may be an Intra (I) pixel block that uses no information from preceding pictures in its coding, a Unidirectionally Predicted (P) pixel block that uses information from one preceding picture, or a Bidirectionally Predicted (B) pixel block that uses information from one preceding picture and one future picture. The details of H.264 can be found in the publicly available MPEG and ITU-T, &#x201c;Joint Final Committee Draft of Joint Video Specification ISO/IEC/JTC1/SC29/WG11 (MPEG) 14496-10 and ITU-T Rec. H.264,&#x201d; Geneva, October 2002, which is incorporated by reference herein in its entirety.</p>
<p id="p-0015" num="0014">For each pixel block in a P picture, a motion vector is computed. Using the motion vector, a prediction pixel block can be formed by translation of pixels in the aforementioned previous picture. The difference between the actual pixel block in the P picture and the prediction block is then coded for transmission. Each motion vector may also be transmitted via predictive coding. That is, a prediction is formed using nearby motion vectors that have already been sent, and then the difference between the actual motion vector and the prediction is coded for transmission. For each B pixel block, two motion vectors are typically computed, one for the aforementioned previous picture and one for the future picture. From these motion vectors, two prediction pixel blocks are computed, which are then averaged together to form the final prediction. The difference between the actual pixel block in the B picture and the prediction block is then coded for transmission. Each motion vector of a B pixel block may be transmitted via predictive coding. That is, a prediction is formed using nearby motion vectors that have already been transmitted, then the difference between the actual motion vector and the prediction is coded for transmission.</p>
<p id="p-0016" num="0015">With B pixel blocks, however, the opportunity exists for interpolating the motion vectors from those in the co-located or nearby pixel blocks of the stored pictures. Note that when decoding a B slice, there exist two lists (list 0 and list 1) of reference pictures stored in the decoded picture buffer. For a pixel block in a B slice, the co-located pixel block is defined as a pixel block that resides in the same geometric location of the first reference picture in list 1 or nearby pixel blocks of the stored pictures. The former case is known as the temporal-direct mode. The latter case is known as the spatial direct mode. In both of these cases, the interpolated value may then be used as a prediction and the difference between the actual motion vector and the prediction coded for transmission. Such interpolation is carried out both at the coder and decoder. In some cases, the interpolated motion vector is good enough to be used without any correction, in which case no motion vector data need be sent. Note that the prediction error of a pixel block or subblock, which is computed as the mean square error between the original pixel block and the decoded pixel block after encoding using direct mode is still transformed, quantized and entropy encoded prior to transmission. This is referred to as Direct Mode in H.264 (and H.263). Direct Mode selection is particularly effective when the camera is slowly panning across a stationary background. Indeed, the interpolation may be good enough to be used as is, which means that no differential information need be transmitted for these B pixel block motion vectors. Therefore, for such sequences that allow good motion vector predictions using neighboring temporal or spatial information, the Direct Mode can provide important bit rate savings.</p>
<p id="p-0017" num="0016">Accordingly, there is a need for a system and method for pan and zoom detection in video sequences that enable classification of video sequences (e.g., documentary movies) in video retrieval systems and adaptive switching of coding parameters (e.g., selection of temporal and spatial Direct Modes in H.264) video coding systems, without performing the computationally intensive task of estimating all the parameters of a global motion model.</p>
<heading id="h-0004" level="1">SUMMARY</heading>
<p id="p-0018" num="0017">The present invention overcomes the deficiencies of the prior art by providing a look-ahead system and method for pan and zoom detection in video sequences based on motion characteristics.</p>
<p id="p-0019" num="0018">One aspect of the present invention includes a method of detecting pan and zoom in a video sequence. The method comprises selecting a set of frames from a video sequence (e.g., by identifying scene cuts), determining a set of motion vectors for each frame in the set of frames, identifying at least two largest regions in each frame in the frame set having motion vectors with substantially similar orientation in a reference coordinate system (e.g., polar coordinates), determining percentages of each frame covered by the at least two largest regions, determining a statistical measure (e.g., variance) of the motion vector orientations in the reference coordinate system for at least one of the two largest regions, and comparing the percentages and statistical measure to threshold values to identify a pan or zoom in the video sequence.</p>
<p id="p-0020" num="0019">Another aspect of the present invention includes a system for detecting pan and zoom sequences in a video sequence. The system comprises: a preprocessor for selecting a set of frames from a video sequence, and a motion analyzer for determining a motion vector for each frame in the set of frames, identifying the two largest regions in each frame having motion vectors with substantially similar orientation in a reference coordinate system, determining percentages of each frame covered by the two largest regions, determining a statistical measure of the motion vector orientations in the reference coordinate system for at least one of the two largest regions, and comparing the percentages and statistical measure to threshold values to identify a pan or zoom in the video sequence.</p>
<p id="p-0021" num="0020">The present invention as defined by the claims herein provides a computationally efficient solution for identifying pans and zooms in video sequences, including but not limited to the enabling of parameter switching for improved encoding in video standards (e.g., H.264) and improved video retrieval of video sequences from databases and other video storage devices.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram of a video retrieval system, in accordance with one embodiment of the present invention.</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram of a video encoder, in accordance with one embodiment of the present invention.</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 3</figref> is a flow diagram of a look-ahead method for pan and zoom detection in video sequences, in accordance with one embodiment of the present invention.</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 4</figref> illustrates the identification of the two largest regions in a video frame k, which form part of a look-ahead video clip, in accordance with one embodiment of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0006" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0026" num="0025">While the embodiments described below include a video retrieval system and a video encoder (with parameter switching capability), the present invention is equally applicable to any video systems that employ pan and/or zoom detection to perform for a particular application.</p>
<p id="h-0007" num="0000">Video Retrieval Application</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram of a video retrieval system <b>100</b>, in accordance with one embodiment of the present invention. The video retrieval system <b>100</b> includes a query analyzer <b>102</b>, a comparison block <b>114</b> and an image database <b>116</b>. The query analyzer <b>102</b> includes one or more analysis blocks, including but not limited to a text analysis block <b>104</b>, a texture analysis block <b>106</b>, a shape analysis block <b>108</b>, a motion analysis block <b>110</b> and a look-ahead detector <b>112</b>. In one embodiment, the analysis blocks <b>104</b>, <b>106</b>, <b>108</b>, <b>110</b>, and the look-ahead detector <b>112</b>, are implemented as software instructions stored on a computer-readable medium and executed by one or more processors in the video retrieval system <b>100</b>.</p>
<p id="p-0028" num="0027">The query analyzer <b>102</b> receives one or more queries (e.g., text, images, image regions, image features, etc.) and analyzes the queries with one or more of the analysis blocks <b>104</b>, <b>106</b>, <b>108</b> and <b>110</b>. For example, the text analysis block <b>104</b> analyzes text queries, the texture analysis block <b>106</b> analyzes textures, the shape analysis block <b>108</b> analyzes shapes and the motion analysis block <b>110</b> analyzes motion. The motion analysis block <b>110</b> also provides motion vectors to the look-ahead detector <b>112</b>. The look-ahead detector <b>112</b> uses the motion vectors to perform pan and zoom detection in accordance with the present invention. The query analyzer <b>102</b> provides query indices to the comparison block <b>114</b>, which compares the query indices with database indices provided by the image/video database <b>116</b>. If there is a match between a query index and a database index, then the comparison block <b>114</b> generates a match index, which is used to retrieve a video sequence, image or image region from the image/video database <b>116</b>.</p>
<p id="p-0029" num="0028">The video retrieval system <b>100</b> uses the look-ahead detector <b>112</b> to identify pans and zooms in video sequences for improved retrieval of video sequences, such as documentaries. More particularly, the look-ahead detector <b>112</b> transforms block-based motion vectors from the motion analysis block <b>110</b> to polar coordinates to detect pan and zoom sequences without computing global motion parameters. The various steps performed by the look-ahead detector <b>112</b> are described more fully with respect to <figref idref="DRAWINGS">FIG. 3</figref>.</p>
<p id="h-0008" num="0000">Video Coding Application</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram of a video encoder <b>200</b>, in accordance with one embodiment of the present invention. The video encoder <b>200</b> includes a preprocessor <b>202</b>, a video sequence analyzer <b>204</b>, a parameter selector <b>212</b> and a compressor <b>214</b>. The video sequence analyzer <b>204</b> includes a text analysis block <b>206</b>, a motion analysis block <b>208</b> and a look-ahead detector <b>210</b>. The look-ahead detector <b>210</b> is coupled to the motion analysis block <b>208</b> and receives motion vectors from the motion analysis block <b>208</b>. In one embodiment, the analysis blocks <b>204</b>, <b>206</b>, <b>208</b>, and the look-ahead detector <b>210</b>, are implemented as software instructions stored on a computer-readable medium and executed by one or more processors.</p>
<p id="p-0031" num="0030">In normal operation, the preprocessor <b>202</b> may perform tasks such as color space conversions, spatial, temporal or spatio-temporal filtering, or down sampling. The texture analysis block <b>206</b> performs a texture analysis for each macroblock and the motion analysis block <b>208</b> performs motion analysis for each macroblock. The video sequence analyzer <b>204</b> provides data (e.g., pan or zoom detection signals) to the parameter selector <b>212</b>, which provides parameter switching for improved encoding (e.g., adaptive switching of temporal and spatial direct modes in H.264).</p>
<p id="h-0009" num="0000">Pan and Zoom Detection</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 3</figref> is a flow diagram of a look-ahead method for pan and zoom detection in video sequences, in accordance with one embodiment of the present invention. For each group of F frames, a look-ahead video clip is determined <b>300</b> by identifying a first scene cut between a first and the Fth frame of a group of frames. Various methods have been employed to identify scene cuts in video sequences. For simplicity, this embodiment of the present invention makes use of frame differences and motion information to identify a scene cut. In one embodiment of the present invention, if the relative difference between two adjacent frames with respect to the first of these frames is larger than a predetermined threshold (e.g. 20%) or if the motion vectors in the second of these frames are equal to zero, then a scene cut is identified. If a scene cut is identified between frames F<sub>c </sub>and F<sub>c</sub>+1, then the look-ahead video clip includes <b>302</b> frames from the first frame to the frame F<sub>c</sub>. If there exists no scene cut, then the look-ahead video clip includes <b>304</b> frames from the first frame of the group frames to the Fth frame.</p>
<p id="p-0033" num="0032">For each frame of the look-ahead video clip, motion vectors are computed <b>306</b> using, e.g., 8&#xd7;8 macroblocks to make use of block-based motion information to characterize global motion in the video sequence. In one embodiment, motion vector data (e.g., one motion vector for each 8&#xd7;8 block) is obtained by motion estimation using techniques such as those disclosed in the publicly available H.264 standard (e.g., reference H.264 encoder version 6.1). Note that other block sizes can be used with the present invention depending upon the application and motion estimation method using various block sizes (e.g., 4&#xd7;4, 16&#xd7;16 pixels).</p>
<p id="p-0034" num="0033">The angle theta of each of the motion vectors is computed <b>308</b> in polar coordinates (r, &#x3b8;), where r is the modulus and theta is the angle of a motion vector. More specifically, the angle &#x3b8; of a motion vector is given by:</p>
<p id="p-0035" num="0034">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>&#x3b8;</mi>
          <mo>=</mo>
          <mrow>
            <mi>a</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mrow>
              <mi>tan</mi>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mfrac>
                  <mi>y</mi>
                  <mi>x</mi>
                </mfrac>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
        <mo>,</mo>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>3</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where (x, y) are the Cartesian coordinates (displacements) on the x, y directions, respectively. Preferably, the value of theta is normalized between 0 and 1. Note that other reference coordinate systems can be use with the present invention, such as Cartesian, spherical, cylindrical and the like.
</p>
<p id="p-0036" num="0035">Next, the two largest regions in each frame containing motion vectors with similar orientation (e.g., values for theta are substantially similar) are identified <b>310</b>. Mathematically, the regions R<sub>k</sub><sup>(1) </sup>and R<sub>k</sub><sup>(2) </sup>are given by the following equations:</p>
<p id="p-0037" num="0036">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <msubsup>
          <mi>R</mi>
          <mi>k</mi>
          <mrow>
            <mo>(</mo>
            <mn>1</mn>
            <mo>)</mo>
          </mrow>
        </msubsup>
        <mo>=</mo>
        <mrow>
          <mo>{</mo>
          <mrow>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mi>i</mi>
                <mo>,</mo>
                <mi>j</mi>
              </mrow>
              <mo>)</mo>
            </mrow>
            <mo>,</mo>
            <mrow>
              <mn>1</mn>
              <mo>&#x2264;</mo>
              <mi>i</mi>
              <mo>&#x2264;</mo>
              <mi>M</mi>
            </mrow>
            <mo>,</mo>
            <mrow>
              <mrow>
                <mn>1</mn>
                <mo>&#x2264;</mo>
                <mi>j</mi>
                <mo>&#x2264;</mo>
                <mi>N</mi>
              </mrow>
              <mo>|</mo>
              <mrow>
                <mrow>
                  <mi>&#x3b8;</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mi>i</mi>
                      <mo>,</mo>
                      <mi>j</mi>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>&#x2248;</mo>
                <mrow>
                  <mi>const</mi>
                  <mo>.</mo>
                  <mstyle>
                    <mspace width="0.8em" height="0.8ex"/>
                  </mstyle>
                  <mo>&#x2062;</mo>
                  <mi>and</mi>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>4</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <msubsup>
            <mi>A</mi>
            <mi>k</mi>
            <mrow>
              <mo>(</mo>
              <mn>1</mn>
              <mo>)</mo>
            </mrow>
          </msubsup>
          <mo>=</mo>
          <mrow>
            <msub>
              <mi>max</mi>
              <mrow>
                <msub>
                  <mi>allA</mi>
                  <mi>m</mi>
                </msub>
                <mo>&#x2062;</mo>
                <mi>inframek</mi>
              </mrow>
            </msub>
            <mo>&#x2062;</mo>
            <mrow>
              <mo>{</mo>
              <msub>
                <mi>A</mi>
                <mi>m</mi>
              </msub>
              <mo>}</mo>
            </mrow>
          </mrow>
        </mrow>
        <mo>}</mo>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>5</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <msubsup>
          <mi>R</mi>
          <mi>k</mi>
          <mrow>
            <mo>(</mo>
            <mn>2</mn>
            <mo>)</mo>
          </mrow>
        </msubsup>
        <mo>=</mo>
        <mrow>
          <mo>{</mo>
          <mrow>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mi>i</mi>
                <mo>,</mo>
                <mi>j</mi>
              </mrow>
              <mo>)</mo>
            </mrow>
            <mo>,</mo>
            <mrow>
              <mn>1</mn>
              <mo>&#x2264;</mo>
              <mi>i</mi>
              <mo>&#x2264;</mo>
              <mi>M</mi>
            </mrow>
            <mo>,</mo>
            <mrow>
              <mrow>
                <mn>1</mn>
                <mo>&#x2264;</mo>
                <mi>j</mi>
                <mo>&#x2264;</mo>
                <mi>N</mi>
              </mrow>
              <mo>|</mo>
              <mrow>
                <mrow>
                  <mi>&#x3b8;</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mi>i</mi>
                      <mo>,</mo>
                      <mi>j</mi>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>&#x2248;</mo>
                <mrow>
                  <mi>const</mi>
                  <mo>.</mo>
                  <mstyle>
                    <mspace width="0.8em" height="0.8ex"/>
                  </mstyle>
                  <mo>&#x2062;</mo>
                  <mi>and</mi>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>6</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <msubsup>
            <mi>A</mi>
            <mi>k</mi>
            <mrow>
              <mo>(</mo>
              <mn>2</mn>
              <mo>)</mo>
            </mrow>
          </msubsup>
          <mo>=</mo>
          <mrow>
            <msub>
              <mi>max</mi>
              <mrow>
                <mrow>
                  <msub>
                    <mi>allA</mi>
                    <mi>m</mi>
                  </msub>
                  <mo>&#x2062;</mo>
                  <mi>\</mi>
                  <mo>&#x2062;</mo>
                  <msub>
                    <mi>A</mi>
                    <mn>1</mn>
                  </msub>
                </mrow>
                <mo>&#x2062;</mo>
                <mi>inframek</mi>
              </mrow>
            </msub>
            <mo>&#x2062;</mo>
            <mrow>
              <mo>{</mo>
              <msub>
                <mi>A</mi>
                <mi>m</mi>
              </msub>
              <mo>}</mo>
            </mrow>
          </mrow>
        </mrow>
        <mo>}</mo>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>7</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where (i,j) are the locations of pixels in a frame, M, N are the width and height of a frame, A<sub>k</sub><sup>(1) </sup>and A<sub>k</sub><sup>(2) </sup>are the areas (e.g., in number of pixels) of the first and second largest regions R<sub>k</sub><sup>(1) </sup>and R<sub>k</sub><sup>(2)</sup>, respectively, which contain motion vectors having similar orientation based on the values for theta computed using Equation (3).
</p>
<p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. 4</figref> illustrates pan and zoom detection in a frame k in a look-ahead video clip <b>400</b> comprising multiple frames. The frame k includes first and second largest regions <b>402</b>, <b>404</b>, which correspond to R<sub>k</sub><sup>(1) </sup>and R<sub>k</sub><sup>(2) </sup>in equations (4) and (6) above.</p>
<p id="p-0039" num="0038">Next, the percentages covered by the regions R<sub>k</sub><sup>(1) </sup>and R<sub>k</sub><sup>(2) </sup>within each frame are computed <b>312</b> and the variance of the &#x3b8; values in the first largest region R<sub>k</sub><sup>(1) </sup>of each frame is computed <b>314</b>. More specifically, the percentages P<sub>k</sub><sup>(1) </sup>and P<sub>k</sub><sup>(2) </sup>covered within each frame by the regions R<sub>k</sub><sup>(1) </sup>and R<sub>k</sub><sup>(2) </sup>with similar orientation of the motion vectors are given by
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i><sub>k</sub><sup>(1)</sup><i>=A</i><sub>k</sub><sup>(1)</sup>&#xd7;100/(<i>M&#xd7;N</i>), <i>P</i><sub>k</sub><sup>(2)</sup><i>=A</i><sub>k</sub><sup>(2)</sup>&#xd7;100/(<i>M&#xd7;N</i>),&#x2003;&#x2003;(8)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where A<sub>k</sub><sup>(1) </sup>is the area (e.g., in number of pixels) of the first largest region R<sub>k</sub><sup>(1) </sup>and A<sub>k</sub><sup>(2) </sup>is the area (e.g., in the number of pixels) of the second largest region R<sub>k</sub><sup>(2) </sup>with motion vectors having similar orientation (e.g., substantially similar theta values). The variance of the theta values within the first largest region R<sup>1</sup><sub>k </sub>is given by
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>var<sup>(1)</sup>(&#x3b8;)={var(&#x3b8;)|&#x3b8;&#x3b5;<i>R</i><sub>k</sub><sup>1</sup>}&#x2003;&#x2003;(9)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0040" num="0039">The above steps are repeated for each frame in the look-ahead video clip until the last frame of the video clip is reached <b>316</b>. Note the variance of the theta values within the second largest region R<sub>k</sub><sup>(2) </sup>can also be computed instead of the theta values for R<sup>1</sup><sub>k</sub>, but this is unnecessary for the present invention. The percentages and variances computed in the previous steps are then tested <b>318</b> to identify if a pan video clip is present and tested <b>320</b> to identify if a zoom video clip is present in the video clip, as follows:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>If ((<i>P</i><sub>k</sub><sup>(1)</sup><i>+P</i><sub>k</sub><sup>(2)</sup>)&#x3e;&#x3b5;<sub>1</sub>) and var<sup>(1)</sup>(&#x3b8;)&#x3c;&#x3b5;<sub>2</sub>, then pan,&#x2003;&#x2003;(10)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>If ((<i>P</i><sub>k</sub><sup>(1)</sup><i>+P</i><sub>k</sub><sup>(2)</sup>)&#x3c;&#x3b5;<sub>3</sub>) and var<sup>(1)</sup>(&#x3b8;)&#x3c;&#x3b5;<sub>2</sub>, then zoom,&#x2003;&#x2003;(11)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where one exemplary set of threshold values &#x3b5;<sub>1</sub>, &#x3b5;<sub>2 </sub>and &#x3b5;<sub>3 </sub>are determined experimentally to be equal to &#x3b5;<sub>1</sub>=0.95, &#x3b5;<sub>2</sub>=0.01, and &#x3b5;<sub>3</sub>=0.5. Note that these threshold values can be adjusted as desired to increase or decrease the number of possible pan and zoom detections. The preceding steps <b>300</b> to <b>322</b> are repeated until the last group of frames is reached <b>322</b>, and then a new group of frames is processed.
</p>
<p id="p-0041" num="0040">Because the present invention does not compute global motion parameters, it provides a simpler and more computationally efficient system and method for pan and zoom detection than conventional systems and methods, which require the computation of global motion parameters for a global motion model.</p>
<p id="p-0042" num="0041">The foregoing description of the embodiments of the invention has been presented for the purposes of illustration and description. It is not intended to be exhaustive or to limit the invention to the precise form disclosed. Many modifications and variations are possible in light of this disclosure. It is intended that the scope of the invention be limited not by this detailed description, but rather by the claims appended hereto.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08625671-20140107-M00001.NB">
<img id="EMI-M00001" he="6.01mm" wi="76.20mm" file="US08625671-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US08625671-20140107-M00002.NB">
<img id="EMI-M00002" he="21.51mm" wi="76.20mm" file="US08625671-20140107-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method of detecting at least one of a pan and a zoom in a video sequence, comprising:
<claim-text>receiving a video sequence, the video sequence comprising a plurality of frames;</claim-text>
<claim-text>determining a set of motion vectors for each frame in the plurality of the frames;</claim-text>
<claim-text>determining a motion angle for each motion vector;</claim-text>
<claim-text>detecting a scene cut in the plurality of the frames based on the motion vectors of the frames;</claim-text>
<claim-text>selecting a set of frames from the plurality of the frames responsive to detection of the scene cut;</claim-text>
<claim-text>identifying at least two largest regions in each frame, wherein a first largest region includes motion vectors having a first orientation and the second largest region includes motion vectors having a second orientation;</claim-text>
<claim-text>determining percentages of each frame covered by each of the at least two largest regions;</claim-text>
<claim-text>determining a statistical measure of the motion vectors for at least one of the two largest regions using a look-ahead detector; and</claim-text>
<claim-text>comparing the percentages and statistical measure to threshold values to identify whether the plurality of frames includes at least one of a pan and a zoom.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein selecting a set of frames from the plurality of the frames responsive to detection of the scene cut comprises:
<claim-text>selecting a set of video frames from the plurality of the frames of that includes all the frames in the video sequence up to and including a frame just before the detected scene cut.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein frame differences and motion vectors of the plurality of the frames are used to detect a scene cut.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the motion angles are computed in one from the group of coordinate systems consisting of polar, Cartesian, spherical and cylindrical coordinate systems.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the percentages of each frame covered by the at least two largest regions are determined from the number of pixels in each region as a percentage of the total number of pixels in a frame.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the statistical measure is a variance of the motion angles of the motion vectors within at least one of the identified two largest regions in the frame.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. A non-transitory computer-readable storage medium storing executable computer program code for detecting at least one of a pan and a zoom in a video sequence, comprising computer program code for:
<claim-text>receiving a video sequence, the video sequence comprising a plurality of frames;</claim-text>
<claim-text>determining a set of motion vectors for each frame in the plurality of the of frames;</claim-text>
<claim-text>determining a motion angle for each motion vector;</claim-text>
<claim-text>detecting a scene cut in the plurality of the frames based on the motion vectors of the frames;</claim-text>
<claim-text>selecting a set of frames from the plurality of the frames responsive to detection of the scene cut;</claim-text>
<claim-text>identifying at least two largest regions in each frame, wherein a first largest region includes motion vectors having a first orientation and the second largest region includes motion vectors having a second orientation;</claim-text>
<claim-text>determining percentages of each frame covered by each of the at least two largest regions;</claim-text>
<claim-text>determining a statistical measure of the motion vectors for at least one of the two largest regions using a look-ahead detector; and</claim-text>
<claim-text>comparing the percentages and statistical measure to threshold values to identify whether the plurality of frames includes at least one of a pan and a zoom.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The computer-readable storage medium of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the computer program code for selecting a set of frames from the plurality of the frames responsive to detection of the scene cut comprises code for:
<claim-text>selecting a set of video frames from the plurality of the frames of that includes all the frames in the video sequence up to and including a frame just before the detected scene cut.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The computer-readable storage medium of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein frame differences and motion vectors of the plurality of the frames are used to detect a scene cut.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The computer-readable storage medium of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the motion angles are computed in one from the group of coordinate systems consisting of polar, Cartesian, spherical and cylindrical coordinate systems.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The computer-readable storage medium of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the percentages of each frame covered by the at least two largest regions are determined from the number of pixels in each region as a percentage of the total number of pixels in a frame.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The computer-readable storage medium of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the statistical measure is a variance of the motion angles of the motion vectors within at least one of the identified two largest regions in the frame.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. A method of detecting at least one of a pan and a zoom in a video sequence, comprising:
<claim-text>detecting motion vectors to detect a scene cut in a plurality of frames; selecting a set of frames from the video sequence in response to detection of a scene cut;</claim-text>
<claim-text>identifying at least two regions in each selected frame, a first region having motion vectors of a first orientation and a second region having motion vectors of a second orientation;</claim-text>
<claim-text>determining amounts of each frame covered by each of the at least two regions; and</claim-text>
<claim-text>comparing the detected amounts to threshold values, and based on comparisons, classifying the video sequence as having a pan or zoom.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein selecting a set of frames from the plurality of the frames responsive to detection of the scene cut comprises:
<claim-text>selecting a set of video frames from the plurality of the frames of that includes all the frames in the video sequence up to and including a frame just before the detected scene cut.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein frame differences and motion vectors of the plurality of the frames are used to detect a scene cut.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the motion angles are computed in one from the group of coordinate systems consisting of polar, Cartesian, spherical and cylindrical coordinate systems.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the amounts of each frame covered by the at least two regions are determined from the number of pixels in each region as a percentage of the total number of pixels in a frame.</claim-text>
</claim>
</claims>
</us-patent-grant>
