<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626686-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626686</doc-number>
<kind>B1</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>11765903</doc-number>
<date>20070620</date>
</document-id>
</application-reference>
<us-application-series-code>11</us-application-series-code>
<us-term-of-grant>
<us-term-extension>1842</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>1</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>5</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>706 20</main-classification>
<further-classification>382156</further-classification>
<further-classification>700 28</further-classification>
</classification-national>
<invention-title id="d2e53">Invariant object recognition</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>4766551</doc-number>
<kind>A</kind>
<name>Begley</name>
<date>19880800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>4803736</doc-number>
<kind>A</kind>
<name>Grossberg et al.</name>
<date>19890200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>4818348</doc-number>
<kind>A</kind>
<name>Stetter</name>
<date>19890400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>4847783</doc-number>
<kind>A</kind>
<name>Grace et al.</name>
<date>19890700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>4958295</doc-number>
<kind>A</kind>
<name>Davidson et al.</name>
<date>19900900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>5106756</doc-number>
<kind>A</kind>
<name>Zaromb</name>
<date>19920400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>5239483</doc-number>
<kind>A</kind>
<name>Weir</name>
<date>19930800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6594382</doc-number>
<kind>B1</kind>
<name>Woodall</name>
<date>20030700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>7949621</doc-number>
<kind>B2</kind>
<name>Xiao et al.</name>
<date>20110500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2002/0168100</doc-number>
<kind>A1</kind>
<name>Woodall</name>
<date>20021100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382156</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2003/0012453</doc-number>
<kind>A1</kind>
<name>Kotlikov et al.</name>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382275</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2004/0071363</doc-number>
<kind>A1</kind>
<name>Kouri et al.</name>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382276</main-classification></classification-national>
</us-citation>
<us-citation>
<nplcit num="00013">
<othercit>Office Action, U.S. Appl. No. 10/292,811, dated Oct. 1, 2007, 9 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00014">
<othercit>United States Patent and Trademark Office Final Office Action dated Oct. 28, 2010 (U.S. Appl. No. 12/558,617, filed Sep. 14, 2009), 7 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00015">
<othercit>United States Patent and Trademark Office Non-Final Office Action dated Sep. 22, 2011 (U.S. Appl. No. 12/542,069, filed Aug. 17, 2009), 10 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>47</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>706 14</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>10</number-of-drawing-sheets>
<number-of-figures>14</number-of-figures>
</figures>
<us-related-documents>
<continuation-in-part>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>11035412</doc-number>
<date>20050114</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>7580907</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>11765903</doc-number>
</document-id>
</child-doc>
</relation>
</continuation-in-part>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60536261</doc-number>
<date>20040114</date>
</document-id>
</us-provisional-application>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Rhodes</last-name>
<first-name>Paul A.</first-name>
<address>
<city>Palm Beach</city>
<state>FL</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Rhodes</last-name>
<first-name>Paul A.</first-name>
<address>
<city>Palm Beach</city>
<state>FL</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Fich &#x26; Richardson P.C.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Evolved Machines, Inc.</orgname>
<role>02</role>
<address>
<city>West Palm Beach</city>
<state>FL</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Chaki</last-name>
<first-name>Kakali</first-name>
<department>2122</department>
</primary-examiner>
<assistant-examiner>
<last-name>Bharadwaj</last-name>
<first-name>Kalpana</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Simulated neural circuitry is trained using sequences of images representing moving objects, such that the simulated neural circuitry recognizes objects by having the presence of lower level object features that occurred in temporal sequence in the images representing moving objects trigger the activation of higher level object representations. Thereafter, an image of an object that includes lower level object features is received, the trained simulated neural circuitry activates a higher level representation of the object in response to the lower level object features from the image, and the object is recognized using the trained simulated neural circuitry.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="112.35mm" wi="96.44mm" file="US08626686-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="175.09mm" wi="112.18mm" file="US08626686-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="161.71mm" wi="95.93mm" file="US08626686-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="167.22mm" wi="85.17mm" file="US08626686-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="129.79mm" wi="145.63mm" file="US08626686-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="131.91mm" wi="139.95mm" file="US08626686-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="151.38mm" wi="139.70mm" file="US08626686-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="149.44mm" wi="122.43mm" file="US08626686-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="182.88mm" wi="140.89mm" orientation="landscape" file="US08626686-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="140.46mm" wi="97.96mm" file="US08626686-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="163.66mm" wi="143.76mm" orientation="landscape" file="US08626686-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATION</heading>
<p id="p-0002" num="0001">This application claims priority to U.S. Provisional Application No. 60/536,261, which was filed Jan. 14, 2004, and is titled &#x201c;INVARIANT OBJECT RECOGNITION,&#x201d; and U.S. application Ser. No. 11/035,412, which was filed Jan. 14, 2005, and is titled &#x201c;INVARIANT OBJECT RECOGNITION.&#x201d; These applications are incorporated by reference.</p>
<heading id="h-0002" level="1">TECHNICAL FIELD</heading>
<p id="p-0003" num="0002">This description relates to invariant object recognition.</p>
<heading id="h-0003" level="1">BACKGROUND</heading>
<p id="p-0004" num="0003">Object recognition is a cornerstone of the function of visual systems, whether neural or manmade. This capacity is indispensable for systems used to review, for example, visual images stored in a computer system or for the function of a sensorimotor system operating in an external world, whether for navigation in complex terrain or rapid identification of the properties of food, friend, or foe.</p>
<heading id="h-0004" level="1">SUMMARY</heading>
<p id="p-0005" num="0004">In one aspect, simulated neural circuitry is trained using sequences of images representing moving objects, such that the simulated neural circuitry recognizes objects by having the presence of lower level object features that occurred in temporal sequence in the images representing moving objects trigger the activation of higher level object representations. Thereafter, an image of an object that includes lower level object features is received, the trained simulated neural circuitry activates a higher level representation of the object in response to the lower level object features from the image, and the object is recognized using the trained simulated neural circuitry.</p>
<p id="p-0006" num="0005">Implementations may include one or more of the following features. For example, the images representing moving objects may represent movement of the objects between different starting and ending positions, rotation of the objects, and/or dilation of the objects.</p>
<p id="p-0007" num="0006">The simulated neural circuitry may include elements having multiple input branches and training the simulated neural circuitry may include initializing connections between elements of the neural circuitry, exposing the simulated neural circuitry to sequences of images representing moving objects, and strengthening consecutively active connections at a common input branch as images of the objects simulate movement of the objects through a visual field of the simulated neural circuitry.</p>
<p id="p-0008" num="0007">Furthermore, the simulated neural circuitry may be configured such that strengthening of connections jointly strengthens inputs which co-occur within a specified window in time. Additionally or alternatively, activity at one connection to an input branch may cause effects on the input branch that remain active for a period of time spanning the duration of more than a single image in the sequence of images. The simulated neural circuit also may be configured such that connections between different simulated neuronal elements have different temporal properties, with some weakening and some strengthening with repeated input of a given frequency.</p>
<p id="p-0009" num="0008">The simulated neural circuitry may include simulated neuronal elements having dendritic trees. In some implementations, the simulated neuronal elements may have local nonlinear integrative properties. Additionally or alternatively, individual simulated neuronal elements may include multiple input branches, one or more of which may have local nonlinear integrative properties. Simulated neuronal elements also may be configured to homeostatically regulate activity in the simulated neural circuitry.</p>
<p id="p-0010" num="0009">In some implementations, the simulated neural circuitry may include a simulated neuronal element represented as including multiple branches. In such implementations, a particular branch may be configured to activate when the particular branch receives input that exceeds a first threshold, and the simulated neuronal element may be configured to activate when a number of the simulated neuronal element's branches that are activated exceeds a second threshold.</p>
<p id="p-0011" num="0010">The simulated neural circuitry may include multiple processing areas, each of which includes multiple neuronal elements. In some implementations, the processing areas may be arranged in a hierarchical series from a lowest processing area to a highest processing area and feed-forward may be provided from outputs of the neuronal elements of one processing area to inputs of neuronal elements of a higher processing area. Additionally or alternatively, higher processing areas may issue feedback to lower processing areas.</p>
<p id="p-0012" num="0011">In some implementations, the simulated neural circuitry may function as a self-organizing content-addressable memory.</p>
<p id="p-0013" num="0012">In another aspect, simulated neural circuitry includes at least a first processing area of neuronal elements and a second processing area of neuronal elements. In order to train the simulated neural circuitry to achieve invariant object recognition, connections between simulated neuronal elements of the first processing area and simulated neuronal elements of the second processing area are initialized. A series of different images of an object then are received at the simulated neural circuitry, and, in response, initial encoded representations of the object are generated in the first processing area by activating one or more of the simulated neuronal elements of the first processing area for each received image of the object. The initial encoded representations of the object are transmitted from the first processing area to the second processing area by transmitting signals from active simulated neuronal elements of the first processing area to the simulated neuronal elements of the second processing area to which the active simulated neuronal elements of the first area are connected, and connections to an individual simulated neuronal element of the second processing area are strengthened when the connections to the individual simulated neuronal element of the second processing area are consecutively active.</p>
<p id="p-0014" num="0013">In some implementations, connections to an individual simulated neuronal element of the second processing area may be strengthened when the connections to the individual simulated neuronal element of the second processing area are active within a specified window of time.</p>
<p id="p-0015" num="0014">In yet another aspect, an input image of an object is received at simulated neural circuitry that includes a hierarchy of processing areas and that has been trained to recognize one or more objects. In response, an initial encoded representation of the object is generated. The initial encoded representation of the object then is transformed into an invariant output representation of the object by propagating the initial encoded representation of the object through the hierarchy of processing areas such that representations of the object generated in each succeeding processing area are increasingly less variant than the initial encoded representation. The object is recognized based on the invariant output representation of the object.</p>
<p id="p-0016" num="0015">In still another aspect, a sequence of images of an object is received at simulated neural circuitry. Using the sequence of images, the simulated neural circuitry is trained to generate an encoded representation of the object that includes a higher level feature triggered by recognizing a temporal sequence of lower level features of the object during the training. A corpus of images is searched by supplying each image of the corpus to the simulated neural circuitry, and images from the corpus of images are designated as being images of the object when an output of the simulated neural circuitry produced as a result of an image being supplied to the simulated neural circuitry matches the encoded representation of the object to within a predetermined threshold.</p>
<p id="p-0017" num="0016">In some implementations, the sequence of images may include a video of the object.</p>
<p id="p-0018" num="0017">The techniques described may have applications in, for example, automated image and/or video searching and robotics. In addition, implementations of any of the techniques described may include a method or process, an apparatus or system, or a computer program embodied on a tangible computer-readable medium. The details of particular implementations are set forth in the accompanying drawings and the description below. Other features will be apparent from the description, including the drawings, and from the claims.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">DESCRIPTION OF DRAWINGS</heading>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram of an object recognition system.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIGS. 2A-2C</figref>, <b>3</b>A, <b>3</b>B and <b>4</b>-<b>6</b> are block diagrams that visually represent invariant object recognition mechanisms.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 7</figref> is a block diagram of a neural circuit of the system of <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 8</figref> is a block diagram of a cell of the neural circuit of <figref idref="DRAWINGS">FIG. 7</figref>.</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 9</figref> is a diagram of an example of a process for training a neural circuit to achieve invariant object recognition.</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 10</figref> is a flow chart of a process for recognizing an object in an input image.</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 11</figref> is a block diagram, of an object recognition system.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0006" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0026" num="0025">Natural visual systems take in raw sensory data, which typically shifts as an object or observer moves in the world, resulting in a sequence of images moving across the visual sensory array (the retina). These visual systems must then transform the patterns of input into an invariant representation associated with the identity of the object, enabling a fast decision to be made about appropriate reaction. Thus, in nature, fast neural visual object recognition involves transforming the sequence of patterns representing an object on the retina into a stable pattern that uniquely encodes the object itself. Consequently, such natural systems may be viewed as a self-organizing, content-addressable memory in which the retinal image or low level preprocessor encoding of an image or object quickly triggers the activation of a high level object representation.</p>
<p id="p-0027" num="0026">The mammalian cortex includes primary areas which receive direct retinal input (after it has been preprocessed in a relay station known as the &#x201c;thalamus&#x201d;). As a result, the pattern on the visual sensor surface that is activated in response to perception of an apple may be entirely different depending upon whether the apple is close to or far from the viewer. Nevertheless, the pattern of activity triggered in the higher association areas ultimately is relatively invariant to the scale and/or position of the object.</p>
<p id="p-0028" num="0027">Conversely, though an apple and a rock of similar size and shape may trigger highly overlapping initial images on visual sensors (e.g., the retina), these two objects nevertheless may trigger different patterns in higher cortical areas. This illustrates the power of these natural object recognition systems in which overlapping representations of different objects of similar size, location and shape in lower areas trigger activation of different representations in higher areas, while entirely non-overlapping and dissimilar representations of the same object in different positions or orientations in lower areas trigger the same invariant pattern in higher areas of the system.</p>
<p id="p-0029" num="0028">Select aspects of these natural visual systems may be used to implement powerful visual object recognition systems. Such systems may be employed to perform visual object recognition for the purpose of, for example, reviewing visual images stored in a computer system, reading artificial images in a virtual world (e.g., Second Life), performing visual searches, or recognizing what is in an image. Particular implementations of these systems may be used in searching a corpus of still or video images to find an object that matches an object for which a still or moving image has been provided. Other applications may include enabling a missile tracking system to discriminate a target from noise and other interfering images, or assisting a robotic vehicle that is exploring a hostile or distant environment and needs to navigate in close quarters in a terrain filled with objects to identify and characterize the properties of both targets and obstacles. In addition, an autonomous commercial vehicle navigation system may use the system to distinguish the identity of moving objects, while an artificial visual system aiding the blind may use the system to identify nearby objects. Other applications include face recognition and character recognition for handwritten characters, in all languages.</p>
<p id="p-0030" num="0029">Many conventional computer vision algorithms for object recognition use a computationally-intensive, three-part strategy: first, the presence of an object is identified, with some form of figure-ground segregation methods; second, the object is transformed into a centered and normalized reference frame (object-centered coordinates) so that it may be compared with stored exemplars; and third, the centered and normalized object is then matched to a template or a set of related templates, or compared with object templates using basis-function or principal component methods. In these systems, the figure-ground and normalization procedures may require significant computation and/or processing power. In addition, iterative comparison of the normalized object of interest with all of the stored objects also may require significant computation and/or processing power. Consequently, these systems may not be suitable for real-time applications.</p>
<p id="p-0031" num="0030">Simulated neural circuitry for object recognition, however, may not require a figure-ground operation to identify an object. Furthermore, such circuitry does not require geometric transformation in order to reach a normalized and centered coordinate system. Accordingly, such simulated neural circuits may be suitable for real-time applications.</p>
<p id="p-0032" num="0031">Two particular aspects of neural circuitry may be used to confer great power to these simulated neural circuits. First, synaptic connections mediating inputs which arrive at a branch within a window in time are mutually strengthened. Neuronal dendritic branches may &#x201c;fire&#x201d; if a threshold amount of input arrives within a window in time. Once activated, synaptic connections remain primed to strengthen upon branch firing within a window in time. Taken together, these properties provide that a connection between an axon and a dendritic branch that was active a short while ago remains ready to strengthen upon firing of the dendritic branch, even if the dendritic branch fires in response to activity in a different axon at the synapse. In this way, separate features active during temporally adjacent views of an object within a time window become wired to the same neuronal element (or to the same dendritic branch of the neuronal element). Thus, the time window allows features encoding successive view of objects in a lower area to be jointly wired to the same cell in a higher area.</p>
<p id="p-0033" num="0032">Secondly, dendritic branches may act as quasi-isolated non-linear elements. In more traditional neural networks, neuronal elements sum input in a simple manner, with linear summation of inputs passed through a thresholding function in a classic neural network unit. By contrast, neuronal elements may be represented as branched &#x201c;dendritic&#x201d; trees with electrically active nonlinear branches receiving input. Neuronal input to a branch over a period of time is integrated and can &#x201c;fire&#x201d; that branch, that is trigger a locally generated excitatory event, so that each branch acts as a quasi-isolated nonlinear element. The reconception of neurons as branched structures with nonlinear active processes in each branch greatly enhances the power of neuronal elements as computational units.</p>
<p id="p-0034" num="0033">Referring to <figref idref="DRAWINGS">FIG. 1</figref>, an object recognition system <b>100</b> employs a neural circuit <b>105</b> that includes an input unit <b>115</b>, a hierarchy of processing areas <b>116</b>, and an output unit <b>117</b> to recognize objects <b>110</b>. Initially, the neural circuit <b>105</b> is trained to recognize different objects by exposing the input unit <b>115</b> of the neural circuit to sequences of time varying images <b>120</b> of the different objects. In some implementations, the sequences of time varying training images <b>120</b> of the objects may be provided to the neural circuit <b>105</b> by a trainer <b>125</b>.</p>
<p id="p-0035" num="0034">In response to receiving a sequence of time varying images of a particular object <b>120</b>, patterns of activity signifying encoded representations of the particular object in the sequence of time varying images <b>120</b> are generated in the input unit <b>115</b> and transmitted to the hierarchy of processing areas <b>116</b>. As described in more detail below, during the training process, components of the hierarchy of processing areas <b>116</b> are wired (i.e., connections are formed, strengthened, weakened, and/or severed) such that, as encoded representations of the particular object within the sequence of time varying images <b>120</b> propagate through the hierarchy of processing areas <b>116</b>, increasingly invariant patterns of activity are generated in each successive processing area. In this manner, the neural circuit <b>105</b> is trained such that exposure to an image of the particular object results in the same (or a substantially similar) pattern of activity in the highest processing area of the hierarchy of processing areas <b>116</b> and the output unit <b>117</b> regardless of, for example, the object's size, shape, orientation or location within the image.</p>
<p id="p-0036" num="0035">Once the neural circuit <b>105</b> is trained, the neural circuit <b>105</b> may be used to detect and/or identify objects <b>110</b> presented to the input unit <b>115</b> of the system from, for example, image files or a live input device such as a camera. For example, as noted above, the system may be used in searching a corpus of still or video images to find an object that matches an object for which a still or moving image has been provided.</p>
<p id="p-0037" num="0036">In some implementations, training of the neural circuit <b>105</b> may be a discrete process that occurs prior to using the neural circuit to identify objects. In other implementations, the neural circuit <b>105</b> may continue to be trained even as it is used to identify objects.</p>
<p id="p-0038" num="0037">Aspects of the neural circuit <b>105</b> are based upon the circuit elements, wiring architecture, and connection (synaptic) properties of the mammalian cortical hierarchy, a neural structure which accomplishes tasks such as object recognition in real-world environments rapidly, and which encodes representations of visual objects which are invariant with respect to changes in object size, orientation, and position in the visual field. The mammalian cortical hierarchy functions as a content-addressable memory, in that simply inputting a visual image of an object causes a cascade of feed-forward and feedback signals which result in increasingly invariant representations of the object within each succeeding cortical area.</p>
<p id="p-0039" num="0038">The neural circuit <b>105</b> is structured similarly to the mammalian cortical hierarchy in that it has a number of simulated neuronal elements that are arranged in a hierarchy of areas <b>116</b>. In response to receiving an input image of an object, a pattern of activity is generated in the first area of the neural circuit <b>105</b>. This pattern of activity in the first area is then transformed in the second area, and retransformed in the third area, and so on, such that the pattern of activity that ultimately reaches the highest area of the neural circuit <b>105</b> results in an invariant representation of the object.</p>
<p id="p-0040" num="0039">The neural circuit <b>105</b> includes neuronal elements that have the local, nonlinear integrative properties of neurons and may be implemented in hardware or software. For example, the neural circuit <b>105</b> may be implemented as a software module or as a module of synthetic neural circuitry rendered in very large scale integrated circuitry (VLSI) or a field-programmable gate array (FPGA) that is configured to receive visual (or other sensory) raw data that has been processed through a topographically organized feature detector. During a period of unsupervised learning that involves the presentation of common objects moving through the environment, the neural circuit <b>105</b> is configured to adjust the connection strengths between neuronal units so that, upon later presentation of a familiar object, a distributed pattern representing an invariant encoding of the object is activated in a high-order area of the neural circuit <b>105</b>.</p>
<p id="p-0041" num="0040">In some implementations of the neural circuit <b>105</b>, the primary flow of information is feed-forward. However, implementations of the neural circuit <b>105</b> also may include feedback and intra-area input. As with mammalian sensory cortices, the feed-forward connections in a cascade of several processing areas within the neural circuit <b>105</b> may be self-organized during the course of early experience. That is to say, the wiring of the neural circuit <b>105</b> may be established by exposure to the environment (e.g., sequences of images of objects), rather than by some supervised method.</p>
<p id="p-0042" num="0041">Before providing a more detailed explanation of how the neural circuit <b>105</b> is organized and trained, an overview of the basic concepts of neural circuit operation is provided with respect to <figref idref="DRAWINGS">FIGS. 2A-2C</figref>, <b>3</b>A, <b>3</b>B and <b>4</b>-<b>6</b>.</p>
<p id="p-0043" num="0042"><figref idref="DRAWINGS">FIGS. 2A-2C</figref> illustrate the encoding of a visual image in the primary, or initial, visual area. In particular, <figref idref="DRAWINGS">FIGS. 2A-2C</figref> present a simplified example of how an object (in this case a triangle) may be encoded in a primary processing area (analogous to an area of the cerebral cortex, e.g., V<b>1</b>).</p>
<p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. 2A</figref> shows a first image <b>200</b> of a triangle at a first time (&#x201c;Time <b>1</b>&#x201d;) and a second image <b>205</b> of the triangle at a second time (&#x201c;Time <b>2</b>&#x201d;). At the second time, the triangle is closer to the viewer than at the first time and hence has dilated (expanded) on the retina or camera. The first image <b>200</b> includes edges <b>202</b>A, <b>202</b>B and <b>202</b>C, and the second image <b>205</b> includes edges <b>207</b>A, <b>207</b>B and <b>207</b>C. While <figref idref="DRAWINGS">FIG. 2A</figref> shows both images <b>200</b> and <b>205</b>, it should be understood that the two images are separately displayed at different moments in time.</p>
<p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. 2B</figref> illustrates an encoded representation of the first image <b>200</b> in a processing area. In particular, for illustrative purposes, the processing area is represented by an 8-by-8 grid <b>210</b> of simulated neural cells and the complex shape of the first image <b>200</b> is encoded as a set of active simulated cells in grid <b>210</b>. As illustrated in <figref idref="DRAWINGS">FIG. 2B</figref>, simulated cells <b>200</b>A, <b>200</b>B, and <b>200</b>C are activated in response to exposure to the first image <b>200</b>. Each active simulated cell represents a different edge of the image <b>200</b>, where each edge has a different location and orientation. More particularly, cell <b>200</b>A represents edge <b>202</b>A, cell <b>200</b>B represents edge <b>202</b>B and cell <b>200</b>C represents edge <b>202</b>C.</p>
<p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. 2C</figref> illustrates an encoded representation of the second image <b>205</b> in the same processing area. As shown in <figref idref="DRAWINGS">FIG. 2C</figref>, the second image <b>205</b> is similarly represented by simulated neural cells <b>205</b>A, <b>205</b>B and <b>205</b>C in a second grid <b>215</b>, where the cells <b>205</b>A, <b>205</b>B and <b>205</b>C represent, respectively, the edges <b>207</b>A, <b>207</b>B and <b>207</b>C. There is no overlap between the encoding of the first image <b>200</b> of the triangle at Time <b>1</b> and the encoding of the second image <b>205</b> of the triangle at Time <b>2</b>, even though both images represent the same object. This is because the second image <b>205</b> of the triangle at Time <b>2</b> exhibits a different location and size from the first image <b>200</b> of the triangle at Time <b>1</b> and, therefore, generates activity in different simulated cells. Indeed, had image <b>205</b> been larger, one or more of its edges could have activated multiple cells such that its representation and that of image <b>200</b> would have included different numbers of active cells.</p>
<p id="p-0047" num="0046"><figref idref="DRAWINGS">FIGS. 3A and 3B</figref> illustrate the desired invariantly encoded representations of the images <b>200</b> and <b>205</b> in a higher processing area. More particularly, <figref idref="DRAWINGS">FIG. 3A</figref> illustrates an encoded representation of the first image <b>200</b> of the triangle in the higher processing area and <figref idref="DRAWINGS">FIG. 3B</figref> illustrates an encoded representation of the second image <b>205</b> of the triangle in the higher processing area. As explained in greater detail below, the representations of the images <b>200</b> and <b>205</b> in the primary processing area are transmitted by simulated neural circuitry to the higher processing area where they are transformed into different representations of the images <b>200</b> and <b>205</b>. As illustrated in <figref idref="DRAWINGS">FIGS. 3A and 3B</figref>, the higher processing area is represented as another 8-by-8 grid of simulated neural cells, with the first image <b>200</b> of the triangle at Time <b>1</b> being represented by active simulated cells <b>300</b>A, <b>300</b>B and <b>300</b>C in a grid <b>310</b>, and the second image <b>205</b> of the triangle at Time <b>2</b> being represented by the same active simulated cells <b>305</b>A, <b>305</b>B and <b>305</b>C in a grid <b>315</b>. <figref idref="DRAWINGS">FIGS. 3A and 3B</figref> are illustrative of how the representation of an object in higher processing areas of the neural circuit becomes less dependent upon the object's position and scale, and more associated with the identity of the object itself. That is to say, in the higher processing area, the encoding stays invariant as an object approaches, or otherwise changes size and/or orientation. This results in a useable representation of the identity and presence of an object.</p>
<p id="p-0048" num="0047"><figref idref="DRAWINGS">FIGS. 4-6</figref> illustrate how the wiring at the single neuronal unit level to enable the invariant representation of the first and second images <b>200</b> and <b>205</b> illustrated in <figref idref="DRAWINGS">FIGS. 3A and 3B</figref> arises given the inputs illustrated in <figref idref="DRAWINGS">FIG. 2A</figref> and the dissimilar representations of the first and second images <b>200</b> and <b>205</b> illustrated in <figref idref="DRAWINGS">FIGS. 2B and 2C</figref>.</p>
<p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. 4</figref> shows a single row <b>400</b> of simulated cells from the primary processing area and a single row <b>405</b> of simulated cells from a higher processing area at Time <b>1</b>. As shown, a single simulated cell <b>410</b> (e.g., a cell representing the point <b>200</b>A of <figref idref="DRAWINGS">FIG. 2B</figref>) in the row <b>400</b> representing the primary processing area is active. The simulated cell <b>410</b> includes outputs <b>415</b> and <b>420</b> (&#x201c;axons&#x201d; in the context of biological neurons) that are connected to provide input to branches <b>425</b> and <b>430</b> (&#x201c;dendrites&#x201d; in the context of biological neurons) of simulated cells <b>435</b> and <b>440</b> (e.g., a cell representing the point <b>300</b>A of <figref idref="DRAWINGS">FIG. 3A</figref>) of the row <b>405</b> that represents the higher visual area. Because the simulated cell <b>410</b> is active, a signal is provided through the outputs <b>415</b> and <b>420</b> to the branches <b>425</b> and <b>430</b>. It should be understood that <figref idref="DRAWINGS">FIG. 4</figref> is exemplary, and that a simulated cell actually may have hundreds of branches, each of which may have on the order of 100 connections to other simulated cells such that a simulated cell may have well over 10,000 inputs. Additionally or alternatively, the branches of a simulated cell may include numerous sub-branches such that the branches of the individual simulated cell exhibit their own hierarchical structure and function.</p>
<p id="p-0050" num="0049">As also shown in <figref idref="DRAWINGS">FIG. 4</figref>, a different simulated cell <b>445</b> (e.g., a simulated cell representing the point <b>205</b>A of <figref idref="DRAWINGS">FIG. 2C</figref>) of the row <b>400</b> representing the primary processing area has an output <b>450</b> connected to the branch <b>430</b>, and an output <b>455</b> connected to a branch <b>460</b> of a simulated cell <b>465</b>. Since the simulated cell <b>445</b> is not active, a signal is not provided through the outputs <b>450</b> and <b>455</b>.</p>
<p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. 5</figref> illustrates the rows <b>400</b> and <b>405</b> of simulated cells at Time <b>2</b>. As illustrated in <figref idref="DRAWINGS">FIG. 5</figref>, the result of the movement of the image of the triangle from the first image <b>200</b> at Time <b>1</b> to the second image <b>205</b> at Time <b>2</b> is that the simulated cell <b>410</b> is no longer active at Time <b>2</b> and the simulated cell <b>445</b> is active at Time <b>2</b>. Since the simulated cell <b>445</b> is active, a signal is provided through the outputs <b>450</b> and <b>455</b>. Because the outputs <b>420</b> and <b>450</b> are sequentially active, branch <b>430</b> remains active for an extended time period. The system is configured such that all connections to a branch that have been active when outputs connected to the branch have been sequentially active are strengthened, and, as a result, the synaptic connections of the outputs <b>420</b> and <b>455</b> to the branch <b>430</b> are strengthened. That is to say, the input resulting from the output <b>420</b> being active at Time <b>1</b> remains ready to strengthen its connection to the branch <b>430</b> at Time <b>2</b>. For purposes of determining whether to strengthen the connections between the outputs and the branch, the activation of an output, in effect, remains active for a period of time spanning the duration of more than a single image in a sequence of images.</p>
<p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. 6</figref> illustrates a blowup of the branch <b>430</b> of the simulated cell <b>440</b> that receives input from the output <b>420</b> at Time <b>1</b> and from the output <b>450</b> at Time <b>2</b>. The black circles <b>600</b> and <b>605</b> indicate newly strengthened synaptic connections between the outputs and the branch. As a result of the strengthening of these connections, when either of simulated cells <b>410</b> and <b>445</b> in the primary processing area is activated, the simulated cell <b>440</b> in the higher area also will be activated. As such, a shifting sequence of patterns of input in the primary processing area becomes associated with a constant (i.e., invariant) pattern of activity in the higher area, thereby resulting in the desired invariant encoding of the object. This process is iterated from area to area to area and results in the self-organization of wiring which provides for the desired invariant encoding in successively higher areas. Furthermore, this process of self-organized wiring may be said to result in content-addressable memory in that the same content (e.g., object) results in the generation of the same pattern of activity in the higher processing areas, irrespective of the location, shape, size, or orientation of the object.</p>
<p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. 7</figref> illustrates an implementation of the neural circuit <b>105</b> of <figref idref="DRAWINGS">FIG. 1</figref>. As discussed above, the neural circuit <b>105</b> may be implemented in software, which may run, for example, on a parallel processing hardware platform, or in hardware, such as in VLSI or an FPGA.</p>
<p id="p-0054" num="0053">The neural circuit <b>105</b> includes the input unit <b>115</b>, which, as discussed above, obtains an input in the form of an image (e.g., when the input unit is a camera) or a digital file representative of an image.</p>
<p id="p-0055" num="0054">An initial processing module <b>700</b> receives an output from the input unit <b>115</b> and converts that output into a form useable by subsequent modules of the neural circuit <b>105</b>. For example, the initial processing module <b>700</b> may convert one frame of a moving image into a set of any order (e.g., 8&#xd7;8 or 1000&#xd7;1000) output signals that represent different aspects of each pixel of the image or aspects of larger regions of the image or of the entire image. The properties conveyed by each pixel may simply be brightness of a given hue at that point, as in a camera, or the output of a filter such as a &#x201c;center-surround&#x201d; filter which responds strongly to a spot of white on a black background, but not to a uniform field of white.</p>
<p id="p-0056" num="0055">The output from the initial processing module <b>700</b> is supplied to a first neural module <b>705</b> that supplies its output to a second neural module <b>710</b> and that receives feedback from the second neural module <b>710</b>. The second neural module <b>710</b> supplies its output to, and receives feedback from, a third neural module <b>715</b>, that supplies its output to, and receives feedback from, a fourth neural module <b>720</b>. The fourth neural module <b>720</b> produces an output that is supplied to an output unit <b>725</b>. As an input signal representing (or including) a visual object propagates from the input unit <b>115</b> through the initial processing module <b>700</b>, the first neural module <b>705</b>, the second neural module <b>710</b>, the third neural module <b>715</b>, and the fourth neural module <b>720</b>, the signal is transformed by each module along the way such that the pattern of activity in the output unit <b>725</b> represents (or includes) an invariant representation of the visual object.</p>
<p id="p-0057" num="0056">Each of the neural modules <b>705</b>, <b>710</b>, <b>715</b>, and <b>720</b> includes an array of simulated neuronal cells, such as, for example, a 16-by-16 array. Though not shown in <figref idref="DRAWINGS">FIG. 7</figref>, in addition to receiving feed-forward input from a preceding module and feedback input from a subsequent module, a neuronal cell of a neural module also may receive input from other neuronal cells of the same module.</p>
<p id="p-0058" num="0057">As shown in <figref idref="DRAWINGS">FIG. 8</figref>, each simulated neuronal cell <b>800</b> includes a hierarchy of input branches <b>805</b>, each of which is connected to multiple inputs, and an output <b>810</b>. For example, in a particular implementation, a simulated cell may include 100 input branches, each of which is connected to receive 100 inputs. A threshold amount of input (at a particular instant or during a window of time) from a set of co-active inputs causes an individual branch to fire, and, in turn, the firing of a threshold number of the input branches <b>805</b> (at a particular instant or during a window of time) causes the neuron <b>800</b> to fire. In some implementations, the threshold amount of activity required to fire individual branches and/or neurons may be dynamically (e.g., automatically) adjusted to achieve a desired (e.g., homeostatic) level of activity and/or performance in the neural circuit. Such dynamic adjusting of the threshold levels required to trigger branches to activate and neurons to fire may be referred to as homeostatic thresholding.</p>
<p id="p-0059" num="0058">The inputs supplied to the input branches <b>805</b> may include a number of input types, including feed-forward inputs <b>815</b>, intra-array inputs <b>820</b>, and feedback inputs <b>825</b>. Feed-forward inputs <b>815</b> are the outputs from the initial processing module <b>700</b>, in the case of a simulated cell from the first neural module <b>705</b>, or the outputs from the simulated neuronal cells of a preceding neural module, in the case of the neural modules <b>710</b>, <b>715</b>, and <b>720</b>.</p>
<p id="p-0060" num="0059">Intra-array inputs <b>820</b> are the outputs from simulated neuronal cells within the same array. Intra-array inputs <b>820</b> also are generally distributed topographically such that intra-array inputs to a simulated neuronal cell often may be provided by neighboring simulated neuronal cells.</p>
<p id="p-0061" num="0060">Feedback inputs <b>825</b> are the outputs of the simulated neuronal cells of a succeeding neural module. Feedback inputs <b>825</b> tend to be much less topographical than feed-forward inputs <b>815</b> or intra-array inputs <b>820</b>, such that the feedback inputs <b>825</b> into an array are much more widely distributed. As illustrated in <figref idref="DRAWINGS">FIG. 7</figref>, the simulated neuronal cells of the fourth neural module <b>720</b> do not receive feedback inputs <b>825</b>.</p>
<p id="p-0062" num="0061">Each array also may include simulated inhibitory cells, referred to as simulated interneuronal cells, that tend to quench activity. Each of these simulated interneuronal cells receives inputs from simulated neuronal cells and from other simulated interneuronal cells. The interneuronal cells may be used to provide local lateral inhibition to sharpen representations at each succeeding neural module.</p>
<p id="p-0063" num="0062">In general, the outputs of the simulated interneuronal cells tend to be used as intra-array inputs <b>820</b>, rather than as feed-forward inputs <b>815</b> or feedback inputs <b>825</b>. In another implementation, each cell of the array includes a simulated neuronal element and a simulated interneuronal element.</p>
<p id="p-0064" num="0063">The system may be initialized with small random connection weights between inputs and input branches. Then, the system may be trained by exposing the system to a library of sequences of images of visual objects. As each object moves through the visual field in the manner described above, synapses will change strength through a process in which active connections are strengthened and inactive connections tend to weaken. The objects are presented and represented in many different starting and ending positions, so that the system is exposed to objects in many different random trajectories. During this training phase, synapse modifications may be ongoing, and it is in this sense and during this time that the wiring of the neural circuitry self organizes. Although not illustrated as such in <figref idref="DRAWINGS">FIG. 8</figref>, the input branches <b>805</b> of the neuron <b>800</b> may include numerous sub-branches connected to various inputs such that the branches <b>805</b> fire in response to a threshold level of activity (at a particular instant or during a window of time). In this manner, individual neurons may exhibit a hierarchical structure and function.</p>
<p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. 9</figref> is a diagram of an example of a process <b>900</b> for training (e.g., wiring) a neural circuit, such as, for example, the neural circuit <b>105</b> of <figref idref="DRAWINGS">FIGS. 1 and 7</figref>, to achieve invariant object recognition. That is to say, <figref idref="DRAWINGS">FIG. 9</figref> is a diagram of an example of a process <b>900</b> for training a neural circuit to recognize an object (or an image, or an object within an image) irrespective of the object's location, size, scale, and/or orientation. The neural circuit includes an initial processing area for generating an initial encoded representation of an input image and one or more additional simulated neural modules for transforming an initial encoded representation of an input image into a representation of the image that is invariant with respect to the input image's location, size, scale, and/or orientation. Before the process <b>900</b> begins, connections already may exist between simulated neuronal cells of the initial processing area and simulated neuronal cells of the one or more succeeding simulated neural modules. However, as discussed above in connection with <figref idref="DRAWINGS">FIGS. 4-6</figref>, new connections may be formed and the initial connections may be strengthened, weakened, and/or severed during the training process to achieve invariant object recognition.</p>
<p id="p-0066" num="0065">The process <b>900</b> is an iterative process that involves repeatedly exposing the neural circuit to different images of the same object having various different locations, orientations, and sizes. The process <b>900</b> begins by exposing the neural circuit to an image of an object (<b>905</b>). The neural circuit receives the image of the object (<b>910</b>) at, for example, an input unit such as input unit <b>115</b> of <figref idref="DRAWINGS">FIGS. 1 and 7</figref>. Thereafter, an initial encoded representation of the object is generated by activating one or more simulated cells in an initial processing module of the neural circuit (<b>915</b>). The initial encoded representation of the object is then transmitted to a second processing module, such as, for example, the first neural module <b>705</b> of <figref idref="DRAWINGS">FIG. 7</figref>, by generating output signals on the connections between the activated simulated cells of the initial processing module and simulated cells of the second processing module (<b>920</b>). At approximately the same time, another image, in which the object exhibits a different location, orientation, and/or size, is exposed to the neural circuit, and a second iteration of the process <b>900</b> begins.</p>
<p id="p-0067" num="0066">Meanwhile, the output signals from the activated simulated cells of the initial processing module are received by the simulated cells of the second processing module that are connected to the activated simulated cells of the initial processing module (<b>925</b>) and the initial encoded representation of the object is transformed into a new encoded representation of the object by activating one or more of the simulated cells of the second processing module based on the received output signals (<b>930</b>).</p>
<p id="p-0068" num="0067">In addition, for each simulated synapse that received an output signal from the initial processing module, a determination is made as to whether the simulated synapse was active during the previous iteration of the process <b>900</b> (or whether the simulated synapse was active within a given window of time) (<b>935</b>). If the simulated synapse was not active during the previous iteration (or within the given window of time), the connections at the synapse may be left unmodified or they may be weakened (<b>940</b>). In contrast, if the simulated synapse was active during the previous iteration (or within the given window of time), both the previously active connection and the currently active connection at the synapse are strengthened (<b>945</b>). As discussed above in connection with <figref idref="DRAWINGS">FIGS. 4-6</figref>, this practice of strengthening connections at a synapse based on consecutive activity may result in the transformation of divergent patterns of input in the primary processing module generated in response to different images of the same object into less variant representations of the object in the second processing module.</p>
<p id="p-0069" num="0068">After the new encoded representation of the object is generated, the representation is transmitted to a succeeding processing area, such as, for example, the second neural module <b>710</b> of <figref idref="DRAWINGS">FIG. 7</figref> (<b>950</b>), where it is received (<b>955</b>) and transformed into yet another encoded representation (<b>960</b>), and where consecutively active connections (or connections that are active within a given window of time) at a synapse also are strengthened (<b>965</b>). The retransformed encoded representation of the object then is transmitted to another succeeding processing module (<b>970</b>). Thereafter, the process of transforming and retransforming encoded representations of the object over multiple processing modules and modifying the connections between simulated cells of adjacent processing modules is continued for each of the remaining processing modules of the simulated neural circuit.</p>
<p id="p-0070" num="0069">As the number of different images of the object to which the neural circuit is exposed increases and the connections between simulated cells of adjacent processing modules are modified, the encoded representations of the object may become increasingly invariant at each subsequent processing area. Eventually, the encoded representation of the object generated in the highest processing area may become relatively invariant irrespective of the location, size, scale and/or orientation of the object in the input image. At this point, the process <b>900</b> may be stopped and repeated for one or more additional objects. (In an alternative approach, the system is trained using intermingled sequences of images of different objects). As the neural circuit is trained to recognize additional objects, the invariant representations of each object may be recorded in a catalog or library of invariant representations such that a later output representation generated by the neural circuit may be identified by comparing the later output representation with the representations recorded in the catalog or library. However, the use of such a catalog or library is not required; other implementations may make use of the system by comparing the representations of target images to the representation of a known image to determine whether the target images include the same object as the known image.</p>
<p id="p-0071" num="0070">In some implementations, additional or alternative mechanisms for modifying synaptic connections may be employed. For example, if an input is active but does not result in the firing of a branch to which it is connected, the connection between the input and the branch may be weakened. In contrast, if an input is active and it results in the firing of the branch to which it is connected, the connection between the input and the branch may be strengthened. As another alternative, if a branch of a neuron is active and results in the firing of the neuron, the significance or weight assigned to the branch for purposes of firing the neuron may be strengthened.</p>
<p id="p-0072" num="0071">After a neural circuit has been trained to recognize one or more objects (e.g., according to the process <b>900</b> of <figref idref="DRAWINGS">FIG. 9</figref>), the neural circuit may be used to recognize one or more objects in an image irrespective of the location, size, scale, and/or orientation of the object in the image. <figref idref="DRAWINGS">FIG. 10</figref> is a flow chart of a process <b>1000</b> for recognizing an object in an input image. The process <b>1000</b> may be performed by a neural circuit having a hierarchy of processing areas that each include one or more simulated neural cells that have been connected to one or more simulated neural cells in a succeeding processing area of the hierarchy as a result of a training process. For example, the process <b>1000</b> may be performed by the neural circuit <b>105</b> of <figref idref="DRAWINGS">FIGS. 1 and 7</figref> after the neural circuit <b>105</b> has been trained.</p>
<p id="p-0073" num="0072">The process <b>1000</b> begins when the neural circuit receives an input image that includes an object (<b>1005</b>). In response to receiving the input image including the object, an initial encoded representation of the object is generated by activating one or more simulated neural cells in an initial processing area (<b>1010</b>). The initial encoded representation of the input image is then iteratively transformed in each processing area of the hierarchy of processing areas by propagating the encoded representation of the image along the connections between the processing areas established during training (<b>1015</b>) Ultimately, a relatively invariant output representation of the image is generated in the highest processing area. The object included in the image is then identified by decoding the output representation of the image generated in the highest processing (<b>1020</b>). In some implementations, the relatively invariant output representation of the image is decoded by comparing the output representation generated in the highest processing area with known output representations of objects, and determining the identity of the object based on the known output representation that is most similar to the output representation generated in the highest processing area.</p>
<p id="p-0074" num="0073">As discussed above, an object recognition system, such as, for example, the object recognition system <b>100</b> of <figref idref="DRAWINGS">FIG. 1</figref>, may have numerous applications. In one example, the object recognition system <b>100</b> may enable image and/or video searching such that a user can search for and locate one or more images and/or videos that include a desired object. Additionally or alternatively, the object recognition system <b>100</b> may enable a user to search for and locate one or more images and/or videos that are similar to an image and/or video provided by the user. For instance, the object recognition system <b>100</b> may be implemented in software on a computer, such as, for example a personal computer (PC) or a Mac, that is connected to the Internet, or another similar content repository, and the object recognition system <b>100</b> may enable a user to locate desired images and/or videos available on the Internet.</p>
<p id="p-0075" num="0074">The following example is illustrative of such an example. A user of the object recognition system <b>100</b> may be an aeronautics enthusiast and may desire to locate images and/or videos that include an airplane, as opposed to other forms of aeronautic transportation (e.g., helicopters, space shuttles, and rockets). In order to locate such images and/or videos, the user may provide one or more images and/or videos of an airplane to the object recognition system <b>100</b> and request that the object recognition system <b>100</b> search for and locate one or more additional images and/or videos of an airplane on the Internet. In response, the object recognition system <b>100</b> may supply the images and/or videos of the airplane to the neural circuit <b>105</b> to train the neural circuit to identify airplanes by generating an invariant pattern of activity in the neural circuit known to correspond to exposure to an airplane. In some implementations, this training may involve, for example, having the system <b>100</b> generate moving, scaled and reoriented versions of the airplane in order to train the neural circuit <b>105</b>. In other implementations, the neural circuit <b>105</b> may have been previously trained using moving, scaled and reoriented images of an airplane from a corpus of images or using available video content. In such implementations, the user may be able to enter a textual search query (e.g., &#x201c;airplane&#x201d;) as opposed to providing one or more images and/or videos of an airplane as the search query.</p>
<p id="p-0076" num="0075">Next, a corpus of additional images and/or videos (e.g., a corpus of images and/or videos available on the Internet) may be accessed and individual images or videos may be input to the neural circuit <b>105</b>. If an input image or video includes an airplane, it will trigger a pattern of activity in the neural circuit that is the same as, or substantially similar to, the pattern of activity known to correspond to an airplane and thus can be identified to the user as an image or video that includes an airplane. When a video including an airplane is identified, the object recognition system <b>100</b> also may be able to identify the time(s) and/frame(s) at which the airplane appears in the video. Because the object recognition system <b>100</b> is able to identify an object based on the pattern of activity generated by the object, the object recognition system <b>100</b> may be able to identify an object more quickly than another object recognition system that employs an algorithmic approach that requires template matching.</p>
<p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. 11</figref> is a block diagram of the object recognition system <b>100</b> of <figref idref="DRAWINGS">FIG. 1</figref> that illustrates how the object recognition system <b>100</b> can be used to search for and locate one or more images and/or videos that include a desired object. As illustrated in <figref idref="DRAWINGS">FIG. 11</figref>, a user interested in locating images and/or videos that include an airplane may provide a video or an input image set <b>1105</b> that includes an airplane to the object recognition system <b>100</b>.</p>
<p id="p-0078" num="0077">The input video or image set <b>1105</b> is then processed by the neural circuit to train the neural circuit to recognize the airplane in the input video or image set <b>1105</b> and an invariant encoded representation of the airplane in the input video or image set <b>1105</b> is generated. The object recognition system <b>100</b> then accesses a number of images from the Internet <b>1110</b>, or another similar content repository, and processes each of the accessed images with the neural circuit. Videos or images that generate encoded representations in the neural circuit that are within a threshold level of similarity to the encoded representation generated in response to the input video or image set <b>1105</b> are then determined to be videos or images that include an airplane and are returned to the user as output videos or images <b>1115</b> and <b>1120</b>. As illustrated in <figref idref="DRAWINGS">FIG. 11</figref>, the object recognition system <b>100</b> identified two of the processed videos or images as videos or images that include an airplane. As shown, the airplanes in the output videos or images <b>1115</b> and <b>1120</b> exhibit different angles, orientations, sizes, and locations. The object recognition system <b>100</b> is nevertheless able to identify each of the output videos or images <b>1115</b> and <b>1120</b> as including an airplane because the object recognition system <b>100</b> has been configured such that videos and/or images of an object generate an invariant representation of the object irrespective of the object's location, size, scale, and/or orientation within the videos or images.</p>
<p id="p-0079" num="0078">Other applications of an object recognition system, such as, for example, the object recognition system <b>100</b> of <figref idref="DRAWINGS">FIG. 1</figref>, arise in the field of robotics. In one example, an object recognition system, such as object recognition system <b>100</b>, embedded within a robot may enable a robot to identify various different objects and obstacles in a foreign environment, thereby assisting the robot as it attempts to navigate and maneuver within the foreign environment.</p>
<p id="p-0080" num="0079">Additionally or alternatively, an object recognition system, such as object recognition system <b>100</b>, embedded in a robot may enable a robot to locate (e.g., recognize) and fetch a desired object, such as, for example, a football. It is important to realize that the object recognition system <b>100</b> may not have been exposed to a football, or a series of time-varying images of a football, during the training process, and yet the object recognition system <b>100</b> still may be capable of recognizing a football. The objective of the training process is to establish connection patterns within the object recognition system <b>100</b> that enable the object recognition system <b>100</b> to recognize a wide variety of objects, not just those objects to which the object recognition system <b>100</b> is exposed during the training process. Such connection patterns may be established within the object recognition system <b>100</b> by exposing the object recognition system <b>100</b> to sequences of time-varying images of a collection of relatively simple shapes or objects, thereby arming the object recognition system with a vocabulary of primitive shapes or objects that enables the object recognition system <b>100</b> to recognize a variety of other (potentially more complex) shapes or objects. After the object recognition system <b>100</b> has been trained and the connection patterns have been established, the object recognition system <b>100</b> may be exposed to a single image of a football, and the resulting pattern of activity in the highest-level processing area of the object recognition system <b>100</b> may be identified as corresponding to a football. Thereafter, even though the object recognition system only has been exposed to a football once, the object recognition system is able to recognize subsequent exposures to footballs by remembering the pattern of activity generated in the highest level of the processing areas in response to the initial exposure to the football.</p>
<p id="p-0081" num="0080">The systems and techniques described above are not limited to any particular hardware or software configuration. Rather, they may be implemented using hardware, software, or a combination of both. In addition, the methods and processes described may be implemented as computer programs that are executed on programmable computers comprising at least one processor and at least one data storage system. The computer programs may be implemented in a high-level compiled or interpreted programming language, or, additionally or alternatively, the computer programs may be implemented in assembly or other lower level languages, if desired. Such computer programs typically will be stored on computer-usable storage media or devices (e.g., CD-Rom, RAM, or magnetic disk). When read into a processor of a computer and executed, the instructions of the programs may cause a programmable computer to carry out the various operations described above.</p>
<p id="p-0082" num="0081">A number of implementations have been described. Nevertheless, it will be understood that various modifications may be made. For example, while the techniques have been discussed in terms of visual object recognition, the techniques may be applied in other settings where a temporal sequence of patterns all represent the same object in an abstract sense. Furthermore, useful results still may be achieved if steps of the disclosed techniques are performed in a different order and/or if components in the disclosed systems are combined in a different manner and/or replaced or supplemented by other components.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A computer-implemented method of object recognition, the method comprising:
<claim-text>training simulated neural circuitry using sequences of images representing moving objects, such that the simulated neural circuitry recognizes objects by having the presence of lower level object features that occurred in temporal sequence in the images representing moving objects trigger the activation of higher level object representations;</claim-text>
<claim-text>receiving an image of an object, the image including lower level object features; and</claim-text>
<claim-text>recognizing the object using the trained simulated neural circuitry, wherein the trained simulated neural circuitry activates a higher level representation of the object in response to the lower level object features from the image,</claim-text>
<claim-text>wherein the simulated neural circuitry includes simulated neuronal elements with local nonlinear integrative properties.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the simulated neural circuitry includes elements having multiple input branches and training the simulated neural circuitry comprises:
<claim-text>initializing connections between elements of the neural circuitry; and</claim-text>
<claim-text>exposing the simulated neural circuitry to sequences of images representing moving objects,</claim-text>
<claim-text>wherein the simulated neural circuitry is configured such that, as images of the objects simulate movement of the objects through a visual field of the simulated neural circuitry, consecutively active connections at a common input branch will be strengthened.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the simulated neural circuitry is configured such that strengthening of connections jointly strengthens inputs which co-occur within a specified window in time.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein activity at one connection to the input branch causes effects on the input branch that remain active for a period of time spanning the duration of more than a single image in the sequence of images.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the images represent movement of the objects between different starting and ending positions.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the images represent rotation of the objects about one or more axes.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the images represent dilation of the objects.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a simulated neuronal element includes multiple input branches, one or more of which have local nonlinear integrative properties.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the simulated neural circuitry is configured such that connections between different simulated neuronal elements have different temporal properties, with some weakening and some strengthening with repeated input of a given frequency.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the simulated neural circuitry includes simulated neuronal elements configured to homeostatically regulate activity in the simulated neural circuitry.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the simulated neural circuitry functions as a self-organizing content-addressable memory.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:
<claim-text>the simulated neural circuitry includes a simulated neuronal element represented as including multiple branches,</claim-text>
<claim-text>a particular branch is configured to activate when the particular branch receives input that exceeds a first threshold, and</claim-text>
<claim-text>the simulated neuronal element is configured to activate when a number of the simulated neuronal element's branches that are activated exceeds a second threshold.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the simulated neural circuitry includes neuronal elements of different types.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the simulated neural circuitry includes multiple processing areas, each of which includes multiple neuronal elements.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the processing areas are arranged in a hierarchical series from a lowest processing area to a highest processing area and feed-forward is provided from outputs of the neuronal elements of one processing area to inputs of neuronal elements of a higher processing area.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein each higher processing area issues feedback to a lower processing area or areas.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the neuronal elements include dendritic trees.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. A computer program embodied on a non-transitory computer-readable medium, the computer program including instructions that, when executed, cause a computer to:
<claim-text>train simulated neural circuitry using sequences of images representing moving objects, such that the simulated neural circuitry recognizes objects by having the presence of lower level object features that occurred in temporal sequence in the images representing moving objects trigger the activation of higher level object representations;</claim-text>
<claim-text>receive an image of an object, the image including lower level object features; and</claim-text>
<claim-text>recognize the object using the trained simulated neural circuitry, wherein the trained simulated neural circuitry activates a higher level representation of the object in response to the lower level object features from the image,</claim-text>
<claim-text>wherein the simulated neural circuitry includes simulated neuronal elements with local nonlinear integrative properties.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The computer program of <claim-ref idref="CLM-00018">claim 18</claim-ref> wherein the simulated neural circuitry includes elements having multiple input branches, and the instructions that cause a computer to train the simulated neural circuitry comprise instructions that, when executed, cause a computer to:
<claim-text>initialize connections to input branches of the elements of the neural circuitry; and</claim-text>
<claim-text>expose the simulated neural circuitry to sequences of images representing moving objects,</claim-text>
<claim-text>wherein the computer program further comprises instructions that, when executed, cause a computer to strengthen consecutively active connections at a common input branch as images of the objects simulate movement of the objects through a visual field of the simulated neural circuitry.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The computer program of claim of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the instructions that, when executed, cause a computer to strengthen consecutively active connections at a common input branch include instructions that, when executed, cause a computer to jointly strengthen inputs which co-occur within a specified window in time.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The computer program of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein a simulated neuronal element includes multiple input branches, one or more of which have local nonlinear integrative properties.</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The computer program of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein:
<claim-text>the simulated neural circuitry is configured such that connections between different simulated neuronal elements have different temporal properties, and</claim-text>
<claim-text>the computer program includes instructions that, when executed, cause a computer to weaken some connections and strengthen some connections in response to repeated input of a given frequency.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The computer program of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein:
<claim-text>the simulated neural circuitry includes a simulated neuronal element represented as including multiple branches, and</claim-text>
<claim-text>the computer program includes instructions that, when executed, cause a computer to:
<claim-text>activate a particular branch when the particular branch receives input that exceeds a first threshold, and</claim-text>
<claim-text>activate the simulated neuronal element when a number of the simulated neuronal element's branches that are activated exceeds a second threshold.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. The computer program of <claim-ref idref="CLM-00018">claim 18</claim-ref> wherein:
<claim-text>the simulated neural circuitry includes processing areas arranged in a hierarchical series from a lowest processing area to a highest processing area, and</claim-text>
<claim-text>the computer program includes instructions that, when executed, cause a computer to provide feed-forward from outputs of the neuronal elements of one processing area to inputs of neuronal elements of a higher processing area.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00025" num="00025">
<claim-text>25. The computer program of <claim-ref idref="CLM-00024">claim 24</claim-ref>, wherein the computer program includes instructions that, when executed, cause a computer to issue feedback from each higher processing area to a lower processing area or areas.</claim-text>
</claim>
<claim id="CLM-00026" num="00026">
<claim-text>26. An object recognition system comprising:
<claim-text>means for receiving images; and</claim-text>
<claim-text>simulated neural circuitry implemented by a processor or electronic circuitry for recognizing objects in received images, wherein the simulated neural circuitry is configured to:
<claim-text>during a training process in which sequences of images representing moving objects are exposed to the neural circuit, learn to recognize the objects by having the presence of lower level object features that occur in temporal sequence in the images representing the moving objects trigger the activation of higher level object representations; and</claim-text>
<claim-text>after the training process, recognize an object in a received image that includes lower level object features based on activating a higher level representation of the object in response to the lower level object features from the image,</claim-text>
<claim-text>wherein the simulated neural circuitry includes simulated neuronal elements with local nonlinear integrative properties.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00027" num="00027">
<claim-text>27. The object recognition system of <claim-ref idref="CLM-00026">claim 26</claim-ref>, wherein:
<claim-text>the simulated neural circuitry includes elements having multiple input branches; and</claim-text>
<claim-text>during the training process, the simulated neural circuitry is further configured to:
<claim-text>initialize connections to input branches of elements of the neural circuitry; and</claim-text>
<claim-text>strengthen consecutively active connections at a common input branch.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00028" num="00028">
<claim-text>28. The object recognition system of <claim-ref idref="CLM-00027">claim 27</claim-ref>, wherein the simulated neural circuitry is configured such that strengthening of connections jointly strengthens inputs which co-occur within a specified window in time.</claim-text>
</claim>
<claim id="CLM-00029" num="00029">
<claim-text>29. The object recognition system of <claim-ref idref="CLM-00026">claim 26</claim-ref>, wherein a simulated neuronal element includes multiple input branches, one or more of which have local nonlinear integrative properties.</claim-text>
</claim>
<claim id="CLM-00030" num="00030">
<claim-text>30. The object recognition system of <claim-ref idref="CLM-00026">claim 26</claim-ref>, wherein the simulated neural circuitry is configured such that connections between different simulated neuronal elements have different temporal properties, with some being configured to weaken and some being configured to strengthen with repeated input of a given frequency.</claim-text>
</claim>
<claim id="CLM-00031" num="00031">
<claim-text>31. The object recognition system of <claim-ref idref="CLM-00026">claim 26</claim-ref>, wherein the simulated neural circuitry functions as a self-organizing content-addressable memory.</claim-text>
</claim>
<claim id="CLM-00032" num="00032">
<claim-text>32. The object recognition system of <claim-ref idref="CLM-00026">claim 26</claim-ref>, wherein:
<claim-text>the simulated neural circuitry includes a simulated neuronal element represented as including multiple branches,</claim-text>
<claim-text>a particular branch is configured to activate when the particular branch receives input that exceeds a first threshold, and</claim-text>
<claim-text>the simulated neuronal element is configured to activate when a number of the simulated neuronal element's branches that are activated exceeds a second threshold.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00033" num="00033">
<claim-text>33. The object recognition system of <claim-ref idref="CLM-00026">claim 26</claim-ref>, wherein:
<claim-text>the simulated neural circuit includes multiple processing areas having multiple neuronal elements, the processing areas being arranged in a hierarchical series from a lowest processing area to a highest processing area; and</claim-text>
<claim-text>the simulated neural circuit is configured to provide feed-forward from outputs of the neuronal elements of one processing area to inputs of neuronal elements of a higher processing area.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00034" num="00034">
<claim-text>34. The object recognition system of <claim-ref idref="CLM-00033">claim 33</claim-ref>, wherein each higher processing area is configured to issue feedback to a lower processing area or areas.</claim-text>
</claim>
<claim id="CLM-00035" num="00035">
<claim-text>35. The object recognition system of <claim-ref idref="CLM-00033">claim 33</claim-ref>, wherein the neuronal elements include dendritic trees.</claim-text>
</claim>
<claim id="CLM-00036" num="00036">
<claim-text>36. A computer-implemented method for training simulated neural circuitry having at least a first processing area and a second processing area to achieve invariant object recognition, wherein the first processing area and the second processing area include simulated neuronal elements, the method comprising:
<claim-text>initializing connections between simulated neuronal elements of the first processing area and simulated neuronal elements of the second processing area;</claim-text>
<claim-text>receiving, at the simulated neural circuitry, a series of different images of an object;</claim-text>
<claim-text>in response to receiving the different images of the object, generating initial encoded representations of the object in the first processing area by activating one or more of the simulated neuronal elements of the first processing area for each received image of the object;</claim-text>
<claim-text>transmitting the initial encoded representations of the object from the first processing area to the second processing area by transmitting signals from active simulated neuronal elements of the first processing area to the simulated neuronal elements of the second processing area to which the active simulated neuronal elements of the first area are connected; and</claim-text>
<claim-text>strengthening connections to an individual simulated neuronal element of the second processing area when the connections to the individual simulated neuronal element of the second processing area are consecutively active.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00037" num="00037">
<claim-text>37. The method of <claim-ref idref="CLM-00036">claim 36</claim-ref>, wherein strengthening connections to an individual simulated neuronal element of the second processing area when the connections to the individual simulated neuronal element of the second processing area are consecutively active includes strengthening connections to the individual simulated neuronal element of the second processing area when the connections to the individual simulated neuronal element of the second processing area are active within a specified window of time.</claim-text>
</claim>
<claim id="CLM-00038" num="00038">
<claim-text>38. A computer-implemented method for recognizing an object in an image comprising:
<claim-text>receiving an input image of an object at simulated neural circuitry that includes a hierarchy of processing areas and that has been trained to recognize one or more objects;</claim-text>
<claim-text>in response to receiving the input image of the object, generating an initial encoded representation of the object;</claim-text>
<claim-text>transforming the initial encoded representation of the object into an invariant output representation of the object by propagating the initial encoded representation of the object through the hierarchy of processing areas, wherein representations of the object generated in each succeeding processing area are increasingly less variant than the initial encoded representation; and</claim-text>
<claim-text>recognizing the object based on the invariant output representation of the object,</claim-text>
<claim-text>wherein the simulated neural circuitry includes simulated neuronal elements with local nonlinear integrative properties.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00039" num="00039">
<claim-text>39. The method of <claim-ref idref="CLM-00038">claim 38</claim-ref>, wherein a simulated neuronal element includes multiple input branches, one or more of which have local nonlinear integrative properties.</claim-text>
</claim>
<claim id="CLM-00040" num="00040">
<claim-text>40. The method of <claim-ref idref="CLM-00038">claim 38</claim-ref>, wherein the simulated neural circuitry functions as a self-organizing content-addressable memory.</claim-text>
</claim>
<claim id="CLM-00041" num="00041">
<claim-text>41. The method of <claim-ref idref="CLM-00038">claim 38</claim-ref>, wherein:
<claim-text>the simulated neural circuitry includes a simulated neuronal element represented as including multiple branches,</claim-text>
<claim-text>a particular branch is configured to activate when the particular branch receives input that exceeds a first threshold, and</claim-text>
<claim-text>the simulated neuronal element is configured to activate when a number of the simulated neuronal element's branches that are activated exceeds a second threshold.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00042" num="00042">
<claim-text>42. A computer-implemented method for searching for images of an object, the method comprising:
<claim-text>receiving a sequence of images of an object at simulated neural circuitry;</claim-text>
<claim-text>training the simulated neural circuitry using the sequence of images to generate an encoded representation of the object, wherein the encoded representation of the object includes a higher level feature triggered by recognizing a temporal sequence of lower level features of the object during the training;</claim-text>
<claim-text>searching a corpus of images by supplying each image of the corpus to the simulated neural circuitry; and</claim-text>
<claim-text>designating images from the corpus of images as being images of the object when an output of the simulated neural circuitry produced as a result of an image being supplied to the simulated neural circuitry matches the encoded representation of the object to within a predetermined threshold,</claim-text>
<claim-text>wherein the simulated neural circuitry includes simulated neuronal elements with local nonlinear integrative properties.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00043" num="00043">
<claim-text>43. The method of <claim-ref idref="CLM-00042">claim 42</claim-ref>, wherein the sequence of images comprises a video of the object.</claim-text>
</claim>
<claim id="CLM-00044" num="00044">
<claim-text>44. The method of <claim-ref idref="CLM-00043">claim 43</claim-ref>, wherein a simulated neuronal element includes multiple input branches, one or more of which have local nonlinear integrative properties.</claim-text>
</claim>
<claim id="CLM-00045" num="00045">
<claim-text>45. The method of <claim-ref idref="CLM-00043">claim 43</claim-ref>, wherein the simulated neural circuitry includes simulated neuronal elements configured to regulate excitation levels and synaptic connection strengths.</claim-text>
</claim>
<claim id="CLM-00046" num="00046">
<claim-text>46. The method of <claim-ref idref="CLM-00043">claim 43</claim-ref>, wherein the simulated neural circuitry functions as a self-organizing content-addressable memory.</claim-text>
</claim>
<claim id="CLM-00047" num="00047">
<claim-text>47. The method of <claim-ref idref="CLM-00043">claim 43</claim-ref>, wherein:
<claim-text>the simulated neural circuitry includes a simulated neuronal element represented as including multiple branches,</claim-text>
<claim-text>a particular branch is configured to activate when the particular branch receives input that exceeds a first threshold, and</claim-text>
<claim-text>the simulated neuronal element is configured to activate when a number of the simulated neuronal element's branches that are activated exceeds a second threshold. </claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
