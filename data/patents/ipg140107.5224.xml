<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626323-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626323</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12607549</doc-number>
<date>20091028</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>CN</country>
<doc-number>2007 1 0129686</doc-number>
<date>20070817</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>894</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>17</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>700 94</main-classification>
<further-classification>715721</further-classification>
</classification-national>
<invention-title id="d2e71">Method and apparatus for playing audio files</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>6795383</doc-number>
<kind>B1</kind>
<name>Yamamoto et al.</name>
<date>20040900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>369 4716</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>2004/0008975</doc-number>
<kind>A1</kind>
<name>Lin</name>
<date>20040100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>2004/0249489</doc-number>
<kind>A1</kind>
<name>Dick</name>
<date>20041200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>700 94</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>2008/0276173</doc-number>
<kind>A1</kind>
<name>Li et al.</name>
<date>20081100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715716</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>CN</country>
<doc-number>1186303</doc-number>
<kind>A</kind>
<date>19980700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>CN</country>
<doc-number>1264121</doc-number>
<kind>A</kind>
<date>20000800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>CN</country>
<doc-number>1467726</doc-number>
<kind>A</kind>
<date>20040100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>CN</country>
<doc-number>1726556</doc-number>
<kind>A</kind>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>CN</country>
<doc-number>1971742</doc-number>
<kind>A</kind>
<date>20070500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>CN</country>
<doc-number>1980390</doc-number>
<kind>A</kind>
<date>20070600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>CN</country>
<doc-number>101110247</doc-number>
<kind>A</kind>
<date>20080100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00012">
<othercit>Written Opinion of the International Searching Authority (translation) dated (mailed) Sep. 18, 2008, issued in related Application No. PCT/CN2008/071232, filed Jun. 6, 2008, Huawei Technologies Co., Ltd.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00013">
<othercit>First Chinese Office Action dated (mailed) Mar. 6, 2009, issued in related Chinese Application No. 200710129686.4 Huawei Technologies Co., Ltd.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00014">
<othercit>Second Chinese Office Action dated (mailed) Mar. 6, 2009, issued in related Chinese Application No. 200710129686.4 Huawei Technologies Co., Ltd.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00015">
<othercit>Rejection Decision dated (mailed) Jan. 29, 2010, issued in related Chinese Application No. 200710129686.4 Huawei Technologies Co., Ltd.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>12</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>700 94</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>715721</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>8</number-of-drawing-sheets>
<number-of-figures>8</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>PCT/CN2008/071232</doc-number>
<date>20080606</date>
</document-id>
<parent-status>PENDING</parent-status>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>12607549</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20100049349</doc-number>
<kind>A1</kind>
<date>20100225</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Deng</last-name>
<first-name>Qingshan</first-name>
<address>
<city>Shenzhen</city>
<country>CN</country>
</address>
</addressbook>
<residence>
<country>CN</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Deng</last-name>
<first-name>Qingshan</first-name>
<address>
<city>Shenzhen</city>
<country>CN</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Conley Rose, P.C.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
<agent sequence="02" rep-type="attorney">
<addressbook>
<last-name>Rodolph</last-name>
<first-name>Grant</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
<agent sequence="03" rep-type="attorney">
<addressbook>
<last-name>Beauleiu</last-name>
<first-name>Nicholas K.</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Huawei Technologies Co,, Ltd.</orgname>
<role>03</role>
<address>
<city>Shenzhen</city>
<country>CN</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Saunders, Jr.</last-name>
<first-name>Joseph</first-name>
<department>2655</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A method for playing audio files includes: parsing data frames of an audio file until basic information about file playing that meets the conditions for playing the audio file is obtained; and playing the audio file according to the basic information about file playing. A location and search method for an audio file includes: determining a frame position corresponding to a playing position in search according to a stored mapping between playing positions and frame positions when receiving input information about the playing position in search; and starting playing an audio file from the determined frame position. An apparatus for playing audio files is also disclosed. With the invention, the playing response speed of an audio file is improved to a great extent.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="168.57mm" wi="209.21mm" file="US08626323-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="241.22mm" wi="167.72mm" file="US08626323-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="249.77mm" wi="144.02mm" file="US08626323-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="242.82mm" wi="189.82mm" file="US08626323-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="215.14mm" wi="147.91mm" file="US08626323-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="123.19mm" wi="134.79mm" file="US08626323-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="226.65mm" wi="182.20mm" orientation="landscape" file="US08626323-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="110.07mm" wi="119.30mm" file="US08626323-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="221.91mm" wi="172.89mm" orientation="landscape" file="US08626323-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application is a continuation of International Application No. PCT/CN2008/071232, filed on Jun. 6, 2008, which claims priority to Chinese Patent Application No. 200710129686.4 filed on Aug. 17, 2007, both of which are hereby incorporated by reference in their entireties.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">FIELD OF THE INVENTION</heading>
<p id="p-0003" num="0002">The present invention relates to audio playing technologies, and in particular, to a method and apparatus for playing audio files.</p>
<heading id="h-0003" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0004" num="0003">In a prior art, an entity that has the audio playing function such as a multimedia mobile phone and a Moving Picture Experts Group Audio Layer III (MP3) player may play audio files stored in the entity or player or locate and search for any position in these audio files. The file structure of audio files, such as the audio files of the MP3 or Advanced Audio Coding (AAC) type, consists of multi-frame data. Each frame includes frame header data and audio data. The frame header data records the audio information of the data frame, such as the sampling rate, channel mode, and frame length. The audio data is the data that is processed through a specific compression algorithm.</p>
<p id="p-0005" num="0004">Playing an audio file includes two processes, namely, parsing and decoding. Parsing is a process of obtaining audio information from the frame header of a frame contained in an audio file. Decoding is a process of restoring audio data that is processed through a compression algorithm to original Pulse Code Modulation (PCM) data.</p>
<p id="p-0006" num="0005">As shown in <figref idref="DRAWINGS">FIG. 1</figref>, when an audio file is played in a prior art, the type of the file is determined preliminarily according to the suffix of the file, and each frame of the file is parsed according to the type to obtain audio information of each frame, such as the sampling rate, channel mode, and frame length. The file information such as the sampling rate, total playing duration, and total number of frames is obtained by parsing the whole file. After the whole file is parsed, the first data frame starts to be decoded and played.</p>
<p id="p-0007" num="0006">Two processes, namely, locating and playing, are included in the location and search of an audio file. Locating is a process of determining the position of a frame that is played first according to the search time in an input search command. Playing is a process of decoding and playing audio data from the determined start frame position.</p>
<p id="p-0008" num="0007">As shown in <figref idref="DRAWINGS">FIG. 2</figref>, before the location and search of an audio file, validity of the search time in the search command needs to be checked. If the search time is valid, location and search can be performed. During location and search, a locating operation is performed according to the search time. The specific method is as follows: the playing duration is cumulated from the first frame of the audio file; the playing duration of each frame can be obtained from the frame header data of the data frame; when the cumulative playing duration until a frame is longer than or equal to the search time, the frame is taken as the start frame to be played. From the start frame, audio data is decoded and played. For example, if the search time is 5 s, and the playing duration of each frame is 10 ms, the cumulative playing duration until the 500<sup>th </sup>frame is 5 s (500&#xd7;10 ms). Then the 500<sup>th </sup>frame is taken as the start frame to be played, and data is read from the 500<sup>th </sup>frame for decoding and playing.</p>
<p id="p-0009" num="0008">In the process of implementing the present invention, the inventor finds at least the following technical defects in the prior art:</p>
<p id="p-0010" num="0009">After a user starts playing an audio file, the user can hear the audio only after waiting for a long period of time due to the lower response speed. This is because the file can be played only after all the frames of the file are read and parsed when the audio file is played. When the size of a file is large, the reading speed is very low. For example, an MP3 file with the size of four MB is stored in an external storage such as a Secure Digital (SD) card; if the reading speed is one MB per second, about four seconds are required to read the entire file; that is, the user can hear the music after at least four seconds when starting playing the audio file.</p>
<p id="p-0011" num="0010">Every time the user starts location and search, data starts to be read from the first frame of an audio File in the locating process, and the playing duration is cumulated until the frame position where the cumulative duration meets the search time is found. The method for location and search from the start of the file frame by frame is inefficient, and the playing response speed is low. When the search time point is close to the end time, the locating process needs to traverse the entire file. In the case of a large-sized file stored on a low-speed device, the locating process takes a longer time.</p>
<heading id="h-0004" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0012" num="0011">Embodiments of the present invention provide a method and apparatus for playing audio files to solve the low response speed problem in the prior art.</p>
<p id="p-0013" num="0012">A method for playing audio files includes:
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0013">parsing data frames of an audio file until basic information about file playing that meets the conditions for playing the audio file is obtained; and playing the audio file according to the basic information about file playing.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0014" num="0014">Another method for playing audio files includes:
<ul id="ul0003" list-style="none">
    <li id="ul0003-0001" num="0000">
    <ul id="ul0004" list-style="none">
        <li id="ul0004-0001" num="0015">determining a frame position corresponding to a playing position in search according to an established and stored mapping between playing positions and frame positions when receiving input information about the playing position in search; and playing an audio file from the determined frame position.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0015" num="0016">An apparatus for playing audio files includes:
<ul id="ul0005" list-style="none">
    <li id="ul0005-0001" num="0000">
    <ul id="ul0006" list-style="none">
        <li id="ul0006-0001" num="0017">a parsing module <b>501</b>, adapted to parse one or more data frames of an audio file until basic information about file playing that meets the conditions for playing the audio file is obtained; and</li>
        <li id="ul0006-0002" num="0018">a first playing module <b>502</b>, adapted to play the audio file according to the basic information about file playing.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0016" num="0019">Another apparatus for playing audio files includes:
<ul id="ul0007" list-style="none">
    <li id="ul0007-0001" num="0000">
    <ul id="ul0008" list-style="none">
        <li id="ul0008-0001" num="0020">a locating and searching module <b>602</b>, adapted to determine a frame position corresponding to a playing position in search according to an established and stored mapping between playing positions and frame positions when receiving input information about the playing position in search; and</li>
        <li id="ul0008-0002" num="0021">a third playing module <b>603</b>, adapted to play an audio file from the determined frame position.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0017" num="0022">A computer readable storage medium stores a computer program that enables a processor to execute the following steps:
<ul id="ul0009" list-style="none">
    <li id="ul0009-0001" num="0000">
    <ul id="ul0010" list-style="none">
        <li id="ul0010-0001" num="0023">parsing data frames of an audio file until basic information about file playing that meets the conditions for playing the audio file is obtained; and playing the audio file according to the basic information about file playing.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0018" num="0024">A computer readable storage medium stores a computer program that enables a processor to execute the following steps:
<ul id="ul0011" list-style="none">
    <li id="ul0011-0001" num="0000">
    <ul id="ul0012" list-style="none">
        <li id="ul0012-0001" num="0025">determining a frame position corresponding to a playing position in search according to an established and stored mapping between playing positions and frame positions when receiving input information about the playing position in search; and playing an audio file from the determined frame position.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0019" num="0026">Embodiments of the present invention may bring the following benefits:</p>
<p id="p-0020" num="0027">In embodiments of the present invention, when an audio file is played, the audio file starts to be played after basic information about file playing is obtained by parsing one or more data frames of the audio file. Thus, the playing response speed of the audio file is improved to a great extent. In addition, during location of an audio file, a specified start playing position can be rapidly found according to the established and stored mapping between playing positions and frame positions, and the audio file is played from the position. Thus, the location and search speed is improved to a great extent, and therefore, the playing response speed of the audio file is improved.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0021" num="0028"><figref idref="DRAWINGS">FIG. 1</figref> shows a flow chart of playing an audio file in a prior art;</p>
<p id="p-0022" num="0029"><figref idref="DRAWINGS">FIG. 2</figref> shows a location and search flow chart of an audio file in a prior art;</p>
<p id="p-0023" num="0030"><figref idref="DRAWINGS">FIG. 3</figref> shows a flow chart of playing an audio file in an embodiment of the present invention;</p>
<p id="p-0024" num="0031"><figref idref="DRAWINGS">FIG. 4</figref> shows a location and search flow chart of an audio file in an embodiment of the present invention:</p>
<p id="p-0025" num="0032"><figref idref="DRAWINGS">FIG. 5</figref> shows an architecture diagram of an apparatus for playing audio files in an embodiment of the present invention;</p>
<p id="p-0026" num="0033"><figref idref="DRAWINGS">FIG. 6</figref> shows another architecture diagram of an apparatus for playing audio files in an embodiment of the present invention;</p>
<p id="p-0027" num="0034"><figref idref="DRAWINGS">FIG. 7</figref> shows a third architecture diagram of an apparatus for playing audio files in an embodiment of the present invention; and</p>
<p id="p-0028" num="0035"><figref idref="DRAWINGS">FIG. 8</figref> shows a fourth architecture diagram of an apparatus for playing audio files in an embodiment of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0006" level="1">DETAILED DESCRIPTION OF THE INVENTION</heading>
<p id="p-0029" num="0036">To improve the playing response speed of an audio file, embodiments of the present invention provide a method for playing the audio file. In the method, the audio file is decoded and played after basic information required for playing the audio file is obtained by parsing one or more data frames of the audio file. Meanwhile, the audio file continues to be parsed for obtaining the complete information of the file. In the process of parsing the audio file, an index table of a mapping between playing positions and frame positions can be created for the use of location and search.</p>
<p id="p-0030" num="0037">A method provided in embodiments of the present invention is hereinafter described with reference to the accompanying drawings.</p>
<p id="p-0031" num="0038">Step S<b>1</b>: Obtain basic information about file playing by parsing one or more data frames of an audio file.</p>
<p id="p-0032" num="0039">As shown in <figref idref="DRAWINGS">FIG. 3</figref>, step S<b>1</b> may include blocks <b>301</b>-<b>304</b>.</p>
<p id="p-0033" num="0040">Block <b>301</b>: Read a data frame of the audio file.</p>
<p id="p-0034" num="0041">The first frame of the audio file starts to be read; that is, the first frame of the audio file is read for the first time, and then the second frame of the audio file is read, and so on.</p>
<p id="p-0035" num="0042">Block <b>302</b>: Parse the read data frame to obtain basic information about file playing.</p>
<p id="p-0036" num="0043">Obtain the required basic information about file playing from the frame header data of the data frame by parsing the data frame. The basic information about file playing is the information required for playing the audio file. For example, the basic information about file playing of an MP3 or ACC audio file includes the sampling rate, sample precision, and channel mode.</p>
<p id="p-0037" num="0044">Also, the obtained basic information about file playing may be stored, for example, stored to a frame index table for the use of location, search and playing.</p>
<p id="p-0038" num="0045">Block <b>303</b>: Check whether the obtained basic information about file playing is complete. If yes, proceed to block <b>304</b>; otherwise, go to block <b>301</b>, and read the next data frame until the complete basic information about file playing is obtained.</p>
<p id="p-0039" num="0046">Block <b>304</b>: Output the obtained basic information about file playing for the use of file decoding and playing.</p>
<p id="p-0040" num="0047">Generally, the complete basic information about file playing can be obtained by parsing the first one or two data frames of an audio file.</p>
<p id="p-0041" num="0048">Step S<b>2</b>: Decode and play the audio file according to the basic information about file playing obtained in step S<b>1</b>.</p>
<p id="p-0042" num="0049">As shown in <figref idref="DRAWINGS">FIG. 3</figref>, step S<b>2</b> includes blocks <b>305</b>-<b>308</b>.</p>
<p id="p-0043" num="0050">Block <b>305</b>: Read a data frame of the audio file.</p>
<p id="p-0044" num="0051">The first frame of the audio file starts to be read; that is, the first frame of the audio file is read for the first time, and then the second frame of the audio file is read, and so on.</p>
<p id="p-0045" num="0052">Block <b>306</b>: Decode the read data frame according to the basic information about file playing output in step <b>304</b> to generate PCM data.</p>
<p id="p-0046" num="0053">Block <b>307</b>: Play the generated PCM data.</p>
<p id="p-0047" num="0054">Block <b>308</b>: Check whether the currently played data belongs to the last frame. If yes, the playing is complete: otherwise, go to block <b>305</b>, and read the next data frame for decoding and playing until the playing is complete.</p>
<p id="p-0048" num="0055">When step S<b>2</b> is executed, step S<b>3</b> (further parse the audio file to obtain complete information about file playing) may also be executed concurrently.</p>
<p id="p-0049" num="0056">As shown in <figref idref="DRAWINGS">FIG. 3</figref>, step S<b>3</b> includes blocks <b>309</b>-<b>312</b>.</p>
<p id="p-0050" num="0057">Block <b>309</b>: Read a data frame of the audio file.</p>
<p id="p-0051" num="0058">Read a data frame next to the parsed data frame in step S<b>1</b>, and then read the next data frame, and so on.</p>
<p id="p-0052" num="0059">Block <b>310</b>: Parse the read data frame to obtain the audio information corresponding to the data frame.</p>
<p id="p-0053" num="0060">Block <b>311</b>: Check whether the current frame is the last frame. If yes, proceed to block <b>312</b>: otherwise, go back to step <b>309</b> to read the next data frame until all frame data is parsed.</p>
<p id="p-0054" num="0061">Block <b>312</b>: Obtain and output complete information about file playing according to audio information of each frame.</p>
<p id="p-0055" num="0062">In addition to basic information such as the sampling rate, sample precision, and channel mode, complete information about file playing also includes the bit rate, total playing duration, and total number of frames of an audio file. With regard to an audio file of the MP3 type, the complete information about file playing may also include the production and publication information, such as the song name, lyric writer, composer, singer, band, and special. The information except the basic information may control the end of playing an audio file and supports the information query of an audio file. For example, the information may control the end of playing an audio file by comparing the current playing duration with the total playing duration or comparing the relation between the current number of played frames and the total number of frames; the information related to an audio file, such as the song name, lyric writer, and composer is displayed. The information is not necessary information about audio file playing. Therefore, in embodiments of the present invention, a file is decoded and played after basic information about file playing is obtained to reduce the playing waiting time.</p>
<p id="p-0056" num="0063">In embodiments of the invention, steps S<b>2</b> and S<b>3</b> may be executed concurrently. According to the multi-task feature of the system, the system schedules one execution process for S<b>2</b> and S<b>3</b> respectively. The two processes execute their own tasks concurrently. The task of S<b>1</b> is complete first. The concurrent execution time of the two processes is within several seconds. Therefore, smooth file playing is not affected.</p>
<p id="p-0057" num="0064">In view of the requirements for the location and search of an audio file, after one data frame is parsed in step S<b>3</b>, audio information such as the playing duration and the size of the data frame is obtained. According to the information, a mapping between playing positions and frame positions is established, and the mapping is stored (such as in a frame index table). Specifically, the mapping may include but not be limited to the mapping between playing time points and frame positions or the mapping between frame numbers and frame positions. The specific method is as follows:</p>
<p id="p-0058" num="0065">After the playing duration and the frame size are obtained by parsing one data frame, the second cumulative duration is obtained by adding up the playing duration and the first cumulative duration, and the size of the second cumulative frame is obtained by adding up the size of the data frame and the size of the first cumulative frame. The first cumulative duration is the sum of the playing durations of the data frames previously traversed. The size of the first cumulative frame is the sum of the sizes of the data frames previously traversed. Obviously, with regard to the first frame, the first cumulative duration and the size of the first cumulative frame are zero. With regard to the next frame, the first cumulative duration and the size of the first cumulative frame used for calculation are respectively the second cumulative duration and the size of the second cumulative frame obtained when the previous frame is calculated.</p>
<p id="p-0059" num="0066">Then, a frame index table is created according to the obtained second cumulative duration and the size of the second cumulative frame, and a mapping between the second cumulative duration and the size of the second cumulative frame is stored in the frame index table. Thus, after all frames are traversed, a mapping between the playing time point and the frame position of each frame is stored in the frame index table. The frame index table may also be created according to a previously specified mark time point. In this case, a mapping between the pre-mark tine point and the size of the second cumulative frame can be stored when the second cumulative duration is longer than or equal to the pre-mark time point. For example, the playing duration of each frame is 150 ms, and the size of each frame is 600 bytes. According to the specification, the frame position is stored at each second. When the seventh frame is traversed, the obtained second cumulative duration is 1050 (150+900=1050) ms (which is longer than 1 s), and the size of the second cumulative frame is 4200 (600+3600=4200) bytes. In this case, a mapping between the playing time point (1 s) and the frame position (4200 bytes) is stored. Similarly, when the 14<sup>th</sup>, 21<sup>st</sup>, and 28<sup>th </sup>frames are traversed respectively, a mapping between the playing time points (2 s, 3 s, and 4 s) and the frame positions, namely, a mapping between 2 s and 8400 bytes, a mapping between 3 s and 12600 bytes, and a mapping between 4 s and 16800 bytes, is stored respectively.</p>
<p id="p-0060" num="0067">With regard to the method for establishing a mapping between frame numbers and frame positions, after the size of one frame is obtained, the size of the second cumulative frame is obtained by adding up the size of the data frame and the size of the first cumulative frame. The size of the first cumulative frame is the sum of the sizes of the frames previously traversed. With regard to the first frame, the size of the first cumulative frame is 0. With regard to the next frame, the size of the first cumulative frame used for calculation is the size of the second cumulative frame obtained when the previous frame is calculated. According to the size of the obtained second cumulative frame, a frame index table is created. The frame index table stores a mapping between the number of the data frame and the size of the second cumulative frame. Thus, when all frames are traversed, a mapping between frame numbers and frame positions is stored.</p>
<p id="p-0061" num="0068">After all frames are traversed and parsed, the frame index table is created accordingly, and then is output and stored. When a user uses the location and search function of an audio file, the created frame index table may be used to implement the location and search function after a location and search command carrying the playing position information in search is input. The specific process shown in <figref idref="DRAWINGS">FIG. 4</figref> is as follows:</p>
<p id="p-0062" num="0069">Block <b>401</b>: Check whether the playing position in search is valid. If yes, go to block <b>402</b>. Otherwise, end the process.</p>
<p id="p-0063" num="0070">When the playing position in search is the playing time point in search, a valid playing time point in search is a time point that does not exceed the total playing duration of an audio file. If a time point exceeds the total playing duration of an audio file, the search time point is invalid. For example, in the case of an audio file whose total playing duration is 15 minutes, if the playing time point in search is 20 minutes, the time point is an invalid search time point; if the playing position in search is a frame number, a valid frame number in search does not exceed the frame number range contained in the audio file. If a frame number in search exceeds the frame number range contained in the audio file, the frame number is invalid.</p>
<p id="p-0064" num="0071">Block <b>402</b>: Query the frame index table, determine a frame position corresponding to a playing position in search, and take the determined frame position as the start playing position.</p>
<p id="p-0065" num="0072">When querying the frame index table, find an entry that matches the playing position in search. The playing position contained in the entry is the same as or most close to the playing position in search. During location, use the frame position corresponding to the playing position to locate the pointer of the audio file, and take the position indicated by the pointer as the start playing position.</p>
<p id="p-0066" num="0073">Block <b>403</b>: Read data frames from the start playing position determined in the previous block.</p>
<p id="p-0067" num="0074">Block <b>404</b>: Decode the read data frames to generate PCM data, and play the generated PCM data.</p>
<p id="p-0068" num="0075">In this block, decode the read data frames according to the obtained basic information about file playing to generate PCM data, and then play the generated PCM data. The specific method for obtaining basic information about file playing may be as follows: obtain basic information about file playing by querying the frame index table; or add a step of obtaining basic information about file playing by parsing one or more data frames of the audio file between blocks <b>402</b>-<b>403</b>. For details, see the foregoing blocks <b>301</b>-<b>304</b>. After the basic information about file playing is obtained, the audio file may be parsed concurrently when the audio file is played to obtain complete information about file playing. For details, see the foregoing blocks <b>309</b>-<b>312</b>.</p>
<p id="p-0069" num="0076">Block <b>405</b>: Check whether the current frame is the last frame. If yes, end the process; otherwise, go to block <b>403</b> to read the next data frame until the last data frame is played.</p>
<p id="p-0070" num="0077">When reading the next data frame in block <b>403</b>, move the pointer of the current audio file to the next frame position and read the data frame in the position.</p>
<p id="p-0071" num="0078">In certain special cases, for example, when a user starts the location and search function of an audio file, and the audio file has never been played before, no created frame index table exists. Therefore, a temporary frame index table needs to be created. The method for creating a temporary frame index table is the same as the method for creating a frame index table when an audio file is parsed. That is, parse each data frame, and fill up the frame index table according to the playing duration and the size of each frame obtained by parsing data frames.</p>
<p id="p-0072" num="0079">Create a frame index table in advance, find the start frame position corresponding to the search time point rapidly according to the frame index table during location and search, and then play the audio file from the position. Thus, the search and location may be implemented efficiently.</p>
<p id="p-0073" num="0080">As shown in <figref idref="DRAWINGS">FIG. 5</figref>, an embodiment of the present invention further provides an apparatus for playing audio files. The apparatus includes a parsing module <b>501</b> and a first playing module <b>502</b>.</p>
<p id="p-0074" num="0081">The parsing module <b>501</b> is adapted to parse one or more data frames of an audio file until basic information about file playing that meets the conditions for playing the audio file is obtained. The basic information about file playing is the information required for playing the audio file, which includes but is not limited to the sampling rate, sample precision, and channel mode.</p>
<p id="p-0075" num="0082">The first playing module <b>502</b> is adapted to play the audio file according to the basic information about file playing.</p>
<p id="p-0076" num="0083">As shown in <figref idref="DRAWINGS">FIG. 6</figref>, an embodiment of the present invention further provides a structure of an apparatus for playing audio files. The apparatus includes a parsing module <b>501</b> and a first playing module <b>502</b>.</p>
<p id="p-0077" num="0084">The parsing module <b>501</b> includes a reading unit <b>5011</b>, an obtaining unit <b>5012</b>, a judging unit <b>5013</b>, and an outputting unit <b>5014</b>. The reading unit <b>5011</b> is adapted to read data frames of an audio file. The obtaining unit <b>5012</b> is adapted to parse the data frames read by the reading unit <b>5011</b> to obtain basic information about file playing. The judging unit <b>5013</b> is adapted to check whether the currently obtained basic information about file playing is complete, and if yes, notify the outputting unit <b>5014</b> to output the obtained basic information about file playing; otherwise, notify the reading unit <b>5011</b> to read the next data frame. The outputting unit <b>5014</b> is adapted to output the obtained basic information about file playing.</p>
<p id="p-0078" num="0085">The apparatus further includes a second module <b>503</b> and a controlling module <b>504</b>. The second module <b>503</b> is adapted to continue to parse data frames of the audio file from the data frame next to the data frames parsed by the parsing unit to obtain complete information about file playing when the first playing unit plays the audio file. The controlling module <b>504</b> is adapted to control the end of playing the audio file and output information related to the audio file according to the complete information about file playing.</p>
<p id="p-0079" num="0086">The apparatus further includes an index establishing module <b>505</b>, a locating and searching module <b>506</b>, and a second playing module <b>507</b>. The index establishing module <b>505</b> is adapted to establish and store a mapping between playing positions and frame positions according to the audio information obtained by the parsing module <b>501</b> and the second module <b>503</b> through a parsing operation. The audio information includes the playing duration and the frame size. The playing position includes a playing time point or frame number. The locating and searching module <b>506</b> is adapted to determine the frame position corresponding to the playing position in search according to the mapping when receiving input playing position information in search. The second playing module <b>507</b> is adapted to play the audio file from the determined frame position.</p>
<p id="p-0080" num="0087">The index establishing module <b>505</b> includes a calculating unit <b>5051</b> and a storing unit <b>5052</b>. The calculating unit <b>5051</b> is adapted to calculate the second cumulative duration and the size of the second cumulative frame after the playing duration and the frame size of data frames are obtained by parsing data frames. Specifically, the calculating unit <b>5051</b> includes a first cumulative unit, which is adapted to calculate the second cumulative duration (The second cumulative duration is equal to the first cumulative duration plus the playing duration, and the first cumulative duration is the sum of the playing durations of all data frames before the data frame); and a second cumulative unit, which is adapted to calculate the size of the second cumulative frame (The size of the second cumulative frame is equal to the size of the first cumulative frame plus the size of the data frame, and the size of the first cumulative frame is the sum of the sizes of all data frames before the data frame). The storing unit <b>5052</b> is adapted to establish and store a mapping between the playing time point and the frame position by taking the second cumulative duration as the playing time point and taking the size of the second cumulative frame as the frame position, or establish and store a mapping between the playing time point and the frame position by taking the current pre-mark time point as the playing time point and taking the size of the second cumulative frame as the frame position when the second cumulative duration is longer than or equal to the current pre-mark time point.</p>
<p id="p-0081" num="0088">As shown in <figref idref="DRAWINGS">FIG. 7</figref>, an embodiment of the present invention further provides an apparatus for playing audio files. The apparatus includes a locating and searching module <b>602</b> and a third playing module <b>603</b>. The locating and searching module <b>602</b> is adapted to determine the frame position corresponding to the playing position in search according to the mapping when receiving input playing position information in search. The third playing module <b>603</b> is adapted to play the audio file from the determined frame position.</p>
<p id="p-0082" num="0089">The apparatus further includes an index establishing module <b>601</b>, which is adapted to parse the data frames of an audio file to obtain audio information and establish and store a mapping between playing positions and frame positions according to the audio information. The audio information includes the playing duration and the frame size. The playing position includes a playing time point or frame number.</p>
<p id="p-0083" num="0090">As shown in <figref idref="DRAWINGS">FIG. 8</figref>, an embodiment of the present invention further provides a stricture of an apparatus for playing audio files.</p>
<p id="p-0084" num="0091">The index establishing module <b>601</b> includes a calculating unit <b>6011</b> and a storing unit <b>6012</b>. The calculating unit <b>6011</b> is adapted to calculate the second cumulative duration and the size of the second cumulative frame after the playing duration and the frame size of data frames are obtained by parsing data frames. Specifically, the calculating unit <b>6011</b> includes a first cumulative unit, which is adapted to calculate the second cumulative duration (The second cumulative duration is equal to the first cumulative duration plus the playing duration, and the first cumulative duration is the sum of the playing durations of all data frames before the data frame); and a second cumulative unit, which is adapted to calculate the size of the second cumulative frame (The size of the second cumulative frame is equal to the size of the first cumulative frame plus the size of the data frame, and the size of the first cumulative frame is the sum of the sizes of all data frames before the data frame). The storing unit <b>6012</b> is adapted to establish and store a mapping between the playing time point and the frame position by taking the second cumulative duration as the playing time point and taking the size of the second cumulative frame as the frame position, or establish and store a mapping between the playing time point and the frame position by taking the current pre-mark time point as the playing time point and taking the size of the second cumulative frame as the frame position when the second cumulative duration is longer than or equal to the current pre-mark time point.</p>
<p id="p-0085" num="0092">The apparatus further includes a parsing module <b>604</b>. The parsing module <b>604</b> is adapted to parse one or more data frames of an audio file until basic information about file playing that meets the conditions for playing the audio file is obtained. The basic information about file playing is the information required for playing the audio file, which includes but is not limited to the sampling rate, sample precision, and channel mode.</p>
<p id="p-0086" num="0093">The third playing module <b>603</b> includes a fourth playing unit <b>6031</b>. The fourth playing unit <b>6031</b> is adapted to play the audio file according to the basic information about file playing.</p>
<p id="p-0087" num="0094">The parsing module <b>604</b> includes a reading unit <b>6041</b>, an obtaining unit <b>6042</b>, a judging unit <b>6043</b>, and an outputting unit <b>6044</b>. The reading unit <b>6041</b> is adapted to read data frames of the audio file. The obtaining unit <b>6042</b> is adapted to parse the data frames read by the reading unit <b>6041</b> to obtain basic information about file playing. The judging unit <b>6043</b> is adapted to check whether the currently obtained basic information about file playing is complete, and if yes, notify the outputting unit <b>6044</b> to output the obtained basic information about file playing; otherwise, notify the reading unit <b>6041</b> to read the next data frame. The outputting unit <b>6044</b> is adapted to output the obtained basic information about file playing.</p>
<p id="p-0088" num="0095">The apparatus further includes a second module <b>605</b> and a controlling module <b>606</b>. The second module <b>605</b> is adapted to continue to parse data frames of the audio file from the data frame next to the data frames parsed by the parsing unit <b>604</b> to obtain complete information about file playing when the fourth playing unit <b>6031</b> plays the audio file. The controlling module <b>606</b> is adapted to control the end of playing the audio file and output information related to the audio file according to the complete information about file playing.</p>
<p id="p-0089" num="0096">Therefore, embodiments of the present invention may bring the following benefits:</p>
<p id="p-0090" num="0097">1. An audio file starts to be played after basic information about file playing is obtained through a parsing operation. The subsequent parsing and decoding operations change from serial operations into concurrent operations. Thus, the playing waiting time is saved to a great extent, and a response at the start playing time can be made within one or two seconds irrespective of the file size and the storage medium.</p>
<p id="p-0091" num="0098">2. The search algorithm is optimized by creating a frame index table. Thus, the response time corresponding to different search positions is consistent, and the response speed in searching and playing is enhanced to a great extent.</p>
<p id="p-0092" num="0099">It is understandable to those skilled in the art that all or part of the steps of the foregoing embodiments may be implemented by hardware instructed by a program. The program may be stored in a computer readable storage medium, such as a Read-Only Memory (ROM), a Random Access Memory (RAM), a magnetic disk and a compact disk.</p>
<p id="p-0093" num="0100">Although the present invention has been described through some exemplary embodiments, the invention is not limited to such embodiments. It is apparent that those skilled in the art can make various modifications and variations to the invention without departing from the spirit and scope of the present invention. The invention is intended to cover these modifications and variations provided that they fall in the scope of protection defined by the following claims or their equivalents.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method for playing an audio file, comprising:
<claim-text>determining a frame position corresponding to a playing position in a search according to a mapping between playing positions and frame positions when receiving input information about the playing position in the search; and</claim-text>
<claim-text>playing the audio file from the determined frame position,</claim-text>
<claim-text>wherein the mapping between the playing positions and the frame positions is obtained by:
<claim-text>calculating a second cumulative duration and a size of a second cumulative frame after a playing duration and a size of a data frame are obtained by parsing the audio file, wherein the second cumulative duration is equal to a first cumulative duration plus the playing duration, wherein the first cumulative duration is a sum of playing durations of all data frames before the data frame, wherein the size of the second cumulative frame is equal to a size of a first cumulative frame plus the size of the data frame, and wherein the size of the first cumulative frame is a sum of sizes of all the data frames before the data frame; and</claim-text>
<claim-text>establishing and storing the mapping between the playing positions and the frame positions before receiving the input information about the playing position in the search by taking the second cumulative duration as one of the playing positions and taking the size of the second cumulative frame as one of the frame positions, or establishing and storing the mapping between the playing positions and the frame positions by taking a current pre-mark time point as one of the playing positions and taking the size of the second cumulative frame as one of the frame positions when the second cumulative duration is longer than or equal to the current pre-mark time point, and</claim-text>
</claim-text>
<claim-text>wherein determining the frame position comprises retrieving the stored mapping between the playing positions and the frame positions and identifying the frame position corresponding to the playing position in the stored mapping.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the playing position in the search comprises a playing time point or a frame number.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein playing the audio file from the determined frame position comprises:
<claim-text>parsing one or more data frames of the audio file until basic information that meets a condition for playing the audio file is obtained; and</claim-text>
<claim-text>according to the basic information, playing the audio file from the determined frame position.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein parsing the one or more data frames of the audio file until basic information that meets the condition for playing the audio file is obtained comprises:
<claim-text>reading the data frame of the audio file;</claim-text>
<claim-text>parsing the data frame of the audio file to obtain the basic information; and</claim-text>
<claim-text>outputting the obtained basic information when complete basic information about file playing is obtained; otherwise, reading a next data frame.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the basic information comprises at least one of: a sampling rate, a sample precision, and a channel mode.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, further comprising:
<claim-text>continuing to parse data frames of the audio file to obtain complete information about file playing; and</claim-text>
<claim-text>controlling an end of playing the audio file and outputting information related to the audio file according to the complete information about file playing.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. An apparatus for playing an audio file, comprising:
<claim-text>a locating and searching module adapted to determine a frame position corresponding to a playing position in a search according to a mapping between playing positions and frame positions when receiving input information about the playing position in the search;</claim-text>
<claim-text>a playing module adapted to play the audio file from the determined frame position; and</claim-text>
<claim-text>an index establishing module, wherein the index establishing module comprises:
<claim-text>a calculating unit adapted to calculate a second cumulative duration and a size of a second cumulative frame after a playing duration and a size of a data frame are obtained by parsing the audio file, wherein the second cumulative duration is equal to a first cumulative duration plus the playing duration, wherein the first cumulative duration is a sum of playing durations of all data frames before the data frame, wherein the size of the second cumulative frame is equal to a size of a first cumulative frame plus the size of the data frame, and wherein the size of the first cumulative frame is a sum of sizes of all the data frames before the data frame; and</claim-text>
<claim-text>a storing unit adapted to establish and store the mapping between the playing positions and the frame positions before receiving the input information about the playing position in the search by taking the second cumulative duration as one of the playing positions and taking the size of the second cumulative frame as one of the frame positions, or establish and store the mapping between the playing positions and the frame positions by taking a current pre-mark time point as one of the playing positions and taking the size of the second cumulative frame as one of the frame positions when the second cumulative duration is longer than or equal to the current pre-mark time point, and</claim-text>
</claim-text>
<claim-text>wherein the locating and searching module is adapted to determine the frame position by retrieving the stored mapping between the playing positions and the frame positions and identifying the frame position corresponding to the playing position in the stored mapping.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The apparatus of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the playing position in the search comprises a playing time point or a frame number.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The apparatus of <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising:
<claim-text>a parsing module adapted to parse one or more data frames of the audio file until basic information that meets a condition for playing the audio file is obtained; and</claim-text>
<claim-text>a second playing module adapted to play the audio file from the determined frame position according to the basic information.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The apparatus of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the parsing module comprises:
<claim-text>a reading unit adapted to read the data frame of the audio file;</claim-text>
<claim-text>an obtaining unit adapted to parse the data frame read by the reading unit to obtain the basic information;</claim-text>
<claim-text>a judging unit adapted to determine whether complete basic information about file playing is obtained, and if so, notify an outputting unit to output the obtained basic information; otherwise, notify the reading unit to continue to read a next data frame; and</claim-text>
<claim-text>the outputting unit adapted to output the obtained basic information.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The apparatus of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the basic information about file playing comprises at least one of: a sampling rate, a sample precision, and a channel mode.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The apparatus of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising:
<claim-text>a second module adapted to continue to parse data frames of the audio file to obtain complete information about file playing when the first playing unit plays the audio file; and</claim-text>
<claim-text>a controlling module adapted to control an end of playing the audio file and output information related to the audio file according to the complete information about file playing. </claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
