<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625669-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625669</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12401831</doc-number>
<date>20090311</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<us-term-of-grant>
<us-term-extension>1304</us-term-extension>
<disclaimer>
<text>This patent is subject to a terminal disclaimer.</text>
</disclaimer>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>7</main-group>
<subgroup>12</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>37524013</main-classification>
<further-classification>37524016</further-classification>
</classification-national>
<invention-title id="d2e55">Predicting motion vectors for fields of forward-predicted interlaced video frames</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>4454546</doc-number>
<kind>A</kind>
<name>Mori</name>
<date>19840600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>4661849</doc-number>
<kind>A</kind>
<name>Hinman</name>
<date>19870400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>4661853</doc-number>
<kind>A</kind>
<name>Roeder et al.</name>
<date>19870400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>4691329</doc-number>
<kind>A</kind>
<name>Juri et al.</name>
<date>19870900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>4695882</doc-number>
<kind>A</kind>
<name>Wada et al.</name>
<date>19870900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>4796087</doc-number>
<kind>A</kind>
<name>Guichard et al.</name>
<date>19890100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>4800432</doc-number>
<kind>A</kind>
<name>Barnett et al.</name>
<date>19890100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>4849812</doc-number>
<kind>A</kind>
<name>Borgers et al.</name>
<date>19890700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>4862267</doc-number>
<kind>A</kind>
<name>Gillard et al.</name>
<date>19890800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>4864393</doc-number>
<kind>A</kind>
<name>Harradine et al.</name>
<date>19890900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>4999705</doc-number>
<kind>A</kind>
<name>Puri</name>
<date>19910300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>5021879</doc-number>
<kind>A</kind>
<name>Vogel</name>
<date>19910600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>5068724</doc-number>
<kind>A</kind>
<name>Krause et al.</name>
<date>19911100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>5089887</doc-number>
<kind>A</kind>
<name>Robert et al.</name>
<date>19920200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>5091782</doc-number>
<kind>A</kind>
<name>Krause et al.</name>
<date>19920200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>5103306</doc-number>
<kind>A</kind>
<name>Weiman et al.</name>
<date>19920400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>5105271</doc-number>
<kind>A</kind>
<name>Niihara</name>
<date>19920400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>5111292</doc-number>
<kind>A</kind>
<name>Kuriacose et al.</name>
<date>19920500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>5117287</doc-number>
<kind>A</kind>
<name>Koike et al.</name>
<date>19920500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>5144426</doc-number>
<kind>A</kind>
<name>Tanaka et al.</name>
<date>19920900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>5155594</doc-number>
<kind>A</kind>
<name>Bernstein et al.</name>
<date>19921000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>5157490</doc-number>
<kind>A</kind>
<name>Kawai et al.</name>
<date>19921000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>5175618</doc-number>
<kind>A</kind>
<name>Ueda</name>
<date>19921200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>5193004</doc-number>
<kind>A</kind>
<name>Wang et al.</name>
<date>19930300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>5223949</doc-number>
<kind>A</kind>
<name>Honjo</name>
<date>19930600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>5227878</doc-number>
<kind>A</kind>
<name>Puri et al.</name>
<date>19930700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524015</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>5258836</doc-number>
<kind>A</kind>
<name>Murata</name>
<date>19931100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>5274453</doc-number>
<kind>A</kind>
<name>Maeda</name>
<date>19931200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>5287420</doc-number>
<kind>A</kind>
<name>Barrett</name>
<date>19940200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>5298991</doc-number>
<kind>A</kind>
<name>Yagasaki et al.</name>
<date>19940300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>US</country>
<doc-number>5317397</doc-number>
<kind>A</kind>
<name>Odaka et al.</name>
<date>19940500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>US</country>
<doc-number>5319463</doc-number>
<kind>A</kind>
<name>Hongu et al.</name>
<date>19940600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00033">
<document-id>
<country>US</country>
<doc-number>5343248</doc-number>
<kind>A</kind>
<name>Fujinami</name>
<date>19940800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00034">
<document-id>
<country>US</country>
<doc-number>5347308</doc-number>
<kind>A</kind>
<name>Wai</name>
<date>19940900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00035">
<document-id>
<country>US</country>
<doc-number>5376968</doc-number>
<kind>A</kind>
<name>Wu et al.</name>
<date>19941200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00036">
<document-id>
<country>US</country>
<doc-number>5376971</doc-number>
<kind>A</kind>
<name>Kadono et al.</name>
<date>19941200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00037">
<document-id>
<country>US</country>
<doc-number>5379351</doc-number>
<kind>A</kind>
<name>Fandrianto et al.</name>
<date>19950100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00038">
<document-id>
<country>US</country>
<doc-number>5386234</doc-number>
<kind>A</kind>
<name>Veltman et al.</name>
<date>19950100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00039">
<document-id>
<country>US</country>
<doc-number>5400075</doc-number>
<kind>A</kind>
<name>Savatier</name>
<date>19950300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00040">
<document-id>
<country>US</country>
<doc-number>5412430</doc-number>
<kind>A</kind>
<name>Nagata</name>
<date>19950500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00041">
<document-id>
<country>US</country>
<doc-number>5412435</doc-number>
<kind>A</kind>
<name>Nakajima</name>
<date>19950500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00042">
<document-id>
<country>US</country>
<doc-number>RE34965</doc-number>
<kind>E</kind>
<name>Sugiyama</name>
<date>19950600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00043">
<document-id>
<country>US</country>
<doc-number>5422676</doc-number>
<kind>A</kind>
<name>Herpel et al.</name>
<date>19950600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00044">
<document-id>
<country>US</country>
<doc-number>5424779</doc-number>
<kind>A</kind>
<name>Odaka</name>
<date>19950600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00045">
<document-id>
<country>US</country>
<doc-number>5426464</doc-number>
<kind>A</kind>
<name>Casavant et al.</name>
<date>19950600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00046">
<document-id>
<country>US</country>
<doc-number>5428396</doc-number>
<kind>A</kind>
<name>Yagasaki et al.</name>
<date>19950600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00047">
<document-id>
<country>US</country>
<doc-number>5442400</doc-number>
<kind>A</kind>
<name>Sun</name>
<date>19950800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00048">
<document-id>
<country>US</country>
<doc-number>5448297</doc-number>
<kind>A</kind>
<name>Alattar et al.</name>
<date>19950900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00049">
<document-id>
<country>US</country>
<doc-number>5453799</doc-number>
<kind>A</kind>
<name>Yang et al.</name>
<date>19950900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00050">
<document-id>
<country>US</country>
<doc-number>5457495</doc-number>
<kind>A</kind>
<name>Hartung</name>
<date>19951000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00051">
<document-id>
<country>US</country>
<doc-number>5461421</doc-number>
<kind>A</kind>
<name>Moon</name>
<date>19951000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00052">
<document-id>
<country>US</country>
<doc-number>RE35093</doc-number>
<kind>E</kind>
<name>Wang et al.</name>
<date>19951100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00053">
<document-id>
<country>US</country>
<doc-number>5465118</doc-number>
<kind>A</kind>
<name>Hancock et al.</name>
<date>19951100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00054">
<document-id>
<country>US</country>
<doc-number>5467086</doc-number>
<kind>A</kind>
<name>Jeong</name>
<date>19951100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00055">
<document-id>
<country>US</country>
<doc-number>5467136</doc-number>
<kind>A</kind>
<name>Odaka</name>
<date>19951100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00056">
<document-id>
<country>US</country>
<doc-number>5477272</doc-number>
<kind>A</kind>
<name>Zhang et al.</name>
<date>19951200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00057">
<document-id>
<country>US</country>
<doc-number>RE35158</doc-number>
<kind>E</kind>
<name>Sugiyama</name>
<date>19960200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00058">
<document-id>
<country>US</country>
<doc-number>5491523</doc-number>
<kind>A</kind>
<name>Sato</name>
<date>19960200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00059">
<document-id>
<country>US</country>
<doc-number>5510840</doc-number>
<kind>A</kind>
<name>Yonemitsu et al.</name>
<date>19960400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00060">
<document-id>
<country>US</country>
<doc-number>5517327</doc-number>
<kind>A</kind>
<name>Nakatani et al.</name>
<date>19960500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00061">
<document-id>
<country>US</country>
<doc-number>5539466</doc-number>
<kind>A</kind>
<name>Igarashi et al.</name>
<date>19960700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00062">
<document-id>
<country>US</country>
<doc-number>5544286</doc-number>
<kind>A</kind>
<name>Laney</name>
<date>19960800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00063">
<document-id>
<country>US</country>
<doc-number>5546129</doc-number>
<kind>A</kind>
<name>Lee</name>
<date>19960800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00064">
<document-id>
<country>US</country>
<doc-number>5550541</doc-number>
<kind>A</kind>
<name>Todd</name>
<date>19960800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00065">
<document-id>
<country>US</country>
<doc-number>5550847</doc-number>
<kind>A</kind>
<name>Zhu</name>
<date>19960800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00066">
<document-id>
<country>US</country>
<doc-number>5552832</doc-number>
<kind>A</kind>
<name>Astle</name>
<date>19960900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00067">
<document-id>
<country>US</country>
<doc-number>5565922</doc-number>
<kind>A</kind>
<name>Krause</name>
<date>19961000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00068">
<document-id>
<country>US</country>
<doc-number>5574504</doc-number>
<kind>A</kind>
<name>Yagasaki et al.</name>
<date>19961100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00069">
<document-id>
<country>US</country>
<doc-number>5594504</doc-number>
<kind>A</kind>
<name>Ebrahimi</name>
<date>19970100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00070">
<document-id>
<country>US</country>
<doc-number>5594813</doc-number>
<kind>A</kind>
<name>Fandrianto et al.</name>
<date>19970100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00071">
<document-id>
<country>US</country>
<doc-number>5598215</doc-number>
<kind>A</kind>
<name>Watanabe</name>
<date>19970100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00072">
<document-id>
<country>US</country>
<doc-number>5598216</doc-number>
<kind>A</kind>
<name>Lee</name>
<date>19970100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00073">
<document-id>
<country>US</country>
<doc-number>5617144</doc-number>
<kind>A</kind>
<name>Lee</name>
<date>19970400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00074">
<document-id>
<country>US</country>
<doc-number>5619281</doc-number>
<kind>A</kind>
<name>Jung</name>
<date>19970400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00075">
<document-id>
<country>US</country>
<doc-number>5621481</doc-number>
<kind>A</kind>
<name>Yasuda et al.</name>
<date>19970400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00076">
<document-id>
<country>US</country>
<doc-number>5623311</doc-number>
<kind>A</kind>
<name>Phillips et al.</name>
<date>19970400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00077">
<document-id>
<country>US</country>
<doc-number>5648819</doc-number>
<kind>A</kind>
<name>Tranchard</name>
<date>19970700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00078">
<document-id>
<country>US</country>
<doc-number>5650829</doc-number>
<kind>A</kind>
<name>Sugimoto et al.</name>
<date>19970700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00079">
<document-id>
<country>US</country>
<doc-number>5654771</doc-number>
<kind>A</kind>
<name>Tekalp et al.</name>
<date>19970800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00080">
<document-id>
<country>US</country>
<doc-number>5659365</doc-number>
<kind>A</kind>
<name>Wilkinson</name>
<date>19970800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00081">
<document-id>
<country>US</country>
<doc-number>5666461</doc-number>
<kind>A</kind>
<name>Igarashi et al.</name>
<date>19970900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00082">
<document-id>
<country>US</country>
<doc-number>5668608</doc-number>
<kind>A</kind>
<name>Lee</name>
<date>19970900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00083">
<document-id>
<country>US</country>
<doc-number>5668932</doc-number>
<kind>A</kind>
<name>Laney</name>
<date>19970900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00084">
<document-id>
<country>US</country>
<doc-number>5687097</doc-number>
<kind>A</kind>
<name>Mizusawa et al.</name>
<date>19971100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00085">
<document-id>
<country>US</country>
<doc-number>5689305</doc-number>
<kind>A</kind>
<name>Ng et al.</name>
<date>19971100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524015</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00086">
<document-id>
<country>US</country>
<doc-number>5689306</doc-number>
<kind>A</kind>
<name>Jung</name>
<date>19971100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00087">
<document-id>
<country>US</country>
<doc-number>5692063</doc-number>
<kind>A</kind>
<name>Lee et al.</name>
<date>19971100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00088">
<document-id>
<country>US</country>
<doc-number>5694173</doc-number>
<kind>A</kind>
<name>Kimura et al.</name>
<date>19971200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00089">
<document-id>
<country>US</country>
<doc-number>5699117</doc-number>
<kind>A</kind>
<name>Uramoto et al.</name>
<date>19971200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00090">
<document-id>
<country>US</country>
<doc-number>5699476</doc-number>
<kind>A</kind>
<name>Van Der Meer</name>
<date>19971200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00091">
<document-id>
<country>US</country>
<doc-number>5701164</doc-number>
<kind>A</kind>
<name>Kato</name>
<date>19971200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00092">
<document-id>
<country>US</country>
<doc-number>5715005</doc-number>
<kind>A</kind>
<name>Masaki</name>
<date>19980200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00093">
<document-id>
<country>US</country>
<doc-number>5717441</doc-number>
<kind>A</kind>
<name>Serizawa et al.</name>
<date>19980200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00094">
<document-id>
<country>US</country>
<doc-number>5731850</doc-number>
<kind>A</kind>
<name>Maturi et al.</name>
<date>19980300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00095">
<document-id>
<country>US</country>
<doc-number>5748784</doc-number>
<kind>A</kind>
<name>Sugiyama</name>
<date>19980500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00096">
<document-id>
<country>US</country>
<doc-number>5748789</doc-number>
<kind>A</kind>
<name>Lee et al.</name>
<date>19980500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00097">
<document-id>
<country>US</country>
<doc-number>5767898</doc-number>
<kind>A</kind>
<name>Urano et al.</name>
<date>19980600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00098">
<document-id>
<country>US</country>
<doc-number>5784175</doc-number>
<kind>A</kind>
<name>Lee</name>
<date>19980700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00099">
<document-id>
<country>US</country>
<doc-number>5786860</doc-number>
<kind>A</kind>
<name>Kim et al.</name>
<date>19980700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00100">
<document-id>
<country>US</country>
<doc-number>5787203</doc-number>
<kind>A</kind>
<name>Lee et al.</name>
<date>19980700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00101">
<document-id>
<country>US</country>
<doc-number>5793897</doc-number>
<kind>A</kind>
<name>Jo et al.</name>
<date>19980800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00102">
<document-id>
<country>US</country>
<doc-number>5796855</doc-number>
<kind>A</kind>
<name>Lee</name>
<date>19980800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00103">
<document-id>
<country>US</country>
<doc-number>5799113</doc-number>
<kind>A</kind>
<name>Lee</name>
<date>19980800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00104">
<document-id>
<country>US</country>
<doc-number>RE35910</doc-number>
<kind>E</kind>
<name>Nagata et al.</name>
<date>19980900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00105">
<document-id>
<country>US</country>
<doc-number>5825830</doc-number>
<kind>A</kind>
<name>Kopf</name>
<date>19981000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00106">
<document-id>
<country>US</country>
<doc-number>5825929</doc-number>
<kind>A</kind>
<name>Chen et al.</name>
<date>19981000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00107">
<document-id>
<country>US</country>
<doc-number>5835144</doc-number>
<kind>A</kind>
<name>Matsumura et al.</name>
<date>19981100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00108">
<document-id>
<country>US</country>
<doc-number>5835146</doc-number>
<kind>A</kind>
<name>Stone</name>
<date>19981100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00109">
<document-id>
<country>US</country>
<doc-number>5835149</doc-number>
<kind>A</kind>
<name>Astle</name>
<date>19981100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00110">
<document-id>
<country>US</country>
<doc-number>5844613</doc-number>
<kind>A</kind>
<name>Chaddha</name>
<date>19981200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00111">
<document-id>
<country>US</country>
<doc-number>5847776</doc-number>
<kind>A</kind>
<name>Khmelnitsky et al.</name>
<date>19981200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00112">
<document-id>
<country>US</country>
<doc-number>5859668</doc-number>
<kind>A</kind>
<name>Aono et al.</name>
<date>19990100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00113">
<document-id>
<country>US</country>
<doc-number>5874995</doc-number>
<kind>A</kind>
<name>Naimpally et al.</name>
<date>19990200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00114">
<document-id>
<country>US</country>
<doc-number>5901248</doc-number>
<kind>A</kind>
<name>Fandrianto et al.</name>
<date>19990500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00115">
<document-id>
<country>US</country>
<doc-number>5905535</doc-number>
<kind>A</kind>
<name>Kerdranvat</name>
<date>19990500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00116">
<document-id>
<country>US</country>
<doc-number>5905542</doc-number>
<kind>A</kind>
<name>Linzer</name>
<date>19990500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00117">
<document-id>
<country>US</country>
<doc-number>5923375</doc-number>
<kind>A</kind>
<name>Pau</name>
<date>19990700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00118">
<document-id>
<country>US</country>
<doc-number>5926573</doc-number>
<kind>A</kind>
<name>Kim et al.</name>
<date>19990700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00119">
<document-id>
<country>US</country>
<doc-number>5929940</doc-number>
<kind>A</kind>
<name>Jeannin</name>
<date>19990700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00120">
<document-id>
<country>US</country>
<doc-number>5946042</doc-number>
<kind>A</kind>
<name>Kato</name>
<date>19990800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00121">
<document-id>
<country>US</country>
<doc-number>5946043</doc-number>
<kind>A</kind>
<name>Lee et al.</name>
<date>19990800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00122">
<document-id>
<country>US</country>
<doc-number>5949489</doc-number>
<kind>A</kind>
<name>Nishikawa et al.</name>
<date>19990900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00123">
<document-id>
<country>US</country>
<doc-number>5959673</doc-number>
<kind>A</kind>
<name>Lee et al.</name>
<date>19990900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00124">
<document-id>
<country>US</country>
<doc-number>5963258</doc-number>
<kind>A</kind>
<name>Nishikawa et al.</name>
<date>19991000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00125">
<document-id>
<country>US</country>
<doc-number>5963259</doc-number>
<kind>A</kind>
<name>Nakaya et al.</name>
<date>19991000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00126">
<document-id>
<country>US</country>
<doc-number>5963673</doc-number>
<kind>A</kind>
<name>Kodama et al.</name>
<date>19991000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00127">
<document-id>
<country>US</country>
<doc-number>5970173</doc-number>
<kind>A</kind>
<name>Lee et al.</name>
<date>19991000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00128">
<document-id>
<country>US</country>
<doc-number>5970175</doc-number>
<kind>A</kind>
<name>Nishikawa et al.</name>
<date>19991000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00129">
<document-id>
<country>US</country>
<doc-number>5973743</doc-number>
<kind>A</kind>
<name>Han</name>
<date>19991000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00130">
<document-id>
<country>US</country>
<doc-number>5973755</doc-number>
<kind>A</kind>
<name>Gabriel</name>
<date>19991000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00131">
<document-id>
<country>US</country>
<doc-number>5982437</doc-number>
<kind>A</kind>
<name>Okazaki et al.</name>
<date>19991100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00132">
<document-id>
<country>US</country>
<doc-number>5982438</doc-number>
<kind>A</kind>
<name>Lin et al.</name>
<date>19991100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00133">
<document-id>
<country>US</country>
<doc-number>5990960</doc-number>
<kind>A</kind>
<name>Murakami et al.</name>
<date>19991100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00134">
<document-id>
<country>US</country>
<doc-number>5991447</doc-number>
<kind>A</kind>
<name>Eifrig et al.</name>
<date>19991100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00135">
<document-id>
<country>US</country>
<doc-number>6002439</doc-number>
<kind>A</kind>
<name>Murakami et al.</name>
<date>19991200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00136">
<document-id>
<country>US</country>
<doc-number>6005980</doc-number>
<kind>A</kind>
<name>Eifrig et al.</name>
<date>19991200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00137">
<document-id>
<country>US</country>
<doc-number>RE36507</doc-number>
<kind>E</kind>
<name>Iu</name>
<date>20000100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00138">
<document-id>
<country>US</country>
<doc-number>6011596</doc-number>
<kind>A</kind>
<name>Burl et al.</name>
<date>20000100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00139">
<document-id>
<country>US</country>
<doc-number>6026195</doc-number>
<kind>A</kind>
<name>Eifrig et al.</name>
<date>20000200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00140">
<document-id>
<country>US</country>
<doc-number>6035070</doc-number>
<kind>A</kind>
<name>Moon et al.</name>
<date>20000300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00141">
<document-id>
<country>US</country>
<doc-number>6040863</doc-number>
<kind>A</kind>
<name>Kato</name>
<date>20000300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00142">
<document-id>
<country>US</country>
<doc-number>6052150</doc-number>
<kind>A</kind>
<name>Kikuchi</name>
<date>20000400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00143">
<document-id>
<country>US</country>
<doc-number>6057884</doc-number>
<kind>A</kind>
<name>Chen et al.</name>
<date>20000500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00144">
<document-id>
<country>US</country>
<doc-number>6058212</doc-number>
<kind>A</kind>
<name>Yokohama</name>
<date>20000500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00145">
<document-id>
<country>US</country>
<doc-number>6067322</doc-number>
<kind>A</kind>
<name>Wang</name>
<date>20000500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00146">
<document-id>
<country>US</country>
<doc-number>6081209</doc-number>
<kind>A</kind>
<name>Schuyler et al.</name>
<date>20000600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00147">
<document-id>
<country>US</country>
<doc-number>6094225</doc-number>
<kind>A</kind>
<name>Han</name>
<date>20000700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00148">
<document-id>
<country>US</country>
<doc-number>RE36822</doc-number>
<kind>E</kind>
<name>Sugiyama</name>
<date>20000800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00149">
<document-id>
<country>US</country>
<doc-number>6097759</doc-number>
<kind>A</kind>
<name>Murakami et al.</name>
<date>20000800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00150">
<document-id>
<country>US</country>
<doc-number>6111914</doc-number>
<kind>A</kind>
<name>Bist</name>
<date>20000800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00151">
<document-id>
<country>US</country>
<doc-number>6130963</doc-number>
<kind>A</kind>
<name>Uz et al.</name>
<date>20001000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00152">
<document-id>
<country>US</country>
<doc-number>6148027</doc-number>
<kind>A</kind>
<name>Song et al.</name>
<date>20001100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00153">
<document-id>
<country>US</country>
<doc-number>6148033</doc-number>
<kind>A</kind>
<name>Pearlstein et al.</name>
<date>20001100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00154">
<document-id>
<country>US</country>
<doc-number>6154495</doc-number>
<kind>A</kind>
<name>Yamaguchi et al.</name>
<date>20001100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00155">
<document-id>
<country>US</country>
<doc-number>6167090</doc-number>
<kind>A</kind>
<name>Iizuka</name>
<date>20001200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00156">
<document-id>
<country>US</country>
<doc-number>6188725</doc-number>
<kind>B1</kind>
<name>Sugiyama</name>
<date>20010200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00157">
<document-id>
<country>US</country>
<doc-number>6188794</doc-number>
<kind>B1</kind>
<name>Nishikawa et al.</name>
<date>20010200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00158">
<document-id>
<country>US</country>
<doc-number>6201927</doc-number>
<kind>B1</kind>
<name>Comer</name>
<date>20010300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00159">
<document-id>
<country>US</country>
<doc-number>6205176</doc-number>
<kind>B1</kind>
<name>Sugiyama</name>
<date>20010300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00160">
<document-id>
<country>US</country>
<doc-number>6208761</doc-number>
<kind>B1</kind>
<name>Passagio et al.</name>
<date>20010300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00161">
<document-id>
<country>US</country>
<doc-number>6215905</doc-number>
<kind>B1</kind>
<name>Lee et al.</name>
<date>20010400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00162">
<document-id>
<country>US</country>
<doc-number>6219070</doc-number>
<kind>B1</kind>
<name>Baker et al.</name>
<date>20010400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00163">
<document-id>
<country>US</country>
<doc-number>6219464</doc-number>
<kind>B1</kind>
<name>Greggain et al.</name>
<date>20010400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00164">
<document-id>
<country>US</country>
<doc-number>6233017</doc-number>
<kind>B1</kind>
<name>Chaddha</name>
<date>20010500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00165">
<document-id>
<country>US</country>
<doc-number>6236806</doc-number>
<kind>B1</kind>
<name>Kojima et al.</name>
<date>20010500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00166">
<document-id>
<country>US</country>
<doc-number>RE37222</doc-number>
<kind>E</kind>
<name>Yonemitsu</name>
<date>20010600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00167">
<document-id>
<country>US</country>
<doc-number>6243418</doc-number>
<kind>B1</kind>
<name>Kim</name>
<date>20010600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00168">
<document-id>
<country>US</country>
<doc-number>6259741</doc-number>
<kind>B1</kind>
<name>Chen et al.</name>
<date>20010700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00169">
<document-id>
<country>US</country>
<doc-number>6263024</doc-number>
<kind>B1</kind>
<name>Matsumoto</name>
<date>20010700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00170">
<document-id>
<country>US</country>
<doc-number>6263065</doc-number>
<kind>B1</kind>
<name>Durinovic-Johri et al.</name>
<date>20010700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00171">
<document-id>
<country>US</country>
<doc-number>6266091</doc-number>
<kind>B1</kind>
<name>Saha et al.</name>
<date>20010700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00172">
<document-id>
<country>US</country>
<doc-number>6271885</doc-number>
<kind>B2</kind>
<name>Sugiyama</name>
<date>20010800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00173">
<document-id>
<country>US</country>
<doc-number>6272179</doc-number>
<kind>B1</kind>
<name>Kadono</name>
<date>20010800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00174">
<document-id>
<country>US</country>
<doc-number>6275528</doc-number>
<kind>B1</kind>
<name>Isozaki et al.</name>
<date>20010800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00175">
<document-id>
<country>US</country>
<doc-number>6275531</doc-number>
<kind>B1</kind>
<name>Li</name>
<date>20010800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00176">
<document-id>
<country>US</country>
<doc-number>6281942</doc-number>
<kind>B1</kind>
<name>Wang</name>
<date>20010800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00177">
<document-id>
<country>US</country>
<doc-number>6282243</doc-number>
<kind>B1</kind>
<name>Kazui et al.</name>
<date>20010800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00178">
<document-id>
<country>US</country>
<doc-number>6289049</doc-number>
<kind>B1</kind>
<name>Kim et al.</name>
<date>20010900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00179">
<document-id>
<country>US</country>
<doc-number>6289132</doc-number>
<kind>B1</kind>
<name>Goertzen</name>
<date>20010900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00180">
<document-id>
<country>US</country>
<doc-number>6292585</doc-number>
<kind>B1</kind>
<name>Yamaguchi et al.</name>
<date>20010900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00181">
<document-id>
<country>US</country>
<doc-number>6295376</doc-number>
<kind>B1</kind>
<name>Nakaya</name>
<date>20010900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00182">
<document-id>
<country>US</country>
<doc-number>6307887</doc-number>
<kind>B1</kind>
<name>Gabriel</name>
<date>20011000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00183">
<document-id>
<country>US</country>
<doc-number>6307973</doc-number>
<kind>B2</kind>
<name>Nishikawa et al.</name>
<date>20011000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00184">
<document-id>
<country>US</country>
<doc-number>6310918</doc-number>
<kind>B1</kind>
<name>Saha et al.</name>
<date>20011000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00185">
<document-id>
<country>US</country>
<doc-number>6320593</doc-number>
<kind>B1</kind>
<name>Sobel et al.</name>
<date>20011100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00186">
<document-id>
<country>US</country>
<doc-number>6324216</doc-number>
<kind>B1</kind>
<name>Igarashi</name>
<date>20011100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00187">
<document-id>
<country>US</country>
<doc-number>6337881</doc-number>
<kind>B1</kind>
<name>Chaddha</name>
<date>20020100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00188">
<document-id>
<country>US</country>
<doc-number>6339656</doc-number>
<kind>B1</kind>
<name>Marui</name>
<date>20020100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00189">
<document-id>
<country>US</country>
<doc-number>6377628</doc-number>
<kind>B1</kind>
<name>Schultz et al.</name>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00190">
<document-id>
<country>US</country>
<doc-number>6381275</doc-number>
<kind>B1</kind>
<name>Fukuhara et al.</name>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00191">
<document-id>
<country>US</country>
<doc-number>6381277</doc-number>
<kind>B1</kind>
<name>Chun et al.</name>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00192">
<document-id>
<country>US</country>
<doc-number>6381279</doc-number>
<kind>B1</kind>
<name>Taubman</name>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00193">
<document-id>
<country>US</country>
<doc-number>6393059</doc-number>
<kind>B1</kind>
<name>Sugiyama</name>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00194">
<document-id>
<country>US</country>
<doc-number>6396876</doc-number>
<kind>B1</kind>
<name>Babonneau et al.</name>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00195">
<document-id>
<country>US</country>
<doc-number>6404813</doc-number>
<kind>B1</kind>
<name>Haskell et al.</name>
<date>20020600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00196">
<document-id>
<country>US</country>
<doc-number>6408029</doc-number>
<kind>B1</kind>
<name>McVeigh et al.</name>
<date>20020600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00197">
<document-id>
<country>US</country>
<doc-number>6418166</doc-number>
<kind>B1</kind>
<name>Wu et al.</name>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00198">
<document-id>
<country>US</country>
<doc-number>6421383</doc-number>
<kind>B2</kind>
<name>Beattie</name>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00199">
<document-id>
<country>US</country>
<doc-number>6430316</doc-number>
<kind>B1</kind>
<name>Wilkinson</name>
<date>20020800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00200">
<document-id>
<country>US</country>
<doc-number>6441842</doc-number>
<kind>B1</kind>
<name>Fandrianto et al.</name>
<date>20020800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00201">
<document-id>
<country>US</country>
<doc-number>6442204</doc-number>
<kind>B1</kind>
<name>Snook et al.</name>
<date>20020800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00202">
<document-id>
<country>US</country>
<doc-number>6449312</doc-number>
<kind>B1</kind>
<name>Zhang et al.</name>
<date>20020900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00203">
<document-id>
<country>US</country>
<doc-number>6483928</doc-number>
<kind>B1</kind>
<name>Bagni et al.</name>
<date>20021100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00204">
<document-id>
<country>US</country>
<doc-number>6493392</doc-number>
<kind>B1</kind>
<name>Chung et al.</name>
<date>20021200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00205">
<document-id>
<country>US</country>
<doc-number>6496608</doc-number>
<kind>B1</kind>
<name>Chui</name>
<date>20021200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00206">
<document-id>
<country>US</country>
<doc-number>6498810</doc-number>
<kind>B1</kind>
<name>Kim et al.</name>
<date>20021200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00207">
<document-id>
<country>US</country>
<doc-number>6519005</doc-number>
<kind>B2</kind>
<name>Bakhmutsky et al.</name>
<date>20030200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00208">
<document-id>
<country>US</country>
<doc-number>6519287</doc-number>
<kind>B1</kind>
<name>Hawkins et al.</name>
<date>20030200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00209">
<document-id>
<country>US</country>
<doc-number>6529632</doc-number>
<kind>B1</kind>
<name>Nakaya et al.</name>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00210">
<document-id>
<country>US</country>
<doc-number>6539056</doc-number>
<kind>B1</kind>
<name>Sato et al.</name>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00211">
<document-id>
<country>US</country>
<doc-number>6563953</doc-number>
<kind>B2</kind>
<name>Lin et al.</name>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00212">
<document-id>
<country>US</country>
<doc-number>6647061</doc-number>
<kind>B1</kind>
<name>Panusopone et al.</name>
<date>20031100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00213">
<document-id>
<country>US</country>
<doc-number>6650781</doc-number>
<kind>B2</kind>
<name>Nakaya</name>
<date>20031100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00214">
<document-id>
<country>US</country>
<doc-number>6661470</doc-number>
<kind>B1</kind>
<name>Kawakami et al.</name>
<date>20031200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00215">
<document-id>
<country>US</country>
<doc-number>6671323</doc-number>
<kind>B1</kind>
<name>Tahara et al.</name>
<date>20031200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00216">
<document-id>
<country>US</country>
<doc-number>6704360</doc-number>
<kind>B2</kind>
<name>Haskell et al.</name>
<date>20040300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00217">
<document-id>
<country>US</country>
<doc-number>6728317</doc-number>
<kind>B1</kind>
<name>Demos</name>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00218">
<document-id>
<country>US</country>
<doc-number>RE38563</doc-number>
<kind>E</kind>
<name>Eifrig et al.</name>
<date>20040800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00219">
<document-id>
<country>US</country>
<doc-number>6778610</doc-number>
<kind>B2</kind>
<name>Lin</name>
<date>20040800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00220">
<document-id>
<country>US</country>
<doc-number>6782053</doc-number>
<kind>B1</kind>
<name>Lainema</name>
<date>20040800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00221">
<document-id>
<country>US</country>
<doc-number>6950469</doc-number>
<kind>B2</kind>
<name>Karczewicz et al.</name>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00222">
<document-id>
<country>US</country>
<doc-number>6968008</doc-number>
<kind>B1</kind>
<name>Ribas-Corbera et al.</name>
<date>20051100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00223">
<document-id>
<country>US</country>
<doc-number>6980596</doc-number>
<kind>B2</kind>
<name>Wang et al.</name>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00224">
<document-id>
<country>US</country>
<doc-number>6983018</doc-number>
<kind>B1</kind>
<name>Lin et al.</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00225">
<document-id>
<country>US</country>
<doc-number>7020200</doc-number>
<kind>B2</kind>
<name>Winger</name>
<date>20060300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00226">
<document-id>
<country>US</country>
<doc-number>7023919</doc-number>
<kind>B2</kind>
<name>Cho et al.</name>
<date>20060400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00227">
<document-id>
<country>US</country>
<doc-number>7233621</doc-number>
<kind>B2</kind>
<name>Jeon</name>
<date>20070600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00228">
<document-id>
<country>US</country>
<doc-number>7295616</doc-number>
<kind>B2</kind>
<name>Sun et al.</name>
<date>20071100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00229">
<document-id>
<country>US</country>
<doc-number>7317839</doc-number>
<kind>B2</kind>
<name>Holcomb</name>
<date>20080100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00230">
<document-id>
<country>US</country>
<doc-number>7453941</doc-number>
<kind>B1</kind>
<name>Yamori et al.</name>
<date>20081100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00231">
<document-id>
<country>US</country>
<doc-number>7486734</doc-number>
<kind>B2</kind>
<name>Machida</name>
<date>20090200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00232">
<document-id>
<country>US</country>
<doc-number>7567617</doc-number>
<kind>B2</kind>
<name>Holcomb</name>
<date>20090700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00233">
<document-id>
<country>US</country>
<doc-number>7616692</doc-number>
<kind>B2</kind>
<name>Holcomb et al.</name>
<date>20091100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00234">
<document-id>
<country>US</country>
<doc-number>7623574</doc-number>
<kind>B2</kind>
<name>Holcomb</name>
<date>20091100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00235">
<document-id>
<country>US</country>
<doc-number>7630438</doc-number>
<kind>B2</kind>
<name>Mukerjee et al.</name>
<date>20091200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00236">
<document-id>
<country>US</country>
<doc-number>7742529</doc-number>
<kind>B1</kind>
<name>Ghanbari</name>
<date>20100600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00237">
<document-id>
<country>US</country>
<doc-number>7822120</doc-number>
<kind>B2</kind>
<name>Kondo et al.</name>
<date>20101000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00238">
<document-id>
<country>US</country>
<doc-number>2001/0019586</doc-number>
<kind>A1</kind>
<name>Kang et al.</name>
<date>20010900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00239">
<document-id>
<country>US</country>
<doc-number>2001/0050957</doc-number>
<kind>A1</kind>
<name>Nakaya et al.</name>
<date>20011200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00240">
<document-id>
<country>US</country>
<doc-number>2002/0122488</doc-number>
<kind>A1</kind>
<name>Takahashi et al.</name>
<date>20020900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00241">
<document-id>
<country>US</country>
<doc-number>2002/0186890</doc-number>
<kind>A1</kind>
<name>Lee et al.</name>
<date>20021200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00242">
<document-id>
<country>US</country>
<doc-number>2003/0072374</doc-number>
<kind>A1</kind>
<name>Sohm</name>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00243">
<document-id>
<country>US</country>
<doc-number>2003/0076883</doc-number>
<kind>A1</kind>
<name>Bottreau et al.</name>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00244">
<document-id>
<country>US</country>
<doc-number>2003/0095603</doc-number>
<kind>A1</kind>
<name>Lan et al.</name>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00245">
<document-id>
<country>US</country>
<doc-number>2003/0099292</doc-number>
<kind>A1</kind>
<name>Wang et al.</name>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00246">
<document-id>
<country>US</country>
<doc-number>2003/0099294</doc-number>
<kind>A1</kind>
<name>Wang et al.</name>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00247">
<document-id>
<country>US</country>
<doc-number>2003/0112864</doc-number>
<kind>A1</kind>
<name>Karczewicz et al.</name>
<date>20030600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00248">
<document-id>
<country>US</country>
<doc-number>2003/0113026</doc-number>
<kind>A1</kind>
<name>Srinivasan et al.</name>
<date>20030600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00249">
<document-id>
<country>US</country>
<doc-number>2003/0142748</doc-number>
<kind>A1</kind>
<name>Tourapis</name>
<date>20030700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00250">
<document-id>
<country>US</country>
<doc-number>2003/0152146</doc-number>
<kind>A1</kind>
<name>Lin et al.</name>
<date>20030800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00251">
<document-id>
<country>US</country>
<doc-number>2003/0156646</doc-number>
<kind>A1</kind>
<name>Hsu et al.</name>
<date>20030800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00252">
<document-id>
<country>US</country>
<doc-number>2003/0161402</doc-number>
<kind>A1</kind>
<name>Horowitz</name>
<date>20030800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00253">
<document-id>
<country>US</country>
<doc-number>2003/0179826</doc-number>
<kind>A1</kind>
<name>Jeon</name>
<date>20030900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00254">
<document-id>
<country>US</country>
<doc-number>2003/0202705</doc-number>
<kind>A1</kind>
<name>Sun</name>
<date>20031000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00255">
<document-id>
<country>US</country>
<doc-number>2004/0057523</doc-number>
<kind>A1</kind>
<name>Koto et al.</name>
<date>20040300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00256">
<document-id>
<country>US</country>
<doc-number>2004/0136461</doc-number>
<kind>A1</kind>
<name>Kondo et al.</name>
<date>20040700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00257">
<document-id>
<country>US</country>
<doc-number>2005/0013497</doc-number>
<kind>A1</kind>
<name>Hsu et al.</name>
<date>20050100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00258">
<document-id>
<country>US</country>
<doc-number>2005/0013498</doc-number>
<kind>A1</kind>
<name>Srinivasan</name>
<date>20050100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00259">
<document-id>
<country>US</country>
<doc-number>2005/0036700</doc-number>
<kind>A1</kind>
<name>Lan et al.</name>
<date>20050200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00260">
<document-id>
<country>US</country>
<doc-number>2005/0036759</doc-number>
<kind>A1</kind>
<name>Lin et al.</name>
<date>20050200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00261">
<document-id>
<country>US</country>
<doc-number>2005/0053137</doc-number>
<kind>A1</kind>
<name>Holcomb</name>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00262">
<document-id>
<country>US</country>
<doc-number>2005/0053147</doc-number>
<kind>A1</kind>
<name>Mukerjee et al.</name>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00263">
<document-id>
<country>US</country>
<doc-number>2005/0053149</doc-number>
<kind>A1</kind>
<name>Mukerjee et al.</name>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00264">
<document-id>
<country>US</country>
<doc-number>2005/0053292</doc-number>
<kind>A1</kind>
<name>Mukerjee et al.</name>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00265">
<document-id>
<country>US</country>
<doc-number>2005/0058205</doc-number>
<kind>A1</kind>
<name>Holcomb et al.</name>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00266">
<document-id>
<country>US</country>
<doc-number>2005/0100093</doc-number>
<kind>A1</kind>
<name>Holcomb</name>
<date>20050500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00267">
<document-id>
<country>US</country>
<doc-number>2005/0226335</doc-number>
<kind>A1</kind>
<name>Lee et al.</name>
<date>20051000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00268">
<document-id>
<country>US</country>
<doc-number>2006/0013307</doc-number>
<kind>A1</kind>
<name>Olivier et al.</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00269">
<document-id>
<country>EP</country>
<doc-number>0 279 053</doc-number>
<date>19880800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00270">
<document-id>
<country>EP</country>
<doc-number>0 397 402</doc-number>
<date>19901100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00271">
<document-id>
<country>EP</country>
<doc-number>0 526 163</doc-number>
<date>19930200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00272">
<document-id>
<country>EP</country>
<doc-number>0 535 746</doc-number>
<date>19930400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00273">
<document-id>
<country>EP</country>
<doc-number>0 540 350</doc-number>
<date>19930500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00274">
<document-id>
<country>EP</country>
<doc-number>0 542 474</doc-number>
<date>19930500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00275">
<document-id>
<country>EP</country>
<doc-number>0 588 653</doc-number>
<date>19940300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00276">
<document-id>
<country>EP</country>
<doc-number>0 614 318</doc-number>
<date>19940900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00277">
<document-id>
<country>EP</country>
<doc-number>0 625 853</doc-number>
<date>19941100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00278">
<document-id>
<country>EP</country>
<doc-number>0 651 574</doc-number>
<date>19950500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00279">
<document-id>
<country>EP</country>
<doc-number>0 676 900</doc-number>
<date>19951000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00280">
<document-id>
<country>EP</country>
<doc-number>0 771 114</doc-number>
<date>19970500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00281">
<document-id>
<country>EP</country>
<doc-number>0 786 907</doc-number>
<date>19970700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00282">
<document-id>
<country>EP</country>
<doc-number>0 825 778</doc-number>
<date>19980200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00283">
<document-id>
<country>EP</country>
<doc-number>0 830 029</doc-number>
<date>19980300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00284">
<document-id>
<country>EP</country>
<doc-number>0 863 674</doc-number>
<date>19980900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00285">
<document-id>
<country>EP</country>
<doc-number>0 863 675</doc-number>
<date>19980900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00286">
<document-id>
<country>EP</country>
<doc-number>0 884 912</doc-number>
<date>19981200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00287">
<document-id>
<country>EP</country>
<doc-number>0 944 245</doc-number>
<date>19990900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00288">
<document-id>
<country>EP</country>
<doc-number>1 335 609</doc-number>
<date>20030800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00289">
<document-id>
<country>EP</country>
<doc-number>1 411 729</doc-number>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00290">
<document-id>
<country>GB</country>
<doc-number>2328337</doc-number>
<date>19990200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00291">
<document-id>
<country>GB</country>
<doc-number>2332115</doc-number>
<date>19990600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00292">
<document-id>
<country>GB</country>
<doc-number>2343579</doc-number>
<date>20000500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00293">
<document-id>
<country>JP</country>
<doc-number>61205086</doc-number>
<date>19860900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00294">
<document-id>
<country>JP</country>
<doc-number>62 213 494</doc-number>
<date>19870900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00295">
<document-id>
<country>JP</country>
<doc-number>3001688</doc-number>
<date>19910100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00296">
<document-id>
<country>JP</country>
<doc-number>3 129 986</doc-number>
<date>19910300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00297">
<document-id>
<country>JP</country>
<doc-number>05-199422</doc-number>
<date>19930800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00298">
<document-id>
<country>JP</country>
<doc-number>6 078 295</doc-number>
<date>19940300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00299">
<document-id>
<country>JP</country>
<doc-number>6 078 298</doc-number>
<date>19940300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00300">
<document-id>
<country>JP</country>
<doc-number>06-276481</doc-number>
<date>19940900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00301">
<document-id>
<country>JP</country>
<doc-number>06-276511</doc-number>
<date>19940900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00302">
<document-id>
<country>JP</country>
<doc-number>6292188</doc-number>
<date>19941000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00303">
<document-id>
<country>JP</country>
<doc-number>07-087331</doc-number>
<date>19950300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00304">
<document-id>
<country>JP</country>
<doc-number>7-274171</doc-number>
<date>19951000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00305">
<document-id>
<country>JP</country>
<doc-number>08-140099</doc-number>
<date>19960500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00306">
<document-id>
<country>JP</country>
<doc-number>08-251601</doc-number>
<date>19960900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00307">
<document-id>
<country>JP</country>
<doc-number>09-322163</doc-number>
<date>19971200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00308">
<document-id>
<country>JP</country>
<doc-number>10 056 644</doc-number>
<date>19980200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00309">
<document-id>
<country>JP</country>
<doc-number>10-271512</doc-number>
<date>19981000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00310">
<document-id>
<country>JP</country>
<doc-number>11-004441</doc-number>
<date>19990100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00311">
<document-id>
<country>JP</country>
<doc-number>11-075191</doc-number>
<date>19990300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00312">
<document-id>
<country>JP</country>
<doc-number>11-196420</doc-number>
<date>19990700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00313">
<document-id>
<country>JP</country>
<doc-number>2001-346215</doc-number>
<date>20011200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00314">
<document-id>
<country>JP</country>
<doc-number>2002-532027</doc-number>
<date>20020900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00315">
<document-id>
<country>JP</country>
<doc-number>2004-048711</doc-number>
<date>20040200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00316">
<document-id>
<country>JP</country>
<doc-number>2004-215229</doc-number>
<date>20040700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00317">
<document-id>
<country>JP</country>
<doc-number>2005-510985</doc-number>
<date>20050400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00318">
<document-id>
<country>KR</country>
<doc-number>10-0233764</doc-number>
<date>19991200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00319">
<document-id>
<country>WO</country>
<doc-number>WO 00/33581</doc-number>
<date>20000800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00320">
<document-id>
<country>WO</country>
<doc-number>WO 01/95633</doc-number>
<date>20011200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00321">
<document-id>
<country>WO</country>
<doc-number>WO 03/026296</doc-number>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00322">
<document-id>
<country>WO</country>
<doc-number>WO 03/026315</doc-number>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00323">
<document-id>
<country>WO</country>
<doc-number>WO 03/047272</doc-number>
<date>20030600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00324">
<document-id>
<country>WO</country>
<doc-number>WO 03/063503</doc-number>
<date>20030700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00325">
<othercit>U.S. Appl. No. 60/341,674, filed Dec. 17, 2001, Lee et al.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00326">
<othercit>U.S. Appl. No. 60/488,710, filed Jul. 18, 2003, Srinivasan et al.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00327">
<othercit>U.S. Appl. No. 60/501,081, filed Sep. 7, 2003, Srinivasan et al.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00328">
<othercit>U.S. Appl. No. 60/501,133, filed Sep. 7, 2003, Holcomb et al.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00329">
<othercit>Bartkowiak et al., &#x201c;Color Video Compression Based on Chrominance Vector Quantization,&#x201d; <i>7th Int'l Workshop on Systems, Signals and Image Processing, IWSSIP 2000</i>, Maribor 7-9 VI, pp. 107-110 (Jun. 2000).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00330">
<othercit>Benzler et al., &#x201c;Improving multiresolution motion compensating hybrid coding by drift reduction,&#x201d; <i>Picture Coding Symposium</i>, 4 pp. (Mar. 1996).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00331">
<othercit>Benzler et al., &#x201c;Motion and aliasing compensating prediction with quarter-pel accuracy and adaptive overlapping blocks as proposal for MPEG-4 tool evaluation&#x2014;Technical description,&#x201d; ISO/IEC JTC1/SC29/WG11, MPEG 95/0552, 5 pp. (document marked Dec. 1995).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00332">
<othercit>Benzler, &#x201c;Results of core experiments P8 (Motion and Aliasing Compensating Prediction),&#x201d; ISO/IEC JTC1/SC29/WG11, MPEG 97/2625, 8 pp. (document marked Oct. 1997).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00333">
<othercit>Borman et al., &#x201c;Block-matching Sub-pixel Motion Estimation from Noisy, Under-Sampled Frames&#x2014;an Empirical Performance Evaluation,&#x201d; <i>SPIE Visual Comm. </i>&#x26; <i>Image Processing</i>, 10 pp. (Jan. 1999).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00334">
<othercit>Conklin et al., &#x201c;Multi-resolution Motion Estimation,&#x201d; <i>Proc. ICASSP '97</i>, Munich, Germany, 4 pp. (Apr. 1997).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00335">
<othercit>Davis et al., &#x201c;Equivalence of subpixel motion estimators based on optical flow and block matching,&#x201d; <i>Proc. IEEE Int'l Symposium on Computer Vision</i>, pp. 7-12 (Nov. 1995).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00336">
<othercit>de Haan et al., &#x201c;Sub-pixel motion estimation with 3-D recursive search block-matching,&#x201d; <i>Signal Processing: Image Comm.6</i>, pp. 229-239 (Jun. 1994).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00337">
<othercit>&#x201c;DivX Multi Standard Video Encoder,&#x201d; 2 pp. (Downloaded from the World Wide Web on Jan. 24, 2006).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00338">
<othercit>Ericsson, &#x201c;Fixed and Adaptive Predictors for Hybrid Predictive/Transform Coding,&#x201d; IEEE Transactions on Comm., vol. COM-33, No. 12, pp. 1291-1302 (Dec. 1985).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00339">
<othercit>Flierl et al., &#x201c;Multihypothesis Motion Estimation for Video Coding,&#x201d; <i>Proc. DCC</i>, 10 pp. (Mar. 2001).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00340">
<othercit>Fogg, &#x201c;Survey of Software and Hardware VLC Architectures,&#x201d; <i>SPIE</i>, vol. 2186, pp. 29-37 (Feb. 9-10, 1994).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00341">
<othercit>Girod, &#x201c;Efficiency Analysis of Multihypothesis Motion-Compensated Prediction for Video Coding,&#x201d; <i>IEEE Transactions on Image Processing</i>, vol. 9, No. 2, pp. 173-183 (Feb. 2000).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00342">
<othercit>Girod, &#x201c;Motion-Compensating Prediction with Fractional-Pel Accuracy,&#x201d; <i>IEEE Transactions on Comm.</i>, vol. 41, No. 4, pp. 604-612 (Apr. 1993).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00343">
<othercit>Girod, &#x201c;Motion Compensation: Visual Aspects, Accuracy, and Fundamental Limits,&#x201d; <i>Motion Analysis and Image Sequence Processing</i>, Kluwer Academic Publishers, pp. 125-152 (Mar. 1993).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00344">
<othercit>Horn et al., &#x201c;Estimation of Motion Vector Fields for Multiscale Motion Compensation,&#x201d; <i>Proc. Picture Coding Symp</i>. (<i>PCS 97</i>), pp. 141-144 (Sep. 1997).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00345">
<othercit>Hsu et al., &#x201c;A Low Bit-Rate Video Codec Based on Two-Dimensional Mesh Motion Compensation with Adaptive Interpolation,&#x201d; <i>IEEE Transactions on Circuits and Systems for Video Technology</i>, vol. II, No. 1, pp. 111-117 (Jan. 2001).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00346">
<othercit>IBM Technical Disclosure Bulletin, &#x201c;Advanced Motion Estimation for Moving Picture Expert Group Encoders,&#x201d; vol. 39, No. 4, pp. 323-324 (Apr. 1996).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00347">
<othercit>ISO/IEC, &#x201c;Information Technology&#x2014;Coding of Audio-Visual Objects: Visual, ISO/IEC 14496-2, Committee Draft,&#x201d; 330 pp. (Mar. 1998).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00348">
<othercit>ISO/IEC, &#x201c;MPEG-4 Video Verification Model Version 10.0,&#x201d; ISO/IEC JTC1/SC29/WG11, MPEG98/N1992, (ed. Ebrahimi) (document marked Feb. 1998).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00349">
<othercit>ISO/IEC, &#x201c;ISO/IEC 11172-2, Information Technology&#x2014;Coding of Moving Pictures and Associated Audio for Digital Storage Media at up to about 1.5 Mbit/s&#x2014;Part 2: Video,&#x201d; 112 pp. (Aug. 1993).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00350">
<othercit>ITU-T, &#x201c;ITU-T Recommendation H.261, Video Codec for Audiovisual Services at <i>p </i>x 64 kbits,&#x201d; 25 pp. (Mar. 1993).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00351">
<othercit>ITU-T, &#x201c;ITU-T Recommendation H.262, Information Technology&#x2014;Generic Coding of Moving Pictures and Associated Audio Information: Video,&#x201d; 205 pp. (Jul. 1995).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00352">
<othercit>ITU-T, &#x201c;ITU-T Recommendation H.263 Video Coding for Low Bit Rate Communication,&#x201d; 167 pp. (Feb. 1998).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00353">
<othercit>ITU&#x2014;Q15-F-24, &#x201c;MVC Video Codec&#x2014;Proposal for H.26L,&#x201d; Study Group 16, Video Coding Experts Group (Question 15), 28 pp. (document marked as generated in Oct. 1998).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00354">
<othercit>ITU-T, &#x201c;H.26L Test Model Long Term No. 5 (TML-5) draft0,&#x201d; Study Group 16, Video Coding Experts Group (Question 15), Document Q15-K-59, 35 pp. (ed. Gisle Bjontegaard) (Document dated Oct. 2000).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00355">
<othercit>Iwahashi et al., &#x201c;A Motion Compensation Technique for Downscaled Pictures in Layered Coding,&#x201d; <i>IEICE Transactions on Comm.</i>, vol. E77-B, No. 8, pp. 1007-1012 (Aug. 1994).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00356">
<othercit>Jeong et al., &#x201c;Adaptive Huffman Coding of 2-D DCT Coefficients for Image Sequence Compression,&#x201d; <i>Signal Processing: Image Communication</i>, vol. 7, 11 pp. (Mar. 1995).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00357">
<othercit>Joint Video Team (JVT) of ISO/IEC MPEG and ITU-T VCEG, &#x201c;Joint Model No. 1, Revision 1 (JM-1r1),&#x201d; JVT-A003r1, Pattaya, Thailand, 80 pp. (Dec. 2001) [document marked &#x201c;Generated: Jan. 18, 2002&#x201d;].</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00358">
<othercit>Joint Video Team (JVT) of ISO/IEC MPEG and ITU-T VCEG, &#x201c;Joint Committee Draft (CD),&#x201d; JVT-C167, 3rd Meeting: Fairfax, Virginia, USA, 142 pp. (May 2002).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00359">
<othercit>Joint Video Team (JVT) of ISO/IEC MPEG and ITU-T VCEG, &#x201c;Final Joint Committee Draft of Joint Video Specification (ITU-T Recommendation H.264, ISO/IEC 14496-10 AVC,&#x201d; JVT-D157, 218 pp. (Aug. 2002).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00360">
<othercit>Keys, &#x201c;Cubic Convolution Interpolation for Digital Image Processing,&#x201d; <i>IEEE Transactions on Acoustics, Speech </i>&#x26; <i>Signal Processing</i>, vol. ASSP-29, No. 6, pp. 1153-1160 (Dec. 1981).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00361">
<othercit>Konrad et al., &#x201c;On Motion Modeling and Estimation for Very Low Bit Rate Video Coding,&#x201d; <i>Visual Comm. </i>&#x26; <i>Image Processing </i>(<i>VCIP '95</i>), 12 pp. (May 1995).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00362">
<othercit>Kossentini et al., &#x201c;Predictive RD Optimized Motion Estimation for Very Low Bit-rate Video Coding,&#x201d; IEEE J. on Selected Areas in Communications, vol. 15, No. 9 pp. 1752-1763 (Dec. 1997).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00363">
<othercit>Lopes et al., &#x201c;Analysis of Spatial Transform Motion Estimation with Overlapped Compensation and Fractional-pixel Accuracy,&#x201d; <i>IEEE Proc. Visual Image Signal Processing</i>, vol. 146, No. 6, pp. 339-344 (Dec. 1999).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00364">
<othercit>Microsoft Corp., &#x201c;Microsoft Debuts New Windows Media Player 9 Series, Redefining Digital Media on the PC,&#x201d; 4 pp. (document marked Sep. 4, 2002) [Downloaded from the World Wide Web on May 14, 2004].</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00365">
<othercit>Mook, &#x201c;Next-Gen Windows Media Player Leaks to the Web,&#x201d; <i>BetaNews</i>, 17 pp. (Jul. 19, 2002) [Downloaded from the World Wide Web on Aug. 8, 2003].</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00366">
<othercit>Morimoto et al., &#x201c;Fast Electronic Digital Image Stabilization,&#x201d; <i>Proc. ICPR</i>, Vienna, Austria, 5 pp. (Aug. 1996).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00367">
<othercit>&#x201c;Overview of MPEG-2 Test Model 5,&#x201d; 5 pp. [Downloaded from the World Wide Web on Mar. 1, 2006].</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00368">
<othercit>Printouts of FTP directories from http://ftp3.itu.ch, 8 pp. (downloaded from the World Wide Web on Sep. 20, 2005).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00369">
<othercit>Reader, &#x201c;History of MPEG Video Compression&#x2014;Ver. 4.0,&#x201d; 99 pp. (document marked Dec. 16, 2003).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00370">
<othercit>Ribas-Corbera et al., &#x201c;On the Optimal Block Size for Block-based Motion-Compensated Video Coders,&#x201d; <i>SPIE Proc. of Visual Comm. </i>&#x26; <i>Image Processing</i>, vol. 3024, 12 pp. (Jan. 1997).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00371">
<othercit>Ribas-Corbera et al., &#x201c;On the Optimal Motion Vector Accuracy for Block-based Motion-Compensated Video Coders,&#x201d; <i>Proc. SPIE Digital Video Compression</i>, San Jose, CA, 13 pp. (Mar. 1996).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00372">
<othercit>Schultz et al., &#x201c;Subpixel Motion Estimation for Super-Resolution Image Sequence Enhancement,&#x201d; <i>Journal of Visual Comm. </i>&#x26; <i>Image Representation</i>, vol. 9, No. 1, pp. 38-50 (Mar. 1998).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00373">
<othercit>Sullivan et al., &#x201c;The H.264/AVC Advanced Video Coding Standard: Overview and Introduction to the Fidelity Range Extensions,&#x201d; 21 pp. (Aug. 2004).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00374">
<othercit>&#x201c;The TML Project WEB-Page and Archive,&#x201d; (including pages of code marked &#x201c;image.cpp for H.26L decoder, Copyright 1999&#x201d; and &#x201c;image.c&#x201d;), 24 pp. [Downloaded from the World Wide Web on Jun. 1, 2005].</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00375">
<othercit>Tourapis et al., &#x201c;Predictive Motion Vector Field Adaptive Search Technique (PMVFAST)&#x2014;Enhancing Block Based Motion Estimation,&#x201d; <i>Proc. Visual Communications and Image Processing</i>, 10 pp. (Jan. 2001).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00376">
<othercit>Triggs, &#x201c;Empirical Filter Estimation for Subpixel Interpolation and Matching,&#x201d; <i>Int'l Conf. Computer Vision '01</i>, Vancouver, Canada, 8 pp. (Jul. 2001).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00377">
<othercit>Triggs, &#x201c;Optimal Filters for Subpixel Interpolation and Matching,&#x201d; <i>Int'l Conf. Computer Vision '01</i>, Vancouver, Canada, 10 pp. (Jul. 2001).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00378">
<othercit>&#x201c;Video Coding Using Wavelet Decomposition for Very Low Bit-Rate Networks,&#x201d; 16 pp. (month unknown, 1997).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00379">
<othercit>Wang et al., &#x201c;Adaptive frame/field coding for JVT Video Coding,&#x201d; ITU-T SG16 Q.6 JVT-B071, 24 pp. (Jan. 2002).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00380">
<othercit>Wang et al., &#x201c;Interlace Coding Tools for H.26L Video Coding,&#x201d; ITU-T SG16/Q.6 VCEG-O37, pp. 1-20 (Dec. 2001).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00381">
<othercit>Wedi, &#x201c;Complexity Reduced Motion Compensated Prediction with 1/8-pel Displacement Vector Resolution,&#x201d; ITU Study Group 16, Video Coding Experts Group (Question 6), Document VCEG-L20, 8 pp. (Document dated Dec. 2000).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00382">
<othercit>Weiss et al., &#x201c;Real Time Implementation of Subpixel Motion Estimation for Broadcast Applications,&#x201d; pp. 7/1-7/3 (Oct. 1990).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00383">
<othercit>Wiegand et al., &#x201c;Long-term Memory Motion Compensated Prediction,&#x201d; <i>IEEE Transactions on Circuits </i>&#x26; <i>Systems for Video Technology</i>, vol. 9, No. 1, pp. 70-84 (Feb. 1999).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00384">
<othercit>Wien, &#x201c;Variable Block-Size Transforms for Hybrid Video Coding,&#x201d; Dissertation, 182 pp. (Feb. 2004).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00385">
<othercit>Wu et al., &#x201c;Joint estimation of forward and backward motion vectors for interpolative prediction of video,&#x201d; <i>IEEE Transactions on Image Processing</i>, vol. 3, No. 5, pp. 684-687 (Sep. 1994).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00386">
<othercit>Yang et al., &#x201c;Very High Efficiency VLSI Chip-pair for Full Search Block Matching with Fractional Precision,&#x201d; <i>Proc. ICASSP/IEEE Int'l Conf. on Acoustics, Speech </i>&#x26; <i>Signal Processing</i>, Glasgow, pp. 2437-2440 (May 1989).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00387">
<othercit>Yu et al., &#x201c;Two-Dimensional Motion Vector Coding for Low Bitrate Videophone Applications,&#x201d; <i>Proc. Int'l Conf. on Image Processing</i>, Los Alamitos, US, pp. 414-417, <i>IEEE Comp. Soc. Press </i>(Oct. 1995).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00388">
<othercit>Zhang et al., &#x201c;Adaptive Field/Frame Selection for High Compression Coding,&#x201d; MERL TR-2003-29, 13 pp. (Jan. 2003).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00389">
<othercit>Chan et al., &#x201c;A Novel Predictive Global Motion Estimation for Video Coding,&#x201d; Proceedings of ISCAS 2002, vol. 3, pp. 5-8 (2002).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00390">
<othercit>Communication dated Mar. 25, 2011, from European Patent Application No. 10009814.4, 6 pp.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00391">
<othercit>Exam Report dated Dec. 22, 2011, from European Patent Application No. 04019094.4, 5 pp.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00392">
<othercit>Final Rejection dated Jul. 2, 2012, from Chinese Patent Application No. 200810144049.9, 4 pp.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00393">
<othercit>Hatori, <i>Digital Video Network</i>, pp. 25-29 (Dec. 2002).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00394">
<othercit>Huang et al., &#x201c;Hardware architecture design for variable block size motion estimation in MPEG-4 AVC/JVT/ITU-T H.264,&#x201d; Proc. of the 2003 Int'l Symposium on Circuits &#x26; Sys. (ISCAS '03), vol. 2, pp. 796-799 (May 2003).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00395">
<othercit>ISO/IEC, &#x201c;Draft Text of Final Draft International Standard for Advanced Video Coding (ITU-T Rec. H.264 | ISO/IEC 14496-10 AVC),&#x201d; ISO/IEC JTC 1/SC 29/WG 11 N5555, 242 pp. (Mar. 2003).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00396">
<othercit>ISO/IEC, &#x201c;ISO/IEC CD 13818-2: Information technology&#x2014;Generic coding of moving pictures and associated audio information&#x2014;Part 2: video,&#x201d; ISO/IEC JTC1/SC29 N659, 189 pp. (Dec. 1993).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00397">
<othercit>ISO/IEC, &#x201c;Text of Committee Draft of Joint Video Specification (ITU-T Rec. H.264 | ISO/IEC 14496-10 AVC,&#x201d; ISO/IEC JTC1/SC29/WG11 MPEG02/N4810, 143 pp. (May 2002).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00398">
<othercit>ISO/IEC, &#x201c;MPEG-4 Video Verification Model Version 7.1,&#x201d; ISO/IEC JTC1/SC29/WG11 MPEG97/M2249, 204 pp. (Ed. Ebrahimi) (Stockholm, Jul. 13, 1997).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00399">
<othercit>ISO/IEC, &#x201c;Draft of 14496-2 Third Edition, Information Technology&#x2014;Coding of Audio-Visual Objects&#x2014;Part 2: Visual,&#x201d; ISO/IEC JTC 1/SC 29/WG 11 M9477, 590 pp. (Pattaya, Mar. 3, 2003).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00400">
<othercit>ITU-T, &#x201c;Adaptive Field/Frame Block Coding Experiment Proposal,&#x201d; Study Group 16, Video Coding Experts Group (VCEG), VCEG-N76, Santa Barbara Meeting, Sep. 24-27, 2001, 8 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00401">
<othercit>ITU (Borgwardt), &#x201c;Core Experiment on Interlaced Video Coding,&#x201d; Study Group 16, Question 6, Video Coding Experts Group (VCEG), VCEG-N85r1, 14th Meeting: Santa Barbara, California, Sep. 24-27, 2001, 10 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00402">
<othercit>Jang et al., &#x201c;MPEG-4 Version 2 Visual Working Draft Rev 5.1,&#x201d; ISO/IEC JTC1/SC29 WG11 M4257, 462 pp. (Dec. 1998).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00403">
<othercit>Joint Video Team (JVT) of ISO/IEC MPEG &#x26; ITU-T VCEG (Tourapis), Direct Prediction for Predictive (P) and Bidirectionally Predictive (B) frames in Video Coding, JVT-C128 (ISO/IEC JTC1/SC29/WG11 and ITU-T SG16 Q.6), 3rd Meeting: Fairfax, Virginia, USA, 12 pages, May 6-10, 2002.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00404">
<othercit>Joint Video Team (JVT) of ISO/IEC MPEG &#x26; ITU-T VCEG (Borgwardt), &#x201c;Core Experiment on Macroblock Adaptive Frame/Field Encoding,&#x201d; JVT-B117 (ISO/IEC JTC1/SC29/WG11 and ITU-T SG16 Q.6), 2nd Meeting: Geneva, Switzerland, Jan. 29-Feb. 1, 2002, 8 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00405">
<othercit>Joint Video Team (JVT) of ISO/IEC MPEG &#x26; ITU-T VCEG (Tourapis), &#x201c;Timestamp Independent Motion Vector Prediction for P and B frames with Division Elimination,&#x201d; JVT-D040 (ISO/IEC JTC1/SC29/WG11 and ITU-T SG16 Q.6), 4th Meeting: Klagenfurt, Austria, Jul. 22-26, 2002, 18 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00406">
<othercit>Joint Video Team (JVT) of ISO/IEC MPEG &#x26; ITU-T VCEG (Kondo), &#x201c;Memory Reduction for Temporal Technique of Direct Mode,&#x201d; JVT-E076 (ISO/IEC JTC1/SC29/WG11 and ITU-T SG16 Q.6), 5th Meeting: Geneva, Switzerland, Oct. 9-17, 2002, 12 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00407">
<othercit>Kuroda et al., A study of motion vectors on chrominance, Proc. of the 2002 Engineering Sciences Society Conference on IEICE, p. 119 (Aug. 2002).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00408">
<othercit>Miki, &#x201c;All of MPEG-4,&#x201d; pp. 44-46, Kogyo Chosakai Publishing, Inc. (Sep. 1998).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00409">
<othercit>Miki ed., &#x201c;All of MPEG-4,&#x201d; 1st Ed., pp. 61-76, Kogyo Chosakai Publishing, Inc. (Sep. 30, 1998).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00410">
<othercit>Nakagawa et al., &#x201c;Chroma Vector Adjustment for H.264/MPEG-4 AVC in Field Coding Mode,&#x201d; 2003 Proceedings of the IEICE general conference, separate volume: information/system 2, p. 10, the Institute of Electronics, Information and Communication Engineers (IEICE), issued on Mar. 3, 2003.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00411">
<othercit>Notice of Allowance dated Feb. 25, 2011, from Japanese Patent Application No. 2004-259053, 6 pp.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00412">
<othercit>Notice on Grant of Patent Right for Invention dated Sep. 4, 2009, from Chinese Patent Application No. 200410077192.2, 4 pp.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00413">
<othercit>Notice of Preliminary Rejection from Korean Patent Application No. 10-2004-70790, 1p.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00414">
<othercit>Notice of Rejection dated Feb. 22, 2008, from Japanese Patent Application No. 2004-259053 (including translation), 6 pp.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00415">
<othercit>Notice of Rejection dated Jun. 2, 2009, from Japanese Patent Application No. 2004-259053 (including translation), 9 pp.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00416">
<othercit>Notice of Rejection dated Sep. 10, 2010, from Japanese Patent Application No. 2004-259053 (including translation), 4 pp.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00417">
<othercit>Notice of Rejection dated Nov. 8, 2011, from Japanese Patent Application No. 2008-212238, 4 pp.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00418">
<othercit>Notice of Rejection dated May 25, 2012, from Japanese Patent Application No. 2008-212238, 5 pp.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00419">
<othercit>Notice on Office Action dated Jun. 30, 2006, from Chinese Patent Application No. 200410077192.2, 7 pp.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00420">
<othercit>Notice on the Second Office Action dated Dec. 31, 2011, from Chinese Patent Application No. 200810144049.9, 8 pp.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00421">
<othercit>Notice on the First Office Action dated Jun. 23, 2010, from Chinese Patent Application No. 200810144049.9, 9 pp.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00422">
<othercit>Odaka et al., &#x201c;A motion compensated prediction method for interlaced image &#x2018;Dual-prime&#x2019;,&#x201d; Information Processing Society of Japan (IPSJ) Technical Report, vol. 94, No. 53 (94-AVM-5), pp. 17-24, Jun. 24, 1994.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00423">
<othercit>Official Communication dated Nov. 24, 2005, from European Patent Application No. EP 04019094, 7 pp.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00424">
<othercit>Official Notice of Final Rejection dated Mar. 30, 2010, from Japanese Patent Application No. 2004-259053 (including translation), 5 pp.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00425">
<othercit>Ono et al., Basic Technologies on International Image Coding Standards, pp. 268-275 (Mar. 1998).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00426">
<othercit>Ono et al., &#x201c;Easy-to-understand method for achieving JPEG/MPEG2,&#x201d; pp. 122, 123, and 134-143 (Jul. 15, 1995).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00427">
<othercit>Ono et al., &#x201c;A Basic Technology of International Standard Image Encoding,&#x201d; First Edition, Corona Publishing Col, Ltd., pp. 227-230, Mar. 20, 1998.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00428">
<othercit>Ono et al., &#x201c;A Basic Technology of International Standard Image Encoding,&#x201d; First Edition, Corona Publishing Col, Ltd., pp. 293-300, Mar. 20, 1998.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00429">
<othercit>Ono et al., &#x201c;A Basic Technology of International Standard Image Encoding,&#x201d; First Edition, Corona Publishing Col, Ltd., pp. 253-254, Mar. 20, 1998.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00430">
<othercit>Search Report dated Feb. 17, 2005, from European Patent Application No. EP 04019094, 4 pp.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00431">
<othercit>Search Report dated Dec. 23, 2010, from European Patent Application No. EP 10009814.4, 4 pp.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00432">
<othercit>Second Office Action dated Mar. 21, 2008, from Chinese Patent Application No. 200410077192.2, 9 pp.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00433">
<othercit>Srinivasan et al., &#x201c;Windows Media Video 9: overview and applications,&#x201d; <i>Signal Processing: Image Communication</i>, vol. 19, No. 9, pp. 851-875 (Oct. 2004).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00434">
<othercit>Suhring et al., &#x201c;JM2.1 video coding reference software, extract from source code module lencod.c,&#x201d; &#x3c;http://iphome.hhi.de/suehring/tml/download/old<sub>&#x2014;</sub>jm/jm21.zip&#x3e;, May 27, 2002, 8 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00435">
<othercit>Sun et al., &#x201c;Improved TML Loop Filter with Lower Complexity,&#x201d; ITU-T VCEG-N17, 8 pp. (Aug. 2001).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00436">
<othercit>Third Office Action dated Mar. 20, 2009, from Chinese Patent Application No. 200410077192.2, 14 pp.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00437">
<othercit>Yang et al., &#x201c;A Controllable Predictive Cross-Diamond Fast Search Algorithm for Block Matching Motion Estimation,&#x201d; Proceedings of PDCAT' 2003, pp. 821-824, Aug. 2003.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00438">
<othercit>Yasuda et al., &#x201c;Information Compression Technique for Digital Broadcasting and the Internet,&#x201d; 1st ed., pp. 210-219, Kyoritsu Shuppan Co., Ltd. (Jun. 10, 1999).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00439">
<othercit>Yasuda et al., &#x201c;Information Compression Technique for Digital Broadcasting and the Internet,&#x201d; 1st ed., pp. 298-308, Kyoritsu Shuppan Co., Ltd. (Jun. 10, 1999).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00440">
<othercit>Yasuda ed., &#x201c;MPEC/International Standard of Multi-Media Coding,&#x201d; 1st ed., pp. 40-48, Maruzen Co., Ltd. (Sep. 30, 1994).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>35</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>37524001-24029</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382240</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>36</number-of-drawing-sheets>
<number-of-figures>55</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>10857473</doc-number>
<date>20040527</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>7567617</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>12401831</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60501081</doc-number>
<date>20030907</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20090168890</doc-number>
<kind>A1</kind>
<date>20090702</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Holcomb</last-name>
<first-name>Thomas W.</first-name>
<address>
<city>Bothell</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Holcomb</last-name>
<first-name>Thomas W.</first-name>
<address>
<city>Bothell</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Klarquist Sparkman, LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Microsoft Corporation</orgname>
<role>02</role>
<address>
<city>Redmond</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Philippe</last-name>
<first-name>Gims</first-name>
<department>2485</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Techniques and tools for encoding and decoding predicted images in interlaced video are described. For example, a video encoder or decoder computes a motion vector predictor for a motion vector for a portion (e.g., a block or macroblock) of an interlaced P-field, including selecting between using a same polarity or opposite polarity motion vector predictor for the portion. The encoder/decoder processes the motion vector based at least in part on the motion vector predictor computed for the motion vector. The processing can comprise computing a motion vector differential between the motion vector and the motion vector predictor during encoding and reconstructing the motion vector from a motion vector differential and the motion vector predictor during decoding. The selecting can be based at least in part on a count of opposite polarity motion vectors for a neighborhood around the portion and/or a count of same polarity motion vectors.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="174.92mm" wi="242.40mm" file="US08625669-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="237.57mm" wi="181.02mm" file="US08625669-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="230.12mm" wi="173.23mm" orientation="landscape" file="US08625669-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="207.52mm" wi="176.28mm" orientation="landscape" file="US08625669-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="233.60mm" wi="167.64mm" file="US08625669-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="249.26mm" wi="192.36mm" orientation="landscape" file="US08625669-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="241.38mm" wi="174.07mm" orientation="landscape" file="US08625669-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="236.22mm" wi="161.54mm" file="US08625669-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="232.75mm" wi="179.75mm" file="US08625669-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="204.47mm" wi="179.24mm" file="US08625669-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="182.80mm" wi="183.22mm" file="US08625669-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="228.77mm" wi="173.65mm" file="US08625669-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="240.96mm" wi="173.31mm" file="US08625669-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="173.65mm" wi="175.01mm" file="US08625669-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="163.66mm" wi="167.22mm" file="US08625669-20140107-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="184.91mm" wi="167.64mm" file="US08625669-20140107-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00016" num="00016">
<img id="EMI-D00016" he="174.07mm" wi="162.81mm" file="US08625669-20140107-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00017" num="00017">
<img id="EMI-D00017" he="157.65mm" wi="169.76mm" file="US08625669-20140107-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00018" num="00018">
<img id="EMI-D00018" he="97.71mm" wi="172.80mm" file="US08625669-20140107-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00019" num="00019">
<img id="EMI-D00019" he="202.35mm" wi="179.75mm" file="US08625669-20140107-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00020" num="00020">
<img id="EMI-D00020" he="240.54mm" wi="173.65mm" file="US08625669-20140107-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00021" num="00021">
<img id="EMI-D00021" he="233.60mm" wi="180.17mm" file="US08625669-20140107-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00022" num="00022">
<img id="EMI-D00022" he="224.03mm" wi="175.01mm" file="US08625669-20140107-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00023" num="00023">
<img id="EMI-D00023" he="241.05mm" wi="177.97mm" file="US08625669-20140107-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00024" num="00024">
<img id="EMI-D00024" he="233.60mm" wi="180.17mm" file="US08625669-20140107-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00025" num="00025">
<img id="EMI-D00025" he="176.70mm" wi="187.20mm" file="US08625669-20140107-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00026" num="00026">
<img id="EMI-D00026" he="240.11mm" wi="178.05mm" file="US08625669-20140107-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00027" num="00027">
<img id="EMI-D00027" he="192.79mm" wi="188.04mm" file="US08625669-20140107-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00028" num="00028">
<img id="EMI-D00028" he="156.72mm" wi="156.72mm" file="US08625669-20140107-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00029" num="00029">
<img id="EMI-D00029" he="220.56mm" wi="155.45mm" file="US08625669-20140107-D00029.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00030" num="00030">
<img id="EMI-D00030" he="246.55mm" wi="186.27mm" file="US08625669-20140107-D00030.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00031" num="00031">
<img id="EMI-D00031" he="189.74mm" wi="180.59mm" file="US08625669-20140107-D00031.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00032" num="00032">
<img id="EMI-D00032" he="161.54mm" wi="171.03mm" file="US08625669-20140107-D00032.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00033" num="00033">
<img id="EMI-D00033" he="183.22mm" wi="188.81mm" file="US08625669-20140107-D00033.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00034" num="00034">
<img id="EMI-D00034" he="204.89mm" wi="191.09mm" file="US08625669-20140107-D00034.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00035" num="00035">
<img id="EMI-D00035" he="153.75mm" wi="170.26mm" file="US08625669-20140107-D00035.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00036" num="00036">
<img id="EMI-D00036" he="97.71mm" wi="173.65mm" file="US08625669-20140107-D00036.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">RELATED APPLICATION INFORMATION</heading>
<p id="p-0002" num="0001">This application is a continuation of U.S. patent application Ser. No. 10/857,473, entitled &#x201c;Predicting Motion Vectors for Fields of Forward-Predicted Interlaced Video Frames,&#x201d; filed May 27, 2004, which claims the benefit of U.S. Provisional Patent Application No. 60/501,081, entitled &#x201c;Video Encoding and Decoding Tools and Techniques,&#x201d; filed Sept. 7, 2003, both of which are hereby incorporated by reference.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">TECHNICAL FIELD</heading>
<p id="p-0003" num="0002">Techniques and tools for interlaced video coding and decoding are described. For example, a video encoder/decoder calculates motion vector predictors for fields in interlaced video frames.</p>
<heading id="h-0003" level="1">BACKGROUND</heading>
<p id="p-0004" num="0003">Digital video consumes large amounts of storage and transmission capacity. A typical raw digital video sequence includes 15 or 30 pictures per second. Each picture can include tens or hundreds of thousands of pixels (also called pels). Each pixel represents a tiny element of the picture. In raw form, a computer commonly represents a pixel with 24 bits or more. Thus, the number of bits per second, or bit rate, of a typical raw digital video sequence can be 5 million bits/second or more.</p>
<p id="p-0005" num="0004">Many computers and computer networks lack the resources to process raw digital video. For this reason, engineers use compression (also called coding or encoding) to reduce the bit rate of digital video. Compression can be lossless, in which quality of the video does not suffer but decreases in bit rate are limited by the complexity of the video. Or, compression can be lossy, in which quality of the video suffers but decreases in bit rate are more dramatic. Decompression reverses compression.</p>
<p id="p-0006" num="0005">In general, video compression techniques include &#x201c;intra&#x201d; compression and &#x201c;inter&#x201d; or predictive compression. Intra compression techniques compress individual pictures, typically called I-pictures or key pictures. Inter compression techniques compress pictures with reference to preceding and/or following pictures, and are typically called predicted pictures, P-pictures, or B-pictures.</p>
<p id="h-0004" num="0000">I. Inter Compression in Windows Media Video, Versions 8 and 9</p>
<p id="p-0007" num="0006">Microsoft Corporation's Windows Media Video, Version 8 [&#x201c;WMV8&#x201d;] includes a video encoder and a video decoder. The WMV8 encoder uses intra and inter compression, and the WMV8 decoder uses intra and inter decompression. Early versions of Windows Media Video, Version 9 [&#x201c;WMV9&#x201d;] use a similar architecture for many operations.</p>
<p id="p-0008" num="0007">Inter compression in the WMV8 encoder uses block-based motion compensated prediction coding followed by transform coding of the residual error. <figref idref="DRAWINGS">FIGS. 1 and 2</figref> illustrate the block-based inter compression for a predicted frame in the WMV8 encoder. In particular, <figref idref="DRAWINGS">FIG. 1</figref> illustrates motion estimation for a predicted frame <b>110</b> and <figref idref="DRAWINGS">FIG. 2</figref> illustrates compression of a prediction residual for a motion-compensated block of a predicted frame.</p>
<p id="p-0009" num="0008">For example, in <figref idref="DRAWINGS">FIG. 1</figref>, the WMV8 encoder computes a motion vector for a macroblock <b>115</b> in the predicted frame <b>110</b>. To compute the motion vector, the encoder searches in a search area <b>135</b> of a reference frame <b>130</b>. Within the search area <b>135</b>, the encoder compares the macroblock <b>115</b> from the predicted frame <b>110</b> to various candidate macroblocks in order to find a candidate macroblock that is a good match. The encoder outputs information specifying the motion vector (entropy coded) for the matching macroblock.</p>
<p id="p-0010" num="0009">Since a motion vector value is often correlated with the values of spatially surrounding motion vectors, compression of the data used to transmit the motion vector information can be achieved by selecting a motion vector predictor from neighboring macroblocks and predicting the motion vector for the current macroblock using the predictor. The encoder can encode the differential between the motion vector and the predictor. After reconstructing the motion vector by adding the differential to the predictor, a decoder uses the motion vector to compute a prediction macroblock for the macroblock <b>115</b> using information from the reference frame <b>130</b>, which is a previously reconstructed frame available at the encoder and the decoder. The prediction is rarely perfect, so the encoder usually encodes blocks of pixel differences (also called the error or residual blocks) between the prediction macroblock and the macroblock <b>115</b> itself.</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 2</figref> illustrates an example of computation and encoding of an error block <b>235</b> in the WMV8 encoder. The error block <b>235</b> is the difference between the predicted block <b>215</b> and the original current block <b>225</b>. The encoder applies a DCT <b>240</b> to the error block <b>235</b>, resulting in an 8&#xd7;8 block <b>245</b> of coefficients. The encoder then quantizes <b>250</b> the DCT coefficients, resulting in an 8&#xd7;8 block of quantized DCT coefficients <b>255</b>. The encoder scans <b>260</b> the 8&#xd7;8 block <b>255</b> into a one-dimensional array <b>265</b> such that coefficients are generally ordered from lowest frequency to highest frequency. The encoder entropy encodes the scanned coefficients using a variation of run length coding <b>270</b>. The encoder selects an entropy code from one or more run/level/last tables <b>275</b> and outputs the entropy code.</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 3</figref> shows an example of a corresponding decoding process <b>300</b> for an inter-coded block. In summary of <figref idref="DRAWINGS">FIG. 3</figref>, a decoder decodes (<b>310</b>, <b>320</b>) entropy-coded information representing a prediction residual using variable length decoding <b>310</b> with one or more run/level/last tables <b>315</b> and run length decoding <b>320</b>. The decoder inverse scans <b>330</b> a one-dimensional array <b>325</b> storing the entropy-decoded information into a two-dimensional block <b>335</b>. The decoder inverse quantizes and inverse discrete cosine transforms (together, <b>340</b>) the data, resulting in a reconstructed error block <b>345</b>. In a separate motion compensation path, the decoder computes a predicted block <b>365</b> using motion vector information <b>355</b> for displacement from a reference frame. The decoder combines <b>370</b> the predicted block <b>365</b> with the reconstructed error block <b>345</b> to form the reconstructed block <b>375</b>.</p>
<p id="p-0013" num="0012">The amount of change between the original and reconstructed frames is the distortion and the number of bits required to code the frame indicates the rate for the frame. The amount of distortion is roughly inversely proportional to the rate.</p>
<p id="h-0005" num="0000">II. Interlaced Video and Progressive Video</p>
<p id="p-0014" num="0013">A typical interlaced video frame consists of two fields scanned starting at different times. For example, referring to <figref idref="DRAWINGS">FIG. 4</figref>, an interlaced video frame <b>400</b> includes top field <b>410</b> and bottom field <b>420</b>. Typically, the even-numbered lines (top field) are scanned starting at one time (e.g., time t) and the odd-numbered lines (bottom field) are scanned starting at a different (typically later) time (e.g., time t+1). This timing can create jagged tooth-like features in regions of an interlaced video frame where motion is present because the two fields are scanned starting at different times. For this reason, interlaced video frames can be rearranged according to a field structure, with the odd lines grouped together in one field, and the even lines grouped together in another field. This arrangement, known as field coding, is useful in high-motion pictures for reduction of such jagged edge artifacts. On the other hand, in stationary regions, image detail in the interlaced video frame may be more efficiently preserved without such a rearrangement. Accordingly, frame coding is often used in stationary or low-motion interlaced video frame, in which the original alternating field line arrangement is preserved.</p>
<p id="p-0015" num="0014">A typical progressive video frame consists of one frame of content with non-alternating lines. In contrast to interlaced video, progressive video does not divide video frames into separate fields, and an entire frame is scanned left to right, top to bottom starting at a single time.</p>
<p id="h-0006" num="0000">III. Interlace P-frame Coding and Decoding in Early Versions of WMV9</p>
<p id="p-0016" num="0015">Early versions of Windows Media Video, Version 9 [&#x201c;WMV9&#x201d;] use interlace P-frame coding and decoding. In these early versions of WMV9, interlaced P-frames can contain macroblocks encoded in field mode or in frame mode, or skipped macroblocks, with a decision generally made on a macroblock-by-macroblock basis. Two motion vectors are associated with each field-coded macroblock, and one motion vector is associated with each frame-coded macroblock. An encoder jointly encodes motion information for the blocks in the macroblock, including horizontal and vertical motion vector differential components, potentially along with other signaling information.</p>
<p id="p-0017" num="0016">In the encoder, a motion vector is encoded by computing a differential between the motion vector and a motion vector predictor, which is computed based on neighboring motion vectors. And, in the decoder, the motion vector is reconstructed by adding the motion vector differential to the motion vector predictor, which is again computed (this time in the decoder) based on neighboring motion vectors.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIGS. 5</figref>, <b>6</b>, and <b>7</b> show examples of candidate predictors for motion vector prediction for frame-coded macroblocks and field-coded macroblocks, respectively, in interlaced P-frames in early versions of WMV9. <figref idref="DRAWINGS">FIG. 5</figref> shows candidate predictors A, B and C for a current frame-coded macroblock in an interior position in an interlaced P-frame (not the first or last macroblock in a macroblock row, not in the top row). Predictors can be obtained from different candidate directions other than those labeled A, B, and C (e.g., in special cases such as when the current macroblock is the first macroblock or last macroblock in a row, or in the top row, since certain predictors are unavailable for such cases). For a current frame-coded macroblock, predictor candidates are calculated differently depending on whether the neighboring macroblocks are field-coded or frame-coded. For a neighboring frame-coded macroblock, the motion vector is simply taken as the predictor candidate. For a neighboring field-coded macroblock, the candidate motion vector is determined by averaging the top and bottom field motion vectors.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIGS. 6 and 7</figref> show candidate predictors A, B and C for a current field in a field-coded macroblock that is not the first or last macroblock in a macroblock row, and not in the top row. In <figref idref="DRAWINGS">FIG. 6</figref>, the current field is a bottom field, and the bottom field motion vectors in the neighboring macroblocks are used as candidate predictors. In <figref idref="DRAWINGS">FIG. 7</figref>, the current field is a top field, and the top field motion vectors are used as candidate predictors. Thus, for each field in a current field-coded macroblock, the number of motion vector predictor candidates for each field is at most three, with each candidate coming from the same field type (e.g., top or bottom) as the current field.</p>
<p id="p-0020" num="0019">A predictor for the current macroblock or field of the current macroblock is selected based on the candidate predictors, and a motion vector differential is calculated based on the predictor. The motion vector can be reconstructed by adding the motion vector differential to the selected motion vector predictor at either the encoder or the decoder side. Typically, luminance motion vectors are reconstructed from the encoded motion information, and chrominance motion vectors are derived from the reconstructed luminance motion vectors.</p>
<p id="h-0007" num="0000">IV. Standards for Video Compression and Decompression</p>
<p id="p-0021" num="0020">Aside from WMV8 and early versions of WMV9, several international standards relate to video compression and decompression. These standards include the Motion Picture Experts Group [&#x201c;MPEG&#x201d;] 1, 2, and 4 standards and the H.261, H.262, H.263, and H.264 standards from the International Telecommunication Union [&#x201c;ITU&#x201d;]. One of the primary methods used to achieve data compression of digital video sequences in the international standards is to reduce the temporal redundancy between pictures. These popular compression schemes (MPEG-1, MPEG-2, MPEG-4, H.261, H.263, etc) use motion estimation and compensation. For example, a current frame is divided into uniform square regions (e.g., blocks and/or macroblocks). A matching region for each current region is specified by sending motion vector information for the region. The motion vector indicates the location of the region in a previously coded (and reconstructed) frame that is to be used as a predictor for the current region. A pixel-by-pixel difference, called the error signal, between the current region and the region in the reference frame is derived. This error signal usually has lower entropy than the original signal. Therefore, the information can be encoded at a lower rate. As in WMV8 and early versions of WMV9, since a motion vector value is often correlated with spatially surrounding motion vectors, compression of the data used to represent the motion vector information can be achieved by coding the differential between the current motion vector and a predictor based upon previously coded, neighboring motion vectors.</p>
<p id="p-0022" num="0021">In addition, some international standards describe motion estimation and compensation in interlaced video frames. The H.262 standard allows an interlaced video frame to be encoded as a single frame or as two fields, where the frame encoding or field encoding can be adaptively selected on a frame-by-frame basis. The H.262 standard describes field-based prediction, which is a prediction mode using only one field of a reference frame. The H.262 standard also, describes dual-prime prediction, which is a prediction mode in which two forward field-based predictions are averaged for a 16&#xd7;16 block in an interlaced P-picture. Section 7.6 of the H.262 standard describes &#x201c;field prediction,&#x201d; including selecting between two reference fields to use for motion compensation for a macroblock of a current field of an interlaced video frame. Section 7.6.3 describes motion vector prediction and reconstruction, in which a reconstructed motion vector for a given macroblock becomes the motion vector predictor for a subsequently encoded/decoded macroblock. Such motion vector prediction fails to adequately predict motion vectors for macroblocks of fields of interlaced video frames in many cases.</p>
<p id="p-0023" num="0022">Given the critical importance of video compression and decompression to digital video, it is not surprising that video compression and decompression are richly developed fields. Whatever the benefits of previous video compression and decompression techniques, however, they do not have the advantages of the following techniques and tools.</p>
<heading id="h-0008" level="1">SUMMARY</heading>
<p id="p-0024" num="0023">In summary, the detailed description is directed to various techniques and tools for encoding and decoding predicted video images in interlaced video. Described techniques include those for computing motion vector predictors for block or macroblocks of fields of interlaced video frames. These techniques improve the accuracy of motion vector prediction in many cases, thereby reducing the bitrate associated with encoding motion vector information for fields of interlaced video frames. The various techniques and tools can be used in combination or independently.</p>
<p id="p-0025" num="0024">In a first aspect, an encoder/decoder computes a motion vector predictor for a motion vector for a portion (e.g., a block or macroblock) of an interlaced P-field, including selecting between using a same polarity motion vector predictor or using an opposite polarity motion vector predictor for the portion. The encoder/decoder processes the motion vector based at least in part on the motion vector predictor computed for the motion vector. The processing can comprise computing a motion vector differential between the motion vector and the motion vector predictor during encoding and reconstructing the motion vector from a motion vector differential and the motion vector predictor during decoding. The selecting can be based at least in part on a count of opposite polarity motion vectors for a neighborhood around the portion and/or a count of same polarity motion vectors for the neighborhood. The selecting also can be based at least in part on a single bit signal in a bit stream. The selecting can comprise determining a dominant predictor based at least in part on a count of opposite polarity motion vectors for a neighborhood around the portion and/or a count of same polarity motion vectors for the neighborhood, and selecting the same polarity motion vector predictor or the opposite polarity motion vector predictor based upon the dominant predictor and a single bit signal in a bit stream.</p>
<p id="p-0026" num="0025">The selecting also can comprise selecting the same polarity motion vector predictor if the motion vector for the portion refers to a same polarity field, or selecting the opposite polarity motion vector predictor if the motion vector for the portion refers to an opposite polarity field.</p>
<p id="p-0027" num="0026">The same polarity motion vector predictor can be computed as a median of plural same polarity motion vector predictor candidates for a neighborhood around the portion, and the opposite polarity motion vector predictor can be computed as a median of plural opposite polarity motion vector predictor candidates for a neighborhood around the portion. Same polarity motion vector predictor candidates can be derived by scaling an opposite polarity motion vector predictor candidate, and opposite polarity motion vector predictor candidates can be derived by scaling a same polarity motion vector predictor candidate.</p>
<p id="p-0028" num="0027">In another aspect, an encoder/decoder computes a motion vector predictor for a motion vector for a portion of an interlaced P-field based at least in part on plural neighboring motion vectors, wherein the plural neighboring motion vectors include one or more opposite polarity motion vectors and one or more same polarity motion vectors. The encoder/decoder processes the motion vector based at least in part on the motion vector predictor computed for the motion vector. A same polarity motion vector predictor can be computed as a median of plural same polarity motion vector predictor candidates, wherein at least one of the plural candidates is derived based at least in part on scaling a value from one of the one or more opposite polarity motion vectors, and an opposite polarity motion vector predictor can be computed as a median of plural opposite polarity motion vector predictor candidates, wherein at least one of the plural candidates is derived based at least in part on scaling a value from one of the one or more same polarity motion vectors.</p>
<p id="p-0029" num="0028">In another aspect, an encoder/decoder processes an interlaced predicted video frame by determining a first set of motion vector predictor candidates for a macroblock in a current field in the video frame, the first set of motion vector predictor candidates referencing a first reference field having a same polarity relative to the current field. A first motion vector predictor is computed for the macroblock based at least in part on one or more of the first set of motion vector predictor candidates. A second set of motion vector predictor candidates is determined for the macroblock, the second set of motion vector predictor candidates referencing a second reference field having an opposite polarity relative to the current field. A second motion vector predictor for the macroblock is computed based at least in part on one or more of the second set of motion vector predictor candidates.</p>
<p id="p-0030" num="0029">The first and second motion vector predictors are used in the processing, such as by selecting one of the first and second motion vector predictors and calculating a motion vector differential for the macroblock based on the selected motion vector predictor, and/or by selecting one of the first and second motion vector predictors and combining the selected predictor with the motion vector differential to reconstruct a motion vector for the macroblock. The motion vector differential can be entropy coded.</p>
<p id="p-0031" num="0030">In another aspect, a first polarity motion vector predictor candidate is computed by scaling an actual motion vector value having a second polarity different than the first polarity. The first polarity motion vector predictor candidate is used when computing a motion vector predictor for a motion vector for a portion of an interlaced P-field. A motion vector differential can be computed between the motion vector and the motion vector predictor during encoding. The motion vector can be reconstructed from a motion vector differential and the motion vector predictor during decoding. The first polarity motion vector predictor candidate can have the opposite field polarity as the portion, wherein the actual motion vector value has the same field polarity as the portion, and wherein the scaling is adapted to scale from the same polarity to the opposite polarity. Or, the first polarity motion vector predictor candidate can have the same field polarity as the portion, wherein the actual motion vector value has the opposite field polarity as the portion, and wherein the scaling is adapted to scale from the opposite polarity to the same polarity. Scaling can be based at least in part on a reference frame distance.</p>
<p id="p-0032" num="0031">In another aspect, an encoder/decoder determines a first motion vector predictor candidate for a macroblock in a current field in the video frame, the first motion vector predictor candidate having a first polarity. The encoder/decoder determines a second motion vector predictor candidate for the macroblock, wherein the second motion vector predictor candidate is derived from the first motion vector predictor candidate using a scaling operation. The second motion vector predictor candidate has a polarity different than the first polarity.</p>
<p id="p-0033" num="0032">In another aspect, an encoder/decoder determines a first motion vector predictor candidate for a macroblock in a current field in the video frame, the first motion vector predictor candidate referencing a field having a first polarity and a reference distance. The encoder/decoder determines a second motion vector predictor candidate for the macroblock, wherein the second motion vector predictor candidate is derived from the first motion vector predictor candidate using a scaling operation that varies based at least in part on the reference distance. The second motion vector predictor candidate can have a polarity different than the first polarity.</p>
<p id="p-0034" num="0033">In another aspect, a decoder decodes a first element at a field level in a bitstream, and a second element and third element at a macroblock level in the bitstream. The first element comprises number-of-reference-field information for a current field in the video frame, which indicates two reference fields for the current field. The second element comprises macroblock mode information for a current macroblock in the current field. The third element comprises motion vector data for the current macroblock, wherein the motion vector data includes differential motion vector data. The decoder processes the motion vector data for the macroblock. The processing comprises determining first and second motion vector predictors for the current macroblock, and reconstructing a motion vector for the current macroblock based at least in part on the first and second motion vector predictors and based at least in part on the differential motion vector data. The first motion vector predictor is an odd field predictor and the second motion vector predictor is an even field predictor.</p>
<p id="p-0035" num="0034">In another aspect, a decoder decodes motion vector data that collectively signal a horizontal differential motion vector component, a vertical differential motion vector component, and flag indicating whether to use a dominant or non-dominant motion vector predictor. The decoder reconstructs a motion vector based at least in part on the decoded motion vector data. The reconstructing can comprise selecting between using the dominant motion vector predictor or the non-dominant motion vector predictor based on the flag, combining the horizontal differential motion vector component with a horizontal component of the selected predictor, and combining the vertical differential motion vector component with a horizontal component of the selected predictor.</p>
<p id="p-0036" num="0035">Additional features and advantages will be made apparent from the following detailed description of different embodiments that proceeds with reference to the accompanying drawings.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0009" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. 1</figref> is a diagram showing motion estimation in a video encoder according to the prior art.</p>
<p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. 2</figref> is a diagram showing block-based compression for an 8&#xd7;8 block of prediction residuals in a video encoder according to the prior art.</p>
<p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. 3</figref> is a diagram showing block-based decompression for an 8&#xd7;8 block of prediction residuals in a video encoder according to the prior art.</p>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. 4</figref> is a diagram showing an interlaced video frame according to the prior art.</p>
<p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. 5</figref> is a diagram showing candidate motion vector predictors for a current frame-coded macroblock in early versions of WMV9.</p>
<p id="p-0042" num="0041"><figref idref="DRAWINGS">FIGS. 6 and 7</figref> are diagrams showing candidate motion vector predictors for a current field-coded macroblock in early versions of WMV9.</p>
<p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. 8</figref> is a block diagram of a suitable computing environment in conjunction with which several described embodiments may be implemented.</p>
<p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. 9</figref> is a block diagram of a generalized video encoder system in conjunction with which several described embodiments may be implemented.</p>
<p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. 10</figref> is a block diagram of a generalized video decoder system in conjunction with which several described embodiments may be implemented.</p>
<p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. 11</figref> is a diagram of a macroblock format used in several described embodiments.</p>
<p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. 12A</figref> is a diagram of part of an interlaced video frame, showing alternating lines of a top field and a bottom field. <figref idref="DRAWINGS">FIG. 12B</figref> is a diagram of the interlaced video frame organized for encoding/decoding as a frame, and <figref idref="DRAWINGS">FIG. 12C</figref> is a diagram of the interlaced video frame organized for encoding/decoding as fields.</p>
<p id="p-0048" num="0047"><figref idref="DRAWINGS">FIGS. 13 and 14</figref> are code listings showing pseudo-code for performing median-of-3 and median-of-4 calculations, respectively.</p>
<p id="p-0049" num="0048"><figref idref="DRAWINGS">FIGS. 15 and 16</figref> are diagrams showing interlaced P-fields each having two reference fields.</p>
<p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. 17</figref> is a diagram showing relationships between vertical components of motion vectors and a corresponding spatial location for different combinations of current and reference field polarities.</p>
<p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. 18</figref> is a flow chart showing a technique for selecting a motion vector predictor for an interlaced P-field having two possible reference fields.</p>
<p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. 19</figref> is a diagram showing two sets of three candidate motion vector predictors for a current macroblock.</p>
<p id="p-0053" num="0052"><figref idref="DRAWINGS">FIGS. 20A-20F</figref> are code listings showing pseudo-code for calculating motion vector predictors in two-reference interlaced P-fields.</p>
<p id="p-0054" num="0053"><figref idref="DRAWINGS">FIGS. 21A-21B</figref> are code listings showing pseudo-code for scaling a predictor from one field to derive a predictor from another field.</p>
<p id="p-0055" num="0054"><figref idref="DRAWINGS">FIGS. 22 and 23</figref> are tables showing scaling operation values associated with different reference frame distances.</p>
<p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. 24</figref> is a diagram showing a frame-layer bitstream syntax for interlaced P-fields in a combined implementation.</p>
<p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. 25</figref> is a diagram showing a field-layer bitstream syntax for interlaced P-fields in a combined implementation.</p>
<p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. 26</figref> is a diagram showing a macroblock-layer bitstream syntax for interlaced P-fields in a combined implementation.</p>
<p id="p-0059" num="0058"><figref idref="DRAWINGS">FIGS. 27A and 27B</figref> are code listings showing pseudo-code illustrating decoding of a motion vector differential for a one-reference interlaced P-field.</p>
<p id="p-0060" num="0059"><figref idref="DRAWINGS">FIGS. 28A and 28B</figref> are code listings showing pseudo-code illustrating decoding of a motion vector differential and dominant/non-dominant predictor information for a two-reference interlaced P-field.</p>
<p id="p-0061" num="0060"><figref idref="DRAWINGS">FIGS. 29A and 29B</figref> are diagrams showing locations of macroblocks for candidate motion vector predictors for a 1 MV macroblock in an interlaced P-field.</p>
<p id="p-0062" num="0061"><figref idref="DRAWINGS">FIGS. 30A and 30B</figref> are diagrams showing locations of blocks for candidate motion vector predictors for a 1 MV macroblock in a mixed 1 MV/4 MV interlaced P-field.</p>
<p id="p-0063" num="0062"><figref idref="DRAWINGS">FIGS. 31A</figref>, <b>31</b>B, <b>32</b>A, <b>32</b>B, <b>33</b>, and <b>34</b> are diagrams showing the locations of blocks for candidate motion vector predictors for a block at various positions in a 4 MV macroblock in a mixed 1 MV/4 MV interlaced P-field.</p>
<p id="p-0064" num="0063"><figref idref="DRAWINGS">FIGS. 35A-35F</figref> are code listings showing pseudo-code for calculating motion vector predictors in two-reference interlaced P-fields in a combined implementation.</p>
<p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. 36</figref> is a code listing showing pseudo-code for determining a reference field in two-reference interlaced P-fields.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0010" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0066" num="0065">The present application relates to techniques and tools for efficient compression and decompression of interlaced video. In various described embodiments, for example, a video encoder and decoder incorporate techniques for predicting motion vectors in encoding and decoding predicted fields in interlaced video frames, as well as signaling techniques for use with a bit stream format or syntax comprising different layers or levels (e.g., sequence level, picture/image level, field level, macroblock level, and/or block level). The techniques and tools can be used, for example, in digital video broadcasting systems (e.g., cable, satellite, DSL, etc.).</p>
<p id="p-0067" num="0066">In particular, described techniques and tools improve the quality of motion vector predictors for blocks and/or macroblocks of forward-predicted fields of interlaced video frames, which in turn allows motion vectors to be more efficiently encoded. For example, techniques are described for generating predictor motion vectors for the blocks and/or macroblocks of a current field using neighborhood motion vectors for surrounding blocks and/or macroblocks, where the neighborhood motion vectors collectively refer to one or both of two fields as references. Innovations implemented in the described techniques and tools include, but are not limited to the following:</p>
<p id="p-0068" num="0067">1) Generating two motion vector predictors for a block or macroblock in an interlaced P-field: one motion vector predictor for an &#x201c;even&#x201d; reference field and one motion vector predictor for an &#x201c;odd&#x201d; reference field. The encoder/decoder considers up to six motion vector predictor candidates (three for the even motion vector predictor and three for the odd motion vector predictor) for a current block or macroblock. Use of up to six motion vector predictor candidates allows better motion vector prediction than prior methods.</p>
<p id="p-0069" num="0068">2) Using a motion vector predictor from the reference field that is referenced by the current motion vector: If the current motion vector references a region in the corresponding field of the reference frame (meaning that the current field and reference field are of the same polarity), the motion vector predictor from that reference field is used. Similarly, if the opposite polarity field is used as a reference, the motion vector predictor from that reference field is used. This allows better motion vector prediction than prior methods.</p>
<p id="p-0070" num="0069">3) Using a scaling operation to generate motion vector predictor candidates for the opposite polarity of an existing motion vector candidate. The encoder/decoder takes a motion vector from a candidate block/macroblock location, where the motion vector references either an odd field or an even field. The encoder/decoder then scales the motion vector to derive a motion vector predictor candidate of the other polarity. For example, if the actual motion vector for an adjacent macroblock references the odd field, that value is used as an odd motion vector predictor candidate from the adjacent macroblock, and a scaled motion vector value (derived from the actual value) is used as an even motion vector predictor candidate from the adjacent macroblock. Though motion vector predictor candidates are obtained from only three block/macroblock locations (with three motion vector values), the encoder/decoder's derivation of different polarity motion vector predictor candidates gives a population of six candidates to choose from.</p>
<p id="p-0071" num="0070">4) Using a scaling operation that is dependent on the reference frame distances when deriving motion vector predictor candidates. The encoder/decoder uses the relative temporal distances between the current field and the two reference fields to perform the scaling operation that derives the predictor candidate for the missing polarity field from the existing motion vector candidate from the other field. An encoder/decoder that takes into account the relative temporal distances to perform scaling can produce more accurate prediction than an encoder/decoder that assumes a constant reference distance.</p>
<p id="p-0072" num="0071">Various alternatives to the implementations described herein are possible. For example, techniques described with reference to flowchart diagrams can be altered by changing the ordering of stages shown in the flowcharts, by repeating or omitting certain stages, etc. As another example, although some implementations are described with reference to specific macroblock and/or block formats, other formats also can be used. Further, techniques and tools described with reference to interlaced P-field type prediction may also be applicable to other types of prediction.</p>
<p id="p-0073" num="0072">The various techniques and tools can be used in combination or independently. Different embodiments implement one or more of the described techniques and tools. The techniques and tools described herein can be used in a video encoder or decoder, or in some other system not specifically limited to video encoding or decoding.</p>
<p id="h-0011" num="0000">I. Computing Environment</p>
<p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. 8</figref> illustrates a generalized example of a suitable computing environment <b>800</b> in which several of the described embodiments may be implemented. The computing environment <b>800</b> is not intended to suggest any limitation as to scope of use or functionality, as the techniques and tools may be implemented in diverse general-purpose or special-purpose computing environments.</p>
<p id="p-0075" num="0074">With reference to <figref idref="DRAWINGS">FIG. 8</figref>, the computing environment <b>800</b> includes at least one processing unit <b>810</b> and memory <b>820</b>. In <figref idref="DRAWINGS">FIG. 8</figref>, this most basic configuration <b>830</b> is included within a dashed line. The processing unit <b>810</b> executes computer-executable instructions and may be a real or a virtual processor. In a multi-processing system, multiple processing units execute computer-executable instructions to increase processing power. The memory <b>820</b> may be volatile memory (e.g., registers, cache, RAM), non-volatile memory (e.g., ROM, EEPROM, flash memory, etc.), or some combination of the two. The memory <b>820</b> stores software <b>880</b> implementing a video encoder or decoder with two-reference field motion vector prediction for interlaced P-fields.</p>
<p id="p-0076" num="0075">A computing environment may have additional features. For example, the computing environment <b>800</b> includes storage <b>840</b>, one or more input devices <b>850</b>, one or more output devices <b>860</b>, and one or more communication connections <b>870</b>. An interconnection mechanism (not shown) such as a bus, controller, or network interconnects the components of the computing environment <b>800</b>. Typically, operating system software (not shown) provides an operating environment for other software executing in the computing environment <b>800</b>, and coordinates activities of the components of the computing environment <b>800</b>.</p>
<p id="p-0077" num="0076">The storage <b>840</b> may be removable or non-removable, and includes magnetic disks, magnetic tapes or cassettes, CD-ROMs, DVDs, or any other medium which can be used to store information and which can be accessed within the computing environment <b>800</b>. The storage <b>840</b> stores instructions for the software <b>880</b> implementing the video encoder or decoder.</p>
<p id="p-0078" num="0077">The input device(s) <b>850</b> may be a touch input device such as a keyboard, mouse, pen, or trackball, a voice input device, a scanning device, or another device that provides input to the computing environment <b>800</b>. For audio or video encoding, the input device(s) <b>850</b> may be a sound card, video card, TV tuner card, or similar device that accepts audio or video input in analog or digital form, or a CD-ROM or CD-RW that reads audio or video samples into the computing environment <b>800</b>. The output device(s) <b>860</b> may be a display, printer, speaker, CD-writer, or another device that provides output from the computing environment <b>800</b>.</p>
<p id="p-0079" num="0078">The communication connection(s) <b>870</b> enable communication over a communication medium to another computing entity. The communication medium conveys information such as computer-executable instructions, audio or video input or output, or other data in a modulated data signal. A modulated data signal is a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example, and not limitation, communication media include wired or wireless techniques implemented with an electrical, optical, RF, infrared, acoustic, or other carrier.</p>
<p id="p-0080" num="0079">The techniques and tools can be described in the general context of computer-readable media. Computer-readable media are any available media that can be accessed within a computing environment. By way of example, and not limitation, with the computing environment <b>800</b>, computer-readable media include memory <b>820</b>, storage <b>840</b>, communication media, and combinations of any of the above.</p>
<p id="p-0081" num="0080">The techniques and tools can be described in the general context of computer-executable instructions, such as those included in program modules, being executed in a computing environment on a target real or virtual processor. Generally, program modules include routines, programs, libraries, objects, classes, components, data structures, etc. that perform particular tasks or implement particular abstract data types. The functionality of the program modules may be combined or split between program modules as desired in various embodiments. Computer-executable instructions for program modules may be executed within a local or distributed computing environment.</p>
<p id="p-0082" num="0081">For the sake of presentation, the detailed description uses terms like &#x201c;estimate,&#x201d; &#x201c;compensate,&#x201d; &#x201c;predict,&#x201d; and &#x201c;apply&#x201d; to describe computer operations in a computing environment. These terms are high-level abstractions for operations performed by a computer, and should not be confused with acts performed by a human being. The actual computer operations corresponding to these terms vary depending on implementation.</p>
<p id="h-0012" num="0000">II. Generalized Video Encoder and Decoder</p>
<p id="p-0083" num="0082"><figref idref="DRAWINGS">FIG. 9</figref> is a block diagram of a generalized video encoder <b>900</b> in conjunction with which described embodiments may be implemented. <figref idref="DRAWINGS">FIG. 10</figref> is a block diagram of a generalized video decoder <b>1000</b> in conjunction with which described embodiments may be implemented.</p>
<p id="p-0084" num="0083">The relationships shown between modules within the encoder <b>900</b> and decoder <b>1000</b> indicate general flows of information in the encoder and decoder; other relationships are not shown for the sake of simplicity. In particular, <figref idref="DRAWINGS">FIGS. 9 and 10</figref> usually do not show side information indicating the encoder settings, modes, tables, etc. used for a video sequence, picture, macroblock, block, etc. Such side information is sent in the output bitstream, typically after entropy encoding of the side information. The format of the output bitstream can be a Windows Media Video version 9 format or other format.</p>
<p id="p-0085" num="0084">The encoder <b>900</b> and decoder <b>1000</b> process video pictures, which may be video frames, video fields or combinations of frames and fields. The bitstream syntax and semantics at the picture and macroblock levels may depend on whether frames or fields are used. There may be changes to macroblock organization and overall timing as well. The encoder <b>900</b> and decoder <b>1000</b> are block-based and use a 4:2:0 macroblock format for frames, with each macroblock including four 8&#xd7;8 luminance blocks (at times treated as one 16&#xd7;16 macroblock) and two 8&#xd7;8 chrominance blocks. For fields, the same or a different macroblock organization and format may be used. The 8&#xd7;8 blocks may be further sub-divided at different stages, e.g., at the frequency transform and entropy encoding stages. Example video frame organizations are described in more detail below. Alternatively, the encoder <b>900</b> and decoder <b>1000</b> are object-based, use a different macroblock or block format, or perform operations on sets of pixels of different size or configuration than 8&#xd7;8 blocks and 16&#xd7;16 macroblocks.</p>
<p id="p-0086" num="0085">Depending on implementation and the type of compression desired, modules of the encoder or decoder can be added, omitted, split into multiple modules, combined with other modules, and/or replaced with like modules. In alternative embodiments, encoders or decoders with different modules and/or other configurations of modules perform one or more of the described techniques.</p>
<p id="p-0087" num="0086">A. Video Frame Organizations</p>
<p id="p-0088" num="0087">In some implementations, the encoder <b>900</b> and decoder <b>1000</b> process video frames organized as follows. A frame contains lines of spatial information of a video signal. For progressive video, these lines contain samples starting from one time instant and continuing through successive lines to the bottom of the frame. A progressive video frame is divided into macroblocks such as the macroblock <b>1100</b> shown in <figref idref="DRAWINGS">FIG. 11</figref>. The macroblock <b>1100</b> includes four 8&#xd7;8 luminance blocks (Y<b>1</b> through Y<b>4</b>) and two 8&#xd7;8 chrominance blocks that are co-located with the four luminance blocks but half resolution horizontally and vertically, following the conventional 4:2:0 macroblock format. The 8&#xd7;8 blocks may be further sub-divided at different stages, e.g., at the frequency transform and entropy encoding stages. A progressive I-frame is an intra-coded progressive video frame. A progressive P-frame is a progressive video frame coded using forward prediction, and a progressive B-frame is a progressive video frame coded using bi-directional prediction. Progressive P and B-frames may include intra-coded macroblocks as well as different types of predicted macroblocks.</p>
<p id="p-0089" num="0088">For interlaced video, a frame consists of two fields, a top field and a bottom field. One of these fields commences one field period later than the other. <figref idref="DRAWINGS">FIG. 12</figref><i>a </i>shows part of an interlaced video frame <b>1200</b>, including the alternating lines of the top field and bottom field at the top left part of the interlaced video frame <b>1200</b>.</p>
<p id="p-0090" num="0089"><figref idref="DRAWINGS">FIG. 12</figref><i>b </i>shows the interlaced video frame <b>1200</b> of <figref idref="DRAWINGS">FIG. 12</figref><i>a </i>organized for encoding/decoding as a frame <b>1230</b>. The interlaced video frame <b>1200</b> has been partitioned into macroblocks such as the macroblocks <b>1231</b> and <b>1232</b>, which use 4:2:0 format as shown in <figref idref="DRAWINGS">FIG. 11</figref>. In the luminance plane each macroblock <b>1231</b>, <b>1232</b> includes 8 lines from the top field alternating with 8 lines from the bottom field for 16 lines total, and each line is 16 pixels long. (The actual organization and placement of luminance blocks and chrominance blocks within the macroblocks <b>1231</b>, <b>1232</b> are not shown, and in fact may vary for different encoding decisions). Within a given macroblock, the top-field information and bottom-field information may be coded jointly or separately at any of various phases. An interlaced I-frame is two intra-coded fields of an interlaced video frame, where a macroblock includes information for the two fields. An interlaced P-frame is two fields of an interlaced video frame coded using forward prediction, and an interlaced B-frame is two fields of an interlaced video frame coded using bi-directional prediction, where a macroblock includes information for the two fields. Interlaced P and B-frames may include intra-coded macroblocks as well as different types of predicted macroblocks.</p>
<p id="p-0091" num="0090"><figref idref="DRAWINGS">FIG. 12</figref><i>c </i>shows the interlaced video frame <b>1200</b> of <figref idref="DRAWINGS">FIG. 12</figref><i>a </i>organized for encoding/decoding as fields <b>1260</b>. Each of the two fields of the interlaced video frame <b>1200</b> is partitioned into macroblocks. The top field is partitioned into macroblocks such as the macroblock <b>1261</b>, and the bottom field is partitioned into macroblocks such as the macroblock <b>1262</b>. (Again, the macroblocks use 4:2:0 format as shown in <figref idref="DRAWINGS">FIG. 11</figref>, and the organization and placement of luminance blocks and chrominance blocks within the macroblocks are not shown). In the luminance plane, the macroblock <b>1261</b> includes 16 lines from the top field and the macroblock <b>1262</b> includes 16 lines from the bottom field, and each line is 16 pixels long. An interlaced I-field is a single, separately represented field of an interlaced video frame. An interlaced P-field is single, separately represented field of an interlaced video frame coded using forward prediction, and an interlaced B-field is a single, separately represented field of an interlaced video frame coded using bi-directional prediction. Interlaced P and B-fields may include intra-coded macroblocks as well as different types of predicted macroblocks.</p>
<p id="p-0092" num="0091">The term picture generally refers to source, coded or reconstructed image data. For progressive video, a picture is a progressive video frame. For interlaced video, a picture may refer to an interlaced video frame, the top field of the frame, or the bottom field of the frame, depending on the context.</p>
<p id="p-0093" num="0092">B. Video Encoder</p>
<p id="p-0094" num="0093"><figref idref="DRAWINGS">FIG. 9</figref> is a block diagram of a generalized video encoder system <b>900</b>. The encoder system <b>900</b> receives a sequence of video pictures including a current picture <b>905</b>, and produces compressed video information <b>995</b> as output. Particular embodiments of video encoders typically use a variation or supplemented version of the generalized encoder <b>900</b>.</p>
<p id="p-0095" num="0094">The encoder system <b>900</b> compresses predicted pictures and key pictures. For the sake of presentation, <figref idref="DRAWINGS">FIG. 9</figref> shows a path for key pictures through the encoder system <b>900</b> and a path for predicted pictures. Many of the components of the encoder system <b>900</b> are used for compressing both key pictures and predicted pictures. The exact operations performed by those components can vary depending on the type of information being compressed.</p>
<p id="p-0096" num="0095">A predicted picture (also called p-picture, b-picture for bi-directional prediction, or inter-coded picture) is represented in terms of prediction (or difference) from one or more other pictures (which are typically referred to as reference pictures or anchors). A prediction residual is the difference between what was predicted and the original picture. In contrast, a key picture (also called I-picture, intra-coded picture) is compressed without reference to other pictures.</p>
<p id="p-0097" num="0096">If the current picture <b>905</b> is a forward-predicted picture, a motion estimator <b>910</b> estimates motion of macroblocks or other sets of pixels of the current picture <b>905</b> with respect to a reference picture or pictures, which is the reconstructed previous picture(s) <b>925</b> buffered in the picture store(s) <b>920</b>, <b>922</b>. If the current picture <b>905</b> is a bi-directionally-predicted picture (a B-picture), a motion estimator <b>910</b> estimates motion in the current picture <b>905</b> with respect to two or more reconstructed reference pictures. Typically, a motion estimator estimates motion in a B-picture with respect to at least one temporally previous reference picture and at least one temporally future reference picture. Accordingly, the encoder system <b>900</b> can use the separate stores <b>920</b> and <b>922</b> for backward and forward reference pictures. For more information on bi-directionally predicted pictures, see U.S. patent application Ser. No. 10/622,378, entitled, &#x201c;Advanced Bi-Directional Predictive Coding of Video Frames,&#x201d; filed Jul. 18, 2003.</p>
<p id="p-0098" num="0097">The motion estimator <b>910</b> can estimate motion by pixel, &#xbd; pixel, &#xbc; pixel, or other increments, and can switch the resolution of the motion estimation on a picture-by-picture basis or other basis. The resolution of the motion estimation can be the same or different horizontally and vertically. The motion estimator <b>910</b> outputs as side information motion information <b>915</b> such as differential motion vector information. The encoder <b>900</b> encodes the motion information <b>915</b> by, for example, computing one or more predictors for motion vectors, computing differentials between the motion vectors and predictors, and entropy coding the differentials. To reconstruct a motion vector, a motion compensator <b>930</b> combines a predictor with differential motion vector information. Various techniques for computing motion vector predictors, computing differential motion vectors, and reconstructing motion vectors for interlaced P-fields are described below.</p>
<p id="p-0099" num="0098">The motion compensator <b>930</b> applies the reconstructed motion vector to the reconstructed picture(s) <b>925</b> to form a motion-compensated current picture <b>935</b>. The prediction is rarely perfect, however, and the difference between the motion-compensated current picture <b>935</b> and the original current picture <b>905</b> is the prediction residual <b>945</b>. Alternatively, a motion estimator and motion compensator apply another type of motion estimation/compensation.</p>
<p id="p-0100" num="0099">A frequency transformer <b>960</b> converts the spatial domain video information into frequency domain (i.e., spectral) data. For block-based video pictures, the frequency transformer <b>960</b> applies a discrete cosine transform [&#x201c;DCT&#x201d;], variant of DCT, or other block transform to blocks of the pixel data or prediction residual data, producing blocks of frequency transform coefficients. Alternatively, the frequency transformer <b>960</b> applies another conventional frequency transform such as a Fourier transform or uses wavelet or sub-band analysis. The frequency transformer <b>960</b> may apply an 8&#xd7;8, 8&#xd7;4, 4&#xd7;8, 4&#xd7;4 or other size frequency transform.</p>
<p id="p-0101" num="0100">A quantizer <b>970</b> then quantizes the blocks of spectral data coefficients. The quantizer applies uniform, scalar quantization to the spectral data with a step-size that varies on a picture-by-picture basis or other basis. Alternatively, the quantizer applies another type of quantization to the spectral data coefficients, for example, a non-uniform, vector, or non-adaptive quantization, or directly quantizes spatial domain data in an encoder system that does not use frequency transformations. In addition to adaptive quantization, the encoder <b>900</b> can use frame dropping, adaptive filtering, or other techniques for rate control.</p>
<p id="p-0102" num="0101">If a given macroblock in a predicted picture has no information of certain types (e.g., no motion information for the macroblock and no residual information), the encoder <b>900</b> may encode the macroblock as a skipped macroblock. If so, the encoder signals the skipped macroblock in the output bitstream of compressed video information <b>995</b>.</p>
<p id="p-0103" num="0102">When a reconstructed current picture is needed for subsequent motion estimation/compensation, an inverse quantizer <b>976</b> performs inverse quantization on the quantized spectral data coefficients. An inverse frequency transformer <b>966</b> then performs the inverse of the operations of the frequency transformer <b>960</b>, producing a reconstructed prediction residual (for a predicted picture) or a reconstructed key picture. If the current picture <b>905</b> was a key picture, the reconstructed key picture is taken as the reconstructed current picture (not shown). If the current picture <b>905</b> was a predicted picture, the reconstructed prediction residual is added to the motion-compensated current picture <b>935</b> to form the reconstructed current picture. One of the he picture store <b>920</b>, <b>922</b> buffers the reconstructed current picture for use in predicting the next picture. In some embodiments, the encoder applies a de-blocking filter to the reconstructed picture to adaptively smooth discontinuities in the picture.</p>
<p id="p-0104" num="0103">The entropy coder <b>980</b> compresses the output of the quantizer <b>970</b> as well as certain side information (e.g., for motion information <b>915</b>, quantization step size). Typical entropy coding techniques include arithmetic coding, differential coding, Huffman coding, run length coding, LZ coding, dictionary coding, and combinations of the above. The entropy coder <b>980</b> typically uses different coding techniques for different kinds of information (e.g., DC coefficients, AC coefficients, different kinds of side information), and can choose from among multiple code tables within a particular coding technique.</p>
<p id="p-0105" num="0104">The entropy coder <b>980</b> provides compressed video information <b>995</b> to the multiplexer [&#x201c;MUX&#x201d;] <b>990</b>. The MUX <b>990</b> may include a buffer, and a buffer level indicator may be fed back to bit rate adaptive modules for rate control. Before or after the MUX <b>990</b>, the compressed video information <b>995</b> can be channel coded for transmission over the network. The channel coding can apply error detection and correction data to the compressed video information <b>995</b>.</p>
<p id="p-0106" num="0105">C. Video Decoder</p>
<p id="p-0107" num="0106"><figref idref="DRAWINGS">FIG. 10</figref> is a block diagram of a general video decoder system <b>1000</b>. The decoder system <b>1000</b> receives information <b>1095</b> for a compressed sequence of video pictures and produces output including a reconstructed picture <b>1005</b>. Particular embodiments of video decoders typically use a variation or supplemented version of the generalized decoder <b>1000</b>.</p>
<p id="p-0108" num="0107">The decoder system <b>1000</b> decompresses predicted pictures and key pictures. For the sake of presentation, <figref idref="DRAWINGS">FIG. 10</figref> shows a path for key pictures through the decoder system <b>1000</b> and a path for forward-predicted pictures. Many of the components of the decoder system <b>1000</b> are used for decompressing both key pictures and predicted pictures. The exact operations performed by those components can vary depending on the type of information being decompressed.</p>
<p id="p-0109" num="0108">A DEMUX <b>1090</b> receives the information <b>1095</b> for the compressed video sequence and makes the received information available to the entropy decoder <b>1080</b>. The DEMUX <b>1090</b> may include a jitter buffer and other buffers as well. Before or after the DEMUX <b>1090</b>, the compressed video information can be channel decoded and processed for error detection and correction.</p>
<p id="p-0110" num="0109">The entropy decoder <b>1080</b> entropy decodes entropy-coded quantized data as well as entropy-coded side information (e.g., for motion information <b>1015</b>, quantization step size), typically applying the inverse of the entropy encoding performed in the encoder. Entropy decoding techniques include arithmetic decoding, differential decoding, Huffman decoding, run length decoding, LZ decoding, dictionary decoding, and combinations of the above. The entropy decoder <b>1080</b> typically uses different decoding techniques for different kinds of information (e.g., DC coefficients, AC coefficients, different kinds of side information), and can choose from among multiple code tables within a particular decoding technique.</p>
<p id="p-0111" num="0110">The decoder <b>1000</b> decodes the motion information <b>1015</b> by, for example, computing one or more predictors for motion vectors, entropy decoding differential motion vectors, and combining decoded differential motion vectors with predictors to reconstruct motion vectors. Various techniques for computing motion vector predictors, computing differential motion vectors, and reconstructing motion vectors for interlaced P-fields are described below.</p>
<p id="p-0112" num="0111">A motion compensator <b>1030</b> applies the motion information <b>1015</b> to one or more reference pictures <b>1025</b> to form a prediction <b>1035</b> of the picture <b>1005</b> being reconstructed. For example, the motion compensator <b>1030</b> uses one or more macroblock motion vectors to find macroblock(s) in the reference picture(s) <b>1025</b>. One or more picture stores (e.g., picture stores <b>1020</b>, <b>1022</b>) store previous reconstructed pictures for use as reference pictures. Typically, B-pictures have more than one reference picture (e.g., at least one temporally previous reference picture and at least one temporally future reference picture). Accordingly, the decoder system <b>1000</b> can use separate picture stores <b>1020</b> and <b>1022</b> for backward and forward reference pictures. The motion compensator <b>1030</b> can compensate for motion at pixel, &#xbd; pixel, &#xbc; pixel, or other increments, and can switch the resolution of the motion compensation on a picture-by-picture basis or other basis. The resolution of the motion compensation can be the same or different horizontally and vertically. Alternatively, a motion compensator applies another type of motion compensation. The prediction by the motion compensator is rarely perfect, so the decoder <b>1000</b> also reconstructs prediction residuals.</p>
<p id="p-0113" num="0112">An inverse quantizer <b>1070</b> inverse quantizes entropy-decoded data. In general, the inverse quantizer applies uniform, scalar inverse quantization to the entropy-decoded data with a step-size that varies on a picture-by-picture basis or other basis. Alternatively, the inverse quantizer applies another type of inverse quantization to the data, for example, a non-uniform, vector, or non-adaptive quantization, or directly inverse quantizes spatial domain data in a decoder system that does not use inverse frequency transformations.</p>
<p id="p-0114" num="0113">An inverse frequency transformer <b>1060</b> converts the quantized, frequency domain data into spatial domain video information. For block-based video pictures, the inverse frequency transformer <b>1060</b> applies an inverse DCT [&#x201c;IDCT&#x201d;], variant of IDCT, or other inverse block transform to blocks of the frequency transform coefficients, producing pixel data or prediction residual data for key pictures or predicted pictures, respectively. Alternatively, the inverse frequency transformer <b>1060</b> applies another conventional inverse frequency transform such as an inverse Fourier transform or uses wavelet or sub-band synthesis. The inverse frequency transformer <b>1060</b> may apply an 8&#xd7;8, 8&#xd7;4, 4&#xd7;8, 4&#xd7;4, or other size inverse frequency transform.</p>
<p id="p-0115" num="0114">For a predicted picture, the decoder <b>1000</b> combines the reconstructed prediction residual <b>1045</b> with the motion compensated prediction <b>1035</b> to form the reconstructed picture <b>1005</b>. When the decoder needs a reconstructed picture <b>1005</b> for subsequent motion compensation, one of the picture stores (e.g., picture store <b>1020</b>) buffers the reconstructed picture <b>1005</b> for use in predicting the next picture. In some embodiments, the decoder <b>1000</b> applies a de-blocking filter to the reconstructed picture to adaptively smooth discontinuities in the picture.</p>
<p id="h-0013" num="0000">III. Motion Vector Prediction</p>
<p id="p-0116" num="0115">Motion vectors for macroblocks (or blocks) can be used to predict the motion vectors in a causal neighborhood of those macroblocks (or blocks). For example, an encoder/decoder can select a motion vector predictor for a current macroblock from among the motion vectors for neighboring candidate macroblocks, and predictively encode the motion vector for the current macroblock using the motion vector predictor. An encoder/decoder can use median-of-three prediction, median-of-four prediction, or some other technique to determine the motion vector predictor for the current macroblock from among candidate motion vectors from neighboring macroblocks. A procedure for median-of-three prediction is described in pseudo-code <b>1300</b> in <figref idref="DRAWINGS">FIG. 13</figref>. A procedure for median-of-four prediction is described in pseudo-code <b>1400</b> in <figref idref="DRAWINGS">FIG. 14</figref>.</p>
<p id="h-0014" num="0000">IV. Field Coding for Interlaced Pictures</p>
<p id="p-0117" num="0116">A typical interlaced video frame consists of two fields (e.g., a top field and a bottom field) scanned at different times. In general, it is more efficient to encode stationary regions of an interlaced video frame by coding fields together (&#x201c;frame mode&#x201d; coding). On the other hand, it is often more efficient to code moving regions of an interlaced video frame by coding fields separately (&#x201c;field mode&#x201d; coding), because the two fields tend to have different motion. A forward-predicted interlaced video frame may be coded as two separate forward-predicted fields&#x2014;interlaced P-fields. Coding fields separately for a forward-predicted interlaced video frame may be efficient, for example, when there is high motion throughout the interlaced video frame, and hence much difference between the fields.</p>
<p id="p-0118" num="0117">A. Reference Fields for Interlaced P-fields</p>
<p id="p-0119" num="0118">Interlaced P-fields reference one or more other fields (typically previous fields, which may or may not be coded in a bitstream). For example, in some implementations an interlaced P-field may have one or two reference fields. If the interlaced P-field has two reference fields, a particular motion vector for a block or macroblock of the P-field refers to a selected one of the two reference fields. <figref idref="DRAWINGS">FIGS. 15 and 16</figref> show examples of interlaced P-fields having two reference fields. In <figref idref="DRAWINGS">FIG. 15</figref>, current field <b>1510</b> refers to a top field <b>1520</b> and bottom field <b>1530</b> in a temporally previous frame. Since fields <b>1540</b> and <b>1550</b> are interlaced B-fields, they are not used as reference fields. In <figref idref="DRAWINGS">FIG. 16</figref>, current field <b>1610</b> refers to a top field <b>1620</b> and bottom field <b>1630</b> in a predicted frame immediately previous to the interlaced video frame containing the current field <b>1610</b>. In other cases, an interlaced P-field references a single field, for example, the most recent or second most recent I-field or P-field.</p>
<p id="p-0120" num="0119">Alternatively, interlaced P-fields may use fields from other fields from frames of different types or temporal positions as reference fields.</p>
<p id="p-0121" num="0120">B. Field Picture Coordinate System and Field Polarities</p>
<p id="p-0122" num="0121">Motion vector units can be expressed in pixel/sub-pixel units or field units. For example, if the vertical component of a motion vector indicates a displacement of six quarter-pixel units, this indicates a displacement of one and a half field lines, because each line in the field in one pixel high.</p>
<p id="p-0123" num="0122"><figref idref="DRAWINGS">FIG. 17</figref> shows a relationship between vertical components of motion vectors and spatial locations in one implementation. The example shown in <figref idref="DRAWINGS">FIG. 17</figref> shows three different scenarios <b>1710</b>, <b>1720</b> and <b>1730</b> for three different combinations of current and reference field types (e.g., top and bottom). If the field types are different for the current and reference fields, the polarity is &#x201c;opposite.&#x201d; If the field types are the same, the polarity is &#x201c;same.&#x201d; For each scenario, <figref idref="DRAWINGS">FIG. 17</figref> shows one vertical column of pixels in a current field and a second vertical column of pixels in a reference field. In reality, the two columns are horizontally aligned. A circle represents an actual integer-pixel position and an X represents an interpolated half or quarter-pixel position. Horizontal component values (not shown) need not account for any offset due to interlacing, as the respective fields are horizontally aligned. Negative values indicate offsets further above, and in the opposite direction, as the positive value vertical offsets shown.</p>
<p id="p-0124" num="0123">In scenario <b>1710</b>, the polarity is &#x201c;opposite.&#x201d; The current field is a top field and the reference field is a bottom field. Relative to the current field, the position of the reference field is offset by a half pixel in the downward direction due to the interlacing. Thus, a vertical motion vector component value of 0 represents a position in the reference field that is offset by a half pixel below the location in the current field, as a default &#x201c;no motion&#x201d; value. A vertical component value of +2 represents a position offset by a full pixel (in absolute terms) below the location in the current field, which is an interpolated value in the reference field, and a vertical component of +4 represents a position offset by one and a half pixels (in absolute terms) below the location in the current field, which is an actual value in the reference field.</p>
<p id="p-0125" num="0124">In scenario <b>1720</b>, the polarity is also &#x201c;opposite.&#x201d; The current field is a bottom field and the reference field is a top field. Relative to the current field, the position of the reference field is offset by a half pixel in the upward direction due to the interlacing. Thus, a vertical motion vector component of 0 represents a position in the reference field that is a half pixel (in absolute terms) above the location in the current field, a vertical component value of +2 represents a position at the same level (in absolute terms) as the location in the current field, and a vertical component of +4 represents a position offset by a half pixel below (in absolute terms) the location in the current field.</p>
<p id="p-0126" num="0125">In scenario <b>1730</b>, the polarity is &#x201c;same,&#x201d; and no vertical offset is applied because the position of the current field is the same relative to the reference field.</p>
<p id="p-0127" num="0126">Alternatively, displacements for motion vectors are expressed according to a different convention.</p>
<p id="h-0015" num="0000">V. Innovations in Motion Vector Prediction for Predictive Coding/Decoding of Interlaced P-fields</p>
<p id="p-0128" num="0127">Described embodiments include techniques and tools for coding and decoding interlaced video (e.g., interlaced P-fields). The described techniques and tools can be used in combination with one another or with other techniques and tools, or can be used independently.</p>
<p id="p-0129" num="0128">In particular, described techniques and tools specify, for example, ways to generate motion vector predictors for blocks and/or macroblocks of interlace P-fields that use two fields as references for motion compensated prediction coding. Described techniques and tools implement one or more innovations, which include but are not limited to the following:
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0129">1. Generating two motion vector predictors&#x2014;one for the even field reference and one for the odd field reference. Each predictor is derived from three previously coded, candidate neighboring motion vectors,</li>
        <li id="ul0002-0002" num="0130">2. Using the motion vector predictor from the same reference field as the current motion vector (both refer to the same reference field).</li>
        <li id="ul0002-0003" num="0131">3. Scaling actual motion vector predictor candidate values for one field to generate motion vector predictor candidates for the other field for motion vector prediction.</li>
        <li id="ul0002-0004" num="0132">4. Using a scaling operation that uses the relative temporal distances between the current field and the two reference fields to derive a motion vector predictor candidate for one reference polarity from the existing motion vector predictor candidate of the other polarity.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0130" num="0133">In interlaced video, fields may be coded using no motion prediction (intra or I fields), using forward motion prediction (P-fields) or using bi-directional prediction (B fields). Described techniques and tools involve computing motion vector predictors for portions of interlaced P-fields, in particular, in cases where the motion compensation can occur with reference to either of the two most recent (in display order) I- or P-fields. It is assumed that the two reference fields are of opposite polarities, meaning that one reference field represents odd lines of an interlaced video frame and the other reference field represents even lines of the same or different interlaced video frame. This is the case for the signaling protocol indicated in <figref idref="DRAWINGS">FIG. 24</figref>, which shows a syntax element (FPTYPE) for field picture type for an interlaced picture. Or, for example, referring again to <figref idref="DRAWINGS">FIG. 15</figref>, current field <b>1510</b> refers to a top field <b>1520</b> and a bottom field <b>1530</b>, which represent the odd and even lines, respectively, of a video frame.</p>
<p id="p-0131" num="0134">A. Motion Vector Prediction in Two-Reference Field Pictures</p>
<p id="p-0132" num="0135">With two-reference field P-fields, a current field can reference two fields in the same temporal direction (e.g., the two most recent previous reference fields). In the case of a two-reference field interlaced P-field, the encoder and decoder select between two motion vector predictors for a motion vector of a block or macroblock. In some embodiments, one predictor is for a reference field of same polarity as the current field, and the other is for a reference field of opposite polarity. Other combinations of polarities also are possible.</p>
<p id="p-0133" num="0136">In some embodiments, an encoder/decoder calculates a motion vector predictor for a current block or macroblock by finding an odd field predictor and an even field predictor, and selecting one of the predictors to process the macroblock. For example, <figref idref="DRAWINGS">FIG. 18</figref> shows a technique <b>1800</b> for calculating a motion vector predictor for a block or macroblock of an interlaced P-field having two possible reference fields.</p>
<p id="p-0134" num="0137">At <b>1810</b>, an encoder/decoder determines an odd field motion vector predictor and even field motion vector predictor. One of the motion vector predictors thus has the same polarity as the current field, and the other motion vector predictor has the opposite polarity.</p>
<p id="p-0135" num="0138">At <b>1820</b>, the encoder/decoder selects a motion vector predictor from among the odd field motion vector predictor and the even field motion vector predictor. For example, the encoder selects between the motion vector predictors based upon which gives better prediction. Or, the encoder selects the motion vector predictor that refers to the same reference field as the motion vector that is currently being predicted. The encoder signals which motion vector predictor to use using a simple selection signal or using more complex signaling that incorporates contextual information to improve coding efficiency. The contextual information may indicate which of the odd field or even field, or which of the same polarity field or opposite polarity field, has been used predominately in the neighborhood around the block or macroblock. The decoder selects which motion vector predictor to use based upon the selection signal and/or the contextual information.</p>
<p id="p-0136" num="0139">At <b>1830</b>, the encoder/decoder processes the motion vector using the selected motion vector predictor. For example, the encoder encodes a differential between the motion vector and the motion vector predictor. Or, the decoder decodes the motion vector by combining the motion vector differential and the motion vector predictor.</p>
<p id="p-0137" num="0140">Alternatively, the encoder and/or decoder may skip determining the odd field motion vector predictor or determining the even field motion vector predictor. For example, if the encoder determines that the odd field will be used for motion compensation for a particular block or macroblock, the encoder determines only the odd field motion vector predictor. Or, if the decoder determines from contextual and/or signaled information that the odd field will be used for motion compensation, the decoder determines only the odd field motion vector predictor. In this way, the encoder and decoder may avoid unnecessary operations.</p>
<p id="p-0138" num="0141">In one implementation, a decoder employs the following technique to determine motion vector predictors for a current interlaced P-field:</p>
<p id="p-0139" num="0142">For each block or macroblock with a motion vector in an interlaced P-field, two sets of three candidate motion vector predictors are obtained. The positions of the neighboring macroblocks from which these candidate motion vector predictors are obtained relative to a current macroblock <b>1900</b> are shown in <figref idref="DRAWINGS">FIG. 19</figref>. Three of the candidates are from the even reference field and three are from the odd reference field. Since the neighboring macroblocks in each candidate direction (A, B or C) will either be intra-coded or have an actual motion vector that references either the even field or the odd field, there is a need to derive the other field's motion vector. For example, for a given macroblock, suppose predictor A has a motion vector which references the odd field. In this case, the &#x201c;even field&#x201d; predictor candidate A is derived from the motion vector of &#x201c;odd field&#x201d; predictor candidate A. This derivation is accomplished using a scaling operation. (See, for example, the explanation of <figref idref="DRAWINGS">FIGS. 21A and 21B</figref> below). Alternatively, the derivation is accomplished in another manner.</p>
<p id="p-0140" num="0143">Once the three odd field candidate motion vector predictors have been obtained, a median operation is used to derive an odd field motion vector predictor from the three odd field candidates. Similarly, once the three even field candidate motion vector predictors have been obtained, a median operation is used to derive an even field motion vector predictor from the three even field candidates. Alternatively, another mechanism is used to select the field motion vector predictor based upon the candidate field motion vector predictors. The decoder decides whether to use the even field or odd field as the motion vector predictor (e.g., by selecting the dominant predictor), and the even or odd motion vector predictor is used to reconstruct the motion vector.</p>
<p id="p-0141" num="0144">The pseudo-code <b>2000</b> in <figref idref="DRAWINGS">FIGS. 20A-20F</figref> illustrates a process used to generate motion vector predictors from predictors A, B, and C as arranged in <figref idref="DRAWINGS">FIG. 19</figref>. While <figref idref="DRAWINGS">FIG. 19</figref> shows a neighborhood for a typical macroblock in the middle of the current interlaced P-field, the pseudo-code <b>2000</b> of <figref idref="DRAWINGS">FIGS. 20A-20F</figref> addresses various special cases for macroblock locations. In addition, the pseudo-code <b>2000</b> may be used to compute a motion vector predictor for the motion vector of a block in various locations.</p>
<p id="p-0142" num="0145">In the pseudo-code <b>2000</b>, the terms &#x201c;same field&#x201d; and &#x201c;Opposite field&#x201d; are to be understood relative to the field currently being coded or decoded. If the current field is an even field, for example, the &#x201c;same field&#x201d; is the even reference field and the &#x201c;opposite field&#x201d; is the odd reference field. The variables samefieldpred_x and samefieldpred_y in the pseudo-code <b>2000</b> represent the horizontal and vertical components of the motion vector predictor from the same field, and the variables oppositefieldpred_x and oppositefieldpred_y represent the horizontal and vertical components of the motion vector predictor from the opposite field. The variables samecount and oppositecount track how many of the motion vectors for the neighbors of the current block or macroblock reference the &#x201c;same&#x201d; polarity reference field for the current field and how many reference the &#x201c;opposite&#x201d; polarity reference field, respectively. The variables samecount and oppositecount are initialized to 0 at the beginning of the pseudo-code.</p>
<p id="p-0143" num="0146">The scaling operations scaleforsame( ) and scaleforopposite( ) mentioned in the pseudo-code <b>2000</b> are used to derive motion vector predictor candidates for the &#x201c;other&#x201d; field from the actual motion vector values of the neighbors. The scaling operations are implementation-dependent. Example scaling operations are described below with reference to <figref idref="DRAWINGS">FIGS. 21A</figref>, <b>21</b>B, <b>22</b>, and <b>23</b>. Alternatively, other scaling operations are used, for example, to compensate for vertical displacements such as those shown in <figref idref="DRAWINGS">FIG. 17</figref>.</p>
<p id="p-0144" num="0147"><figref idref="DRAWINGS">FIGS. 20A and 20B</figref> show pseudo-code for computing a motion vector predictor for a typical internal block or macroblock. The motion vectors for &#x201c;intra&#x201d; neighbors are set to 0. For each neighbor, the same field motion vector predictor and opposite field motion vector predictor are set, where one is set from the actual value of the motion vector for the neighbor, and the other is derived therefrom. The median of the candidates is computed for the same field motion vector predictor and the opposite field motion vector predictor, and the &#x201c;dominant&#x201d; predictor is determined from samecount and oppositecount. The variable dominantpredictor indicates which field contains the dominant motion vector predictor. A motion vector predictor is dominant if it has the same polarity as more of the three candidate predictors. (The signaled value predictor_flag, which is decoded along with the motion vector differential data, indicates whether the dominant or non-dominant predictor is used).</p>
<p id="p-0145" num="0148">The pseudo-code in <figref idref="DRAWINGS">FIG. 20C</figref> addresses the situation of a macroblock in an interlaced P-field with only one macroblock per row, for which there are no neighbors B or C. The pseudo-code in <figref idref="DRAWINGS">FIGS. 20D and 20E</figref> addresses the situation of a block or macroblock at the left edge of an interlaced P-field, for which there is no neighbor C. Here, a motion vector predictor is dominant if it has the same polarity as more of the two candidate predictors, with the opposite field motion vector predictor being dominant in the case of a tie. Finally, the pseudo-code in <figref idref="DRAWINGS">FIG. 20F</figref> addresses, for example, the cases of macroblock in the top row of an interlaced P-field.</p>
<p id="p-0146" num="0149">B. Scaling for Derivation of One Field Motion Vector Predictor from Another Field Motion Vector Predictor</p>
<p id="p-0147" num="0150">In one implementation, an encoder/decoder derives one field motion vector predictor from another field motion vector predictor using the scaling operation illustrated in the pseudo-code <b>2100</b> of <figref idref="DRAWINGS">FIGS. 21A and 21B</figref>. The values of SCALEOPP, SCALESAME<b>1</b>, SCALESAME<b>2</b>, SCALEZONE<b>1</b>_X, SCALEZONE<b>1</b>_Y ZONE<b>1</b>OFFSET_X and ZONE<b>1</b>OFFSET_Y are implementation dependent. Two possible sets of values are shown in table <b>2200</b> in <figref idref="DRAWINGS">FIG. 22</figref> for the case where the current field is first field in the interlaced video frame, and in table <b>2300</b> in <figref idref="DRAWINGS">FIG. 23</figref> for the case where the current field is the second field in the interlaced video frame. In tables <b>2200</b> and <b>2300</b>, the reference frame distance is defined as the number of B-pictures (i.e., a video frame containing two B-fields) between the current field and the frame containing the temporally furthest reference field.</p>
<p id="p-0148" num="0151">In the examples shown in tables <b>2200</b> and <b>2300</b>, the value of N is dependant on a motion vector range. For example, an extended motion vector range can be signaled by the syntax element EXTENDED_MV=1. If EXTENDED_MV=1, the MVRANGE syntax element is present in the picture header and signals the motion vector range. If EXTENDED_MV=0 then a default motion vector range is used. Table 1 shows the relationship between N and the MVRANGE.</p>
<p id="p-0149" num="0152">
<tables id="TABLE-US-00001" num="00001">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="center"/>
<thead>
<row>
<entry namest="1" nameend="1" rowsep="1">TABLE 1</entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
<row>
<entry>Derivation of N in FIGS. 22 and 23</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="offset" colwidth="42pt" align="left"/>
<colspec colname="1" colwidth="77pt" align="center"/>
<colspec colname="2" colwidth="98pt" align="center"/>
<tbody valign="top">
<row>
<entry/>
<entry>MVRANGE</entry>
<entry>N</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="offset" colwidth="42pt" align="left"/>
<colspec colname="1" colwidth="77pt" align="center"/>
<colspec colname="2" colwidth="98pt" align="char" char="."/>
<tbody valign="top">
<row>
<entry/>
<entry>0 or default</entry>
<entry>1</entry>
</row>
<row>
<entry/>
<entry>&#x2002;10</entry>
<entry>2</entry>
</row>
<row>
<entry/>
<entry>110</entry>
<entry>8</entry>
</row>
<row>
<entry/>
<entry>111</entry>
<entry>16</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
<br/>
The values shown in tables <b>2200</b> and <b>2300</b> can be modified depending on implementation.
</p>
<p id="p-0150" num="0153">Alternatively, extended motion vector range information is not used, or scaling can be performed in some other way.</p>
<p id="h-0016" num="0000">VI. Combined Implementations</p>
<p id="p-0151" num="0154">A detailed combined implementation for a bitstream syntax and decoder are now described, in addition to an alternative combined implementation with minor differences from the main combined implementation.</p>
<p id="p-0152" num="0155">A. Bitstream Syntax</p>
<p id="p-0153" num="0156">In various combined implementations, data for interlaced P-fields is presented in the form of a bitstream having plural layers (e.g., sequence, frame, field, macroblock, block and/or sub-block layers). Data for each frame including an interlaced P-field consists of a frame header followed by data for the field layers. The bitstream elements that make up the frame header for a frame including an interlaced P-field are shown in <figref idref="DRAWINGS">FIG. 24</figref>. The bitstream elements that make up the field headers for interlaced P-fields are shown in <figref idref="DRAWINGS">FIG. 25</figref>. The bitstream elements that make up the macroblock layer for interlaced P-fields are shown in <figref idref="DRAWINGS">FIG. 26</figref>. The following sections describe selected bitstream elements in the frame, field and macroblock layers that are related to signaling for motion vector prediction for blocks or macroblocks of interlaced P-fields.</p>
<p id="p-0154" num="0157">1. Selected Frame Layer Elements</p>
<p id="p-0155" num="0158"><figref idref="DRAWINGS">FIG. 24</figref> is a diagram showing a frame-level bitstream syntax for frames including interlaced P-fields in a combined implementation. Specific bitstream elements are described below.</p>
<p id="h-0017" num="0000">Frame Coding Mode (FCM) (Variable Size)</p>
<p id="p-0156" num="0159">FCM is a variable length codeword [&#x201c;VLC&#x201d;] used to indicate the picture coding type. FCM takes on values for frame coding modes as shown in Table 2 below:</p>
<p id="p-0157" num="0160">
<tables id="TABLE-US-00002" num="00002">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="center"/>
<thead>
<row>
<entry namest="1" nameend="1" rowsep="1">TABLE 2</entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
<row>
<entry>Frame Coding Mode VLC</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="112pt" align="center"/>
<colspec colname="2" colwidth="105pt" align="left"/>
<tbody valign="top">
<row>
<entry>FCM value</entry>
<entry>Frame Coding Mode</entry>
</row>
<row>
<entry namest="1" nameend="2" align="center" rowsep="1"/>
</row>
<row>
<entry>&#x2002;0</entry>
<entry>Progressive</entry>
</row>
<row>
<entry>10</entry>
<entry>Frame-Interlace</entry>
</row>
<row>
<entry>11</entry>
<entry>Field-Interlace</entry>
</row>
<row>
<entry namest="1" nameend="2" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
<br/>
Field Picture Type (FPTYPE) (3 Bits)
</p>
<p id="p-0158" num="0161">FPTYPE is a three-bit syntax element present in the frame header for a frame including interlaced P-fields. FPTYPE takes on values for different combinations of field types according to Table 3 below.</p>
<p id="p-0159" num="0162">
<tables id="TABLE-US-00003" num="00003">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="center"/>
<thead>
<row>
<entry namest="1" nameend="1" rowsep="1">TABLE 3</entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
<row>
<entry>Field Picture Type FLC</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="1" colwidth="98pt" align="center"/>
<colspec colname="2" colwidth="21pt" align="center"/>
<colspec colname="3" colwidth="98pt" align="center"/>
<tbody valign="top">
<row>
<entry/>
<entry>First</entry>
<entry>Second</entry>
</row>
<row>
<entry>FPTYPE</entry>
<entry>Field</entry>
<entry>Field</entry>
</row>
<row>
<entry>FLC</entry>
<entry>Type</entry>
<entry>Type</entry>
</row>
<row>
<entry namest="1" nameend="3" align="center" rowsep="1"/>
</row>
<row>
<entry>000</entry>
<entry>I</entry>
<entry>I</entry>
</row>
<row>
<entry>001</entry>
<entry>I</entry>
<entry>P</entry>
</row>
<row>
<entry>010</entry>
<entry>P</entry>
<entry>I</entry>
</row>
<row>
<entry>011</entry>
<entry>P</entry>
<entry>P</entry>
</row>
<row>
<entry>100</entry>
<entry>B</entry>
<entry>B</entry>
</row>
<row>
<entry>101</entry>
<entry>B</entry>
<entry>BI</entry>
</row>
<row>
<entry>110</entry>
<entry>BI</entry>
<entry>B</entry>
</row>
<row>
<entry>111</entry>
<entry>BI</entry>
<entry>BI</entry>
</row>
<row>
<entry namest="1" nameend="3" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
<br/>
P Reference Distance (REFDIST) (Variable Size)
</p>
<p id="p-0160" num="0163">REFDIST is a variable sized syntax element. This element indicates the number of frames between the current frame and the reference frame. Table 4 shows the a VLC used to encode the REFDIST values.</p>
<p id="p-0161" num="0164">
<tables id="TABLE-US-00004" num="00004">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="center"/>
<thead>
<row>
<entry namest="1" nameend="1" rowsep="1">TABLE 4</entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
<row>
<entry>REFDIST VLC Table</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="1" colwidth="84pt" align="center"/>
<colspec colname="2" colwidth="56pt" align="center"/>
<colspec colname="3" colwidth="77pt" align="center"/>
<tbody valign="top">
<row>
<entry>Reference</entry>
<entry>VLC Codeword</entry>
<entry/>
</row>
<row>
<entry>Frame Dist.</entry>
<entry>(Binary)</entry>
<entry>VLC Size</entry>
</row>
<row>
<entry namest="1" nameend="3" align="center" rowsep="1"/>
</row>
<row>
<entry>0</entry>
<entry>00</entry>
<entry>2</entry>
</row>
<row>
<entry>1</entry>
<entry>01</entry>
<entry>2</entry>
</row>
<row>
<entry>2</entry>
<entry>10</entry>
<entry>2</entry>
</row>
<row>
<entry>N</entry>
<entry>11[(N &#x2212; 3) 1s]0</entry>
<entry>N</entry>
</row>
<row>
<entry namest="1" nameend="3" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
<br/>
The last row in Table 4 indicates the codewords used to represent reference frame distances greater than 2. These are coded as (binary) 11 followed by N&#x2212;3 1s, where N is the reference frame distance. The last bit in the codeword is 0. For example:
</p>
<p id="p-0162" num="0165">N=3, VLC Codeword=110, VLC Size=3</p>
<p id="p-0163" num="0166">N=4, VLC Codeword=1110, VLC Size=4</p>
<p id="p-0164" num="0167">N=5, VLC Codeword=11110, VLC Size=5</p>
<p id="p-0165" num="0168">In an alternative combined implementation, the picture type information is signaled at the beginning of the field level for an interlaced P-field, instead of at the frame level for the frame including the interlaced P-field, and P reference distance is omitted.</p>
<p id="p-0166" num="0169">2. Selected Field Layer Elements</p>
<p id="p-0167" num="0170"><figref idref="DRAWINGS">FIG. 25</figref> is a diagram showing a field-level bitstream syntax for interlaced P-fields in the combined implementation. Specific bitstream elements are described below.</p>
<p id="h-0018" num="0000">Number of Reference Pictures (NUMREF) (1 Bit)</p>
<p id="p-0168" num="0171">The NUMREF syntax element is a one-bit syntax element that indicates whether the current field may reference one or two previous reference field pictures. If NUMREF=0, then the current P field picture may only reference one field. In this case, the REFFIELD syntax element follows in the picture layer bitstream. For an interlaced P-field with two reference fields, NUMREF=1.</p>
<p id="h-0019" num="0000">Reference Field Picture Indicator (REFFIELD) (1 Bit)</p>
<p id="p-0169" num="0172">REFFIELD is a 1 bit syntax element present in interlace P-field picture headers if NUMREF=0. REFFIELD indicates which previously decoded field is used as a reference. If REFFIELD=0, then the temporally closest (in display order) I or P field is used as a reference. If REFFIELD=1, then the second most temporally recent I or P field picture is used as reference.</p>
<p id="h-0020" num="0000">Extended MV Range Flag (MVRANGE) (Variable Size)</p>
<p id="p-0170" num="0173">MVRANGE is a variable-sized syntax element present when the sequence-layer EXTENDED_MV bit is set to 1. The MVRANGE VLC represents a motion vector range.</p>
<p id="h-0021" num="0000">Extended Differential MV Range Flag (DMVRANGE) (Variable Size)</p>
<p id="p-0171" num="0174">DMVRANGE is a variable sized syntax element present if the sequence level syntax element EXTENDED_DMV=1. The DMVRANGE VLC represents a motion vector differential range.</p>
<p id="h-0022" num="0000">Motion Vector Mode (MVMODE) (Variable Size or 1 Bit)</p>
<p id="p-0172" num="0175">The MVMODE syntax element signals one of four motion vector coding modes or one intensity compensation mode. Several subsequent elements provide additional motion vector mode and/or intensity compensation information.</p>
<p id="h-0023" num="0000">Macroblock Mode Table (MBMODETAB) (2 or 3 Bits)</p>
<p id="p-0173" num="0176">The MBMODETAB syntax element is a fixed length field. For interlace P-fields, MBMODETAB is a 3 bit value that indicates which one of the eight Huffman tables is used to decode the macroblock mode syntax element (MBMODE) in the macroblock layer.</p>
<p id="h-0024" num="0000">Motion Vector Table (MVTAB) (2 or 3 Bits)</p>
<p id="p-0174" num="0177">The MVTAB syntax element is a 2 or 3 bit value. For interlace P-fields in which NUMREF=1, MVTAB is a 3 bit syntax element that indicates which of eight interlace Huffman tables are used to decode the motion vector data.</p>
<p id="h-0025" num="0000">4 MV Block Pattern Table (4 MVBPTAB) (2 Bits)</p>
<p id="p-0175" num="0178">The 4 MVBPTAB syntax element is a 2 bit value. For interlace P-fields, it is only present if MVMODE (or MVMODE<b>2</b>, if MVMODE is set to intensity compensation) indicates that the picture is of &#x201c;Mixed MV&#x201d; type. The 4 MVBPTAB syntax element signals which of four Huffman tables is used to decode the 4 MV block pattern (4 MVBP) syntax element in 4 MV macroblocks.</p>
<p id="p-0176" num="0179">3. Selected Macroblock Layer Elements</p>
<p id="p-0177" num="0180"><figref idref="DRAWINGS">FIG. 26</figref> is a diagram showing a macroblock-level bitstream syntax for interlaced P-fields in the combined implementation. Specific bitstream elements are described below. Data for a macroblock consists of a macroblock header followed by block layer data.</p>
<p id="h-0026" num="0000">Macroblock Mode (MBMODE) (Variable Size)</p>
<p id="p-0178" num="0181">The MBMODE syntax element indicates the macroblock type (1 MV, 4 MV or Intra) and also the presence of the CBP flag and motion vector data, as described in detail in Section VI.B.3. below.</p>
<p id="h-0027" num="0000">Motion Vector Data (MVDATA) (Variable Size)</p>
<p id="p-0179" num="0182">MVDATA is a variable sized syntax element that encodes differentials for the motion vector(s) for the macroblock, the decoding of which is described in detail in Section VI.B.3. below.</p>
<p id="h-0028" num="0000">4 MV Block Pattern (4 MVBP) (4 Bits)</p>
<p id="p-0180" num="0183">The 4 MVBP syntax element indicates which of the 4 luminance blocks contain non-zero motion vector differentials, the use of which is described in detail in Section VI.B.3. below.</p>
<p id="h-0029" num="0000">Block-level Motion Vector Data (BLKMVDATA) (Variable Size)</p>
<p id="p-0181" num="0184">BLKMVDATA is a variable-size syntax element that contains motion information for the block, and is present in 4 MV macroblocks.</p>
<p id="h-0030" num="0000">Hybrid Motion Vector Prediction (HYBRIDPRED) (1 Bit)</p>
<p id="p-0182" num="0185">HYBRIDPRED is a 1-bit syntax element per motion vector. If the predictor is explicitly coded in the bitstream, a bit is present that indicates whether to use predictor A or predictor C as the motion vector predictor.</p>
<p id="p-0183" num="0186">B. Decoding Interlaced P-fields</p>
<p id="p-0184" num="0187">The following sections describe a process for decoding interlaced P-fields in the combined implementation.</p>
<p id="p-0185" num="0188">1. Frame/Field Layer Decoding</p>
<p id="h-0031" num="0000">Reference Pictures</p>
<p id="p-0186" num="0189">A P-field Picture may reference either one or two previously decoded fields. The NUMREF syntax element in the picture layer is a one-bit syntax element that indicates whether the current field may reference one or two previous reference field pictures. If NUMREF=0, then the blocks and macroblocks of a current interlaced P-field picture may only reference one field. If NUMREF=1, then the blocks and macroblocks of the current interlaced P-field picture may use either of the two temporally closest (in display order) I or P field pictures as a reference.</p>
<p id="p-0187" num="0190"><figref idref="DRAWINGS">FIGS. 15 and 16</figref> show examples of two-reference P-fields.</p>
<p id="h-0032" num="0000">P-field Picture Types</p>
<p id="p-0188" num="0191">Interlaced P-field pictures may be one of two types: 1 MV or Mixed-MV. The following sections describe each type. In 1 MV P-fields, a single motion vector is used per motion-compensated macroblock to indicate the displacement of the predicted blocks for all 6 blocks in the macroblock. The 1 MV mode is signaled by the MVMODE and MVMODE<b>2</b> picture layer syntax elements.</p>
<p id="p-0189" num="0192">In Mixed-MV P-fields, each motion-compensated macroblock may be encoded as a 1 MV or a 4 MV macroblock. In a 4 MV macroblock, each of the 4 luminance blocks has a motion vector associated with it. In a Mixed-MV P-field, the 1 MV or 4 MV mode for each macroblock is indicated by the MBMODE syntax element at every macroblock. The Mixed-MV mode is signaled by the MVMODE and MVMODE<b>2</b> picture layer syntax elements.</p>
<p id="p-0190" num="0193">2. Macroblock Layer Decoding</p>
<p id="p-0191" num="0194">Macroblocks in interlaced P-field pictures may be one of 3 possible types: 1 MV, 4 MV, and Intra. The macroblock type is signaled by the MBMODE syntax element in the macroblock layer. The following sections describe the 1 MV and 4 MV types and how they are signaled.</p>
<p id="h-0033" num="0000">1 MV Macroblocks</p>
<p id="p-0192" num="0195">1 MV macroblocks may occur in 1 MV and Mixed-MV P-field pictures. A 1 MV macroblock is one where a single motion vector represents the displacement between the current and reference pictures for all 6 blocks in the macroblock. For a 1 MV macroblock, the MBMODE syntax element in the macroblock layer indicates three things:
<ul id="ul0003" list-style="none">
    <li id="ul0003-0001" num="0000">
    <ul id="ul0004" list-style="none">
        <li id="ul0004-0001" num="0196">1) That the macroblock type is 1 MV</li>
        <li id="ul0004-0002" num="0197">2) Whether the CBPCY syntax element is present</li>
        <li id="ul0004-0003" num="0198">3) Whether the MVDATA syntax element is present
<br/>
If the MBMODE syntax element indicates that the CBPCY syntax element is present, then the CBPCY syntax element is present in the macroblock layer in the corresponding position. The CBPCY indicates which of the 6 blocks are coded in the block layer. If the MBMODE syntax element indicates that the CBPCY syntax element is not present, then CBPCY is assumed to equal 0 and no block data is present for any of the 6 blocks in the macroblock.
</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0193" num="0199">If the MBMODE syntax element indicates that the MVDATA syntax element is present, then the MVDATA syntax element is present in the macroblock layer in the corresponding position. The MVDATA syntax element encodes the motion vector differential. The motion vector differential is combined with the motion vector predictor to reconstruct the motion vector. If the MBMODE syntax element indicates that the MVDATA syntax element is not present, then the motion vector differential is assumed to be zero and therefore the motion vector is equal to the motion vector predictor.</p>
<p id="h-0034" num="0000">4 MV Macroblocks</p>
<p id="p-0194" num="0200">4 MV macroblocks may only occur in Mixed-MV P-field pictures. A 4 MV macroblock is one where each of the 4 luminance blocks in a macroblock has an associated motion vector which indicates the displacement between the current and reference pictures for that block. The displacement for the chroma blocks is derived from the 4 luminance motion vectors.</p>
<p id="p-0195" num="0201">For a 4 MV macroblock, the MBMODE syntax element in the macroblock layer indicates three things:
<ul id="ul0005" list-style="none">
    <li id="ul0005-0001" num="0000">
    <ul id="ul0006" list-style="none">
        <li id="ul0006-0001" num="0202">1) That the macroblock type is 4 MV</li>
        <li id="ul0006-0002" num="0203">2) Whether the CBPCY syntax element is present</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0196" num="0204">The 4 MVBP syntax element indicates which of the 4 luminance blocks contain non-zero motion vector differentials. The 4 MVBP syntax element decodes to a value between 0 and 15. For each of the 4 bit positions in the 4 MVBP, a value of 0 indicates that no motion vector differential (BLKMVDATA) is present for that block and the motion vector differential is assumed to be 0. A value of 1 indicates that a motion vector differential (BLKMVDATA) is present for that block in the corresponding position. For example, if 4 MVBP decodes to a value of 1100 (binary), then the bitstream contains BLKMVDATA for blocks <b>0</b> and <b>1</b> and no BLKMVDATA is present for blocks <b>2</b> and <b>3</b>.</p>
<p id="p-0197" num="0205">In an alternative implementation, the MBMODE syntax element can indicate if whether the 4 MVBP syntax element is present; if the MBMODE syntax element indicates that the 4 MVBP syntax element is not present, then it is assumed that motion vector differential data (BLKMVDATA) is present for all 4 luminance blocks.</p>
<p id="p-0198" num="0206">Depending on whether the MVMODE/MVMODE<b>2</b> syntax element indicates mixed-MV or all-1 MV the MBMODE signals the information as follows. Table 5 shows how the MBMODE signals information about the macroblock in all-1 MV pictures.</p>
<p id="p-0199" num="0207">
<tables id="TABLE-US-00005" num="00005">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="center"/>
<thead>
<row>
<entry namest="1" nameend="1" rowsep="1">TABLE 5</entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
<row>
<entry>Macroblock Mode in All-1 MV Pictures</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="5">
<colspec colname="1" colwidth="21pt" align="left"/>
<colspec colname="2" colwidth="21pt" align="center"/>
<colspec colname="3" colwidth="70pt" align="center"/>
<colspec colname="4" colwidth="42pt" align="center"/>
<colspec colname="5" colwidth="63pt" align="center"/>
<tbody valign="top">
<row>
<entry/>
<entry>Index</entry>
<entry>Macroblock Type</entry>
<entry>CBP Present</entry>
<entry>MV Present</entry>
</row>
<row>
<entry namest="1" nameend="5" align="center" rowsep="1"/>
</row>
<row>
<entry/>
<entry>0</entry>
<entry>Intra</entry>
<entry>No</entry>
<entry>NA</entry>
</row>
<row>
<entry/>
<entry>1</entry>
<entry>Intra</entry>
<entry>Yes</entry>
<entry>NA</entry>
</row>
<row>
<entry/>
<entry>2</entry>
<entry>1 MV</entry>
<entry>No</entry>
<entry>No</entry>
</row>
<row>
<entry/>
<entry>3</entry>
<entry>1 MV</entry>
<entry>No</entry>
<entry>Yes</entry>
</row>
<row>
<entry/>
<entry>4</entry>
<entry>1 MV</entry>
<entry>Yes</entry>
<entry>No</entry>
</row>
<row>
<entry/>
<entry>5</entry>
<entry>1 MV</entry>
<entry>Yes</entry>
<entry>Yes</entry>
</row>
<row>
<entry namest="1" nameend="5" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0200" num="0208">Table 6 shows how the MBMODE signals information about the macroblock in mixed-MV pictures.</p>
<p id="p-0201" num="0209">
<tables id="TABLE-US-00006" num="00006">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="center"/>
<thead>
<row>
<entry namest="1" nameend="1" rowsep="1">TABLE 6</entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
<row>
<entry>Macroblock Mode in Mixed-1 MV Pictures</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="5">
<colspec colname="1" colwidth="14pt" align="left"/>
<colspec colname="2" colwidth="21pt" align="center"/>
<colspec colname="3" colwidth="77pt" align="center"/>
<colspec colname="4" colwidth="42pt" align="center"/>
<colspec colname="5" colwidth="63pt" align="center"/>
<tbody valign="top">
<row>
<entry/>
<entry>Index</entry>
<entry>Macroblock Type</entry>
<entry>CBP Present</entry>
<entry>MV Present</entry>
</row>
<row>
<entry namest="1" nameend="5" align="center" rowsep="1"/>
</row>
<row>
<entry/>
<entry>0</entry>
<entry>Intra</entry>
<entry>No</entry>
<entry>NA</entry>
</row>
<row>
<entry/>
<entry>1</entry>
<entry>Intra</entry>
<entry>Yes</entry>
<entry>NA</entry>
</row>
<row>
<entry/>
<entry>2</entry>
<entry>1 MV</entry>
<entry>No</entry>
<entry>No</entry>
</row>
<row>
<entry/>
<entry>3</entry>
<entry>1 MV</entry>
<entry>No</entry>
<entry>Yes</entry>
</row>
<row>
<entry/>
<entry>4</entry>
<entry>1 MV</entry>
<entry>Yes</entry>
<entry>No</entry>
</row>
<row>
<entry/>
<entry>5</entry>
<entry>1 MV</entry>
<entry>Yes</entry>
<entry>Yes</entry>
</row>
<row>
<entry/>
<entry>6</entry>
<entry>4 MV</entry>
<entry>No</entry>
<entry>NA</entry>
</row>
<row>
<entry/>
<entry>7</entry>
<entry>4 MV</entry>
<entry>Yes</entry>
<entry>NA</entry>
</row>
<row>
<entry namest="1" nameend="5" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
<br/>
One of 8 tables is used to signal the MBMODE. The table is signaled at the picture layer via the MBMODETAB syntax element.
</p>
<p id="p-0202" num="0210">3. Motion Vector Decoding Process</p>
<p id="p-0203" num="0211">The following sections describe the motion vector decoding process for blocks and macroblocks of P-field pictures.</p>
<p id="h-0035" num="0000">Decoding Motion Vector Differential</p>
<p id="p-0204" num="0212">The MVDATA or BLKMVDATA syntax elements encode motion information for macroblocks or the blocks in the macroblock. 1 MV macroblocks have a single MVDATA syntax element, and 4 MV macroblocks may have between zero and four BLKMVDATA elements. The following sections describe how to compute the motion vector differential for the one-reference (picture layer syntax element NUMREF=0) and two-reference (picture layer syntax element NUMREF=1) cases.</p>
<p id="h-0036" num="0000">Motion Vector Differentials in One-Reference Field Pictures</p>
<p id="p-0205" num="0213">In field pictures that have only one reference field, each MVDATA or BLKMVDATA syntax element in the macroblock layer jointly encodes two things: 1) the horizontal motion vector differential component and 2) the vertical motion vector differential component.</p>
<p id="p-0206" num="0214">The MVDATA or BLKMVDATA syntax element is a variable length Huffman codeword followed by a fixed length codeword. The value of the Huffman codeword determines the size of the fixed length codeword. The MVTAB syntax element in the picture layer specifies the Huffman table used to decode the variable sized codeword.</p>
<p id="p-0207" num="0215">The pseudo-code <b>2700</b> in <figref idref="DRAWINGS">FIG. 27A</figref> illustrates how the motion vector differential is decoded for a one-reference field. The values dmv_x and dmv_y are computed in the pseudo-code <b>2700</b>. The values are defined as follows:</p>
<p id="p-0208" num="0216">dmv_x: differential horizontal motion vector component,</p>
<p id="p-0209" num="0217">dmv_y: differential vertical motion vector component,</p>
<p id="p-0210" num="0218">k_x, k_y: fixed length for long motion vectors,</p>
<p id="p-0211" num="0219">k_x and k_y depend on the motion vector range as defined by the MVRANGE symbol.</p>
<p id="p-0212" num="0220">
<tables id="TABLE-US-00007" num="00007">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="center"/>
<thead>
<row>
<entry namest="1" nameend="1" rowsep="1">TABLE 7</entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
<row>
<entry>k_x and k_y specified by MVRANGE</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="5">
<colspec colname="1" colwidth="70pt" align="center"/>
<colspec colname="2" colwidth="14pt" align="center"/>
<colspec colname="3" colwidth="49pt" align="center"/>
<colspec colname="4" colwidth="28pt" align="center"/>
<colspec colname="5" colwidth="56pt" align="center"/>
<tbody valign="top">
<row>
<entry>MVRANGE</entry>
<entry>k_x</entry>
<entry>k_y</entry>
<entry>range_x</entry>
<entry>range_y</entry>
</row>
<row>
<entry namest="1" nameend="5" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="5">
<colspec colname="1" colwidth="70pt" align="center"/>
<colspec colname="2" colwidth="14pt" align="char" char="."/>
<colspec colname="3" colwidth="49pt" align="char" char="."/>
<colspec colname="4" colwidth="28pt" align="char" char="."/>
<colspec colname="5" colwidth="56pt" align="char" char="."/>
<tbody valign="top">
<row>
<entry>0 (default)</entry>
<entry>9</entry>
<entry>8</entry>
<entry>256</entry>
<entry>128</entry>
</row>
<row>
<entry>&#x2002;10</entry>
<entry>10</entry>
<entry>9</entry>
<entry>512</entry>
<entry>256</entry>
</row>
<row>
<entry>110</entry>
<entry>12</entry>
<entry>10</entry>
<entry>2048</entry>
<entry>512</entry>
</row>
<row>
<entry>111</entry>
<entry>13</entry>
<entry>11</entry>
<entry>4096</entry>
<entry>1024</entry>
</row>
<row>
<entry namest="1" nameend="5" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0213" num="0221">extend_x: extended range for horizontal motion vector differential,</p>
<p id="p-0214" num="0222">extend_y: extended range for vertical motion vector differential,</p>
<p id="p-0215" num="0223">extend_x and extend_y are derived from the DMVRANGE picture field syntax element. If DMVRANGE indicates that extended range for the horizontal component is used, then extend_x=1. Otherwise extend_x=0. Similarly, if DMVRANGE indicates that extended range for the vertical component is used, then extend_y=1 otherwise extend_y=0.</p>
<p id="p-0216" num="0224">The offset_table is an array used in the pseudo-code <b>2700</b> and is defined as shown in <figref idref="DRAWINGS">FIG. 27A</figref>.</p>
<p id="p-0217" num="0225">The pseudo-code <b>2710</b> in <figref idref="DRAWINGS">FIG. 27B</figref> illustrates how the motion vector differential is decoded for a one-reference field in an alternative combined implementation. Pseudo-code <b>2710</b> decodes motion vector differentials in a different way. For example, pseudo-code <b>2710</b> omits handling of extended motion vector differential ranges.</p>
<p id="h-0037" num="0000">Motion Vector Differentials in Two-Reference Field Pictures</p>
<p id="p-0218" num="0226">Two-reference field pictures occur in the coding of interlace frames using field pictures. Each frame of the sequence is separated into two fields, and each field is coded using what is essentially the progressive code path. Field pictures often have two reference fields and the coding of field picture motion vectors in this case is described below.</p>
<p id="p-0219" num="0227">In field pictures that have two reference fields, each MVDATA or BLKMVDATA syntax element in the macroblock layer jointly encodes three things: 1) the horizontal motion vector differential component, 2) the vertical motion vector differential component and 3) whether the dominant or non-dominant predictor is used, i.e., which of the two fields is referenced by the motion vector.</p>
<p id="p-0220" num="0228">The MVDATA or BLKMVDATA syntax element is a variable length Huffman codeword followed by a fixed length codeword. The value of the Huffman codeword determines the size of the fixed length codeword. The MVTAB syntax element in the picture layer specifies the Huffman table used to decode the variable sized codeword. The pseudo-code <b>2800</b> in <figref idref="DRAWINGS">FIG. 28A</figref> illustrates how the motion vector differential, and dominant/non-dominant predictor information are decoded.</p>
<p id="p-0221" num="0229">The values predictor_flag, dmv_x and dmv_y are computed in the pseudo-code <b>2800</b> in <figref idref="DRAWINGS">FIG. 28A</figref>. In addition to the variables and arrays shown in the pseudo-code <b>2700</b> in <figref idref="DRAWINGS">FIG. 27A</figref>, pseudo-code <b>2800</b> contains the variable predictor_flag, which is a binary flag indicating whether the dominant or non-dominant motion vector predictor is used (0=dominant predictor used, 1=non-dominant predictor used), and the table size_table, which is an array defined as shown in <figref idref="DRAWINGS">FIG. 28</figref>.</p>
<p id="p-0222" num="0230">The pseudo-code <b>2810</b> in <figref idref="DRAWINGS">FIG. 28B</figref> illustrates how the motion vector differential is decoded for a two-reference field in an alternative combined implementation. Pseudo-code <b>2810</b> decodes motion vector differentials in a different way. For example, pseudo-code <b>2810</b> omits handling of extended motion vector differential ranges.</p>
<p id="h-0038" num="0000">Motion Vector Predictors</p>
<p id="p-0223" num="0231">Motion vectors are computed by adding the motion vector differential computed in the previous section to a motion vector predictor. The predictor is computed from up to three neighboring motion vectors. The following sections describe how the motion vector predictors are calculated for macroblocks in 1 MV P-field pictures and Mixed-MV P-field pictures in this combined implementation.</p>
<p id="h-0039" num="0000">Motion Vector Predictors in 1 MV Interlaced P-fields</p>
<p id="p-0224" num="0232"><figref idref="DRAWINGS">FIGS. 29A and 29B</figref> are diagrams showing the locations of macroblocks considered for candidate motion vector predictors for a 1 MV macroblock in an interlaced P-field. In 1 MV interlaced P-fields, the candidate predictors are taken from the left, top and top-right macroblocks, except in the case where the macroblock is the last macroblock in the row. In this case, Predictor B is taken from the top-left macroblock instead of the top-right. For the special case where the frame is one macroblock wide, the predictor is always Predictor A (the top predictor). The special cases for the current macroblock being in the top row (with no A and B predictors, or with no predictors at all) are addressed above with reference to <figref idref="DRAWINGS">FIGS. 20A-20F</figref>.</p>
<p id="h-0040" num="0000">Motion Vector Predictors in Mixed-MV P Pictures</p>
<p id="p-0225" num="0233"><figref idref="DRAWINGS">FIGS. 30A-34</figref> show the locations of the blocks or macroblocks considered for the up to 3 candidate motion vectors for a motion vector for a 1 MV or 4 MV macroblock in Mixed-MV P-field pictures. In the following figures, the larger squares are macroblock boundaries and the smaller squares are block boundaries. For the special case where the frame is one macroblock wide, the predictor is always Predictor A (the top predictor). The special cases for the current block or macroblock being in the top row are addressed above with reference to <figref idref="DRAWINGS">FIGS. 20A-20F</figref>.</p>
<p id="p-0226" num="0234"><figref idref="DRAWINGS">FIGS. 30A and 30B</figref> are diagrams showing locations of blocks considered for candidate motion vector predictors for a 1 MV current macroblock in a mixed 1 MV/4 MV interlaced P-field. The neighboring macroblocks may be 1 MV or 4 MV macroblocks. <figref idref="DRAWINGS">FIGS. 30A and 30B</figref> show the locations for the candidate motion vectors assuming the neighbors are 4 MV (i.e., predictor A is the motion vector for block <b>2</b> in the macroblock above the current macroblock, and predictor C is the motion vector for block <b>1</b> in the macroblock immediately to the left of the current macroblock). If any of the neighbors is a 1 MV macroblock, then the motion vector predictor shown in <figref idref="DRAWINGS">FIGS. 29A and 29B</figref> is taken to be the vector for the entire macroblock. As <figref idref="DRAWINGS">FIG. 30B</figref> shows, if the macroblock is the last macroblock in the row, then Predictor B is from block <b>3</b> of the top-left macroblock instead of from block <b>2</b> in the top-right macroblock as is the case otherwise.</p>
<p id="p-0227" num="0235"><figref idref="DRAWINGS">FIGS. 31A-34</figref> show the locations of blocks considered for candidate motion vector predictors for each of the 4 luminance blocks in a 4 MV macroblock. <figref idref="DRAWINGS">FIGS. 31A and 31B</figref> are diagrams showing the locations of blocks considered for candidate motion vector predictors for a block at position <b>0</b> in a 4 MV macroblock in a mixed 1 MV/4 MV interlaced P-field; <figref idref="DRAWINGS">FIGS. 32A and 32B</figref> are diagrams showing the locations of blocks considered for candidate motion vector predictors for a block at position <b>1</b> in a 4 MV macroblock in a mixed interlaced P-field; <figref idref="DRAWINGS">FIG. 33</figref> is a diagram showing the locations of blocks considered for candidate motion vector predictors for a block at position <b>2</b> in a 4 MV macroblock in a mixed 1 MV/4 MV interlaced P-field; and <figref idref="DRAWINGS">FIG. 34</figref> is a diagram showing the locations of blocks considered for candidate motion vector predictors for a block at position <b>3</b> in a 4 MV macroblock in a mixed 1 MV/4 MV interlaced P-field. Again, if a neighbor is a 1 MV macroblock, the motion vector predictor for the macroblock is used for the blocks of the macroblock.</p>
<p id="p-0228" num="0236">For the case where the macroblock is the first macroblock in the row, Predictor B for block <b>0</b> is handled differently than block <b>0</b> the remaining macroblocks in the row. In this case, Predictor B is taken from block <b>3</b> in the macroblock immediately above the current macroblock instead of from block <b>3</b> in the macroblock above and to the left of current macroblock, as is the case otherwise. Similarly, for the case where the macroblock is the last macroblock in the row, Predictor B for block <b>1</b> is handled differently. In this case, the predictor is taken from block <b>2</b> in the macroblock immediately above the current macroblock instead of from block <b>2</b> in the macroblock above and to the right of the current macroblock, as is the case otherwise. If the macroblock is in the first macroblock column, then Predictor C for blocks <b>0</b> and <b>2</b> are set equal to 0.</p>
<p id="h-0041" num="0000">Dominant and Non-Dominant MV Predictors</p>
<p id="p-0229" num="0237">In two-reference field P-field pictures, for each inter-coded macroblock, two motion vector predictors are derived. One is from the dominant field and the other is from the non-dominant field. The dominant field is considered to be the field containing the majority of the actual-value motion vector predictor candidates in the neighborhood. In the case of a tie, the motion vector predictor for the opposite field is considered to be the dominant predictor. Intra-coded macroblocks are not considered in the calculation of the dominant/non-dominant predictor. If all candidate predictor macroblocks are intra-coded, then the dominant and non-dominant motion vector predictors are set to zero and the dominant predictor is taken to be from the opposite field.</p>
<p id="h-0042" num="0000">Calculating the Motion Vector Predictor</p>
<p id="p-0230" num="0238">If NUMREF=1, then the current field picture may refer to the two most recent field pictures, and two motion vector predictors are calculated for each motion vector of a block or macroblock. The pseudo-code <b>3500</b> in <figref idref="DRAWINGS">FIGS. 35A-35F</figref> describes how the motion vector predictors are calculated for the two-reference case in the combined implementation. (The pseudo-code <b>2000</b> in <figref idref="DRAWINGS">FIGS. 20A-20F</figref> describes how the motion vector predictors are calculated for the two-reference case in another implementation). In two-reference pictures (NUMREF=1) the current field may reference the two most recent fields. One predictor is for the reference field of the same polarity and the other is for the reference field with the opposite polarity.</p>
<p id="h-0043" num="0000">Scaling Operations in Combined Implementation</p>
<p id="p-0231" num="0239"><figref idref="DRAWINGS">FIGS. 21A-B</figref> are code diagrams showing pseudo-code <b>2100</b> for scaling a predictor from one field to derive a predictor from another field. The values of SCALEOPP, SCALESAME<b>1</b>, SCALESAME<b>2</b>, SCALEZONE<b>1</b>_X, SCALEZONE<b>1</b>_Y, ZONE<b>1</b>OFFSET_X and ZONE<b>1</b>OFFSET_Y in this combined implementation are shown in the table <b>2200</b> in <figref idref="DRAWINGS">FIG. 22</figref> (for the case where the current field is the first field) and the table <b>2300</b> in <figref idref="DRAWINGS">FIG. 23</figref> (for the case where the current field is the second field). The reference frame distance is encoded in the REFDIST field in the picture header. The reference frame distance is REFDIST+1.</p>
<p id="h-0044" num="0000">Reconstructing Motion Vectors</p>
<p id="p-0232" num="0240">The following sections describe how to reconstruct the luminance and chroma motion vectors for 1 MV and 4 MV macroblocks. After a motion vector is reconstructed, it may be subsequently used as a neighborhood motion vector to predict the motion vector for a nearby macroblock. If the motion vector is for a block or macroblock in a two-reference field interlaced P-field, the motion vector will have an associated polarity of &#x201c;same&#x201d; or &#x201c;opposite,&#x201d; and may be used to derive a motion vector predictor for the other field polarity for motion vector prediction.</p>
<p id="h-0045" num="0000">Luminance Motion Vector Reconstruction</p>
<p id="p-0233" num="0241">In all cases (1 MV and 4 MV macroblocks) the luminance motion vector is reconstructed by adding the differential to the predictor as follows:</p>
<p id="p-0234" num="0242">mv_x=(dmv_x+predictor_x) smod range_x</p>
<p id="p-0235" num="0243">mv_y=(dmv_y+predictor_y) smod range_y</p>
<p id="h-0046" num="0000">The modulus operation &#x201c;smod&#x201d; is a signed modulus, defined as follows:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>A</i>smod<i>b</i>=((<i>A+b</i>)%(2<i>*b</i>))&#x2212;<i>b </i><?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
This ensures that the reconstructed vectors are valid. (A smod b) lies within &#x2212;b and b&#x2212;1. range_x and range_y depend on MVRANGE.
</p>
<p id="p-0236" num="0244">If the interlaced P-field picture uses two reference pictures (NUMREF=1), then the predictor_flag derived after decoding the motion vector differential is combined with the value of dominantpredictor derived from motion vector prediction to determine which field is used as reference. The pseudo-code <b>3600</b> in <figref idref="DRAWINGS">FIG. 36</figref> describes how the reference field is determined.</p>
<p id="p-0237" num="0245">In 1 MV macroblocks there will be a single motion vector for the 4 blocks that make up the luminance component of the macroblock. If the MBMODE syntax element indicates that no MV data is present in the macroblock layer, then dmv_x=0 and dmv_y=0 (mv_x=predictor_x and mv_y=predictor_y).</p>
<p id="p-0238" num="0246">In 4 MV macroblocks, each of the inter-coded luminance blocks in a macroblock will have its own motion vector. Therefore there will be between 0 and 4 luminance motion vectors in each 4 MV macroblock. If the 4 MVBP syntax element indicates that no motion vector information is present for a block, then dmv_x=0 and dmv_y for that block (mv_x=predictor_x and mv_y=predictor_y).</p>
<p id="h-0047" num="0000">Chroma Motion Vector Reconstruction</p>
<p id="p-0239" num="0247">The chroma motion vectors are derived from the luminance motion vectors. In an alternative implementation, for 4 MV macroblocks, the decision on whether to code the chroma blocks as Inter or Intra is made based on the status of the luminance blocks or fields.</p>
<p id="p-0240" num="0248">Having described and illustrated the principles of my invention with reference to various embodiments, it will be recognized that the various embodiments can be modified in arrangement and detail without departing from such principles. It should be understood that the programs, processes, or methods described herein are not related or limited to any particular type of computing environment, unless indicated otherwise. Various types of general purpose or specialized computing environments may be used with or perform operations in accordance with the teachings described herein. Elements of embodiments shown in software may be implemented in hardware and vice versa.</p>
<p id="p-0241" num="0249">In view of the many possible embodiments to which the principles of my invention may be applied, I claim as my invention all such embodiments as may come within the scope and spirit of the following claims and equivalents thereto.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>I claim:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. One or more computer-readable storage media storing computer-executable instructions for causing a computing device that implements a video decoder to perform a method comprising:
<claim-text>at the computing device that implements the video decoder, receiving, from a bit stream, information indicating a motion vector differential for a motion vector of a current portion of a current interlaced P-field;</claim-text>
<claim-text>with the computing device that implements the video decoder, computing a motion vector predictor for the motion vector, including:
<claim-text>determining a first motion vector predictor candidate from a first neighbor of the current portion, the first motion vector predictor candidate referencing a first reference field having a first polarity;</claim-text>
<claim-text>computing a second motion vector predictor candidate by scaling an actual motion vector value of a second neighbor of the current portion, the actual motion vector value referencing a second reference field having a second polarity different than the first polarity; and</claim-text>
<claim-text>using the first motion vector predictor candidate and the second motion vector predictor candidate when computing the motion vector predictor for the motion vector for the current portion of the current interlaced P-field; and</claim-text>
</claim-text>
<claim-text>with the computing device that implements the video decoder, reconstructing the motion vector from the motion vector differential and the motion vector predictor.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The one or more computer-readable storage media of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the first motion vector predictor candidate has the opposite field polarity as the current portion, wherein the actual motion vector value has the same field polarity as the current portion, and wherein the scaling is adapted to scale from the same polarity to the opposite polarity.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The one or more computer-readable storage media of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the first motion vector predictor candidate has the same field polarity as the current portion, wherein the actual motion vector value has the opposite field polarity as the current portion, and wherein the scaling is adapted to scale from the opposite polarity to the same polarity.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The one or more computer-readable storage media of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the scaling is based at least in part on a reference frame distance.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. In a computing device that implements a video encoder, a method comprising:
<claim-text>with the computing device that implements the video encoder, computing a motion vector predictor for a motion vector of a current portion of a current interlaced P-field, including:
<claim-text>identifying a first candidate motion vector of a first neighbor of the current portion, the first candidate motion vector referencing a first reference field having a first polarity;</claim-text>
<claim-text>identifying a second candidate motion vector of a second neighbor of the current portion, the second candidate motion vector referencing a second reference field having a second polarity different than the first polarity;</claim-text>
<claim-text>scaling the second candidate motion vector; and</claim-text>
<claim-text>determining the motion vector predictor using at least the first candidate motion vector and the scaled second candidate motion vector;</claim-text>
</claim-text>
<claim-text>with the computing device that implements the video encoder, computing a motion vector differential between the motion vector and the motion vector predictor; and</claim-text>
<claim-text>with the computing device that implements the video encoder, signaling information indicating the motion vector differential in a bit stream.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref> wherein the first polarity is same polarity as the current interlaced P-field and the second polarity is opposite polarity as the current interlaced P-field, and wherein the scaling is adapted to scale from opposite polarity to same polarity.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref> wherein the first polarity is opposite polarity as the current interlaced P-field and the second polarity is same polarity as the current interlaced P-field, and wherein the scaling is adapted to scale from same polarity to opposite polarity.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref> wherein the scaling is based at least in part on reference frame distance, and wherein the reference frame distance indicates distance between a current interlaced frame that includes the current interlaced P-field and a previous interlaced frame that includes whichever of the first reference field and the second reference field is temporally further from the current interlaced P-field.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref> wherein the scaling includes looking up a scaling value in one of two tables, using a first table of the two tables when the current interlaced P-field is first field in a current interlaced frame and using a second table of the two tables when the current interlaced P-field is second field in the current interlaced frame.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref> wherein the motion vector predictor is a first polarity motion vector predictor that references the first reference field, the method further comprising:
<claim-text>with the computing device that implements the video encoder, computing a second polarity motion vector predictor that references the second reference field; and</claim-text>
<claim-text>with the computing device that implements the video encoder, selecting between using the first polarity motion vector predictor and the second polarity motion vector predictor.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref> further comprising:
<claim-text>using a motion estimator to find the motion vector for the current portion, wherein the motion estimator outputs side information including the motion vector differential.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref> further comprising, for each of multiple portions of the current interlaced P-field as the current portion:
<claim-text>performing motion estimation to find the motion vector for the current portion, wherein the motion vector references a matching portion in the first reference field, and wherein the motion vector predictor is selected as referring to the first reference field.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref> further comprising:
<claim-text>performing rate control that includes at least one of frame dropping, adaptive filtering and consideration of buffer feedback; and</claim-text>
<claim-text>performing channel coding of the bit stream.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. In a computing device that implements a video decoder, a method comprising:
<claim-text>at the computing device that implements the video decoder, receiving, from a bit stream, information indicating a motion vector differential for a motion vector of a current portion of a current interlaced P-field;</claim-text>
<claim-text>with the computing device that implements the video decoder, computing a motion vector predictor for the motion vector, including:
<claim-text>identifying a first candidate motion vector of a first neighbor of the current portion, the first candidate motion vector referencing a first reference field having a first polarity;</claim-text>
<claim-text>identifying a second candidate motion vector of a second neighbor of the current portion, the second candidate motion vector referencing a second reference field having a second polarity different than the first polarity;</claim-text>
<claim-text>scaling the second candidate motion vector; and</claim-text>
<claim-text>determining the motion vector predictor using at least the first candidate motion vector and the scaled second candidate motion vector; and</claim-text>
</claim-text>
<claim-text>with the computing device that implements the video decoder, reconstructing the motion vector from the motion vector differential and the motion vector predictor.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref> wherein the first polarity is same polarity as the current interlaced P-field and the second polarity is opposite polarity as the current interlaced P-field, and wherein the scaling is adapted to scale from opposite polarity to same polarity.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref> wherein the first polarity is opposite polarity as the current interlaced P-field and the second polarity is same polarity as the current interlaced P-field, and wherein the scaling is adapted to scale from same polarity to opposite polarity.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref> wherein the scaling is based at least in part on reference frame distance.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The method of <claim-ref idref="CLM-00017">claim 17</claim-ref> wherein the reference frame distance indicates distance between a current interlaced frame that includes the current interlaced P-field and a previous interlaced frame that includes whichever of the first reference field and the second reference field is temporally further from the current interlaced P-field.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref> wherein the scaling includes looking up a scaling value in a table of plural scaling values.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref> wherein the scaling includes looking up a scaling value in one of two tables, using a first table of the two tables when the current interlaced P-field is first field in a current interlaced frame and using a second table of the two tables when the current interlaced P-field is second field in the current interlaced frame.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref> wherein the portion is a macroblock.</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref> wherein the portion is a block of a macroblock.</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref> wherein the motion vector predictor is a first polarity motion vector predictor that references the first reference field, the method further comprising:
<claim-text>with the computing device that implements the video decoder, computing a second polarity motion vector predictor that references the second reference field; and</claim-text>
<claim-text>with the computing device that implements the video decoder, selecting between using the first polarity motion vector predictor and the second polarity motion vector predictor.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. A computer system that implements a video decoder, wherein the computer system comprises a processor, memory, speaker, voice input device, display and wireless communication connection, and wherein the computer system is adapted to:
<claim-text>receive, from a bit stream, information indicating a motion vector differential for a motion vector of a current macroblock of a current interlaced P-field;</claim-text>
<claim-text>compute a motion vector predictor for the motion vector, including:
<claim-text>identifying a first candidate motion vector of a first neighbor of the current macroblock, the first candidate motion vector referencing a first reference field having a first polarity;</claim-text>
<claim-text>identifying a second candidate motion vector of a second neighbor of the current macroblock, the second candidate motion vector referencing a second reference field having a second polarity different than the first polarity;</claim-text>
<claim-text>scaling the second candidate motion vector; and</claim-text>
<claim-text>determining the motion vector predictor using at least the first candidate motion vector and the scaled second candidate motion vector; and</claim-text>
</claim-text>
<claim-text>reconstruct the motion vector from the motion vector differential and the motion vector predictor.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00025" num="00025">
<claim-text>25. The computer system of <claim-ref idref="CLM-00024">claim 24</claim-ref> wherein:
<claim-text>when the first polarity is same polarity as the current interlaced P-field and the second polarity is opposite polarity as the current interlaced P-field, the scaling is adapted to scale from opposite polarity to same polarity; and</claim-text>
<claim-text>when the first polarity is opposite polarity as the current interlaced P-field and the second polarity is same polarity as the current interlaced P-field, the scaling is adapted to scale from same polarity to opposite polarity.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00026" num="00026">
<claim-text>26. The computer system of <claim-ref idref="CLM-00024">claim 24</claim-ref> wherein the scaling is based at least in part on reference frame distance, and wherein the reference frame distance indicates distance between a current interlaced frame that includes the current interlaced P-field and a previous interlaced frame that includes whichever of the first reference field and the second reference field is temporally further from the current interlaced P-field.</claim-text>
</claim>
<claim id="CLM-00027" num="00027">
<claim-text>27. The computer system of <claim-ref idref="CLM-00024">claim 24</claim-ref> wherein the scaling includes looking up a scaling value in one of two tables, using a first table of the two tables when the current interlaced P-field is first field in a current interlaced frame and using a second table of the two tables when the current interlaced P-field is second field in the current interlaced frame.</claim-text>
</claim>
<claim id="CLM-00028" num="00028">
<claim-text>28. A computer system that implements a video encoder, wherein the computer system comprises a processor and memory, and wherein the computer system is adapted to perform motion estimation that includes:
<claim-text>finding a motion vector of a current macroblock of a current interlaced P-field, wherein the motion vector references a matching macroblock of a first reference field having a first polarity;</claim-text>
<claim-text>computing a motion vector predictor for the motion vector, wherein the motion vector predictor is selected as referring to the first reference field, including:
<claim-text>identifying a first candidate motion vector of a first neighbor of the current macroblock, the first candidate motion vector referencing the first reference field;</claim-text>
<claim-text>identifying a second candidate motion vector of a second neighbor of the current macroblock, the second candidate motion vector referencing a second reference field having a second polarity different than the first polarity;</claim-text>
<claim-text>scaling the second candidate motion vector; and</claim-text>
<claim-text>determining the motion vector predictor using at least the first candidate motion vector and the scaled second candidate motion vector; and</claim-text>
</claim-text>
<claim-text>computing a motion vector differential between the motion vector and the motion vector predictor, wherein information indicating the motion vector differential is later signaled in a bit stream.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00029" num="00029">
<claim-text>29. The computer system of <claim-ref idref="CLM-00028">claim 28</claim-ref> wherein:
<claim-text>when the first polarity is same polarity as the current interlaced P-field and the second polarity is opposite polarity as the current interlaced P-field, the scaling is adapted to scale from opposite polarity to same polarity; and</claim-text>
<claim-text>when the first polarity is opposite polarity as the current interlaced P-field and the second polarity is same polarity as the current interlaced P-field, the scaling is adapted to scale from same polarity to opposite polarity.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00030" num="00030">
<claim-text>30. The computer system of <claim-ref idref="CLM-00028">claim 28</claim-ref> wherein the scaling is based at least in part on reference frame distance, and wherein the reference frame distance indicates distance between a current interlaced frame that includes the current interlaced P-field and a previous interlaced frame that includes whichever of the first reference field and the second reference field is temporally further from the current interlaced P-field.</claim-text>
</claim>
<claim id="CLM-00031" num="00031">
<claim-text>31. The computer system of <claim-ref idref="CLM-00028">claim 28</claim-ref> wherein the scaling includes looking up a scaling value in one of two tables, using a first table of the two tables when the current interlaced P-field is first field in a current interlaced frame and using a second table of the two tables when the current interlaced P-field is second field in the current interlaced frame.</claim-text>
</claim>
<claim id="CLM-00032" num="00032">
<claim-text>32. The computer system of <claim-ref idref="CLM-00028">claim 28</claim-ref> wherein the computer system is further adapted to:
<claim-text>perform rate control that includes at least one of frame dropping, adaptive filtering and consideration of buffer feedback; and</claim-text>
<claim-text>perform channel coding of the bit stream.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00033" num="00033">
<claim-text>33. One or more computer-readable storage media storing computer-executable instructions for causing a computing device that implements a video encoder to perform a method comprising:
<claim-text>computing a motion vector predictor for a motion vector of a current portion of a current interlaced P-field, including:
<claim-text>identifying a first candidate motion vector of a first neighbor of the current portion, the first candidate motion vector referencing a first reference field having a first polarity;</claim-text>
<claim-text>identifying a second candidate motion vector of a second neighbor of the current portion, the second candidate motion vector referencing a second reference field having a second polarity different than the first polarity;</claim-text>
<claim-text>scaling the second candidate motion vector; and</claim-text>
<claim-text>determining the motion vector predictor using at least the first candidate motion vector and the scaled second candidate motion vector;</claim-text>
</claim-text>
<claim-text>computing a motion vector differential between the motion vector and the motion vector predictor; and</claim-text>
<claim-text>signaling information indicating the motion vector differential in a bit stream.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00034" num="00034">
<claim-text>34. The one or more computer-readable storage media of <claim-ref idref="CLM-00033">claim 33</claim-ref> wherein the method further comprises:
<claim-text>using a motion estimator to find the motion vector for the current portion, wherein the motion estimator outputs side information including the motion vector differential.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00035" num="00035">
<claim-text>35. The one or more computer-readable storage media of <claim-ref idref="CLM-00033">claim 33</claim-ref> wherein the method further comprises, for each of multiple portions of the current interlaced P-field as the current portion:
<claim-text>performing motion estimation to find the motion vector for the current portion, wherein the motion vector references a matching portion in the first reference field, and wherein the motion vector predictor is selected as referring to the first reference field. </claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
