<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625851-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625851</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13547788</doc-number>
<date>20120712</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>JP</country>
<doc-number>2007-035918</doc-number>
<date>20070216</date>
</priority-claim>
</priority-claims>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382103</main-classification>
<further-classification>382100</further-classification>
</classification-national>
<invention-title id="d2e61">Measurement apparatus, measurement method, and feature identification apparatus</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>7180050</doc-number>
<kind>B2</kind>
<name>Imagawa et al.</name>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>7474964</doc-number>
<kind>B1</kind>
<name>Welty et al.</name>
<date>20090100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>702  2</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>7689032</doc-number>
<kind>B2</kind>
<name>Strassenburg-Kleciak</name>
<date>20100300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>7728833</doc-number>
<kind>B2</kind>
<name>Verma et al.</name>
<date>20100600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345420</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>8208689</doc-number>
<kind>B2</kind>
<name>Savolainen et al.</name>
<date>20120600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382110</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2004/0062418</doc-number>
<kind>A1</kind>
<name>Ishikura et al.</name>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2005/0100220</doc-number>
<kind>A1</kind>
<name>Keaton et al.</name>
<date>20050500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2006/0136126</doc-number>
<kind>A1</kind>
<name>Coombes et al.</name>
<date>20060600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>701208</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2008/0260237</doc-number>
<kind>A1</kind>
<name>Savolainen et al.</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2008/0319673</doc-number>
<kind>A1</kind>
<name>Welty et al.</name>
<date>20081200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>702  5</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2010/0250482</doc-number>
<kind>A1</kind>
<name>Ma</name>
<date>20100900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>706 54</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>JP</country>
<doc-number>7-262375</doc-number>
<date>19951000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>JP</country>
<doc-number>10-207351</doc-number>
<date>19980800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>JP</country>
<doc-number>2000-74667</doc-number>
<date>20000300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>JP</country>
<doc-number>2001-118182</doc-number>
<date>20010400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>JP</country>
<doc-number>2002-34055</doc-number>
<date>20020100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>JP</country>
<doc-number>2002-170102</doc-number>
<date>20020600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>JP</country>
<doc-number>2002-303671</doc-number>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>JP</country>
<doc-number>2003-30792</doc-number>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>JP</country>
<doc-number>2003-44995</doc-number>
<date>20030200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>JP</country>
<doc-number>2003-84064</doc-number>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>JP</country>
<doc-number>2004-30226</doc-number>
<date>20040100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>JP</country>
<doc-number>2004-348575</doc-number>
<date>20041200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>JP</country>
<doc-number>2005-77385</doc-number>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>JP</country>
<doc-number>2005-98853</doc-number>
<date>20050400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>JP</country>
<doc-number>2005-241609</doc-number>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>JP</country>
<doc-number>2005-337863</doc-number>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>JP</country>
<doc-number>2006-113645</doc-number>
<date>20060400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>JP</country>
<doc-number>2006-172099</doc-number>
<date>20060600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>JP</country>
<doc-number>2006-234703</doc-number>
<date>20060900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>WO</country>
<doc-number>WO 2005/088252</doc-number>
<kind>A1</kind>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00032">
<othercit>Visat:Mapping what you see, Ek-sheimy et al XP-002521410, May 2007.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00033">
<othercit>H. Gontran, et al., &#x201c;A Mobile Mapping System for Road Data Capture via a Single Camera,&#x201d; [online], retrieved on Feb. 14, 2006, Internet, &#x3c;URL:http://topo.epfl.ch/personnes/jsk/Papers/3dopt<sub>&#x2014;</sub>hg.pdf&#x3e;, 5 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00034">
<othercit>G. Manzoni, et al., &#x201c;Mobile Mapping Systems in Cultural Heritages Survey,&#x201d; CIPA 2005 XX International Symposium, 26 Sep. 1-Oct. 2005, 4 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00035">
<othercit>Shingo Ando, et al., &#x201c;A Study of Autonomous Mobile System in Outdoor Environment,&#x201d; Part 37 Improvement of Range Estimaion Accuracy by Baseline Optimization in Motion Stereo Using GPS/INS/ODV), 2 pages, (with English Abstract).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00036">
<othercit>Office Action mailed Aug. 14, 2012, in co-pending U.S. Appl. No. 12/527,478.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00037">
<othercit>Extended European Search Report Issued Nov. 7, 2012 in Patent Application No. 08711339.5.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00038">
<othercit>Office Action issued Nov. 2, 2012 in Canadian Patent Application No. 2,678,156.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00039">
<othercit>Dorota A. Grejner-Brzezinska, et al., &#x201c;High Accuracy Dynamic Highway Mapping Using a GPS/INS/CCD System with On-The-Fly GPS Ambiguity Resolution,&#x201d; GISS Users Manual Version 2.0, FHWA/OH-2004/013, Sep. 2004, pp. 1-41.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00040">
<othercit>H. Gontran, et al., &#x201c;A Mobile Mapping System for Road Data Capture via a Single Camera,&#x201d; [online], retrieved on Feb. 14, 2006, Internet, &#x3c;URL:http://topo.epfl.ch/personnes/jsk/Papers/3dopt<sub>'</sub>hg.pdf&#x3e;, 5 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00041">
<othercit>G. Manzoni, et al., &#x201c;Mobile Mapping Systems in Cultural Heritages Survey,&#x201d; CIPA 2005 XX International Symposium, Sep. 26-Oct. 1, 2005, 4 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00042">
<othercit>Shingo Ando, et al., &#x201c;A Study of Autonomous Mobile System in Outdoor Environment,&#x201d; Part 37 Improvement of Range Estimation Accuracy by Baseline Optimization in Motion Stereo Using GPS/INS/ODV), 2 pages, (with English Abstract), Sep. 2005.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00043">
<othercit>Jun-ichi Takiguchi, et al., &#x201c;High-Precision Range Estimation from an Omnidirectional Stereo System,&#x201d; vol. 69, No. 680, No. 01-1497, Apr. 2003, pp. 153-160, (with English Abstract).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00044">
<othercit>Japanese Office Action Issued Jun. 4, 2013 in Patent Application No. 2008-267247 (with partial English translation).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>6</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>None</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>46</number-of-drawing-sheets>
<number-of-figures>46</number-of-figures>
</figures>
<us-related-documents>
<division>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>12527478</doc-number>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>8483442</doc-number>
</document-id>
</parent-grant-document>
<parent-pct-document>
<document-id>
<country>WO</country>
<doc-number>PCT/JP2008/052509</doc-number>
<date>20080215</date>
</document-id>
</parent-pct-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13547788</doc-number>
</document-id>
</child-doc>
</relation>
</division>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20130011013</doc-number>
<kind>A1</kind>
<date>20130110</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Takiguchi</last-name>
<first-name>Junichi</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Kajiwara</last-name>
<first-name>Naoyuki</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Shima</last-name>
<first-name>Yoshihiro</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
<us-applicant sequence="004" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Kurosaki</last-name>
<first-name>Ryujiro</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
<us-applicant sequence="005" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Hashizume</last-name>
<first-name>Takumi</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Takiguchi</last-name>
<first-name>Junichi</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Kajiwara</last-name>
<first-name>Naoyuki</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Shima</last-name>
<first-name>Yoshihiro</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
<inventor sequence="004" designation="us-only">
<addressbook>
<last-name>Kurosaki</last-name>
<first-name>Ryujiro</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
<inventor sequence="005" designation="us-only">
<addressbook>
<last-name>Hashizume</last-name>
<first-name>Takumi</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Oblon, Spivak, McClelland, Maier &#x26; Neustadt, L.L.P.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Mitsubishi Electric Corporation</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
<assignee>
<addressbook>
<orgname>Waseda University</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Bitar</last-name>
<first-name>Nancy</first-name>
<department>2669</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">It is an object to measure a position of a feature around a road. An image memory unit stores images in which neighborhood of the road is captured. Further, a three-dimensional point cloud model memory unit <b>709</b> stores a point cloud showing three-dimensional coordinates obtained by laser measurement which is carried out simultaneously to the image-capturing of the images as a road surface shape model. Using an image point inputting unit <b>342</b>, a pixel on a feature of a measurement target is specified by a user as a measurement image point. A neighborhood extracting unit <b>171</b> extracts a point which is located adjacent to the measurement image point and superimposed on the feature for the measurement target from the point cloud. A feature position calculating unit <b>174</b> outputs three-dimensional coordinates shown by the extracted point as three-dimensional coordinates of the feature for the measurement target.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="166.45mm" wi="252.48mm" file="US08625851-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="266.62mm" wi="193.46mm" orientation="landscape" file="US08625851-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="224.96mm" wi="187.54mm" orientation="landscape" file="US08625851-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="257.47mm" wi="182.96mm" orientation="landscape" file="US08625851-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="248.75mm" wi="171.45mm" orientation="landscape" file="US08625851-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="248.07mm" wi="182.96mm" orientation="landscape" file="US08625851-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="254.00mm" wi="187.20mm" orientation="landscape" file="US08625851-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="235.46mm" wi="173.14mm" orientation="landscape" file="US08625851-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="246.30mm" wi="185.42mm" orientation="landscape" file="US08625851-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="233.34mm" wi="189.99mm" orientation="landscape" file="US08625851-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="250.87mm" wi="166.88mm" orientation="landscape" file="US08625851-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="265.18mm" wi="187.20mm" orientation="landscape" file="US08625851-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="264.50mm" wi="171.11mm" file="US08625851-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="249.77mm" wi="148.34mm" orientation="landscape" file="US08625851-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="255.78mm" wi="164.76mm" orientation="landscape" file="US08625851-20140107-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="258.15mm" wi="165.86mm" file="US08625851-20140107-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00016" num="00016">
<img id="EMI-D00016" he="221.06mm" wi="187.20mm" orientation="landscape" file="US08625851-20140107-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00017" num="00017">
<img id="EMI-D00017" he="216.24mm" wi="188.21mm" orientation="landscape" file="US08625851-20140107-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00018" num="00018">
<img id="EMI-D00018" he="240.71mm" wi="125.56mm" orientation="landscape" file="US08625851-20140107-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00019" num="00019">
<img id="EMI-D00019" he="203.28mm" wi="163.75mm" orientation="landscape" file="US08625851-20140107-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00020" num="00020">
<img id="EMI-D00020" he="232.66mm" wi="190.33mm" orientation="landscape" file="US08625851-20140107-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00021" num="00021">
<img id="EMI-D00021" he="215.14mm" wi="137.84mm" orientation="landscape" file="US08625851-20140107-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00022" num="00022">
<img id="EMI-D00022" he="254.34mm" wi="178.73mm" file="US08625851-20140107-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00023" num="00023">
<img id="EMI-D00023" he="276.35mm" wi="192.11mm" orientation="landscape" file="US08625851-20140107-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00024" num="00024">
<img id="EMI-D00024" he="206.08mm" wi="196.26mm" orientation="landscape" file="US08625851-20140107-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00025" num="00025">
<img id="EMI-D00025" he="257.81mm" wi="194.48mm" orientation="landscape" file="US08625851-20140107-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00026" num="00026">
<img id="EMI-D00026" he="255.35mm" wi="187.54mm" orientation="landscape" file="US08625851-20140107-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00027" num="00027">
<img id="EMI-D00027" he="204.98mm" wi="181.95mm" orientation="landscape" file="US08625851-20140107-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00028" num="00028">
<img id="EMI-D00028" he="278.55mm" wi="199.81mm" orientation="landscape" file="US08625851-20140107-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00029" num="00029">
<img id="EMI-D00029" he="258.15mm" wi="186.10mm" orientation="landscape" file="US08625851-20140107-D00029.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00030" num="00030">
<img id="EMI-D00030" he="248.07mm" wi="189.48mm" orientation="landscape" file="US08625851-20140107-D00030.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00031" num="00031">
<img id="EMI-D00031" he="262.72mm" wi="186.10mm" orientation="landscape" file="US08625851-20140107-D00031.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00032" num="00032">
<img id="EMI-D00032" he="220.73mm" wi="186.44mm" orientation="landscape" file="US08625851-20140107-D00032.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00033" num="00033">
<img id="EMI-D00033" he="268.65mm" wi="190.33mm" orientation="landscape" file="US08625851-20140107-D00033.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00034" num="00034">
<img id="EMI-D00034" he="265.51mm" wi="198.37mm" orientation="landscape" file="US08625851-20140107-D00034.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00035" num="00035">
<img id="EMI-D00035" he="263.40mm" wi="186.86mm" orientation="landscape" file="US08625851-20140107-D00035.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00036" num="00036">
<img id="EMI-D00036" he="223.35mm" wi="185.93mm" orientation="landscape" file="US08625851-20140107-D00036.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00037" num="00037">
<img id="EMI-D00037" he="260.27mm" wi="194.90mm" orientation="landscape" file="US08625851-20140107-D00037.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00038" num="00038">
<img id="EMI-D00038" he="267.72mm" wi="189.91mm" orientation="landscape" file="US08625851-20140107-D00038.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00039" num="00039">
<img id="EMI-D00039" he="232.66mm" wi="179.92mm" orientation="landscape" file="US08625851-20140107-D00039.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00040" num="00040">
<img id="EMI-D00040" he="256.20mm" wi="189.74mm" orientation="landscape" file="US08625851-20140107-D00040.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00041" num="00041">
<img id="EMI-D00041" he="254.17mm" wi="189.57mm" orientation="landscape" file="US08625851-20140107-D00041.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00042" num="00042">
<img id="EMI-D00042" he="273.22mm" wi="183.30mm" orientation="landscape" file="US08625851-20140107-D00042.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00043" num="00043">
<img id="EMI-D00043" he="266.28mm" wi="189.99mm" orientation="landscape" file="US08625851-20140107-D00043.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00044" num="00044">
<img id="EMI-D00044" he="263.48mm" wi="189.48mm" orientation="landscape" file="US08625851-20140107-D00044.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00045" num="00045">
<img id="EMI-D00045" he="261.79mm" wi="192.45mm" orientation="landscape" file="US08625851-20140107-D00045.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00046" num="00046">
<img id="EMI-D00046" he="262.72mm" wi="192.45mm" orientation="landscape" file="US08625851-20140107-D00046.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application is a divisional application of and claims the benefit of priority under 35 U.S.C. &#xa7;120 for U.S. Ser. No. 12/527,478, filed Aug. 17, 2009, pending, which is a National Stage application of PCT/JP2008/52509, filed Feb. 15, 2008 and claims benefit of priority under 35 U.S.C. &#xa7;119 from JP 2007-035918, filed Feb. 16, 2007, the entire contents of each of which are incorporated herein by reference.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">TECHNICAL FIELD</heading>
<p id="p-0003" num="0002">The present invention relates to, for example, a road feature measurement apparatus for measuring a feature position located on the road/side of the road, a feature identification apparatus, a road feature measuring method, a road feature measuring program, a measurement apparatus, a measuring method, a measuring program, measurement position data, a measurement terminal device, a measurement server device, a plotting apparatus, a plotting method, a plotting program, and plotting data.</p>
<heading id="h-0003" level="1">BACKGROUND ART</heading>
<p id="p-0004" num="0003">Recent years, a product combining GIS (Geographical Information System) and GPS (Global Positioning System) represented by a car navigation system, etc. has become remarkably popular. Further, on the other hand, it has been expected that the position information by GIS and GPS is applied to safe driving of an ITS (Intelligent Transport Systems); the position information of features located on the road/side of the road is considered to be effective information.</p>
<p id="p-0005" num="0004">Further, on the other hand, precision improvement and sophisticating of the road management ledger, which records information of features around roads, is expected. However, in order to generate the road management ledger, which records positions of features located on the road/side of the road such as a kilo post, a sign, a guardrail, a white line, etc. in 1/500 scale, surveying with a high precision is necessary, so that static survey using GPS and total station measuring distance/angle is carried out. Further, on bothways of a 30-kilometer section of national roads, sometimes there exist about 2,000 features to be a measurement target. Therefore, it requires huge cost and time to sophisticate and improve the precision of the road management ledgers across the country.</p>
<p id="p-0006" num="0005">Then, aiming to reduce time and cost for collecting information, MMS (Mobile Mapping System) has drawn attention and research and development thereof have been made.</p>
<p id="p-0007" num="0006">For example, for obtaining position information of a white line, stereo view using plural cameras or a method for estimating the position information of the white line from the setting position of a camera based on relation between a camera parameter and a vehicle are used.
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0007">Non-patent Document 1: Dorota A. Grejner-Brzezinska and Charles Toth, &#x201c;High Accuracy Dynamic Highway Mapping Using a GPS/INS/CCD System with On-The-Fly GPS Ambiguity Resolution&#x201d;, Center for Mapping Department of Civil and Environmental Engineering and Geodetic Science The Ohio State University, Ohio Department of Transportation, District 1, September 2004.</li>
    <li id="ul0001-0002" num="0008">Non-patent Document 2: H. Gontran, J, Skaloud, P.- Y. Gilliron, &#x201c;A MOBILE MAPPING SYSTEM FOR ROAD DATA CAPTURE VIA A SINGLE CAMERA&#x201d;, [online], [retrieved on Feb. 14, 2006], Internet, &#x3c;URL: http://topo.epfl.ch/personnes/jsk/Papers/3dopt_hg.pdf</li>
    <li id="ul0001-0003" num="0009">Non-patent Document 3: G. Manzoni, R. G. Rizzo, C. Robiglio, &#x201c;MOBILE MAPPING SYSTEMS IN CULTURAL HERITAGES SURVEY&#x201d;, CIPA ODOMETRY APPARATUS 2005 XX International Symposium, 26 Sep.-1 Oct. 2005, Torino, Italy.</li>
    <li id="ul0001-0004" num="0010">Patent Document 1: JP2005-098853</li>
    <li id="ul0001-0005" num="0011">Patent Document 2: JP2006-234703</li>
</ul>
</p>
<heading id="h-0004" level="1">DISCLOSURE OF THE INVENTION</heading>
<heading id="h-0005" level="1">Problems to be Solved by the Invention</heading>
<p id="p-0008" num="0012">The above methods include the following characteristics:</p>
<p id="p-0009" num="0013">a) Detection of white line position by the stereo view</p>
<p id="h-0006" num="0000">(1) It is possible to obtain the position of the white line using two cameras</p>
<p id="h-0007" num="0000">(2) In case of an endless white line, automatic search for a corresponding point is difficult, so that manual search for the corresponding point is necessary.</p>
<p id="h-0008" num="0000">(3) Effective view angle is narrow.</p>
<p id="h-0009" num="0000">(4) Absolute precision is low.</p>
<p id="p-0010" num="0014">b) Estimation of white line position by a camera parameter</p>
<p id="h-0010" num="0000">(1) Since the prescribed distance from the camera to the road is fixed and calculated, the precision is bad.</p>
<p id="h-0011" num="0000">(2) The precision is effected by oscillation of a vehicle.</p>
<p id="h-0012" num="0000">(3) The precision is largely degraded on an uneven road.</p>
<p id="h-0013" num="0000">(4) A single camera can obtain the position of a white line.</p>
<p id="p-0011" num="0015">The present invention aims, for example, to measure a position of a feature located on the road/side of the road other than the white line using MMS.</p>
<p id="p-0012" num="0016">In particular, the present invention aims to measure a position of a narrow feature such as a kilo post and a specular feature such as glass, for which the measurement data is difficult to obtain by MMS that obtains the measurement data during running, with a high precision.</p>
<p id="p-0013" num="0017">Further, the present invention aims to aid the user to specify a feature for the measurement target in order to provide measured result of the point desired by the user.</p>
<p id="p-0014" num="0018">Yet further, the present invention aims to measure the position of the feature with a high precision even if the road is uneven.</p>
<heading id="h-0014" level="1">Means to Solve the Problems</heading>
<p id="p-0015" num="0019">According to the present invention, a measurement apparatus includes: an image displaying unit for displaying and superimposing an image in which a feature is captured and a point cloud, which corresponds to the image and of which a three-dimensional position is known, on a screen of a displaying device, and for prompting a user to specify a position of the feature for a measurement target within the image; a measurement image point obtaining unit for inputting the position within the image specified by the user as a measurement image point from an inputting device; a corresponding point detecting unit for detecting a corresponding point corresponding to the measurement image point obtained by the measurement image point obtaining unit from the point cloud; and a position calculating unit for discriminating a three-dimensional position of the measurement image point obtained by the measurement image point obtaining unit using a three-dimensional position of the corresponding point detected by the corresponding point detecting unit.</p>
<p id="p-0016" num="0020">The above measurement apparatus further includes: an image memory unit for storing an image captured by a camera; and a three-dimensional point cloud model memory unit for storing a point cloud which is formed by a point cloud measured by a laser device and of which a three-dimensional position is known as a three-dimensional point cloud model, and the image displaying unit displays and superimposes the image stored in the image memory unit and the three-dimensional point cloud model stored in the three-dimensional point cloud model memory unit on the screen of the displaying device, and prompts the user to specify a point corresponding to the position within the image which the user watches from the point cloud of the three-dimensional point cloud model; the corresponding point detecting unit detects a corresponding point corresponding to the measurement image point obtained by the measurement image point obtaining unit from the point cloud of the three-dimensional point cloud model stored by the three-dimensional point cloud model memory unit; and the position calculating unit discriminates a three-dimensional position of the measurement image point obtained by the measurement image point obtaining unit using the three-dimensional position of the corresponding point detected by the corresponding point detecting unit.</p>
<p id="p-0017" num="0021">The above measurement apparatus further includes a feature region detecting unit for analyzing the image stored in the image memory unit and detecting an image region in which the feature for the measurement target is captured as a feature image region, and the image displaying unit prompts the user to specify a position of an image for the feature image region detected by the feature region detecting unit.</p>
<p id="p-0018" num="0022">The above corresponding point detecting unit, when a point of the point cloud displayed within the feature image region detected by the feature region detecting unit exists at the position in the image shown by the measurement image point, detects the point as the corresponding point corresponding to the measurement image point.</p>
<p id="p-0019" num="0023">The above corresponding point detecting unit, when a point of the point cloud displayed within the feature image region detected by the feature region detecting unit does not exist at the position in the image shown by the measurement image point, detects a point which is closest to the measurement image point as the corresponding point corresponding to the measurement image point.</p>
<p id="p-0020" num="0024">The above measurement apparatus further includes a result memory unit for assuming the three-dimensional position discriminated by the position calculating unit as a three-dimensional position of the feature for the measurement target, and storing the three-dimensional position by relating to a type of the feature for the measurement target.</p>
<p id="p-0021" num="0025">According to the present invention, a measurement apparatus includes: an image displaying unit for displaying and superimposing an image in which a feature is captured by a camera and a point cloud, which corresponds to the image and of which a three-dimensional position is known, on a screen of a displaying device, and for prompting a user to specify a position of a feature for a measurement target within the image; a measurement image point obtaining unit for inputting the position within the image specified by the user as a measurement image point from an inputting device; a vector calculating unit for calculating a vector showing direction from a center of the camera to the measurement image point inputted by the measurement image point obtaining unit; a corresponding point detecting unit for detecting a corresponding point corresponding to the measurement image point obtained by the measurement image point obtaining unit from the point cloud; a plane calculating unit for calculating a particular plane including the corresponding point detected by the corresponding point detecting unit; a position calculating unit for obtaining a three-dimensional position of the corresponding point detected by the corresponding point detecting unit as a first candidate showing a three-dimensional position of the measurement image point, and calculating an intersecting point of the particular plane calculated by the plane calculating unit and the vector calculated by the vector calculating unit as a second candidate showing the three-dimensional position of the measurement image point; a position displaying unit for displaying the first candidate and the second candidate obtained by the position calculating unit on the screen of the displaying device and prompting the user to specify one of the first candidate and the second candidate; and a result memory unit for storing one of the first candidate and the second candidate specified by the user as the three-dimensional position of the measurement image point.</p>
<p id="p-0022" num="0026">According to the present invention, a measurement apparatus includes: an image displaying unit for displaying and superimposing an image in which a feature is captured by a camera and a point cloud, which corresponds to the image and of which a three-dimensional position is known, on a screen of a displaying device, and for prompting a user to specify a position of a feature for a measurement target within the image; a measurement image point obtaining unit for inputting the position within the image specified by the user as a measurement image point from an inputting device; a vector calculating unit for calculating a vector showing direction from a center of the camera to the measurement image point inputted by the measurement image point obtaining unit; a corresponding point detecting unit for detecting a corresponding point corresponding to the measurement image point obtained by the measurement image point obtaining unit from the point cloud; a plane calculating unit for calculating a particular plane including the corresponding point detected by the corresponding point detecting unit; a type inputting unit for making the user specify a type of the feature for the measurement target and inputting the type of the feature specified by the user from an inputting device; and a position calculating unit for discriminating either of the corresponding point detected by the corresponding point detecting unit, and the intersecting point of the particular plane calculated by the plane calculating unit and the vector calculated by the vector calculating unit as a three-dimensional position of the measurement image point based on the type of the feature inputted by the type inputting unit.</p>
<p id="p-0023" num="0027">According to the present invention, a measuring method includes: by an image displaying unit, performing an image displaying process for displaying and superimposing an image in which a feature is captured and a point cloud, which corresponds to the image and of which a three-dimensional position is known, on a screen of a displaying device, and prompting a user to specify a position of a feature for a measurement target within the image; by a measurement image point obtaining unit, performing a measurement image point obtaining process for inputting the position within the image specified by the user as a measurement image point from an inputting device; by a corresponding point detecting unit, performing a corresponding point detecting process for detecting a corresponding point corresponding to the measurement image point obtained by the measurement image point obtaining unit from the point cloud; and by a position calculating unit, performing a position calculating process for discriminating a three-dimensional position of the measurement image point obtained by the measurement image point obtaining unit using a three-dimensional position of the corresponding point detected by the corresponding point detecting unit, and generating measurement position data showing the three-dimensional position of the measurement image point discriminated.</p>
<p id="p-0024" num="0028">According to the present invention, a measuring method includes: by an image displaying unit, performing an image displaying process for displaying and superimposing an image in which a feature is captured by a camera and a point cloud, which corresponds to the image and of which a three-dimensional position is known, on a screen of a displaying device, and for prompting a user to specify a position of a feature for a measurement target within the image; by a measurement image point obtaining unit, performing a measurement image point obtaining process for inputting the position within the image specified by the user as a measurement image point from an inputting device; by a vector calculating unit, performing a vector calculating process for calculating a vector showing direction from a center of the camera to the measurement image point inputted by the measurement image point obtaining unit; by a corresponding point detecting unit, performing a corresponding point detecting process for detecting a corresponding point corresponding to the measurement image point obtained by the measurement image point obtaining unit from the point cloud; by a plane calculating unit, performing a plane calculating process for calculating a particular plane including the corresponding point detected by the corresponding point detecting unit; by a position calculating unit, performing a position calculating process for obtaining a three-dimensional position of the corresponding point as a first candidate showing a three-dimensional position of the measurement image point, and for calculating an intersecting point of the particular plane calculated by the plane calculating unit and the vector calculated by the vector calculating unit as a second candidate showing the three-dimensional position of the measurement image point; by a position displaying unit, performing a position displaying process for displaying the first candidate and the second candidate obtained by the position calculating unit on the screen of a displaying device and prompting the user to specify one of the first candidate and the second candidate; and by a result memory unit, performing a result storing process for storing one of the first candidate and the second candidate specified by the user as measurement position data showing the three-dimensional position of the measurement image point.</p>
<p id="p-0025" num="0029">According to the present invention, a measuring method includes: by an image displaying unit, performing an image displaying process for displaying and superimposing an image in which a feature is captured by a camera and a point cloud, which corresponds to the image and of which a three-dimensional position is known, on a screen of a displaying device, and for prompting a user to specify a position of a feature for a measurement target within the image; by a measurement image point obtaining unit, performing a measurement image point obtaining process for inputting the position within the image specified by the user as a measurement image point from an inputting device; by a vector calculating unit, performing a vector calculating process for calculating a vector showing direction from a center of the camera to the measurement image point inputted by the measurement image point obtaining unit; by a corresponding point detecting unit, performing a corresponding point detecting process for detecting a corresponding point corresponding to the measurement image point obtained by the measurement image point obtaining unit from the point cloud; by a plane calculating unit, performing a plane calculating process for calculating a particular plane including the corresponding point detected by the corresponding point detecting unit; by a type inputting unit, performing a type inputting process for prompting the user to specify a type of the feature for the measurement target and for inputting the type of the feature specified by the user from the inputting device; by a position calculating unit, performing a position calculating process for discriminating either of the corresponding point detected by the corresponding point detecting unit, and an intersecting point of the particular plane calculated by the plane calculating unit and the vector calculated by the vector calculating unit as a three-dimensional position of the measurement image point based on the type of the feature inputted by the type inputting unit, and generating measurement position data showing the three-dimensional position of the measurement image point discriminated.</p>
<p id="p-0026" num="0030">According to the present invention, a measurement terminal device includes: an image displaying unit for displaying superimposing an image in which a feature is captured and a point cloud, which corresponds to the image and of which a three-dimensional position is known, on a screen of a displaying device, and for prompting a user to specify a position of a feature for a measurement target within the image; a terminal-side measurement image point obtaining unit for inputting the position within the image specified by the user as a measurement image point from an inputting device and sending the measurement image point inputted to a measurement server device calculating a three-dimensional position of the measurement image point; and a result memory unit for receiving the three-dimensional position of the measurement image point from the measurement server device and storing the three-dimensional position of the measurement image point received.</p>
<p id="p-0027" num="0031">According to the present invention, a measurement server device includes: a server-side measurement image point obtaining unit for receiving from a terminal device a position of a feature for a measurement target within an image in which the feature is captured as a measurement image point; a corresponding point detecting unit for detecting a corresponding point corresponding to the measurement image point obtained by the server-side measurement image point obtaining unit from a point cloud, which corresponds to the image and of which a three-dimensional position is known; and a position calculating unit for discriminating a three-dimensional position of the measurement image point obtained by the server-side measurement image point obtaining unit using a three-dimensional position of the corresponding point detected by the corresponding point detecting unit and sending the three-dimensional position of the measurement image point discriminated to the measurement terminal device.</p>
<p id="p-0028" num="0032">According to the present invention, a measurement terminal device includes: an image displaying unit for displaying an image in which a feature is captured and prompting a user to specify a position of a feature for a measurement target within the image; a terminal-side measurement image point obtaining unit for inputting the position within the image specified by the user as a measurement image point from an inputting device and sending the measurement image point inputted to a measurement server device calculating a three-dimensional position of the measurement image point; and a result memory unit for receiving from the measurement server the three-dimensional position of the measurement image point and storing the three-dimensional position of the measurement image point received.</p>
<p id="p-0029" num="0033">According to the present invention, a measurement apparatus includes: a three-dimensional point cloud model memory unit for storing a three-dimensional point cloud model including a point cloud each showing a three-dimensional position; an image displaying unit for displaying an image captured by a camera on a displaying device and prompting a user to specify a position within the image; a measurement image point obtaining unit for inputting the position within the image specified by the user as a measurement image point from an inputting device; a vector calculating unit for calculating a vector showing direction from a center of the camera to the measurement image point inputted by the measurement image point obtaining unit; a neighborhood extracting unit for extracting one neighboring point of the measurement image point from the point cloud of the three-dimensional point cloud model; a neighboring plane calculating unit for calculating a particular plane including the one neighboring point extracted by the neighborhood extracting unit; and a feature position calculating unit for calculating an intersecting point of the particular plane calculated by the neighboring plane calculating unit and the vector calculated by the vector calculating unit as a three-dimensional position of the measurement image point.</p>
<p id="p-0030" num="0034">The above measurement apparatus further includes: a model projecting unit for projecting the point cloud of the three-dimensional point cloud model on an image-capturing plane of the camera corresponding to the image, and the neighborhood extracting unit extracts one of a closest point from the measurement image point in the image-capturing plane among the point cloud of the three-dimensional point cloud model, a closest point from the measurement image point in direction of a horizontal axis of the image-capturing plane, and a closest point from the measurement image point in direction of a vertical axis of the image-capturing plane as the one neighboring point.</p>
<p id="p-0031" num="0035">The above neighboring plane calculating unit calculates a horizontal plane including the one neighboring point of the measurement image point as the particular plane.</p>
<p id="p-0032" num="0036">The above neighboring plane calculating unit calculates a plane including the one neighboring point of the measurement image point and orthogonal to one of an X axis, a Y axis, and a Z axis of an X-Y-Z coordinate system showing a coordinate system used for the three-dimensional point cloud model as the particular plane.</p>
<p id="p-0033" num="0037">The above measurement apparatus further includes: a type inputting unit for making the user specify a type of a feature which is a position measurement target and inputting the type of the feature specified by the user from the inputting device, and the neighboring plane calculating unit represents a plane formed by the feature represented by a point cloud including the one neighboring point of the measurement image point based on the type of the feature inputted by the type inputting unit and calculates the particular plane.</p>
<p id="p-0034" num="0038">According to the present invention, a measuring method includes: by an image displaying unit, performing an image displaying process for displaying an image captured by a camera on a displaying device and prompting a user to specify a position within the image; by a measurement image point obtaining unit, performing a measurement image point obtaining process for inputting the position within the image specified by the user as a measurement image point from an inputting device; by a vector calculating unit, performing a vector calculating process for calculating a vector showing direction from a center of the camera to the measurement image point inputted by the measurement image point obtaining unit; by a neighborhood extracting unit, performing a neighborhood extracting process for extracting one neighboring point of the measurement image point from a three-dimensional point cloud model memory unit storing a three-dimensional point cloud model including a point cloud each showing a three-dimensional position; by a neighboring plane calculating unit, performing a neighboring plane calculating process for calculating a particular plane including the one neighboring point extracted by the neighborhood extracting unit; and by a feature position calculating unit, performing a feature position calculating process for calculating an intersecting point of the particular plane calculated by the neighboring plane calculating unit and the vector calculated by the vector calculating unit as a three-dimensional position of the measurement image point, and generating measurement position data showing the three-dimensional position of the measurement image point calculated.</p>
<p id="p-0035" num="0039">According to the present invention, a measurement apparatus includes: an image memory unit for storing an image captured by a camera; a three-dimensional point cloud model memory unit for storing a three-dimensional point cloud model which is formed by a point cloud obtained by measuring an image-capturing place of the camera by a laser device and of which a position of each point cloud is known; an image displaying unit for displaying an image stored in the image memory unit on a screen of a displaying device and prompting a user to specify a position within the image; a measurement image point obtaining unit for inputting the position within the image specified by the user as a measurement image point from an inputting device; and a position calculating unit for detecting a corresponding point corresponding to the measurement image point obtained by the measurement image point obtaining unit from the point cloud of the three-dimensional point cloud model stored by the three-dimensional point cloud model memory unit, and discriminating a three-dimensional position of the measurement image point obtained by the measurement image point obtaining unit using a position of the corresponding point detected.</p>
<p id="p-0036" num="0040">The above image displaying unit displays a list of a plurality of images stored in the image memory unit on the screen of the displaying device, prompts the user to specify an image, displays the image specified by the user on the screen of the displaying device, and prompts the user to specify a position within the image.</p>
<p id="p-0037" num="0041">The above measurement apparatus further includes: a result displaying unit for displaying a three-dimensional position of the measurement image point discriminated by the position calculating unit on the screen of the displaying device on which the image displaying unit displays the image.</p>
<p id="p-0038" num="0042">The above measurement apparatus further includes: a type inputting unit for making the user specify a type of a feature which is a position measurement target and inputting the type of the feature specified by the user from the inputting device; and a result memory unit for storing the measurement image point obtained by the measurement image point obtaining unit, a three-dimensional position of the measurement image point discriminated by the position calculating unit, and the type of the feature inputted by the type inputting unit in a memory equipment by relating.</p>
<p id="p-0039" num="0043">According to the present invention, a measuring method, using: an image memory unit for storing images captured by a camera; and a three-dimensional point cloud model memory unit for storing a three-dimensional point cloud model which is formed by a point cloud obtained by measuring an image-capturing place of the camera by a laser device and of which a position of each point cloud is known, the method includes: by an image displaying unit, performing an image displaying process for displaying an image stored in the image memory unit on a screen of a displaying device and prompting a user to specify a position within the image; by a measurement image point obtaining unit, performing a measurement image point obtaining process for inputting the position within the image specified by the user as a measurement image point from an inputting device; and by a position calculating unit, performing a position calculating process for detecting a corresponding point corresponding to the measurement image point obtained by the measurement image point obtaining unit from a point cloud of the three-dimensional point cloud model stored in the three-dimensional point cloud model memory unit, discriminating a three-dimensional position of the measurement image point obtained by the measurement image point obtaining unit using a position of the corresponding point detected, and generating measurement position data showing the three-dimensional position of the measurement image point discriminated.</p>
<p id="p-0040" num="0044">According to the present invention, a measuring program has a computer execute the above measuring methods.</p>
<p id="p-0041" num="0045">According to the present invention, measurement position data is characterized to be generated by the above measuring method.</p>
<p id="p-0042" num="0046">According to the present invention, a plotting apparatus includes: an image memory unit for storing images captured by a camera; a three-dimensional point cloud model memory unit for storing a three-dimensional point cloud model which is formed by a point cloud obtained by measuring an image-capturing place of the camera by a laser device and of which a position of each point cloud is known, an image displaying unit for displaying an image stored in the image memory unit on a screen of a displaying device and prompting a user to specify a position within the image; a measurement image point obtaining unit for inputting the position within the image specified by the user as a measurement image point from an inputting device; a position calculating unit for detecting a corresponding point corresponding to the measurement image point obtained by the measurement image point obtaining unit from a point cloud of the three-dimensional point cloud model stored in the three-dimensional point cloud model memory unit, and discriminating a three-dimensional position of the measurement image point obtained by the measurement image point obtaining unit using a position of the corresponding point detected; a drawing unit for inputting a plotting command showing contents of a figure to be generated from an inputting equipment and drawing the figure including a plurality of elements on the screen of the displaying device based on the plotting command inputted; and a plotting unit for making the user specify one of the plurality of elements included in the figure drawn by the drawing unit, obtaining a three-dimensional position of the measurement image point corresponding to the element specified from the position calculating unit, and generating plotting data representing the figure drawn by the drawing unit and showing the three-dimensional position of the measurement image point discriminated by the position calculating unit as a three-dimensional position of the element specified by the user.</p>
<p id="p-0043" num="0047">According to the present invention, a plotting method using: an image memory unit for storing images captured by a camera; and a three-dimensional point cloud model memory unit for storing a three-dimensional point cloud model which is formed by a point cloud obtained by measuring an image-capturing place of the camera by a laser device and of which a position of each point cloud is known, the method includes: by an image displaying unit, performing an image displaying process for displaying an image stored in the image memory unit on a screen of a displaying device and prompting a user to specify a position within the image; by a measurement image point obtaining unit, performing a measurement image point obtaining process for inputting the position within the image specified by the user as a measurement image point from an inputting device; by a position calculating unit, performing a position calculating process for detecting a corresponding point corresponding to the measurement image point obtained by the measurement image point obtaining unit from a point cloud of the three-dimensional point cloud model stored in the three-dimensional point cloud model memory unit, and discriminating a three-dimensional position of the measurement image point obtained by the measurement image point obtaining unit using a position of the corresponding point detected; by a drawing unit, performing a drawing process for inputting a plotting command showing contents of a figure to be generated from an inputting equipment and drawing the figure including a plurality of elements on the screen of the displaying device based on the plotting command inputted; and by a plotting unit, performing a plotting process for making the user specify one of the plurality of elements included in the figure drawn by the drawing unit, obtaining a three-dimensional position of the measurement image point corresponding to the element specified from the position calculating unit, and generating plotting data representing the figure drawn by the drawing unit and showing the three-dimensional position of the measurement image point discriminated by the position calculating unit as a three-dimensional position of the element specified by the user.</p>
<p id="p-0044" num="0048">According to the present invention, a plotting program has a computer execute the above plotting method.</p>
<p id="p-0045" num="0049">According to the present invention, plotting data is characterized to be generated by the above plotting method.</p>
<p id="p-0046" num="0050">According to the present invention, a road feature measurement apparatus includes: a motion stereo unit for generating a three-dimensional model of a stationary body for a plurality of images captured by a camera mounted on a running vehicle at different times by a motion stereo process as a stationary body model; a moving body removing unit for removing a difference between road surface shape model which is a three-dimensional point cloud model generated based on distance and orientation data showing distance and orientation for a feature measured from the running vehicle and the stationary body model generated by the motion stereo unit from the road surface shape model, and generating a moving body removed model which is made by removing a moving body region from the road surface shape model; a feature identifying unit for determining a type of the stationary body represented by each point cloud based on a position and a shape shown by a point cloud of the moving body removed model generated by the moving body removing processing unit; a measurement image point obtaining unit for displaying at least one of the image, the moving body removed model, and the type of the stationary body determined by the feature identifying unit on a displaying device, and inputting information of a position on the image specified by the user as a target for position measurement as a measurement image point from an inputting device; a vector calculating unit for calculating a vector showing direction from a center of the camera to the measurement image point inputted by the measurement image point obtaining unit; a three neighboring points extracting unit for extracting three neighboring points of the measurement image point from a point cloud of the road surface shape model; and a feature position calculating unit for calculating a plane formed by the three neighboring points of the measurement image point extracted by the three neighboring points extracting unit, and calculating an intersecting point of the plane calculated and the vector calculated by the vector calculating unit as a position of the measurement image point.</p>
<p id="p-0047" num="0051">According to the present invention, a road feature measurement apparatus includes: a feature identifying unit for determining a type of a feature represented by each point cloud based on a position and a shape shown by a point cloud of road surface shape model which is a three-dimensional point cloud model generated based on distance and orientation data showing distance and orientation for the feature measured from a running vehicle; a measurement image point obtaining unit for displaying an image and the type of the feature determined by the feature identifying unit on a displaying device, and inputting information of a position on the image, specified by the user as a target for position measurement, as a measurement image point from an inputting device; a vector calculating unit for calculating a vector showing direction from a center of a camera to the measurement image point inputted by the measurement image point obtaining unit; a three neighboring points extracting unit for extracting three neighboring points of the measurement image point from a point cloud of the road surface shape model; and a feature position calculating unit for calculating a plane formed by the three neighboring points of the measurement image point extracted by the three neighboring points extracting unit, and calculating an intersecting point of the plane calculated and the vector calculated by the vector calculating unit as a position of the measurement image point.</p>
<p id="p-0048" num="0052">According to the present invention, a feature identification apparatus includes: a motion stereo unit for generating a three-dimensional model of a stationary body for a plurality of images captured by a camera mounted on a running vehicle at different times by a motion stereo process as a stationary body model; a moving body removing unit for removing a difference between road surface shape model which is a three-dimensional point cloud model generated based on distance and orientation data showing distance and orientation for a feature measured from the running vehicle and the stationary body model generated by the motion stereo unit from the road surface shape model, and generating a moving body removed model which is made by removing a moving body region from the road surface shape model; and a feature identifying unit for determining a type of the stationary body represented by each point cloud based on a position and a shape shown by a point cloud of the moving body removed model generated by the moving body removing processing unit.</p>
<p id="p-0049" num="0053">According to the present invention, a feature identification apparatus includes: a labeling processing unit for extracting a point cloud continuing from a position of a point cloud of a road surface shape model which is a three-dimensional point cloud model generated based on distance and orientation data showing distance and orientation for the feature measured from a running vehicle, and for grouping the point cloud of the road surface shape model; an edge determining unit for determining an edge part from a line segment formed by a point cloud for each group grouped by the labeling processing unit, and for grouping the group using the edge part as a border; and a feature identifying unit for determining a type of feature represented by a point cloud of each group based on a position and a shape shown by a point cloud for each group grouped by the edge determining unit.</p>
<p id="p-0050" num="0054">In the above road feature measurement apparatus, the three neighboring points extracting unit calculates a most neighboring point of the measurement image point, selects a second line segment which places the measurement image point inside between the second line segment and a first line segment including the most neighboring point among line segments formed by a point cloud of the road surface shape model, calculates a straight line connecting the measurement image point and the most neighboring point, calculates a second neighboring point which is closest to the straight line in a left side of the straight line among the point cloud forming the second line segment and a third neighboring point which is closest to the straight line in a right side of the straight line, and the most neighboring point, the second neighboring point, and the third neighboring point are assumed as the three neighboring points of the measurement image point.</p>
<p id="p-0051" num="0055">According to the present invention, a road feature measuring method includes: by a motion stereo unit, performing a motion stereo process for generating a three-dimensional model of a stationary body for a plurality of images captured by a camera mounted on a running vehicle at different times by a motion stereo process as a stationary body model; by a moving body removing unit, performing a moving body removing process for removing a difference between road surface shape model which is a three-dimensional point cloud model generated based on distance and orientation data showing distance and orientation for a feature measured from the running vehicle and the stationary body model generated by the motion stereo unit from the road surface shape model, and generating a moving body removed model which is made by removing a moving body region from the road surface shape model; by a feature identifying unit, performing a feature identifying process for determining a type of the stationary body represented by each point cloud based on a position and a shape shown by a point cloud of the moving body removed model generated by the moving body removing processing unit; by a measurement image point obtaining unit, performing a measurement image point obtaining process for displaying at least one of the image, the moving body removed model, and the type of the stationary body determined by the feature identifying unit on a displaying device, and inputting information of a position on the image specified by the user as a position measurement target as a measurement image point from an inputting device; by a vector calculating unit, performing a vector calculating process for calculating a vector showing direction from a center of the camera to the measurement image point inputted by the measurement image point obtaining unit; by a three neighboring points extracting unit, performing a three neighboring points extracting process for extracting three neighboring points of the measurement image point from a point cloud of the road surface shape model; and by a feature position calculating unit, performing a feature position calculating process for calculating a plane formed by the three neighboring points of the measurement image point extracted by the three neighboring points extracting unit, and calculating an intersecting point of the plane calculated and the vector calculated by the vector calculating unit as a position of the measurement image point.</p>
<p id="p-0052" num="0056">According to the present invention, a road feature measuring program has a computer execute the above road feature measuring method.</p>
<heading id="h-0015" level="1">Effect of the Invention</heading>
<p id="p-0053" num="0057">According to the present invention, for example, using MMS, it is possible to measure a position of a feature located on the road/side of the road other than a white line.</p>
<p id="p-0054" num="0058">Further, according to the present invention, in the road surface shape model represented by laser point cloud, by measuring the position based on the neighboring three points of the location point, it is possible to measure a position of a narrow feature such as a kilo post and a specular feature such as glass, which may not receive the laser beam in MMS that obtains the measurement data by the laser radar during running, with a high precision.</p>
<p id="p-0055" num="0059">Further, for example, the present invention enables to measure a position of the feature with a good precision regardless of existence/absence of a moving body on the road or the sidewalk, since when the target feature is hidden by a vehicle running on the road or a pedestrian on a sidewalk, etc., it is possible to remove only a moving body from the road surface shape model.</p>
<p id="p-0056" num="0060">Further, for example, the present invention enables to aid the user to specify the feature for the measurement target by displaying the three-dimensional model from which the moving body is removed or a type of the feature together with the image.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0016" level="1">PREFERRED EMBODIMENTS FOR CARRYING OUT THE INVENTION</heading>
<heading id="h-0017" level="1">Embodiment 1</heading>
<p id="p-0057" num="0061"><figref idref="DRAWINGS">FIG. 1</figref> shows a system configuration of a road feature measurement system <b>101</b> and a functional configuration of a road feature measurement apparatus <b>100</b> according to the first embodiment.</p>
<p id="p-0058" num="0062">The road feature measurement system <b>101</b> in the first embodiment includes an odometry apparatus <b>200</b>, three gyros <b>210</b> (a part of a positioning unit, a posture detecting unit, and a GPS gyro), three GPSs <b>220</b> (a part of the positioning unit, the posture detecting unit, and the GPS gyro), a camera <b>230</b> (an imaging unit), a laser radar <b>240</b> (an optical scanning unit, a laser scanner, and a LRF [Laser Range Finder]), and a road feature measurement apparatus <b>100</b> (a computer).</p>
<p id="p-0059" num="0063">The odometry apparatus <b>200</b>, the three gyros <b>210</b>, the three GPSs <b>220</b>, the camera <b>230</b>, and the laser radar <b>240</b> (examples of a measurement sensor, respectively) are mounted on a top board <b>103</b> (base) (refer to <figref idref="DRAWINGS">FIG. 4</figref>) of a measuring carriage <b>102</b> (a vehicle, hereinafter). Here, a positive direction of the Z axis of <figref idref="DRAWINGS">FIG. 5</figref> corresponds to the forward direction of the measuring carriage <b>102</b>. Further, the setting position of the laser radar <b>240</b> can be located ahead of the vehicle as well as the camera <b>230</b>.</p>
<p id="p-0060" num="0064">The odometry apparatus <b>200</b> performs the odometry method to calculate distance data showing the running distance of the vehicle.</p>
<p id="p-0061" num="0065">The three gyros <b>210</b> calculate angle velocity data showing tilting of the vehicle in the three axial directions (a pitch angle, a roll angle, and a yaw angle).</p>
<p id="p-0062" num="0066">The three GPSs <b>220</b> calculate positioning data showing the running position (coordinates) of the vehicle.</p>
<p id="p-0063" num="0067">The odometry apparatus <b>200</b>, the gyro <b>210</b>, and the GPS <b>220</b> measure the position and posture of the vehicle by the GPS/dead reckoning compound operation.</p>
<p id="p-0064" num="0068">The camera <b>230</b> captures images and outputs image data of time series.</p>
<p id="p-0065" num="0069">The laser radar <b>240</b> is provided ahead or back of the vehicle, with swinging an optical axis, irradiates laser obliquely downwardly, and calculates orientation/distance data showing distance to the road surface in each direction (LRF data, hereinafter).</p>
<p id="p-0066" num="0070">The road feature measurement apparatus <b>100</b> calculates the position of the feature specified by the user based on the distance data, the angle velocity data, the positioning data, the image data, and the orientation/distance data.</p>
<p id="p-0067" num="0071">The road feature measurement apparatus <b>100</b> includes a vehicle position and posture (3-axis) computing unit <b>110</b>, a camera position and posture computing unit <b>130</b>, a camera LOS computing unit <b>140</b>, a road surface shape model generating unit <b>150</b>, a laser radar position and posture computing unit <b>160</b>, a road surface model corresponding point searching unit <b>170</b>, a feature identification apparatus <b>300</b>, an observation data inputting unit <b>191</b>, and an observation data memory unit <b>199</b>.</p>
<p id="p-0068" num="0072">The vehicle position and posture (3-axis) computing unit <b>110</b> calculates the position and posture of the vehicle (vehicle position and posture) based on the distance data, the angle velocity data, and the positioning data.</p>
<p id="p-0069" num="0073">The feature identification apparatus <b>300</b> generates three-dimensional model of a stationary body based on the image data, and by comparing the three-dimensional model of the stationary body with a road surface shape model based on the LRF data, which will be described later, generates a road surface shape model of the stationary body. Further, the feature identification apparatus <b>300</b> classifies a laser measured point cloud which forms the road surface shape model into groups, and identifies a type of the feature shown by each group based on the shape which the laser measured point cloud forms. Further, the feature identification apparatus <b>300</b> displays the road surface shape model of the stationary body and the type of the feature superimposed with the image to provide to the user. Then, the feature identification apparatus <b>300</b> inputs the position of the feature specified by the user on the image as the measurement image point.</p>
<p id="p-0070" num="0074">The camera position and posture computing unit <b>130</b> calculates the position and the posture of the camera <b>230</b> (camera position and posture) based on the vehicle position and posture and a camera attachment offset. The camera attachment offset shows quantity of displacement formed by an axis of attachment of the camera <b>230</b> against a vehicle axis (orthogonal coordinate). The camera attachment offset is a value corresponding to the relation between the camera <b>230</b> and the top board <b>103</b> in <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0071" num="0075">The camera LOS computing unit <b>140</b> (an example of a vector calculating unit) calculates an angle (LOS vector) in LOS (Line Of Sight), which is a direction of sight from the camera to the measurement image point, based on the measurement image point specified by the user on the image and the camera position and posture.</p>
<p id="p-0072" num="0076">The laser radar position and posture computing unit <b>160</b> calculates the position and posture of the laser radar <b>240</b> (laser radar position and posture) based on the vehicle position and posture and a laser radar attachment offset. The laser radar attachment offset shows quantity of displacement formed by an axis of attachment of the laser radar <b>240</b> against a vehicle axis (orthogonal coordinates). The laser radar attachment offset is a value corresponding to the relation between the laser radar <b>240</b> and the top board <b>103</b> in <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0073" num="0077">The road surface shape model generating unit <b>150</b> generates a road surface shape model (a three-dimensional point cloud model) showing a shape (curve, slope, irregularity, etc.) of an uneven road on which the vehicle runs based on the orientation/distance data and the laser radar position and posture.</p>
<p id="p-0074" num="0078">The road surface model corresponding point searching unit <b>170</b> (an example of the feature position calculating unit) calculates the position of the feature specified by the user based on the LOS vector and the road surface shape model for the measurement image point. The road surface model corresponding point searching unit <b>170</b> can calculate the feature position with high precision by considering the curve, slope, irregularities, etc. of the road surface</p>
<p id="p-0075" num="0079">The distance data, the angle velocity data, the positioning data, the image data, and the orientation/distance data are called as observation data.</p>
<p id="p-0076" num="0080">The observation data inputting unit <b>191</b> inputs the observation data obtained by the measuring carriage <b>102</b> and stores in the observation data memory unit <b>199</b>.</p>
<p id="p-0077" num="0081">The observation data memory unit <b>199</b> stores the observation data obtained by the measuring carriage <b>102</b>, the laser radar attachment offset, the camera attachment offset, and various kinds of data generated based on the observation data. Each unit included in the road feature measurement apparatus <b>100</b> and each unit included in the feature identification apparatus <b>300</b> input data to be used from the observation data memory unit <b>199</b>, perform various kinds of processes, and store generated data in the observation data memory unit <b>199</b>.</p>
<p id="p-0078" num="0082">In the first embodiment, &#x201c;road&#x201d; means not only &#x201c;on the road&#x201d; but also &#x201c;side of the road&#x201d; and &#x201c;around the road&#x201d; within the image-capturing range. Further, &#x201c;side of the road&#x201d; and &#x201c;around the road&#x201d; includes a road shoulder, an edge stone, a sidewalk, etc.</p>
<p id="p-0079" num="0083"><figref idref="DRAWINGS">FIG. 2</figref> shows an example of hardware resource for the road feature measurement apparatus <b>100</b> and the feature identification apparatus <b>300</b> according to the first embodiment.</p>
<p id="p-0080" num="0084">In <figref idref="DRAWINGS">FIG. 2</figref>, the road feature measurement apparatus <b>100</b> and the feature identification apparatus <b>300</b> include a CPU <b>911</b> (Central Processing Unit; also called as a processing device, an operation device, a microprocessor, a microcomputer, and a processor) which executes programs. The CPU <b>911</b> is connected to a ROM <b>913</b>, a RAM <b>914</b>, a communication board <b>915</b>, a displaying device <b>901</b>, a keyboard <b>902</b>, a mouse <b>903</b>, an FDD <b>904</b> (Flexible Disk Drive), a CDD <b>905</b> (Compact Disk Drive), a printer device <b>906</b>, a scanner device <b>907</b>, a microphone <b>908</b>, a speaker <b>909</b>, and a magnetic disk drive <b>920</b> via a bus <b>912</b>, and controls these hardware devices. Instead of the magnetic disk drive <b>920</b>, a storage device such as an optical disk drive, a memory card reader/writer device, etc. can be used.</p>
<p id="p-0081" num="0085">The RAM <b>914</b> is an example of a volatile memory. Storage medium of the ROM <b>913</b>, the FDD <b>904</b>, the CDD <b>905</b>, and the magnetic disk drive <b>920</b> are examples of a nonvolatile memory. These are examples of memory equipment, a memory device, or a memory unit.</p>
<p id="p-0082" num="0086">The communication board <b>915</b>, the keyboard <b>902</b>, the scanner device <b>907</b>, the FDD <b>904</b>, etc. are examples of an inputting equipment, an inputting device, or an inputting unit.</p>
<p id="p-0083" num="0087">Further, the communication board <b>915</b>, the displaying device <b>901</b>, the printer device <b>906</b>, etc. are examples of an outputting equipment, an outputting device, or an outputting unit.</p>
<p id="p-0084" num="0088">The communication board <b>915</b> is connected wiredly or wirelessly to communication network such as LAN (Local Area Network), the Internet, WAN (Wide Area Network) such as ISDN, etc., telephone lines, and so on.</p>
<p id="p-0085" num="0089">An OS <b>921</b> (Operating System), a window system <b>922</b>, a group of programs <b>923</b>, a group of files <b>924</b> are stored in the magnetic disk drive <b>920</b>. Programs of the group of programs <b>923</b> are executed by the CPU <b>911</b>, the OS <b>921</b>, or the window system <b>922</b>.</p>
<p id="p-0086" num="0090">In the group of programs <b>923</b>, programs performing functions that will be explained in the embodiments as &#x201c;&#x2014;unit&#x201d; and &#x201c;&#x2014;means&#x201d; are stored. The programs are read and executed by the CPU <b>911</b>.</p>
<p id="p-0087" num="0091">In the group of files <b>924</b>, result data such as &#x201c;determined result of&#x2014;&#x201d;, &#x201c;calculated result of&#x2014;&#x201d;, &#x201c;processed result of&#x2014;&#x201d;, etc. when functions that will be explained in the embodiments as &#x201c;&#x2014;unit&#x201d; or &#x201c;&#x2014;means&#x201d; are performed, data to be received/transmitted between programs performing functions of &#x201c;&#x2014;unit&#x201d; or &#x201c;&#x2014;means&#x201d;, and other information, data, signal values, variable values, parameters are stored as each item of &#x201c;&#x2014;file&#x201d; or &#x201c;&#x2014;database&#x201d;. &#x201c;&#x2014;file&#x201d; or &#x201c;&#x2014;database&#x201d; is stored in the recording medium such as disks or memories. Information, data, signal values, variable values, and parameters stored in the storage medium such as disks or memories are read by the CPU <b>911</b> to a main memory or a cache memory via a reading/writing circuit, and used for operations of the CPU such as extraction, search, reference, comparison, computation, calculation, processing, output, print, display, etc. During the operations of the CPU such as extraction, search, reference, comparison, computation, calculation, processing, output, print, display, and extraction, information, data, signal values, variable values, and parameters are temporarily stored in the main memory, the cache memory, or a buffer memory.</p>
<p id="p-0088" num="0092">Further, arrows in flowcharts which will be explained in the embodiments mainly show inputting/outputting of data or signals, and the data or the signal values are recorded in the recording medium such as a memory of the RAM <b>914</b>, a flexible disk of the FDD <b>904</b>, a compact disk of the CDD <b>905</b>, a magnetic disk of the magnetic disk drive <b>920</b>, other optical disk, a mini disk, a DVD (digital Versatile Disc), etc. Further, the data or the signal values are transmitted online by transmission medium such as the bus <b>912</b>, signal lines, cables, and others.</p>
<p id="p-0089" num="0093">Further, &#x201c;&#x2014;unit&#x201d; or &#x201c;&#x2014;means&#x201d; which will be explained in the embodiments can be &#x201c;&#x2014;circuit&#x201d;, &#x201c;&#x2014;device&#x201d;, &#x201c;&#x2014;equipment&#x201d;, or &#x201c;means&#x201d;, and also can be &#x201c;&#x2014;step&#x201d;, &#x201c;&#x2014;procedure&#x201d;, and &#x201c;&#x2014;process&#x201d;. Namely, &#x201c;&#x2014;unit&#x201d; and &#x201c;&#x2014;means&#x201d; which will be explained can be implemented by firmware stored in the ROM <b>913</b>. Or it can be implemented by only software, only hardware such as elements, devices, boards, wirings, etc., or it can be also implemented by a combination of software and hardware, or further a combination with firmware. Firmware and software are stored as programs in the recording medium such as the magnetic disk, the flexible disk, the optical disk, the compact disk, the mini disk, the DVD, etc. The programs are read by the CPU <b>911</b> and executed by the CPU <b>911</b>. That is, the programs are to function the computer to perform &#x201c;&#x2014;unit&#x201d; and &#x201c;&#x2014;means&#x201d;. Or the programs are to have the computer perform a procedure or a method of &#x201c;&#x2014;unit&#x201d; and &#x201c;&#x2014;means&#x201d;.</p>
<p id="p-0090" num="0094">&#x201c;&#x2014;unit&#x201d; and &#x201c;&#x2014;apparatus&#x201d; which form the road feature measurement system <b>101</b> are executed by performing each process which will be explained later using the CPU.</p>
<p id="p-0091" num="0095"><figref idref="DRAWINGS">FIG. 3</figref> is a flowchart showing a flow of road feature position measuring process of the road feature measurement system <b>101</b> according to the first embodiment.</p>
<p id="p-0092" num="0096">The flow of road feature position measuring process of the road feature measurement system <b>101</b> according to the first embodiment will be explained in the following with reference to <figref idref="DRAWINGS">FIG. 3</figref>.</p>
<p id="p-0093" num="0097">&#x3c;S<b>101</b>: Measurement Running&#x3e;</p>
<p id="p-0094" num="0098">First, by running a road of which features are to be measured with a vehicle, the odometry apparatus <b>200</b>, the gyro <b>210</b>, and the GPS <b>220</b> respectively perform measurement during the running, and distance data, angle velocity data, and positioning data (GPS/IMU (Inertial Measurement Unit) data, hereinafter) are obtained in time series. Further, the camera <b>230</b> captures images during the running and obtains time series image data and time of image data showing an image-capturing time of each image. Further, the laser radar <b>240</b> irradiates laser during the running, with swinging in a transverse direction to the vehicle, and obtains distance/orientation data (LRF data) showing the distance and the orientation of the feature located on the road/side of the road (around the road) in time series.</p>
<p id="p-0095" num="0099">For example, the LRF data shows the distance/orientation to the feature in binary format, the image data shows RAW image in Bayer pattern, and the time of image data shows an identification number of the image and imaging time by relating in CSV (Comma Separated Values) format.</p>
<p id="p-0096" num="0100">&#x3c;S<b>102</b>: Observation Data Storing Process&#x3e;</p>
<p id="p-0097" num="0101">Next, in the road feature measurement apparatus <b>100</b>, the observation data inputting unit <b>191</b> inputs the GPS/IMU data (the distance data, the angle velocity data, and the positioning data) obtained by each measuring sensor of the vehicle, the image data, the time of image data, and the LRF data (orientation/distance data). Then, the observation data inputting unit <b>191</b> decodes the compressed data (the GPS/IMU data, the LRF data, for example) (S <b>102</b><i>a</i>: data decoding process), and further, copies particular data (the image data, the time of image data, for example) (S<b>102</b><i>b</i>: image data copying process) if necessary, and stores each data in the observation data memory unit <b>199</b>.</p>
<p id="p-0098" num="0102">For example, the LRF data is converted from the binary format to the text format by the data decoding process (S<b>102</b><i>a</i>).</p>
<p id="p-0099" num="0103">Further, for example, by the image data copying process (S<b>102</b><i>b</i>), the image data of 24-bit BMP (bitmap) format is generated.</p>
<p id="p-0100" num="0104">Further, the observation data inputting unit <b>191</b> stores the camera attachment offset to the top board <b>103</b> of the vehicle on which the camera <b>230</b> is mounted, and the laser radar attachment offset to the top board <b>103</b> of the vehicle on which the laser radar <b>240</b> is mounted in the observation data memory unit <b>199</b>.</p>
<p id="p-0101" num="0105">&#x3c;S<b>103</b>: Positioning/Compounding Process&#x3e;</p>
<p id="p-0102" num="0106">Next, in the road feature measurement apparatus <b>100</b>, the vehicle position and posture (3-axis) computing unit <b>110</b> calculates the position and posture of the vehicle in the ENU coordinate system based on the GPS/IMU data. Hereinafter, data showing the position and posture of the vehicle in time series in the ENU coordinate system is called as the vehicle position and posture data.</p>
<p id="p-0103" num="0107">For example, the vehicle position and posture data shows the ENU coordinate, an angle of rotation (roll), an angle of elevation (pitch), and an angle of orientation (yaw) of the vehicle in CSV format.</p>
<p id="p-0104" num="0108">&#x3c;S<b>104</b>: Digitizing Process&#x3e;</p>
<p id="p-0105" num="0109">Further, the feature identification apparatus <b>300</b> identifies the features captured on the image by classifying into a moving body (a vehicle, a pedestrian, for example) and a stationary body (street, sidewalk, wall, other (a kilo-post, a sign, for example)) based on the image data and the LRF data, and displays a type of each of the features captured in the image together with the image on the displaying device.</p>
<p id="p-0106" num="0110">Then, the feature identification apparatus <b>300</b> inputs a position of a position measurement target specified by the user on the image (a measurement image point, hereinafter), a type of a feature captured in the measurement image point (a feature type ID (Identifier), hereinafter), and an identification number of the image in which the measurement image point is specified (a specified image number, hereinafter) from the inputting equipment such as the keyboard <b>902</b>, the mouse <b>903</b>, the touch panel, etc.</p>
<p id="p-0107" num="0111">For example, the measurement image point shows a two-dimensional position (u,v) on the image specified by the user.</p>
<p id="p-0108" num="0112">A detail of the digitizing process (S<b>104</b>) will be discussed later.</p>
<p id="p-0109" num="0113">&#x3c;S<b>105</b>: 3D Modeling Process&#x3e;</p>
<p id="p-0110" num="0114">Next, in the road feature measurement apparatus <b>100</b>, the road surface shape model generating unit <b>150</b> generates three-dimensional road surface shape model which represents in the ENU coordinates each laser measured point of the LRF data corresponding to the image in which the measurement image point is specified based on the vehicle position and posture data, the LRF data, the time of image data, the specified image number, and the laser radar position and posture data.</p>
<p id="p-0111" num="0115">A detail of the 3D modeling process (S<b>105</b>) will be discussed later.</p>
<p id="p-0112" num="0116">&#x3c;S<b>106</b>: Feature Position Locating Process&#x3e;</p>
<p id="p-0113" num="0117">Next, in the road feature measurement apparatus <b>100</b>, the camera LOS computing unit <b>140</b> calculates a LOS vector from the center of the camera to the measurement image point in the ENU coordinate system based on the vehicle position and posture data, the image data, the time of image data, the specified image number, the camera position and posture data, and the measurement image point.</p>
<p id="p-0114" num="0118">Then, the road surface model corresponding point searching unit <b>170</b> extracts three neighboring points of the measurement image point out of the laser measured point cloud of the road surface shape model, and calculates an ENU coordinate of an intersecting point of the LOS vector to the measurement image point and a plane formed by the three neighboring points of the measurement image point as the position of the feature specified by the user.</p>
<p id="p-0115" num="0119">A detail of the feature position locating process (S<b>106</b>) will be discussed later.</p>
<p id="p-0116" num="0120">In the following, a detail of the digitizing process (S<b>104</b>), the 3D modeling process (S<b>105</b>), and the feature position locating process (S<b>106</b>) will be explained.</p>
<p id="p-0117" num="0121">First, the digitizing process (S<b>104</b>) will be explained, which removes data of the moving body from the LRF data (orientation/distance data to the feature), identifies a type of the remaining stationary body, and displays the type of each feature and the road surface shape model of the stationary body superimposed with the image on the displaying device.</p>
<p id="p-0118" num="0122">The feature identification apparatus <b>300</b> aids the user to specify the feature to be measured by digitizing process (S<b>104</b>).</p>
<p id="p-0119" num="0123"><figref idref="DRAWINGS">FIG. 11</figref> shows a configuration of the feature identification apparatus <b>300</b> according to the first embodiment.</p>
<p id="p-0120" num="0124">The functional configuration of the feature identification apparatus <b>300</b> performing the digitizing process (S<b>104</b>) will be explained in the following with reference to <figref idref="DRAWINGS">FIG. 11</figref>.</p>
<p id="p-0121" num="0125">The feature identification apparatus <b>300</b> includes a motion stereo unit <b>310</b>, a moving body removing unit <b>320</b>, a feature identifying unit <b>330</b>, and a measurement image point obtaining unit <b>340</b>.</p>
<p id="p-0122" num="0126">Further, it is assumed that the feature identification apparatus <b>300</b> can access the observation data memory unit <b>199</b> to obtain the observation data. However, the feature identification apparatus <b>300</b> can also include a memory unit corresponding to the observation data memory unit <b>199</b>.</p>
<p id="p-0123" num="0127">Further, it is assumed that the feature identification apparatus <b>300</b> can obtain the road surface shape model based on the LRF data from the road surface shape model generating unit <b>150</b>. However, the feature identification apparatus <b>300</b> can also include a processing unit corresponding to the road surface shape model generating unit <b>150</b>.</p>
<p id="p-0124" num="0128">The motion stereo unit <b>310</b> includes a stationary body discriminating unit <b>311</b> and a stationary body model generating unit <b>312</b>, and generates a three-dimensional model of the stationary body captured in the image based on the image data (a stationary body model, hereinafter).</p>
<p id="p-0125" num="0129">The stationary body discriminating unit <b>311</b> discriminates a part of the image in which the stationary body is captured.</p>
<p id="p-0126" num="0130">The stationary body model generating unit <b>312</b> generates the three-dimensional model of the stationary body captured in the image.</p>
<p id="p-0127" num="0131">The moving body removing unit <b>320</b> includes a moving body discriminating unit <b>321</b> and a moving body removed model generating unit <b>322</b>, and generates a road surface shape model by removing a laser measured point cloud for the moving body from the LRF data.</p>
<p id="p-0128" num="0132">The moving body discriminating unit <b>321</b> discriminates the laser measured point cloud for the moving body in the road surface shape model.</p>
<p id="p-0129" num="0133">The moving body removed model generating unit <b>322</b> removes the laser measured point cloud for the moving body and generates the road surface shape model.</p>
<p id="p-0130" num="0134">The feature identifying unit <b>330</b> includes a labeling unit <b>331</b>, an edge determining unit <b>332</b>, and a feature determining unit <b>333</b>, and discriminates a type of a feature located at each laser measured point shown by the road surface shape model.</p>
<p id="p-0131" num="0135">The labeling unit <b>331</b> classifies each laser measured point cloud of the road surface shape model into groups.</p>
<p id="p-0132" num="0136">The edge determining unit <b>332</b> discriminates an edge part being a border for segmentalizing the laser measured point cloud.</p>
<p id="p-0133" num="0137">The feature determining unit <b>333</b> discriminates a type of the feature for each group of the laser measured points. The type of the feature is identified as, for example, &#x201c;road&#x201d; for the group in which the user's vehicle is running, and &#x201c;outside of road&#x201d; for the group next to it.</p>
<p id="p-0134" num="0138">The measurement image point obtaining unit <b>340</b> includes an image displaying unit <b>341</b> and an image point inputting unit <b>342</b>, and obtains the measurement image point showing the position on the image specified by the user.</p>
<p id="p-0135" num="0139">The image displaying unit <b>341</b> displays the road surface shape model and the type of the feature captured in the image superimposed with the image on the displaying device.</p>
<p id="p-0136" num="0140">The image point inputting unit <b>342</b> inputs the measurement image point showing the position on the image specified by the user from the inputting device.</p>
<p id="p-0137" num="0141"><figref idref="DRAWINGS">FIG. 12</figref> is a flowchart showing a flow of the digitizing process (S<b>104</b>) of the feature identification apparatus <b>300</b> according to the first embodiment.</p>
<p id="p-0138" num="0142">The flow of the digitizing process (S<b>104</b>) performed by the feature identification apparatus <b>300</b> according to the first embodiment will be explained with reference to <figref idref="DRAWINGS">FIG. 12</figref>. Here, a detail of each of processes, which form the digitizing process (S<b>104</b>) that will be explained below, will be discussed later separately.</p>
<p id="p-0139" num="0143">&#x3c;S<b>201</b>: Motion Stereo Process&#x3e;</p>
<p id="p-0140" num="0144">First, the motion stereo unit <b>310</b> discriminates the part of the image in which the stationary body is captured by stereo view of a plurality of images of the road ahead of the vehicle captured by a single camera from the running vehicle (S<b>201</b><i>a</i>: stationary body discriminating process), and generates a three-dimensional model of the stationary body captured in the image (a stationary body model, hereinafter) by projecting the discriminated part of the image on the ENU coordinate system (S<b>201</b><i>b</i>: stationary body model generating process).</p>
<p id="p-0141" num="0145">&#x3c;S<b>202</b>: Moving Body Removing Process&#x3e;</p>
<p id="p-0142" num="0146">Next, the moving body removing unit <b>320</b> compares the stationary body model based on the image generated by the motion stereo unit <b>310</b> and the road surface shape model based on the LRF data generated by the road surface shape model generating unit <b>150</b>, discriminates the laser measured point cloud for the moving body (S<b>202</b><i>a</i>: moving body discriminating process), removes the laser measured point cloud for the moving body, and generates the stationary body model (S<b>202</b><i>b</i>: moving body removed model generating process).</p>
<p id="p-0143" num="0147">&#x3c;S<b>203</b>: Feature Identifying Process&#x3e;</p>
<p id="p-0144" num="0148">Next, the feature identifying unit <b>330</b> classifies the laser measured point cloud shown by the road surface shape model generated by the moving body removing unit <b>320</b>, from which the moving body is removed into groups (S<b>203</b><i>a</i>: labeling process), discriminates the edge part of a line segment represented by the laser measured point cloud (S<b>203</b><i>b</i>: edge determining process), segmentalizes the laser measured point cloud into groups having the edge as a border, and discriminates a type of the feature located at each of the laser measured point for each group (S<b>203</b><i>c</i>: feature determining process).</p>
<p id="p-0145" num="0149">&#x3c;S<b>204</b>: Measurement Image Point Obtaining Process&#x3e;</p>
<p id="p-0146" num="0150">Next, the measurement image point obtaining unit <b>340</b> projects the road surface shape model generated by the moving body removing unit <b>320</b>, from which the moving body is removed, on the image-capturing plane of the camera <b>230</b>, and displays the road surface shape model, from which the moving body is removed, and the type of feature identified by the feature identifying unit <b>330</b> superimposed with the image on the displaying device (S<b>204</b><i>a</i>: image displaying process).</p>
<p id="p-0147" num="0151">Then, the measurement image point obtaining unit <b>340</b> inputs the position on the image specified by the user (the measurement image point), the type of feature captured at the measurement image point (feature type ID), and the identification number of the image in which the measurement image point is specified (specified image number) from the inputting equipment such as the keyboard <b>902</b>, the mouse <b>903</b>, the touch panel, etc. (S<b>204</b><i>b</i>: image point inputting process).</p>
<p id="p-0148" num="0152">Here, a detail of the motion stereo process (S<b>201</b>) performed by the motion stereo unit <b>310</b> will be explained in the following.</p>
<p id="p-0149" num="0153">Three-dimensional data based on the LRF data (orientation/distance data) obtained by the LRF (the laser radar <b>240</b>) shows the road surface shape model with high density and high precision, and the road feature measurement apparatus <b>100</b> measures the position of feature using this road surface shape model with high precision. However, since there normally exist many moving bodies such as a pedestrian, an oncoming vehicle, etc., the road surface shape model includes many laser measured points of the moving body which hides the stationary body. Therefore, when the stationary body is desired to be a target for location survey, the existence of the moving body which hides the stationary body causes erroneous extraction of the laser measured point used for the measurement and decrease of the precision of measured result.</p>
<p id="p-0150" num="0154"><figref idref="DRAWINGS">FIG. 13</figref> shows a road surface shape model when a truck does not hide a pole.</p>
<p id="p-0151" num="0155"><figref idref="DRAWINGS">FIG. 14</figref> shows a road surface shape model when the truck hides the pole.</p>
<p id="p-0152" num="0156"><figref idref="DRAWINGS">FIGS. 13 and 14</figref> are the LRF data of the same truck and pole from different viewpoints projected in three-dimensional models. In <figref idref="DRAWINGS">FIG. 13</figref>, it is possible to determine the laser measured point cloud of the pole since the truck and the pole are captured separately; however, in <figref idref="DRAWINGS">FIG. 14</figref>, the point cloud of the pole is hidden by the truck. Therefore, when the position of the pole is measured under the status of <figref idref="DRAWINGS">FIG. 14</figref>, it is impossible to correctly select the point cloud of the pole from the road surface shape model, which may generate a large measurement error.</p>
<p id="p-0153" num="0157">Here, the feature identification apparatus <b>300</b>, in the motion stereo process (S<b>201</b>), generates a three-dimensional model of the stationary body by the motion stereo method using a plurality of time series of images captured by the single camera <b>230</b>. Then, the feature identification apparatus <b>300</b>, in the moving body removing process (S<b>202</b>), extracts and removes the laser measured point cloud of the moving body region from the road surface shape model based on the LRF data by comparing with the stationary model based on the image.</p>
<p id="p-0154" num="0158">Motion stereo method using a plurality of time series of images captured by the single camera <b>230</b> is an operating principle based on the assumption that the movement of the camera <b>230</b> is known and the image-capturing target remain stationary. Because of this, the feature identification apparatus <b>300</b> can generate the three-dimensional model only representing the stationary body based on the image by using the motion stereo method.</p>
<p id="p-0155" num="0159">Then, it is possible to remove only the laser measured point cloud for the moving body from the laser measured point cloud for the road surface shape model based on the LRF data using the stationary body model obtained by the motion stereo method.</p>
<p id="p-0156" num="0160">Shingo Ando et al., &#x201c;A Study of Autonomous Mobile System in Outdoor Environment&#x201d; (Part 37 Improvement of Range Estimation Accuracy by Baseline Optimization in Motion Stereo Using GPS/INS/ODV), Robotics and Mechatronics Conference (Kobe), 2005 is a document related to an algorithm of the motion stereo method.</p>
<p id="p-0157" num="0161"><figref idref="DRAWINGS">FIG. 15</figref> is a flowchart showing a flow of the motion stereo process (S<b>201</b>).</p>
<p id="p-0158" num="0162">The motion stereo process (S<b>201</b>) performed by the motion stereo unit <b>310</b> in the feature identification apparatus <b>300</b> will be explained in the following with reference to <figref idref="DRAWINGS">FIG. 15</figref>.</p>
<p id="p-0159" num="0163">From a first epipolar line calculating process (S<b>301</b>) through a bi-directional matching process (S<b>303</b>) correspond to the stationary body discriminating process (S<b>201</b><i>a</i>) for discriminating the stationary body captured in the image, and a distance image three-dimensional reconstructing process (S<b>304</b>) through a voxel deleting process of volume intersection (S<b>306</b>) correspond to the stationary body model generating process (S<b>201</b><i>b</i>) for generating a stationary body model based on the image.</p>
<p id="p-0160" num="0164">&#x3c;S<b>301</b>: First Epipolar Line Calculating Process&#x3e;</p>
<p id="p-0161" num="0165">First, the stationary body discriminating unit <b>311</b> calculates an epipolar line for an arbitrary point on the image based on the camera position and posture by the camera position and posture computing unit <b>130</b>.</p>
<p id="p-0162" num="0166"><figref idref="DRAWINGS">FIG. 16</figref> shows a calculating method of an epipolar line L<b>1</b> according to the first embodiment.</p>
<p id="p-0163" num="0167">The calculating method of the epipolar line L<b>1</b> used by the stationary body discriminating unit <b>311</b> will be explained in the following with reference to <figref idref="DRAWINGS">FIG. 16</figref>.</p>
<p id="p-0164" num="0168">First, the stationary body discriminating unit <b>311</b> inputs an image A captured at a time T<b>1</b> and an image B captured at a time T<b>2</b> from the observation data memory unit <b>199</b> (S<b>301</b><i>a</i>).</p>
<p id="p-0165" num="0169">Next, the stationary body discriminating unit <b>311</b> sets a three-dimensional space to form a triangular pyramid by a center of the camera at the time of image-capturing and an image showing an image-capturing plane being away from the center of the camera with a focal distance (image plane) for each image (S<b>301</b><i>b</i>).</p>
<p id="p-0166" num="0170">Next, the stationary body discriminating unit <b>311</b> calculates an epipolar plane D<b>1</b> as a plane including a plane d<b>1</b> formed by the center C<b>1</b> of the camera of the image plane A, a feature point P<b>1</b> on the image plane A and the center C<b>2</b> of the camera of the image plane B (S<b>301</b><i>c</i>).</p>
<p id="p-0167" num="0171">The epipolar plane D<b>1</b> is represented by the following expression 20.</p>
<p id="p-0168" num="0172">Here, it is assumed that the coordinates of the center of the camera C<b>1</b> are (E1, N1, U1), the coordinates of the feature point P<b>1</b> of the set three-dimensional space (three-dimensional real space coordinates, hereinafter) are (Ep1, Np1, Up1), and the coordinates of the center of the camera C<b>2</b> are (E2, N2, U2).</p>
<p id="p-0169" num="0173">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mo>&#xf603;</mo>
          <mtable>
            <mtr>
              <mtd>
                <mi>x</mi>
              </mtd>
              <mtd>
                <mi>y</mi>
              </mtd>
              <mtd>
                <mi>z</mi>
              </mtd>
              <mtd>
                <mn>1</mn>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>E</mi>
                  <mn>2</mn>
                </msub>
              </mtd>
              <mtd>
                <msub>
                  <mi>N</mi>
                  <mn>2</mn>
                </msub>
              </mtd>
              <mtd>
                <msub>
                  <mi>U</mi>
                  <mn>2</mn>
                </msub>
              </mtd>
              <mtd>
                <mn>1</mn>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>E</mi>
                  <mrow>
                    <mi>p</mi>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.3em" height="0.3ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mn>1</mn>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <msub>
                  <mi>N</mi>
                  <mrow>
                    <mi>p</mi>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.3em" height="0.3ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mn>1</mn>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <msub>
                  <mi>U</mi>
                  <mrow>
                    <mi>p</mi>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.3em" height="0.3ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mn>1</mn>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <mn>1</mn>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>E</mi>
                  <mn>1</mn>
                </msub>
              </mtd>
              <mtd>
                <msub>
                  <mi>N</mi>
                  <mn>1</mn>
                </msub>
              </mtd>
              <mtd>
                <msub>
                  <mi>U</mi>
                  <mn>1</mn>
                </msub>
              </mtd>
              <mtd>
                <mn>1</mn>
              </mtd>
            </mtr>
          </mtable>
          <mo>&#xf604;</mo>
        </mrow>
        <mo>=</mo>
        <mn>0</mn>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>expression</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>20</mn>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0170" num="0174">Then, the stationary body discriminating unit <b>311</b> calculates the epipolar line L<b>1</b> as a line including an intersecting line of the epipolar plane D<b>1</b> and the image plane B (S<b>302</b><i>d</i>).</p>
<p id="p-0171" num="0175">Here, in <figref idref="DRAWINGS">FIG. 16</figref>, when a measurement target point Pout which is captured on the image plane A as the feature point P<b>1</b> is stationary, the measurement target point Pout always exists on the epipolar plane D<b>1</b>, and a corresponding point P<b>2</b> which is the image-capturing point on the image plane B of the measurement target point Pout exists on the epipolar line L<b>1</b>.</p>
<p id="p-0172" num="0176"><figref idref="DRAWINGS">FIG. 17</figref> shows that the corresponding point P<b>2</b> of a measurement target point PT<b>2</b> after moving does not exist on the epipolar line L<b>1</b>.</p>
<p id="p-0173" num="0177">On the other hand, if both the camera and the measurement target point P move, as shown in <figref idref="DRAWINGS">FIG. 17</figref>, the corresponding point P<b>2</b> corresponding to the measurement target point PT<b>2</b> on the image plane B after moving (time T<b>2</b>) does not exist on the epipolar line L<b>1</b> which is set based on the feature point P<b>1</b> showing the measurement target point PT<b>1</b> on the image plane A before moving (time T<b>1</b>). As shown in <figref idref="DRAWINGS">FIG. 17</figref>, if the measurement target point P does not move, the corresponding point P<b>1</b> corresponding to the measurement target point PT<b>1</b> on the image plane B exists on the epipolar line L<b>1</b>.</p>
<p id="p-0174" num="0178">Namely, since the moving body is not on the epipolar line, if the corresponding point exists on the epipolar line, the measurement target point shows the stationary body, so that it is possible to extract only the stationary body by searching the corresponding point on the epipolar line.</p>
<p id="p-0175" num="0179">The first corresponding point searching process (S<b>302</b>) will be explained in <figref idref="DRAWINGS">FIG. 15</figref>.</p>
<p id="p-0176" num="0180">&#x3c;S<b>302</b>: First Corresponding Point Searching Process&#x3e;</p>
<p id="p-0177" num="0181">The stationary body discriminating unit <b>311</b> searches on the epipolar line to find the corresponding point P<b>2</b>, corresponding to the feature point P<b>1</b> on the image plane A, on the image plane B as will be discussed in the following.</p>
<p id="p-0178" num="0182">First, the stationary body discriminating unit <b>311</b> compares color information of pixels of the feature point P<b>1</b> on the image A with color information of each pixel on the epipolar line L<b>1</b> on the image B (S<b>302</b><i>a</i>).</p>
<p id="p-0179" num="0183">Then, the stationary body discriminating unit <b>311</b> discriminates the pixel, of which color information corresponds to the pixel of the feature point P<b>1</b>, on the epipolar line L<b>1</b> as a corresponding point P<b>2</b> (S<b>302</b><i>b</i>).</p>
<p id="p-0180" num="0184">At this time, the stationary body discriminating unit <b>311</b> discriminates the corresponding point P<b>2</b> of which a differential value or RGB value of color edge is matched with or similar to (a difference of values is within a particular range) the feature point P<b>1</b>. Further, the stationary body discriminating unit <b>311</b> discriminates the corresponding point P<b>2</b> by a window matching.</p>
<p id="p-0181" num="0185">&#x3c;S<b>303</b>: Bi-directional Matching Process&#x3e;</p>
<p id="p-0182" num="0186">Next, the stationary body discriminating unit <b>311</b> sets an epipolar line L<b>2</b> on the image plane A based on the corresponding point P<b>2</b> on the image plane B, discriminates the corresponding point P<b>3</b> on the epipolar line L<b>2</b> corresponding to the corresponding point P<b>2</b> on the image plane B, and discriminates the corresponding point P<b>2</b> based on the positional relation between the feature point P<b>1</b> and the corresponding point P<b>3</b> on the image plane A.</p>
<p id="p-0183" num="0187">The stationary body discriminating unit <b>311</b>, not only one directional matching which only performs search for the corresponding point P<b>2</b> on the image plane B corresponding to the feature point P<b>1</b> on the image plane A, but also performs bi-directional matching for determining properness of the corresponding point P<b>2</b> by searching for the corresponding point P<b>3</b> on the image plane A corresponding to the corresponding point P<b>2</b> searched on the image plane B and comparing the feature point P<b>1</b> and the corresponding point P<b>3</b>, which tries to improve search precision of the corresponding point P<b>2</b>. Here, Jun-ichi Takiguchi, &#x201c;High-Precision Range Estimation from an Omnidirectional Stereo System&#x201d; Transactions of the Japan Society of Mechanical Engineers (C), Vol. 69, No. 680 (2003-4) is a document related to the bi-directional matching process.</p>
<p id="p-0184" num="0188">Then, the stationary body discriminating unit <b>311</b>, as well as the first epipolar line calculating process (S<b>301</b>), calculates an epipolar plane D<b>2</b> as a plane including a plane d<b>2</b> formed by the center of the camera C<b>2</b> on the image plane B, the corresponding point P<b>2</b> on the image plane B, and the center of the camera C<b>1</b> on the image plane A, and calculates the epipolar line L<b>2</b> as a line including an intersecting line of the epipolar plane D<b>2</b> and the image plane A (S<b>303</b><i>a</i>).</p>
<p id="p-0185" num="0189">Next, the stationary body discriminating unit <b>311</b>, as well as the first corresponding point searching process (S<b>302</b>), compares the color information of pixels of the corresponding point P<b>2</b> on the image plane B and the color information of each pixel on the epipolar line L<b>2</b> in the image A, and discriminates a pixel, corresponding to color information of the corresponding point P<b>2</b>, on the epipolar line L<b>2</b> as the corresponding point P<b>3</b> (S<b>303</b><i>b</i>).</p>
<p id="p-0186" num="0190">Then, the stationary body discriminating unit <b>311</b> compares the position of the feature point P<b>1</b> on the image plane A and the position of the corresponding point P<b>3</b>, decides the corresponding point P<b>2</b> as a correct corresponding point for the feature point P<b>1</b> if a distance between the feature point P<b>1</b> and the corresponding point P<b>3</b> is within a predetermined range, and deletes the corresponding point P<b>2</b> if the distance between the feature point P<b>1</b> and the corresponding point P<b>3</b> is not within the predetermined range (S<b>303</b><i>c</i>).</p>
<p id="p-0187" num="0191">In <figref idref="DRAWINGS">FIG. 15</figref>, the image three-dimensional reconstructing process (S<b>304</b>) will be explained.</p>
<p id="p-0188" num="0192">&#x3c;S<b>304</b>: Image Three-Dimensional Reconstructing Process&#x3e;</p>
<p id="p-0189" num="0193">Next, the stationary body model generating unit <b>312</b> generates a three-dimensional model of the stationary body captured in the image A by calculating three-dimensional coordinates of the measurement target point Pout captured in the image A as the feature point P<b>1</b> in the following manner.</p>
<p id="p-0190" num="0194">First, the stationary body model generating unit <b>312</b> calculates a LOS vector V<b>1</b> showing a direction from the center of the camera C<b>1</b> to the feature point P<b>1</b> on the image plane A and a LOS vector V<b>2</b> showing a direction from the center of the camera C<b>2</b> to the corresponding point P<b>2</b> on the image plane B (S<b>304</b><i>a</i>).</p>
<p id="p-0191" num="0195">Then, the stationary body model generating unit <b>312</b> calculates three-dimensional coordinates shown by an intersecting point of the LOS vector V<b>1</b> and the LOS vector V<b>2</b> as the three-dimensional coordinates of the measurement target point Pout (S<b>304</b><i>b</i>).</p>
<p id="p-0192" num="0196">Further, the stationary body model generating unit <b>312</b> sets the color information of the pixels of the feature point P<b>1</b> on the image plane A as color information of the measurement target point Pout (S<b>304</b><i>c</i>).</p>
<p id="p-0193" num="0197">The three-dimensional coordinates of the measurement target point Pout calculated here is coordinate values in a three-dimensional real space which is set so that the center of the camera C<b>1</b> and the image plane A should form a triangular pyramid.</p>
<p id="p-0194" num="0198">Here, the first epipolar line calculating process (S<b>301</b>) through the image three-dimensional reconstructing process (S<b>304</b>) are performed for each combination of images by using each pixel as the feature point.</p>
<p id="p-0195" num="0199">Namely, by the above process, for all pixels of all images, it is determined to be the stationary body or the moving body, and for the pixels showing the stationary body, the three-dimensional coordinates and the color information are decided. Further, for all images, the stationary body model represented by the point cloud data showing the three-dimensional coordinates and the color information is generated. This stationary body model is a model on the three-dimensional real space.</p>
<p id="p-0196" num="0200">In <figref idref="DRAWINGS">FIG. 15</figref>, a voxel space voting process (S<b>305</b>) will be explained.</p>
<p id="p-0197" num="0201">&#x3c;S<b>305</b>: Voxel Space Voting Process&#x3e;</p>
<p id="p-0198" num="0202">Next, the stationary body model generating unit <b>312</b> votes the stationary body model to the voxel space, and deletes the part of the moving body which has been erroneously extracted.</p>
<p id="p-0199" num="0203">First, the stationary body model generating unit <b>312</b> sets a voxel space composed of plural voxels on the three-dimensional real space (S<b>305</b><i>a</i>).</p>
<p id="p-0200" num="0204">Next, the stationary body model generating unit <b>312</b> discriminates a voxel in which each point that forms the stationary body model is located based on the three-dimensional coordinates for each point, and votes to the voxel discriminated (S<b>305</b><i>b</i>).</p>
<p id="p-0201" num="0205">Next, the stationary body model generating unit <b>312</b> calculates point density of each voxel, and deletes a voxel of which the calculated point density is smaller than the threshold value from the voxel space (S<b>305</b><i>c</i>).</p>
<p id="p-0202" num="0206"><figref idref="DRAWINGS">FIG. 18</figref> is an image drawing of the voxel space voting process (S<b>305</b>) according to the first embodiment.</p>
<p id="p-0203" num="0207"><figref idref="DRAWINGS">FIG. 18</figref> shows (1) a preset voxel space, (2) the voxel space after voting the stationary body model, and (3) the voxel space after processing which is composed of only high-density voxels after removing low-density voxels.</p>
<p id="p-0204" num="0208">There may be a case, in which the stationary body model generated by the image three-dimensional reconstructing process (S<b>304</b>) includes the corresponding point for the moving body which has been erroneously discriminated (error point, hereinafter) because of influence by the color precision of the image, properness of the threshold value of the color information used for determination of the corresponding point in the first corresponding point searching process (S<b>302</b>), and properness of the threshold value of the distance used for determination of the corresponding point in the bi-directional matching process (S<b>303</b>).</p>
<p id="p-0205" num="0209">Then, the stationary body model generating unit <b>312</b> removes the error point from the stationary body model by the voxel space voting process (S<b>305</b>). At the time of voxel space voting, since the point density of the voxel where the moving body is located is reduced, such voxel is removed.</p>
<p id="p-0206" num="0210">In <figref idref="DRAWINGS">FIG. 15</figref>, voxel deleting process of volume intersection (S<b>306</b>) will be explained.</p>
<p id="p-0207" num="0211">&#x3c;S<b>306</b>: Voxel Deleting Process of Volume Intersection&#x3e;</p>
<p id="p-0208" num="0212">The stationary body model generating unit <b>312</b> deletes the error part from the stationary body model by the volume intersection method.</p>
<p id="p-0209" num="0213">In the above process, the stationary body model obtained by the stereo view of the image includes errors because of influence by mismatching in the stereo computation process or occlusion. Although a model error because of minute mismatching of the corresponding point is reduced by deleting the low-density voxel in the voxel space voting process (S<b>305</b>), there may a case, in which the errors cannot be deleted by the voxel space voting process (S<b>305</b>) such as when error points are concentrated and density in the voxel becomes high.</p>
<p id="p-0210" num="0214">Then, the stationary body model generating unit <b>312</b> deletes the erroneous part in the voxel space by applying the volume intersection method to the voxel space obtained by the voxel space voting process (S<b>305</b>) as follows:</p>
<p id="p-0211" num="0215">First, for each voxel which forms the voxel space obtained by the voxel space voting process (S<b>305</b>), the stationary body model generating unit <b>312</b> projects the corresponding voxel on plural image planes (S<b>306</b><i>a</i>).</p>
<p id="p-0212" num="0216">Next, the stationary body model generating unit <b>312</b> compares color information of the voxel with color information of a pixel to which the corresponding voxel is projected (S<b>306</b><i>b</i>).</p>
<p id="p-0213" num="0217">Here, the color information of the voxel means the color information of points of the stationary body model included in the voxel.</p>
<p id="p-0214" num="0218">Then, the stationary body model generating unit <b>312</b> deletes the corresponding voxel from the voxel space if the color information does not match (S<b>306</b><i>c</i>).</p>
<p id="p-0215" num="0219">Namely, the volume intersection method is a method, when each point of the three-dimensional space is projected on each image plane, to keep points projected inside of all silhouette as points within a range of the target, and remove the other points as points being outside of the range of the target.</p>
<p id="p-0216" num="0220"><figref idref="DRAWINGS">FIG. 19</figref> shows the volume intersection method used in the first embodiment.</p>
<p id="p-0217" num="0221">In <figref idref="DRAWINGS">FIG. 19</figref>, there exists a set of voxels (voxel space) representing a stationary body model.</p>
<p id="p-0218" num="0222">In the volume intersection method, the following processing will be done:</p>
<p id="h-0018" num="0000">(1) when a voxel A is projected on an image A, obtain color information of a pixel A;</p>
<p id="h-0019" num="0000">(2) compare color information of the voxel A with the color information of the pixel A, if the color information are matched or similar, assume there exists the voxel A;</p>
<p id="h-0020" num="0000">(3) further, when the voxel A is projected on another image B having a different view point from the image A, obtain color information of a pixel B; and</p>
<p id="h-0021" num="0000">(4) compare the color information of the voxel A with the color information of the pixel B, if the color information are dissimilar, delete the voxel A as not existing on the real environment.</p>
<p id="p-0219" num="0223">The above processes are performed among a plurality of images in time series.</p>
<p id="p-0220" num="0224">Here, in <figref idref="DRAWINGS">FIG. 12</figref>, the moving body removing process (S<b>202</b>) performed by the moving body removing unit <b>320</b> will be explained.</p>
<p id="p-0221" num="0225">As discussed above, in the three-dimensional model obtained by the image data using the motion stereo method, only the stationary body is three-dimensionalized.</p>
<p id="p-0222" num="0226">Then, the moving body discriminating unit <b>321</b> inputs the three-dimensional model obtained based on the image data (the stationary body model) and the three-dimensional model obtained based on the LRF data (the road surface shape model), compares, and extracts the difference, so that the difference region is extracted from the road surface shape model as a region of the moving body (S<b>202</b><i>a</i>: the moving body discriminating process).</p>
<p id="p-0223" num="0227">Then, the moving body removed model generating unit <b>322</b> generates the road surface shape model representing the stationary body by removing the region extracted by the moving body discriminating unit <b>321</b> from the road surface shape model.</p>
<p id="p-0224" num="0228">In the moving body discriminating process (S<b>202</b><i>a</i>), the difference of the three-dimensional models can be compared for each point which forms each three-dimensional model, or in order to reduce the computation time, the difference can be compared by a unit of voxel with voting each point which forms each three-dimensional model.</p>
<p id="p-0225" num="0229">Next, in <figref idref="DRAWINGS">FIG. 12</figref>, the feature identifying process (S<b>203</b>) performed by the feature identifying unit <b>330</b> will be explained.</p>
<p id="p-0226" num="0230">In the road feature measurement apparatus <b>100</b>, three neighboring points of an arbitrary point (a measurement image point) specified on the image by the user are selected from the laser measured point cloud of the road surface shape model, and a coordinate of an intersecting point of a plane formed by the three points and a LOS vector for the measurement image point is outputted as the position measured result.</p>
<p id="p-0227" num="0231"><figref idref="DRAWINGS">FIG. 20</figref> is an image showing a place where a feature specified by the user is easily misrecognized.</p>
<p id="p-0228" num="0232">For example, when the user specifies a measurement image point for the image such as <figref idref="DRAWINGS">FIG. 20</figref>, the road feature measurement apparatus <b>100</b> sometimes selects three points of the road surface shape model for a point different from the measurement image point which the user intends.</p>
<p id="p-0229" num="0233">For example, in <figref idref="DRAWINGS">FIG. 20</figref>, when the user specifies a pole part which is backward of a guardrail, there is possibility that the road feature measurement apparatus <b>100</b> erroneously selects the laser measured point cloud of the guardrail part.</p>
<p id="p-0230" num="0234">Then, the feature identifying unit <b>330</b> identifies a type of each feature shown by the road surface shape model in order to show the user. By this operation, the user can specify correctly the feature part which the user desires to measure as the measurement image point, and the road feature measurement apparatus <b>100</b> can select correct three neighboring points from the road surface shape model based on the measurement image point specified correctly, and can output the measured result with high precision.</p>
<p id="p-0231" num="0235">Further, the road feature measurement apparatus <b>100</b> can output the measured result with high precision when a point of the road surface is specified as the measurement image point. Therefore, by showing a type of the feature, it is possible to make the user determine whether a point which the user intends to specify is on the road surface or above the road surface, which enables the road feature measurement apparatus <b>100</b> to output the measured result with high precision.</p>
<p id="p-0232" num="0236"><figref idref="DRAWINGS">FIG. 21</figref> shows the feature identifying process (S<b>203</b>) according to the first embodiment.</p>
<p id="p-0233" num="0237">The feature identifying process (S<b>203</b>) performed by the feature identifying unit <b>330</b> will be explained in the following with reference to <figref idref="DRAWINGS">FIG. 21</figref>.</p>
<p id="p-0234" num="0238"><figref idref="DRAWINGS">FIG. 21</figref> shows an example of the road surface shape model based on the LRF data obtained by swinging the laser radar <b>240</b> once in the transverse direction from the vehicle. This corresponds to a cross section of the road when the road is cut vertically to the direction of running. In this example, the first group A is a street where the user's car is running, and an edge position is a road shoulder which is a border of a road and a sidewalk; the first group B is the sidewalk; the first group C is a wall surface; and the second group is a pole, etc.</p>
<p id="p-0235" num="0239">In the feature identifying unit <b>330</b>, a labeling unit <b>331</b> classifies the laser measured point cloud shown by the road surface shape model which is generated by the moving body removing unit <b>320</b> with removing the moving body (a moving body removed model, hereinafter) into groups (S<b>203</b><i>a</i>: labeling process).</p>
<p id="p-0236" num="0240">At this time, the labeling unit <b>331</b> assumes that a laser measured point having three-dimensional coordinates of the position of the running vehicle and a laser measured point cloud of which three-dimensional coordinate values are continuous from the laser measured point are the first group and the laser measured point cloud other than the above is the second group.</p>
<p id="p-0237" num="0241">Next, an edge determining unit <b>332</b> discriminates a part of which a change of angle in the line represented by the laser measured point cloud of the first group exceeds the threshold value as the edge part (S<b>203</b><i>b</i>: an edge determining process).</p>
<p id="p-0238" num="0242">Then, the feature determining unit <b>333</b> segmentalizes the laser measured point cloud of the first group into plural groups using the edge part as a border, and decides a type of the feature located at each laser measured point for each group (S<b>203</b><i>c</i>: a feature determining process).</p>
<p id="p-0239" num="0243">At this time, the feature determining unit <b>333</b> classifies the laser measured point cloud of the first group into the first group A corresponding to the position of the running vehicle, the first group B located at a height from the first group A within a specific range and being continuous in the vertical and horizontal directions, and the first group C being continuous in the vertical direction.</p>
<p id="p-0240" num="0244">Further, the feature determining unit <b>333</b> identifies the first group A as &#x201c;street&#x201d;, the first group B as &#x201c;sidewalk&#x201d;, the first group C as &#x201c;wall surface&#x201d;, and the second group as &#x201c;others (a pole, a kilo-post, a sign, etc.)&#x201d;.</p>
<p id="p-0241" num="0245">As for another form of the digitizing process (S<b>104</b>) shown in <figref idref="DRAWINGS">FIG. 12</figref> which has been explained above, the feature identification apparatus <b>300</b> can be formed not to remove the moving body from the road surface shape model. Namely, in the digitizing process (S<b>104</b>), the motion stereo process (S<b>201</b>) and the moving body removing process (S<b>202</b>) can remain unperformed.</p>
<p id="p-0242" num="0246">In this case, the feature identifying unit <b>330</b> performs the feature identifying process (S<b>203</b>) with, for example, an algorithm shown in <figref idref="DRAWINGS">FIG. 22</figref>.</p>
<p id="p-0243" num="0247">Further, in this case, the feature identifying unit <b>330</b> includes a processing unit corresponding to the road surface shape model generating unit <b>150</b>. Here, the processing unit corresponding to the road surface shape model generating unit <b>150</b> is a 3D model generating unit <b>334</b> (illustration omitted).</p>
<p id="p-0244" num="0248"><figref idref="DRAWINGS">FIG. 22</figref> is a flowchart showing a flow of the feature identifying process (S<b>203</b>) according to the first embodiment.</p>
<p id="p-0245" num="0249">In <figref idref="DRAWINGS">FIG. 22</figref>, the labeling unit <b>331</b> inputs the LRF data from the observation data memory unit <b>199</b>, performs labeling for two-dimensional distance information obtained from the distance/orientation shown by the LRF data, and classifies the LRF data into the first group showing a part continuous to the street and the second group representing other features (S<b>203</b><i>a</i>: a labeling process).</p>
<p id="p-0246" num="0250">At this time, the labeling unit <b>331</b> classifies the LRF data into the first group of the point cloud showing the part continuous to the street and the second group of others using the fact that the laser measured point located directly below the vehicle is always the street. Here, the laser measured point located directly below the vehicle is distance data of which the orientation is 90[deg] corresponding to the direction of running of the vehicle.</p>
<p id="p-0247" num="0251">Next, the edge determining unit <b>332</b> extracts the edge from the first group which is a point cloud showing the part continuous to the street (S<b>203</b><i>b</i>: an edge determining process).</p>
<p id="p-0248" num="0252">Next, the feature determining unit <b>333</b> classifies the first group into a sidewalk, a street, and a wall surface based on the edge (S<b>203</b><i>c</i>: a feature determining process).</p>
<p id="p-0249" num="0253">Then, a 3D model generating unit <b>334</b> inputs the vehicle position and posture data and the laser radar position and posture data from the observation data memory unit <b>199</b>, performs the three-dimensional Affine transformation of the LRF data based on the position and posture of the vehicle and the position and posture of the laser radar <b>240</b>, and generates the road surface shape model represented by the three-dimensional point cloud (S<b>203</b><i>d</i>: 3D model generating process).</p>
<p id="p-0250" num="0254">Next, in <figref idref="DRAWINGS">FIG. 12</figref>, the measurement image point obtaining process (S<b>204</b>) performed by the measurement image point obtaining unit <b>340</b> will be explained.</p>
<p id="p-0251" num="0255">The measurement image point obtaining unit <b>340</b> notifies the user of the types including the street surface, the sidewalk surface, the wall surface, and other features identified by the feature identifying unit <b>330</b> (S<b>204</b><i>a</i>: an image displaying process), and inputs the measurement image point specified by the user (<b>204</b><i>b</i>: an image point inputting process).</p>
<p id="p-0252" num="0256">As a concrete notification method for notifying the user of types of the features, there is a method to display an attribute including &#x201c;street&#x201d;, &#x201c;sidewalk&#x201d;, &#x201c;wall surface&#x201d;, &#x201c;other feature&#x201d; for the selected region specified by the user.</p>
<p id="p-0253" num="0257">Or, the moving body removed model generated by the moving body removing unit <b>320</b> or the road surface shape model is projected on the image-capturing plane of the camera <b>230</b> and the moving body removed model (or the road surface shape model), and the type of feature can be superimposed with the image and displayed on the displaying device. Further, either of the moving body removed model (or the road surface shape model) and the type of feature can be superimposed with the image and displayed. Further, the display is not limited to superimposing, but can be also done by arranging the information vertically/horizontally. Since the type of feature corresponds to the laser measured point cloud of the moving body removed model (or the road surface shape model), the type of feature can be superimposed with the image by displaying at the position corresponding to the moving body removed model (or the road surface shape model). Further, for the moving body region removed, at least one of the road surface shape model and the type of feature (the moving body) can be superimposed with the image.</p>
<p id="p-0254" num="0258">Yet further, the moving body removed model can be separately colored and displayed according to the type of feature.</p>
<p id="p-0255" num="0259">A method for projecting the moving body removed model on the image-capturing plane of the camera <b>230</b> and superimposing the moving body removed model with the image to display is the same as the processing method in the 3D modeling process (S<b>105</b>), which will be discussed later.</p>
<p id="p-0256" num="0260">Here, the 3D modeling process (S<b>105</b>) in <figref idref="DRAWINGS">FIG. 3</figref> will be explained in the following with reference to <figref idref="DRAWINGS">FIG. 23</figref>.</p>
<p id="p-0257" num="0261"><figref idref="DRAWINGS">FIG. 23</figref> is a flowchart showing a flow of the 3D modeling process (S<b>105</b>) according to the first embodiment.</p>
<p id="p-0258" num="0262">&#x3c;S<b>401</b>: Vehicle-Laser Synchronous Data Generating Process&#x3e;</p>
<p id="p-0259" num="0263">First, the road surface shape model generating unit <b>150</b> generates vehicle-laser synchronous data by synchronizing the vehicle position and posture data and the LRF data (orientation/distance data).</p>
<p id="p-0260" num="0264">&#x3c;S<b>402</b>: Laser-Camera Synchronous Data Generating Process&#x3e;</p>
<p id="p-0261" num="0265">Further, the road surface shape model generating unit <b>150</b> generates laser-camera synchronous data by synchronizing the LRF data and the time of image data.</p>
<p id="p-0262" num="0266">&#x3c;S<b>403</b>: Corresponding LRF Data Extracting Process&#x3e;</p>
<p id="p-0263" num="0267">Next, the road surface shape model generating unit <b>150</b> extracts the LRF data which is synchronized with the specified image from the laser-camera synchronous data (the corresponding LRF data, hereinafter) based on the specified image number which identifies the image of which the measurement image point is specified by the user (the specified image, hereinafter).</p>
<p id="p-0264" num="0268">&#x3c;S<b>404</b>: Three-Dimensional Affine Transformation Process&#x3e;</p>
<p id="p-0265" num="0269">Then, the road surface shape model generating unit <b>150</b> extracts the position and posture of the vehicle which is synchronized with the corresponding LRF data from the vehicle/laser synchronous data, and performs three-dimensional Affine transformation of the corresponding LRF data based on the position and posture of the vehicle and the position and posture of the laser radar. By the three-dimensional Affine transformation, the road surface shape model generating unit <b>150</b> generates the road surface shape model representing with the ENU coordinates the laser measured point cloud corresponding to the image in which the measurement image point is specified.</p>
<p id="p-0266" num="0270">Here, a detail of the three-dimensional Affine transformation process (S<b>404</b>) will be explained.</p>
<p id="p-0267" num="0271">When the three-dimensional coordinates of the laser measured point obtained from the LRF data is (x0, y0, z0), the laser measured point cloud of the LRF data is converted into the three-dimensional coordinates (x2, y2, z2) for the position of the vehicle with the following equations 1 and 2. Further, <figref idref="DRAWINGS">FIGS. 4 and 5</figref> show positional relation between the top board <b>103</b> mounted on the vehicle, the LRF (the laser radar <b>240</b>), and the camera <b>230</b>.</p>
<p id="p-0268" num="0272">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mo>[</mo>
        <mrow>
          <mi>Expression</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>2</mn>
        </mrow>
        <mo>]</mo>
      </mrow>
    </mtd>
    <mtd>
      <mstyle>
        <mspace width="0.3em" height="0.3ex"/>
      </mstyle>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mstyle>
          <mspace width="0.3em" height="0.3ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mrow>
          <mrow>
            <mo>[</mo>
            <mtable>
              <mtr>
                <mtd>
                  <msub>
                    <mi>x</mi>
                    <mn>2</mn>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>y</mi>
                    <mn>2</mn>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>z</mi>
                    <mn>2</mn>
                  </msub>
                </mtd>
              </mtr>
            </mtable>
            <mo>]</mo>
          </mrow>
          <mo>=</mo>
          <mrow>
            <mo>[</mo>
            <mtable>
              <mtr>
                <mtd>
                  <mrow>
                    <msub>
                      <mi>x</mi>
                      <mn>1</mn>
                    </msub>
                    <mo>+</mo>
                    <mrow>
                      <mi>&#x394;</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>x</mi>
                        <mi>lrf</mi>
                      </msub>
                    </mrow>
                  </mrow>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mrow>
                    <msub>
                      <mi>y</mi>
                      <mn>1</mn>
                    </msub>
                    <mo>+</mo>
                    <mrow>
                      <mi>&#x394;</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>y</mi>
                        <mi>lrf</mi>
                      </msub>
                    </mrow>
                  </mrow>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mrow>
                    <msub>
                      <mi>z</mi>
                      <mn>1</mn>
                    </msub>
                    <mo>+</mo>
                    <mrow>
                      <mi>&#x394;</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>z</mi>
                        <mi>lrf</mi>
                      </msub>
                    </mrow>
                  </mrow>
                </mtd>
              </mtr>
            </mtable>
            <mo>]</mo>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>equation</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>1</mn>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0269" num="0273">However,</p>
<p id="p-0270" num="0274">
<maths id="MATH-US-00003" num="00003">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mo>[</mo>
          <mtable>
            <mtr>
              <mtd>
                <msub>
                  <mi>x</mi>
                  <mn>1</mn>
                </msub>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>y</mi>
                  <mn>1</mn>
                </msub>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>z</mi>
                  <mn>1</mn>
                </msub>
              </mtd>
            </mtr>
          </mtable>
          <mo>]</mo>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mrow>
            <mrow>
              <mo>[</mo>
              <mtable>
                <mtr>
                  <mtd>
                    <mrow>
                      <mi>cos</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3c8;</mi>
                        <mi>lrf</mi>
                      </msub>
                    </mrow>
                  </mtd>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mi>sin</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3c8;</mi>
                        <mi>lrf</mi>
                      </msub>
                    </mrow>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mn>1</mn>
                  </mtd>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mrow>
                      <mrow>
                        <mo>-</mo>
                        <mi>sin</mi>
                      </mrow>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3c8;</mi>
                        <mi>lrf</mi>
                      </msub>
                    </mrow>
                  </mtd>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mi>cos</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3c8;</mi>
                        <mi>lrf</mi>
                      </msub>
                    </mrow>
                  </mtd>
                </mtr>
              </mtable>
              <mo>]</mo>
            </mrow>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>[</mo>
              <mtable>
                <mtr>
                  <mtd>
                    <mn>1</mn>
                  </mtd>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mi>cos</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3b8;</mi>
                        <mi>lrf</mi>
                      </msub>
                    </mrow>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mrow>
                        <mo>-</mo>
                        <mi>sin</mi>
                      </mrow>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3b8;</mi>
                        <mi>lrf</mi>
                      </msub>
                    </mrow>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mi>sin</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3b8;</mi>
                        <mi>lrf</mi>
                      </msub>
                    </mrow>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mi>cos</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3b8;</mi>
                        <mi>lrf</mi>
                      </msub>
                    </mrow>
                  </mtd>
                </mtr>
              </mtable>
              <mo>]</mo>
            </mrow>
          </mrow>
          <mo>&#x2062;</mo>
          <mrow>
            <mo>&#x2003;</mo>
            <mrow>
              <mrow>
                <mo>[</mo>
                <mtable>
                  <mtr>
                    <mtd>
                      <mrow>
                        <mi>cos</mi>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <msub>
                          <mi>&#x3d5;</mi>
                          <mi>lrf</mi>
                        </msub>
                      </mrow>
                    </mtd>
                    <mtd>
                      <mrow>
                        <mrow>
                          <mo>-</mo>
                          <mi>sin</mi>
                        </mrow>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <msub>
                          <mi>&#x3d5;</mi>
                          <mi>lrf</mi>
                        </msub>
                      </mrow>
                    </mtd>
                    <mtd>
                      <mn>0</mn>
                    </mtd>
                  </mtr>
                  <mtr>
                    <mtd>
                      <mrow>
                        <mi>sin</mi>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <msub>
                          <mi>&#x3d5;</mi>
                          <mi>lrf</mi>
                        </msub>
                      </mrow>
                    </mtd>
                    <mtd>
                      <mrow>
                        <mi>cos</mi>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <msub>
                          <mi>&#x3d5;</mi>
                          <mi>lrf</mi>
                        </msub>
                      </mrow>
                    </mtd>
                    <mtd>
                      <mn>0</mn>
                    </mtd>
                  </mtr>
                  <mtr>
                    <mtd>
                      <mn>0</mn>
                    </mtd>
                    <mtd>
                      <mn>0</mn>
                    </mtd>
                    <mtd>
                      <mn>1</mn>
                    </mtd>
                  </mtr>
                </mtable>
                <mo>]</mo>
              </mrow>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>[</mo>
                <mtable>
                  <mtr>
                    <mtd>
                      <msub>
                        <mi>x</mi>
                        <mn>0</mn>
                      </msub>
                    </mtd>
                  </mtr>
                  <mtr>
                    <mtd>
                      <msub>
                        <mi>y</mi>
                        <mn>0</mn>
                      </msub>
                    </mtd>
                  </mtr>
                  <mtr>
                    <mtd>
                      <msub>
                        <mi>z</mi>
                        <mn>0</mn>
                      </msub>
                    </mtd>
                  </mtr>
                </mtable>
                <mo>]</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>equation</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>2</mn>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<ul id="ul0002" list-style="none">
    <li id="ul0002-0001" num="0000">
    <ul id="ul0003" list-style="none">
        <li id="ul0003-0001" num="0275">where &#x394;xlrf: distance in transverse direction from the position of vehicle; &#x394;ylrf: distance in height direction from the position of vehicle; &#x394;zlrf: distance in depth direction from the position of vehicle; &#x3c6;lrf: an attachment roll angle to the top plate; &#x3b8;lrf: an attachment pitch angle to the top plate; and &#x3c8;lrf: an attachment yaw angle to the top plate.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0271" num="0276">&#x201c;z0&#x201d; is a height of the laser scanning plane of the laser radar <b>240</b>, and is 0 here, since the reference of the laser radar <b>240</b> is set on the scanning plane.</p>
<p id="p-0272" num="0277">Next, with considering the position and posture of the vehicle, the road surface shape model of the ENU coordinate system by transformation using the equations 3 and 4. Here, points on the road surface shape model are assumed to be (Nlrf, Ulrf, Elrf).</p>
<p id="p-0273" num="0278">
<maths id="MATH-US-00004" num="00004">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mo>[</mo>
        <mrow>
          <mi>Expression</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>3</mn>
        </mrow>
        <mo>]</mo>
      </mrow>
    </mtd>
    <mtd>
      <mstyle>
        <mspace width="0.3em" height="0.3ex"/>
      </mstyle>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mo>[</mo>
          <mtable>
            <mtr>
              <mtd>
                <msub>
                  <mi>E</mi>
                  <mi>lrf</mi>
                </msub>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>U</mi>
                  <mi>lrf</mi>
                </msub>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>N</mi>
                  <mi>lrf</mi>
                </msub>
              </mtd>
            </mtr>
          </mtable>
          <mo>]</mo>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mo>[</mo>
          <mtable>
            <mtr>
              <mtd>
                <mrow>
                  <msub>
                    <mi>x</mi>
                    <mi>lrf</mi>
                  </msub>
                  <mo>+</mo>
                  <msub>
                    <mi>E</mi>
                    <mi>lrf</mi>
                  </msub>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mrow>
                  <msub>
                    <mi>y</mi>
                    <mi>lrf</mi>
                  </msub>
                  <mo>+</mo>
                  <msub>
                    <mi>U</mi>
                    <mi>lrf</mi>
                  </msub>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mrow>
                  <msub>
                    <mi>z</mi>
                    <mi>lrf</mi>
                  </msub>
                  <mo>+</mo>
                  <msub>
                    <mi>N</mi>
                    <mi>lrf</mi>
                  </msub>
                </mrow>
              </mtd>
            </mtr>
          </mtable>
          <mo>]</mo>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>equation</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>3</mn>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0274" num="0279">However,</p>
<p id="p-0275" num="0280">
<maths id="MATH-US-00005" num="00005">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mo>[</mo>
          <mtable>
            <mtr>
              <mtd>
                <msub>
                  <mi>x</mi>
                  <mi>lrf</mi>
                </msub>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>y</mi>
                  <mi>lrf</mi>
                </msub>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>z</mi>
                  <mi>lrf</mi>
                </msub>
              </mtd>
            </mtr>
          </mtable>
          <mo>]</mo>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mrow>
            <mrow>
              <mo>[</mo>
              <mtable>
                <mtr>
                  <mtd>
                    <mrow>
                      <mi>cos</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3c8;</mi>
                        <mi>v</mi>
                      </msub>
                    </mrow>
                  </mtd>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mi>sin</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3c8;</mi>
                        <mi>v</mi>
                      </msub>
                    </mrow>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mn>1</mn>
                  </mtd>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mrow>
                      <mrow>
                        <mo>-</mo>
                        <mi>sin</mi>
                      </mrow>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3c8;</mi>
                        <mi>v</mi>
                      </msub>
                    </mrow>
                  </mtd>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mi>cos</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3c8;</mi>
                        <mi>v</mi>
                      </msub>
                    </mrow>
                  </mtd>
                </mtr>
              </mtable>
              <mo>]</mo>
            </mrow>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>[</mo>
              <mtable>
                <mtr>
                  <mtd>
                    <mn>1</mn>
                  </mtd>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mi>cos</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3b8;</mi>
                        <mi>v</mi>
                      </msub>
                    </mrow>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mrow>
                        <mo>-</mo>
                        <mi>sin</mi>
                      </mrow>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3b8;</mi>
                        <mi>v</mi>
                      </msub>
                    </mrow>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mi>sin</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3b8;</mi>
                        <mi>v</mi>
                      </msub>
                    </mrow>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mi>cos</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3b8;</mi>
                        <mi>v</mi>
                      </msub>
                    </mrow>
                  </mtd>
                </mtr>
              </mtable>
              <mo>]</mo>
            </mrow>
          </mrow>
          <mo>&#x2062;</mo>
          <mrow>
            <mo>&#x2003;</mo>
            <mrow>
              <mrow>
                <mo>[</mo>
                <mtable>
                  <mtr>
                    <mtd>
                      <mrow>
                        <mi>cos</mi>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <msub>
                          <mi>&#x3d5;</mi>
                          <mi>v</mi>
                        </msub>
                      </mrow>
                    </mtd>
                    <mtd>
                      <mrow>
                        <mrow>
                          <mo>-</mo>
                          <mi>sin</mi>
                        </mrow>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <msub>
                          <mi>&#x3d5;</mi>
                          <mi>v</mi>
                        </msub>
                      </mrow>
                    </mtd>
                    <mtd>
                      <mn>0</mn>
                    </mtd>
                  </mtr>
                  <mtr>
                    <mtd>
                      <mrow>
                        <mi>sin</mi>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <msub>
                          <mi>&#x3d5;</mi>
                          <mi>v</mi>
                        </msub>
                      </mrow>
                    </mtd>
                    <mtd>
                      <mrow>
                        <mi>cos</mi>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <msub>
                          <mi>&#x3d5;</mi>
                          <mi>v</mi>
                        </msub>
                      </mrow>
                    </mtd>
                    <mtd>
                      <mn>0</mn>
                    </mtd>
                  </mtr>
                  <mtr>
                    <mtd>
                      <mn>0</mn>
                    </mtd>
                    <mtd>
                      <mn>0</mn>
                    </mtd>
                    <mtd>
                      <mn>1</mn>
                    </mtd>
                  </mtr>
                </mtable>
                <mo>]</mo>
              </mrow>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>[</mo>
                <mtable>
                  <mtr>
                    <mtd>
                      <msub>
                        <mi>x</mi>
                        <mn>2</mn>
                      </msub>
                    </mtd>
                  </mtr>
                  <mtr>
                    <mtd>
                      <msub>
                        <mi>y</mi>
                        <mn>2</mn>
                      </msub>
                    </mtd>
                  </mtr>
                  <mtr>
                    <mtd>
                      <msub>
                        <mi>z</mi>
                        <mn>2</mn>
                      </msub>
                    </mtd>
                  </mtr>
                </mtable>
                <mo>]</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>equation</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>4</mn>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<ul id="ul0004" list-style="none">
    <li id="ul0004-0001" num="0000">
    <ul id="ul0005" list-style="none">
        <li id="ul0005-0001" num="0281">where Ev: east direction from the position of vehicle; Nv: north direction from the position of vehicle; Uv: height direction from the position of vehicle; (&#x3c6;v: a roll angle of the vehicle; &#x3b8;v: a pitch angle of the vehicle; and &#x3c8;v: a yaw angle of the vehicle; and the coordinate system is a right-handed coordinate system where y is an upper direction.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0276" num="0282"><figref idref="DRAWINGS">FIGS. 6</figref>, <b>7</b>, and <b>8</b> show the road surface shape model represented by a point cloudpoint cloud of the ENU coordinates generated by the above.</p>
<p id="p-0277" num="0283">For example, in <figref idref="DRAWINGS">FIG. 7</figref>, a border of the sidewalk located in the left side (a step part) and the road in the right side can be observed in the three-dimensional point cloud data (road shoulder) of the road surface shape model.</p>
<p id="p-0278" num="0284">Further, in <figref idref="DRAWINGS">FIG. 8</figref>, which shows the optical image shown in <figref idref="DRAWINGS">FIG. 9</figref> by the three-dimensional point cloud data, a slope form along the road (a cross section shape of the road) can be observed.</p>
<p id="p-0279" num="0285">Next, the feature position locating process (S<b>106</b>) in <figref idref="DRAWINGS">FIG. 3</figref> will be explained.</p>
<p id="p-0280" num="0286"><figref idref="DRAWINGS">FIG. 24</figref> shows a calculating method of feature position in the feature position locating process (S<b>106</b>) according to the first embodiment.</p>
<p id="p-0281" num="0287">An outline of the calculating method of feature position according to the first embodiment will be explained in the following with reference to <figref idref="DRAWINGS">FIG. 24</figref>.</p>
<p id="p-0282" num="0288">In the feature position locating process (S<b>106</b>), the ENU coordinate of an intersecting point of a LOS vector from the center of camera to the measurement image point on the image plane and a plane formed by three points (P<b>1</b>, P<b>2</b>, P<b>3</b>) before projecting corresponding to three neighboring points (P<b>1</b>&#x2032;, P<b>2</b>&#x2032;, P<b>3</b>&#x2032;) of the measurement image point on projecting the road surface shape model on the image plane is calculated as the feature position.</p>
<p id="p-0283" num="0289"><figref idref="DRAWINGS">FIG. 25</figref> is a flowchart showing a flow of the feature position locating process (S<b>106</b>) according to the first embodiment.</p>
<p id="p-0284" num="0290">&#x3c;S<b>501</b>: Vehicle-Camera Synchronous Data Generating Process&#x3e;</p>
<p id="p-0285" num="0291">First, the camera LOS computing unit <b>140</b> generates the vehicle-camera synchronous data by synchronizing the vehicle position and posture data with the time of image data.</p>
<p id="p-0286" num="0292">&#x3c;S<b>502</b>: 3D Model Projecting Process&#x3e;</p>
<p id="p-0287" num="0293">Next, the model projecting unit <b>172</b> of the road surface model corresponding point searching unit <b>170</b> extracts the position and posture of the vehicle which is synchronized with the specified image from the vehicle-camera synchronous data, and projects the road surface shape model on the image plane based on the position and posture of vehicle and the position and posture of camera. Then, the road surface shape model is superposed with the image and displayed on the displaying device.</p>
<p id="p-0288" num="0294">Here, a detail of the 3D model projecting process (S<b>502</b>) performed by the model projecting unit <b>172</b> will be explained.</p>
<p id="p-0289" num="0295">The model projecting unit <b>172</b> projects and converts the road surface shape model to the image plane of the camera.</p>
<p id="p-0290" num="0296">The position (xcam, ycam, zcam) of the point cloud of the road surface shape model for the camera coordinate system is represented by the following equations 5 and 6.</p>
<p id="p-0291" num="0297">
<maths id="MATH-US-00006" num="00006">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mstyle>
          <mspace width="4.4em" height="4.4ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mrow>
          <mo>[</mo>
          <mrow>
            <mi>Expression</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.8em" height="0.8ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mn>4</mn>
          </mrow>
          <mo>]</mo>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mstyle>
        <mspace width="0.3em" height="0.3ex"/>
      </mstyle>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mo>[</mo>
          <mtable>
            <mtr>
              <mtd>
                <msub>
                  <mi>x</mi>
                  <mi>cam</mi>
                </msub>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>y</mi>
                  <mi>cam</mi>
                </msub>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>z</mi>
                  <mi>cam</mi>
                </msub>
              </mtd>
            </mtr>
          </mtable>
          <mo>]</mo>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mrow>
            <mrow>
              <mo>[</mo>
              <mtable>
                <mtr>
                  <mtd>
                    <mrow>
                      <mi>cos</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3d5;</mi>
                        <mi>cam</mi>
                      </msub>
                    </mrow>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mrow>
                        <mo>-</mo>
                        <mi>sin</mi>
                      </mrow>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3d5;</mi>
                        <mi>cam</mi>
                      </msub>
                    </mrow>
                  </mtd>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mrow>
                      <mi>sin</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3d5;</mi>
                        <mi>cam</mi>
                      </msub>
                    </mrow>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mi>cos</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3d5;</mi>
                        <mi>cam</mi>
                      </msub>
                    </mrow>
                  </mtd>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mn>1</mn>
                  </mtd>
                </mtr>
              </mtable>
              <mo>]</mo>
            </mrow>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>[</mo>
              <mtable>
                <mtr>
                  <mtd>
                    <mn>1</mn>
                  </mtd>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mi>cos</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3b8;</mi>
                        <mi>cam</mi>
                      </msub>
                    </mrow>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mrow>
                        <mo>-</mo>
                        <mi>sin</mi>
                      </mrow>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3b8;</mi>
                        <mi>cam</mi>
                      </msub>
                    </mrow>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mi>sin</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3b8;</mi>
                        <mi>cam</mi>
                      </msub>
                    </mrow>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mi>cos</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3b8;</mi>
                        <mi>cam</mi>
                      </msub>
                    </mrow>
                  </mtd>
                </mtr>
              </mtable>
              <mo>]</mo>
            </mrow>
          </mrow>
          <mo>&#x2062;</mo>
          <mrow>
            <mo>&#x2003;</mo>
            <mrow>
              <mrow>
                <mo>[</mo>
                <mtable>
                  <mtr>
                    <mtd>
                      <mrow>
                        <mi>cos</mi>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <msub>
                          <mi>&#x3c8;</mi>
                          <mi>cam</mi>
                        </msub>
                      </mrow>
                    </mtd>
                    <mtd>
                      <mn>0</mn>
                    </mtd>
                    <mtd>
                      <mrow>
                        <mi>sin</mi>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <msub>
                          <mi>&#x3c8;</mi>
                          <mi>cam</mi>
                        </msub>
                      </mrow>
                    </mtd>
                  </mtr>
                  <mtr>
                    <mtd>
                      <mn>0</mn>
                    </mtd>
                    <mtd>
                      <mn>1</mn>
                    </mtd>
                    <mtd>
                      <mn>0</mn>
                    </mtd>
                  </mtr>
                  <mtr>
                    <mtd>
                      <mrow>
                        <mrow>
                          <mo>-</mo>
                          <mi>sin</mi>
                        </mrow>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <msub>
                          <mi>&#x3c8;</mi>
                          <mi>cam</mi>
                        </msub>
                      </mrow>
                    </mtd>
                    <mtd>
                      <mn>0</mn>
                    </mtd>
                    <mtd>
                      <mrow>
                        <mi>cos</mi>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <msub>
                          <mi>&#x3c8;</mi>
                          <mi>cam</mi>
                        </msub>
                      </mrow>
                    </mtd>
                  </mtr>
                </mtable>
                <mo>]</mo>
              </mrow>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>[</mo>
                <mtable>
                  <mtr>
                    <mtd>
                      <msub>
                        <mi>x</mi>
                        <mn>3</mn>
                      </msub>
                    </mtd>
                  </mtr>
                  <mtr>
                    <mtd>
                      <msub>
                        <mi>y</mi>
                        <mn>3</mn>
                      </msub>
                    </mtd>
                  </mtr>
                  <mtr>
                    <mtd>
                      <msub>
                        <mi>z</mi>
                        <mn>3</mn>
                      </msub>
                    </mtd>
                  </mtr>
                </mtable>
                <mo>]</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>equation</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>5</mn>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<ul id="ul0006" list-style="none">
    <li id="ul0006-0001" num="0000">
    <ul id="ul0007" list-style="none">
        <li id="ul0007-0001" num="0298">where the attachment position of the camera is &#x394;xcam: distance in transverse direction from the position of vehicle; &#x394;ycam: distance in height direction from the position of vehicle; &#x394;zcam: distance in depth direction from the position of vehicle; &#x3c6;cam: an attachment roll angle to the top plate; &#x3b8;cam: an attachment pitch angle to the top plate; and &#x3c8;cam: an attachment yaw angle to the top plate. However,</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0292" num="0299">
<maths id="MATH-US-00007" num="00007">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mo>[</mo>
          <mtable>
            <mtr>
              <mtd>
                <msub>
                  <mi>x</mi>
                  <mn>2</mn>
                </msub>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>y</mi>
                  <mn>2</mn>
                </msub>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>z</mi>
                  <mn>2</mn>
                </msub>
              </mtd>
            </mtr>
          </mtable>
          <mo>]</mo>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mrow>
            <mrow>
              <mo>[</mo>
              <mtable>
                <mtr>
                  <mtd>
                    <mrow>
                      <mi>cos</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3d5;</mi>
                        <mi>v</mi>
                      </msub>
                    </mrow>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mrow>
                        <mo>-</mo>
                        <mi>sin</mi>
                      </mrow>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3d5;</mi>
                        <mi>v</mi>
                      </msub>
                    </mrow>
                  </mtd>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mrow>
                      <mi>sin</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3d5;</mi>
                        <mi>v</mi>
                      </msub>
                    </mrow>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mi>cos</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3d5;</mi>
                        <mi>v</mi>
                      </msub>
                    </mrow>
                  </mtd>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mn>1</mn>
                  </mtd>
                </mtr>
              </mtable>
              <mo>]</mo>
            </mrow>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>[</mo>
              <mtable>
                <mtr>
                  <mtd>
                    <mn>1</mn>
                  </mtd>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mi>cos</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3b8;</mi>
                        <mi>v</mi>
                      </msub>
                    </mrow>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mrow>
                        <mo>-</mo>
                        <mi>sin</mi>
                      </mrow>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3b8;</mi>
                        <mi>v</mi>
                      </msub>
                    </mrow>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mi>sin</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3b8;</mi>
                        <mi>v</mi>
                      </msub>
                    </mrow>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mi>cos</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3b8;</mi>
                        <mi>v</mi>
                      </msub>
                    </mrow>
                  </mtd>
                </mtr>
              </mtable>
              <mo>]</mo>
            </mrow>
          </mrow>
          <mo>&#x2062;</mo>
          <mrow>
            <mo>&#x2003;</mo>
            <mrow>
              <mrow>
                <mrow>
                  <mo>[</mo>
                  <mtable>
                    <mtr>
                      <mtd>
                        <mrow>
                          <mi>cos</mi>
                          <mo>&#x2062;</mo>
                          <mstyle>
                            <mspace width="0.3em" height="0.3ex"/>
                          </mstyle>
                          <mo>&#x2062;</mo>
                          <msub>
                            <mi>&#x3c8;</mi>
                            <mi>v</mi>
                          </msub>
                        </mrow>
                      </mtd>
                      <mtd>
                        <mn>0</mn>
                      </mtd>
                      <mtd>
                        <mrow>
                          <mi>sin</mi>
                          <mo>&#x2062;</mo>
                          <mstyle>
                            <mspace width="0.3em" height="0.3ex"/>
                          </mstyle>
                          <mo>&#x2062;</mo>
                          <msub>
                            <mi>&#x3c8;</mi>
                            <mi>v</mi>
                          </msub>
                        </mrow>
                      </mtd>
                    </mtr>
                    <mtr>
                      <mtd>
                        <mn>0</mn>
                      </mtd>
                      <mtd>
                        <mn>1</mn>
                      </mtd>
                      <mtd>
                        <mn>0</mn>
                      </mtd>
                    </mtr>
                    <mtr>
                      <mtd>
                        <mrow>
                          <mrow>
                            <mo>-</mo>
                            <mi>sin</mi>
                          </mrow>
                          <mo>&#x2062;</mo>
                          <mstyle>
                            <mspace width="0.3em" height="0.3ex"/>
                          </mstyle>
                          <mo>&#x2062;</mo>
                          <msub>
                            <mi>&#x3c8;</mi>
                            <mi>v</mi>
                          </msub>
                        </mrow>
                      </mtd>
                      <mtd>
                        <mn>0</mn>
                      </mtd>
                      <mtd>
                        <mrow>
                          <mi>cos</mi>
                          <mo>&#x2062;</mo>
                          <mstyle>
                            <mspace width="0.3em" height="0.3ex"/>
                          </mstyle>
                          <mo>&#x2062;</mo>
                          <msub>
                            <mi>&#x3c8;</mi>
                            <mi>v</mi>
                          </msub>
                        </mrow>
                      </mtd>
                    </mtr>
                  </mtable>
                  <mo>]</mo>
                </mrow>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>[</mo>
                  <mtable>
                    <mtr>
                      <mtd>
                        <msub>
                          <mi>x</mi>
                          <mi>lrf</mi>
                        </msub>
                      </mtd>
                    </mtr>
                    <mtr>
                      <mtd>
                        <msub>
                          <mi>y</mi>
                          <mi>lrrf</mi>
                        </msub>
                      </mtd>
                    </mtr>
                    <mtr>
                      <mtd>
                        <msub>
                          <mi>z</mi>
                          <mi>lrf</mi>
                        </msub>
                      </mtd>
                    </mtr>
                  </mtable>
                  <mo>]</mo>
                </mrow>
              </mrow>
              <mo>-</mo>
              <mrow>
                <mo>[</mo>
                <mtable>
                  <mtr>
                    <mtd>
                      <mrow>
                        <mi>&#x394;</mi>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <msub>
                          <mi>x</mi>
                          <mi>cam</mi>
                        </msub>
                      </mrow>
                    </mtd>
                  </mtr>
                  <mtr>
                    <mtd>
                      <mrow>
                        <mi>&#x394;</mi>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <msub>
                          <mi>y</mi>
                          <mi>cam</mi>
                        </msub>
                      </mrow>
                    </mtd>
                  </mtr>
                  <mtr>
                    <mtd>
                      <mrow>
                        <mi>&#x394;</mi>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <msub>
                          <mi>z</mi>
                          <mi>cam</mi>
                        </msub>
                      </mrow>
                    </mtd>
                  </mtr>
                </mtable>
                <mo>]</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>equation</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>6</mn>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0293" num="0300">Next, a straight line formed by these points and the center of the camera (xcam<b>0</b>, ycam<b>0</b>, zcam<b>0</b>) is represented by the following equations 7, 8, 9, and 10.</p>
<p id="p-0294" num="0301">
<maths id="MATH-US-00008" num="00008">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mo>[</mo>
        <mrow>
          <mi>Expression</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>5</mn>
        </mrow>
        <mo>]</mo>
      </mrow>
    </mtd>
    <mtd>
      <mstyle>
        <mspace width="0.3em" height="0.3ex"/>
      </mstyle>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mfrac>
          <mrow>
            <mi>x</mi>
            <mo>-</mo>
            <msub>
              <mi>x</mi>
              <mrow>
                <mi>cam</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mn>0</mn>
              </mrow>
            </msub>
          </mrow>
          <mi>&#x3bb;</mi>
        </mfrac>
        <mo>=</mo>
        <mrow>
          <mfrac>
            <mrow>
              <mi>y</mi>
              <mo>-</mo>
              <msub>
                <mi>y</mi>
                <mrow>
                  <mi>cam</mi>
                  <mo>&#x2062;</mo>
                  <mstyle>
                    <mspace width="0.3em" height="0.3ex"/>
                  </mstyle>
                  <mo>&#x2062;</mo>
                  <mn>0</mn>
                </mrow>
              </msub>
            </mrow>
            <mi>&#x3bc;</mi>
          </mfrac>
          <mo>=</mo>
          <mfrac>
            <mrow>
              <mi>z</mi>
              <mo>-</mo>
              <msub>
                <mi>z</mi>
                <mrow>
                  <mi>cam</mi>
                  <mo>&#x2062;</mo>
                  <mstyle>
                    <mspace width="0.3em" height="0.3ex"/>
                  </mstyle>
                  <mo>&#x2062;</mo>
                  <mn>0</mn>
                </mrow>
              </msub>
            </mrow>
            <mi>&#x3c5;</mi>
          </mfrac>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>equation</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>7</mn>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0295" num="0302">However,</p>
<p id="p-0296" num="0303">
<maths id="MATH-US-00009" num="00009">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mi>&#x3bb;</mi>
        <mo>=</mo>
        <mfrac>
          <mrow>
            <msub>
              <mi>x</mi>
              <mi>cam</mi>
            </msub>
            <mo>-</mo>
            <msub>
              <mi>x</mi>
              <mrow>
                <mi>cam</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mn>0</mn>
              </mrow>
            </msub>
          </mrow>
          <msqrt>
            <mrow>
              <msup>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <msub>
                      <mi>x</mi>
                      <mi>cam</mi>
                    </msub>
                    <mo>-</mo>
                    <msub>
                      <mi>x</mi>
                      <mrow>
                        <mi>cam</mi>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <mn>0</mn>
                      </mrow>
                    </msub>
                  </mrow>
                  <mo>)</mo>
                </mrow>
                <mn>2</mn>
              </msup>
              <mo>+</mo>
              <msup>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <msub>
                      <mi>y</mi>
                      <mi>cam</mi>
                    </msub>
                    <mo>-</mo>
                    <msub>
                      <mi>y</mi>
                      <mrow>
                        <mi>cam</mi>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <mn>0</mn>
                      </mrow>
                    </msub>
                  </mrow>
                  <mo>)</mo>
                </mrow>
                <mn>2</mn>
              </msup>
              <mo>+</mo>
              <msup>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <msub>
                      <mi>z</mi>
                      <mi>cam</mi>
                    </msub>
                    <mo>-</mo>
                    <msub>
                      <mi>z</mi>
                      <mrow>
                        <mi>cam</mi>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <mn>0</mn>
                      </mrow>
                    </msub>
                  </mrow>
                  <mo>)</mo>
                </mrow>
                <mn>2</mn>
              </msup>
            </mrow>
          </msqrt>
        </mfrac>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>equation</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>8</mn>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mtable>
        <mtr>
          <mtd>
            <mrow>
              <mi>&#x3bc;</mi>
              <mo>=</mo>
              <mfrac>
                <mrow>
                  <msub>
                    <mi>y</mi>
                    <mi>cam</mi>
                  </msub>
                  <mo>-</mo>
                  <msub>
                    <mi>y</mi>
                    <mrow>
                      <mi>cam</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <mn>0</mn>
                    </mrow>
                  </msub>
                </mrow>
                <msqrt>
                  <mrow>
                    <msup>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <msub>
                            <mi>x</mi>
                            <mi>cam</mi>
                          </msub>
                          <mo>-</mo>
                          <msub>
                            <mi>x</mi>
                            <mrow>
                              <mi>cam</mi>
                              <mo>&#x2062;</mo>
                              <mstyle>
                                <mspace width="0.3em" height="0.3ex"/>
                              </mstyle>
                              <mo>&#x2062;</mo>
                              <mn>0</mn>
                            </mrow>
                          </msub>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                      <mn>2</mn>
                    </msup>
                    <mo>+</mo>
                    <msup>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <msub>
                            <mi>y</mi>
                            <mi>cam</mi>
                          </msub>
                          <mo>-</mo>
                          <msub>
                            <mi>y</mi>
                            <mrow>
                              <mi>cam</mi>
                              <mo>&#x2062;</mo>
                              <mstyle>
                                <mspace width="0.3em" height="0.3ex"/>
                              </mstyle>
                              <mo>&#x2062;</mo>
                              <mn>0</mn>
                            </mrow>
                          </msub>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                      <mn>2</mn>
                    </msup>
                    <mo>+</mo>
                    <msup>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <msub>
                            <mi>z</mi>
                            <mi>cam</mi>
                          </msub>
                          <mo>-</mo>
                          <msub>
                            <mi>z</mi>
                            <mrow>
                              <mi>cam</mi>
                              <mo>&#x2062;</mo>
                              <mstyle>
                                <mspace width="0.3em" height="0.3ex"/>
                              </mstyle>
                              <mo>&#x2062;</mo>
                              <mn>0</mn>
                            </mrow>
                          </msub>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                      <mn>2</mn>
                    </msup>
                  </mrow>
                </msqrt>
              </mfrac>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mrow>
              <mstyle>
                <mspace width="4.4em" height="4.4ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mrow>
                <mo>(</mo>
                <mn>4.14</mn>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mtd>
        </mtr>
      </mtable>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>equation</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>9</mn>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mi>&#x3c5;</mi>
        <mo>=</mo>
        <mfrac>
          <mrow>
            <msub>
              <mi>z</mi>
              <mi>cam</mi>
            </msub>
            <mo>-</mo>
            <msub>
              <mi>z</mi>
              <mrow>
                <mi>cam</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mn>0</mn>
              </mrow>
            </msub>
          </mrow>
          <msqrt>
            <mrow>
              <msup>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <msub>
                      <mi>x</mi>
                      <mi>cam</mi>
                    </msub>
                    <mo>-</mo>
                    <msub>
                      <mi>x</mi>
                      <mrow>
                        <mi>cam</mi>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <mn>0</mn>
                      </mrow>
                    </msub>
                  </mrow>
                  <mo>)</mo>
                </mrow>
                <mn>2</mn>
              </msup>
              <mo>+</mo>
              <msup>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <msub>
                      <mi>y</mi>
                      <mi>cam</mi>
                    </msub>
                    <mo>-</mo>
                    <msub>
                      <mi>y</mi>
                      <mrow>
                        <mi>cam</mi>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <mn>0</mn>
                      </mrow>
                    </msub>
                  </mrow>
                  <mo>)</mo>
                </mrow>
                <mn>2</mn>
              </msup>
              <mo>+</mo>
              <msup>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <msub>
                      <mi>z</mi>
                      <mi>cam</mi>
                    </msub>
                    <mo>-</mo>
                    <msub>
                      <mi>z</mi>
                      <mrow>
                        <mi>cam</mi>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <mn>0</mn>
                      </mrow>
                    </msub>
                  </mrow>
                  <mo>)</mo>
                </mrow>
                <mn>2</mn>
              </msup>
            </mrow>
          </msqrt>
        </mfrac>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>equation</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>10</mn>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0297" num="0304">Further, at this time, the image plane is represented by the following equation 11 using a focal distance f assuming that the camera <b>230</b> is an ideal pin hole camera:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>z=f&#x2003;&#x2003;(equation 11)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0298" num="0305">An intersecting point of this image plane and the straight line is a point which is the laser measured point of the road surface shape model projected on the image.</p>
<p id="p-0299" num="0306"><figref idref="DRAWINGS">FIG. 10</figref> shows the image on which projected transformation has been performed. As understood from the figure, the point projected transformed and the image are matched well. For example, it can be understood from the figure that a step represented by the point projected transformed matches the step of the road surface shown by the image.</p>
<p id="p-0300" num="0307">Next, in <figref idref="DRAWINGS">FIG. 25</figref>, the three neighboring points extracting process (S<b>503</b>) will be explained.</p>
<p id="p-0301" num="0308">A neighborhood extracting unit <b>171</b> of the road surface model corresponding point searching unit <b>170</b> extracts three neighboring points of the measurement image point from the laser measured point cloud of the road surface shape model.</p>
<p id="p-0302" num="0309"><figref idref="DRAWINGS">FIG. 26</figref> is a flowchart showing a flow of the three neighboring points extracting process (S<b>503</b>) according to the first embodiment.</p>
<p id="p-0303" num="0310"><figref idref="DRAWINGS">FIG. 27</figref> shows the three neighboring points extracting process (S<b>503</b>) according to the first embodiment.</p>
<p id="p-0304" num="0311">A detail of the three neighboring points extracting process (S<b>503</b>) performed by the neighborhood extracting unit <b>171</b> of the road surface model corresponding point searching unit <b>170</b> will be explained in the following with reference to <figref idref="DRAWINGS">FIGS. 26 and 27</figref>.</p>
<p id="p-0305" num="0312">In <figref idref="DRAWINGS">FIGS. 26 and 27</figref>, the neighborhood extracting unit <b>171</b> calculates the most neighboring point P<b>1</b> of the measurement image point Pin from the laser measured point cloud of the road surface shape model projected on the image plane (S<b>601</b>).</p>
<p id="p-0306" num="0313">Next, the neighborhood extracting unit <b>171</b> selects a scanning line S<b>3</b>, an interval from which to a scanning line S<b>2</b> includes the measurement image point Pin, out of a scanning line S<b>1</b> and the scanning line S<b>3</b> which are scanning lines directly before and after the scanning line S<b>2</b> including the most neighboring point P<b>1</b> (S<b>602</b>).</p>
<p id="p-0307" num="0314">Next, the neighborhood extracting unit <b>171</b> calculates a straight line L which connects the most neighboring point P<b>1</b> and the measurement image point Pin (S<b>603</b>).</p>
<p id="p-0308" num="0315">Then, the neighborhood extracting unit <b>171</b>, in the selected scanning line S<b>3</b>, calculates a point P<b>2</b> which is the most neighboring to the straight line L at the right side of the straight line L and a point P<b>3</b> which is the most neighboring to the straight line L at the left side of the straight line L, and outputs the points corresponding to the calculated most neighboring points P<b>1</b>, P<b>2</b>, and P<b>3</b> before projecting on the image plane as three neighboring points of the measurement image point Pin (S<b>604</b>).</p>
<p id="p-0309" num="0316">The scanning line means the laser measured point cloud obtained by irradiation of laser with swinging once in the transverse direction by the laser radar <b>240</b>, and the laser measured point cloud of one scanning line forms a line segment.</p>
<p id="p-0310" num="0317">If the three neighboring points of the measurement image point extracted in the three neighboring points extracting process (S<b>503</b>) are P<b>1</b> (xp1, yp1, zp1), P<b>1</b> (xp2, yp2, zp2), and P<b>3</b> (xp3, yp3, zp3), an equation form of a three-dimensional plane (the road surface shape model corresponding plane) formed by the three neighboring points of the measurement image point is represented by the following equation 12.</p>
<p id="p-0311" num="0318">
<maths id="MATH-US-00010" num="00010">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mo>[</mo>
        <mrow>
          <mi>Expression</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>6</mn>
        </mrow>
        <mo>]</mo>
      </mrow>
    </mtd>
    <mtd>
      <mstyle>
        <mspace width="0.3em" height="0.3ex"/>
      </mstyle>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mo>&#xf603;</mo>
          <mtable>
            <mtr>
              <mtd>
                <mi>x</mi>
              </mtd>
              <mtd>
                <mi>y</mi>
              </mtd>
              <mtd>
                <mi>z</mi>
              </mtd>
              <mtd>
                <mn>1</mn>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>x</mi>
                  <mrow>
                    <mi>p</mi>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.3em" height="0.3ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mn>1</mn>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <msub>
                  <mi>y</mi>
                  <mrow>
                    <mi>p</mi>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.3em" height="0.3ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mn>1</mn>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <msub>
                  <mi>z</mi>
                  <mrow>
                    <mi>p</mi>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.3em" height="0.3ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mn>1</mn>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <mn>1</mn>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>x</mi>
                  <mrow>
                    <mi>p</mi>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.3em" height="0.3ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mn>2</mn>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <msub>
                  <mi>y</mi>
                  <mrow>
                    <mi>p</mi>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.3em" height="0.3ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mn>2</mn>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <msub>
                  <mi>z</mi>
                  <mrow>
                    <mi>p</mi>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.3em" height="0.3ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mn>2</mn>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <mn>1</mn>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>x</mi>
                  <mrow>
                    <mi>p</mi>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.3em" height="0.3ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mn>3</mn>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <msub>
                  <mi>y</mi>
                  <mrow>
                    <mi>p</mi>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.3em" height="0.3ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mn>3</mn>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <msub>
                  <mi>z</mi>
                  <mrow>
                    <mi>p</mi>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.3em" height="0.3ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mn>3</mn>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <mn>1</mn>
              </mtd>
            </mtr>
          </mtable>
          <mo>&#xf604;</mo>
        </mrow>
        <mo>=</mo>
        <mn>0</mn>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>equation</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>12</mn>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0312" num="0319">Next, in <figref idref="DRAWINGS">FIG. 25</figref>, a LOS computing process (S<b>504</b>) will be explained.</p>
<p id="p-0313" num="0320">&#x3c;S<b>504</b>: LOS Calculating Process&#x3e;</p>
<p id="p-0314" num="0321">The camera LOS computing unit <b>140</b> extracts the position and posture of the vehicle which is synchronized with the specified image from the vehicle-camera synchronous data, calculates ENU coordinates of the measurement image point and the center of the camera of the specified image based on the position and posture of vehicle and the position and posture of camera, and calculates a LOS vector from the center of the camera to the measurement image point in the ENU coordinate system.</p>
<p id="p-0315" num="0322">Here, if the position on the image shown by the measurement image point is (UL, VL), the measurement image point (NL, UL, EL) in the ENU coordinate can be obtained using the following equations 13, 14, and 15.</p>
<p id="p-0316" num="0323">
<maths id="MATH-US-00011" num="00011">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mstyle>
          <mspace width="4.4em" height="4.4ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mrow>
          <mo>[</mo>
          <mrow>
            <mi>Expression</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.8em" height="0.8ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mn>7</mn>
          </mrow>
          <mo>]</mo>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mstyle>
        <mspace width="0.3em" height="0.3ex"/>
      </mstyle>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mo>[</mo>
          <mtable>
            <mtr>
              <mtd>
                <msub>
                  <mi>N</mi>
                  <mi>L</mi>
                </msub>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>U</mi>
                  <mi>L</mi>
                </msub>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>E</mi>
                  <mi>L</mi>
                </msub>
              </mtd>
            </mtr>
          </mtable>
          <mo>]</mo>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mrow>
            <mrow>
              <mo>[</mo>
              <mtable>
                <mtr>
                  <mtd>
                    <mrow>
                      <mi>cos</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3c8;</mi>
                        <mi>v</mi>
                      </msub>
                    </mrow>
                  </mtd>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mi>sin</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3c8;</mi>
                        <mi>v</mi>
                      </msub>
                    </mrow>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mn>1</mn>
                  </mtd>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mrow>
                      <mrow>
                        <mo>-</mo>
                        <mi>sin</mi>
                      </mrow>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3c8;</mi>
                        <mi>v</mi>
                      </msub>
                    </mrow>
                  </mtd>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mi>cos</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3c8;</mi>
                        <mi>v</mi>
                      </msub>
                    </mrow>
                  </mtd>
                </mtr>
              </mtable>
              <mo>]</mo>
            </mrow>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>[</mo>
              <mtable>
                <mtr>
                  <mtd>
                    <mn>1</mn>
                  </mtd>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mi>cos</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3b8;</mi>
                        <mi>v</mi>
                      </msub>
                    </mrow>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mrow>
                        <mo>-</mo>
                        <mi>sin</mi>
                      </mrow>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3b8;</mi>
                        <mi>v</mi>
                      </msub>
                    </mrow>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mi>sin</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3b8;</mi>
                        <mi>v</mi>
                      </msub>
                    </mrow>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mi>cos</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3b8;</mi>
                        <mi>v</mi>
                      </msub>
                    </mrow>
                  </mtd>
                </mtr>
              </mtable>
              <mo>]</mo>
            </mrow>
          </mrow>
          <mo>&#x2062;</mo>
          <mrow>
            <mo>&#x2003;</mo>
            <mrow>
              <mrow>
                <mrow>
                  <mo>[</mo>
                  <mtable>
                    <mtr>
                      <mtd>
                        <mrow>
                          <mi>cos</mi>
                          <mo>&#x2062;</mo>
                          <mstyle>
                            <mspace width="0.3em" height="0.3ex"/>
                          </mstyle>
                          <mo>&#x2062;</mo>
                          <msub>
                            <mi>&#x3d5;</mi>
                            <mi>v</mi>
                          </msub>
                        </mrow>
                      </mtd>
                      <mtd>
                        <mrow>
                          <mrow>
                            <mo>-</mo>
                            <mi>sin</mi>
                          </mrow>
                          <mo>&#x2062;</mo>
                          <mstyle>
                            <mspace width="0.3em" height="0.3ex"/>
                          </mstyle>
                          <mo>&#x2062;</mo>
                          <msub>
                            <mi>&#x3d5;</mi>
                            <mi>v</mi>
                          </msub>
                        </mrow>
                      </mtd>
                      <mtd>
                        <mn>0</mn>
                      </mtd>
                    </mtr>
                    <mtr>
                      <mtd>
                        <mrow>
                          <mi>sin</mi>
                          <mo>&#x2062;</mo>
                          <mstyle>
                            <mspace width="0.3em" height="0.3ex"/>
                          </mstyle>
                          <mo>&#x2062;</mo>
                          <msub>
                            <mi>&#x3d5;</mi>
                            <mi>v</mi>
                          </msub>
                        </mrow>
                      </mtd>
                      <mtd>
                        <mrow>
                          <mi>cos</mi>
                          <mo>&#x2062;</mo>
                          <mstyle>
                            <mspace width="0.3em" height="0.3ex"/>
                          </mstyle>
                          <mo>&#x2062;</mo>
                          <msub>
                            <mi>&#x3d5;</mi>
                            <mi>v</mi>
                          </msub>
                        </mrow>
                      </mtd>
                      <mtd>
                        <mn>0</mn>
                      </mtd>
                    </mtr>
                    <mtr>
                      <mtd>
                        <mn>0</mn>
                      </mtd>
                      <mtd>
                        <mn>0</mn>
                      </mtd>
                      <mtd>
                        <mn>1</mn>
                      </mtd>
                    </mtr>
                  </mtable>
                  <mo>]</mo>
                </mrow>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>[</mo>
                  <mtable>
                    <mtr>
                      <mtd>
                        <msub>
                          <mi>N</mi>
                          <mi>Lcam</mi>
                        </msub>
                      </mtd>
                    </mtr>
                    <mtr>
                      <mtd>
                        <msub>
                          <mi>U</mi>
                          <mi>Lcam</mi>
                        </msub>
                      </mtd>
                    </mtr>
                    <mtr>
                      <mtd>
                        <msub>
                          <mi>E</mi>
                          <mi>Lcam</mi>
                        </msub>
                      </mtd>
                    </mtr>
                  </mtable>
                  <mo>]</mo>
                </mrow>
              </mrow>
              <mo>+</mo>
              <mrow>
                <mo>[</mo>
                <mtable>
                  <mtr>
                    <mtd>
                      <msub>
                        <mi>N</mi>
                        <mi>v</mi>
                      </msub>
                    </mtd>
                  </mtr>
                  <mtr>
                    <mtd>
                      <msub>
                        <mi>U</mi>
                        <mi>v</mi>
                      </msub>
                    </mtd>
                  </mtr>
                  <mtr>
                    <mtd>
                      <msub>
                        <mi>E</mi>
                        <mi>v</mi>
                      </msub>
                    </mtd>
                  </mtr>
                </mtable>
                <mo>]</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>equation</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>13</mn>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0317" num="0324">However,</p>
<p id="p-0318" num="0325">
<maths id="MATH-US-00012" num="00012">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mo>[</mo>
          <mtable>
            <mtr>
              <mtd>
                <msub>
                  <mi>N</mi>
                  <mi>L</mi>
                </msub>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>U</mi>
                  <mi>L</mi>
                </msub>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>E</mi>
                  <mi>L</mi>
                </msub>
              </mtd>
            </mtr>
          </mtable>
          <mo>]</mo>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mrow>
            <mrow>
              <mo>[</mo>
              <mtable>
                <mtr>
                  <mtd>
                    <mrow>
                      <mi>cos</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3c8;</mi>
                        <mi>cam</mi>
                      </msub>
                    </mrow>
                  </mtd>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mi>sin</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3c8;</mi>
                        <mi>cam</mi>
                      </msub>
                    </mrow>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mn>1</mn>
                  </mtd>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mrow>
                      <mrow>
                        <mo>-</mo>
                        <mi>sin</mi>
                      </mrow>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3c8;</mi>
                        <mi>cam</mi>
                      </msub>
                    </mrow>
                  </mtd>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mi>cos</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3c8;</mi>
                        <mi>cam</mi>
                      </msub>
                    </mrow>
                  </mtd>
                </mtr>
              </mtable>
              <mo>]</mo>
            </mrow>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>[</mo>
              <mtable>
                <mtr>
                  <mtd>
                    <mn>1</mn>
                  </mtd>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mi>cos</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3b8;</mi>
                        <mi>cam</mi>
                      </msub>
                    </mrow>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mrow>
                        <mo>-</mo>
                        <mi>sin</mi>
                      </mrow>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3b8;</mi>
                        <mi>cam</mi>
                      </msub>
                    </mrow>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mn>0</mn>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mi>sin</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3b8;</mi>
                        <mi>cam</mi>
                      </msub>
                    </mrow>
                  </mtd>
                  <mtd>
                    <mrow>
                      <mi>cos</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msub>
                        <mi>&#x3b8;</mi>
                        <mi>cam</mi>
                      </msub>
                    </mrow>
                  </mtd>
                </mtr>
              </mtable>
              <mo>]</mo>
            </mrow>
          </mrow>
          <mo>&#x2062;</mo>
          <mrow>
            <mo>&#x2003;</mo>
            <mrow>
              <mrow>
                <mrow>
                  <mo>[</mo>
                  <mtable>
                    <mtr>
                      <mtd>
                        <mrow>
                          <mi>cos</mi>
                          <mo>&#x2062;</mo>
                          <mstyle>
                            <mspace width="0.3em" height="0.3ex"/>
                          </mstyle>
                          <mo>&#x2062;</mo>
                          <msub>
                            <mi>&#x3d5;</mi>
                            <mi>cam</mi>
                          </msub>
                        </mrow>
                      </mtd>
                      <mtd>
                        <mrow>
                          <mrow>
                            <mo>-</mo>
                            <mi>sin</mi>
                          </mrow>
                          <mo>&#x2062;</mo>
                          <mstyle>
                            <mspace width="0.3em" height="0.3ex"/>
                          </mstyle>
                          <mo>&#x2062;</mo>
                          <msub>
                            <mi>&#x3d5;</mi>
                            <mi>cam</mi>
                          </msub>
                        </mrow>
                      </mtd>
                      <mtd>
                        <mn>0</mn>
                      </mtd>
                    </mtr>
                    <mtr>
                      <mtd>
                        <mrow>
                          <mi>sin</mi>
                          <mo>&#x2062;</mo>
                          <mstyle>
                            <mspace width="0.3em" height="0.3ex"/>
                          </mstyle>
                          <mo>&#x2062;</mo>
                          <msub>
                            <mi>&#x3d5;</mi>
                            <mi>cam</mi>
                          </msub>
                        </mrow>
                      </mtd>
                      <mtd>
                        <mrow>
                          <mi>cos</mi>
                          <mo>&#x2062;</mo>
                          <mstyle>
                            <mspace width="0.3em" height="0.3ex"/>
                          </mstyle>
                          <mo>&#x2062;</mo>
                          <msub>
                            <mi>&#x3d5;</mi>
                            <mi>cam</mi>
                          </msub>
                        </mrow>
                      </mtd>
                      <mtd>
                        <mn>0</mn>
                      </mtd>
                    </mtr>
                    <mtr>
                      <mtd>
                        <mn>0</mn>
                      </mtd>
                      <mtd>
                        <mn>0</mn>
                      </mtd>
                      <mtd>
                        <mn>1</mn>
                      </mtd>
                    </mtr>
                  </mtable>
                  <mo>]</mo>
                </mrow>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>[</mo>
                  <mtable>
                    <mtr>
                      <mtd>
                        <msubsup>
                          <mi>U</mi>
                          <mi>L</mi>
                          <mi>&#x2032;</mi>
                        </msubsup>
                      </mtd>
                    </mtr>
                    <mtr>
                      <mtd>
                        <msubsup>
                          <mi>V</mi>
                          <mi>L</mi>
                          <mi>&#x2032;</mi>
                        </msubsup>
                      </mtd>
                    </mtr>
                    <mtr>
                      <mtd>
                        <mi>f</mi>
                      </mtd>
                    </mtr>
                  </mtable>
                  <mo>]</mo>
                </mrow>
              </mrow>
              <mo>+</mo>
              <mrow>
                <mo>[</mo>
                <mtable>
                  <mtr>
                    <mtd>
                      <mrow>
                        <mi>&#x394;</mi>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <msub>
                          <mi>x</mi>
                          <mi>cam</mi>
                        </msub>
                      </mrow>
                    </mtd>
                  </mtr>
                  <mtr>
                    <mtd>
                      <mrow>
                        <mi>&#x394;</mi>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <msub>
                          <mi>y</mi>
                          <mi>cam</mi>
                        </msub>
                      </mrow>
                    </mtd>
                  </mtr>
                  <mtr>
                    <mtd>
                      <mrow>
                        <mi>&#x394;</mi>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <msub>
                          <mi>z</mi>
                          <mi>cam</mi>
                        </msub>
                      </mrow>
                    </mtd>
                  </mtr>
                </mtable>
                <mo>]</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>equation</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>14</mn>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mstyle>
          <mspace width="4.4em" height="4.4ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mrow>
          <mrow>
            <mo>[</mo>
            <mtable>
              <mtr>
                <mtd>
                  <msubsup>
                    <mi>U</mi>
                    <mi>L</mi>
                    <mi>&#x2032;</mi>
                  </msubsup>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msubsup>
                    <mi>V</mi>
                    <mi>L</mi>
                    <mi>&#x2032;</mi>
                  </msubsup>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mi>f</mi>
                </mtd>
              </mtr>
            </mtable>
            <mo>]</mo>
          </mrow>
          <mo>=</mo>
          <mrow>
            <mo>[</mo>
            <mtable>
              <mtr>
                <mtd>
                  <mrow>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <msub>
                          <mi>U</mi>
                          <mi>L</mi>
                        </msub>
                        <mo>-</mo>
                        <mi>U_SIZE</mi>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                    <mo>&#xd7;</mo>
                    <mi>Pixel_SIZE</mi>
                  </mrow>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mrow>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <msub>
                          <mi>V</mi>
                          <mi>L</mi>
                        </msub>
                        <mo>-</mo>
                        <mi>V_SIZE</mi>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                    <mo>&#xd7;</mo>
                    <mi>Pixel_SIZE</mi>
                  </mrow>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mi>f</mi>
                </mtd>
              </mtr>
            </mtable>
            <mo>]</mo>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>equation</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>15</mn>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0319" num="0326">Here, U_SIZE is a horizontal CCD (Charge Coupled Devices) pixel size, for example, in NTSC (National Television Standards Committee) camera, 640 [pixel]; V_SIZE is a vertical CCD pixel size, similarly, 480 [pixel]; (UL,VL) is the position of the measurement image point on the image plane, and Pixel_SIZE is the size of the pixel, for example, some tens [&#x3bc;m] in a square CCD element.</p>
<p id="p-0320" num="0327">Then, a LOS vector passing through this measurement image point (NL, UL, EL) and the center of the camera is represented by the following equations 16, 17, 18, and 19.</p>
<p id="p-0321" num="0328">
<maths id="MATH-US-00013" num="00013">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mo>[</mo>
        <mrow>
          <mi>Expression</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>8</mn>
        </mrow>
        <mo>]</mo>
      </mrow>
    </mtd>
    <mtd>
      <mstyle>
        <mspace width="0.3em" height="0.3ex"/>
      </mstyle>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mfrac>
          <mrow>
            <mi>x</mi>
            <mo>-</mo>
            <msub>
              <mi>N</mi>
              <mrow>
                <mi>cam</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mn>0</mn>
              </mrow>
            </msub>
          </mrow>
          <mi>&#x3bb;</mi>
        </mfrac>
        <mo>=</mo>
        <mrow>
          <mfrac>
            <mrow>
              <mi>y</mi>
              <mo>-</mo>
              <msub>
                <mi>U</mi>
                <mrow>
                  <mi>cam</mi>
                  <mo>&#x2062;</mo>
                  <mstyle>
                    <mspace width="0.3em" height="0.3ex"/>
                  </mstyle>
                  <mo>&#x2062;</mo>
                  <mn>0</mn>
                </mrow>
              </msub>
            </mrow>
            <mi>&#x3bc;</mi>
          </mfrac>
          <mo>=</mo>
          <mfrac>
            <mrow>
              <mi>z</mi>
              <mo>-</mo>
              <msub>
                <mi>E</mi>
                <mrow>
                  <mi>cam</mi>
                  <mo>&#x2062;</mo>
                  <mstyle>
                    <mspace width="0.3em" height="0.3ex"/>
                  </mstyle>
                  <mo>&#x2062;</mo>
                  <mn>0</mn>
                </mrow>
              </msub>
            </mrow>
            <mi>&#x3c5;</mi>
          </mfrac>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>equation</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>16</mn>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0322" num="0329">However,</p>
<p id="p-0323" num="0330">
<maths id="MATH-US-00014" num="00014">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mi>&#x3bb;</mi>
        <mo>=</mo>
        <mfrac>
          <mrow>
            <msub>
              <mi>N</mi>
              <mi>cam</mi>
            </msub>
            <mo>-</mo>
            <msub>
              <mi>N</mi>
              <mrow>
                <mi>cam</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mn>0</mn>
              </mrow>
            </msub>
          </mrow>
          <msqrt>
            <mrow>
              <msup>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <msub>
                      <mi>N</mi>
                      <mi>L</mi>
                    </msub>
                    <mo>-</mo>
                    <msub>
                      <mi>N</mi>
                      <mrow>
                        <mi>cam</mi>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <mn>0</mn>
                      </mrow>
                    </msub>
                  </mrow>
                  <mo>)</mo>
                </mrow>
                <mn>2</mn>
              </msup>
              <mo>+</mo>
              <msup>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <msub>
                      <mi>U</mi>
                      <mi>L</mi>
                    </msub>
                    <mo>-</mo>
                    <msub>
                      <mi>U</mi>
                      <mrow>
                        <mi>cam</mi>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <mn>0</mn>
                      </mrow>
                    </msub>
                  </mrow>
                  <mo>)</mo>
                </mrow>
                <mn>2</mn>
              </msup>
              <mo>+</mo>
              <msup>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <msub>
                      <mi>E</mi>
                      <mi>L</mi>
                    </msub>
                    <mo>-</mo>
                    <msub>
                      <mi>E</mi>
                      <mrow>
                        <mi>cam</mi>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <mn>0</mn>
                      </mrow>
                    </msub>
                  </mrow>
                  <mo>)</mo>
                </mrow>
                <mn>2</mn>
              </msup>
            </mrow>
          </msqrt>
        </mfrac>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>equation</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>17</mn>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mi>&#x3bc;</mi>
        <mo>=</mo>
        <mfrac>
          <mrow>
            <msub>
              <mi>U</mi>
              <mi>L</mi>
            </msub>
            <mo>-</mo>
            <msub>
              <mi>U</mi>
              <mrow>
                <mi>cam</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mn>0</mn>
              </mrow>
            </msub>
          </mrow>
          <msqrt>
            <mrow>
              <msup>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <msub>
                      <mi>N</mi>
                      <mi>L</mi>
                    </msub>
                    <mo>-</mo>
                    <msub>
                      <mi>N</mi>
                      <mrow>
                        <mi>cam</mi>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <mn>0</mn>
                      </mrow>
                    </msub>
                  </mrow>
                  <mo>)</mo>
                </mrow>
                <mn>2</mn>
              </msup>
              <mo>+</mo>
              <msup>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <msub>
                      <mi>U</mi>
                      <mi>L</mi>
                    </msub>
                    <mo>-</mo>
                    <msub>
                      <mi>U</mi>
                      <mrow>
                        <mi>cam</mi>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <mn>0</mn>
                      </mrow>
                    </msub>
                  </mrow>
                  <mo>)</mo>
                </mrow>
                <mn>2</mn>
              </msup>
              <mo>+</mo>
              <msup>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <msub>
                      <mi>E</mi>
                      <mi>L</mi>
                    </msub>
                    <mo>-</mo>
                    <msub>
                      <mi>E</mi>
                      <mrow>
                        <mi>cam</mi>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <mn>0</mn>
                      </mrow>
                    </msub>
                  </mrow>
                  <mo>)</mo>
                </mrow>
                <mn>2</mn>
              </msup>
            </mrow>
          </msqrt>
        </mfrac>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>equation</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>18</mn>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mi>&#x3c5;</mi>
        <mo>=</mo>
        <mfrac>
          <mrow>
            <msub>
              <mi>E</mi>
              <mi>L</mi>
            </msub>
            <mo>-</mo>
            <msub>
              <mi>E</mi>
              <mrow>
                <mi>cam</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mn>0</mn>
              </mrow>
            </msub>
          </mrow>
          <msqrt>
            <mrow>
              <msup>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <msub>
                      <mi>N</mi>
                      <mi>L</mi>
                    </msub>
                    <mo>-</mo>
                    <msub>
                      <mi>N</mi>
                      <mrow>
                        <mi>cam</mi>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <mn>0</mn>
                      </mrow>
                    </msub>
                  </mrow>
                  <mo>)</mo>
                </mrow>
                <mn>2</mn>
              </msup>
              <mo>+</mo>
              <msup>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <msub>
                      <mi>U</mi>
                      <mi>L</mi>
                    </msub>
                    <mo>-</mo>
                    <msub>
                      <mi>U</mi>
                      <mrow>
                        <mi>cam</mi>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <mn>0</mn>
                      </mrow>
                    </msub>
                  </mrow>
                  <mo>)</mo>
                </mrow>
                <mn>2</mn>
              </msup>
              <mo>+</mo>
              <msup>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <msub>
                      <mi>E</mi>
                      <mi>L</mi>
                    </msub>
                    <mo>-</mo>
                    <msub>
                      <mi>E</mi>
                      <mrow>
                        <mi>cam</mi>
                        <mo>&#x2062;</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>&#x2062;</mo>
                        <mn>0</mn>
                      </mrow>
                    </msub>
                  </mrow>
                  <mo>)</mo>
                </mrow>
                <mn>2</mn>
              </msup>
            </mrow>
          </msqrt>
        </mfrac>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>equation</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>19</mn>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0324" num="0331">Next, in <figref idref="DRAWINGS">FIG. 25</figref>, an intersecting point computing process (S<b>505</b>) will be explained.</p>
<p id="p-0325" num="0332">&#x3c;S<b>505</b>: Intersecting Point Calculating Process&#x3e;</p>
<p id="p-0326" num="0333">The road surface model corresponding point searching unit <b>170</b> calculates ENU coordinates of an intersecting point of a plane formed by the three neighboring points extracted by the three neighboring points extracting process (S<b>503</b>) and the LOS vector calculated by the LOS computing process (S<b>504</b>) as the position of the feature captured in the measurement image point specified by the user.</p>
<p id="p-0327" num="0334">Namely, the road surface model corresponding point searching unit <b>170</b> calculates an intersecting point of the plane formed by the three neighboring points represented by the equation 12 and the LOS vector represented by the equations 16 through 19 as the feature position (x, y, z).</p>
<heading id="h-0022" level="1">Embodiment 2</heading>
<p id="p-0328" num="0335">In the second embodiment, a road feature measurement screen <b>400</b>, which is displayed by the image displaying unit <b>341</b> in the image displaying process (S<b>204</b><i>a</i>) of the road feature position measuring process that has been explained in the first embodiment, will be explained.</p>
<p id="p-0329" num="0336"><figref idref="DRAWINGS">FIG. 28</figref> shows a system configuration of a road feature measurement system <b>101</b> and a functional configuration of a road feature measurement apparatus B <b>500</b> according to the second embodiment.</p>
<p id="p-0330" num="0337">The road feature measurement apparatus B <b>500</b> corresponds to the road feature measurement apparatus <b>100</b> that has been explained in the first embodiment, from which the vehicle position and posture (3-axis) computing unit <b>110</b>, the road surface shape model generating unit <b>150</b>, the laser radar position and posture computing unit <b>160</b>, and a part of the feature identification apparatus <b>300</b> (the motion stereo unit <b>310</b>, the moving body removing unit <b>320</b>, and the feature identifying unit <b>330</b>) are removed to the outside.</p>
<p id="p-0331" num="0338">Various data such as vehicle position and posture, camera attachment offset calculated by the vehicle position and posture (3-axis) computing unit <b>110</b>, road surface shape model generated by the road surface shape model generating unit <b>150</b>, image data obtained by the camera <b>230</b>, and feature types, etc. specified by the feature identifying unit <b>330</b> are stored in an observation data memory unit B <b>598</b>, inputted from the observation data memory unit B <b>598</b> to a road feature measurement apparatus B <b>500</b>, and stored in the observation data memory unit <b>199</b>. Physical storage medium such as a hard disk drive, a DVD, and a USB memory, etc. are examples of the observation data memory unit B <b>598</b>. Further, the input of each data from the observation data memory unit B <b>598</b> to the road feature measurement apparatus B <b>500</b> is done by directly reading from the storage medium, or by communication via network such as the Internet, LAN (local area network), etc.</p>
<p id="p-0332" num="0339">Using a function of an OS <b>921</b> (or a browser), the image displaying unit <b>341</b> displays the image captured by the camera <b>230</b> (a captured image <b>401</b>, hereinafter), the feature position calculated by the road surface model corresponding point searching unit <b>170</b>, etc. on the displaying device <b>901</b>.</p>
<p id="p-0333" num="0340">Using a function of the OS <b>921</b> (or the browser), the image point inputting unit <b>342</b> inputs information (an measurement image point, for example) specified by the user using an inputting device such as a mouse <b>903</b>, a keyboard <b>902</b>, etc.</p>
<p id="p-0334" num="0341">The road surface model corresponding point searching unit <b>170</b> stores the feature position calculated as three-dimensional coordinate values corresponding to the measurement image point in the measurement position data memory unit <b>599</b>. Further, the road surface model corresponding point searching unit <b>170</b> stores the measurement image point, a specified image number for identifying the captured image <b>401</b> in which the measurement image point is specified, and the feature type for identifying a type of the feature specified by the measurement image point with relating to the feature position in the measurement position data memory unit <b>599</b>. Hereinafter, the feature position, the measurement image point, the specified image number, and the feature type which are stored with relating to each other are referred to as measurement position data.</p>
<p id="p-0335" num="0342"><figref idref="DRAWINGS">FIG. 29</figref> shows the road feature measurement screen <b>400</b> according to the second embodiment.</p>
<p id="p-0336" num="0343">The image displaying unit <b>341</b> displays the road feature measurement screen <b>400</b> as shown in <figref idref="DRAWINGS">FIG. 29</figref> on the displaying device <b>901</b> of the personal computer which functions as the road feature measurement apparatus B <b>500</b> in the image displaying process (S<b>204</b><i>a</i>) that has been explained in the first embodiment. The road feature measurement screen <b>400</b> is a user interface (man-machine interface) to prompt the user to specify a measurement image point and provide the measured result (feature position) to the user.</p>
<p id="p-0337" num="0344">Hereinafter, configurational elements of the road feature measurement screen <b>400</b> and the operation of the image displaying unit <b>341</b> will be explained with reference to <figref idref="DRAWINGS">FIG. 29</figref>.</p>
<p id="p-0338" num="0345">The road feature measurement screen <b>400</b> includes a captured image <b>401</b>.</p>
<p id="p-0339" num="0346">Further, the road feature measurement screen <b>400</b> includes textboxes showing a specific image number <b>411</b>, a measurement image point <b>412</b>, and a feature position <b>414</b>, respectively.</p>
<p id="p-0340" num="0347">Further, the road feature measurement screen <b>400</b> includes a type list box <b>417</b> showing a list of feature types.</p>
<p id="p-0341" num="0348">Further, the road feature measurement screen <b>400</b> includes a calculation requesting button <b>415</b> to be pressed on requesting calculation of the feature position <b>414</b> and a storage requesting button <b>416</b> to be pressed on requesting storage of the measurement position data.</p>
<p id="p-0342" num="0349">The road feature measurement screen <b>400</b> is displayed on the displaying device <b>901</b> by the image displaying unit <b>341</b>.</p>
<p id="p-0343" num="0350">The image displaying unit <b>341</b> displays the captured image <b>401</b> selected by the user on the road feature measurement screen <b>400</b>.</p>
<p id="p-0344" num="0351">For example, before displaying the road feature measurement screen <b>400</b>, the image displaying unit <b>341</b> displays an image selecting screen to prompt the user to select the captured image <b>401</b> on the displaying device <b>901</b>. For example, the image selecting screen is display of a list of image numbers of each of the captured image <b>401</b>, or display of a list of thumbnails (minified image) of each of the captured image <b>401</b>. The image displaying unit <b>341</b> displays the list of image numbers or the list of thumbnails that cannot be displayed on the screen of the displaying device <b>901</b> by scrolling according the mouse operation of the user. Using the mouse <b>903</b>, the user selects one desired image from a plurality of the captured images <b>401</b> displayed on the image selecting screen. Then, the image displaying unit <b>341</b> obtains the captured image <b>401</b> (image data) selected by the user from the observation data memory unit <b>199</b> and displays on the road feature measurement screen <b>400</b>. Further, the image displaying unit <b>341</b> displays the image number of the captured image <b>401</b> selected by the user in the textbox of the road feature measurement screen <b>400</b> as the specific image number <b>411</b>.</p>
<p id="p-0345" num="0352">Further, for example, the user can specify the desired captured image <b>401</b> by entering the image number in the textbox of the specific image number <b>411</b> in the road feature measurement screen <b>400</b> using the keyboard <b>902</b>. The image displaying unit <b>341</b> obtains the captured image <b>401</b> (image data) identified by the image number which is entered in the textbox of the specific image number <b>411</b> from the observation data memory unit <b>199</b> and displays on the road feature measurement screen <b>400</b>.</p>
<p id="p-0346" num="0353">In <figref idref="DRAWINGS">FIG. 29</figref>, the captured image <b>401</b> which is identified by the image number &#x201c;nnnnn&#x201d; is displayed. In the captured image <b>401</b> displayed, a street <b>405</b> having two lanes and sidewalks <b>404</b> provided at both sides of the street <b>405</b> are shown. A center line <b>406</b> and two white lines <b>407</b> are drawn on the street <b>405</b>. Further, road signs <b>408</b> and electric poles <b>409</b> are provided vertically along the sidewalk <b>404</b> which is shown in the right side of the screen, and a km post <b>403</b> (kilo-post) is provided vertically along the sidewalk <b>404</b> which is shown in the left side of the screen.</p>
<p id="p-0347" num="0354">The image displaying unit <b>341</b> displays the measurement image point <b>412</b> specified by the user on the road feature measurement screen <b>400</b>.</p>
<p id="p-0348" num="0355">For example, by operating the mouse <b>903</b>, the user moves a mouse cursor <b>402</b> to the desired position on the captured image <b>401</b>, and specifies the measurement image point <b>412</b> by clicking the mouse <b>903</b>. The image displaying unit <b>341</b> displays coordinates (U2,V2) on the captured image <b>401</b> specified with the mouse cursor <b>402</b> when the mouse <b>903</b> is clicked as the measurement image point <b>412</b> on the road feature measurement screen <b>400</b>.</p>
<p id="p-0349" num="0356">Further, for example, the user can enter in the textbox of the road feature measurement screen <b>400</b> the desired coordinates on the captured image <b>401</b> as the measurement image point <b>412</b> using the keyboard <b>902</b>.</p>
<p id="p-0350" num="0357">The image displaying unit <b>341</b> clearly shows the feature type <b>413</b> specified in the type list box <b>417</b> by the user.</p>
<p id="p-0351" num="0358">For example, the user selects one feature type corresponding to the measurement image point <b>412</b> from a plurality of feature types shown in the type list box <b>417</b>, moves the mouse cursor <b>402</b> to the above of the selected feature type <b>413</b>, and clicks the mouse <b>903</b> to specify the feature type <b>413</b>. The image displaying unit <b>341</b> changes the background color of the feature type <b>413</b> specified.</p>
<p id="p-0352" num="0359">In <figref idref="DRAWINGS">FIG. 29</figref>, from &#x201c;left edge of left white line&#x201d;, &#x201c;left edge of right white line&#x201d;, &#x201c;right edge of left white line&#x201d;, &#x201c;right edge of right white line&#x201d;, &#x201c;regulatory sign (blue circle)&#x201d;, &#x201c;regulatory sign (red circle)&#x201d;, &#x201c;warning sign (yellow triangle)&#x201d;, &#x201c;indication sign (blue square)&#x201d;, &#x201c;crosswalk sign (blue triangle)&#x201d;, &#x201c;information sign (green square)&#x201d;, etc., &#x201c;km post&#x201d; is specified as the feature type <b>413</b>.</p>
<p id="p-0353" num="0360">Further, instead of selection using the type list box <b>417</b>, for example, the feature type <b>413</b> can be directly entered in a textbox prepared separately by the user using the keyboard <b>902</b>.</p>
<p id="p-0354" num="0361">The image displaying unit <b>341</b> displays the feature position <b>414</b> calculated by the road surface model corresponding point searching unit <b>170</b> on the road feature measurement screen <b>400</b>.</p>
<p id="p-0355" num="0362">For example, when the user wants to obtain the feature position <b>414</b> corresponding to the measurement image point <b>412</b> specified, the user moves the mouse <b>903</b> to place the mouse cursor <b>402</b> above the calculation requesting button <b>415</b>, and presses the calculation requesting button <b>415</b> by clicking the mouse <b>903</b>. When the calculation requesting button <b>415</b> is pressed, the image point inputting unit <b>342</b> inputs the specific image number <b>411</b>, the measurement image point <b>412</b>, and the feature type <b>413</b> from the OS <b>921</b> or the browser. Then, the image point inputting unit <b>342</b> outputs the measurement image point <b>412</b> to the camera LOS computing unit <b>140</b> and the measurement position data memory unit <b>599</b>, outputs the specific image number <b>411</b> to the camera LOS computing unit <b>140</b>, the road surface model corresponding point searching unit <b>170</b> and the measurement position data memory unit <b>599</b>, and outputs the feature type <b>413</b> to the measurement position data memory unit <b>599</b>. Then, the road surface model corresponding point searching unit <b>170</b> calculates the feature position <b>414</b> corresponding to the measurement image point <b>412</b> by the feature position locating process (S<b>106</b>). Further, the image displaying unit <b>341</b> displays the feature position <b>414</b> calculated by the road surface model corresponding point searching unit <b>170</b> on the road feature measurement screen <b>400</b>.</p>
<p id="p-0356" num="0363">Further, when the user wants to store the measurement position data including the feature position <b>414</b>, the user moves the mouse <b>903</b> to place the mouse cursor <b>402</b> above the storage requesting button <b>416</b>, and presses the storage requesting button <b>416</b> by clicking the mouse <b>903</b>. When the storage requesting button <b>416</b> is pressed, the measurement position data memory unit <b>599</b> stores the feature position <b>414</b>, the specific image number <b>411</b>, the measurement image point <b>412</b>, and the feature type <b>413</b> with linking each other as the measurement position data. The linking (making correspondence) means relating each other addresses of storage areas where respective data are stored.</p>
<p id="p-0357" num="0364">Namely, the image displaying unit <b>341</b> displays, out of the image data in time series stored in the observation data memory unit <b>199</b>, the image data specified by scrolling selection with the mouse operation of the user on the screen of a personal computer (the road feature measurement screen <b>400</b>) (a PC screen, hereinafter) which functions as the road feature measurement apparatus B <b>500</b>.</p>
<p id="p-0358" num="0365">Further, the image point inputting unit <b>342</b> obtains the measurement image point <b>412</b> specified by the mouse operation of the user, the specific image number <b>411</b> of the image for which scroll selection is done by the mouse operation on the PC screen, and a feature ID (the feature type <b>413</b>) showing a type of the feature.</p>
<p id="p-0359" num="0366">Further, the road surface model corresponding point searching unit <b>170</b> calculates the feature position <b>414</b> (output data) based on the data stored in the observation data memory unit <b>199</b> such as data of the vehicle position and posture, the camera attachment offset, the road surface shape model, etc. related by the image-capturing time, the measurement image point <b>412</b>, and the specific image number <b>411</b>.</p>
<p id="p-0360" num="0367">Then, the image displaying unit <b>341</b> displays the feature position <b>414</b> on the PC screen, and the measurement position data memory unit <b>599</b> stores the feature position <b>414</b>, the measurement image point <b>412</b>, the specified image number <b>411</b>, and the feature type <b>413</b> with relating each other as the measurement position data.</p>
<p id="p-0361" num="0368">The image displaying unit <b>341</b> also can display the measured result by sentences using the feature type <b>413</b> and the feature position <b>414</b> such as &#x201c;the coordinates of &#x201c;aaa&#x201d; are (x, y, z)&#x201d;. The feature type <b>413</b> is set to &#x201c;aaa&#x201d;, and the feature position <b>414</b> is set to &#x201c;x, y, z&#x201d;.</p>
<p id="p-0362" num="0369">The measurement apparatus such as follows has been discussed in the second embodiment.</p>
<p id="p-0363" num="0370">The measurement apparatus (the road feature measurement apparatus B <b>500</b>, for example) includes the image memory unit (the observation data memory unit <b>199</b>), the three-dimensional point cloud model memory unit (the observation data memory unit <b>199</b>), the image displaying unit, the measurement image point obtaining unit (the image point inputting unit <b>342</b>), and the position calculating unit (the road surface model corresponding point searching unit <b>170</b>).</p>
<p id="p-0364" num="0371">The image memory unit stores images captured by the camera <b>230</b> (the captured image <b>401</b>).</p>
<p id="p-0365" num="0372">The three-dimensional point cloud model memory unit stores the three-dimensional point cloud model (road surface shape model) formed by a point cloud which is an image-capturing point captured by the camera measured by the laser device and also a position of each point cloud is known.</p>
<p id="p-0366" num="0373">The image displaying unit displays the image stored in the image memory unit on the screen of the displaying device and prompts the user to specify the position within the image. The position within the image is two dimensional coordinates position (u, v) of a pixel on the image plane.</p>
<p id="p-0367" num="0374">The measurement image point obtaining unit inputs the position within the image specified by the user as the measurement image point using the inputting device.</p>
<p id="p-0368" num="0375">The position calculating unit detects, from the point cloud of the three-dimensional point cloud model stored by the three-dimensional point cloud model memory unit, a corresponding point (neighboring point) corresponding to the measurement image point obtained by the measurement image point obtaining unit, and, using the position of the corresponding point detected, decides a three-dimensional position of the measurement image point (the feature position <b>414</b>) obtained by the measurement image point obtaining unit.</p>
<p id="p-0369" num="0376">Further, the image displaying unit makes a list of a plurality of images stored in the image memory unit to display on the screen, prompts the user to specify the image, displays the image specified by the user on the screen of the displaying device and prompts the user to specify a position within the image.</p>
<p id="p-0370" num="0377">Further, the measurement apparatus includes the result displaying unit (the image displaying unit <b>341</b>) for displaying the position of the measurement image point decided by the position calculating unit on the screen of the displaying device by which the image displaying unit displays the image.</p>
<p id="p-0371" num="0378">Further, the measurement apparatus includes a type inputting unit (the image point inputting unit) for making the user specify a type of the feature to be a target of position measurement and inputting the type of the feature specified by the user from the inputting device.</p>
<p id="p-0372" num="0379">Further, the measurement apparatus includes a result memory unit (the measurement position data memory unit <b>599</b>) for storing the measurement image point obtained by the measurement image point obtaining unit, the position of the measurement image point decided by the position calculating unit, and the type of feature inputted by the type inputting unit with relating each other in the storage equipment.</p>
<p id="p-0373" num="0380">According to the above measurement apparatus (the road feature measurement apparatus B <b>500</b>, for example), conventional survey work in which a worker goes to the actual place to be measured and carries out survey for each measurement point is unnecessary, and it is possible to carry out survey by only clicking on a PC screen using automatically measured data obtained during running of the vehicle. Consequently, the above measurement apparatus can obtain a large effect in various industry related to the survey. For example, it is possible to largely reduce time and cost for the survey work. Further, for example, conventionally, survey result of the measurement point is noted on paper, etc. by the worker; however, in the above measurement apparatus, since the survey result (feature position) is stored automatically in a hard disk of the PC, transcription error can be prevented, and the reliability of the survey result is improved.</p>
<heading id="h-0023" level="1">Embodiment 3</heading>
<p id="p-0374" num="0381">In the third embodiment, a feature position locating process (S<b>106</b>) performed by the road surface model corresponding point searching unit <b>170</b> will be explained.</p>
<p id="p-0375" num="0382">Items being different from the first embodiment will be mainly discussed in the following, and items for which explanation is omitted can be considered as the same as the first embodiment.</p>
<p id="p-0376" num="0383"><figref idref="DRAWINGS">FIG. 30</figref> shows a functional configuration of the road surface model corresponding point searching unit <b>170</b> according to the third embodiment.</p>
<p id="p-0377" num="0384">As shown in <figref idref="DRAWINGS">FIG. 30</figref>, the road surface model corresponding point searching unit <b>170</b> includes a neighborhood extracting unit <b>171</b>, a model projecting unit <b>172</b>, a neighboring plane calculating unit <b>173</b>, and a feature position calculating unit <b>174</b>.</p>
<p id="p-0378" num="0385">The model projecting unit <b>172</b> projects the point cloud of the road surface shape model on the image plane as explained in the first embodiment.</p>
<p id="p-0379" num="0386">The neighborhood extracting unit <b>171</b> extracts one neighboring point of the measurement image point from the point cloud of the road surface shape model projected on the image plane by the model projecting unit <b>172</b>.</p>
<p id="p-0380" num="0387">The neighboring plane calculating unit <b>173</b> calculates a particular plane (neighboring plane) including the one neighboring point extracted by the neighborhood extracting unit <b>171</b>.</p>
<p id="p-0381" num="0388">The feature position calculating unit <b>174</b> calculates an intersecting point of the particular plane calculated by the neighboring plane calculating unit <b>173</b> and the LOS vector calculated by the camera LOS computing unit <b>140</b> (the vector calculating unit) as a feature position (a three-dimensional position of the measurement image point).</p>
<p id="p-0382" num="0389"><figref idref="DRAWINGS">FIG. 31</figref> is a flowchart showing a flow of the feature position locating process (S<b>106</b>) according to the third embodiment, which corresponds to <figref idref="DRAWINGS">FIG. 25</figref> in the first embodiment.</p>
<p id="p-0383" num="0390">As shown in <figref idref="DRAWINGS">FIG. 31</figref>, in the feature position locating process (S<b>106</b>) in the third embodiment, instead of the three neighboring points extracting process (S<b>503</b>) of the first embodiment, the neighborhood extracting process (S<b>503</b>B<b>1</b>) and the neighboring plane calculating process (S<b>503</b>B<b>2</b>) are performed. Other processes are the same as the first embodiment.</p>
<p id="p-0384" num="0391"><figref idref="DRAWINGS">FIG. 32</figref> shows a method for calculating the feature position of the feature position locating process (S<b>106</b>) according to the third embodiment, which corresponds to <figref idref="DRAWINGS">FIG. 24</figref> in the first embodiment.</p>
<p id="p-0385" num="0392">The neighborhood extracting process (S<b>503</b>B<b>1</b>) and the neighboring plane calculating process (S<b>503</b>B<b>2</b>) will be explained in the following with reference to <figref idref="DRAWINGS">FIG. 32</figref>.</p>
<p id="p-0386" num="0393">Here, the laser measured point cloud (a black circle in <figref idref="DRAWINGS">FIG. 32</figref>) of the road surface shape model is projected on the image plane (an image-capturing plane of the camera <b>230</b>) by a model projecting unit <b>172</b> in the 3D model projecting process (S<b>502</b>).</p>
<p id="p-0387" num="0394">In the neighborhood extracting process (S<b>503</b>B<b>1</b>), the neighborhood extracting unit <b>171</b> discriminates a laser measured point which is close to the measurement image point on the image plane as a neighboring point P<b>1</b>. For example, the neighboring point P<b>1</b> is the laser measured point which is the closest to the measurement image point on the image plane, the laser measured point which is the closest to the measurement image point in the horizontal axial direction of the image plane, or the laser measured point which is the closest to the measurement image point in the vertical axial direction of the image plane. Further, for example, the neighboring point P<b>1</b> is either of a plurality of the laser measured points projected within the predetermined range from the measurement image point on the image plane.</p>
<p id="p-0388" num="0395">In <figref idref="DRAWINGS">FIG. 32</figref>, the point which projects the neighboring point P<b>1</b> on the image plane is shown by &#x201c;P<b>1</b>&#x2032;&#x201d;. Further, three-dimensional coordinates of the neighboring point P<b>1</b> show (x0, y0, z0) in the ENU coordinate system.</p>
<p id="p-0389" num="0396">Then, in the neighboring plane calculating process (S<b>503</b>B<b>2</b>), the neighboring plane calculating unit <b>173</b> calculates a horizontal plane (U=z0) having the same height (in the direction of U coordinate axis) with the neighboring point P<b>1</b> as a neighboring plane. The neighboring plane includes the neighboring point P<b>1</b>, and is a plane orthogonal to the U coordinate axis.</p>
<p id="p-0390" num="0397">After the neighboring plane calculating process (S<b>503</b>B<b>2</b>), the feature position calculating unit <b>174</b> calculates an intersecting point (x, y, z0) of the neighboring plane calculated by the neighboring plane calculating process (S<b>503</b>B<b>2</b>) and the LOS vector directing to the measurement image point from the center of the camera calculated by the LOS computing process (S<b>504</b>) as the feature position <b>414</b> (the intersecting computing process [S<b>505</b>]).</p>
<p id="p-0391" num="0398">The neighboring plane calculated by the neighboring plane calculating unit <b>173</b> shows road surface (the sidewalk <b>404</b> or the street <b>405</b>) where the feature specified as the measurement image point <b>412</b> is located. This is because it can be considered that the road surface is almost flat between the neighboring point P<b>1</b> and the feature corresponding to the measurement image point, since the road surface is not largely slanted. Therefore, the intersecting point of the flat neighboring plane including the neighboring point P<b>1</b> and the LOS vector of the measurement image point can be considered as the position on the road of the feature corresponding to the measurement image point <b>412</b>.</p>
<p id="p-0392" num="0399">When the feature for the measurement target is located not on the road but on an approximately vertical wall, etc., the neighboring plane can be calculated as a vertical plane which is orthogonal to the N coordinate axis or the E coordinate axis.</p>
<p id="p-0393" num="0400">Further, when the feature for the measurement target is located not on the road but on a slope of a mountain, etc., the neighboring plane can be calculated as a plane which has the same gradient with the slope of the mountain.</p>
<p id="p-0394" num="0401">It is possible to decide whether the neighboring plane is represented by a horizontal plane, a vertical plane or a slope by deciding the plane formed by the feature and determining the type of the plane with the feature identifying unit <b>330</b> based on the feature type of the feature represented by a point cloud including the neighboring point P<b>1</b> among the feature types determined by the feature identifying unit <b>330</b>.</p>
<p id="p-0395" num="0402">Further, in the three neighboring points extracting process (S<b>503</b>) of the first embodiment, it is also possible to set a value (x0+n, y0, z0) obtained by adding a predetermined value n to the value of the E coordinate axis of the neighboring point P<b>1</b> as a coordinate value of the neighboring point P<b>2</b>, and set a value (x0, y0+n, z0) obtained by adding a predetermined value n to the value of the N coordinate axis of the neighboring point P<b>1</b> as a coordinate value of a neighboring point P<b>3</b>. After the three neighboring points extracting process (S<b>503</b>), in the intersecting point computing process (S<b>505</b>), the feature position calculating unit <b>174</b> calculates an intersecting point of a plane including the neighboring points P<b>1</b>, P<b>2</b>, and P<b>3</b> and the LOS vector of the measurement image point as the feature position <b>414</b>. The plane including the neighboring points P<b>1</b>, P<b>2</b>, and P<b>3</b> corresponds to the neighboring plane.</p>
<p id="p-0396" num="0403">In the third embodiment, the following measurement apparatus has been explained.</p>
<p id="p-0397" num="0404">The measurement apparatus (the road feature measurement apparatus <b>100</b>, for example) includes a measurement image point obtaining unit (the image point inputting unit <b>342</b>), a vector calculating unit (the camera LOS computing unit <b>140</b>), the neighborhood extracting unit <b>171</b>, the neighboring plane calculating unit <b>173</b>, and the feature position calculating unit <b>174</b>.</p>
<p id="p-0398" num="0405">The measurement image point obtaining unit displays the image captured by the camera on the displaying device and inputs a position within the image specified by the user as a target of position measurement from the inputting device.</p>
<p id="p-0399" num="0406">The vector calculating unit calculates a vector (a LOS vector) showing a direction from the center of the camera to the measurement image point inputted by the measurement image point obtaining unit.</p>
<p id="p-0400" num="0407">The neighborhood extracting unit <b>171</b> extracts one neighboring point (at least one or only one) of the measurement image point from the point cloud of the three-dimensional point cloud model.</p>
<p id="p-0401" num="0408">The neighboring plane calculating unit <b>173</b> calculates a particular plane (neighboring plane) including the one neighboring point extracted by the neighborhood extracting unit <b>171</b>.</p>
<p id="p-0402" num="0409">The feature position calculating unit <b>174</b> calculates an intersecting point of the particular plane calculated by the neighboring plane calculating unit <b>173</b> and the vector calculated by the vector calculating unit as a three-dimensional position (the feature position) of the measurement image point.</p>
<p id="p-0403" num="0410">The measurement apparatus further includes the model projecting unit <b>172</b> projecting the point cloud of the three-dimensional point cloud model on the image-capturing plane of the camera corresponding to the image.</p>
<p id="p-0404" num="0411">Then, the neighborhood extracting unit <b>171</b> extracts, among the point clouds of the three-dimensional point cloud model projected on the image-capturing plane by the model projecting unit <b>172</b>, one of the closest point from the measurement image point within the image-capturing plane, the closest point from the measurement image point in the horizontal axial direction of the image-capturing plane, and the closest point from the measurement image point in the vertical axial of the image-capturing plane as the one neighboring point.</p>
<p id="p-0405" num="0412">The neighboring plane calculating unit <b>173</b> calculates a horizontal plane including the one neighboring point of the measurement image point as the particular plane.</p>
<p id="p-0406" num="0413">The neighboring plane calculating unit <b>173</b> calculates a plane including the one neighboring point of the measurement image point and also orthogonal to one of an X axis (E axis), a Y axis (N axis), and a Z axis (U axis) in the X-Y-Z coordinate system (ENU coordinate system, for example) showing a coordinate system used for the three-dimensional point cloud model as the particular plane.</p>
<p id="p-0407" num="0414">The measurement apparatus further includes a type inputting unit (the image point inputting unit <b>342</b>) prompting the user to specify a type of the feature for the target of position measurement and inputting the type of the feature specified by the user from the inputting device.</p>
<p id="p-0408" num="0415">The neighboring plane calculating unit <b>173</b> calculates the particular plane by representing a plane formed by the feature represented by the point cloud included in the one neighboring point of the measurement image point based on the type of the feature inputted by the type inputting unit.</p>
<p id="p-0409" num="0416">In the road feature measurement apparatus B <b>500</b> explained in the second embodiment, the road surface model corresponding point searching unit <b>170</b> can perform the neighborhood extracting process (S<b>503</b>B<b>1</b>) and the neighboring plane calculating process (S<b>503</b>B<b>2</b>) instead of the three neighboring points extracting process (S<b>503</b>).</p>
<p id="p-0410" num="0417">The road feature measurement system <b>101</b> and the road feature measurement apparatus <b>100</b> (or the road feature measurement apparatus B <b>500</b>) can combine respective items that have been explained in the first through third embodiment and allow them to selectively operate.</p>
<p id="p-0411" num="0418">In the first through third embodiments, the following road feature measurement system <b>101</b> has been explained:</p>
<p id="p-0412" num="0419">The road feature measurement system <b>101</b> measures the position of feature around the road using the orientation/distance data (the three-dimensional point cloud data, the road surface shape model) and the image data (the captured image <b>401</b>) around the road obtained by the three-dimensional feature position measuring carriage (the MMS, the measuring carriage <b>102</b>).</p>
<p id="p-0413" num="0420">The road feature measurement system <b>101</b> measures the three-dimensional position of the feature around the road for the image data (the captured image <b>401</b>) shown within the terminal screen (the road feature measurement screen <b>400</b>) by relating the two-dimensional displaying point (the measurement image point <b>412</b>) obtained by clicking the position of feature to the three-dimensional point cloud data within the terminal screen.</p>
<p id="p-0414" num="0421">For example, in the measuring carriage <b>102</b>, the camera <b>230</b> captures an image of the road and obtains image data of the road, and the laser radar <b>240</b> obtains the orientation/distance data for the feature around the road. Further, the road surface shape model generating unit <b>150</b> generates the three-dimensional model of the stationary body captured in the image, compares the three-dimensional model of the stationary body captured in the image with the road surface shape model based on the orientation/distance data, and generates the road surface shape model showing only the stationary body. Then, the road surface model corresponding point searching unit <b>170</b> relates the position of the feature specified using a mouse, etc. on the road image captured by the camera to the point cloud data of the road surface shape model, and measures the three-dimensional position of the specified point based on a line of sight vector of the camera <b>230</b>.</p>
<heading id="h-0024" level="1">Embodiment 4</heading>
<p id="p-0415" num="0422">A CAD apparatus <b>600</b> (CAD: Computer Aided Design), in which the road feature measurement apparatus B <b>500</b> explained in the second embodiment is installed, will be explained.</p>
<p id="p-0416" num="0423">For example, the CAD apparatus <b>600</b> drafts roads and generates road data (plotting data) representing the drafted road. Maps used in the road management ledger and the car navigation system are examples of the road data (plotting data).</p>
<p id="p-0417" num="0424">Items being different from the second embodiment will be mainly discussed in the following, and items for which explanation is omitted can be considered as the same as the second embodiment.</p>
<p id="p-0418" num="0425"><figref idref="DRAWINGS">FIG. 33</figref> shows a system configuration of the road feature measurement system <b>101</b> and a functional configuration of the CAD apparatus <b>600</b> according to the fourth embodiment.</p>
<p id="p-0419" num="0426">In <figref idref="DRAWINGS">FIG. 33</figref>, in addition to the configuration of the road feature measurement apparatus B <b>500</b> explained in the second embodiment, the CAD apparatus <b>600</b> includes a drafting unit <b>610</b> and a CAD memory unit <b>699</b>. For example, the CAD apparatus <b>600</b> is a CAD for generating attached maps of the road management ledger.</p>
<p id="p-0420" num="0427">The drafting unit <b>610</b> (a drawing unit) carries out a function of the CAD. For example, the drafting unit <b>610</b> inputs a plotting command showing contents of image to be generated from the inputting device and draws an image including plural elements on the screen of the displaying device <b>901</b> using the CPU based on the plotting command inputted (a drawing process).</p>
<p id="p-0421" num="0428">Further, the drafting unit <b>610</b> (a plotting unit) prompts the user to specify any of plural elements included in the drawn image, obtains the feature position corresponding to the specified element from the road surface model corresponding point searching unit <b>170</b> (the position calculating unit), and generates plotting data (road data, for example) representing the drawn image and showing the feature position as a three-dimensional position of element specified by the user using the CPU (a plotting process).</p>
<p id="p-0422" num="0429">The CAD memory unit <b>699</b> stores the plotting data generated by the drafting unit <b>610</b> using the storage equipment.</p>
<p id="p-0423" num="0430"><figref idref="DRAWINGS">FIG. 34</figref> shows a CAD screen <b>620</b> according to the fourth embodiment.</p>
<p id="p-0424" num="0431">The drafting unit <b>610</b> and the image displaying unit <b>341</b> display the CAD screen <b>620</b> as shown in <figref idref="DRAWINGS">FIG. 34</figref> on the displaying device <b>901</b> of the CAD apparatus <b>600</b>. For example, the CAD screen <b>620</b> is a screen of the CAD for generating the attached maps of the road management ledger.</p>
<p id="p-0425" num="0432">In the following, the configurational elements of the CAD screen <b>620</b> will be explained with reference to <figref idref="DRAWINGS">FIG. 34</figref>.</p>
<p id="p-0426" num="0433">The CAD screen <b>620</b> includes a figure toolbar <b>621</b>, a road map <b>622</b>, a measurement screen displaying button <b>623</b>, a storage requesting button <b>624</b>, and a road feature measurement screen <b>400</b>.</p>
<p id="p-0427" num="0434">The figure toolbar <b>621</b> shows types of plural figures such as a straight line, a curve, a circle, a polygon, an arrow, etc. and is a figure-drawing toolbar for prompting the user to specify a type of a figure to be drawn.</p>
<p id="p-0428" num="0435">The road map <b>622</b> is an image drawn based on the specification of the user.</p>
<p id="p-0429" num="0436">The measurement screen displaying button <b>623</b> is a button to be pressed at the time of display request of the road feature measurement screen <b>400</b> (refer to the second embodiment).</p>
<p id="p-0430" num="0437">The storage requesting button <b>624</b> is a button to be pressed at the time of storage request of the road map <b>622</b>.</p>
<p id="p-0431" num="0438">The road feature measurement screen <b>400</b> is displayed by arranging side by side with the road map <b>622</b> or superimposing with the road map <b>622</b> when the measurement screen displaying button <b>623</b> is pressed. Here, it is also possible that the road feature measurement screen <b>400</b> has been already displayed.</p>
<p id="p-0432" num="0439">The CAD screen <b>620</b> is displayed on the displaying device <b>901</b> by the drafting unit <b>610</b> and the image displaying unit <b>341</b>.</p>
<p id="p-0433" num="0440">Next, the operation of the drafting unit <b>610</b> and the image displaying unit <b>341</b> displaying the CAD screen <b>620</b> will be explained.</p>
<p id="p-0434" num="0441">For example, the user operates the mouse <b>903</b> to move the mouse cursor <b>402</b> to a part showing a type of the desired figure on the figure toolbar <b>621</b> and specifies the type of figure to be drawn by clicking the mouse <b>903</b>. The drafting unit <b>610</b> draws a figure of the specified type on the CAD screen <b>620</b>. Further, the user operates the mouse <b>903</b> to manipulate the mouse cursor <b>402</b>, and specify figuring operation such as move, zooming up/down, transformation, etc. of the figure displayed on the CAD screen <b>620</b>. The drafting unit <b>610</b> re-draws the figure displayed on the CAD screen <b>620</b> according to the specified figuring operation. By repeating these specifications, the user generates the road map <b>622</b> formed by combining plural figures. The road map <b>622</b> includes plural figures (elements) representing features (the km post <b>625</b>, the road sign <b>626</b>, etc.) spotted around the road. The specification of the type of figures or the specification of the figuring operation by the user is an example of the plotting command showing contents of the image to be generated.</p>
<p id="p-0435" num="0442">Further, when the user wants to set the three-dimensional coordinates of a position where an actual item is located to the feature shown on the road map <b>622</b>, the user operates the mouse <b>903</b> to move the mouse cursor <b>402</b> to above the measurement screen displaying button <b>623</b>, and presses the measurement screen displaying button <b>623</b> by clicking the mouse <b>903</b>. When the measurement screen displaying button <b>623</b> is pressed, the image displaying unit <b>341</b> displays the road feature measurement screen <b>400</b>.</p>
<p id="p-0436" num="0443">In the following, a case in which the user wants to set the three-dimensional coordinates at the km post <b>625</b> of the road map <b>622</b> will be explained.</p>
<p id="p-0437" num="0444">Next, the user specifies the captured image <b>401</b> corresponding to the road map <b>622</b>, and the image displaying unit <b>341</b> displays the captured image <b>401</b> specified on the road feature measurement screen <b>400</b>. Next, the user discriminates visually the km post <b>403</b> from plural features captured in the captured image <b>401</b> and specifies the displayed part of the km post <b>403</b> by the mouse <b>903</b>. Further, the user specifies &#x201c;km post&#x201d; from the type list box <b>417</b> as the feature type <b>413</b> using the mouse <b>903</b>. Then, the user presses the calculation requesting button <b>415</b> using the mouse <b>903</b>, and the road surface model corresponding point searching unit <b>170</b> calculates the three-dimensional coordinates (the feature position) of the specified km post <b>403</b>.</p>
<p id="p-0438" num="0445">Then, the user operates the mouse <b>903</b> to move the mouse cursor <b>402</b> to above the km post <b>625</b> of the road map <b>622</b> and specifies the km post <b>625</b> as the feature corresponding to the three-dimensional coordinates calculated by the road surface model corresponding point searching unit <b>170</b> by clicking the mouse <b>903</b>. The drafting unit <b>610</b> sets the three-dimensional coordinates calculated by the road surface model corresponding point searching unit <b>170</b> and the feature type <b>413</b> specified by the user on the road map <b>622</b> by relating to the specified figure (the km post <b>625</b>). The three-dimensional coordinates and the feature type <b>413</b> can be or do not have to be displayed on the CAD screen <b>620</b>.</p>
<p id="p-0439" num="0446">Further, when the user wants to store the road data representing the road map <b>622</b>, the user operates the mouse <b>903</b> to move the mouse cursor <b>402</b> to above the storage requesting button <b>624</b> and presses the storage requesting button <b>624</b> by clicking the mouse <b>903</b>. When the storage request button is pressed, the drafting unit <b>610</b> generates the road data representing the road map <b>622</b> and stores the generated road data in the CAD memory unit <b>699</b>. In the road data, the three-dimensional coordinates calculated by the road surface model corresponding point searching unit <b>170</b> is set as the three-dimensional coordinates of the km post <b>625</b>. Further, in the road data, the feature type <b>413</b> specified by the user is set as the type of the km post <b>625</b>.</p>
<p id="p-0440" num="0447">Namely, the drafting unit <b>610</b> generates the road data by setting the feature position obtained by the function of the road feature measurement apparatus B <b>500</b> which has been explained in the second embodiment and the feature type <b>413</b> in the road map <b>622</b> generated by the function of the CAD.</p>
<p id="p-0441" num="0448">According to the fourth embodiment, while the road map <b>622</b> is drafted, the three-dimensional coordinates to be set in the road map <b>622</b> can be calculated, it is possible to easily generate the road data such as the road management ledger and maps of the car navigation, etc.</p>
<heading id="h-0025" level="1">Embodiment 5</heading>
<p id="p-0442" num="0449">In the fifth embodiment, another embodiment will be explained, in which the feature position is decided without calculating the neighboring plane.</p>
<p id="p-0443" num="0450">Items being different from the first through fourth embodiments will be mainly discussed in the following, and items for which explanation is omitted can be considered as the same as the first through fourth embodiments.</p>
<p id="p-0444" num="0451"><figref idref="DRAWINGS">FIG. 35</figref> shows a functional configuration of a road feature measurement apparatus C <b>700</b> according to the fifth embodiment.</p>
<p id="p-0445" num="0452">The functional configuration of the road feature measurement apparatus C <b>700</b> according to the fifth embodiment will be explained with reference to <figref idref="DRAWINGS">FIG. 35</figref>.</p>
<p id="p-0446" num="0453">The road feature measurement apparatus C (an example of a measurement apparatus) corresponds to the road feature measurement apparatus <b>100</b>, the road feature measurement apparatus B <b>500</b>, and the CAD apparatus <b>600</b> which have been explained in the respective embodiments.</p>
<p id="p-0447" num="0454">The road feature measurement apparatus C includes an image displaying unit <b>341</b>, an image point inputting unit <b>342</b>, the road surface model corresponding point searching unit <b>170</b>, an image memory unit <b>708</b>, a three-dimensional point cloud model memory unit <b>709</b>, a measurement position data memory unit <b>599</b>, and the displaying device <b>901</b>, and measures three-dimensional coordinates of the feature specified by the user from the features captured in the image as the feature position.</p>
<p id="p-0448" num="0455">The road surface model corresponding point searching unit <b>170</b> includes the neighborhood extracting unit <b>171</b>, the model projecting unit <b>172</b>, and a feature position calculating unit <b>174</b>.</p>
<p id="p-0449" num="0456">The neighborhood extracting unit <b>171</b> and the model projecting unit <b>172</b> include functions which have been explained in the respective embodiments.</p>
<p id="p-0450" num="0457">Further, the feature position calculating unit <b>174</b> includes a function to carry out the process calculating the feature position (position calculating process) which has been explained as the process of the road surface model corresponding point searching unit <b>170</b> in the respective embodiments.</p>
<p id="p-0451" num="0458">The image displaying unit <b>341</b>, the image point inputting unit <b>342</b>, and the measurement position data memory unit <b>599</b> include functions explained in the second and fourth embodiments.</p>
<p id="p-0452" num="0459">Further, the image memory unit <b>708</b> and the three-dimensional point cloud model memory unit <b>709</b> correspond to the observation data memory unit <b>199</b> in the respective embodiments.</p>
<p id="p-0453" num="0460">The image memory unit <b>708</b> stores images captured by the camera <b>230</b>.</p>
<p id="p-0454" num="0461">The three-dimensional point cloud model memory unit <b>709</b> stores the road surface shape model (the three-dimensional point cloud model).</p>
<p id="p-0455" num="0462">Between the image and the road surface shape model, data showing the same place are stored by relating each other. Namely, the image and the point cloud of the road surface shape models show the same place.</p>
<p id="p-0456" num="0463">The image displaying unit <b>341</b> displays the point cloud, of which three-dimensional positions are known, superimposed with the image, in which the feature is captured, on screen of the displaying device <b>901</b> and prompts the user to specify the position of the feature for the measurement target within the image.</p>
<p id="p-0457" num="0464">For example, the image displaying unit <b>341</b> superimposes and displays the image stored in the image memory unit <b>708</b> and the road surface shape model (the three-dimensional point cloud model) stored in the three-dimensional point cloud model memory unit <b>709</b> and prompts the user to specify a point corresponding to the position within the image which the user observes from the point cloud of the road surface shape model.</p>
<p id="p-0458" num="0465">The image point inputting unit <b>342</b> (a measurement image point obtaining unit) inputs the position within the image specified by the user (the position of two-dimensional coordinates (u, v) of the image on the image plane) as the measurement image point from the inputting device.</p>
<p id="p-0459" num="0466">The neighborhood extracting unit <b>171</b> (a corresponding point detecting unit) detects a corresponding point corresponding to the measurement image point inputted by the image point inputting unit <b>342</b> from the point cloud.</p>
<p id="p-0460" num="0467">For example, the neighborhood extracting unit <b>171</b> detects the corresponding point corresponding to the measurement image point inputted by the image point inputting unit <b>342</b> from the point cloud of the road surface shape model stored in the three-dimensional point cloud model memory unit <b>709</b>.</p>
<p id="p-0461" num="0468">The corresponding point is, for example, a point of the road surface shape model projected on the same coordinates, the closest coordinates, or the neighboring coordinates in the two-dimensional coordinate of pixels on the image plane. Hereinafter, the corresponding point is referred to as a neighboring point.</p>
<p id="p-0462" num="0469">The feature position calculating unit <b>174</b> (a position calculating unit) decides a three-dimensional position of the measurement image point obtained by the image point inputting unit <b>342</b> using the three-dimensional position of the corresponding point detected by the neighborhood extracting unit <b>171</b>.</p>
<p id="p-0463" num="0470">The measurement position data memory unit <b>599</b> (a result memory unit) assumes the three-dimensional position decided by the feature position calculating unit <b>174</b> as the three-dimensional position of the feature for the measurement target and stores the three-dimensional position with relating to the type of the feature for the measurement target.</p>
<p id="p-0464" num="0471">The image memory unit <b>708</b> stores the image captured by the camera <b>230</b>.</p>
<p id="p-0465" num="0472">The three-dimensional point cloud model memory unit <b>709</b> stores the point cloud which is composed of the point cloud measured by the laser device and as well the three-dimensional positions of which are known as the group of the three-dimensional points model.</p>
<p id="p-0466" num="0473">The model projecting unit <b>172</b> projects the road surface shape model stored in the three-dimensional point cloud model memory unit <b>709</b> on the image plane of the image displayed by the image displaying unit <b>341</b>. The point cloud of the road surface shape model projected (a projected point cloud, hereinafter) on the image plane by the model projecting unit <b>172</b> is superimposed with the image and displayed on the displaying device <b>901</b> by the image displaying unit <b>341</b>.</p>
<p id="p-0467" num="0474"><figref idref="DRAWINGS">FIG. 36</figref> is a flowchart showing a measuring method according to the fifth embodiment.</p>
<p id="p-0468" num="0475">A measurement method according to the fifth embodiment will be explained in the following with reference to <figref idref="DRAWINGS">FIG. 36</figref>.</p>
<p id="p-0469" num="0476">&#x3c;S<b>1111</b>: Image Displaying Process A&#x3e;</p>
<p id="p-0470" num="0477">First, the image displaying unit <b>341</b> displays the image specified by the user on the displaying device <b>901</b>.</p>
<p id="p-0471" num="0478">For example, as has been explained in the second embodiment, the image displaying unit <b>341</b> displays a list of the image numbers or a list of thumbnails of the images on the displaying device <b>901</b> as a list of images stored in the image memory unit <b>708</b>. Then, the image displaying unit <b>341</b> obtains the image, which is specified by the user from the displayed list of images, from the image memory unit <b>708</b>, and displays the road feature measurement screen <b>400</b> (refer to the second embodiment) on the displaying device <b>901</b> with the obtained image as the captured image <b>401</b>.</p>
<p id="p-0472" num="0479">&#x3c;S<b>1112</b>: Point cloud Projecting Process&#x3e;</p>
<p id="p-0473" num="0480">Next, the model projecting unit <b>172</b> projects the road surface shape model on the image plane.</p>
<p id="p-0474" num="0481">At this time, the model projecting unit <b>172</b> obtains the road surface shape model corresponding to the image-capturing time and the image-capturing position of the image (the captured image <b>401</b>, hereinafter) displayed by the image displaying process A (S<b>1111</b>) from the three-dimensional point cloud model memory unit <b>709</b>, and projects the obtained road surface shape model on the image plane of the captured image <b>401</b>. The projection of the road surface shape model on the image plane is carried out by &#x201c;the 3D model projecting process (S<b>502</b>)&#x201d; that has been explained in the first embodiment.</p>
<p id="p-0475" num="0482">Hereinafter, the point cloud of the road surface shape models projected on the image plane is referred to as &#x201c;a projected point cloud&#x201d;.</p>
<p id="p-0476" num="0483">&#x3c;S<b>1113</b>: Image Displaying Process B&#x3e;</p>
<p id="p-0477" num="0484">Next, the image displaying unit <b>341</b> displays the projected point cloud superimposed with the image on the displaying device <b>901</b>.</p>
<p id="p-0478" num="0485"><figref idref="DRAWINGS">FIG. 37</figref> shows the road feature measurement screen <b>400</b> according to the fifth embodiment.</p>
<p id="p-0479" num="0486">In the image displaying process B (S<b>1113</b>), the image displaying unit <b>341</b> displays the projected point cloud superimposed with the captured image <b>401</b> as shown in <figref idref="DRAWINGS">FIG. 37</figref>. The road feature measurement screen <b>400</b> and the captured image <b>401</b> are displayed on the displaying device <b>901</b> by the image displaying unit <b>341</b> in the image displaying process A (S<b>1111</b>). Plural black points within the captured image <b>401</b> show the projected point cloud projected on the image plane of the captured image <b>401</b> in the point cloud projecting process (S<b>1112</b>). Point &#x201c;a&#x201d; <b>421</b><i>a</i>, point &#x201c;b&#x201d; <b>421</b><i>b</i>, point &#x201c;c&#x201d; <b>421</b><i>c</i>, point &#x201c;d&#x201d; <b>421</b><i>d</i>, etc. are projected points <b>421</b> which form the projected point cloud.</p>
<p id="p-0480" num="0487">The point &#x201c;a&#x201d; <b>421</b><i>a </i>is located at an intermediate stage of the km post <b>403</b>, the point &#x201c;b&#x201d; <b>421</b><i>b </i>is located on the sidewalk <b>404</b> of the left back of the km post <b>403</b>, the point &#x201c;c&#x201d; <b>421</b><i>c </i>is located on the sidewalk <b>404</b> of the right back of the km post <b>403</b>, and the point &#x201c;d&#x201d; <b>421</b><i>d </i>is located at an upper stage of the km post <b>403</b>.</p>
<p id="p-0481" num="0488">Since the point &#x201c;a&#x201d; <b>421</b><i>a </i>and the point &#x201c;d&#x201d; <b>421</b><i>d </i>show the laser measured point obtained by reflecting with the km post <b>403</b>, a latitude and a longitude of the km post <b>403</b> are correctly shown. When a height of the km post <b>403</b> is obtained at the lower end (a contacted point with the sidewalk <b>404</b>), since the point &#x201c;a&#x201d; <b>421</b><i>a </i>is located below, the result shows a more precise height than the point &#x201c;d&#x201d; <b>421</b><i>d. </i></p>
<p id="p-0482" num="0489">Since the point &#x201c;b&#x201d; <b>421</b><i>b </i>and the point &#x201c;c&#x201d; <b>421</b><i>c </i>show the laser measured points obtained by reflecting with the sidewalk <b>404</b>, the point &#x201c;b&#x201d; <b>421</b><i>b </i>and the point &#x201c;c&#x201d; <b>421</b><i>c </i>do not show the three-dimensional coordinates of the km post <b>403</b>. Between the point &#x201c;b&#x201d; <b>421</b><i>b </i>and the point &#x201c;c&#x201d; <b>421</b><i>c</i>, since the point &#x201c;c&#x201d; <b>421</b><i>c </i>is closer to the km post <b>403</b> than the point &#x201c;b&#x201d; <b>421</b><i>b</i>, the point &#x201c;c&#x201d; <b>421</b><i>c </i>shows a closer value of the three-dimensional coordinates of the km post <b>403</b> than the point &#x201c;b&#x201d; <b>421</b><i>b. </i></p>
<p id="p-0483" num="0490">&#x3c;S<b>1120</b>: Measurement Image Point Obtaining Process, Type Inputting Process&#x3e;</p>
<p id="p-0484" num="0491">Next, the image point inputting unit <b>342</b> inputs the measurement image point <b>412</b>, the feature type <b>413</b>, and the specific image number <b>411</b> from the inputting equipment such as the mouse <b>903</b>, the FDD <b>904</b>, etc. via the OS <b>921</b>. The measurement image point <b>412</b>, the feature type <b>413</b>, and the specific image number <b>411</b> inputted by the image point inputting unit <b>342</b> are stored in the measurement position data memory unit <b>599</b> with the feature position <b>414</b> in a result storing process (S<b>1150</b>) which will be discussed later.</p>
<p id="p-0485" num="0492">For example, the image point inputting unit <b>342</b> inputs two-dimensional coordinates (uv coordinate) within the image shown by the mouse cursor <b>402</b> when the user clicks the mouse <b>903</b> as the measurement image point <b>412</b>, and displays the measurement image point <b>412</b> inputted on the road feature measurement screen <b>400</b>.</p>
<p id="p-0486" num="0493">Further, for example, the image point inputting unit <b>342</b> inputs a type specified by the user within the type list box <b>417</b> with clicking the mouse <b>903</b> as the feature type <b>413</b> as explained in the second embodiment, and changes the background color of the specified part of the feature type <b>413</b>.</p>
<p id="p-0487" num="0494">Further, the specific image number <b>411</b> is an identification number of the captured image <b>401</b> displayed on the road feature measurement screen <b>400</b>.</p>
<p id="p-0488" num="0495">In <figref idref="DRAWINGS">FIG. 37</figref>, when the user wants to measure the three-dimensional coordinates at which the km post <b>403</b> captured in the captured image <b>401</b> is actually located, the user specifies the point &#x201c;a&#x201d; <b>421</b><i>a </i>superimposed with the km post <b>403</b> by the mouse cursor <b>402</b>. However, since the point &#x201c;a&#x201d; <b>421</b><i>a </i>is small, the user cannot always specify the point &#x201c;a&#x201d; <b>421</b><i>a</i>, so that the measurement image point <b>412</b> shows different uv coordinates from the point &#x201c;a&#x201d; <b>421</b><i>a. </i></p>
<p id="p-0489" num="0496">For example, each projected point <b>421</b> is shown by one dot (pixel, picture element).</p>
<p id="p-0490" num="0497">Further, the image displaying unit <b>341</b> displays the pixel of the measurement image point <b>412</b> specified by the user in the measurement image point obtaining process (S<b>1120</b>) separately from other pixels. For example, the image displaying unit <b>341</b> blinks or changes the color of the pixel showing the measurement image point <b>412</b> (and pixels around the measurement image point <b>412</b>).</p>
<p id="p-0491" num="0498">By this operation, the user can repeat the operation of the mouse <b>903</b> until the projected point <b>421</b> (the point &#x201c;a&#x201d; <b>421</b><i>a</i>, for example) superimposed with the feature for the measurement target is finally specified as the measurement image point <b>412</b>.</p>
<p id="p-0492" num="0499">&#x3c;S<b>1130</b>: Corresponding Point Detecting Process&#x3e;</p>
<p id="p-0493" num="0500">Next, the neighborhood extracting unit <b>171</b> extracts a neighboring point (corresponding point) of the measurement image point <b>412</b> from the projected point cloud.</p>
<p id="p-0494" num="0501">At this time, the neighborhood extracting unit <b>171</b> discriminates the projected point <b>421</b> having the closest uv coordinate to the measurement image point <b>412</b> (an example of the corresponding point) as the neighboring point from the projected point cloud.</p>
<p id="p-0495" num="0502">For example, as shown in <figref idref="DRAWINGS">FIG. 37</figref>, when the user specifies a pixel which is slightly dislocated from the point &#x201c;a&#x201d; <b>421</b><i>a </i>using the mouse cursor <b>402</b> as the measurement image point <b>412</b> in the measurement image point obtaining process (S<b>1120</b>), the neighboring point is the point &#x201c;a&#x201d; <b>421</b><i>a</i>. Further, in the measurement image point obtaining process (S<b>1120</b>), when the pixel itself on which the point &#x201c;a&#x201d; <b>421</b><i>a </i>is projected is specified as the measurement image point <b>412</b>, the point &#x201c;a&#x201d; <b>421</b><i>a </i>is assumed as the neighboring point.</p>
<p id="p-0496" num="0503">Further, the image displaying unit <b>341</b> displays the projected point <b>421</b> extracted by the neighborhood extracting unit <b>171</b> in the corresponding point detecting process (S<b>1130</b>) separately from other projected points <b>421</b>. For example, the image displaying unit <b>341</b> blinks, changes the color of, or enlarges the projected point <b>421</b> decided (the point &#x201c;a&#x201d; <b>421</b><i>a </i>in <figref idref="DRAWINGS">FIG. 37</figref>).</p>
<p id="p-0497" num="0504">&#x3c;S<b>1140</b>: Position Calculating Process&#x3e;</p>
<p id="p-0498" num="0505">Next, the feature position calculating unit <b>174</b> obtains a three-dimensional position of the neighboring point as the feature position <b>414</b>.</p>
<p id="p-0499" num="0506">When the user wants to measure a three-dimensional position of the measurement image point <b>412</b>, the user operates the mouse <b>903</b> to press the calculation requesting button <b>415</b> of the road feature measurement screen <b>400</b> (refer to <figref idref="DRAWINGS">FIG. 37</figref>).</p>
<p id="p-0500" num="0507">When the calculation requesting button <b>415</b> is pressed, the feature position calculating unit <b>174</b> extracts the laser measured point corresponding to the neighboring point extracted in the corresponding point detecting process (S<b>1130</b>) from the point cloud of the road surface shape model stored in the three-dimensional point cloud model memory unit <b>709</b> and obtains the three-dimensional coordinates shown by the extracted laser measured point as the feature position <b>414</b>.</p>
<p id="p-0501" num="0508">Further, the image displaying unit <b>341</b> displays the feature position <b>414</b> obtained in the position calculating process (S<b>1140</b>) on the road feature measurement screen <b>400</b> as shown in <figref idref="DRAWINGS">FIG. 37</figref>.</p>
<p id="p-0502" num="0509">&#x3c;S<b>1150</b>: Result Storing Process&#x3e;</p>
<p id="p-0503" num="0510">Then, the measurement position data memory unit <b>599</b> stores the feature position <b>414</b>, the measurement image point <b>412</b>, the specific image number <b>411</b>, and the feature type <b>413</b>.</p>
<p id="p-0504" num="0511">When the user wants to store the feature position <b>414</b>, the user operates the mouse <b>903</b> to press the storage requesting button <b>416</b> of the road feature measurement screen <b>400</b> (refer to <figref idref="DRAWINGS">FIG. 37</figref>).</p>
<p id="p-0505" num="0512">When the storage requesting button <b>416</b> is pressed, the measurement position data memory unit <b>599</b> stores the specific image number <b>411</b>, the measurement image point <b>412</b>, and the feature type <b>413</b> inputted in the measurement image point obtaining process/type inputting process (S<b>1120</b>) with relating to the feature position <b>414</b> obtained in the position calculating process (S<b>1140</b>) as the measurement position data.</p>
<p id="p-0506" num="0513">As discussed above, the road feature measurement apparatus C <b>700</b> can measure the three-dimensional coordinates at which the feature for the measurement target is actually located by displaying the projected point cloud superimposed with the captured image <b>401</b> and prompting the user to specify the laser measured point representing the feature for the measurement target. Using the road feature measurement apparatus C <b>700</b>, the user can measure the three-dimensional coordinates of the feature by operating the mouse <b>903</b> on the screen of the PC without survey in the actual place.</p>
<p id="p-0507" num="0514">The road feature measurement apparatus C <b>700</b> can be used as the CAD apparatus <b>600</b> by including the CAD function (the drafting unit <b>610</b>) as well as the fourth embodiment.</p>
<heading id="h-0026" level="1">Embodiment 6</heading>
<p id="p-0508" num="0515">In the sixth embodiment, another embodiment to detect the feature from the image and measure a three-dimensional position of the detected feature will be explained.</p>
<p id="p-0509" num="0516">Items being different from the fifth embodiment will be mainly discussed in the following, and items for which explanation is omitted can be considered as the same as the fifth embodiment.</p>
<p id="p-0510" num="0517"><figref idref="DRAWINGS">FIG. 38</figref> shows a functional configuration of the road feature measurement apparatus C <b>700</b> according to the sixth embodiment.</p>
<p id="p-0511" num="0518">The functional configuration of the road feature measurement apparatus C <b>700</b> according to the sixth embodiment will be explained in the following with reference to <figref idref="DRAWINGS">FIG. 38</figref>.</p>
<p id="p-0512" num="0519">The road feature measurement apparatus C <b>700</b> of the sixth embodiment additionally includes a feature region detecting unit <b>701</b> to the road feature measurement apparatus C <b>700</b> which has been explained in the fifth embodiment.</p>
<p id="p-0513" num="0520">The feature region detecting unit <b>701</b> detects an image region where the feature for the measurement target is captured as a feature image region by analyzing the image stored in the image memory unit <b>708</b>.</p>
<p id="p-0514" num="0521">The image displaying unit <b>341</b> prompts the user to specify the position of the image in the feature image region detected by the feature region detecting unit <b>701</b>.</p>
<p id="p-0515" num="0522">When a point of the point cloud shown in the feature image region detected by the feature region detecting unit <b>701</b> is located at the position within the image shown by the measurement image point, the neighborhood extracting unit <b>171</b> (the corresponding point detecting unit) detects that point as the corresponding point corresponding to the measurement image point.</p>
<p id="p-0516" num="0523">Further, when the point of the point cloud shown in the feature image region detected by the feature region detecting unit <b>701</b> is not located at the position within the image shown by the measurement image point, the neighborhood extracting unit <b>171</b> detects a closest point to the measurement image point as the corresponding point corresponding to the measurement image point.</p>
<p id="p-0517" num="0524"><figref idref="DRAWINGS">FIG. 39</figref> is a flowchart showing a measuring method according to the sixth embodiment.</p>
<p id="p-0518" num="0525">The measuring method according to the sixth embodiment will be explained with reference to <figref idref="DRAWINGS">FIG. 39</figref>.</p>
<p id="p-0519" num="0526">The measuring method according to the sixth embodiment additionally includes a feature region detecting process (S<b>1213</b>) to the measuring method which has been explained in the fifth embodiment.</p>
<p id="p-0520" num="0527">From the image displaying process A (S<b>1211</b>) through the result storing process (S<b>1250</b>) (except for the feature region detecting process [S<b>1213</b>]) in the sixth embodiment are the same as the image displaying process A (S<b>1111</b>) through the result storing process (S<b>1150</b>) in the fifth embodiment.</p>
<p id="p-0521" num="0528">A feature region detecting process (S<b>1213</b>) and an image displaying process B (S<b>1214</b>) will be explained in the following.</p>
<p id="p-0522" num="0529">&#x3c;S<b>1213</b>: Feature Region Detecting Process&#x3e;</p>
<p id="p-0523" num="0530">The feature region detecting unit <b>701</b> carries out image processing of the image specified by the user in the image displaying process A (S<b>1211</b>), and detects a part in which a feature to be a candidate of the measurement target from the image specified by the user as a feature image region.</p>
<p id="p-0524" num="0531">For example, a feature pattern in which a specific feature is represented by a shape and color is previously stored in a memory unit (an image memory unit <b>708</b>, for example), and the feature region detecting unit <b>701</b> carries out pattern matching of the image and the feature pattern and discriminates a region matched with the feature pattern within the image as the feature image region. For example, the feature pattern includes a blue circle or a red circle representing a regulatory sign, a yellow triangle representing a warning sign, a blue quadrangle representing an indication sign, a blue triangle representing a crosswalk sign, a green quadrangle representing an information sign, a white straight line representing a white line, etc.</p>
<p id="p-0525" num="0532">The regulatory sign, the warning sign, the direction sign, the crosswalk sign, and the information sign are respectively kinds of the road signs. The white line is a kind of the road indication.</p>
<p id="p-0526" num="0533"><figref idref="DRAWINGS">FIG. 40</figref> shows the road feature measurement screen <b>400</b> according to the sixth embodiment.</p>
<p id="p-0527" num="0534">For example, the feature region detecting unit <b>701</b> detects a region in which the km post <b>403</b> is captured within the captured image <b>401</b> in <figref idref="DRAWINGS">FIG. 40</figref> as the feature image region.</p>
<p id="p-0528" num="0535">&#x3c;S<b>1214</b>: Image Displaying Process B&#x3e;</p>
<p id="p-0529" num="0536">The image displaying unit <b>341</b>, similarly to the fifth embodiment, displays the projected point cloud superimposed with the image and as well shows the feature image region detected by the feature region detecting process (S<b>1213</b>) within the image.</p>
<p id="p-0530" num="0537">For example, the image displaying unit <b>341</b> displays a mark (an arrow or a frame border, for example) for indicating the feature image region.</p>
<p id="p-0531" num="0538">For example, the image displaying unit <b>341</b> displays the mark (illustration omitted) for indicating the region in which the km post <b>403</b> is captured within the captured image <b>401</b> in <figref idref="DRAWINGS">FIG. 40</figref>.</p>
<p id="p-0532" num="0539">By this operation, the road feature measurement apparatus C <b>700</b> aids the user to find the feature to be the measurement target within the image or reduces the measurement leakage of the feature caused by oversight of the measurement target by the user within the image.</p>
<p id="p-0533" num="0540">The feature image region detected in the feature region detecting process (S<b>1213</b>) can be also used in the corresponding point detecting process (S<b>1230</b>).</p>
<p id="p-0534" num="0541">In the following, the using method of the feature image region in the corresponding point detecting process (S<b>1230</b>) will be explained.</p>
<p id="p-0535" num="0542">It is assumed that in <figref idref="DRAWINGS">FIG. 40</figref>, in order to measure three-dimensional coordinates of the km post <b>403</b>, the user does not specify (neighborhood of) the point &#x201c;a&#x201d; <b>421</b><i>a </i>or the point &#x201c;d&#x201d; <b>421</b><i>d </i>superimposed on the km post <b>403</b> as the measurement image point <b>412</b>, but specifies a point &#x201c;A&#x201d; <b>412</b>A of the upper end of the km post <b>403</b> as the measurement image point <b>412</b>.</p>
<p id="p-0536" num="0543">At this time, in the corresponding point detecting process (S<b>1230</b>), the point &#x201c;e&#x201d; <b>421</b><i>e </i>which is the closest projected point <b>421</b> to the point &#x201c;A&#x201d; <b>412</b>A is extracted. The point &#x201c;e&#x201d; <b>421</b><i>e </i>is not the projected point <b>421</b> superimposed on the km post <b>403</b>.</p>
<p id="p-0537" num="0544">Namely, the laser measured point corresponding to the point &#x201c;e&#x201d; <b>421</b><i>e </i>is not a point obtained by reflection from the km post <b>403</b>, but a point obtained by reflection from one point on the sidewalk <b>404</b> which is far away behind the km post <b>403</b>. Therefore, the laser measured point corresponding to the point &#x201c;e&#x201d; <b>421</b><i>e </i>shows the three-dimensional coordinates of one point on the sidewalk <b>404</b> which is far away from the km post <b>403</b> instead of the three-dimensional coordinates of the km post <b>403</b>.</p>
<p id="p-0538" num="0545">Further, in <figref idref="DRAWINGS">FIG. 40</figref>, when the user specifies the point &#x201c;B&#x201d; <b>412</b>B which is the lower end of the km post <b>403</b> as the measurement image point <b>412</b>, the point &#x201c;c&#x201d; <b>421</b><i>c</i>, which is not overlapped with the km post <b>403</b> and shown, is extracted in the corresponding point detecting process (S<b>1230</b>).</p>
<p id="p-0539" num="0546">However, since the laser measured point corresponding to the point &#x201c;c&#x201d; <b>421</b><i>c </i>is a point obtained by reflecting with the sidewalk <b>404</b> adjacent to the point where the km post <b>403</b> is provided, the point &#x201c;c&#x201d; shows a close value to the three-dimensional coordinates of the km post <b>403</b>. The laser measured point is obtained with high density (with some cm intervals, for example) by the laser radar <b>240</b>, so that if the laser measured point corresponding to the point &#x201c;c&#x201d; <b>421</b><i>c </i>is assumed to be the three-dimensional coordinates of the km post <b>403</b>, an error should be small.</p>
<p id="p-0540" num="0547">Then, in the measurement image point obtaining process (S<b>1220</b>), when the measurement image point <b>412</b> is specified within the feature image region (or within a predetermined range of the feature image region), in the corresponding point detecting process (S<b>1230</b>), the neighborhood extracting unit <b>171</b> extracts the closest projected point <b>421</b> (an example of the corresponding point) to the measurement image point <b>412</b> among the projected points <b>421</b> projected within the feature image region.</p>
<p id="p-0541" num="0548">Namely, if the projected point exists within the feature image region, the neighborhood extracting unit <b>171</b> extracts the projected point projected within the feature image region as the corresponding point (the neighboring point) corresponding to the measurement image point.</p>
<p id="p-0542" num="0549">For example, in <figref idref="DRAWINGS">FIG. 40</figref>, it is assumed that the region in which the km post <b>403</b> is captured is detected as the feature image region. At this time, the point &#x201c;e&#x201d; <b>421</b><i>e </i>which is the closest among all of the projected points <b>421</b> is not extracted, since it is not displayed within the km post <b>403</b> (the feature image region). Then, the neighborhood extracting unit <b>171</b> extracts the point &#x201c;d&#x201d; <b>421</b><i>d </i>which is the closest to the point &#x201c;A&#x201d; <b>412</b>A among the projected points <b>421</b> (the point &#x201c;a&#x201d; <b>421</b><i>a</i>, the point &#x201c;d&#x201d; <b>421</b><i>d</i>) superimposed with the km post <b>403</b>.</p>
<p id="p-0543" num="0550">However, in the measurement image point obtaining process (S<b>1220</b>), when although the measurement image point <b>412</b> is specified within the feature image region, the projected point <b>421</b> projected does not exist within the feature image region, in the corresponding point detecting process (S<b>1230</b>), the neighborhood extracting unit <b>171</b> extracts the projected point <b>421</b> (an example of the corresponding point) which is the closest to the measurement image point <b>412</b> among the projected points <b>421</b> projected outside of the feature image region.</p>
<p id="p-0544" num="0551">Namely, the neighborhood extracting unit <b>171</b>, when there is no projected point projected within the feature image region, detects the projected point which is the closest to the measurement image point as the corresponding point (the neighboring point) corresponding to the measurement image point.</p>
<p id="p-0545" num="0552">For example, in <figref idref="DRAWINGS">FIG. 40</figref>, when an edge <b>424</b> of the white line <b>407</b> (a border line of the white line <b>407</b>) is the feature image region, and if there is no projected point <b>421</b> on the edge <b>424</b>, the neighborhood extracting unit <b>171</b> extracts the point &#x201c;f&#x201d; <b>421</b><i>f </i>which is the closest projected point to the point &#x201c;x&#x201d; <b>423</b> showing the measurement image point.</p>
<p id="p-0546" num="0553">Further, in the measurement image point obtaining process (S<b>1220</b>), when the measurement image point <b>412</b> is specified within the predetermined range <b>422</b> of the lower end part of the feature image region (within or outside of the feature image region), the neighborhood extracting unit <b>171</b> can extract the projected point <b>421</b> which is the closest to the measurement image point <b>412</b> (an example of the corresponding point) among all of the projected points <b>421</b>.</p>
<p id="p-0547" num="0554">For example, in <figref idref="DRAWINGS">FIG. 40</figref>, when the point &#x201c;B&#x201d; <b>412</b>B located within the predetermined range <b>422</b> of the lower end part of the km post <b>403</b> which is detected as the feature image region is specified, the neighborhood extracting unit <b>171</b> extracts the point &#x201c;c&#x201d; <b>421</b><i>c </i>which is the closest among all of the projected points <b>421</b>. Although the point &#x201c;c&#x201d; <b>421</b><i>c </i>is not overlapped with the km post <b>403</b> and is shown, the point &#x201c;c&#x201d; <b>421</b><i>c </i>is extracted as the closest projected point <b>421</b>.</p>
<p id="p-0548" num="0555">By employing the feature image region in the corresponding point detecting process (S<b>1230</b>), the road feature measurement apparatus C <b>700</b> can measure the three-dimensional coordinates of the feature with high precision by extracting the most appropriate projected point <b>421</b> even if the measurement image point <b>412</b> is erroneously specified by the user.</p>
<heading id="h-0027" level="1">Embodiment 7</heading>
<p id="p-0549" num="0556">In the seventh embodiment, another embodiment will be explained, in which a neighboring point of the measurement image point is obtained as the first candidate of the feature position, and as well an intersecting point of a neighboring plane and a LOS vector for the measurement image point is calculated as the second candidate of the feature position, and either of the first candidate and the second candidate is decided as the feature position.</p>
<p id="p-0550" num="0557">Items being different from the sixth embodiment will be mainly discussed in the following, and items for which explanation is omitted can be considered as the same as the sixth embodiment.</p>
<p id="p-0551" num="0558"><figref idref="DRAWINGS">FIG. 41</figref> shows a functional configuration of the road feature measurement apparatus C <b>700</b> according to the seventh embodiment.</p>
<p id="p-0552" num="0559">The functional configuration of the road feature measurement apparatus C <b>700</b> according to the seventh embodiment will be explained in the following with reference to <figref idref="DRAWINGS">FIG. 41</figref>.</p>
<p id="p-0553" num="0560">The road feature measurement apparatus C <b>700</b> according to the seventh embodiment additionally includes a camera LOS computing unit <b>140</b> and a neighboring plane calculating unit <b>173</b> to the road feature measurement apparatus C <b>700</b> which has been explained in the sixth embodiment.</p>
<p id="p-0554" num="0561">A camera LOS computing unit <b>1401</b> (a vector calculating unit) calculates a LOS vector showing a direction from the center of camera of the camera <b>230</b> to the measurement image point.</p>
<p id="p-0555" num="0562">The neighboring plane calculating unit <b>173</b> (a plane calculating unit) calculates a particular plane including a neighboring point (the corresponding point) detected by the neighborhood extracting unit <b>171</b> (the corresponding point detecting unit).</p>
<p id="p-0556" num="0563">The feature position calculating unit <b>174</b> (a position calculating unit) obtains a three-dimensional position of the neighboring point detected by the neighborhood extracting unit <b>171</b> (the corresponding point detecting unit) as the first candidate showing a three-dimensional position of the measurement image point. Further, the feature position calculating unit <b>174</b> calculates an intersecting point of the particular plane calculated by the neighboring plane calculating unit <b>173</b> and the LOS vector calculated by the camera LOS computing unit <b>140</b> as the second candidate of the three-dimensional position of the measurement image point.</p>
<p id="p-0557" num="0564">The image displaying unit <b>341</b> (the position displaying unit) displays the first candidate and the second candidate obtained by the feature position calculating unit <b>174</b> on the screen of the displaying device <b>901</b>, and prompts the user to specify either of the first candidate and the second candidate.</p>
<p id="p-0558" num="0565">The measurement position data memory unit <b>599</b> (the result memory unit) stores the one specified by the user between the first candidate and the second candidate as the three-dimensional position of the measurement image point (the feature position).</p>
<p id="p-0559" num="0566"><figref idref="DRAWINGS">FIG. 42</figref> is a flowchart showing a measuring method according to the seventh embodiment.</p>
<p id="p-0560" num="0567">The measuring method according to the seventh embodiment will be explained in the following with reference to <figref idref="DRAWINGS">FIG. 42</figref>.</p>
<p id="p-0561" num="0568">The measuring method according to the seventh embodiment additionally includes a plane calculating process (S<b>1351</b>), a vector calculating process (S<b>1352</b>), a position calculating process B (S<b>1353</b>), and a position displaying process (S<b>1360</b>) to the measuring method which has been explained in the sixth embodiment.</p>
<p id="p-0562" num="0569">The image displaying process A (S<b>1311</b>) through the corresponding point detecting process (S<b>1330</b>), the position calculating process A (S<b>1340</b>) and the result storing process (S<b>1370</b>) in the seventh embodiment are the same as the image displaying process A (S<b>1211</b>) through the corresponding point detecting process (S<b>1230</b>), the position calculating process (S<b>1240</b>), and the result storing process (S<b>1250</b>) in the sixth embodiment.</p>
<p id="p-0563" num="0570">Hereinafter, the plane calculating process (S<b>1351</b>), the vector calculating process (S<b>1352</b>), the position calculating process B (S<b>1353</b>), and the position displaying process (S<b>1360</b>) will be explained.</p>
<p id="p-0564" num="0571">&#x3c;S<b>1351</b>: Plane Calculating Process&#x3e;</p>
<p id="p-0565" num="0572">The neighboring plane calculating unit <b>173</b> calculates a particular plane including the neighboring point extracted in the corresponding point detecting process (S<b>1330</b>) as a neighboring plane.</p>
<p id="p-0566" num="0573">For example, the neighboring plane calculating unit <b>173</b> calculates a horizontal plane including the neighboring point as the neighboring plane.</p>
<p id="p-0567" num="0574">The plane calculating process (S<b>1351</b>) is the same as the neighboring plane calculating process (S<b>503</b>B) which has been explained in the third embodiment.</p>
<p id="p-0568" num="0575">&#x3c;S<b>1352</b>: Vector Calculating Process&#x3e;</p>
<p id="p-0569" num="0576">The camera LOS computing unit <b>140</b> calculates the LOS vector of the camera <b>230</b> for the measurement image point inputted in the measurement image point obtaining process (S<b>1320</b>) based on a posture angle of the camera <b>230</b> when the image displayed in the image displaying process A (S<b>1311</b>) is captured. The measurement image point inputted in the measurement image point obtaining process (S<b>1320</b>) and the specified image number to identify the image displayed in the image displaying process A (S<b>1311</b>) are outputted from the image point inputting unit <b>342</b> to the camera LOS computing unit <b>140</b>. The camera LOS computing unit <b>140</b> decides the time when the image is captured based on the specified image number outputted from the image point inputting unit <b>342</b> and obtains the posture angle of the camera <b>230</b> at the decided time.</p>
<p id="p-0570" num="0577">The plane calculating process (S<b>1352</b>) is the same as the LOS calculating process (S<b>504</b>) which has been explained in the first embodiment.</p>
<p id="p-0571" num="0578">&#x3c;S<b>1353</b>: Position Calculating Process B&#x3e;</p>
<p id="p-0572" num="0579">Next, the feature position calculating unit <b>174</b> calculates an intersecting point of the neighboring plane calculated in the plane calculating process (S<b>1351</b>) and the LOS vector calculated in the vector calculating process (S<b>1352</b>) as the second candidate of the feature position.</p>
<p id="p-0573" num="0580">A calculating method of the second candidate of the feature position by the position calculating process B (S<b>1353</b>) is the same as the calculating method of the feature position by the intersecting point calculating process (S<b>505</b>) which has been explained in the third embodiment.</p>
<p id="p-0574" num="0581">Here, the three-dimensional coordinates of the neighboring point obtained in the position calculating process A (S<b>1340</b>) is the first candidate of the feature position.</p>
<p id="p-0575" num="0582">&#x3c;S<b>1360</b>: Position Displaying Process&#x3e;</p>
<p id="p-0576" num="0583">Next, the image displaying unit <b>341</b> displays the first candidate of the feature position obtained in the position calculating process A (S<b>1340</b>) and the second candidate of the feature position calculated in the position calculating process B (S<b>1353</b>) on the displaying device <b>901</b>.</p>
<p id="p-0577" num="0584"><figref idref="DRAWINGS">FIG. 43</figref> shows the road feature measurement screen <b>400</b> according to the seventh embodiment.</p>
<p id="p-0578" num="0585">For example, the image displaying unit <b>341</b> displays, in the position displaying process (S<b>1360</b>), as shown in <figref idref="DRAWINGS">FIG. 43</figref>, the first candidate of the feature position (the feature position <b>414</b><i>a</i>) and the second candidate of the feature position (the feature position <b>414</b><i>b</i>) on the road feature measurement screen <b>400</b>.</p>
<p id="p-0579" num="0586">Here, the road feature measurement screen <b>400</b> includes a selection list box <b>420</b> to make the user specify either of the feature position <b>414</b><i>a </i>and the feature position <b>414</b><i>b</i>. The user specifies either of the feature position <b>414</b><i>a </i>and the feature position <b>414</b><i>b </i>using the selection list box <b>420</b> as the feature position. For example, in <figref idref="DRAWINGS">FIG. 43</figref>, the feature position <b>414</b><i>a </i>(the first candidate) shown as the &#x201c;feature position <b>1</b>&#x201d; is specified.</p>
<p id="p-0580" num="0587">Then, when the storage requesting button <b>416</b> is pressed, the measurement position data memory unit <b>599</b>, in the result storing process (S<b>1370</b>), stores the feature position specified in the selection list box <b>420</b> with relating to the measurement image point <b>412</b>, the specific image number <b>411</b>, and the feature type <b>413</b> as the measurement position data.</p>
<p id="p-0581" num="0588">On specifying either of the first candidate and the second candidate as the feature position, the user determines, for example, according to the type of the feature for the measurement target.</p>
<p id="p-0582" num="0589">For example, in <figref idref="DRAWINGS">FIG. 43</figref>, when the user wants to measure the position of the feature having a width (the feature having a wide plane compared with the obtained density of the laser measured point) such as the km post <b>403</b> (a column), the user can obtain a correct feature position (the first candidate) by specifying the projected point <b>421</b> (the point &#x201c;a&#x201d; <b>421</b><i>a</i>, for example) superimposed on the feature as the measurement image point as has been explained in the fifth embodiment.</p>
<p id="p-0583" num="0590">However, when the user wants to measure the position of the feature having no width (a line-shaped feature having a narrow width compared with the obtained density of the laser measured point) such as an edge <b>424</b> of the white line <b>407</b> (a border line of the white line <b>407</b>), since not always there exists the projected point <b>421</b> superimposed on the feature, the user is to specify one point (one pixel) on the feature which has no projected point <b>421</b> (the point &#x201c;x&#x201d; <b>423</b>, for example) as the measurement image point. Therefore, the position calculating process A (S<b>1340</b>) in which the three-dimensional coordinates of the laser measured point corresponding to the projected point <b>421</b> (the point &#x201c;f&#x201d; <b>421</b><i>f</i>, for example) which is the closest to the measurement image point is assumed to be the feature position (the first candidate) cannot always obtain the feature to position with higher precision than the feature position calculated in the position calculating process B (S<b>1353</b>) (the second candidate).</p>
<p id="p-0584" num="0591">Then, when the projected point <b>421</b> superimposed on the feature for the measurement target can be specified as the measurement image point <b>412</b>, the user should specify &#x201c;feature position <b>1</b>&#x201d; showing the feature position (the first candidate) obtained in the position calculating process A (S<b>1340</b>) in the selection list box <b>420</b>.</p>
<p id="p-0585" num="0592">Further, when one point on the feature having no projected point <b>421</b> is specified as the measurement image point <b>412</b>, the user should specify &#x201c;feature position <b>2</b>&#x201d; showing the feature position (the second candidate) obtained in the position calculating process B (S<b>1353</b>) in the selection list box <b>420</b>.</p>
<p id="p-0586" num="0593">Since the position calculating process B (S<b>1353</b>) calculates the feature position (the second candidate) based on the LOS vector for the measurement image point, it is possible to obtain the feature position (the second candidate) with a high precision even if there is no projected point <b>421</b> on the feature.</p>
<p id="p-0587" num="0594">As discussed above, the user can select the feature position showing a higher precision among plural feature positions decided by different methods (the position calculating process A (S<b>1340</b>), the position calculating process B [S<b>1353</b>]) according to the type of the feature of which position is to be measured or presence/absence of the projected point <b>421</b> superimposed on the feature.</p>
<heading id="h-0028" level="1">Embodiment 8</heading>
<p id="p-0588" num="0595">In the eighth embodiment, another embodiment will be explained, in which a candidate of the feature position with a higher precision is decided as the feature position between the first candidate and the second candidate of the feature position, while the user's selection is unnecessary.</p>
<p id="p-0589" num="0596">Items being different from the seventh embodiment will be mainly discussed in the following, and items for which explanation is omitted can be considered as the same as the seventh embodiment.</p>
<p id="p-0590" num="0597"><figref idref="DRAWINGS">FIG. 44</figref> is a flowchart showing a measuring method according to the eighth embodiment.</p>
<p id="p-0591" num="0598">The measuring method according to the eighth embodiment will be explained in the following with reference to <figref idref="DRAWINGS">FIG. 44</figref>.</p>
<p id="p-0592" num="0599">The measuring method according to the eighth embodiment carries out a position calculating process C (S<b>1360</b>B) instead of the position displaying process (S<b>1360</b>) which has been explained in the seventh embodiment.</p>
<p id="p-0593" num="0600">Hereinafter, the position calculating process C (S<b>1360</b>B) will be explained.</p>
<p id="p-0594" num="0601">&#x3c;S<b>1360</b>B: Position Calculating Process C&#x3e;</p>
<p id="p-0595" num="0602">The feature position calculating unit <b>174</b> decides either of the feature position (the first candidate) obtained in the position calculating process A (S<b>1340</b>) and the feature position (the second candidate) calculated in the position calculating process B (S<b>1353</b>) based on the feature type inputted in the measurement image point obtaining process (S<b>1320</b>).</p>
<p id="p-0596" num="0603">As has been explained in the seventh embodiment, when the projected point <b>421</b> superimposed on the feature for the measurement target is specified as the measurement image point <b>412</b>, normally the feature position (the first candidate) obtained in the position calculating process A (S<b>1340</b>) shows a higher precision than the feature position (the second candidate) calculated in the position calculating process B (S<b>1353</b>). Further, when there is no projected point <b>421</b> superimposed on the feature for the measurement target, the feature position (the second candidate) calculated in the position calculating process B (S<b>1353</b>) shows a higher precision than the feature position (the first candidate) obtained in the position calculating process A (S<b>1340</b>).</p>
<p id="p-0597" num="0604">Further, when the feature having a width such as the km post, various kinds of road signs, etc. is the measurement target, normally there exists the projected point <b>421</b> superimposed on the feature for the measurement target. Further, when the feature having no width such as the edge of the white line, etc. is the measurement target, there may be no projected point <b>421</b> superimposed on the feature for the measurement target.</p>
<p id="p-0598" num="0605">Then, the feature position calculating unit <b>174</b>, in the position calculating process C (S<b>1360</b>B), selects the feature position (the first candidate) obtained in the position calculating process A (S<b>1340</b>) when the feature type <b>413</b> shows the feature having a width. Further, the feature position calculating unit <b>174</b> selects the feature position (the second candidate) calculated in the position calculating process B (S<b>1353</b>) if the feature type <b>413</b> shows the feature having no width.</p>
<p id="p-0599" num="0606">In the result storing process (S<b>1370</b>), the measurement position data memory unit <b>599</b> stores the feature position selected (decided) in the position calculating process C (S<b>1360</b>B) with relating to the measurement image point <b>412</b>, the specific image number <b>411</b>, and the feature type <b>413</b> as the measurement position data.</p>
<p id="p-0600" num="0607">As discussed above, the road feature measurement screen <b>400</b> can decide a candidate having a higher precision between the first candidate and the second candidate without making the user select.</p>
<heading id="h-0029" level="1">Embodiment 9</heading>
<p id="p-0601" num="0608">In the second through the eighth embodiments, the measurement apparatus (the road feature measurement apparatus, or the CAD apparatus <b>600</b>, for example) includes the image memory unit (the observation data memory unit <b>199</b>), the three-dimensional point cloud model memory unit (the observation data memory unit <b>199</b>), the image displaying unit <b>341</b>, the measurement image point obtaining unit (the image point inputting unit <b>342</b>), and the position calculating unit (the road surface model corresponding point searching unit <b>170</b>). Then, the position calculating unit detects the corresponding point corresponding to the measurement image point obtained by the measurement image point obtaining unit from the point cloud of the three-dimensional point cloud model stored in the three-dimensional point cloud model memory unit, and decides a three-dimensional position of the measurement image point obtained by the measurement image unit using the position of the detected corresponding point.</p>
<p id="p-0602" num="0609">In the ninth embodiment, another embodiment will be explained, in which the user terminal (a measurement terminal device) includes (1) a measurement image point obtaining unit, (2) an image displaying unit, and a server (a measurement server device) includes (3) an image memory unit, (4) a three-dimensional point cloud model memory unit, and (5) a position calculating unit.</p>
<p id="p-0603" num="0610">The user terminal sends information of the measurement image point to the server, and the server decides a three-dimensional position of the measurement image point (the feature position) received and sends to the user terminal.</p>
<p id="p-0604" num="0611">Items being different from the first through the eighth embodiments will be mainly discussed in the following, and items for which explanation is omitted can be considered as the same as the first through the eighth embodiment.</p>
<p id="p-0605" num="0612"><figref idref="DRAWINGS">FIG. 45</figref> shows a functional configuration of the measurement server device <b>710</b> and the measurement terminal device <b>720</b> according to the ninth embodiment.</p>
<p id="p-0606" num="0613">In <figref idref="DRAWINGS">FIG. 45</figref>, the measurement terminal device <b>720</b> (a user terminal, hereinafter) and the measurement server device <b>710</b> (a server, hereinafter) communicate via the Internet <b>940</b>.</p>
<p id="p-0607" num="0614">The server (the measurement server device <b>710</b>) provides, as a Web server, the user terminal connected to the Internet <b>940</b> with a function to measure the feature position.</p>
<p id="p-0608" num="0615">The user terminal (the measurement terminal device <b>720</b>) accesses the server using the Web browser, requests the server to measure a feature position corresponding to a measurement image point, and obtains the feature position corresponding to the measurement image point by receiving the measured feature position from the server.</p>
<p id="p-0609" num="0616">Here, the server and the user terminal can be connected with communication network other than the Internet <b>940</b> (LAN, for example); the server can be other than the Web server; and the user terminal can access the server without using the Web browser.</p>
<p id="p-0610" num="0617">In <figref idref="DRAWINGS">FIG. 45</figref>, the same signs are appended to similar configuration to the ones which have been explained in other embodiments.</p>
<p id="p-0611" num="0618">Further, the server-side image point inputting unit <b>342</b><i>s </i>and the user-side image point inputting unit <b>342</b><i>u </i>form a configuration corresponding to the image point inputting unit <b>342</b> which has been explained in other embodiments.</p>
<p id="p-0612" num="0619"><figref idref="DRAWINGS">FIG. 46</figref> is a flowchart showing a measuring method according to the ninth embodiment.</p>
<p id="p-0613" num="0620">The measuring method according to the ninth embodiment will be explained in the following with reference to <figref idref="DRAWINGS">FIG. 46</figref>.</p>
<p id="p-0614" num="0621">&#x3c;S<b>1411</b>: Image Presenting Process&#x3e;</p>
<p id="p-0615" num="0622">The image presenting unit <b>711</b> of the server (the measurement server device) sends the image stored in (3) the image memory unit <b>708</b> and the road surface shape model stored in (4) the three-dimensional point cloud model memory unit <b>709</b> to the user terminal by data transmission via broadband. At this time, the road surface shape model to be sent to the user terminal is the projected point cloud of the image, which is sent together, projected by the model projecting unit <b>172</b>. Further, only the image can be sent to the user terminal and the projected point cloud can remain unsent to the user terminal.</p>
<p id="p-0616" num="0623">&#x3c;S<b>1412</b>: Image Displaying Process&#x3e;</p>
<p id="p-0617" num="0624">In the user terminal (the measurement terminal device), (2) the image displaying unit <b>341</b> displays the measurement screen by superimposing the image and the projected point cloud sent from the server on the display (the displaying device <b>901</b>) of the user terminal. For example, the image displaying unit <b>341</b> displays the road feature measurement screen <b>400</b> (<figref idref="DRAWINGS">FIG. 37</figref>) or the CAD screen <b>620</b> (<figref idref="DRAWINGS">FIG. 34</figref>) as the measurement screen by superimposing the image and the projected point cloud.</p>
<p id="p-0618" num="0625">By selection by the user, the display can be switched between superimposed display of the image and the projected point cloud and display of only the image.</p>
<p id="p-0619" num="0626">The image displaying unit <b>341</b> can be a program supplied from the server to the user terminal. At this time, it is assumed that the image displaying unit <b>341</b> has been previously downloaded from the server and installed in the user terminal.</p>
<p id="p-0620" num="0627">The image displaying process (S<b>1412</b>) corresponds to, for example, the image displaying process A (S<b>1111</b>) and the image displaying process B (S<b>1113</b>).</p>
<p id="p-0621" num="0628">&#x3c;S<b>1421</b>: Terminal-Side Measurement image point Obtaining Process&#x3e;</p>
<p id="p-0622" num="0629">The user, by the user terminal, using an inputting device such as a mouse or a light pen, etc., clicks a point (a pixel) within the image of the measurement screen as a measurement image point. The user terminal obtains the measurement image point by a user-side image point inputting unit <b>342</b><i>u </i>((1) the measurement image point obtaining unit) based on the clicked point and sends the measurement image point to the server.</p>
<p id="p-0623" num="0630">Further, the user-side image point inputting unit <b>342</b><i>u </i>can send the identification number of the image together with the measurement image point. Further, the user-side image point inputting unit <b>342</b><i>u </i>can input a feature type as well as the type inputting process (S<b>1120</b>) and can send the measurement image point and the feature type to the server.</p>
<p id="p-0624" num="0631">&#x3c;S<b>1422</b>: Server-Side Measurement Image Point Obtaining Process&#x3e;</p>
<p id="p-0625" num="0632">In the server, the server-side image point inputting unit <b>342</b><i>s </i>receives the measurement image point from the user terminal.</p>
<p id="p-0626" num="0633">The terminal-side measurement image point obtaining process (S<b>1421</b>) and the server-side measurement image point obtaining process (S<b>1422</b>) correspond to, for example, the measurement image point obtaining process (S<b>1120</b>).</p>
<p id="p-0627" num="0634">&#x3c;S<b>1430</b>: Corresponding Point Detecting Process&#x3e;</p>
<p id="p-0628" num="0635">In the server, the neighborhood extracting unit <b>171</b> extracts a neighboring point of the measurement image point from a projected point cloud similarly to the corresponding point detecting process (S<b>1130</b>).</p>
<p id="p-0629" num="0636">&#x3c;S<b>1440</b>: Position Calculating Process&#x3e;</p>
<p id="p-0630" num="0637">In the server, the feature position calculating unit <b>174</b> obtains a three-dimensional position of the neighboring point from the three-dimensional point cloud model memory unit <b>709</b> as the feature position similarly to the position calculating process (S<b>1140</b>) and sends the feature position to the user terminal.</p>
<p id="p-0631" num="0638">&#x3c;S<b>1450</b>: Result Storing Process&#x3e;</p>
<p id="p-0632" num="0639">In the user terminal, the image displaying unit <b>341</b> displays the feature position sent from the server on the measurement screen, and the measurement position data memory unit <b>599</b> stores the feature position sent from the server by relating to the measurement image point, the feature type, and the image number as the measurement position data.</p>
<p id="p-0633" num="0640">The result storing process (S<b>1450</b>) corresponds to, for example, the result storing process (S<b>1150</b>).</p>
<p id="p-0634" num="0641">Any method can be used for presenting the image (and the projected point cloud) from the server to the user terminal in the image presenting process (S<b>1411</b>).</p>
<p id="p-0635" num="0642">For example, it is possible that the server sends an image list such as a list of image numbers, a list of thumbnails of images, etc. to the user terminal, the user terminal sends a request for an image selected by the user from the list of images to the server, and the server sends the requested image to the user terminal.</p>
<p id="p-0636" num="0643">Further, for example, it is also possible that the server sends all the images to the user terminal and the user terminal stores all the images.</p>
<p id="p-0637" num="0644">Further, for example, the server generates arrangement information of all the images by assuming a case in which all the images are arranged lengthwise and crosswise (vertically and horizontally) or a case in which all the images are arranged depthwise, and sends one image (or images) A to the user terminal. Further, the user terminal displays the image A sent from the server on the image selection screen. Further, the user operates the mouse to move the mouse cursor in the image selection screen, and the user terminal sends the operation information of the mouse to the server. For example, the operation of the mouse is the operation to scroll vertically and horizontally in the image selection screen or the operation to turn up the image displayed on the image selection screen. Then, the server, according to the operation of the mouse, sends the image B arranged above/below/left to/right to or in front/back of the image A in the arrangement information to the user terminal, and the user terminal displays the image B sent from the server on the image selection screen according to the operation of the mouse. For example, the user terminal, according to the scroll in the image selection screen, displays the image A and the image B, which are arranged vertically, by scrolling. Further, for example, the user terminal displays that the image A is turned up so that the image B appears from beneath the image A.</p>
<p id="p-0638" num="0645">Namely, in ultrafast broadband environment, when the user scrolls the measurement screen, the scroll information is sent to the server side, the server sends the image located at scrolled destination (for example, an image which will appear when the currently displayed image is turned up) to the user terminal at each scroll.</p>
<p id="p-0639" num="0646">However, it is also possible that the server does not send the image at each scroll, but sends the images arranged in front of or after (or above/below/left to/right to) the image of the display target together with the image of the display target to the user terminal, and it is also possible the server at first sends all the images to the user terminal. The user terminal stores the sent images in the memory unit. These operations can reduce the number of transmission of the images from the server to the user terminal.</p>
<p id="p-0640" num="0647">The ninth embodiment can be combined with another embodiment.</p>
<p id="p-0641" num="0648">For example, the server includes the neighboring plane calculating unit <b>173</b> and the camera LOS computing unit <b>140</b>. At this time, the neighboring plane calculating unit <b>173</b> calculates the neighboring plane including the neighboring point similarly to the plane calculating process (S<b>1351</b>), and the camera LOS computing unit <b>140</b> calculates the LOS vector similarly to the vector calculating process (S<b>1352</b>). Then, the feature position calculating unit <b>174</b> calculates an intersecting point of the neighboring plane and the LOS vector as the feature position.</p>
<p id="p-0642" num="0649">Further, for example, the server also can include the feature region detecting unit <b>701</b>. At this time, the feature region detecting unit <b>701</b> detects the feature image region from the image similarly to the feature region detecting process (S<b>1213</b>), and the image presenting unit <b>711</b> sends the feature image region together with the image to the user terminal. Then, the image displaying unit <b>341</b> of the user terminal displays the image and the feature image region similarly to the image displaying process B (S<b>1214</b>).</p>
<p id="p-0643" num="0650">The ninth embodiment enables simultaneous operation of plural user terminals to decide three-dimensional coordinates of the features captured in the images.</p>
<p id="p-0644" num="0651">Further, even if the user itself does not have a server, the user can decide the three-dimensional coordinates of the feature captured in the image, which improves the convenience.</p>
<p id="p-0645" num="0652">In each embodiment, the motion stereo process (S<b>201</b>) and the moving body removing process (S<b>202</b>) can remain unperformed. Namely, the road surface shape model can include the laser measured point cloud showing the moving body.</p>
<p id="p-0646" num="0653">Further, in each embodiment, the feature identifying process (S<b>203</b>) can remain unperformed. Namely, the laser measured point cloud of the road surface shape model can remain unclassified for each feature type.</p>
<heading id="h-0030" level="1">BRIEF EXPLANATION OF THE DRAWINGS</heading>
<p id="p-0647" num="0654"><figref idref="DRAWINGS">FIG. 1</figref> shows a system configuration of a road feature measurement system <b>101</b> and a functional configuration of a road feature measurement apparatus <b>100</b> according to the first embodiment.</p>
<p id="p-0648" num="0655"><figref idref="DRAWINGS">FIG. 2</figref> shows an example of hardware resource for the road feature measurement apparatus <b>100</b> and a feature identification apparatus <b>300</b> according to the first embodiment.</p>
<p id="p-0649" num="0656"><figref idref="DRAWINGS">FIG. 3</figref> is a flowchart showing a flow of road feature position measuring process of the road feature measurement system <b>101</b> according to the first embodiment.</p>
<p id="p-0650" num="0657"><figref idref="DRAWINGS">FIG. 4</figref> shows positional relation among locations of a vehicle, LRF, and a camera <b>230</b> according to the first embodiment.</p>
<p id="p-0651" num="0658"><figref idref="DRAWINGS">FIG. 5</figref> shows positional relation among locations of the vehicle, LRF, and the camera <b>230</b> according to the first embodiment.</p>
<p id="p-0652" num="0659"><figref idref="DRAWINGS">FIG. 6</figref> shows a road surface shape model according to the first embodiment.</p>
<p id="p-0653" num="0660"><figref idref="DRAWINGS">FIG. 7</figref> shows a road surface shape model according to the first embodiment.</p>
<p id="p-0654" num="0661"><figref idref="DRAWINGS">FIG. 8</figref> shows a road surface shape model according to the first embodiment.</p>
<p id="p-0655" num="0662"><figref idref="DRAWINGS">FIG. 9</figref> shows an optical image corresponding to <figref idref="DRAWINGS">FIG. 8</figref>.</p>
<p id="p-0656" num="0663"><figref idref="DRAWINGS">FIG. 10</figref> shows an image on which projection transformation is performed according to the first embodiment.</p>
<p id="p-0657" num="0664"><figref idref="DRAWINGS">FIG. 11</figref> shows a configuration of a feature identification apparatus <b>300</b> according to the first embodiment.</p>
<p id="p-0658" num="0665"><figref idref="DRAWINGS">FIG. 12</figref> is a flowchart showing a flow of digitizing process (S<b>104</b>) of the feature identification apparatus <b>300</b> according to the first embodiment.</p>
<p id="p-0659" num="0666"><figref idref="DRAWINGS">FIG. 13</figref> shows a road surface shape model when a truck does not hide a pole.</p>
<p id="p-0660" num="0667"><figref idref="DRAWINGS">FIG. 14</figref> shows a road surface shape model when a truck hides a pole.</p>
<p id="p-0661" num="0668"><figref idref="DRAWINGS">FIG. 15</figref> is a flowchart showing a flow of motion stereo process (S<b>201</b>) according to the first embodiment.</p>
<p id="p-0662" num="0669"><figref idref="DRAWINGS">FIG. 16</figref> shows a calculating method of an epipolar line L<b>1</b> according to the first embodiment.</p>
<p id="p-0663" num="0670"><figref idref="DRAWINGS">FIG. 17</figref> shows that a corresponding point P<b>2</b> corresponding to a measurement target point PT<b>2</b> after moving does not exist on the epipolar line L<b>1</b>.</p>
<p id="p-0664" num="0671"><figref idref="DRAWINGS">FIG. 18</figref> is an image drawing of a voxel space voting process (S<b>305</b>) according to the first embodiment.</p>
<p id="p-0665" num="0672"><figref idref="DRAWINGS">FIG. 19</figref> shows a volume intersection according to the first embodiment.</p>
<p id="p-0666" num="0673"><figref idref="DRAWINGS">FIG. 20</figref> is an image showing a place where a feature specified by the user is easily misrecognized.</p>
<p id="p-0667" num="0674"><figref idref="DRAWINGS">FIG. 21</figref> shows feature identifying process (S<b>203</b>) according to the first embodiment.</p>
<p id="p-0668" num="0675"><figref idref="DRAWINGS">FIG. 22</figref> is a flowchart showing a flow of the feature identifying process (S<b>203</b>) according to the first embodiment.</p>
<p id="p-0669" num="0676"><figref idref="DRAWINGS">FIG. 23</figref> is a flowchart showing a flow of 3D modeling process (S<b>105</b>) according to the first embodiment.</p>
<p id="p-0670" num="0677"><figref idref="DRAWINGS">FIG. 24</figref> shows a calculating method of a feature position in feature position locating process (S<b>106</b>) according to the first embodiment.</p>
<p id="p-0671" num="0678"><figref idref="DRAWINGS">FIG. 25</figref> is a flowchart showing a flow of the feature position locating process (S<b>106</b>) according to the first embodiment.</p>
<p id="p-0672" num="0679"><figref idref="DRAWINGS">FIG. 26</figref> is a flowchart showing a flow of three neighboring points extracting process (S<b>503</b>) according to the first embodiment.</p>
<p id="p-0673" num="0680"><figref idref="DRAWINGS">FIG. 27</figref> shows the three neighboring points extracting process (S<b>503</b>) according to the first embodiment.</p>
<p id="p-0674" num="0681"><figref idref="DRAWINGS">FIG. 28</figref> shows a system configuration of a road feature measurement system <b>101</b> and a functional configuration of a road feature measurement apparatus B <b>500</b> according to the second embodiment.</p>
<p id="p-0675" num="0682"><figref idref="DRAWINGS">FIG. 29</figref> shows a road feature measurement screen <b>400</b> according to the second embodiment.</p>
<p id="p-0676" num="0683"><figref idref="DRAWINGS">FIG. 30</figref> shows a functional configuration of a road surface model corresponding point searching unit <b>170</b> according to the third embodiment.</p>
<p id="p-0677" num="0684"><figref idref="DRAWINGS">FIG. 31</figref> is a flowchart showing a flow of feature position locating process (S<b>106</b>) according to the third embodiment.</p>
<p id="p-0678" num="0685"><figref idref="DRAWINGS">FIG. 32</figref> shows a calculating method of feature position in feature position locating process (S<b>106</b>) according to the third embodiment.</p>
<p id="p-0679" num="0686"><figref idref="DRAWINGS">FIG. 33</figref> shows a system configuration of a road feature measurement system <b>101</b> and a functional configuration of a CAD apparatus <b>600</b> according to the fourth embodiment.</p>
<p id="p-0680" num="0687"><figref idref="DRAWINGS">FIG. 34</figref> shows a CAD screen <b>620</b> according to the fourth embodiment.</p>
<p id="p-0681" num="0688"><figref idref="DRAWINGS">FIG. 35</figref> shows a functional configuration of a road feature measurement apparatus C <b>700</b> according to the fifth embodiment.</p>
<p id="p-0682" num="0689"><figref idref="DRAWINGS">FIG. 36</figref> is a flowchart showing a measuring method according to the fifth embodiment.</p>
<p id="p-0683" num="0690"><figref idref="DRAWINGS">FIG. 37</figref> shows a road feature measurement image road feature measurement screen <b>400</b> according to the fifth embodiment.</p>
<p id="p-0684" num="0691"><figref idref="DRAWINGS">FIG. 38</figref> shows a functional configuration of a road feature measurement apparatus C <b>700</b> according to the sixth embodiment.</p>
<p id="p-0685" num="0692"><figref idref="DRAWINGS">FIG. 39</figref> is a flowchart showing a measuring method according to the sixth embodiment.</p>
<p id="p-0686" num="0693"><figref idref="DRAWINGS">FIG. 40</figref> shows a road feature measurement screen <b>400</b> according to the sixth embodiment.</p>
<p id="p-0687" num="0694"><figref idref="DRAWINGS">FIG. 41</figref> shows a functional configuration of a road feature measurement apparatus C <b>700</b> according to the seventh embodiment.</p>
<p id="p-0688" num="0695"><figref idref="DRAWINGS">FIG. 42</figref> is a flowchart showing a measuring method according to the seventh embodiment.</p>
<p id="p-0689" num="0696"><figref idref="DRAWINGS">FIG. 43</figref> shows a road feature measurement screen <b>400</b> according to the seventh embodiment.</p>
<p id="p-0690" num="0697"><figref idref="DRAWINGS">FIG. 44</figref> is a flowchart showing a measuring method according to the eighth embodiment.</p>
<p id="p-0691" num="0698"><figref idref="DRAWINGS">FIG. 45</figref> shows functional configurations of a measurement server device <b>710</b> and a measurement terminal device <b>720</b> according to the ninth embodiment.</p>
<p id="p-0692" num="0699"><figref idref="DRAWINGS">FIG. 46</figref> is a flowchart showing a measuring method according to the ninth embodiment.</p>
<heading id="h-0031" level="1">EXPLANATION OF SIGNS</heading>
<p id="p-0693" num="0000">
<ul id="ul0008" list-style="none">
    <li id="ul0008-0001" num="0000">
    <ul id="ul0009" list-style="none">
        <li id="ul0009-0001" num="0700"><b>100</b>: a road feature measurement apparatus; <b>101</b>: a road feature measurement system; <b>102</b>: a measuring carriage; <b>103</b>: a top board; <b>110</b>: a vehicle position and posture (3-axis) computing unit; <b>120</b>: a white line recognition processing unit, <b>130</b>: a camera position and posture computing unit; <b>140</b>: a camera LOS computing unit; <b>150</b>: a road surface shape model generating unit; <b>160</b>: a laser radar position and posture computing unit; <b>170</b>: a road surface model corresponding point searching unit; <b>171</b>: a neighborhood extracting unit; <b>172</b>: a model projecting unit; <b>173</b>: a neighboring plane calculating unit; <b>174</b>: a feature position calculating unit; <b>180</b>: a white line position computing unit; <b>191</b>: an observation data inputting unit: <b>199</b> an observation data memory unit; <b>200</b>: an odometry apparatus; <b>210</b>: a gyro; <b>220</b>: GPS; <b>230</b>: a camera; <b>240</b>: a laser radar; <b>300</b>: feature identification apparatus; <b>310</b>: a motion stereo unit; <b>311</b>: a stationary body discriminating unit; <b>312</b>: a stationary body model generating unit; <b>320</b>: a moving body removing unit; <b>321</b>: a moving body discriminating unit; <b>322</b>: moving body removed model generating unit; <b>330</b>: a feature identifying unit; <b>331</b>: a labeling unit; <b>332</b>: an edge determining unit; <b>333</b>: a feature determining unit; <b>340</b>: a measurement image point obtaining unit; <b>341</b>: an image displaying unit; <b>342</b>: an image point inputting unit; <b>342</b><i>s</i>: a server-side image point inputting unit; <b>342</b><i>u</i>: a user-side image point inputting unit; <b>400</b>: a road feature measurement screen; <b>401</b>: a captured image; <b>402</b>: a mouse cursor; <b>403</b>: a km post; <b>404</b>: a sidewalk; <b>405</b>: a street; <b>406</b>: a center line; <b>407</b>: a white line; <b>408</b>: a road sign; <b>409</b>: an electric pole; <b>411</b>: a specific image number; <b>412</b>: a measurement image point; <b>412</b>A: a point &#x201c;A&#x201d;; <b>412</b>B: a point &#x201c;B&#x201d;; <b>413</b>: a feature type; <b>414</b>, <b>414</b><i>a</i>, and <b>414</b><i>b</i>: feature positions; <b>415</b>: a calculation requesting button; <b>416</b>: a storage requesting button; <b>417</b>: a type list box; <b>418</b>: an edge stone; <b>420</b>: a selection list box; <b>421</b>: a projected point; <b>421</b><i>a</i>: a point &#x201c;a&#x201d;; <b>421</b><i>b</i>: a point &#x201c;b&#x201d;; <b>421</b><i>c</i>: a point &#x201c;c&#x201d;; <b>421</b><i>d</i>: a point &#x201c;d&#x201d;; <b>421</b><i>e</i>: a point &#x201c;e&#x201d;; <b>421</b><i>f</i>: a point &#x201c;f&#x201d;; <b>422</b>: a predetermined range; <b>423</b>: a point &#x201c;x&#x201d;; <b>424</b>: an edge; <b>500</b>: a road feature measurement apparatus B; <b>598</b>: an observation data memory unit B; <b>599</b>: a measurement position data memory unit; <b>600</b>: a CAD apparatus; <b>610</b>: a drafting unit; <b>620</b>: a CAD screen; <b>621</b>: a figure toolbar; <b>622</b>: a road map; <b>623</b>: a measurement screen displaying button; <b>624</b>: a storage requesting button; <b>625</b>: a km post; <b>699</b>: a CAD memory unit; <b>700</b>: a road feature measurement apparatus C; <b>701</b>: a feature region detecting unit; <b>708</b>: an image memory unit; <b>709</b>: a three-dimensional point cloud model memory unit; <b>710</b>: a measurement server device; <b>711</b>: an image presenting unit; <b>720</b>: a measurement terminal device; <b>901</b>: a displaying device; <b>902</b>: a keyboard; <b>903</b>: a mouse; <b>904</b>: FDD; <b>905</b>: CDD; <b>906</b>: a printer device; <b>907</b>: a scanner device; <b>908</b>: a microphone; <b>909</b>: a speaker; <b>911</b>: CPU; <b>912</b>: a bus; <b>913</b>: ROM; <b>914</b>: RAM; <b>915</b>: a communication board; <b>920</b>: a magnetic disk drive; <b>921</b>: OS; <b>922</b>: a window system; <b>923</b>: a group of programs; <b>924</b>: a group of files; and <b>940</b>: the Internet.</li>
    </ul>
    </li>
</ul>
</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08625851-20140107-M00001.NB">
<img id="EMI-M00001" he="15.49mm" wi="76.20mm" file="US08625851-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US08625851-20140107-M00002.NB">
<img id="EMI-M00002" he="17.27mm" wi="76.20mm" file="US08625851-20140107-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00003" nb-file="US08625851-20140107-M00003.NB">
<img id="EMI-M00003" he="25.74mm" wi="76.20mm" file="US08625851-20140107-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00004" nb-file="US08625851-20140107-M00004.NB">
<img id="EMI-M00004" he="17.27mm" wi="76.20mm" file="US08625851-20140107-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00005" nb-file="US08625851-20140107-M00005.NB">
<img id="EMI-M00005" he="25.74mm" wi="76.20mm" file="US08625851-20140107-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00006" nb-file="US08625851-20140107-M00006.NB">
<img id="EMI-M00006" he="29.29mm" wi="76.20mm" file="US08625851-20140107-M00006.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00007" nb-file="US08625851-20140107-M00007.NB">
<img id="EMI-M00007" he="25.74mm" wi="76.20mm" file="US08625851-20140107-M00007.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00008" nb-file="US08625851-20140107-M00008.NB">
<img id="EMI-M00008" he="11.26mm" wi="76.20mm" file="US08625851-20140107-M00008.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00009" nb-file="US08625851-20140107-M00009.NB">
<img id="EMI-M00009" he="32.09mm" wi="76.20mm" file="US08625851-20140107-M00009.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00010" nb-file="US08625851-20140107-M00010.NB">
<img id="EMI-M00010" he="21.17mm" wi="76.20mm" file="US08625851-20140107-M00010.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00011" nb-file="US08625851-20140107-M00011.NB">
<img id="EMI-M00011" he="29.29mm" wi="76.20mm" file="US08625851-20140107-M00011.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00012" nb-file="US08625851-20140107-M00012.NB">
<img id="EMI-M00012" he="38.44mm" wi="76.20mm" file="US08625851-20140107-M00012.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00013" nb-file="US08625851-20140107-M00013.NB">
<img id="EMI-M00013" he="11.68mm" wi="76.20mm" file="US08625851-20140107-M00013.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00014" nb-file="US08625851-20140107-M00014.NB">
<img id="EMI-M00014" he="29.29mm" wi="76.20mm" file="US08625851-20140107-M00014.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A feature identification apparatus comprising:
<claim-text>a labeling processing unit that
<claim-text>extracts, via a processor, a point cloud continuing from a position within a point cloud of a road surface shape model, the road surface shape model being a three-dimensional point cloud model generated based on distance and orientation data showing distance and orientation for a feature measured from a running vehicle, and</claim-text>
<claim-text>forms groups within the point cloud of the road surface shape model; an edge determining unit that</claim-text>
<claim-text>determines an edge part from a line segment formed by a point cloud for each group grouped by the labeling processing unit, and</claim-text>
<claim-text>groups the group using the edge part as a border; and</claim-text>
</claim-text>
<claim-text>a feature identifying unit that determines a type of feature represented by a point cloud of each group based on a position and a shape shown by a point cloud for each group grouped by the edge determining unit,</claim-text>
<claim-text>wherein the groups formed by the labeling processing unit include a first group that is grouped into a first sub-group, second sub-group and third sub-group,</claim-text>
<claim-text>the first sub-group having a point cloud corresponding to a position of the running vehicle, the second sub-group having one or more point clouds located higher than the first sub-group within a predetermined range and being continuous in vertical and horizontal directions, and the third-sub group having one or more point clouds continuous in the vertical direction.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The feature identification apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein
<claim-text>the groups formed by the labeling processing unit further include a second group,</claim-text>
<claim-text>the first group includes a measured point cloud having distance and orientation data that is continuous from a measured point and similar to that of the running vehicle, and</claim-text>
<claim-text>the second group includes any measured point cloud that is not included within the first group.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The feature identification apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the feature identifying unit identifies the type of the first-sub group as a street and the second sub-group as a sidewalk.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The feature identification apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>a motion stereo unit that generates a three-dimensional model of a stationary body for a plurality of images captured at different times by a motion stereo process as a stationary body model; and</claim-text>
<claim-text>a moving body removing unit that removes a difference between the three-dimensional point cloud model and the stationary body model generated by the motion stereo unit from the three-dimensional point cloud model, and generates a moving body removed model,</claim-text>
<claim-text>wherein the feature identifying unit determines a type of the stationary body represented by each group based on a position and a shape shown by a point cloud of the moving body removed model generated by the moving body removing unit.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. A feature identification method implemented by a feature identification apparatus comprising:
<claim-text>extracting, via a processor, a point cloud continuing from a position within a point cloud of a road surface shape model, the road surface shape model being a three-dimensional point cloud model generated based on distance and orientation data showing distance and orientation for a feature measured from a running vehicle;</claim-text>
<claim-text>forming groups within the point cloud of the road surface shape model;</claim-text>
<claim-text>determining an edge part from a line segment formed by a point cloud for each group;</claim-text>
<claim-text>grouping the group using the edge part as a border; and</claim-text>
<claim-text>determining a type of feature represented by a point cloud of each group based on a position and a shape shown by a point cloud for each group,</claim-text>
<claim-text>wherein the groups include a first group that is grouped into a first sub-group, second sub-group and third sub-group,</claim-text>
<claim-text>the first sub-group having a point cloud corresponding to a position of the running vehicle, the second sub-group having one or more point clouds located higher than the first sub-group within a predetermined range and being continuous in vertical and horizontal directions, and the third-sub group having one or more point clouds continuous in the vertical direction.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. A non-transitory computer-readable medium storing computer readable instructions thereon that when executed by a computer cause the computer to perform a method comprising:
<claim-text>extracting, via a processor, a point cloud continuing from a position within a point cloud of a road surface shape model, the road surface shape model being a three-dimensional point cloud model generated based on distance and orientation data showing distance and orientation for a feature measured from a running vehicle;</claim-text>
<claim-text>forming groups within the point cloud of the road surface shape model;</claim-text>
<claim-text>determining an edge part from a line segment formed by a point cloud for each group;</claim-text>
<claim-text>grouping the group using the edge part as a border; and</claim-text>
<claim-text>determining a type of feature represented by a point cloud of each group based on a position and a shape shown by a point cloud for each group,</claim-text>
<claim-text>wherein the groups include a first group that is grouped into a first sub-group, second sub-group and third sub-group,</claim-text>
<claim-text>the first sub-group having a point cloud corresponding to a position of the running vehicle, the second sub-group having one or more point clouds located higher than the first sub-group within a predetermined range and being continuous in vertical and horizontal directions, and the third-sub group having one or more point clouds continuous in the vertical direction. </claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
