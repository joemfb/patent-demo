<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08624902-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08624902</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12699896</doc-number>
<date>20100204</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<us-term-of-grant>
<us-term-extension>611</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20110101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>T</subclass>
<main-group>13</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>345473</main-classification>
<further-classification>345419</further-classification>
<further-classification>345427</further-classification>
</classification-national>
<invention-title id="d2e53">Transitioning between top-down maps and local navigation of reconstructed 3-D scenes</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>6212420</doc-number>
<kind>B1</kind>
<name>Wang et al.</name>
<date>20010400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>600407</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6324469</doc-number>
<kind>B1</kind>
<name>Okude et al.</name>
<date>20011100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6571024</doc-number>
<kind>B1</kind>
<name>Sawhney et al.</name>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6619406</doc-number>
<kind>B1</kind>
<name>Kacyra et al.</name>
<date>20030900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>172  45</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6639594</doc-number>
<kind>B2</kind>
<name>Zhang et al.</name>
<date>20031000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6760027</doc-number>
<kind>B2</kind>
<name>Endo et al.</name>
<date>20040700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>7148892</doc-number>
<kind>B2</kind>
<name>Robertson et al.</name>
<date>20061200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>7386394</doc-number>
<kind>B2</kind>
<name>Shulman</name>
<date>20080600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2002/0076085</doc-number>
<kind>A1</kind>
<name>Shimazu</name>
<date>20020600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382100</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2004/0085335</doc-number>
<kind>A1</kind>
<name>Burlnyk et al.</name>
<date>20040500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345716</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2004/0125138</doc-number>
<kind>A1</kind>
<name>Jetha</name>
<date>20040700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2005/0134945</doc-number>
<kind>A1</kind>
<name>Gallagher</name>
<date>20050600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2005/0156945</doc-number>
<kind>A1</kind>
<name>Asami</name>
<date>20050700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2008/0221843</doc-number>
<kind>A1</kind>
<name>Shenkar et al.</name>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2008/0222558</doc-number>
<kind>A1</kind>
<name>Cho et al.</name>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2008/0246759</doc-number>
<kind>A1</kind>
<name>Summers</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>2008/0247636</doc-number>
<kind>A1</kind>
<name>Davis et al.</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>2008/0268876</doc-number>
<kind>A1</kind>
<name>Gelfand et al.</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>2008/0291217</doc-number>
<kind>A1</kind>
<name>Vincent et al.</name>
<date>20081100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>2009/0002394</doc-number>
<kind>A1</kind>
<name>Chen et al.</name>
<date>20090100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>2009/0237510</doc-number>
<kind>A1</kind>
<name>Chen et al.</name>
<date>20090900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>WO</country>
<doc-number>2009089125</doc-number>
<kind>A2</kind>
<date>20090700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00023">
<othercit>Zitnick, et al. &#x201c;High-Quality Video View Interpolation Using a Layered Representation&#x201d;, Retrieved at&#x3c;&#x3c;http://vclab.gist.ac.kr/&#x2dc;sblee/blog/attachment/1087389239.pdf&#x3e;&#x3e;, pp. 9.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00024">
<othercit>Mastumoto, et al. &#x201c;Visual Navigation Using Omnidirectional View Sequence&#x201d;, Retrieved at&#x3c;&#x3c;http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&#x26;arnumber=813023&#x26;isnumber=17600&#x3e;&#x3e;. Proceedings of the 1999 IEEVRSJ International Conference on Intelligent Robots and Systems, 1999 IEEE, pp. 317-322.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00025">
<othercit>Terzopoulos, et al. &#x201c;Dynamic 3D Models with Local and Global Deformations: Deformable Superquadrics&#x201d;, Retrieved at &#x3c;&#x3c;www.cs.ucla.edu/&#x2dc;dt/papers/pami91/pami91.pdf&#x3e;&#x3e;, IEEE Transcatons on Pattern Analysis and Machine Intelligence, vol. 13, No. 7, Jul. 1991, pp. 703-714.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00026">
<othercit>Oskam, et al. &#x201c;Visibility Transition Planning for Dynamic Camera Control&#x201d;, Retrieved at&#x3c;&#x3c;http://graphics.ethz.ch/Downloads/Publications/Papers/2009/Osk09/Osk09.pdf&#x3e;&#x3e;, Eurographics/ACM SIGGRAPH Symposium on Computer Animation (2009), pp. 47-57.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00027">
<othercit>Koch, et al. &#x201c;3D Reconstruction and Rendering from Image Sequences&#x201d;, Retrieved at&#x3c;&#x3c;http://www.ist.matris.org/publications/WIAMIS-Reconstruction.pdf&#x3e;&#x3e;, pp. 4.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00028">
<othercit>U.S. Official Action dated Jun. 21, 2012 in U.S. Appl. No. 12/699,902.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00029">
<othercit>U.S. Official Action dated Jun. 22, 2012 in U.S. Appl. No. 12/699,904.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00030">
<othercit>&#x201c;Genesis IV Tutorial: Getting Started&#x2014;The Main Window,&#x201d; Oct. 5. 2009, Retrieved from http://www.geomantics.com/tutorial01.htm, 1 pp.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00031">
<othercit>Chen et al. &#x201c;A Virtual Environment System for the Comparison of Dome and HMD Systems&#x201d;, 2003, Proceedings of the International Conference on Computer Graphics and Spatial Information Systems, pp. 50-58.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00032">
<othercit>Hachet et al. &#x201c;3D Panorama Service on Mobile Device for Hiking,&#x201d; Apr. 28-May 3, 2007, Retrieved at&#x3c;&#x3c;http://msLftw.aUpapers/6<sub>&#x2014;</sub>hachet.pdf&#x3e;&#x3e;, CHI Workshop 2007, San Jose, USA, pp. 1-4.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00033">
<othercit>Hogue et al. &#x201c;Underwater Environment Reconstruction using Stereo and Inertial Data&#x201d;, Oct. 7-10. 2007, IEEE International Conference on Systems, Man and Cybernetics, 6 pp.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00034">
<othercit>Kang et al. &#x201c;Smooth Scene Transition for Virtual Tour on the World Wide Web&#x201d;, 2005, IEEE Proceedings of the Sixth International Conference on Computational Intelligence and Multimedia Applications (ICCIMA'05), 6 pp.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00035">
<othercit>Koch et al. &#x201c;3D Reconstruction and Rendering from Image Sequences&#x201d;, 2005, WIAMIS 05, Retrieved at http://www.ist-matrix.org/ publications/WIAMIS-Reconstruction.pdf, pp. 4.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00036">
<othercit>Sudarsanam et al. &#x201c;Non-linear Perspective Widgets for Creating Multiple-View Images&#x201d;, Retrieved at http:// www.cs.wustl.edu/-cmg/contentlpapers/npar2008np/npar2008np.pdf, 9 pp, Jun. 2008.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00037">
<othercit>Wulf , et al. &#x201c;Colored 2D Maps for Robot Navigation with 3D Sensor Data&#x201d;, Sep. 28-Oct. 2, 2004, IEEE/RSJ International Conference on Intelligent Robots and Systems, 6 pp.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00038">
<othercit>Wulf, et al. &#x201c;2D Mapping of Cluttered Indoor Environments by Means of 3D Perception,&#x201d; Apr. 2004, Proceedings of the 2004 IEEE International Conference on Robotics &#x26; Automatics, New Orleans, LA, pp. 4204-4209.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00039">
<othercit>U.S. Official Action dated Dec. 14, 2012 in U.S. Appl. No. 12/699,902.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00040">
<othercit>U.S. Official Action dated Apr. 5, 2013 in U.S. Appl. No. 12/699,902.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00041">
<othercit>U.S. Official Action dated Dec. 12, 2012 in U.S. Appl. No. 12/699,904.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00042">
<othercit>U.S. Official Action dated Dec. 15, 2012 in U.S. Appl. No. 12/699,904.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00043">
<othercit>Zitnick et al. &#x201c;High-Quality Video View Interpolation Using a Layered Representation&#x201d;, Aug. 2004, Proceedings of ACM SIGGRAPH 2004, 23(3): 600-608.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00044">
<othercit>Sudarsanam et al. &#x201c;Non-linear Perspective Widgets for Creating Multiple-View Images&#x201d;, Retrieved at http:// www.cs.wustl.edu/-cmg/contentlpapers/npar2008np/npar2008np.pdf, Proceedings of the 6th International Symposium on Non-Photorealistic Animation and Rendering, ACM, 2008, 9 pp.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00045">
<othercit>Koch et al. &#x201c;3D Reconstruction and Rendering from Image Sequences&#x201d;, 2005 WIAMIS 05, Retrieved at http://wwww.ist-matns.org/ publications/WIAMIS-Reconstruction.pdf, Pages 4.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00046">
<othercit>U.S. Official Action dated Sep. 18, 2013 in U.S. Appl. No. 12/699,902.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00047">
<othercit>U.S. Official Action dated Sep. 23, 2013 in U.S. Appl. No. 12/699,904.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>17</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>None</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>6</number-of-drawing-sheets>
<number-of-figures>8</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20110187723</doc-number>
<kind>A1</kind>
<date>20110804</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Chen</last-name>
<first-name>Billy</first-name>
<address>
<city>Bellevue</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Ofek</last-name>
<first-name>Eyal</first-name>
<address>
<city>Redmond</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Gedye</last-name>
<first-name>David Maxwell</first-name>
<address>
<city>Seattle</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="004" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Dughi</last-name>
<first-name>Jonathan Robert</first-name>
<address>
<city>Bainbridge Island</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="005" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Dawson</last-name>
<first-name>Mark Ruane</first-name>
<address>
<city>Sammamish</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="006" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Podolak</last-name>
<first-name>Joshua</first-name>
<address>
<city>Seattle</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Chen</last-name>
<first-name>Billy</first-name>
<address>
<city>Bellevue</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Ofek</last-name>
<first-name>Eyal</first-name>
<address>
<city>Redmond</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Gedye</last-name>
<first-name>David Maxwell</first-name>
<address>
<city>Seattle</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="004" designation="us-only">
<addressbook>
<last-name>Dughi</last-name>
<first-name>Jonathan Robert</first-name>
<address>
<city>Bainbridge Island</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="005" designation="us-only">
<addressbook>
<last-name>Dawson</last-name>
<first-name>Mark Ruane</first-name>
<address>
<city>Sammamish</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="006" designation="us-only">
<addressbook>
<last-name>Podolak</last-name>
<first-name>Joshua</first-name>
<address>
<city>Seattle</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Hope Baldauff, LLC</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Microsoft Corporation</orgname>
<role>02</role>
<address>
<city>Redmond</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Broome</last-name>
<first-name>Said</first-name>
<department>2679</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Technologies are described herein for transitioning between a top-down map of a reconstructed structure within a 3-D scene and an associated local-navigation display. An application transitions between the top-down map and the local-navigation display by animating a view in a display window over a period of time while interpolating camera parameters from values representing a starting camera view to values representing an ending camera view. In one embodiment, the starting camera view is the top-down map view and the ending camera view is the camera view associated with a target photograph. In another embodiment, the starting camera view is the camera view associated with a currently-viewed photograph in the local-navigation display and the ending camera view is the top-down map.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="140.97mm" wi="255.69mm" file="US08624902-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="269.66mm" wi="175.09mm" orientation="landscape" file="US08624902-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="273.22mm" wi="203.45mm" orientation="landscape" file="US08624902-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="225.98mm" wi="158.75mm" orientation="landscape" file="US08624902-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="289.64mm" wi="165.69mm" orientation="landscape" file="US08624902-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="282.62mm" wi="194.06mm" orientation="landscape" file="US08624902-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="235.03mm" wi="192.28mm" orientation="landscape" file="US08624902-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND</heading>
<p id="p-0002" num="0001">Using the processing power of computers, it is possible to create a visual reconstruction of a scene or structure from a collection of digital photographs (&#x201c;photographs&#x201d;) of the scene. The reconstruction may consist of the various perspectives provided by the photographs coupled with a group of three-dimensional (&#x201c;3-D&#x201d;) points computed from the photographs. The 3-D points may be computed by locating common features, such as objects or edges, in a number of the photographs, and using the position, perspective, and visibility or obscurity of the features in each photograph to determine a 3-D position of the feature. The visualization of 3-D points computed for the collection of photographs is referred to as a &#x201c;3-D point cloud.&#x201d; For example, given a collection of photographs of a cathedral from several points of view, a 3-D point cloud may be computed that represents the cathedral's geometry. The 3-D point cloud may be utilized to enhance the visualization of the cathedral's structure when viewing the various photographs in the collection.</p>
<p id="p-0003" num="0002">Current applications may allow a user to navigate a visual reconstruction by moving from one photograph to nearby photographs within the view. For example, to move to a nearby photograph, the user may select a highlighted outline or &#x201c;quad&#x201d; representing the nearby photograph within the view. This may result in the view of the scene and accompanying structures being changed to the perspective of the camera position, or &#x201c;pose,&#x201d; corresponding to the selected photograph in reference to the 3-D point cloud. This form of navigation is referred to as &#x201c;local navigation.&#x201d;</p>
<p id="p-0004" num="0003">Local navigation, however, may be challenging for a user. First, photographs that are not locally accessible or shown as a quad within the view may be difficult to discover. Second, after exploring a reconstruction, the user may not retain an understanding of the environment or spatial context of the captured scene. For example, the user may not appreciate the size of a structure captured in the reconstruction or have a sense of which aspects of the overall scene have been explored. Furthermore, since the photographs likely do not sample the scene at a regular rate, a local navigation from one photograph to the next may result in a small spatial move or a large one, with the difference not being easily discernable by the user. This ambiguity may further reduce the ability of the user to track the global position and orientation of the current view of the reconstruction.</p>
<p id="p-0005" num="0004">It is with respect to these considerations and others that the disclosure made herein is presented.</p>
<heading id="h-0002" level="1">SUMMARY</heading>
<p id="p-0006" num="0005">Technologies are described herein for transitioning between a top-down map of a reconstructed structure within a 3-D scene and an associated local-navigation display. In a display of a visual reconstruction of the 3-D scene, a user may utilize the top-down map as an alternative means of navigating the photographs within the reconstruction, enhancing the user's understanding of the environment and spatial context of the scene while improving the discoverability of photographs not easily discovered through local navigation. If the user selects a camera pose, object, point, or other element of the reconstruction on the top-down map, the display may be transitioned to the local-navigation display showing a representative photograph based on the selected element. Utilizing the technologies described herein, this transition may be performed in such way as to preserve the continuity between the top-down map and the local-navigation display without causing confusing or visually unpleasant effects like camera spiral or vertigo.</p>
<p id="p-0007" num="0006">According to embodiments, an application transitions between the top-down map and the local-navigation display by animating a view in a display window over a period of time while interpolating camera parameters from values representing a starting camera view to values representing an ending camera view. In one embodiment, the starting camera view is the top-down map view and the ending camera view is the camera view associated with a target photograph. In another embodiment, the starting camera view is the camera view associated with a currently-viewed photograph in the local-navigation display and the ending camera view is the top-down map. In yet another embodiment, the starting camera view is the camera view associated with a currently-viewed photograph in the local-navigation display and the ending camera view is the camera view associated with another photograph within the reconstruction, with the animation moving from the starting camera view to the top-down map view and then from the top-down map view to the ending camera view.</p>
<p id="p-0008" num="0007">It should be appreciated that the above-described subject matter may be implemented as a computer-controlled apparatus, a computer process, a computing system, or as an article of manufacture such as a computer-readable medium. These and various other features will be apparent from a reading of the following Detailed Description and a review of the associated drawings.</p>
<p id="p-0009" num="0008">This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended that this Summary be used to limit the scope of the claimed subject matter. Furthermore, the claimed subject matter is not limited to implementations that solve any or all disadvantages noted in any part of this disclosure.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram showing aspects of an illustrative operating environment and several software components provided by the embodiments presented herein;</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 2</figref> is a display diagram showing an illustrative user interface for displaying a local-navigation display of a visual reconstruction, according to embodiments presented herein;</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 3</figref> is a display diagram showing an illustrative user interface for displaying a top-down map of the visual reconstruction, according to embodiments presented herein;</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 4</figref> is a diagram showing aspects of one technique for transitioning from the local-navigation display to the top-down map, according to embodiments presented herein;</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 5</figref> is a diagram showing aspects of one technique for transitioning from the top-down map to the local-navigation display, according to embodiments presented herein;</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 6</figref> is a flow diagram showing methods for performing the transition from the local-navigation display to the top-down map, according to embodiments described herein;</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 7</figref> is a flow diagram showing methods for performing the transition from the top-down map to the local-navigation display, according to embodiments described herein; and</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 8</figref> is a block diagram showing an illustrative computer hardware and software architecture for a computing system capable of implementing aspects of the embodiments presented herein.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0018" num="0017">The following detailed description is directed to technologies for transitioning between a top-down map of a reconstructed structure within a 3-D scene and an associated local-navigation display. While the subject matter described herein is presented in the general context of program modules that execute in conjunction with the execution of an operating system and application programs on a computer system, those skilled in the art will recognize that other implementations may be performed in combination with other types of program modules. Generally, program modules include routines, programs, components, data structures, and other types of structures that perform particular tasks or implement particular abstract data types. Moreover, those skilled in the art will appreciate that the subject matter described herein may be practiced with other computer system configurations, including hand-held devices, multiprocessor systems, microprocessor-based or programmable consumer electronics, minicomputers, mainframe computers, and the like.</p>
<p id="p-0019" num="0018">In the following detailed description, references are made to the accompanying drawings that form a part hereof and that show, by way of illustration, specific embodiments or examples. In the accompanying drawings, like numerals represent like elements through the several figures.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 1</figref> shows an illustrative operating environment <b>100</b> including several software components for transitioning between a top-down map of a reconstructed structure within a 3-D scene and an associated local navigation display, according to embodiments provided herein. The environment <b>100</b> includes a server computer <b>102</b>. The server computer <b>102</b> shown in <figref idref="DRAWINGS">FIG. 1</figref> may represent one or more web servers, application servers, network appliances, dedicated computer hardware devices, personal computers (&#x201c;PC&#x201d;), or any combination of these and/or other computing devices known in the art.</p>
<p id="p-0021" num="0020">According to one embodiment, the server computer <b>102</b> stores a collection of photographs <b>104</b>. The collection of photographs <b>104</b> may consist of two or more digital photographs taken by a user of a particular structure or scene, or the collection of photographs may be an aggregation of several digital photographs taken by multiple photographers of the same scene, for example. The digital photographs in the collection of photographs <b>104</b> may be acquired using digital cameras, may be digitized from photographs taken with traditional film-based cameras, or may be a combination of both.</p>
<p id="p-0022" num="0021">A spatial processing engine <b>106</b> executes on the server computer <b>102</b> and is responsible for computing a 3-D point cloud <b>108</b> representing the structure or scene from the collection of photographs <b>104</b>. The spatial processing engine <b>106</b> may compute the 3-D point cloud <b>108</b> by locating recognizable features, such as objects or textures, that appear in two or more photographs in the collection of photographs <b>104</b>, and calculating the position of the feature in space using the location, perspective, and visibility or obscurity of the features in each photograph. The spatial processing engine <b>106</b> may be implemented as hardware, software, or a combination of the two, and may include a number of application program modules and other components on the server computer <b>102</b>.</p>
<p id="p-0023" num="0022">A visualization service <b>110</b> executes on the server computer <b>102</b> that provides services for users to view and navigate visual reconstructions of the scene or structure captured in the collection of photographs <b>104</b>. The visualization service <b>110</b> may be implemented as hardware, software, or a combination of the two, and may include a number of application program modules and other components on the server computer <b>102</b>.</p>
<p id="p-0024" num="0023">The visualization service <b>110</b> utilizes the collection of photographs <b>104</b> and the computed 3-D point cloud <b>108</b> to create a visual reconstruction <b>112</b> of the scene or structure, and serves the reconstruction over a network <b>114</b> to a visualization client <b>116</b> executing on a user computer <b>118</b>. The user computer <b>118</b> may be a PC, a desktop workstation, a laptop, a notebook, a mobile device, a personal digital assistant (&#x201c;PDA&#x201d;), an application server, a Web server hosting Web-based application programs, or any other computing device. The network <b>114</b> may be a local-area network (&#x201c;LAN&#x201d;), a wide-area network (&#x201c;WAN&#x201d;), the Internet, or any other networking topology that connects the user computer <b>118</b> to the server computer <b>102</b>. It will be appreciated that the server computer <b>102</b> and user computer <b>118</b> shown in <figref idref="DRAWINGS">FIG. 1</figref> may represent the same computing device.</p>
<p id="p-0025" num="0024">The visualization client <b>116</b> receives the visual reconstruction <b>112</b> from the visualization service <b>110</b> and displays the visual reconstruction to a user of the user computer <b>118</b> using a display device <b>120</b> attached to the computer. The visualization client <b>116</b> may be implemented as hardware, software, or a combination of the two, and may include a number of application program modules and other components on the user computer <b>118</b>. In one embodiment, the visualization client <b>116</b> consists of a web browser application and a plug-in module that allows the user of the user computer <b>118</b> to view and navigate the visual reconstruction <b>112</b> served by the visualization service <b>110</b>.</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 2</figref> shows an example of an illustrative user interface <b>200</b> displayed by the visualization client <b>116</b> allowing a user to locally navigate the photographs in the visual reconstruction <b>112</b>. The user interface <b>200</b> includes a window <b>202</b> in which a local-navigation display <b>204</b> is displayed. The local-navigation display <b>204</b> shows a currently-viewed photograph <b>206</b> from the photographs in the visual reconstruction <b>112</b>. The local-navigation display <b>204</b> may show the currently-viewed photograph <b>206</b> in relation to other photographs and the 3-D point cloud that make-up the visual reconstruction <b>112</b>. The visualization client <b>116</b> may provide a set of navigation controls <b>208</b> that allows the user to pan and zoom the currently-viewed photograph <b>206</b> in the local-navigation display <b>204</b>, as well as move between the photographs of the visual reconstruction <b>112</b>. The set of navigation controls <b>208</b> may further contain a particular control <b>210</b> to change the display to a top-down map display, as described below.</p>
<p id="p-0027" num="0026">According to embodiments, the visual reconstruction <b>112</b> includes a top-down map. Generally, the top-down map is a two-dimensional view of the 3-D scene from the top. In one embodiment, the top-down map is generated by projecting all the points of the 3-D point cloud <b>108</b> into the x-y plane. The positions of the identifiable features, or points, computed in the 3-D point cloud <b>108</b> may be represented as dots in the top-down map. The points of the 3-D point cloud <b>108</b> shown in the top-down map may be filtered and/or enhanced to reduce the noise and enhance the top-down visualization, as described in co-pending U.S. patent application Ser. No. 12/699,902 filed concurrently herewith, having, and entitled &#x201c;Generating and Displaying Top-Down Maps of Reconstructed 3-D Scenes,&#x201d; which is incorporated herein by reference in its entirety.</p>
<p id="p-0028" num="0027">In other embodiments, the top-down map may be a photograph or image of the scene from above, a top view of a 3-D model of the scene, or some other two-dimensional representation of the 3-D scene. Further, the top-down map may be projected onto a reference plane other than the x-y plane. For example, in a visual reconstruction <b>112</b> of a cathedral with a large amount of detail on the fa&#xe7;ade, the plane of the fa&#xe7;ade may serve as the reference plane for the top-down map. In addition, a non-planar reference surface may be utilized. For example, in a visual reconstruction <b>112</b> of the interior of a room, a cylinder centered at the center of the room may be utilized as the reference surface, with the details of the room's walls projected onto the cylindrical surface.</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 3</figref> shows an illustrative user interface <b>300</b> provided by the visualization client <b>116</b> for displaying a top-down map <b>302</b> generated from the 3-D point cloud <b>108</b>, as described above. In this example, the top-down map <b>302</b> is displayed separately from the local-navigation display <b>204</b>. This view is referred to as the &#x201c;modal view.&#x201d; The visualization client <b>116</b> may provide a similar set of navigation controls <b>208</b> as those described above that allows the user to pan and zoom the top-down map <b>302</b> to reveal the entire scene or structure represented in the visual reconstruction <b>112</b>, or to see more detail of a particular section. The user may toggle back and forth between the display of the top-down map <b>302</b> and the local-navigation display <b>204</b> using the particular control <b>210</b> in the set of navigation controls <b>208</b>, for example.</p>
<p id="p-0030" num="0029">The visualization client <b>116</b> may further provide a number of techniques allowing the user to interact with the top-down map <b>302</b>, as described in co-pending U.S. patent application Ser. No. 12/699,904 filed concurrently herewith, having, and entitled &#x201c;User Interfaces for Interacting with Top-Down Maps of Reconstructed 3-D Scenes,&#x201d; which is incorporated herein by reference in its entirety. These interactions may include the user selecting camera poses, objects, points, or other elements of the visual reconstruction <b>112</b> on the top-down map <b>302</b> in order to view associated representative photographs in the local-navigation display <b>204</b>. For example, the user interface <b>300</b> may include a selection control <b>304</b> that allows the user to select a point or group of points on the top-down map <b>302</b>. The selection control <b>304</b> may be a pointer, circle, square, or other iconic indicator that the user may move around the map using a mouse or other input device connected to the user computer <b>118</b>.</p>
<p id="p-0031" num="0030">According to one embodiment, if the user hovers the selection control <b>304</b> over a point on the top-down map <b>302</b>, the visualization client <b>116</b> may display a thumbnail image <b>306</b> of a representative photograph at an appropriate position on the corresponding map. In addition to the thumbnail image <b>306</b>, the visualization client <b>116</b> may further display a view frustum <b>308</b> or other indicator on the top-down map <b>302</b> that indicates the position and point-of-view of the camera that captured the representative photograph. Further, if the user selects the point under the selection control <b>304</b>, by clicking a button on the mouse, for example, the visualization client <b>116</b> may transition the display in the window <b>202</b> to the local-navigation display <b>204</b> showing the representative photograph.</p>
<p id="p-0032" num="0031">Both the local-navigation display <b>204</b> and the top-down map <b>302</b> inform the user about the reconstructed 3D scene. In the local-navigation display <b>204</b>, the main benefit is visualization of local details. A user may zoom into the currently-viewed photograph <b>206</b> in the local-navigation display <b>204</b> to appreciate the finer details of the subject. In contrast, the top-down map <b>302</b> provides a global context of the 3-D scene, providing the user with an understanding of the environment in which the photographs were taken. Connecting the context of the top-down map <b>302</b> to the details of the local-navigation display <b>204</b> is important because it enables the user to better explore the 3D scene.</p>
<p id="p-0033" num="0032">In order to preserve the spatial connection between the top-down map <b>302</b> and the local-navigation display <b>204</b>, the visualization client <b>116</b> may employ a number of techniques to transition between the displays. In the simplest approach, the visualization client <b>116</b> toggles instantly between the two displays. However, this approach lacks any continuity between the views and the spatial connection is lost. In another approach, the visualization client <b>116</b> fades between the two displays, taking advantage of the user's ability to retain a temporary visual imprint. This approach is only successful in retaining continuity between the views, however, when the top-down map <b>302</b> and the local-navigation display <b>204</b> have related orientations. For example, if the currently-viewed photograph <b>206</b> in the local-navigation display <b>204</b> was taken from the top of a mountain looking down into a valley, then a fade to a top-down map <b>302</b> of the same valley oriented in a &#x201c;camera-up&#x201d; direction may be adequate. If, instead, the currently-viewed photograph <b>206</b> was taken in the valley of a store fa&#xe7;ade, for example, then a fade to a top-down map <b>302</b> of the valley may be discontinuous.</p>
<p id="p-0034" num="0033">Alternatively, the visualization client <b>116</b> may transition between the top-down map <b>302</b> and the local-navigation display <b>204</b> by animating a smooth interpolation between the views, according to the embodiments described herein. Further, these transitions may be performed in such a way as to preserve the continuity between the two views without causing confusing or visually unpleasant effects like camera spiral or vertigo. Both the top-down map <b>302</b> display and the local-navigation display <b>204</b> can be expressed in terms of a &#x201c;camera view.&#x201d; Generally, the camera view may comprise a set of seven parameters, grouped into 3 categories: position (x, y, z), orientation (pitch, yaw, roll), and field-of-view. Together, these parameters define the top-down map <b>302</b> and local-navigation display <b>204</b> views and how they are rendered by the visualization client <b>116</b> in the window <b>202</b>. The camera view for the local-navigation display <b>204</b> is typically defined by the position and orientation of the camera that took the currently-viewed photograph <b>206</b>, while the top-down map <b>302</b> camera view is typically positioned high above the 3-D scene, pointed downward with a small field-of-view.</p>
<p id="p-0035" num="0034">According to embodiments, to perform the animated transition from one display to the other, the visualization client <b>116</b> interpolates the parameters between the starting camera view and the ending camera view. In one embodiment, the visualization client <b>116</b> linearly interpolates between the start values and end values of the parameters over the time of the transition. For example, the visualization client may employ a formula such as:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i>(<i>t</i>)=(1<i>&#x2212;t</i>)*i S+t*E<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where P(t) is the value of the parameter at time t, t having a value between 0 and 1, S is the start value of the parameter, and E is the end value of the parameter. The linear interpolation provides a constant acceleration, however, which may not provide the most intuitive transition.
</p>
<p id="p-0036" num="0035">To achieve a smoother result, the visualization client <b>116</b> employs a sigmoid function to perform the interpolations, according to another embodiment. A sigmoid function is defined as:</p>
<p id="p-0037" num="0036">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mrow>
  <mrow>
    <mi>F</mi>
    <mo>&#x2061;</mo>
    <mrow>
      <mo>(</mo>
      <mi>t</mi>
      <mo>)</mo>
    </mrow>
  </mrow>
  <mo>=</mo>
  <mfrac>
    <mn>1</mn>
    <mrow>
      <mn>1</mn>
      <mo>+</mo>
      <msup>
        <mi>&#x2147;</mi>
        <mrow>
          <mo>-</mo>
          <mi>t</mi>
        </mrow>
      </msup>
    </mrow>
  </mfrac>
</mrow>
</math>
</maths>
<br/>
This function produces an &#x201c;S-curve,&#x201d; such as that shown in Table 1. Utilizing the sigmoid function allows the animation of the transition between the views to initially progress slowly, accelerate to a maximum at the center, and decelerate at the end.
</p>
<p id="p-0038" num="0037">
<tables id="TABLE-US-00001" num="00001">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="center"/>
<thead>
<row>
<entry namest="1" nameend="1" rowsep="1">TABLE 1</entry>
</row>
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry><chemistry id="CHEM-US-00001" num="00001">
<img id="EMI-C00001" he="18.80mm" wi="40.05mm" file="US08624902-20140107-C00001.TIF" alt="embedded image" img-content="table" img-format="tif"/>
</chemistry>
</entry>
</row>
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0039" num="0038">The simplicity and smoothness properties of the S-curve make it ideal for user interactions when smooth motion is required. Integrating the sigmoid function into the interpolation function yields:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i>(<i>t</i>)=(1<i>&#x2212;F</i>(<i>t</i>))*<i>S+F</i>(<i>t</i>)*<i>E </i><?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
The visualization client <b>116</b> may utilize such a function to perform the interpolations of the parameters for the transition between the camera views. It will be appreciated that the visualization client <b>116</b> may utilize a combination of both the linear and sigmoid-based interpolation functions described above, or it may utilize any general function known in the art for interpolating the parameters for the transition between the views, as described below.
</p>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. 4</figref> illustrates one method of transitioning the display from the local-navigation display <b>204</b> to the top-down map (<b>302</b>) utilizing the interpolation techniques described above, according to one embodiment. This transition may be performed when the user selects the specific control <b>210</b> in the set of navigation controls <b>208</b> to toggle the display from the local-navigation display <b>204</b> to the top-down map <b>302</b>, for example. In one embodiment, the visualization client <b>116</b> may perform the transition between the local-navigation display <b>204</b> and the top-down map <b>302</b> in stages, in order to reduce confusing or visually unpleasant effects like camera spiral or vertigo. It will be appreciated that the transition between stages may be performed in a smooth fashion as to position and speed, such that a user may not notice that the overall motion is composed of separate stages. The transition from local-navigation display <b>204</b> to the top-down map <b>302</b> may occur in two stages: 1) transition the position and pitch of the camera view to that of the top-down map, and 2) transition the field-of-view.</p>
<p id="p-0041" num="0040">As shown in <figref idref="DRAWINGS">FIG. 4</figref>, in the first stage, the camera view transitions from the starting camera view <b>402</b> of the currently-viewed photograph <b>206</b> to a position high above the 3D scene, and is pointed downward. Adjusting the pitch enables the camera view to point downward. Note that the camera view does not transition into the last viewed orientation (roll) of the top-down map <b>302</b> display. Instead, the transition results in a &#x201c;camera-up&#x201d; orientation of the top-down map <b>302</b> in respect to the currently-viewed photograph <b>206</b>. This results in a transition without spiral effect as the camera transitions through the first stage.</p>
<p id="p-0042" num="0041">Once the camera view is positioned and oriented, the second stage transitions the field-of-view from that of the currently-viewed photograph <b>206</b> to that of the ending camera view <b>404</b> of the top-down map <b>302</b>. Typically, the top-down map's field-of-view is small, e.g. about 1 degree, creating a near-orthographic projection. The local-navigation display <b>204</b>, however, may have a more typical photographic field-of-view, such as 45 degrees. The visual effect of transitioning between these two fields-of-view is that walls and vertical structures visualized in the top-down map <b>302</b> will appear to bend inward until they become lines.</p>
<p id="p-0043" num="0042">One reason for separating the transition of the position and orientation of the camera view from the transition of the field-of-view is that the combination of the two transitions could produce unwanted visual effects. For example, adjusting the position and pitch of the camera view while adjusting the field-of-view could result in vertigo, or the 3D scene may appear to throb back and forth if the field-of-view shrinks more rapidly than the movement of the camera view <b>402</b>. In addition, if the field-of-view decreases rapidly, the camera view <b>402</b> approach a near-orthographic projection before completion of the camera movement, resulting in a loss of all perspective cues, referred to as &#x201c;foreshortening.&#x201d;</p>
<p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. 5</figref> illustrates one method of transitioning from the top-down map (<b>302</b>) display to a target photograph <b>406</b> in the local-navigation display <b>204</b>, according to another embodiment. This transition may be performed when the user selects a point or group of points on the top-down map <b>302</b> in order to view a corresponding representative photograph containing the points in the local-navigation display <b>204</b>, for example. Similar to the method described above in regard to <figref idref="DRAWINGS">FIG. 4</figref>, the transition from top-down map <b>302</b> to local-navigation display <b>204</b> may occur in stages to avoid spiraling, vertigo, and non-perspective effects. Specifically, the transition may occur in four stages: 1) rotate the top-down map view, 2) transition the field-of-view, 3) transition the position and pitch of the camera view from high above the scene to ground level, and 4) approach the ending camera view <b>404</b> of the target photograph <b>406</b> on the ground level.</p>
<p id="p-0045" num="0044">As shown in <figref idref="DRAWINGS">FIG. 5</figref>, in the first stage, the top-down map <b>302</b> is rotated to a camera-up orientation for the map in respect to the target photograph <b>406</b> by adjusting the roll parameter of the starting camera view <b>402</b>. This avoids any spiraling effects during the transition since the remaining stages only require the camera to &#x201c;swoop&#x201d; downward in a straightforward manner. In the second stage, the field-of-view transitions from the starting camera view <b>402</b> to that of the target photograph <b>406</b>. Decoupling the field-of-view transition from any position or orientation transitions may remove visually unpleasant effects due to vertigo and throbbing, as described above.</p>
<p id="p-0046" num="0045">In the third stage, the position of the camera view is transitioned from high above the top-down map <b>302</b> to an interim camera position <b>408</b> approximately at ground level while adjusting the pitch to that of the target photograph <b>406</b>. This provides the effect of the camera flying down to ground level at a position slightly behind the ending camera view <b>404</b> of the target photograph <b>406</b>. In the last stage, the position of the camera view is transitioned from the interim camera position <b>408</b> so that the camera approaches the ending camera view <b>404</b> for the target photograph <b>406</b> in local-navigation display <b>204</b>. To avoid visual artifacts that may occur from neighboring photos in the local-navigation display <b>204</b>, only the target photograph <b>406</b> may be displayed initially as the transition is still distant from the ending camera view <b>404</b> position and orientation. As the transition approaches the target photograph <b>406</b>, however, neighboring photos may be faded-in.</p>
<p id="p-0047" num="0046">It is possible that before the transition between the displays, the user may have changed the zoom level of the starting camera view. For example, the user may have zoomed-in to the currently-viewed photograph <b>206</b> in the local-navigation display <b>204</b> to examine more detail, or zoomed-out of the top-down map <b>302</b> display to get a bigger picture of the 3-D scene. In either case, the zooming can cause problems with the transition because the context may be lost. There are two options for transitions in this case: perform the transition starting from the zoomed view, or first transition to a canonical view, then transition into the target view. In the former option, the transition is fast, but if the 3-D scene is sparse (e.g. a sparse 3D point cloud), then the user may lose context in the transition. In the latter option, the disadvantage is that there is an added transition to the canonical view, but the transitions between the views remains consistent and context may be maintained.</p>
<p id="p-0048" num="0047">It will be appreciated that the transitions from the top-down map <b>302</b> display to the local-navigation display <b>204</b> or from the local-navigation display to the top-down map may be accomplished in any number of stages performed in any order, beyond that described above in regard to <figref idref="DRAWINGS">FIGS. 4 and 5</figref>. Further the various interpolations of position, orientation, and field-of-view parameters may be performed using linear interpolation functions, sigmoid-based interpolation functions, or any combination of these and other functions known in the art for interpolating the parameters between camera views. It is intended that this application include all such transitional stages and interpolation functions.</p>
<p id="p-0049" num="0048">According to further embodiments, other transitions beyond the transitions from the top-down map (<b>302</b>) view to the local-navigation display <b>204</b> or from the local-navigation display to the top-down map (<b>302</b>) may be performed using the same approach as described above. For example, while viewing the currently-viewed photograph <b>206</b> in the local-navigation display <b>204</b>, the user may select another photograph to view from a list of highlighted photographs in the visual reconstruction <b>112</b>. Further, the selected photograph may be visually distant from the currently-viewed photograph <b>206</b> in the 3-D scene, or may not have a discernable visual connection to the currently-viewed photograph.</p>
<p id="p-0050" num="0049">Simply transitioning the camera view <b>402</b> along the ground between the currently-viewed photograph <b>206</b> and the target photograph <b>406</b> may be confusing. In this case, the visualization client <b>116</b> may transition the camera view <b>402</b> from that of the currently-viewed photograph <b>206</b> in the local-navigation display <b>204</b> to the top-down map <b>302</b> using the approach described above in regard to <figref idref="DRAWINGS">FIG. 4</figref>, then transition from the top-down map to the target photograph <b>406</b> in the local-navigation display <b>204</b> using the approach described above in <figref idref="DRAWINGS">FIG. 5</figref>. This may provide a smoother transition for long distances while allowing the user to retain a visual relationship between the two photographs and their position in the overall 3-D scene.</p>
<p id="p-0051" num="0050">Referring now to <figref idref="DRAWINGS">FIGS. 6 and 7</figref>, additional details will be provided regarding the embodiments presented herein. It should be appreciated that the logical operations described with respect to <figref idref="DRAWINGS">FIGS. 6 and 7</figref> are implemented (1) as a sequence of computer implemented acts or program modules running on a computing system and/or (2) as interconnected machine logic circuits or circuit modules within the computing system. The implementation is a matter of choice dependent on the performance and other requirements of the computing system. Accordingly, the logical operations described herein are referred to variously as operations, structural devices, acts, or modules. These operations, structural devices, acts, and modules may be implemented in software, in firmware, in special purpose digital logic, and any combination thereof. It should also be appreciated that more or fewer operations may be performed than shown in the figures and described herein. The operations may also be performed in a different order than described.</p>
<p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. 6</figref> illustrates a routine <b>600</b> for performing the transition from the local-navigation display <b>204</b> to the top-down map <b>302</b>, in the manner described above in regard to <figref idref="DRAWINGS">FIG. 4</figref>. According to embodiments, the routine <b>600</b> may be performed by the visualization client <b>116</b> described above in regard to <figref idref="DRAWINGS">FIG. 1</figref>. It will be appreciated that the routine <b>600</b> may also be performed by other modules or components executing on the server computer <b>102</b> and/or user computer <b>118</b>, or by any combination of modules and components.</p>
<p id="p-0053" num="0052">The routine <b>600</b> begins at operation <b>602</b>, where the visualization client <b>116</b> transitions the camera view from the position and orientation of the starting camera view <b>402</b> of the currently-viewed photograph <b>206</b> in the local-navigation display <b>204</b> to high above the 3-D scene looking down at the top-down map <b>302</b>. This may be performed by animating the view in the window <b>202</b> over a period of time while interpolating between the camera parameters for position and pitch, as described above in regard to stage <b>1</b> of <figref idref="DRAWINGS">FIG. 4</figref>. The interpolation of the parameters may be performed using a linear interpolation function, a sigmoid-based interpolation function, or any combination of these and other functions known in the art for interpolating parameters between camera views.</p>
<p id="p-0054" num="0053">From operation <b>602</b>, the routine <b>600</b> proceeds to operation <b>604</b>, where the visualization client <b>116</b> then adjusts the field-of-view of the camera view <b>402</b> to produce the near-orthographic projection of the top-down map <b>302</b>, as described above in regard to stage <b>2</b> of <figref idref="DRAWINGS">FIG. 4</figref>. This may also be performed by animating the view in the window <b>202</b> over a period of time while interpolating the field-of-view parameter between the initial field-of-view and the field-of-view of the ending camera view <b>404</b>. The visual effect of transitioning between these two fields-of-views is that walls and vertical structures visualized in the top-down map <b>302</b> will appear to bend inward until they become lines. From operation <b>604</b>, the routine <b>600</b> ends.</p>
<p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. 7</figref> illustrates a routine <b>700</b> for performing the transition from the top-down map <b>302</b> display to the local-navigation display <b>204</b>, in the manner described above in regard to <figref idref="DRAWINGS">FIG. 5</figref>. According to embodiments, the routine <b>700</b> may be performed by the visualization client <b>116</b> described above in regard to <figref idref="DRAWINGS">FIG. 1</figref>. It will be appreciated that the routine <b>700</b> may also be performed by other modules or components executing on the server computer <b>102</b> and/or user computer <b>118</b>, or by any combination of modules and components.</p>
<p id="p-0056" num="0055">The routine <b>700</b> begins at operation <b>702</b>, where the visualization client <b>116</b> rotates the starting camera view <b>402</b> of the top-down map <b>302</b> to a camera-up orientation in respect to the target photograph <b>406</b>, as described above in regard to stage <b>1</b> of <figref idref="DRAWINGS">FIG. 5</figref>. This may be performed by animating the view in the window <b>202</b> over a period of time while interpolating the roll parameter of the camera view for the top-down map <b>302</b>. The roll parameter may be interpolated using a linear interpolation function, a sigmoid-based interpolation function, or any combination of these and other functions known in the art for interpolating parameters between camera views.</p>
<p id="p-0057" num="0056">From operation <b>702</b>, the routine <b>700</b> proceeds to operation <b>704</b>, where the visualization client <b>116</b> adjusts the field-of-view from that of the top-down map <b>302</b> to that of the target photograph <b>406</b>, as described above in regard to stage <b>2</b> of <figref idref="DRAWINGS">FIG. 5</figref>. The routine <b>700</b> then proceeds to operation <b>706</b>, where the visualization client <b>116</b> transitions the camera view from the starting camera view <b>402</b> high above the 3-D scene to an interim camera position <b>408</b> approximately at ground level with the orientation of the target-photograph <b>406</b>, as described above in regard to stage <b>3</b> of <figref idref="DRAWINGS">FIG. 4</figref>. This may be performed by animating the view in the window <b>202</b> over a period of time while interpolating between the camera parameters for position, pitch, and roll. The animation provides the effect of the camera flying down to ground level at a position slightly behind the ending camera view <b>404</b> of the target photograph <b>406</b>.</p>
<p id="p-0058" num="0057">From operation <b>706</b>, the routine <b>700</b> proceeds to operation <b>708</b>, where the visualization client <b>116</b> animates the camera view to approach the ending camera view <b>404</b> of the target photograph <b>406</b> within the local-navigation display <b>204</b>, as described above in regard to stage <b>4</b> of <figref idref="DRAWINGS">FIG. 5</figref>. This may be performed by animating the view in the window <b>202</b> over a period of time while interpolating the camera position parameters using a linear interpolation function, a sigmoid-based interpolation function, or some other function or combination of functions known in the art. As the transition approaches the target photograph <b>406</b>, the visualization client <b>116</b> may further fade-in neighboring photographs in the local-navigation display <b>204</b>. From operation <b>708</b>, the routine <b>700</b> ends.</p>
<p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. 8</figref> shows an example computer architecture for a computer <b>10</b> capable of executing the software components described herein for transitioning between a top-down map of a reconstructed structure within a 3-D scene and an associated local-navigation display, in the manner presented above. The computer architecture shown in <figref idref="DRAWINGS">FIG. 8</figref> illustrates a conventional computing device, PDA, digital cellular phone, communication device, desktop computer, laptop, or server computer, and may be utilized to execute any aspects of the software components presented herein described as executing on the user computer <b>118</b>, server computer <b>102</b>, or other computing platform.</p>
<p id="p-0060" num="0059">The computer architecture shown in <figref idref="DRAWINGS">FIG. 8</figref> includes one or more central processing units (&#x201c;CPUs&#x201d;) <b>12</b>. The CPUs <b>12</b> may be standard central processors that perform the arithmetic and logical operations necessary for the operation of the computer <b>10</b>. The CPUs <b>12</b> perform the necessary operations by transitioning from one discrete, physical state to the next through the manipulation of switching elements that differentiate between and change these states. Switching elements may generally include electronic circuits that maintain one of two binary states, such as flip-flops, and electronic circuits that provide an output state based on the logical combination of the states of one or more other switching elements, such as logic gates. These basic switching elements may be combined to create more complex logic circuits, including registers, adders-subtractors, arithmetic logic units, floating-point units, and other logic elements.</p>
<p id="p-0061" num="0060">The computer architecture further includes a system memory <b>18</b>, including a random access memory (&#x201c;RAM&#x201d;) <b>24</b> and a read-only memory <b>26</b> (&#x201c;ROM&#x201d;), and a system bus <b>14</b> that couples the memory to the CPUs <b>12</b>. A basic input/output system containing the basic routines that help to transfer information between elements within the computer <b>10</b>, such as during startup, is stored in the ROM <b>26</b>. The computer <b>10</b> also includes a mass storage device <b>20</b> for storing an operating system <b>28</b>, application programs, and other program modules, which are described in greater detail herein.</p>
<p id="p-0062" num="0061">The mass storage device <b>20</b> is connected to the CPUs <b>12</b> through a mass storage controller (not shown) connected to the bus <b>14</b>. The mass storage device <b>20</b> provides non-volatile storage for the computer <b>10</b>. The computer <b>10</b> may store information on the mass storage device <b>20</b> by transforming the physical state of the device to reflect the information being stored. The specific transformation of physical state may depend on various factors, in different implementations of this description. Examples of such factors may include, but are not limited to, the technology used to implement the mass storage device, whether the mass storage device is characterized as primary or secondary storage, and the like.</p>
<p id="p-0063" num="0062">For example, the computer <b>10</b> may store information to the mass storage device <b>20</b> by issuing instructions to the mass storage controller to alter the magnetic characteristics of a particular location within a magnetic disk drive, the reflective or refractive characteristics of a particular location in an optical storage device, or the electrical characteristics of a particular capacitor, transistor, or other discrete component in a solid-state storage device. Other transformations of physical media are possible without departing from the scope and spirit of the present description. The computer <b>10</b> may further read information from the mass storage device <b>20</b> by detecting the physical states or characteristics of one or more particular locations within the mass storage device.</p>
<p id="p-0064" num="0063">As mentioned briefly above, a number of program modules and data files may be stored in the mass storage device <b>20</b> and RAM <b>24</b> of the computer <b>10</b>, including an operating system <b>28</b> suitable for controlling the operation of a computer. The mass storage device <b>20</b> and RAM <b>24</b> may also store one or more program modules. In particular, the mass storage device <b>20</b> and the RAM <b>24</b> may store the visualization service <b>110</b> and visualization client <b>116</b>, both of which were described in detail above in regard to <figref idref="DRAWINGS">FIG. 1</figref>. The mass storage device <b>20</b> and the RAM <b>24</b> may also store other types of program modules or data.</p>
<p id="p-0065" num="0064">In addition to the mass storage device <b>20</b> described above, the computer <b>10</b> may have access to other computer-readable media to store and retrieve information, such as program modules, data structures, or other data. By way of example, and not limitation, computer-readable media may include volatile and non-volatile, removable and non-removable media implemented in any method or technology for storage of information such as computer-readable instructions, data structures, program modules, or other data. For example, computer-readable media includes, but is not limited to, RAM, ROM, EPROM, EEPROM, flash memory or other solid state memory technology, CD-ROM, digital versatile disks (DVD), HD-DVD, BLU-RAY, or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium that can be used to store the desired information and that can be accessed by the computer <b>10</b>.</p>
<p id="p-0066" num="0065">The computer-readable storage medium may be encoded with computer-executable instructions that, when loaded into the computer <b>10</b>, may transform the computer system from a general-purpose computing system into a special-purpose computer capable of implementing the embodiments described herein. The computer-executable instructions may be encoded on the computer-readable storage medium by altering the electrical, optical, magnetic, or other physical characteristics of particular locations within the media. These computer-executable instructions transform the computer <b>10</b> by specifying how the CPUs <b>12</b> transition between states, as described above. According to one embodiment, the computer <b>10</b> may have access to computer-readable storage media storing computer-executable instructions that, when executed by the computer, perform the routines <b>600</b> and <b>700</b> for transitioning between the top-down map <b>302</b> display and the local-navigation display <b>204</b>, described above in regard to <figref idref="DRAWINGS">FIGS. 6 and 7</figref>.</p>
<p id="p-0067" num="0066">According to various embodiments, the computer <b>10</b> may operate in a networked environment using logical connections to remote computing devices and computer systems through a network <b>114</b>. The computer <b>10</b> may connect to the network <b>114</b> through a network interface unit <b>16</b> connected to the bus <b>14</b>. It should be appreciated that the network interface unit <b>16</b> may also be utilized to connect to other types of networks and remote computer systems.</p>
<p id="p-0068" num="0067">The computer <b>10</b> may also include an input/output controller <b>22</b> for receiving and processing input from a number of input devices, including a keyboard <b>30</b>, a mouse <b>32</b>, a touchpad, a touch screen, an electronic stylus, or other type of input device. Similarly, the input/output controller <b>22</b> may provide output to a display device <b>120</b>, such as a computer monitor, a flat-panel display, a digital projector, a printer, a plotter, or other type of output device. It will be appreciated that the computer <b>10</b> may not include all of the components shown in <figref idref="DRAWINGS">FIG. 8</figref>, may include other components that are not explicitly shown in <figref idref="DRAWINGS">FIG. 8</figref>, or may utilize an architecture completely different than that shown in <figref idref="DRAWINGS">FIG. 8</figref>.</p>
<p id="p-0069" num="0068">Based on the foregoing, it should be appreciated that technologies for transitioning between a top-down map of a reconstructed structure within a 3-D scene and an associated local-navigation display are provided herein. Although the subject matter presented herein has been described in language specific to computer structural features, methodological acts, and computer-readable media, it is to be understood that the invention defined in the appended claims is not necessarily limited to the specific features, acts, or media described herein. Rather, the specific features, acts, and mediums are disclosed as example forms of implementing the claims.</p>
<p id="p-0070" num="0069">The subject matter described above is provided by way of illustration only and should not be construed as limiting. Various modifications and changes may be made to the subject matter described herein without following the example embodiments and applications illustrated and described, and without departing from the true spirit and scope of the present invention, which is set forth in the following claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-chemistry idref="CHEM-US-00001" cdx-file="US08624902-20140107-C00001.CDX" mol-file="US08624902-20140107-C00001.MOL"/>
<us-math idrefs="MATH-US-00001" nb-file="US08624902-20140107-M00001.NB">
<img id="EMI-M00001" he="6.01mm" wi="76.20mm" file="US08624902-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. An apparatus comprising:
<claim-text>a computer processing device; and</claim-text>
<claim-text>a memory configured to execute computer-executable instructions that, when executed by the computer processing device perform an animated transition between
<claim-text>a top-down map display generated from a 3-D point cloud computed from a collection of photographs and</claim-text>
<claim-text>a local-navigation display showing a photograph from the collection of photographs,</claim-text>
<claim-text>wherein the starting camera view is associated with a currently-viewed photograph in the local-navigation display, the ending camera view is associated with the top-down map display, and interpolating the camera parameters comprises
<claim-text>interpolate camera position and orientation parameters of the camera view in a first stage from starting values associated with the currently-viewed photograph to ending values associated with the top-down map display; and</claim-text>
<claim-text>interpolate a field-of-view parameter of the camera view in a second stage from a starting value associated with the currently-viewed photograph to an ending value associated with the top-down map display.</claim-text>
</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the animated transition is created by animating a view in a display window over a period of time while interpolating camera parameters from values representing a starting camera view to values representing an ending camera view.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The apparatus of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the interpolation of the camera parameters is performed utilizing linear functions over the period of time.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The apparatus of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the interpolation of the camera parameters is performed utilizing sigmoid-based functions over the period of time.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The apparatus of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the interpolation of the camera parameters is performed in multiple stages.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The apparatus of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the starting camera view is associated with the top-down map display, the ending camera view is associated with a target photograph to be shown in the local-navigation display, and interpolating the camera parameters comprises:
<claim-text>interpolate a roll parameter of the camera view in a first stage from a starting value associated with the top-down map display to an ending value reflecting a camera-up orientation of the top-down map in regard to the target photograph;</claim-text>
<claim-text>interpolate a field-of-view parameter of the camera view in a second stage from a starting value associated with the top-down map display to an ending value associated with the target photograph;</claim-text>
<claim-text>interpolate camera position and orientation parameters of the camera view in a third stage from starting values associated with the top-down map display to ending values reflecting an interim camera position and orientation related to the target photograph; and</claim-text>
<claim-text>interpolate camera position and orientation parameters of the camera view in a fourth stage from starting values reflecting the interim camera position and orientation to ending values associated with the target photograph.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The apparatus of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the animated transition further comprises causing neighboring photographs to fade into view of the local-navigation display as the transition approaches the target photograph.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The apparatus of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the starting camera view is associated with a currently-viewed photograph in the local-navigation display, the ending camera view is associated with a target photograph to be shown in the local-navigation display, and the animated transition comprises:
<claim-text>an animated transition from the currently-viewed photograph in the local-navigation display to the top-down map display; and</claim-text>
<claim-text>an animated transition from the top-down map display to the target photograph in the local-navigation display.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. A computer-implemented method comprising:
<claim-text>performing, by a computing device, a transition between a top-down map display generated from a 3-D point cloud computed from a collection of photographs and a local-navigation display showing a photograph from the collection of photographs; and</claim-text>
<claim-text>animating, by a computing device, a view in a display window over a period of time while interpolating, by a computing device, camera parameters from values representing a starting camera view to values representing an ending camera view,</claim-text>
<claim-text>wherein the starting camera view is associated with a currently-viewed photograph in the local-navigation display, the ending camera view is associated with the top-down map display, and interpolating the camera parameters comprises
<claim-text>interpolating camera position and orientation parameters of the camera view in a first stage from starting values associated with the currently-viewed photograph to ending values associated with the top-down map display; and</claim-text>
<claim-text>interpolating a field-of-view parameter of the camera view in a second stage from a starting value associated with the currently-viewed photograph to an ending value associated with the top-down map display.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The computer-implemented method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the interpolation of the camera parameters is performed utilizing linear functions over the period of time.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The computer-implemented method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the interpolation of the camera parameters is performed utilizing sigmoid-based functions over the period of time.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The computer-implemented method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the interpolation of the camera parameters is performed in multiple stages.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The computer-implemented method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the starting camera view is associated with the top-down map display, the ending camera view is associated with a target photograph to be shown in the local-navigation display, and interpolating the camera parameters comprises:
<claim-text>interpolating a roll parameter of the camera view in a first stage from a starting value associated with the top-down map display to an ending value reflecting a camera-up orientation of the top-down map in regard to the target photograph;</claim-text>
<claim-text>interpolating a field-of-view parameter of the camera view in a second stage from a starting value associated with the top-down map display to an ending value associated with the target photograph;</claim-text>
<claim-text>interpolating camera position and orientation parameters of the camera view in a third stage from starting values associated with the top-down map display to ending values reflecting an interim camera position and orientation related to the target photograph; and</claim-text>
<claim-text>interpolating camera position and orientation parameters of the camera view in a fourth stage from starting values reflecting the interim camera position and orientation to ending values associated with the target photograph.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. A system comprising:
<claim-text>a user computer connected to a display device; and</claim-text>
<claim-text>a visualization client executing on a processor of the user computer and configured to
<claim-text>provide transitions between a top-down map display generated from a 3-D point cloud computed from a collection of photographs and a local-navigation display showing a photograph from the collection of photographs, and</claim-text>
<claim-text>animate a view in a display window on the display device over a period of time while interpolating camera parameters from values representing a starting camera view to values representing an ending camera view,</claim-text>
</claim-text>
<claim-text>wherein the starting camera view is associated with a currently-viewed photograph in the local-navigation display, the ending camera view is associated with the top-down map display, and interpolating the camera parameters comprises
<claim-text>interpolating camera position and orientation parameters of the camera view in a first stage from starting values associated with the currently-viewed photograph to ending values associated with the top-down map display; and</claim-text>
<claim-text>interpolating a field-of-view parameter of the camera view in a second stage from a starting value associated with the currently-viewed photograph to an ending value associated with the top-down map display.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The system <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the interpolation of the camera parameters is performed utilizing one or more of linear functions and sigmoid-based functions over the period of time.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The system <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the interpolation of the camera parameters is performed in multiple stages.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The system <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the starting camera view is associated with the top-down map display, the ending camera view is associated with a target photograph to be shown in the local-navigation display, and interpolating the camera parameters comprises:
<claim-text>interpolating a roll parameter of the camera view in a first stage from a starting value associated with the top-down map display to an ending value reflecting a camera-up orientation of the top-down map in regard to the target photograph;</claim-text>
<claim-text>interpolating a field-of-view parameter of the camera view in a second stage from a starting value associated with the top-down map display to an ending value associated with the target photograph;</claim-text>
<claim-text>interpolating camera position and orientation parameters of the camera view in a third stage from starting values associated with the top-down map display to ending values reflecting an interim camera position and orientation related to the target photograph; and</claim-text>
<claim-text>interpolating camera position and orientation parameters of the camera view in a fourth stage from starting values reflecting the interim camera position and orientation to ending values associated with the target photograph.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
