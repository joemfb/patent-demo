<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08624952-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08624952</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12755221</doc-number>
<date>20100406</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<us-term-of-grant>
<us-term-extension>410</us-term-extension>
<disclaimer>
<text>This patent is subject to a terminal disclaimer.</text>
</disclaimer>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>7</main-group>
<subgroup>14</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classifications-cpc>
<main-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>7</main-group>
<subgroup>14</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
</main-cpc>
</classifications-cpc>
<classification-national>
<country>US</country>
<main-classification>348 1401</main-classification>
<further-classification>348 1402</further-classification>
<further-classification>348 1407</further-classification>
<further-classification>348 1408</further-classification>
</classification-national>
<invention-title id="d2e55">Video telephony image processing</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5870138</doc-number>
<kind>A</kind>
<name>Smith et al.</name>
<date>19990200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348143</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>7167519</doc-number>
<kind>B2</kind>
<name>Comaniciu et al.</name>
<date>20070100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>7184047</doc-number>
<kind>B1</kind>
<name>Crampton</name>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345473</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>2003/0076421</doc-number>
<kind>A1</kind>
<name>Dutta</name>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>2003/0161500</doc-number>
<kind>A1</kind>
<name>Blake et al.</name>
<date>20030800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2003/0174773</doc-number>
<kind>A1</kind>
<name>Comaniciu et al.</name>
<date>20030900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2004/0174438</doc-number>
<kind>A1</kind>
<name>Jung</name>
<date>20040900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2004/0212712</doc-number>
<kind>A1</kind>
<name>Stavely et al.</name>
<date>20041000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2006/0012701</doc-number>
<kind>A1</kind>
<name>Hong</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2006/0050141</doc-number>
<kind>A1</kind>
<name>Yoshimura</name>
<date>20060300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2006/0204019</doc-number>
<kind>A1</kind>
<name>Suzuki et al.</name>
<date>20060900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>EP</country>
<doc-number>598355</doc-number>
<kind>A1</kind>
<date>19940500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00013">
<othercit>Myers; Videophone With Enhanced User Defined Imaging System; Nov. 11, 1999; WO 99/58900.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00014">
<othercit>Kombach; Method of Automatically Allocating a Video Conference Participant Voice Signal to His Image; Nov. 20, 1997; WO 97/43857.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00015">
<othercit>Myers; Videophone With Enhanced User Defined Imaging System; Nov. 11, 1999; WO 99/57900.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>23</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>348 1401- 1416</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382118</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382254</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382286</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382276</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382103</main-classification>
</classification-national>
<classification-cpc-combination-text>H04N 7/147</classification-cpc-combination-text>
<classification-cpc-combination-text>H04N 7/157</classification-cpc-combination-text>
<classification-cpc-combination-text>H04N 7/14</classification-cpc-combination-text>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>6</number-of-drawing-sheets>
<number-of-figures>6</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>11266448</doc-number>
<date>20051103</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>7728866</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>12755221</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20100202689</doc-number>
<kind>A1</kind>
<date>20100812</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Currivan</last-name>
<first-name>Bruce J.</first-name>
<address>
<city>Dove Canyon</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Chen</last-name>
<first-name>Xuemin</first-name>
<address>
<city>San Diego</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Currivan</last-name>
<first-name>Bruce J.</first-name>
<address>
<city>Dove Canyon</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Chen</last-name>
<first-name>Xuemin</first-name>
<address>
<city>San Diego</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Thomas|Horstemeyer, LLP.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Broadcom Corporation</orgname>
<role>02</role>
<address>
<city>Irvine</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Ramakrishnaiah</last-name>
<first-name>Melur</first-name>
<department>2656</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Herein described is a system and method for modifying facial video transmitted from a first videophone to a second videophone during a videophone conversation. A videophone comprises a videophone image processing system (VIPS) that stores one or more preferred images. The one or more preferred images may comprise an image of a person presented in an attractive appearance. The one or more preferred images may comprise one or more avatars. Additionally, the VIPS may be used to incorporate one or more facial features of the person into a preferred image or avatar. Furthermore, a replacement background may be incorporated into the preferred image or avatar. The VIPS transmits a preferred image of a first speaker of a first videophone to a second speaker of a second videophone by capturing an actual image of the first speaker and substituting at least a portion of said actual image with a stored image.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="154.52mm" wi="234.10mm" file="US08624952-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="233.76mm" wi="173.65mm" orientation="landscape" file="US08624952-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="239.61mm" wi="191.01mm" file="US08624952-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="239.61mm" wi="186.77mm" file="US08624952-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="207.86mm" wi="167.72mm" orientation="landscape" file="US08624952-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="235.12mm" wi="149.44mm" file="US08624952-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="183.47mm" wi="125.73mm" file="US08624952-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS/INCORPORATION BY REFERENCE</heading>
<p id="p-0002" num="0001">This application is a continuation of U.S. patent application Ser. No. 11/266,448 filed on Nov. 3, 2005. This application is related to and/or makes reference to U.S. application Ser. No. 11/266,442, filed on Nov. 3, 2005, which is hereby incorporated herein by reference in its entirety.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">FEDERALLY SPONSORED RESEARCH OR DEVELOPMENT</heading>
<p id="p-0003" num="0002">[Not Applicable]</p>
<heading id="h-0003" level="1">MICROFICHE/COPYRIGHT REFERENCE</heading>
<p id="p-0004" num="0003">[Not Applicable]</p>
<heading id="h-0004" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0005" num="0004">As a result of advances in technology, cellular videophones may be used to enhance a phone conversation between two parties. During the conversation, the persons speaking may transmit headshots or facial images of each other, in the form of live streaming video, as a way to enhance and improve communication between the two parties. For example, facial expressions and lip movements may enhance communication between parties. On occasion, however, a party in a conversation may find that his appearance is less than desirable. As a consequence, he may be unwilling to transmit such live video of his facial image using his cellular videophone. Unfortunately, when this occurs, the benefit of transmitting such facial expressions and lip movements are eliminated during a conversation between the two parties.</p>
<p id="p-0006" num="0005">The limitations and disadvantages of conventional and traditional approaches will become apparent to one of skill in the art, through comparison of such systems with some aspects of the present invention as set forth in the remainder of the present application with reference to the drawings.</p>
<heading id="h-0005" level="1">BRIEF SUMMARY OF THE INVENTION</heading>
<p id="p-0007" num="0006">Aspects of the invention provide at least a system and a method that modifies facial video transmitted from a first videophone to a second videophone during a videophone conversation. The various aspects of the invention are substantially shown in and/or described in connection with at least one of the following figures, as set forth more completely in the claims.</p>
<p id="p-0008" num="0007">These and other advantages, aspects, and novel features of the present invention, as well as details of illustrated embodiments, thereof, will be more fully understood from the following description and drawings.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram illustrating a videophone image processing system (VIPS), as used in a videophone, which transmits video of a preferred image during a video telephony conversation, in accordance with an embodiment of the invention.</p>
<p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. 2A</figref> is relational diagram illustrating a user using a videophone in accordance with an embodiment of the invention.</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 2B</figref> is a relational diagram illustrating a user using a videophone in accordance with an embodiment of the invention.</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 2C</figref> is diagram that illustrates how image processing may be employed using the video image processing system (VIPS) to improve the appearance of a speaker, in accordance with an embodiment of the invention.</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIGS. 3A and 3B</figref> are operational flow diagrams that describe the use of the videophone image processing system (VIPS) during a videophone conversation, in accordance with an embodiment of the invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0007" level="1">DETAILED DESCRIPTION OF THE INVENTION</heading>
<p id="p-0014" num="0013">Various aspects of the invention provide at least a system and a method for substituting and/or modifying an image provided by a speaker engaged in a video telephony conversation. In a preferred embodiment, a first speaker uses a wireless or cellular capable videophone to communicate with a second speaker using a compatible wireless or cellular capable videophone. Aspects of the invention provide for at least processing and transmitting an image of a speaker's head or face during a videophone conversation. The image may be generated by using a preferred image or avatar. One or more facial features of the speaker may be captured and edited into the preferred image or avatar. For example, when the lips of the speaker are incorporated into the preferred image or avatar, the image generated may comprise a lip-synched celebrity or politician. Else, the image may comprise a lip-synched speaker in a preferred facial appearance. For one or more reasons, an individual engaged in a video telephony conversation may wish to substitute his actual facial image with a preferred image or an avatar (i.e., an icon or any type of visual representation of the person speaking into the videophone). A speaker may utilize the system and method of the invention, for example, when he feels that his facial appearance is less than desirable. The speaker may select one of several preferred images or avatars that replaces or substitutes the actual facial image captured by the videophone. The preferred image (i.e., an image that provides an attractive appearance of the speaker) may be stored in a memory of the videophone. This stored image may be used to replace an undesirable image that is captured by the videophone. The avatar may comprise any type of image desired by a user of the videophone. For example, the avatar may comprise a facial image that resembles a celebrity or sports figure. Additionally, the various aspects of the invention allow one or more facial objects of a person's face to be edited (i.e., cropped and inserted) into the avatar or desired image. In a representative embodiment, the objects cropped and inserted may comprise the individual's lips, eyes, and/or nose, for example. As a result, the video presented to a receiving party of a videophone conversation may view the actual movements of the one or more facial objects selected, and a preferred image or avatar retains the actual lip movements present in the captured facial image. For example, an image of a famous celebrity may incorporate the expression provided by the speaker's lips, nose, and eyes. Furthermore, the various aspects of the invention may be adapted to incorporate one or more background images with the avatar. For example, a facial avatar may be presented pictured in a scenic environment. The scenic environment may be a country setting or a beach setting, for example.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram illustrating a videophone image processing system (VIPS), as used in a videophone, which transmits video of a preferred image during a video telephony conversation, in accordance with an embodiment of the invention. The VIPS comprises a controller/graphics processor <b>104</b>, a memory <b>108</b>, an image sensor <b>112</b>, a lens <b>116</b>, a user interface <b>120</b>, a display <b>124</b>, and a receiver/transmitter <b>128</b>. Also shown <figref idref="DRAWINGS">FIG. 1</figref> is a bus used for providing a common electrical communication path between the components <b>104</b>, <b>108</b>, <b>112</b>, <b>120</b>, <b>124</b>, <b>128</b> of the VIPS. The controller/graphics processor <b>104</b> is used to process captured facial images from a user (or speaker) speaking into a videophone. The lens <b>116</b> is used to focus and provide an image of the user's face onto the image sensor <b>112</b>. Thereafter, the actual image is captured by the image sensor <b>112</b> and transmitted to the controller/graphics processor <b>104</b>, where further processing may be performed. The controller/graphics processor <b>104</b> performs various processing that modifies the actual image captured by the videophone. The controller/graphics processor <b>104</b> may comprise circuitry that is used to control the memory <b>108</b>, the image sensor <b>112</b>, and the receiver/transmitter <b>128</b>. The controller/graphics processor <b>100</b> also interfaces with the user interface <b>120</b>, and the display <b>124</b>. The user interface <b>120</b> may generate signals to the controller/graphics processor <b>104</b> based on one or more inputs provided by a user. The user interface <b>120</b> may be used to input one or more commands into the controller/graphics processor <b>104</b>. For example, a user who wishes to engage in a videophone conversation with another party may use the user interface <b>120</b> to specify what preferred image or avatar he wishes to use. In another instance, a user of the videophone may use the user interface <b>120</b> to specify one or more background images to be used with the preferred image or avatar chosen. In a representative embodiment, the invention may be adapted to incorporate a person's actual facial image into the one or more selected backgrounds. A background image excludes the preferred image (facial image) or avatar. The selected background image may be used to replace the existing background. The avatar or preferred image may be incorporated into the selected background. The background may comprise an image of a fantasy location, such as a palace or tropical island, for example. The controller/graphics processor <b>104</b> may execute software and/or firmware that implements one or more video segmentation techniques. The one or more video segmentation techniques may employ object based video segmentation wherein each of the one or more facial features of the actual facial image is captured, isolated, and individually stored as objects into the memory <b>108</b>. These facial features or facial objects may be inserted into an avatar or a preferred image. The controller/graphics processor <b>104</b> may crop a facial feature of a selected avatar and subsequently insert a corresponding facial feature from the actual facial image. For example, a person's lips may be substituted or replaced using the object based video segmentation discussed. As a result, a user's lips, and associated lip movements are captured, and incorporated into the avatar or preferred facial image, for transmission to the other party, during a videophone conversation. Similarly, the user's eyes may be cropped and inserted into the selected avatar, for example, using the object-based video segmentation. The software and/or firmware may be stored in the memory <b>108</b> of the videophone. The memory <b>108</b> may comprise a non-volatile memory, such as a flash memory, for example. After the desired actual facial features are incorporated into the selected avatar or preferred image, a preferred background may be selected to replace the actual background image, in accordance with the various aspects of the invention. Thereafter, the desired image is transmitted to the party that is speaking to the user. Thus, in a preferred embodiment, a modified image or preferred display image may incorporate one or more actual facial features and a substituted background. This modified or preferred display image is transmitted to the other party's videophone. In accordance with the various aspects of the invention, the controller/graphics processor <b>104</b> may process one or more facial movements and/or facial expressions of the user, such that the avatar simulates those movements using the avatar's facial features. For example, the avatar's lips may lip-synch with the user's lip movements. Other facial features of the avatar may imitate or mimic the actual face of the user. Various aspects of the invention allow the videophone image processing system (VIPS) to provide various image processing functions. The software may implement an algorithm that merges the actual captured facial image of the user's head with information from an object database, such that the hidden side of the head may be rotated back, resulting in a frontal view. Furthermore, the software may implement an algorithm that combines the data obtained from the captured facial video with the object database to render an optimized facial image. The VIPS may comprise system with sufficient processing resources capable of developing a profile or model image of the user without using a stored image in memory as a reference.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 2A</figref> is relational diagram illustrating a user using a videophone <b>208</b> in accordance with an embodiment of the invention. The diagram of <figref idref="DRAWINGS">FIG. 2A</figref> shows a user <b>204</b> speaking into the videophone <b>208</b> during a conversation with another party. In this representative embodiment, a female user (or local user) <b>204</b> is shown speaking to a male user (or remote user) <b>206</b>. The frontal aspect <b>212</b> of the videophone <b>208</b> provides an illustration of the display of the videophone <b>208</b> while the female user <b>204</b> is talking to the male user <b>206</b>. In this representative embodiment, the videophone <b>208</b> is capable of providing a picture-in-picture (PIP) display <b>216</b>, such that the female user <b>204</b> may monitor the image being transmitted to the videophone of the male user <b>206</b>. As illustrated in <figref idref="DRAWINGS">FIG. 2A</figref>, the PIP displays a modified image of the female user <b>204</b>. The PIP is provided within the display <b>220</b> of the videophone <b>208</b>. The female user <b>204</b> instructs the videophone <b>208</b> to incorporate one or more of her facial features into a stored image. In this representative embodiment, the stored image comprises a preferred image of her. For example, the stored facial image may comprise a picture of the female user <b>204</b> in a more attractive appearance. In this representative embodiment, the female user <b>204</b> has not combed her hair, and as such, presents an untidy appearance. Further, the face of the female user <b>204</b> contains a number of unsightly blemishes that are undesirable. As a consequence, the female user <b>204</b> utilizes the videophone <b>208</b> to substitute the actual facial image with a preferred stored facial image. Further, in this representative embodiment, the videophone <b>208</b> crops the eyes and lips from the actual facial image and incorporates them into the stored facial image. As shown by way of the &#x201c;actual&#x201d; insert <b>224</b> and the &#x201c;with image processing&#x201d; insert <b>228</b> illustrated in <figref idref="DRAWINGS">FIG. 2A</figref>, the female user's lips and eyes from the actual facial image are incorporated into the stored facial image. As illustrated in <figref idref="DRAWINGS">FIG. 2A</figref>, the local user <b>204</b> was winking when the actual facial image was captured. Because the eyes and lips are incorporated into the stored facial image, the wink is retained when the preferred facial image is transmitted to the male user <b>206</b>. The disheveled hair and unsightly blemishes are eliminated by way of using the stored facial image.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 2B</figref> is a relational diagram illustrating a user using a videophone <b>236</b> in accordance with an embodiment of the invention. The diagram of <figref idref="DRAWINGS">FIG. 2B</figref> shows a user <b>232</b> speaking into the videophone <b>236</b> during a conversation with another party. In this representative embodiment, a female user <b>232</b> is shown speaking to a male user <b>234</b>. The frontal aspect <b>240</b> of the videophone <b>236</b> provides an illustration of the display of the videophone <b>236</b> while the female user <b>232</b> is speaking to the male user <b>234</b>. In this representative embodiment, the videophone <b>236</b> is capable of providing a PIP display <b>244</b>, such that the female user <b>232</b> may monitor the image being transmitted to the videophone used by the male user <b>234</b>. As illustrated in <figref idref="DRAWINGS">FIG. 2B</figref>, the PIP displays a modified image of the female user <b>232</b>. The PIP is provided within the display <b>248</b> of the videophone <b>236</b>. The female user <b>232</b> instructs the videophone <b>236</b> to incorporate one or more of her facial features into a stored image. In this representative embodiment, the female user <b>232</b> prefers being visualized by way of using an avatar with pig-like facial features. The avatar is stored in the memory of the videophone <b>236</b>. It is contemplated that the female user <b>232</b> may have any number of reasons to use such an avatar when transmitting a live video of her when speaking to the male user <b>234</b>. As a consequence, the female user <b>232</b> utilizes the videophone <b>236</b> to substitute the actual facial image with the preferred facial image (the pig avatar). Further, in this representative embodiment, the videophone <b>236</b> crops the eyes and lips from the actual facial image and incorporates them into the avatar. As shown by way of the &#x201c;actual&#x201d; insert <b>252</b> and the &#x201c;with image processing&#x201d; insert <b>256</b> illustrated in <figref idref="DRAWINGS">FIG. 2B</figref>, the female user's lips and eyes from the actual facial image are incorporated into the avatar. As shown, the local user <b>232</b> was winking when the actual facial image was captured. Because one or more user selectable facial objects or facial features (e.g., eyes and lips) are incorporated into the avatar, the wink exhibited by the female user <b>232</b> is retained when the avatar is transmitted to the male user <b>234</b>.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 2C</figref> is diagram that illustrates how image processing may be employed using the video image processing system (VIPS) to improve the appearance of a speaker, in accordance with an embodiment of the invention. In relation to the embodiment of <figref idref="DRAWINGS">FIG. 2A</figref>, <figref idref="DRAWINGS">FIG. 2C</figref> portrays two scenarios: a) a videophone <b>260</b> and its corresponding display <b>264</b> when no image processing is applied to facial video captured from a female speaker, and b) a videophone <b>268</b> and its corresponding display <b>272</b> when image processing is applied to facial video captured from a female speaker. For the case where no image processing is applied, the display <b>264</b> of this videophone <b>260</b> illustrates an actual facial image in which the female speaker's appearance is undesirable. On the other hand, in the case when image processing is applied, the display <b>272</b> of the videophone <b>268</b> illustrates a preferred image, in which a stored facial image of the female speaker is used. Furthermore, the movements of the female's eyes and lips are incorporated into the stored facial image. In addition, the embodiment of <figref idref="DRAWINGS">FIG. 2C</figref>, which incorporates image processing in accordance with the various aspects of the invention, also illustrates the incorporation of a preferred background&#x2014;a tropical island setting.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIGS. 3A and 3B</figref> are operational flow diagrams that describe the use of the videophone image processing system (VIPS) during a videophone conversation, in accordance with an embodiment of the invention. At step <b>304</b>, a user or speaker of a first videophone selects a stored image (i.e., a desired facial image) from a database of preferred images. The database may be established in a memory of the VIPS, as an initialization procedure, prior to engaging in one or more videophone conversations. The stored image may comprise an image that provides an attractive appearance of the user, for example. The desired facial image may comprise an avatar that is used to represent the user. The avatar may comprise, for example, a computer generated character, a cartoon character, a historically significant public figure, a character from a video game, a literary character, a movie character, or an animal. At step <b>308</b>, the user may select one or more of his facial features that will be incorporated into the selected facial image. The facial features he uses may comprise his eyes, lips, and nose, for example. The user may select any number of facial features or facial objects from his face that are incorporated into the selected facial image. Next, at step <b>312</b>, the user may optionally, select a desired background. One or more backgrounds may be stored in a memory of the VIPS. The background may comprise a tropical island setting or beach setting, for example. Next, at step <b>316</b>, the user of the first videophone may subsequently employ one or more tracking algorithms and voice recognition algorithms for tracking the facial features that were previously selected by the user. The one or more tracking algorithms and voice recognition algorithms may be used for cropping the desired facial objects and subsequently inserting the facial objects into the selected facial image. Details of the tracking and voice algorithms may be found in U.S. application Ser. No. 11/266,442, filed on Nov. 3, 2005, which is hereby incorporated herein by reference in its entirety. At step <b>320</b>, a controller/graphics processor located in the VIPS, may utilize video object segmentation techniques to crop one or more facial objects and incorporate the one or more facial objects into the desired facial image. Further, the desired facial image may be incorporated into a preferred background. Optionally, at step <b>324</b>, by way of control from the user, the modified image may be displayed locally to the user. Next, at step <b>328</b>, the one or more facial objects, the desired facial image, and the preferred background are compressed/coded for transmission to a user or speaker of a second videophone. Thereafter, at step <b>332</b>, the compressed/coded video is transmitted from the first videophone to the second videophone. At step <b>336</b>, the receiver of the second videophone decodes/decompresses the compressed/coded video. Next, at step <b>340</b>, the controller/graphics processor of the second videophone appropriately renders the video to be displayed after receiving the one or more selected facial objects, the preferred background, and the selected facial image. Finally, at step <b>344</b>, the received video is displayed to the user of the second videophone.</p>
<p id="p-0020" num="0019">While the invention has been described with reference to certain embodiments, it will be understood by those skilled in the art that various changes may be made and equivalents may be substituted without departing from the scope of the invention. In addition, many modifications may be made to adapt a particular situation or material to the teachings of the invention without departing from its scope. Therefore, it is intended that the invention not be limited to the particular embodiment disclosed, but that the invention will include all embodiments falling within the scope of the appended claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A device comprising:
<claim-text>one or more circuits operable to, at least:
<claim-text>receive a first image of at least a portion of a user of the device;</claim-text>
<claim-text>perform video object segmentation of the first image;</claim-text>
<claim-text>crop one or more objects from the first image; and</claim-text>
<claim-text>insert the cropped one or more objects into a second image to generate a modified image.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The device of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein said one or more circuits are further operable to display the generated modified image.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The device of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein said one or more circuits are further operable to merge the received first image of at least a portion of the user of the device with object database information describing a different view of the user of the device.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The device of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein said one or more circuits are further operable to transmit the generated modified image.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The device of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein said one or more circuits reside in a first phone and said one or more circuits are further operable to transmit the generated modified image to a second phone.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The device of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein said one or more circuits are further operable to store the generated modified image in a memory.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The device of <claim-ref idref="CLM-00006">claim 6</claim-ref> wherein the memory resides in the device.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The device of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein said one or more circuits are further operable to receive a designation of the generated modified image as a preferred image.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. An image processing device comprising:
<claim-text>circuitry operable to:
<claim-text>receive a selection of a first facial image from a plurality of stored facial images;</claim-text>
<claim-text>receive a selection of a facial feature;</claim-text>
<claim-text>capture a second facial image of a user of the image processing device;</claim-text>
<claim-text>identify the selected facial feature in the captured second facial image;</claim-text>
<claim-text>crop the identified facial feature from the captured second facial image;</claim-text>
<claim-text>insert the identified facial feature into the selected first facial image to produce an edited facial image; and</claim-text>
<claim-text>transmit the edited facial image to another user of another image processing device.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The device of <claim-ref idref="CLM-00009">claim 9</claim-ref> wherein said circuitry is further operable to merge the captured second facial image of the user of the image processing device with object database information describing a different view of the user of the image processing device.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The device of <claim-ref idref="CLM-00009">claim 9</claim-ref> wherein said circuitry is further operable to use video object based segmentation.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The device of <claim-ref idref="CLM-00009">claim 9</claim-ref> wherein said circuitry is further operable to use voice recognition to track the selected facial feature in the captured second facial image.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The image processing device of <claim-ref idref="CLM-00009">claim 9</claim-ref> wherein said circuitry is further operable to store the edited facial image in a memory.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The image processing device of <claim-ref idref="CLM-00013">claim 13</claim-ref> wherein the memory resides in the image processing device.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The image processing device of <claim-ref idref="CLM-00009">claim 9</claim-ref> wherein said circuitry is further operable to receive a designation of the first facial image as a preferred image.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. A method comprising:
<claim-text>capturing a first facial image of a user of a device;</claim-text>
<claim-text>extracting a user-selected facial feature from the captured first facial image;</claim-text>
<claim-text>editing a second facial image to include the extracted facial feature, the second facial image being user-selected from a plurality of stored facial images; and</claim-text>
<claim-text>transmitting the edited second facial image to another user of another device.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref> wherein the method is performed in a first phone and the transmitting further comprises transmitting the edited second facial image to a second phone.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref> wherein the extracting comprises cropping the user-selected facial feature from the captured first facial image of the user and the editing comprises inserting the cropped facial feature into the second facial image.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref> wherein the extracting uses video object based segmentation.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The method of <claim-ref idref="CLM-00019">claim 19</claim-ref> wherein the extracting uses voice recognition to track the user-selected facial feature in the captured facial first image.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref> further comprising storing the edited second facial image in a memory.</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The method of <claim-ref idref="CLM-00021">claim 21</claim-ref> wherein the memory resides in the image processing device.</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref> further comprising receiving a designation of the second facial image as a preferred image.</claim-text>
</claim>
</claims>
</us-patent-grant>
