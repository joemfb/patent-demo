<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626237-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626237</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12207230</doc-number>
<date>20080909</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<us-term-of-grant>
<us-term-extension>486</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>B</subclass>
<main-group>1</main-group>
<subgroup>38</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>B</subclass>
<main-group>7</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>455557</main-classification>
<further-classification>455 413</further-classification>
<further-classification>455420</further-classification>
<further-classification>4555752</further-classification>
<further-classification>455563</further-classification>
<further-classification>455 412</further-classification>
</classification-national>
<invention-title id="d2e53">Integrating a cellular phone with a speech-enabled softphone</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>7315521</doc-number>
<kind>B2</kind>
<name>Gadamsetty et al.</name>
<date>20080100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>370311</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>2002/0077095</doc-number>
<kind>A1</kind>
<name>Fu et al.</name>
<date>20020600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>2004/0063452</doc-number>
<kind>A1</kind>
<name>Tomoda</name>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>455519</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>2004/0147282</doc-number>
<kind>A1</kind>
<name>Nakasato et al.</name>
<date>20040700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>4555521</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>2005/0014487</doc-number>
<kind>A1</kind>
<name>Kobayashi et al.</name>
<date>20050100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2005/0272415</doc-number>
<kind>A1</kind>
<name>McConnell et al.</name>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>455418</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2006/0019713</doc-number>
<kind>A1</kind>
<name>Rokusek et al.</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>455563</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2007/0004473</doc-number>
<kind>A1</kind>
<name>Clark et al.</name>
<date>20070100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>4555752</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2008/0037727</doc-number>
<kind>A1</kind>
<name>Sivertsen et al.</name>
<date>20080200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>379 8813</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2008/0043718</doc-number>
<kind>A1</kind>
<name>Chu</name>
<date>20080200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>370352</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2008/0261603</doc-number>
<kind>A1</kind>
<name>Sever et al.</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>455445</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>WO</country>
<doc-number>WO 0137262</doc-number>
<kind>A1</kind>
<date>20010500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00013">
<othercit>Gkeli, Maria, &#x201c;EP Application No. 08253043.7 Search Report&#x201d;, Feb. 9, 2009, Publisher: EPO, Published in: EP.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00014">
<othercit>&#x201c;EP Application No. 08253043.7-2414 / 2040444 Office Action Sep. 29, 2009&#x201d;, , Publisher: EPO, Published in: EP.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00015">
<othercit>Avaya Inc., Korean Patent Application No. 2008-0093570, Office Action dated Jan. 14, 2013, 2 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>16</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>None</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>5</number-of-drawing-sheets>
<number-of-figures>5</number-of-figures>
</figures>
<us-related-documents>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60974817</doc-number>
<date>20070924</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20090082062</doc-number>
<kind>A1</kind>
<date>20090326</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Boyaci</last-name>
<first-name>Omer</first-name>
<address>
<city>Ahmetpasa</city>
<country>TR</country>
</address>
</addressbook>
<residence>
<country>TR</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Krishnan</last-name>
<first-name>Parameshwaran</first-name>
<address>
<city>Basking Ridge</city>
<state>NJ</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Krishnakumar</last-name>
<first-name>Anjur Sundaresan</first-name>
<address>
<city>Rocky Hill</city>
<state>NJ</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="004" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Yajnik</last-name>
<first-name>Shalini</first-name>
<address>
<city>Berkeley Heights</city>
<state>NJ</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Boyaci</last-name>
<first-name>Omer</first-name>
<address>
<city>Ahmetpasa</city>
<country>TR</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Krishnan</last-name>
<first-name>Parameshwaran</first-name>
<address>
<city>Basking Ridge</city>
<state>NJ</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Krishnakumar</last-name>
<first-name>Anjur Sundaresan</first-name>
<address>
<city>Rocky Hill</city>
<state>NJ</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="004" designation="us-only">
<addressbook>
<last-name>Yajnik</last-name>
<first-name>Shalini</first-name>
<address>
<city>Berkeley Heights</city>
<state>NJ</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<last-name>Maldjian, Esq.</last-name>
<first-name>John</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
<agent sequence="02" rep-type="attorney">
<addressbook>
<last-name>Walter, Esq.</last-name>
<first-name>Alexander D.</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
<agent sequence="03" rep-type="attorney">
<addressbook>
<orgname>Kacvinsky Daisak PLLC</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Avaya Inc.</orgname>
<role>02</role>
<address>
<city>Basking Ridge</city>
<state>NJ</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Pan</last-name>
<first-name>Yuwen</first-name>
<department>2649</department>
</primary-examiner>
<assistant-examiner>
<last-name>Wang</last-name>
<first-name>Fanghwa</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A method is disclosed that enables access to the paired combination of a personal computer and cellular telephone, the access being provided through voice commands from a Bluetooth headset or other type of wireless headset. Through the headset, a user is able to invoke a speech-recognition engine during a call in progress that involves the cell phone with which the personal computer has been paired. The set of phrases that can be recognized by the engine, which is resident at the personal computer, are based on the current call-processing state of the call that is being handled by the paired cell phone. Some of the phrases correspond to commands to retrieve one or more entries from a database, such as one stored at the cell phone. One or more of the commands can be for controlling the cell phone or a softphone application that executes at the personal computer.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="139.62mm" wi="148.93mm" file="US08626237-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="178.82mm" wi="143.17mm" orientation="landscape" file="US08626237-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="218.95mm" wi="157.06mm" orientation="landscape" file="US08626237-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="167.89mm" wi="156.46mm" orientation="landscape" file="US08626237-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="206.93mm" wi="139.78mm" orientation="landscape" file="US08626237-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="217.68mm" wi="157.40mm" orientation="landscape" file="US08626237-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">The underlying concepts, but not necessarily the language, of the following case are incorporated by reference:</p>
<p id="p-0003" num="0002">U.S. Patent Application Ser. No. 60/974,817, filed Sep. 24, 2007. If there are any contradictions or inconsistencies in language between the present application and the case that has been incorporated by reference that might affect the interpretation of the claims in the present application, the claims in this application should be interpreted to be consistent with the language in this application.</p>
<heading id="h-0002" level="1">FIELD OF THE INVENTION</heading>
<p id="p-0004" num="0003">The present invention relates to telecommunications in general, and, more particularly, to integrating a cellular telephone with a speech-enabled softphone.</p>
<heading id="h-0003" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0005" num="0004">A cellular telephone is a portable communication device that is used extensively by business enterprise personnel. As a portal of telephony communication, the cellular telephone (or &#x201c;cell phone&#x201d;) provides mobile communication to its user, thereby enabling the user to be reached most anywhere and at any time.</p>
<p id="p-0006" num="0005">A personal computer is another communication device that is used extensively by business enterprise personnel. As a portal of data communication, enabling exchanges of email and instant messaging, the computer can also be furnished with a &#x201c;softphone&#x201d; software application that enables the computer to act as a telephone. A notebook computer, or other type of portable computer, further enhances its user's ability to communicate with others most anywhere and at any time.</p>
<p id="p-0007" num="0006">A wireless headset, such as a Bluetooth headset, can be paired with a communication device such as a cellular telephone or a personal computer. For example, when paired with a cellular telephone, the headset is able to exchange audio signals with the telephone, enabling the headset user to converse with the other party on a call via the headset. Additionally, the headset comprises control buttons that enable the user to answer or hang up on a call, or to adjust the volume of the audio signal being provided to the headset user.</p>
<p id="p-0008" num="0007">Given the extensive use of cell phones, softphones on personal computers, and wireless headsets, integrating at least some aspects of all three devices would be desirable.</p>
<heading id="h-0004" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0009" num="0008">The present invention enables access to the paired combination of a personal computer and cellular telephone, the access being provided through voice commands from a Bluetooth headset or other type of wireless headset. In accordance with the illustrative embodiment, voice commands given through the headset enable a user to perform a variety of call interactions in a hands-free manner, on a call in progress that is being handled by the cell phone or by a softphone application that executes at the personal computer. A speech-recognition engine that recognizes the voice commands is resident at the personal computer. The engine is invoked on demand, a benefit of which is that the speech recognition, as a result, can be made targeted and efficient. The set of phrases that can be recognized by the engine are based on the current call-processing state of a call, such as one that is being handled by the paired cell phone. Some of the phrases correspond to commands to retrieve one or more entries from a database, such as a call log or a phone book database, which is either stored at the cell phone or at a server in a network.</p>
<p id="p-0010" num="0009">The personal computer of the illustrative embodiment features a personal area network (PAN) interface that the computer uses to communicate directly with each of the cell phone and headset. The PAN itself is a shared computer network that is used to enable the direct communication of the personal computer with each of the other two devices. The coverage area of the PAN is considered short-range, in that it is typically only a few meters across.</p>
<p id="p-0011" num="0010">A key difference between the network configuration of the illustrative embodiment and some network configurations in the prior art is that the network configuration of the illustrative embodiment consists of a headset paired with a personal computer and the personal computer paired with a cell phone, but not the headset paired with the cell phone. The network configuration of the illustrative embodiment is significant, in that it enables the personal computer to essentially act as an intermediary between the headset and cell phone and, as a result, provides value-added features such as on-demand speech recognition and execution of recognized voice commands. And the personal computer is able to interpret one or more control signals arriving from the headset as different types of stimuli or indications, in some embodiments based on the call-processing state of a call being handled by the cell phone or the softphone application.</p>
<p id="p-0012" num="0011">In some embodiments, the personal computer is able to receive and recognize a series of control signals from the headset. A predetermined combination of control signals can be used as a pass phrase that allows the headset user subsequent access to the softphone application, such as to issue voice commands to be recognized by the softphone. If the received series of control signals matches the predetermined combination for that user at that personal computer, the computer grants the user subsequent access to the softphone.</p>
<p id="p-0013" num="0012">The illustrative embodiment features a softphone terminal at a personal computer that is paired with a cellular telephone. However, it will be clear to those skilled in the art, after reading this specification, how to make and use alternative embodiments that feature another type of terminal that is capable of performing the tasks described herein and a second telecommunications device to be controlled by voice commands that are recognized by the terminal. Furthermore, although the PAN of the illustrative embodiment operates in conformity with the Bluetooth specification, it will be clear to those skilled in the art how to make and use alternative embodiments in which the PAN operates in conformity with other sets of protocols.</p>
<p id="p-0014" num="0013">The illustrative embodiment of the present invention comprises a method comprising: establishing i) a first connection via a personal area network between a first telecommunications device and a headset device and ii) a second connection between the first telecommunications device and a second telecommunications device; receiving, at the first telecommunications device, an indication that the second telecommunications device is handling a first telephone call; receiving, at the first telecommunications device from the headset device, i) a control signal generated by the headset device as the result of an actuation of an answer/end-call button at the headset device, followed by ii) a voice signal that conveys an utterance made by a user of the headset device; and invoking, at the first telecommunications device, a speech-recognition function on the utterance received, based on the receiving of i) the indication and ii) the control signal.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 1</figref> depicts a schematic diagram of telecommunications system <b>100</b> in accordance with the illustrative embodiment of the present invention.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram of the salient components of personal computer <b>101</b> within system <b>100</b>.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 3</figref> depicts a flowchart of the salient tasks associated with the operation of the illustrative embodiment of the present invention.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 4</figref> depicts a flowchart of the salient tasks associated with the performance of task <b>306</b> of <figref idref="DRAWINGS">FIG. 3</figref>.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 5</figref> depicts a flowchart of the salient tasks associated with the performance of task <b>307</b> of <figref idref="DRAWINGS">FIG. 3</figref>.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0006" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 1</figref> depicts a schematic diagram of telecommunications system <b>100</b> in accordance with the illustrative embodiment of the present invention. System <b>100</b> comprises personal computer <b>101</b>, wireless headset <b>102</b>, cellular telephone <b>103</b>, telecommunications network <b>106</b>, and database <b>109</b>, interconnected as shown.</p>
<p id="p-0021" num="0020">Personal computer <b>101</b> is computer that is intended to be operated directly by an end user, such as a desktop computer, a laptop computer, or a tablet computer. In accordance with the illustrative embodiment, computer <b>101</b> provides the functionality of a softphone terminal, in that it executes a softphone application as is known in the art. A softphone terminal is the combination of a general-purpose, personal computer and a softphone software application that runs on the computer. The resident softphone application enables computer <b>101</b>'s user to make and receive telephone calls through the computer.</p>
<p id="p-0022" num="0021">It will be clear to those skilled in the art, after reading this specification, how to make and use alternative embodiments of the present invention in which computer <b>101</b> is another type of device that is capable of executing software or firmware that embodies at least some of the tasks of the illustrative embodiment. For example, computer <b>101</b> can instead be a SIP- or IP-capable deskset, an automobile-based component that is Bluetooth-capable, or a cellular phone.</p>
<p id="p-0023" num="0022">The salient components of computer <b>101</b>, described below and with respect to <figref idref="DRAWINGS">FIG. 2</figref>, are together capable of performing the tasks of the illustrative embodiment. The salient tasks that are performed by computer <b>101</b> are described below and with respect to <figref idref="DRAWINGS">FIGS. 3 through 5</figref>.</p>
<p id="p-0024" num="0023">Wireless headset <b>102</b> is a device that is capable of exchanging control signals and voice signals with another wireless-capable device, such as personal computer <b>101</b>. In some embodiments, headset <b>102</b> comprises one or more user-actuated buttons, such as an &#x201c;answer/end-call&#x201d; button (or &#x201c;main&#x201d; button), an &#x201c;increase volume&#x201d; button, and a &#x201c;decrease volume&#x201d; button. Each button, when actuated, produces a different control signal that is transmitted to the other wireless-capable device, in this case personal computer <b>101</b>. Additionally, during a call that involves either i) the softphone application of computer <b>101</b> or ii) cellular telephone <b>103</b>, headset <b>102</b> transmits its user's voice to computer <b>101</b> and receives the other party's voice from computer <b>101</b>.</p>
<p id="p-0025" num="0024">Cellular telephone <b>103</b> is a telecommunications device that is capable of handling a telephone call for its user via a cellular-based network. Telephone <b>103</b> is able to call, or to be called by, another device within telecommunications system <b>100</b>. For example, a calling party might attempt to call telephone <b>103</b> by dialing a telephone number that routes to telephone <b>103</b>, whereupon telephone <b>103</b> receives an incoming call from the calling party.</p>
<p id="p-0026" num="0025">In accordance with the illustrative embodiment, telephone <b>103</b> is a cellular telephone. However, it will be clear to those skilled in the art, after reading this specification, how to make and use embodiments of the present invention in which telephone <b>103</b> is a different type of device than a cell phone.</p>
<p id="p-0027" num="0026">Personal computer <b>101</b> is able to communicate directly with wireless headset <b>102</b> and cellular telephone <b>103</b> via a personal area network. Each pairing of computer <b>101</b> with headset <b>102</b> and computer <b>101</b> with telephone <b>103</b> is established separately within the personal area network, wherein the two pairings are depicted as connections <b>104</b> and <b>105</b>, respectively. As is known in the art, a personal area network (PAN) is a computer network used for communication among computer devices and telecommunications devices that are near a person, such as devices <b>101</b> through <b>103</b>. The coverage area of a PAN is considered short-range, in that it is typically only a few meters across. In accordance with the illustrative embodiment, the PAN used by devices <b>101</b> and <b>102</b> to communicate directly with each other, as well as by devices <b>101</b> and <b>103</b>, operates in accordance with the Bluetooth specification. Additionally, the direct communications within each pairing of devices is conducted over a secure, globally unlicensed Industrial, Scientific, and Medical (ISM) radio-frequency bandwidth in the 2.4 GHz range.</p>
<p id="p-0028" num="0027">In some alternative embodiments, the devices within each pairing (i.e., devices <b>101</b> and <b>102</b>, devices <b>101</b> and <b>103</b>) communicate with each other via a radio-frequency bandwidth and/or type of wireless PAN that is different from that in the illustrative embodiment, such as one based on the IrDA, UWB, or ZigBee specification. In some other alternative embodiments, the paired devices communicate directly via a wired PAN, such as one based on USB or FireWire.</p>
<p id="p-0029" num="0028">Telecommunications network <b>106</b> provides the connectivity among various telecommunications terminals in system <b>100</b> and enables the transport and control of communications signals between two or more terminals per call. The communications signals convey bitstreams of encoded media such as audio, video, and so forth. To this end, network <b>106</b> comprises one or more interconnected data-processing systems such as switches, servers, routers, and gateways, as are well-known in the art.</p>
<p id="p-0030" num="0029">In accordance with the illustrative embodiment, network <b>106</b> comprises a variety of subnetworks for the purpose of providing connectivity to their constituent devices. For example, network <b>106</b> comprises a local area network (LAN) for the purpose of providing connectivity to computer <b>101</b>. In accordance with the illustrative embodiment, computer <b>101</b> connects to the LAN via wired Ethernet connection <b>107</b>, while in some alternative embodiments, the connection to the LAN is wireless. Additionally, network <b>106</b> comprises a cellular network for the purpose of providing connectivity to telephone <b>103</b>. In accordance with the illustrative embodiment, telephone <b>103</b> connects to the cellular network via radio-frequency link <b>108</b>, which is provided by whichever radio base station in the network is assigned to handle telephone <b>103</b> at any particular moment.</p>
<p id="p-0031" num="0030">Database <b>109</b> comprises a server that stores information such as call logs and telephone directories (i.e., &#x201c;phone books&#x201d;). In situations where cellular telephone <b>103</b> does not support call log or phone book retrieval through connection <b>105</b>, computer <b>101</b> is able to retrieve such information from database <b>109</b>, such as via web access.</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram of the salient components of personal computer <b>101</b> in accordance with the illustrative embodiment of the present invention. The depicted components are interconnected as shown. In accordance with the illustrative embodiment, computer <b>101</b> comprises:</p>
<p id="p-0033" num="0032">i. personal area network interface <b>201</b>,</p>
<p id="p-0034" num="0033">ii. local area network interface <b>202</b>,</p>
<p id="p-0035" num="0034">iii. processor <b>203</b>,</p>
<p id="p-0036" num="0035">iv. memory <b>204</b>,</p>
<p id="p-0037" num="0036">V. microphone <b>205</b>,</p>
<p id="p-0038" num="0037">vi. display <b>206</b>,</p>
<p id="p-0039" num="0038">vii. loudspeaker <b>207</b>, and</p>
<p id="p-0040" num="0039">viii. keyboard/pointer <b>208</b>.</p>
<p id="h-0007" num="0000">It will be clear to those skilled in the art, after reading this disclosure, how to make and use alternative embodiments of the present invention in which telecommunications computer <b>101</b> comprises any subcombination of the components listed above.</p>
<p id="p-0041" num="0040">Personal area network (PAN) interface <b>201</b> enables computer <b>101</b> to receive signals directly from and transmit signals directly to headset <b>102</b> and telephone <b>103</b>, in well-known fashion. To do so, interface <b>201</b> comprises one or more wireless transceivers, each transceiver being either internal to computer <b>101</b> or a peripheral device that is added to the computer (e.g., via USB interface, etc.). In accordance with the illustrative embodiment, computer <b>101</b> receives and transmits control signals via PAN interface <b>201</b> in well-known fashion. For example, PAN interface <b>201</b> can receive control signals from headset <b>102</b> and a notification from telephone <b>103</b> of an incoming call. Computer <b>101</b> receives and transmits voice signals as well via PAN interface <b>201</b> to either device with which computer <b>101</b> is paired (i.e., headset <b>102</b> and telephone <b>103</b>). In any event, it will be clear to those skilled in the art how to make and user PAN interface <b>201</b>.</p>
<p id="p-0042" num="0041">Local area network (LAN) interface <b>202</b> enables computer <b>101</b> to receive signals from and transmit signals to one or more devices within telecommunications network <b>106</b>, as well as database <b>109</b>, in well-known fashion. In accordance with the illustrative embodiment, computer <b>101</b> receives and transmits control signals via LAN interface <b>202</b> in well-known fashion. Additionally, LAN interface <b>202</b> receives and transmits media waveform signals in well-known fashion, such as audio signals that are encoded via the ITU G.729 standard (or other standard) and represented in Voice over Internet Protocol (VoIP) packet streams of data. As those who are skilled in the art will appreciate, in some alternative embodiments computer <b>101</b> receives and transmits media waveform signals that are encoded and/or represented in a different format. It will be clear to those skilled in the art how to make and user LAN interface <b>202</b>.</p>
<p id="p-0043" num="0042">Processor <b>203</b> is a general-purpose processor that is capable of receiving information from PAN interface <b>201</b>, LAN interface <b>202</b>, microphone <b>205</b>, and keyboard/pointer <b>208</b>, of executing instructions stored in memory <b>204</b> such as those that correspond to some or all of the tasks of the illustrative embodiment, of reading data from and writing data into memory <b>204</b>, and of transmitting information to PAN interface <b>201</b> and LAN interface <b>202</b>. Additionally, processor <b>203</b> is able to provide signals to display <b>206</b> and loudspeaker <b>207</b>. Processor <b>203</b> is also able to perform voice recognition on signals received from microphone <b>205</b> and speech-to-text conversion on signals received from PAN interface <b>201</b> or LAN interface <b>202</b>.</p>
<p id="p-0044" num="0043">In some alternative embodiments of the present invention, processor <b>203</b> might be a special-purpose processor. In some other alternative embodiments, the functionality performed by processor <b>203</b> might be divided up among multiple processors (e.g., one for speech recognition, another for general processing, etc.).</p>
<p id="p-0045" num="0044">Memory <b>204</b> stores the instructions and data used by processor <b>203</b>, in well-known fashion. Memory <b>204</b> can be any combination of dynamic random-access memory (RAM), flash memory, disk drive memory, and so forth. In accordance with the illustrative embodiment, memory <b>204</b> is further capable of storing database entries received from either telephone <b>103</b> or database <b>109</b>.</p>
<p id="p-0046" num="0045">Microphone <b>205</b> is a transducer that is able to receive acoustic signals and to convert them to electrical signals for transmission and/or storage. It will be clear to those skilled in the art how to make and use microphone <b>205</b>.</p>
<p id="p-0047" num="0046">Display <b>206</b> is a device that is able to present the computer's user with a visual representation of information, including database entries received from either telephone <b>103</b> or database <b>109</b>. It will be clear to those skilled on the art how to make and use display <b>206</b>.</p>
<p id="p-0048" num="0047">Loudspeaker <b>207</b> is an electro-acoustic transducer that is able to present the computer's user with an audible representation of information. As those who are skilled in the art will appreciate, in some alternative embodiments of computer <b>101</b>, loudspeaker <b>207</b> can instead be a type of electro-acoustic transducer other than a loudspeaker, such as an earpiece. In any event, it will be clear to those skilled on the art how to make and use loudspeaker <b>207</b>.</p>
<p id="p-0049" num="0048">Keyboard/pointer <b>208</b> is a keyboard-based and/or pointer-based device that is able to accept user input signals and to convert them to electrical signals for the purpose of controlling other elements of computer <b>101</b>. It will be clear to those skilled in the art how to make and use keyboard/pointer <b>208</b>.</p>
<p id="p-0050" num="0049">In accordance with the illustrative embodiment, processor <b>203</b> of computer <b>101</b> performs the tasks described below and with respect to <figref idref="DRAWINGS">FIGS. 3 through 5</figref>. As those who are skilled in the art will appreciate, in some alternative embodiments, two or more components within computer <b>101</b> can perform different subsets of the described tasks.</p>
<p id="p-0051" num="0050"><figref idref="DRAWINGS">FIGS. 3 through 5</figref> depict flowcharts of the salient tasks associated with the operation of the illustrative embodiment of the present invention. As those who are skilled in the art will appreciate, in some alternative embodiments, only a subset of the depicted tasks is performed. In some other alternative embodiments, at least some of the tasks are performed simultaneously or in a different order from that depicted.</p>
<p id="p-0052" num="0051">In the example that follows, computer <b>101</b>'s user, headset <b>102</b>'s user, and telephone <b>103</b>'s user are the same person. Computer <b>101</b> is a notebook computer, which runs a softphone application, and telephone <b>103</b> is a cellular telephone. The user of the three devices has situated headset <b>102</b> and telephone <b>103</b> to be within close enough proximity to computer <b>101</b>, so that the devices are able to communicate directly with each other via a personal area network. Also, the personal area network has been initialized, so that devices <b>101</b> and <b>102</b> are able to exchange information directly with each other, as are devices <b>101</b> and <b>103</b>. In accordance with the illustrative embodiment, headset <b>102</b> and telephone <b>103</b> do not directly communicate with each other, for they are each paired instead with computer <b>101</b>.</p>
<p id="p-0053" num="0052">Referring now to <figref idref="DRAWINGS">FIG. 3</figref>, at task <b>301</b>, computer <b>101</b> executes a softphone application in well-known fashion. Going forward from this event, the softphone will be at least able to handle incoming and outgoing telephone calls via connection <b>107</b>, based on input from computer <b>101</b>'s user. And if it is paired with cellular telephone <b>103</b>, the softphone is also able to handle calls via connection <b>105</b>, as described below.</p>
<p id="p-0054" num="0053">At task <b>302</b>, computer <b>101</b> establishes a first connection via a personal area network (PAN) between the computer (i.e., a &#x201c;first telecommunications device&#x201d;) and wireless headset <b>102</b> (i.e., a &#x201c;headset device&#x201d;), in well-known fashion. It will be clear to those skilled in the art how devices &#x201c;pair&#x201d; with each other within a PAN.</p>
<p id="p-0055" num="0054">At task <b>303</b>, computer <b>101</b> establishes a second connection via a personal area network between the computer and cellular telephone <b>103</b> (i.e., a &#x201c;second telecommunications device&#x201d;). In some alternative embodiments, computer <b>101</b> does not establish the second connection because the computer will not be controlling or exchanging signals directly with a separate telephone and will instead have access to telephony through the computer's softphone application only.</p>
<p id="p-0056" num="0055">At task <b>304</b>, computer <b>101</b> determines that a telephone call is being handled, either by telephone <b>103</b> or by the softphone application. In a first illustrative example, computer <b>101</b> determines that telephone <b>103</b> is handling a telephone call. The determination is based on computer <b>101</b> receiving, or otherwise deriving based on one or more signals from headset <b>102</b> and/or telephone <b>103</b>, an indication as to the specific call-processing state that the call is in, as described below and with respect to task <b>504</b>. As described below, the set of commands that computer <b>101</b> can recognize and act upon depends on the current call-processing state.</p>
<p id="p-0057" num="0056">In one set of embodiments, the indication might signify that an incoming call to telephone <b>103</b> has been answered by its user. In this first case, the indication might comprise receiving both i) an incoming call notification from telephone <b>103</b>, followed by ii) a control signal from headset <b>102</b>; the control signal has been generated by the headset as the result of an actuation of the answer/end-call button at the headset and, in this first case, is interpreted as stimulus to answer the call. In another set of embodiments, the indication might signify that an outgoing call from telephone <b>103</b> has been answered by the far-end party. In this second case, the indication is the result of receiving a call-answer indication from telephone <b>103</b>. As those who are skilled in the art will appreciate, the indication that telephone <b>103</b> is handling a call might be based on other combinations of signals than those described.</p>
<p id="p-0058" num="0057">In a second illustrative example, computer <b>101</b> determines instead that its softphone application is handling a telephone call with a far-end party (i.e., via connection <b>107</b>). Computer <b>101</b> determines this independently of whether it is paired with telephone <b>103</b> or not. The determination is based on computer <b>101</b> receiving&#x2014;or otherwise deriving based on one or more signals from headset <b>102</b> and/or the telephony equipment of the far-end party&#x2014;an indication as to the specific call-processing state that the call is in, as described below and with respect to task <b>504</b>. As described below, the set of commands that computer <b>101</b> can recognize and act upon depends on the current call-processing state.</p>
<p id="p-0059" num="0058">At task <b>305</b>, computer <b>101</b> receives and interprets a control signal from headset <b>102</b>. The control signal is the result of an actuation of one of the buttons at the headset. In accordance with the illustrative embodiment, the control signal is specifically the result of an actuation of the answer/end-call button. In fact, each time a control signal is received from the headset&#x2014;and there can be multiple control signals received throughout an interval associated with a call&#x2014;computer <b>101</b> interprets the receiving of the control signal as i) a stimulus to initiate an outgoing call, ii) a stimulus to answer an incoming call, iii) a stimulus to invoke a speech-recognition function on an utterance received from headset <b>102</b>, if such an utterance is received, or iv) an indication of something else. In accordance with the illustrative embodiment, the interpretation is based on the current call-processing state&#x2014;that is, the call-processing state of the call at the time that the control signal is received.</p>
<p id="p-0060" num="0059">At task <b>306</b>, computer <b>101</b> performs authentication on the user. Task <b>306</b> is described below and with respect to <figref idref="DRAWINGS">FIG. 4</figref>. In some alternative embodiments, task <b>306</b> is skipped entirely.</p>
<p id="p-0061" num="0060">At task <b>307</b>, computer <b>101</b> performs speech recognition on an utterance made by the user, based on having received the control signal at task <b>305</b>. Task <b>307</b> is described below and with respect to <figref idref="DRAWINGS">FIG. 5</figref>.</p>
<p id="p-0062" num="0061">Although task execution is depicted as then terminating, it will be clear to those skilled in the art, after reading this specification, how to make and use embodiments in which computer <b>101</b> continues to monitor for additional control signals and performs accordingly the functions that are invoked by those control signals. Additionally, it will be clear to those skilled in the art, after reading this specification, how to make and use embodiments in which computer <b>101</b> performs at least some of the tasks described herein on subsequent incoming or outgoing calls that involve cellular telephone <b>103</b> and/or the softphone application.</p>
<p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. 4</figref> depicts a flowchart of the salient tasks associated with the performance of task <b>306</b>. At task <b>401</b>, computer <b>101</b> receives, from headset <b>102</b>, a series of control signals. Each control signal in the series is generated by headset <b>102</b> as the result of an actuation of a volume-control button or an answer/end-call button at the headset by its user.</p>
<p id="p-0064" num="0063">At task <b>402</b>, computer <b>101</b> checks whether the series of control signals received matches a predetermined combination of actuations that corresponds to an authentication of the user at computer <b>101</b>. For example, each user within an enterprise system can be assigned his or her own unique combination of actuations ahead of time, essentially to serve as a pass phrase.</p>
<p id="p-0065" num="0064">If the series matches, task execution proceeds to task <b>403</b>. Otherwise, task execution proceeds to task <b>404</b>.</p>
<p id="p-0066" num="0065">At task <b>403</b>, computer <b>101</b> allows the user of headset <b>102</b> subsequent access to the softphone terminal and/or to other functionality that includes access to the speech-recognition function and voice command capability. Task execution then proceeds to task <b>307</b>.</p>
<p id="p-0067" num="0066">At task <b>404</b>, computer <b>101</b> performs the function that corresponds to the series of control signals received, in the event that the series does not constitute an authentication of the particular user on the particular softphone terminal. Task execution then proceeds to task <b>401</b> in order to accept possible additional control signals.</p>
<p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. 5</figref> depicts a flowchart of the salient tasks associated with the performance of task <b>307</b>. At task <b>501</b>, computer <b>101</b> receives, from headset <b>102</b>, a voice signal that conveys an utterance made by the user. In order to keep the far-end party from hearing the utterance, in some embodiments, computer <b>101</b> has already stopped transmitting the voice signals from headset <b>102</b> to the far-end party (i.e., through telephone <b>103</b>, if involved in the call). If computer <b>101</b> has stopped the transmitting of voice signals, it has done so either as the result of i) the control signal (corresponding to the &#x201c;answer/end-call&#x201d; button) having been received at task <b>305</b> and subsequently performing the speech recognition at task <b>307</b> or ii) the authentication of the user having occurred at task <b>306</b>.</p>
<p id="p-0069" num="0068">At task <b>502</b>, computer <b>101</b> invokes a speech-recognition function on the utterance received at task <b>501</b>. In some embodiments, the speech-recognition function works on demand and only in response to having received the control signal from headset <b>102</b> at task <b>305</b>, in contrast to the speech-recognition function being always on.</p>
<p id="p-0070" num="0069">At task <b>503</b>, computer <b>101</b> checks whether the utterance is recognized as corresponding to a command to retrieve at least one entry in a database stored at telephone <b>103</b>. For example, the entry to be retrieved can be from a call log database or from a phone book database. If the utterance is recognized as such, task execution proceeds to task <b>505</b>.</p>
<p id="p-0071" num="0070">At task <b>504</b>, computer <b>101</b> performs the recognized command, in the event that the command is not one to retrieve the entry in the database.</p>
<p id="p-0072" num="0071">Depending on the call-processing state of the call, recognizable commands include, but are not limited to, the following phrases or equivalent phrases:
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0072">i. &#x201c;show options&#x201d;&#x2014;when received in any call-processing state,</li>
        <li id="ul0002-0002" num="0073">ii. &#x201c;call name/number&#x201d;; &#x201c;redial&#x201d;; &#x201c;show missed [or received or dialed] calls&#x201d;&#x2014;when received before receiving incoming call notification (i.e., when not in a call),</li>
        <li id="ul0002-0003" num="0074">iii. &#x201c;mute&#x201d;; &#x201c;hang up&#x201d;; &#x201c;volume up [or down]&#x201d;; &#x201c;hold&#x201d;; &#x201c;transfer&#x201d;&#x2014;when received during a call after the call has been answered.
<br/>
In some embodiments, computer <b>101</b> also recognizes the phrases &#x201c;answer&#x201d; and &#x201c;ignore&#x201d; when a call is incoming, but before the call has been answered, instead of regarding the control signal itself as the answer call indication.
</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0073" num="0075">At task <b>505</b>, computer <b>101</b> transmits the command to telephone <b>103</b> via the PAN. In accordance with the illustrative embodiment, computer <b>101</b> transmits the command by using one or more AT commands, as are well-known in the art. As those who are skilled in the art will appreciate, however, other command communications protocols can be used instead.</p>
<p id="p-0074" num="0076">At task <b>506</b>, computer <b>101</b> receives a message that conveys the entry being retrieved.</p>
<p id="p-0075" num="0077">At task <b>507</b>, computer <b>101</b> displays the received entry on display <b>206</b>.</p>
<p id="p-0076" num="0078">At task <b>508</b>, computer <b>101</b> stores the retrieved entry in memory <b>204</b>.</p>
<p id="p-0077" num="0079">At task <b>509</b>, computer <b>101</b> determines a party to be called, based on the received entry, and places a telephone call to the party to be called, via the softphone application.</p>
<p id="p-0078" num="0080">It is to be understood that the disclosure teaches just one example of the illustrative embodiment and that many variations of the invention can easily be devised by those skilled in the art after reading this disclosure and that the scope of the present invention is to be determined by the following claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method comprising:
<claim-text>establishing via a personal area network i) a first connection between a first telecommunications device and a headset device, wherein the first connection is configured to support telephony, and ii) a second connection between the first telecommunications device and a second telecommunications device;</claim-text>
<claim-text>receiving, at the first telecommunications device via the personal area network, an indication that the second telecommunications device is handling a first telephone call;</claim-text>
<claim-text>receiving, at the first telecommunications device from the headset device, i) a control signal generated by the headset device as the result of an actuation of an answer/end-call button at the headset device, followed by ii) a voice signal that conveys an utterance made by a user of the headset device, wherein the utterance is a command directed at the second telecommunications device, and wherein the voice signal is conveyed via a pairing of the headset device to the first telecommunications device then via a pairing of the first telecommunications device to the second telecommunications device;</claim-text>
<claim-text>invoking, at the first telecommunications device, a speech-recognition function to recognize the command in the utterance received, based on the receiving of i) the indication and ii) the control signal; and</claim-text>
<claim-text>transmitting the command by the first telecommunications device to the second telecommunications device via the personal area network.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the first telecommunications device communicates with the headset device and with the second telecommunications device through the personal area network, in accordance with the Bluetooth specification.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the first telecommunications device is a personal computer and the second telecommunications device is a cellular telephone.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref> wherein the indication signifies that the first telephone call has been answered.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref> further comprising:
<claim-text>recognizing the utterance as a command to retrieve at least one entry in a database stored at the second telecommunications device; and</claim-text>
<claim-text>receiving, at the first telecommunications device, a message that conveys the at least one entry.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref> wherein the database is that of a call log.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref> wherein the database is that of a phone book.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref> wherein the transmitting of the command to the second telecommunications device occurs by using one or more AT commands.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref> further comprising executing a softphone application at the first telecommunications device.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref> further comprising:
<claim-text>determining a party to be called, based on the at least one entry; and</claim-text>
<claim-text>placing a second telephone call from the first telecommunications device to the party to be called, via the softphone application.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. A method comprising:
<claim-text>establishing a first Bluetooth connection between a first telecommunications device and a headset device, the first telecommunications device being capable of telephony and the first Bluetooth connection being capable of telephony;</claim-text>
<claim-text>establishing a second Bluetooth connection between the first telecommunications device and a cellular telephone;</claim-text>
<claim-text>receiving, at the first telecommunications device:</claim-text>
<claim-text>i) a control signal from the headset device, wherein the control signal is generated by the headset device as the result of an actuation of an answer/end-call button at the headset device,</claim-text>
<claim-text>ii) an indication that a call that is incoming at the cellular telephone, and</claim-text>
<claim-text>iii) a voice signal from the headset device, wherein the voice signal conveys an utterance made by a user of the headset device, wherein the utterance is a command directed at the cellular telephone, and wherein the voice signal is conveyed via a pairing of the headset device to the first telecommunications device then via a pairing of the first telecommunications device to the cellular telephone;</claim-text>
<claim-text>interpreting, at the first telecommunications device, the receiving of the control signal as being one of a plurality of possible interpretations that includes i) a stimulus to answer the call and ii) a stimulus to invoke a speech-recognition function to recognize the command in the utterance received, the particular interpretation being based on the call-processing state of the call at the time that the control signal is received; and</claim-text>
<claim-text>transmitting the command to the cellular phone from the first telecommunications device via the second Bluetooth connection.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref> wherein the first telecommunications device is a cellular telephone.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref> further comprising executing a softphone application at the first telecommunications device.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref> wherein the command is for retrieving at least one entry in a first database stored at the cellular phone, and further comprising:
<claim-text>receiving, at the first telecommunications device, a message that conveys the at least one entry; and</claim-text>
<claim-text>storing the at least one entry into a second database that is accessible by the softphone application.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref> wherein the first database is that of a call log.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref> wherein the first database is that of a phone book. </claim-text>
</claim>
</claims>
</us-patent-grant>
