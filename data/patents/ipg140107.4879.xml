<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625973-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625973</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12915941</doc-number>
<date>20101029</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<us-term-of-grant>
<us-term-extension>559</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>5</main-group>
<subgroup>917</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>5</main-group>
<subgroup>91</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>386328</main-classification>
<further-classification>386326</further-classification>
</classification-national>
<invention-title id="d2e53">Method and apparatus for operating a video system</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>2009/0290855</doc-number>
<kind>A1</kind>
<name>Kowalski et al.</name>
<date>20091100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>386109</main-classification></classification-national>
</us-citation>
</us-references-cited>
<number-of-claims>18</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>348231</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>386326</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>386328</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>386329</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>386330</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>386332</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>11</number-of-drawing-sheets>
<number-of-figures>11</number-of-figures>
</figures>
<us-related-documents>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>61256463</doc-number>
<date>20091030</date>
</document-id>
</us-provisional-application>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>61256476</doc-number>
<date>20091030</date>
</document-id>
</us-provisional-application>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>61256506</doc-number>
<date>20091030</date>
</document-id>
</us-provisional-application>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>61256535</doc-number>
<date>20091030</date>
</document-id>
</us-provisional-application>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>61256553</doc-number>
<date>20091030</date>
</document-id>
</us-provisional-application>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>61256569</doc-number>
<date>20091030</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20110102634</doc-number>
<kind>A1</kind>
<date>20110505</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Pardue</last-name>
<first-name>William David</first-name>
<address>
<city>Dawsonville</city>
<state>GA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Pardue</last-name>
<first-name>William David</first-name>
<address>
<city>Dawsonville</city>
<state>GA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Meunier Carlin &#x26; Curfman</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Verint Systems, Inc.</orgname>
<role>02</role>
<address>
<city>Santa Clara</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Chevalier</last-name>
<first-name>Robert</first-name>
<department>2484</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A method for operating a video system to generate and store encoded video comprising a plurality of key frames and a plurality of delta frames is provided. The method includes capturing video data comprising a plurality of frames of a scene, and selecting a fundamental view of at least a portion of the scene contained in the plurality of frames. The method also includes generating the plurality of key frames from a first subset of the plurality of frames and the fundamental view of at least the portion of the scene, generating the plurality of delta frames from a second subset of the plurality of frames and the plurality of key frames, and transferring the encoded video for delivery to storage.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="197.70mm" wi="148.42mm" file="US08625973-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="237.24mm" wi="179.58mm" orientation="landscape" file="US08625973-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="239.52mm" wi="161.46mm" orientation="landscape" file="US08625973-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="245.45mm" wi="180.85mm" orientation="landscape" file="US08625973-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="221.40mm" wi="171.96mm" orientation="landscape" file="US08625973-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="231.99mm" wi="196.00mm" file="US08625973-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="151.89mm" wi="124.21mm" file="US08625973-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="163.41mm" wi="164.08mm" file="US08625973-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="164.76mm" wi="183.56mm" file="US08625973-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="199.98mm" wi="175.94mm" orientation="landscape" file="US08625973-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="174.92mm" wi="173.31mm" file="US08625973-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="232.33mm" wi="154.52mm" file="US08625973-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application hereby claims the benefit of and priority to U.S. Provisional Patent Application No. 61/256,463, titled &#x201c;METHOD AND APPARATUS TO LEVERAGE VIDEO ANALYSIS TO OPTIMIZE VIDEO COMPRESSION&#x201d;, filed on Oct. 30, 2009, and which is hereby incorporated by reference in its entirety.</p>
<p id="p-0003" num="0002">This application also hereby claims the benefit of and priority to U.S. Provisional Patent Application No. 61/256,476, titled &#x201c;METHOD AND APPARATUS TO ENCODE AND STORE VARIABLE RESOLUTION IMAGES PLUS METADATA&#x201d;, filed on Oct. 30, 2009, and which is hereby incorporated by reference in its entirety.</p>
<p id="p-0004" num="0003">This application also hereby claims the benefit of and priority to U.S. Provisional Patent Application No. 61/256,506, titled &#x201c;METHOD AND APPARATUS TO USE METADATA TO SUPPORT RAPID SEARCHING&#x201d;, filed on Oct. 30, 2009, and which is hereby incorporated by reference in its entirety.</p>
<p id="p-0005" num="0004">This application also hereby claims the benefit of and priority to U.S. Provisional Patent Application No. 61/256,535, titled &#x201c;METHOD AND APPARATUS TO VARY THE FRAME RATE AS NEEDED TO CAPTURE ACTIVITY&#x201d;, filed on Oct. 30, 2009, and which is hereby incorporated by reference in its entirety.</p>
<p id="p-0006" num="0005">This application also hereby claims the benefit of and priority to U.S. Provisional Patent Application No. 61/256,553, titled &#x201c;METHOD AND APPARATUS TO VARY THE KEY FRAME INTERVAL TO OPTIMIZE VIDEO STORAGE&#x201d;, filed on Oct. 30, 2009, and which is hereby incorporated by reference in its entirety.</p>
<p id="p-0007" num="0006">This application also hereby claims the benefit of and priority to U.S. Provisional Patent Application No. 61/256,569, titled &#x201c;METHOD AND APPARATUS TO STORE A MASTER REFERENCE FRAME IN EACH FILE&#x201d;, filed on Oct. 30, 2009, and which is hereby incorporated by reference in its entirety.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">TECHNICAL BACKGROUND</heading>
<p id="p-0008" num="0007">Camera manufacturers have begun offering digital cameras in a wide variety of resolutions ranging up to several megapixels for video recording. These high resolution cameras offer the opportunity to capture increased detail, but potentially at a greatly increased cost in terms of central processing unit (CPU) power, bandwidth, and storage required for high resolution images</p>
<p id="p-0009" num="0008">In order to build the most cost effective solutions for video applications, system designers must leverage available technology to capture and store optimal video evidence as opposed to simply recording video. In the past, video analysis algorithms, video compression algorithms, and video storage methods have all been designed and developed independently.</p>
<heading id="h-0003" level="1">OVERVIEW</heading>
<p id="p-0010" num="0009">A method for operating a video system to generate and store encoded video comprising a plurality of key frames and a plurality of delta frames is provided. The method includes capturing video data comprising a plurality of frames of a scene, and selecting a fundamental view of at least a portion of the scene contained in the plurality of frames. The method also includes generating the plurality of key frames from a first subset of the plurality of frames and the fundamental view of at least the portion of the scene, generating the plurality of delta frames from a second subset of the plurality of frames and the plurality of key frames, and transferring the encoded video for delivery to storage.</p>
<p id="p-0011" num="0010">A video system to generate and store encoded video comprising a plurality of key frames and a plurality of delta frames is provided. The video system includes a video capture device configured to capture video data comprising a plurality of frames of a scene, a memory configured to store the encoded video, and a video processor coupled with the video capture device and the memory. The video processor is configured to select a fundamental view of at least a portion of the scene contained in the plurality of frames, and generate the plurality of key frames from a first subset of the plurality of frames and the fundamental view of at least the portion of the scene. The video processor is also configured to generate the plurality of delta frames from a second subset of the plurality of frames and the plurality of key frames, and transfer the encoded video to the memory.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 1</figref> illustrates a block diagram of an example of a video system.</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 2</figref> illustrates a block diagram of an example of a video source.</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 3</figref> illustrates a block diagram of an example of a video processing system.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 4</figref> illustrates a block diagram of an example of a video system.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 5</figref> illustrates an example of a plurality of frames of a scene.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 6</figref> illustrates an example fundamental view of a scene.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 7</figref> illustrates a diagram of memory containing example encoded video data.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 8</figref> illustrates a diagram of memory containing example encoded video data.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 9</figref> illustrates an example view of a scene including a plurality of frames.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 10</figref> illustrates a diagram of memory containing example encoded video data.</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 11</figref> illustrates a flow chart of a method of encoding video data.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0023" num="0022">The following description and associated drawings teach the best mode of the invention. For the purpose of teaching inventive principles, some conventional aspects of the best mode may be simplified or omitted. The following claims specify the scope of the invention. Some aspects of the best mode may not fall within the scope of the invention as specified by the claims. Thus, those skilled in the art will appreciate variations from the best mode that fall within the scope of the invention. Those skilled in the art will appreciate that the features described below can be combined in various ways to form multiple variations of the invention. As a result, the invention is not limited to the specific examples described below, but only by claims and their equivalents.</p>
<p id="p-0024" num="0023">For fixed cameras, such as that illustrated in <figref idref="DRAWINGS">FIG. 2</figref>, viewing a scene with consistent lighting, there may be a great deal of redundancy between key frames. Many of the macroblocks (or portions of the scene) in each key frame will depict unchanging portions of the camera view. Repeatedly encoding those fixed portions of the scene wastes space. However, it is necessary to periodically encode a key frame in order to support efficient random access to any video image.</p>
<p id="p-0025" num="0024">To reduce the video redundancy for fixed cameras, a possible approach would be as follows:
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0025">Identify the unchanging portions of the camera view. Save macroblocks (or portions of the scene) from one or more images to build up a reference image (or fundamental view). Each macroblock within the reference image (or fundamental view) should be the most common view of that portion of the picture.</li>
        <li id="ul0002-0002" num="0026">Each video file will start with a master reference image (fundamental view) that uses intra-encoding. This master reference image (fundamental view) is never displayed&#x2014;it only serves as raw material for encoding other images.</li>
        <li id="ul0002-0003" num="0027">The frame at the beginning of each group of pictures (GOP) will be encoded using inter-encoding from the reference frame instead of the typical intra-encoding used for key frames. That is, calculate and store the differences between the reference image and the current image in exactly the same way a delta frame would be encoded. If there are a lot of closely matching macroblocks between the reference image and the image being encoded, then the resulting compressed image should be significantly smaller than a key frame.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0026" num="0028">For any macroblock, the encoder always has the choice of using intra-encoding if the encoder is unable to find a suitable motion vector. Therefore, encoding the initial frame of a GOP using this approach should not create results that are worse than standard key frame encoding. Some macroblocks within the reference image could be flagged to indicate that the encoder should always use intra-encoding for that macroblock. For example, macroblocks in regions of the camera view that have constant activity might use that flag.</p>
<p id="p-0027" num="0029">To improve encoding efficiency for the first frame of each GOP, the encoder could bypass motion vector searching and use the following simpler method. Encode each macroblock using two approaches and retain the best (smallest) of the two encoded results:
<ul id="ul0003" list-style="none">
    <li id="ul0003-0001" num="0000">
    <ul id="ul0004" list-style="none">
        <li id="ul0004-0001" num="0030">1. Intra-encoding (the method normally used for all macroblocks in a key frame)</li>
        <li id="ul0004-0002" num="0031">2. Inter-encoding relative to the master reference image, where the only motion vector evaluated is 0, 0 (no horizontal or vertical displacement).</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0028" num="0032">If there is no significant change between a macroblock in the reference image and the macroblock being encoded, the encoder can skip the macroblock entirely. During decoding, when a macroblock is skipped in the encoded image the decoder would simply copy the macroblock from the master reference image.</p>
<p id="p-0029" num="0033">Disclosed herein is a video system. Video system <b>100</b> includes video source <b>102</b>, video processing system <b>104</b>, and video storage system <b>106</b>. Video source <b>102</b> is coupled to video processing system <b>104</b>, and video processing system <b>104</b> is coupled to video storage system <b>106</b>. The connections between the elements of video system <b>100</b> may use various communication media, such as air, metal, optical fiber, or some other signal propagation path&#x2014;including combinations thereof. They may be direct links, or they might include various intermediate components, systems, and networks.</p>
<p id="p-0030" num="0034">In some embodiments, a large number of video sources may all communicate with video processing system <b>104</b>, this results in bandwidth concerns as video processing system <b>104</b> may have an input port incapable of receiving full resolution, real time video from all of the video sources. In such a case, it is desirable to incorporate some video processing functionality within each of the video sources such that the bandwidth requirements between the various video sources and video processing system <b>104</b> are reduced. An example of such a video source is illustrated in <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0031" num="0035">In this example embodiment, video source <b>102</b> captures video data comprising a plurality of frames of a scene and transfers the video data to video processing system <b>104</b>. Video processing system <b>104</b> selects a fundamental view of at least a portion of the scene contained in the plurality of frames. This portion of the scene may include one or more macroblocks of the scene. A portion of the scene may be selected for inclusion in the fundamental view based on the most common content of the portion of the scene as illustrated in <figref idref="DRAWINGS">FIGS. 5 and 6</figref>.</p>
<p id="p-0032" num="0036">Video processing system <b>104</b> then generates a plurality of key frames from a first subset of the plurality of frames and the fundamental view of at least a portion of the scene. Video processing system <b>104</b> then generates a plurality of delta frames from a second subset of the plurality of frames and the plurality of key frames. For example, video processing system <b>104</b> may break the incoming video data into a plurality of groups of pictures (or groups of frames). The first frame of each group of frames may be selected for encoding as a key frame, while the remaining frames of each group of frames may be selected for encoding as delta frames.</p>
<p id="p-0033" num="0037">Key frames are encoded based on the fundamental view and portions of the key frame substantially similar to corresponding portions of the fundamental view may be encoded as pointers to the particular portion of the fundamental view instead of intra-encoding the portion. Thus, key frames which reference portions of the fundamental view will be smaller in memory size than key frames which include intra-encoding of the entire frame.</p>
<p id="p-0034" num="0038">For example, in a scene having a background with a number of people walking past, each portion of the scene would have a lot of frames capturing the background a few frames capturing people. The most common content of each portion of the scene would then most likely contain just the background and no people.</p>
<p id="p-0035" num="0039">Video processing system <b>104</b> generates a fundamental view from the most common content captured in each portion of the scene. In this example, the frames capturing the background would be much more common than the frames capturing people, and the fundamental view would contain the background without any people.</p>
<p id="p-0036" num="0040"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram of an example of a video source <b>200</b>, such as video source <b>102</b> from <figref idref="DRAWINGS">FIG. 1</figref>. Video source <b>200</b> includes lens <b>202</b>, sensor <b>204</b>, processor <b>206</b>, memory <b>208</b>, and communication interface <b>210</b>. Lens <b>202</b> is configured to focus an image of a scene on sensor <b>204</b>. Lens <b>202</b> may be any type of lens, pinhole, zone plate, or the like able to focus an image on sensor <b>204</b>. Sensor <b>204</b> then digitally captures video of the scene and passes the video images to processor <b>206</b>. Processor <b>206</b> is configured to store some or all of the video in memory <b>208</b>, process the video, and send the processed video to external devices <b>212</b> through communication interface <b>210</b>. In some examples, external devices <b>212</b> include video processing system <b>104</b> and video storage system <b>106</b>.</p>
<p id="p-0037" num="0041">In this example embodiment, video source <b>200</b> captures video data comprising a plurality of frames of a scene. Lens <b>202</b> and sensor <b>204</b> capture the video data and transfer the video data to processor <b>206</b>. Processor <b>206</b> selects a fundamental view of at least a portion of the scene contained in the plurality of frames. Processor <b>206</b> generates a plurality of key frames from a first subset of the plurality of frames and the fundamental view of at least a portion of the scene. Processor <b>206</b> also generates a plurality of delta frames from a second subset of the plurality of frames, the plurality of key frames, and preceding delta frames. For example, in a scene having a background with a number of people walking past, each portion of the scene would have a lot of frames capturing the background a few frames capturing people.</p>
<p id="p-0038" num="0042">Processor <b>206</b> then generates a fundamental view from the most common content captured in each portion of the scene. Other examples may use criteria other than the most common content for determining which portions of the scene to include in the fundamental view. For example, some embodiments may determine how long a scene has been unchanged. In this example, the portions of the scene capturing the background would be much more common than the frames capturing people, and the fundamental view would contain the background without any people.</p>
<p id="p-0039" num="0043">Many embodiments include a video processing system such as video processing system <b>104</b> from <figref idref="DRAWINGS">FIG. 1</figref>, processor <b>206</b> from <figref idref="DRAWINGS">FIG. 2</figref> and video processing system <b>410</b> from <figref idref="DRAWINGS">FIG. 4</figref>. Any of these video processing systems may be implemented on a computer system such as that shown in <figref idref="DRAWINGS">FIG. 3</figref>. Video processing system <b>300</b> includes communication interface <b>311</b>, and processing system <b>301</b>. Processing system <b>301</b> is linked to communication interface <b>311</b> through a bus. Processing system <b>301</b> includes <b>302</b> and memory devices <b>303</b> that store operating software.</p>
<p id="p-0040" num="0044">Communication interface <b>311</b> includes network interface <b>312</b>, input ports <b>313</b>, and output ports <b>314</b>. Communication interface <b>311</b> includes components that communicate over communication links, such as network cards, ports, RF transceivers, processing circuitry and software, or some other communication devices. Communication interface <b>311</b> may be configured to communicate over metallic, wireless, or optical links. Communication interface <b>311</b> may be configured to use TDM, IP, Ethernet, optical networking, wireless protocols, communication signaling, or some other communication format&#x2014;including combinations thereof.</p>
<p id="p-0041" num="0045">Network interface <b>312</b> is configured to connect to external devices over network <b>315</b>. In some examples these network devices may include video sources and video storage systems as illustrated in <figref idref="DRAWINGS">FIGS. 1 and 4</figref>. Input ports <b>313</b> are configured to connect to input devices <b>316</b> such as a keyboard, mouse, or other user input devices. Output ports <b>314</b> are configured to connect to output devices <b>317</b> such as a display, a printer, or other output devices.</p>
<p id="p-0042" num="0046">Processor <b>302</b> includes microprocessor and other circuitry that retrieves and executes operating software from memory devices <b>303</b>. Memory devices <b>303</b> include random access memory (RAM) <b>304</b>, read only memory (ROM) <b>305</b>, a hard drive <b>306</b>, and any other memory apparatus. Operating software includes computer programs, firmware, or some other form of machine-readable processing instructions. In this example, operating software includes operating system <b>307</b>, applications <b>308</b>, modules <b>309</b>, and data <b>310</b>. Operating software may include other software or data as required by any specific embodiment. When executed by processor <b>302</b>, operating software directs processing system <b>301</b> to operate video processing system <b>300</b> as described herein.</p>
<p id="p-0043" num="0047">In this example embodiment, one or more video sources (input devices <b>316</b>) capture video data comprising a plurality of frames of a scene. This video data is transferred to processing system <b>301</b> through input ports <b>313</b> and communication interface <b>311</b>. Processor <b>302</b> selects a fundamental view of at least a portion of the scene contained in the plurality of frames. Processor <b>302</b> generates a plurality of key frames from a first subset of the plurality of frames and the fundamental view of at least a portion of the scene. Processor <b>302</b> also generates a plurality of delta frames from a second subset of the plurality of frames, the plurality of key frames, and preceding delta frames. For example, in a scene having a background with a number of people walking past, each portion of the scene would have a lot of frames capturing the background a few frames capturing people.</p>
<p id="p-0044" num="0048">Processor <b>302</b> then generates a fundamental view from the most common content (or other criteria) captured in each macroblock. In this example, the frames capturing the background would be much more common than the frames capturing people, and the fundamental view would contain the background without any people.</p>
<p id="p-0045" num="0049"><figref idref="DRAWINGS">FIG. 4</figref> illustrates a block diagram of an example of a video system <b>400</b>. Video system <b>400</b> includes video source <b>1</b> <b>406</b>, video source N <b>408</b>, video processing system <b>410</b>, and video storage system <b>412</b>. Video source <b>1</b> <b>406</b> is configured to capture video of scene <b>1</b> <b>402</b>, while video source N <b>408</b> is configured to capture video of scene N <b>404</b>. Video source <b>1</b> <b>406</b> and video source N <b>408</b> are coupled to video processing system <b>410</b>, and video processing system <b>410</b> is coupled to video storage system <b>412</b>. The connections between the elements of video system <b>400</b> may use various communication media, such as air, metal, optical fiber, or some other signal propagation path&#x2014;including combinations thereof. They may be direct links, or they might include various intermediate components, systems, and networks.</p>
<p id="p-0046" num="0050">In some embodiments, a large number of video sources may all communicate with video processing system <b>410</b>, this results in bandwidth concerns as video processing system <b>410</b> may have an input port incapable of receiving full resolution, real time video from all of the video sources. In such a case, it is desirable to incorporate some video processing functionality within each of the video sources such that the bandwidth requirements between the various video sources and video processing system <b>410</b> are reduced. An example of such a video source is illustrated in <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0047" num="0051">In some embodiments including a large number of video sources it may be advantageous to configure the video sources to create fundamental views for the video data that they capture. In other embodiments, the video sources may transfer the raw video data to video processing system <b>410</b> for creating fundamental views for the video data and storage in video storage <b>412</b>.</p>
<p id="p-0048" num="0052"><figref idref="DRAWINGS">FIG. 5</figref> illustrates an example of a plurality of frames of a scene. In this example embodiment, four frames <b>500</b>, <b>502</b>, <b>504</b>, and <b>506</b> of a scene are illustrated in <figref idref="DRAWINGS">FIGS. 5</figref> (<i>a</i>), (<i>b</i>), (<i>c</i>), and (<i>d</i>). The scene is divided into four portions (or macroblocks). The upper left portion of each frame contains an unchanging view of a rectangular block. Thus, the most common view of the upper left portion of the scene is the view of the rectangular block.</p>
<p id="p-0049" num="0053">The upper right portion of each frame contains a changing view of a cylinder. Frames <b>500</b> and <b>502</b> contain identical views of the cylinder, while frames <b>504</b> and <b>506</b> contain different views of the cylinder. Thus, frames <b>500</b> and <b>502</b> contain the most common view of the cylinder in the upper right portion of the scene.</p>
<p id="p-0050" num="0054">The lower right portion of each frame contains a changing view of a rectangular box. Frames <b>500</b>, <b>504</b>, and <b>506</b> contain identical views of the rectangular box, while frame <b>502</b> contains an end view of the rectangular box. Thus frames <b>500</b>, <b>504</b>, and <b>506</b> contain the most common view of the rectangular box in the lower right portion of the scene.</p>
<p id="p-0051" num="0055">The lower left portion of each frame contains a changing view of an L-shaped rod. Frames <b>500</b>, <b>502</b>, <b>504</b>, and <b>506</b> each contain a different view of the L-shaped rod. Thus, in this portion of the scene there is no most common view of the L-shaped rod in the lower left portion of the scene.</p>
<p id="p-0052" num="0056"><figref idref="DRAWINGS">FIG. 6</figref> illustrates an example fundamental view of the scene from <figref idref="DRAWINGS">FIG. 5</figref>. In this example, a fundamental view <b>600</b> is assembled from the most common views of the portions of the scene illustrated in <figref idref="DRAWINGS">FIG. 5</figref>. The upper left portion of the fundamental view <b>600</b> contains a view of the rectangular box as captured in any of frames <b>500</b>, <b>502</b>, <b>504</b>, or <b>506</b>. The upper right portion of the fundamental view <b>600</b> contains a view of the cylinder as captured in either of frames <b>500</b> or <b>502</b>. The lower right portion of the fundamental view <b>600</b> contains a view of the rectangular box as captured in any of frames <b>500</b>, <b>504</b>, or <b>506</b>. Since there was no most common view of the lower left portion of the scene this portion is flagged in the fundamental view <b>600</b>.</p>
<p id="p-0053" num="0057">When key frames are encoded based upon fundamental view <b>600</b>, the upper left, upper right, and lower right portions of each key frame will contain any differences between the key frame portions and the corresponding portions of fundamental view <b>600</b>. The lower left portion of each key frame will be intra-encoded since there is no most common view of this portion of the scene. Other embodiments may use other criteria for deciding which frames to use in constructing fundamental view <b>600</b>.</p>
<p id="p-0054" num="0058"><figref idref="DRAWINGS">FIG. 7</figref> illustrates a diagram of memory containing example encoded video data. This example memory map <b>700</b> illustrates one possible method of encoding video data using a fundamental view <b>702</b>. In this example, fundamental view <b>702</b> is stored in a file in the memory and key frames <b>704</b> with respect to fundamental view <b>702</b> follow in the file. Here no delta frames are generated, and each frame of the video data is inter-encoded with respect to fundamental view <b>702</b> and stored in the file following fundamental view <b>702</b>.</p>
<p id="p-0055" num="0059"><figref idref="DRAWINGS">FIG. 8</figref> illustrates a diagram of memory containing example encoded video data. This example memory map <b>800</b> illustrates another possible method of encoding video data using a fundamental view <b>702</b>, key frames <b>804</b>, <b>806</b>, <b>808</b>, and <b>810</b>, and a plurality of delta frames. In this simple example, the video data is divided into four groups of pictures <b>812</b>, <b>814</b>, <b>816</b>, and <b>818</b>. Each group of pictures includes a key frame and four delta frames.</p>
<p id="p-0056" num="0060">For example, the first group of pictures <b>812</b> includes key frame key<sub>1 </sub><b>804</b> and delta frames &#x394;<sub>1</sub><sub><sub2>&#x2014;</sub2></sub><sub>1</sub>, &#x394;<sub>1</sub><sub><sub2>&#x2014;</sub2></sub><sub>2</sub>, &#x394;<sub>1</sub><sub><sub2>&#x2014;</sub2></sub><sub>3</sub>, and &#x394;<sub>1</sub><sub><sub2>&#x2014;</sub2></sub><sub>4</sub>. Likewise, the second group of pictures <b>814</b> includes key frame key<sub>2 </sub><b>806</b> and delta frames &#x394;<sub>2</sub><sub><sub2>&#x2014;</sub2></sub><sub>1</sub>, &#x394;<sub>2</sub><sub><sub2>&#x2014;</sub2></sub><sub>2</sub>, &#x394;<sub>2</sub><sub><sub2>&#x2014;</sub2></sub><sub>3</sub>, and &#x394;<sub>2</sub><sub><sub2>&#x2014;</sub2></sub><sub>4</sub>. Likewise, the third group of pictures <b>816</b> includes key frame key<sub>3 </sub><b>808</b> and delta frames &#x394;<sub>3</sub><sub><sub2>&#x2014;</sub2></sub><sub>1</sub>, &#x394;<sub>3</sub><sub><sub2>&#x2014;</sub2></sub><sub>2</sub>, &#x394;<sub>3</sub><sub><sub2>&#x2014;</sub2></sub><sub>3</sub>, and &#x394;<sub>3</sub><sub><sub2>&#x2014;</sub2></sub><sub>4</sub>. Likewise, the fourth group of pictures <b>818</b> includes key frame key<sub>4 </sub><b>810</b> and delta frames &#x394;<sub>4</sub><sub><sub2>&#x2014;</sub2></sub><sub>1</sub>, &#x394;<sub>4</sub><sub><sub2>&#x2014;</sub2></sub><sub>2</sub>, &#x394;<sub>4</sub><sub><sub2>&#x2014;</sub2></sub><sub>3</sub>, and &#x394;<sub>4</sub><sub><sub2>&#x2014;</sub2></sub><sub>4</sub>. Each key frame is inter-encoded with respect to fundamental view <b>802</b>, and each delta frame is inter-encoded with respect to its corresponding key frame and preceding delta frames.</p>
<p id="p-0057" num="0061"><figref idref="DRAWINGS">FIG. 9</figref> illustrates an example view of a scene including a plurality of frames. In some embodiments, a video capture device may not be stationary, but may pan over a scene. Typically these pans are continuously repeated and each pan generates a plurality of different views of the scene. In the very simple example illustrated in <figref idref="DRAWINGS">FIG. 9</figref>, a video capture device repeatedly pans scene <b>900</b> taking six different views of scene <b>900</b> during each pan cycle.</p>
<p id="p-0058" num="0062">In this example, scene <b>900</b> is covered by views <b>902</b>, <b>904</b>, <b>906</b>, <b>908</b>, <b>910</b>, and <b>910</b>. Each of these views is further divided into four portions. Since the panning motion of the video capture device is repeatable, a fundamental view may be constructed for each of views <b>902</b>, <b>904</b>, <b>906</b>, <b>908</b>, <b>910</b>, and <b>910</b> using the techniques illustrated in <figref idref="DRAWINGS">FIGS. 5 and 6</figref>.</p>
<p id="p-0059" num="0063"><figref idref="DRAWINGS">FIG. 10</figref> illustrates a diagram of memory containing example encoded video data. This example memory map <b>1000</b> illustrates one possible method of encoding video data received from a video capture device panning over scene <b>900</b> from <figref idref="DRAWINGS">FIG. 9</figref>. Fundamental view FV<sub>1 </sub>is generated for upper left view <b>902</b>, fundamental view FV<sub>2 </sub>is generated for upper middle view <b>904</b>, fundamental view FV<sub>3 </sub>is generated for upper right view <b>906</b>, fundamental view FV<sub>4 </sub>is generated for lower left view <b>908</b>, fundamental view FV<sub>5 </sub>is generated for lower middle view <b>910</b>, and fundamental view FV<sub>6 </sub>is generated for lower right view <b>912</b>.</p>
<p id="p-0060" num="0064">Following the fundamental views <b>1002</b> in memory map <b>1000</b> are the first six key frames K<sub>1</sub>, K<sub>2</sub>, K<sub>3</sub>, K<sub>4</sub>, K<sub>5</sub>, and K<sub>6 </sub><b>1004</b> corresponding to each of the views. Following the key frames <b>1004</b> in memory map <b>1000</b> are the delta frames <b>1006</b>, <b>1008</b>, and <b>1010</b> associated with each of the key frames. Thus, multiple fundamental views may be used to encode regularly repeating panning video into a single file.</p>
<p id="p-0061" num="0065"><figref idref="DRAWINGS">FIG. 11</figref> illustrates a flow chart of a method of encoding video data. In this example method to generate and store encoded video comprising a plurality of key frames and a plurality of delta frames, video data comprising a plurality of frames of a scene is captured, (operation <b>1100</b>). A video processing system selects a fundamental view of at least a portion of the scene contained in the plurality of frames, (operation <b>1102</b>).</p>
<p id="p-0062" num="0066">The video processing system generates a plurality of key frames from a first subset of the plurality of frames and the fundamental view of at least a portion of the scene, (operation <b>1104</b>). The video processing system generates a plurality of delta frames from a second subset of the plurality of frames, the plurality of key frames, and preceding delta frames, (operation <b>1106</b>). Finally, the video processing system transfers the resulting encoded video for delivery to storage, (operation <b>1108</b>).</p>
<p id="p-0063" num="0067">Sophisticated video surveillance systems should not simply record video. Systems should be designed to gather optimal visual evidence that can be used to solve crimes or investigate incidents. Systems should use video analysis to identify specific types of activity and events that need to be recorded. The system should then tailor the recorded images to fit the activity&#x2014;providing just the right level of detail (pixels per foot) and just the right image refresh rate for just long enough to capture the video of interest. The system should minimize the amount of space that is wasted storing images that will be of little value.</p>
<p id="p-0064" num="0068">In addition to storing video images, the system should also store searchable metadata that describes the activity that was detected through video analysis. The system should enable users to leverage metadata to support rapid searching for activity that matches user-defined criteria without having to wait while the system decodes and analyzes images. All images should be analyzed one time when the images are originally captured (before compression) and the results of that analysis should be saved as searchable metadata.</p>
<p id="p-0065" num="0069">Modern video compression methods like H.264 involve detecting changes between images and then determining the optimal way to encode those changes. A large image is divided into a grid of small images called macroblocks, where a typical macroblock size is 8&#xd7;8 pixels. When the pixels of a macroblock have changed from one image to the next, the compression algorithm can choose between two methods of encoding the macroblock:
<ul id="ul0005" list-style="none">
    <li id="ul0005-0001" num="0000">
    <ul id="ul0006" list-style="none">
        <li id="ul0006-0001" num="0070">Inter-encoding means the compression algorithm finds a macroblock in the previous image (the reference image) that closely matches the macroblock to be encoded. The encoding for the new macroblock consists of a motion vector (which identifies the location of the referenced macroblock in the reference image) and a compressed representation of the difference between the encoded macroblock and the referenced macroblock.</li>
        <li id="ul0006-0002" num="0071">Intra-encoding means the macroblock is encoded without reference to any other macroblock. This approach can be used if the algorithm is unable to identify a motion vector that provides more optimal encoding.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0066" num="0072">The main characteristic that distinguishes a more effective compression algorithm from a less effective one is the degree to which the algorithm always finds the optimal motion vector for each macroblock to be encoded. If the amount of time and CPU power available for searching is unlimited, then the encoder can simply perform an exhaustive evaluation of all macroblocks in the reference image in order to identify the motion vector that will produce the smallest encoding. More advanced algorithms can take it one step further by using multiple reference images before and sometimes even after the image being encoded. More advanced algorithms can also select fractional pixel displacements for motion vectors (because moving objects won't always exactly align with macroblock boundaries).</p>
<p id="p-0067" num="0073">Using reference images after the image being encoded would not be an option if latency must be minimized when encoding live video images&#x2014;for live video, the current image must be completely encoded and transmitted before the next image is captured.</p>
<p id="p-0068" num="0074">In general, the CPU power currently available in digital cameras for surveillance and other applications is not sufficient for the most advanced video encoding methods. The camera must use a highly optimized method to locate acceptable motion vectors quickly. Even within cameras that implement the same encoding standard (like H.264), the quality of the implementation can vary significantly&#x2014;some algorithms find more optimal motion vectors more quickly than other algorithms.</p>
<p id="p-0069" num="0075">For any macroblock, the ideal motion vector would be one that identifies the location of an identical macroblock in the reference image. That can occur when an object has moved within the camera view but otherwise the pixels representing the object have not changed. In general, if you know that an object is moving within the camera view, then the macroblocks that made up that same object in the reference image should be among the most promising motion vectors.</p>
<p id="p-0070" num="0076">For video surveillance, one is normally interested in moving objects&#x2014;in particular we are interested in people and their activity. One is also interested in moving vehicles (because they are operated by people). Video analysis algorithms attempt to make sense out of the pixels in an image. These algorithms separate pixels into distinct objects (people, vehicles, etc.) and track the movement, behaviors, and interaction of objects.</p>
<p id="p-0071" num="0077">It seems clear that if a video analysis algorithm has identified distinct objects and the movement of those objects from image to image, then that information should provide an ideal basis for selecting optimal motion vectors for image encoding.</p>
<p id="p-0072" num="0078">In an example embodiment, a method for operating a video source is provided. The method includes capturing first video data of a scene including a moving object, and processing the first video data to determine a motion vector for the moving object. The method also includes capturing second video data of the scene including the moving object, and compressing the second video data using the motion vector producing compressed second video data.</p>
<p id="p-0073" num="0079">Within a particular high resolution image, portions of the image may contain information that is relevant to surveillance (moving people and vehicles) while the majority of the image may depict relatively static portions of the scene that closely match thousands of previous images from the same camera.</p>
<p id="p-0074" num="0080">One way to handle a fixed surveillance camera, such as that illustrated in <figref idref="DRAWINGS">FIG. 2</figref>, would be to store a complete image at maximum resolution only at a very long interval&#x2014;maybe once every few hours for example&#x2014;to capture the details of the static objects within the scene. These images could be stored separately and could be made available for reference when an operator is reviewing recorded video from the camera. Except for these reference images, all other images would use the following technique:
<ul id="ul0007" list-style="none">
    <li id="ul0007-0001" num="0000">
    <ul id="ul0008" list-style="none">
        <li id="ul0008-0001" num="0081">Capture an image at maximum resolution</li>
        <li id="ul0008-0002" num="0082">Identify the regions of the image that contain objects of interest where maximum resolution may provide value. Divide those regions into macroblocks and encode those macroblocks at full resolution. Macroblocks may be 8 pixels by 8 pixels in size in some examples, while other embodiments may user macroblocks of other sizes.</li>
        <li id="ul0008-0003" num="0083">Scale the image down to a lower resolution image&#x2014;e.g. one half or one quarter the size of the original. Divide the lower resolution image into macroblocks and encode those macroblocks. If a macroblock in the lower resolution image corresponds completely to macroblocks already encoded at a higher resolution, that macroblock can be excluded from the lower resolution image.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0075" num="0084">It would be possible to apply this technique repeatedly so that a single image might be encoded in three or more different resolutions as opposed to only two resolutions.</p>
<p id="p-0076" num="0085">Various law enforcement organizations including the FBI have published guidelines for the video resolution needed for specific purposes. These guidelines are expressed in terms of pixels per foot.
<ul id="ul0009" list-style="none">
    <li id="ul0009-0001" num="0000">
    <ul id="ul0010" list-style="none">
        <li id="ul0010-0001" num="0086">Overview detail is in the range of 20 to 30 pixels per foot. This is sufficient detail to track people and their movement but is not sufficient detail to recognize faces.</li>
        <li id="ul0010-0002" num="0087">Forensic detail is sufficient detail to serve as legal evidence in order to identify a particular person or read a license plate. Most sources recommend at least 40 pixels per foot to recognize faces and at least 60 pixels per foot to read license plates.</li>
        <li id="ul0010-0003" num="0088">High detail is sufficient to identify specific currency or casino chip values or small items being purchased at a point-of-sale terminal. High detail is 80 or more pixels per foot.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0077" num="0089">By combining knowledge about the role of the camera in the surveillance system with video analysis, an intelligent camera could encode images at variable resolution in order to provide the optimum resolution for the activity that is currently visible within the camera view.
<ul id="ul0011" list-style="none">
    <li id="ul0011-0001" num="0000">
    <ul id="ul0012" list-style="none">
        <li id="ul0012-0001" num="0090">If a video analysis algorithm detects that an original high resolution image includes a clear view of a person's face and the image provides at least 40 pixels per foot in the area where the face is detected, then the camera could retain full resolution for the macroblocks required to encode the image of the face. The camera could also generate metadata (where metadata is additional data stored separately from video images, describing the contents of the video images) to indicate that the image includes a face encoded with forensic detail.</li>
        <li id="ul0012-0002" num="0091">If a camera uses video analysis to identify and track individuals, then the camera could capture a limited number of images of each person's face at forensic detail. The camera could identify and eliminate duplicate images showing exactly the same viewing angle of the person's face, but could retain additional images if images are captured at different viewing angles as the person moves within the camera view.</li>
        <li id="ul0012-0003" num="0092">If a camera uses video analysis to identify license plates within the camera view, then the camera could retain an image of the license plate at forensic detail whenever that much detail becomes available. After an image has been captured at sufficient detail to read the license plate, there is no need for the camera to store additional images of the same license plate at forensic detail. Other images of the same vehicle should provide sufficient detail to see the make and model of the vehicle, the movement of the vehicle, and any visible information about the occupants of the vehicle.</li>
        <li id="ul0012-0004" num="0093">For a camera that views a point-of-sale terminal, the camera could retain high detail of items on the conveyor and of cash-handling activity when the cash drawer is open. The camera could retain forensic detail of customers, employees, and merchandise scanning and bagging activity. Since employees typically enter the scene and then remain for long periods, there would be no need for the camera to repeatedly capture forensic detail of the employee's face&#x2014;the camera should only retain enough detail to positively identify the employee when they arrive on the scene. For any uninterrupted sequence of images of the same employee, the metadata should identify the images that contain forensic detail to identify that employee. The camera would retain overview detail during periods when there is no customer present, the cash drawer is closed, and there is no activity of any known significance.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0078" num="0094">During video review, images could be displayed at low resolution initially. The low resolution macroblocks would be decoded first. Higher resolution macroblocks would be decoded and then scaled down to match the lower resolution macroblocks. The operator could select a portion of the image to be viewed in more detail. The application would determine if higher resolution macroblocks exist for that area, and would decode those macroblocks at full resolution. The application could automatically adjust the zoom level so that the selected area could be shown at full resolution&#x2014;i.e. each pixel in the decoded image would correspond to one pixel in the displayed image.</p>
<p id="p-0079" num="0095">If requested, the application could highlight the portions of the image where higher resolution is available for display. The application could leverage metadata generated and stored during video encoding to highlight objects and specific types of activity in the displayed video. The operator could keep the overview image displayed in one window and use separate windows to display regions of interest at higher resolution.</p>
<p id="p-0080" num="0096">In an example embodiment, a method for operating a video source is provided. The method includes capturing video data of a scene, and processing the video data to determine an area of interest within the scene. The method also includes dividing the video data into macroblocks, determining the identity of the macroblocks including the area of interest, and encoding the macroblocks including the area of interest at a first resolution. The method further includes encoding the remaining macroblocks at a second resolution, wherein the first resolution is greater than the second resolution.</p>
<p id="p-0081" num="0097">If a system, such as that illustrated in <figref idref="DRAWINGS">FIG. 1</figref>, uses video analysis to generate metadata during video encoding, that metadata can support rapid scanning and searching for specific types of activity. For example, an operator might want to review video of a doorway and see every person that went through that door during a period of time. If the metadata contains a history of the times at which the camera captured forensic detail of a new person arriving in the scene, then the application could leverage the metadata to quickly display images of each person. Searching metadata would be many times faster (and more efficient with system resources) than decoding and analyzing video images to detect activity. For example, what if only one person goes through that door on average in a 10 hour period? Instead of decoding and analyzing <b>10</b> hours worth of video images to find the next event of interest, the system might only need to search through a few kilobytes of metadata.</p>
<p id="p-0082" num="0098">In an example embodiment, a method for operating a video source is provided. The method includes capturing video data of a scene, detecting an event within the video data, and placing an event timestamp including the date and time of the event in metadata corresponding to the video data.</p>
<p id="p-0083" num="0099">Just like different numbers of pixels per foot are required to provide sufficient detail depending on the scene and the purpose of the camera, different numbers of images per second are required to capture activity that occurs at different speeds.</p>
<p id="p-0084" num="0100">Intelligent cameras, such as that illustrated in <figref idref="DRAWINGS">FIG. 2</figref>, should capture and analyze images at the maximum frame rate available from the camera, and then based on an analysis of the activity the camera should decide which images need to be retained. Since the camera will not store images at a consistent frame rate, each image should include its own timestamp with millisecond precision.
<ul id="ul0013" list-style="none">
    <li id="ul0013-0001" num="0000">
    <ul id="ul0014" list-style="none">
        <li id="ul0014-0001" num="0101">When there is very little activity, the camera may drop down to a very low image rate&#x2014;one frame per second or less.</li>
        <li id="ul0014-0002" num="0102">To track normal movement of people walking through the camera view, a medium frame rate like 2-5 frames per second would be sufficient.</li>
        <li id="ul0014-0003" num="0103">If a person is detected running through the camera view or if a vehicle drives through the camera view at high speed, the camera may need to retain several consecutive images at the maximum image refresh rate to capture sufficient images of the fast-moving subject.</li>
        <li id="ul0014-0004" num="0104">For a camera that views a point-of-sale terminal, conveyor belt movement, merchandise scanning, and cash drawer activity would trigger higher frame rates, then the frame rate would slow down during low activity periods between transactions.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0085" num="0105">In an example embodiment, a method for operating a video source is provided. The method includes capturing video data comprising a plurality of frames of a scene, and identifying a first frame from the plurality of frames that includes a first aspect of an activity occurring within the scene. The method also includes subsequent to identifying the first frame, identifying at least a second frame from the plurality of frames that includes a second aspect of the activity occurring within the scene, and transferring the first frame and the second frame for delivery to storage.</p>
<p id="p-0086" num="0106">Modern video compression methods like H.264 produce two types of compressed images:
<ul id="ul0015" list-style="none">
    <li id="ul0015-0001" num="0000">
    <ul id="ul0016" list-style="none">
        <li id="ul0016-0001" num="0107">I-Frames or intra-encoded frames are frames that can be decoded without referencing any other frame. I-Frames are also known as key frames.</li>
        <li id="ul0016-0002" num="0108">P-Frames or predicted frames are frames that require one or more reference frames during decoding. P-Frames are also known as delta frames because they store the differences between frames.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0087" num="0109">A Group of Pictures or GOP is a sequence of images consisting of a key frame followed by the delta frames that depend on that key frame. The number of images in a GOP is the key frame interval. For example, if the key frame interval is 32, that means one out of every 32 images is a key frame.</p>
<p id="p-0088" num="0110">It is common to use a fixed key frame interval, but an intelligent encoding algorithm could produce better results by selecting the optimum number of images to include in each GOP. When activity in the camera view is low, P-Frames will be small because very few macroblocks need to be encoded for each new image. In this situation, an intelligent camera, such as that illustrated in <figref idref="DRAWINGS">FIG. 2</figref>, can keep the video data rate as low as possible by continuing to extend the current GOP instead of encoding a key frame to start a new GOP.</p>
<p id="p-0089" num="0111">The encoder should only encode a key frame to start a new GOP when there is a reason for doing that&#x2014;the encoder should not adhere to a fixed key frame interval. Following are valid reasons for starting a new GOP:
<ul id="ul0017" list-style="none">
    <li id="ul0017-0001" num="0000">
    <ul id="ul0018" list-style="none">
        <li id="ul0018-0001" num="0112">Starting a new video file. Each video file must start with a key frame if a goal is to be able to decode all images in a video file without referencing any other video file.</li>
        <li id="ul0018-0002" num="0113">Random access performance. During video playback, long sequences of images in a GOP can decrease responsiveness when a user attempts to reposition to a particular image within a GOP. To decode any particular image, the decoder must start with the key frame at the beginning of the GOP and then decode each successive image until the target image has been decoded. The maximum time required for random access to a particular image will depend on the maximum number of macroblocks that must be decoded in order to decode the selected image. Every GOP requires decoding all of the macroblocks in the key frame. After that, the number of macroblocks to be decoded depends on the amount of activity (which determines the number of macroblocks per image) and the number of images. When there are few macroblocks per delta frame because of low activity, the GOP can be extended to a larger number of images without causing excessive random access times. Conversely, when there is very high activity it may be necessary to reduce the number of images in a GOP in order to maintain an acceptable maximum random access time.</li>
        <li id="ul0018-0003" num="0114">Accumulation of encoding errors. The encoding algorithm uses lossy compression when encoding the differences between images. Over time, the image that results from decoding successive delta frames can diverge from the image that would result from decoding a new key frame. If an intelligent camera has sufficient CPU power, it would be possible to encode a key frame for each captured image and then measure the encoding error of the image that would result from decoding the delta frames vs. the image that would result from decoding the key frame. To avoid having to transmit a new key frame, the encoder could select the macroblocks with the worst errors and encode and transmit those (using intra-encoding at the macroblock level if needed). If a large number of macroblocks have errors exceeding a threshold level, then it is time to end the current GOP and encode a new key frame.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0090" num="0115">In an example embodiment, a method for operating a video source is provided. The method includes capturing video data of a scene comprising a plurality of frames, opening a file for the video data, and storing a key frame in the file corresponding to a first of the plurality of frames. For each of the remaining plurality of frames, the method encodes the frame into macroblocks, determines a quantity of total macroblocks encoded since a last key frame was stored, and determines a quantity of macroblocks having errors since the last key frame was stored. The method also includes storing a new key frame when the quantity of total macroblocks encoded since the last key frame was stored exceeds a total encoded threshold, and storing a new key frame when the quantity of macroblocks having errors since the last key frame was stored exceeds an error threshold.</p>
<p id="p-0091" num="0116">The above description and associated figures teach the best mode of the invention. The following claims specify the scope of the invention. Note that some aspects of the best mode may not fall within the scope of the invention as specified by the claims. Those skilled in the art will appreciate that the features described above can be combined in various ways to form multiple variations of the invention. As a result, the invention is not limited to the specific embodiments described above, but only by the following claims and their equivalents.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method of operating a video system to generate and store encoded video comprising a plurality of key frames and a plurality of delta frames, the method comprising:
<claim-text>capturing video data comprising a plurality of frames of a scene;
<claim-text>selecting a fundamental view of at least a portion of the scene contained in the plurality of frames, wherein selecting the fundamental view includes flagging portions of the scene contained in the plurality of frames that do not have a most common view;</claim-text>
</claim-text>
<claim-text>generating the plurality of key frames from a first subset of the plurality of frames and the fundamental view of at least the portion of the scene;</claim-text>
<claim-text>generating the plurality of delta frames from a second subset of the plurality of frames and the plurality of key frames; and</claim-text>
<claim-text>transferring the encoded video for delivery to storage.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the encoded video further comprises the fundamental view of at least the portion of the scene.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein selecting the fundamental view includes selecting one or more most common portions of the scene contained in the plurality of frames.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein generating the plurality of key frames includes selecting portions of the scene contained in the fundamental view that match corresponding portions of the scene contained in the first subset of the plurality of frames, and determining differences between portions of the scene contained in the fundamental view that do not match corresponding portions of the scene contained in the first subset of the plurality of frames.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>dividing the video data into a plurality of groups of frames; and</claim-text>
<claim-text>selecting a first frame from each of the plurality of groups of frames as the first subset of the plurality of frames; and</claim-text>
<claim-text>selecting remaining frames other than the first frame from each of the plurality of groups of frames as the second subset of the plurality of frames.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each of the plurality of key frames includes a difference between a corresponding one of the first subset of the plurality of frames and the fundamental view.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each of the plurality of delta frames includes a difference between a corresponding one of the second subset of the plurality of frames and a corresponding key frame.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. A video system to generate and store encoded video comprising a plurality of key frames and a plurality of delta frames, the video system comprising:
<claim-text>a video capture device configured to capture video data comprising a plurality of frames of a scene;</claim-text>
<claim-text>a memory configured to store the encoded video; and</claim-text>
<claim-text>a video processor coupled with the video capture device and the memory configured to:
<claim-text>select a fundamental view of at least a portion of the scene contained in the plurality of frames, wherein selecting the fundamental view includes flagging portions of the scene contained in the plurality of frames that do not have a most common view;</claim-text>
<claim-text>generate the plurality of key frames from a first subset of the plurality of frames and the fundamental view of at least the portion of the scene;</claim-text>
<claim-text>generate the plurality of delta frames from a second subset of the plurality of frames and the plurality of key frames; and</claim-text>
<claim-text>transfer the encoded video to the memory.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The video system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the encoded video further comprises the fundamental view of at least the portion of the scene.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The video system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein selecting the fundamental view includes selecting one or more most common portions of the scene contained in the plurality of frames.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The video system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein generating the plurality of key frames includes selecting portions of the scene contained in the fundamental view that match corresponding portions of the scene contained in the first subset of the plurality of frames, and determining differences between portions of the scene contained in the fundamental view that do not match corresponding portions of the scene contained in the first subset of the plurality of frames.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The video system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the video processor is further configured to:
<claim-text>divide the video data into a plurality of groups of frames; and</claim-text>
<claim-text>select a first frame from each of the plurality of groups of frames as the first subset of the plurality of frames; and</claim-text>
<claim-text>select remaining frames other than the first frame from each of the plurality of groups of frames as the second subset of the plurality of frames.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The video system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein each of the plurality of key frames includes a difference between a corresponding one of the first subset of the plurality of frames and the fundamental view.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The video system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein each of the plurality of delta frames includes a difference between a corresponding one of the second subset of the plurality of frames and a corresponding key frame.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. A non-transitory computer-readable medium having instructions stored thereon for operating a computer system to generate and store encoded video comprising a plurality of key frames and a plurality of delta frames, wherein the instructions, when executed by the computer system, direct the computer system to:
<claim-text>capture video data of a scene, the video data comprising a plurality of frames;</claim-text>
<claim-text>select a fundamental view of at least a portion of the scene contained in the plurality of frames, wherein selecting the fundamental view includes flagging portions of the scene contained in the plurality of frames that do not have a most common view;</claim-text>
<claim-text>generate the plurality of key frames from a first subset of the plurality of frames and the fundamental view of at least the portion of the scene;</claim-text>
<claim-text>generate the plurality of delta frames from a second subset of the plurality of frames and the plurality of key frames; and</claim-text>
<claim-text>transfer the encoded video for delivery to storage.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The non-transitory computer-readable medium of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the encoded video further comprises the fundamental view of at least the portion of the scene.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The non-transitory computer-readable medium of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein selecting the fundamental view includes selecting one or more most common portions of the scene contained in the plurality of frames.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The non-transitory computer-readable medium of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein generating the plurality of key frames includes selecting portions of the scene contained in the fundamental view that match corresponding portions of the scene contained in the first subset of the plurality of frames, and determining differences between portions of the scene contained in the fundamental view that do not match corresponding portions of the scene contained in the first subset of the plurality of frames.</claim-text>
</claim>
</claims>
</us-patent-grant>
