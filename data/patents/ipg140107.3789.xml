<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08624855-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08624855</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12949627</doc-number>
<date>20101118</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<us-term-of-grant>
<us-term-extension>178</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>3</main-group>
<subgroup>041</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20130101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>3</main-group>
<subgroup>033</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>345173</main-classification>
<further-classification>345156</further-classification>
<further-classification>345174</further-classification>
<further-classification>715863</further-classification>
<further-classification>178 1801</further-classification>
<further-classification>178 1806</further-classification>
</classification-national>
<invention-title id="d2e53">Recognizing multiple input point gestures</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5543590</doc-number>
<kind>A</kind>
<name>Gillespie et al.</name>
<date>19960800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>178 1806</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5796406</doc-number>
<kind>A</kind>
<name>Shigematsu et al.</name>
<date>19980800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5825352</doc-number>
<kind>A</kind>
<name>Bisset et al.</name>
<date>19981000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5943043</doc-number>
<kind>A</kind>
<name>Furuhata et al.</name>
<date>19990800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6208329</doc-number>
<kind>B1</kind>
<name>Ballare</name>
<date>20010300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6323846</doc-number>
<kind>B1</kind>
<name>Westerman et al.</name>
<date>20011100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345173</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6590567</doc-number>
<kind>B1</kind>
<name>Nagao et al.</name>
<date>20030700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6856259</doc-number>
<kind>B1</kind>
<name>Sharp</name>
<date>20050200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6943779</doc-number>
<kind>B2</kind>
<name>Satoh</name>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6992660</doc-number>
<kind>B2</kind>
<name>Kawano et al.</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>7053887</doc-number>
<kind>B2</kind>
<name>Kraus et al.</name>
<date>20060500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>7479949</doc-number>
<kind>B2</kind>
<name>Jobs</name>
<date>20090100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2002/0015064</doc-number>
<kind>A1</kind>
<name>Robotham et al.</name>
<date>20020200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345863</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2002/0036618</doc-number>
<kind>A1</kind>
<name>Wakai et al.</name>
<date>20020300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345157</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2004/0021644</doc-number>
<kind>A1</kind>
<name>Enomoto</name>
<date>20040200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345173</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2005/0052427</doc-number>
<kind>A1</kind>
<name>Wu et al.</name>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345173</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>2005/0057524</doc-number>
<kind>A1</kind>
<name>Hill et al.</name>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345173</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>2005/0264541</doc-number>
<kind>A1</kind>
<name>Satoh</name>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345173</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>2006/0026521</doc-number>
<kind>A1</kind>
<name>Hotelling et al.</name>
<date>20060200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>2006/0066588</doc-number>
<kind>A1</kind>
<name>Lyon et al.</name>
<date>20060300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345173</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>2008/0143975</doc-number>
<kind>A1</kind>
<name>Dennard et al.</name>
<date>20080600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>353 42</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>2011/0260986</doc-number>
<kind>A1</kind>
<name>Weiss</name>
<date>20111000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00023">
<othercit>Lee, SK, et al., &#x201c;A Multi-Touch Three Dimensional Touch-Sensitive Table&#x201d;, CHI'85 Proceedings, Apr. 1985, pp. 21-25.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00024">
<othercit>Wu, Mike, et la., &#x201c;Gesture Registration, Relaxation, and Reuse for Multi-Point Direct-Touch Surfaces&#x201d;, IEEE 2006.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00025">
<othercit>Davis, James, et al, &#x201c;Recognizing Hand Gestures&#x201d;, ECCV-94, Stockholm, Sweden, May 2-6, 1994.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00026">
<othercit>Notice of Allowance mailed Sep. 16, 2010 for U.S. Appl. No. 11/620,557.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00027">
<othercit>Office Action dated Apr. 29, 2010 cited in U.S. Appl. No. 11/620,557.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00028">
<othercit>Office Action dated Jan. 23, 2012 cited in U.S. Appl. No. 12/906,716.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00029">
<othercit>Notice of Allowance dated Jun. 26, 2012 cited in U.S. Appl. No. 12/906,716.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>13</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>345156</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345173-179</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>178 1801- 1809</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>715863</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>6</number-of-drawing-sheets>
<number-of-figures>6</number-of-figures>
</figures>
<us-related-documents>
<division>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>11620557</doc-number>
<date>20070105</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>7907125</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>12949627</doc-number>
</document-id>
</child-doc>
</relation>
</division>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20110157041</doc-number>
<kind>A1</kind>
<date>20110630</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Weiss</last-name>
<first-name>John Guido Atkins</first-name>
<address>
<city>Lake Forest Park</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Duncan</last-name>
<address>
<city>Camano Island</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Vale</last-name>
<first-name>Peter</first-name>
<address>
<city>Redmond</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Weiss</last-name>
<first-name>John Guido Atkins</first-name>
<address>
<city>Lake Forest Park</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Duncan</last-name>
<address>
<city>Camano Island</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Vale</last-name>
<first-name>Peter</first-name>
<address>
<city>Redmond</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Workman Nydegger</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Microsoft Corporation</orgname>
<role>02</role>
<address>
<city>Redmond</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Lao</last-name>
<first-name>Lun-Yi</first-name>
<department>2692</department>
</primary-examiner>
<assistant-examiner>
<last-name>Siddiqui</last-name>
<first-name>Md Saiful A</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">The present invention extends to methods, systems, and computer program products for recognizing multiple input point gestures. A recognition module receives an ordered set of points indicating that contacts have been detected in a specified order at multiple different locations on a multi-touch input surface. The recognition module determines the position of subsequently detected locations (e.g., third detected location) relative to (e.g., to the left of right of) line segments connecting previously detected locations (e.g., connecting first and second detected locations). The gesture module also detects whether line segments connecting subsequently detected locations (e.g., connecting third and fourth detected locations) intersect line segments connecting previously detected locations (e.g., connecting first and second detected locations). The gesture module recognizes an input gesture based on the relative positions and whether or not line segments intersect. The gesture module then identifies a corresponding input operation (e.g., cut, paste, etc.) to be performed.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="154.35mm" wi="279.40mm" file="US08624855-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="213.44mm" wi="169.59mm" orientation="landscape" file="US08624855-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="228.85mm" wi="152.99mm" orientation="landscape" file="US08624855-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="207.86mm" wi="153.84mm" file="US08624855-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="201.51mm" wi="166.79mm" file="US08624855-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="224.87mm" wi="159.94mm" file="US08624855-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="141.31mm" wi="103.12mm" file="US08624855-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application is a divisional of U.S. patent application Ser. No. 11/620,557, entitled &#x201c;RECOGNIZING MULTIPLE INPUT POINT GESTURES&#x201d;, filed Jan. 5, 2007, which is herein incorporated by reference in its entirety.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">BACKGROUND</heading>
<p id="p-0003" num="0002">1. Background and Relevant Art</p>
<p id="p-0004" num="0003">Computer systems and related technology affect many aspects of daily life in society. Indeed, the computer system's ability to process information has transformed the way we live and work. Computer systems now commonly perform a host of tasks (e.g., word processing, scheduling, accounting, etc.) that prior to the advent of the computer system were performed manually. More recently, computer systems have been coupled to one another and to other electronic devices to form both wired and wireless computer networks over which the computer systems and other electronic devices can transfer electronic data. Accordingly, the performance of many computing tasks are distributed across a number of different computer systems and/or a number of different computing components.</p>
<p id="p-0005" num="0004">In many environments, a computer system typically includes a Central Processing Unit (&#x201c;CPU&#x201d;), system memory (e.g., Random Access Memory (&#x201c;RAM&#x201d;), some type of mass storage device (e.g., magnetic disk), input devices, and output devices. Input devices are used to accept user input to the computer system and relay that user input to the CPU. Output devices are used to present CPU output to the user. Thus, a user typically uses input devices to input data into a computer system, the input devices relay the input data to the CPU, the CPU processes the input data to generate corresponding output data (e.g., through system or application programs), the CPU sends the output data to the output devices, and the output devices present the output data.</p>
<p id="p-0006" num="0005">Both input and output devices can include a combination of hardware (physical objects) and software (e.g., device drivers, user-interface controls) to implement their desired functionality. Specific to input devices, hardware devices, such as, for example, keyboards, mice, joysticks, etc., are used to receive input data. These or other hardware devices can be used to manipulate and navigate to user-interface controls (e.g., buttons, menus, etc) that have specified input functions. For example, a user can manipulate a mouse to move a cursor over a &#x201c;print&#x201d; button and click a mouse button to cause a document to print.</p>
<p id="p-0007" num="0006">More recently, computer display devices have been configured to function both as an input device and a video output device. For example, computer display devices can be configured with touch surface mechanisms that facilitate the entry of input data through a display surface. Sensors (e.g., pressure sensors) embedded in the display surface can detect when objects (e.g., fingers) contact a particular location on the display surface. A computer system can then perform some action in response to detecting the contact. For example, in response to detecting contact between an object and a display surface at a location corresponding to user-interface control, a computer system can perform some action associated with the user-interface control.</p>
<p id="p-0008" num="0007">Accordingly, input devices advantageously permit a user to meaningfully interact with a computer system. In many public and semi-public environments (e.g., libraries, restaurants, private clubs, airline courtesy rooms, etc.), computer systems (e.g., kiosks, table top computer systems, etc.) are made available to large numbers of users. Within these environments, it is often desirable to permit multiple users to use the same computer simultaneously to conserve space, computer system resources, etc. It is also often desirable to allow any simultaneous users to work collaboratively together or to work separately based on individual user needs or to work collaboratively together. For example, it may be desirable to allow multiple users to work collaboratively together on a presentation. On the other hand, it may also be desirable to allow a first user to browser the Internet, while a second user edits a word processing document.</p>
<p id="p-0009" num="0008">Unfortunately, existing input device technologies have at least a few drawbacks when attempting to provide multi-user access (either collaboratively or individually) to the same computer system. For hardware devices, at least one device per user must typically be provided. For example, for multiple users to simultaneously perform different computing tasks, each user may require a separate keyboard and mouse. Thus, even though multiple computer systems are not utilized, there typically must be sufficient workspace to co-locate multiple sets of input devices per computer system.</p>
<p id="p-0010" num="0009">For software, each instance of a user-interface control can consume a portion of a display surface. Further, multiple sets of user-interface controls must be displayed to provide corresponding multiple users with simultaneous computer access. Depending on the display area of the display surface, the number of presented sets of user-interface controls, and the number of supported users, existing display area can be crowded and potentially of limited use. Further, user-interface controls (e.g., a button) typically have a single (or &#x201c;right-side up&#x201d;) orientation. Thus, when multiple users are working collaboratively, user-interface controls are properly oriented only for one of the multiple users. User-interface controls are also typically optimized for large finger sizes causing them to consume more display areas than may be needed for many users.</p>
<p id="p-0011" num="0010">In some environments, motion based gesture recognition can be used in combination with a touch surface to receive input data. Software modules within a computer system can recognize motion based gestures made on the touch surface. For example, the software modules can track a user's finger(s) in constant contact with the touch surface as the user's finger(s) makes an arc, draws an outline, etc., When the user's finger(s) are no longer detected on the touch surface, the software modules can process the contact path to recognize a corresponding motion based gesture. Motion based gestures can correspond to pre-determined input functions. Thus, when the software modules detect a motion based gesture, the corresponding input function is performed in response to the motion based gesture. Accordingly, motion based gestures, at least to some extent, can alleviate the need for per user hardware input devices and can reduce the size of per user sets of user-interface controls.</p>
<p id="p-0012" num="0011">However, recognition of motion based gestures is relatively complex. As a result, the motion tracking, coordinate buffering, algorithmic processing, etc. required to recognize a single motion based gesture can consume significant computer system resources (e.g., CPU cycles, system memory). Thus, attempting to recognize motion based gestures for multiple users (working either collaboratively or individually) at a single computer system can (potentially significantly) degrade system performance. For example, motion based gesture recognition, as well as other processes at the computer system, can take longer to complete due to limited resource availability.</p>
<heading id="h-0003" level="1">BRIEF SUMMARY</heading>
<p id="p-0013" num="0012">The present invention extends to methods, systems, and computer program products for recognizing multiple input point gestures. A computer system including a multi-touch input surface receives an ordered set of points. The ordered set of points indicates that contact between an object and the multi-touch input surface was detected at a first location on the multi-touch input surface. The ordered set of points also indicates that contact between an object and the multi-touch input surface was detected at a second location on the multi-touch input surface simultaneously with the detected contact at the first location and subsequent to detecting contact with the multi-touch input surface at the first location. The ordered set of points also indicates that contact between an object and the multi-touch input surface was detected at a third location on the multi-touch input surface simultaneously with the detected contact at the first location and at the second location and subsequent to detecting contact with the multi-touch input surface at the second location.</p>
<p id="p-0014" num="0013">The computer system calculates a line segment between the first location and the second location. The computer system determines that the third location is on a specified side of the line segment. The computer system recognizes an input gesture corresponding to detected contact at three or more locations on the multi-touch input surface based at least on the determination that the third location is on the specified side of the line segment.</p>
<p id="p-0015" num="0014">Embodiments of the invention can also be used to recognize gestures for four or more input points. For example, the ordered set of points can also indicate that contact between an object and the multi-touch input surface was detected at a fourth location on the multi-touch input surface simultaneously with the detected contact at the first location, at the second location, and at the third location and subsequent to detecting contact with the multi-touch input surface at the third location. The computer system calculates a second line segment between the second location and the third location and a third line segment between the third location and the fourth location.</p>
<p id="p-0016" num="0015">The computer system determines whether or not the fourth location is on the specified side of the second line segment. The computer system also detects whether or not the third line segment intersects with the first line segment. The computer system recognizes an input gesture corresponding to detected contact at four or more locations on the multi-touch input surface. Recognition is based at least on the determination of whether or not the fourth location is on the specified side of the second line segment and detecting whether or not the third line segment intersects with the first line segment.</p>
<p id="p-0017" num="0016">Other similar embodiments are applicable to five or more input points.</p>
<p id="p-0018" num="0017">This summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used as an aid in determining the scope of the claimed subject matter.</p>
<p id="p-0019" num="0018">Additional features and advantages of the invention will be set forth in the description which follows, and in part will be obvious from the description, or may be learned by the practice of the invention. The features and advantages of the invention may be realized and obtained by means of the instruments and combinations particularly pointed out in the appended claims. These and other features of the present invention will become more fully apparent from the following description and appended claims, or may be learned by the practice of the invention as set forth hereinafter.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0020" num="0019">In order to describe the manner in which the above-recited and other advantages and features of the invention can be obtained, a more particular description of the invention briefly described above will be rendered by reference to specific embodiments thereof which are illustrated in the appended drawings. Understanding that these drawings depict only typical embodiments of the invention and are not therefore to be considered to be limiting of its scope, the invention will be described and explained with additional specificity and detail through the use of the accompanying drawings in which:</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 1A</figref> illustrates an example computer architecture that facilitates recognizing multiple input point gestures.</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 1B</figref> illustrates a series of views of an input region of a multi-touch input surface depicting receiving points of an input gesture.</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 2</figref> illustrates a flowchart of a method for recognizing a multiple input point gesture.</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 3</figref> illustrates a third point on a specified side of a line segment calculated between a first and a second point.</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 4</figref> illustrates various multiple input point gestures.</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 5</figref> illustrates a flowchart of another method for recognizing a multiple input point gesture.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0027" num="0026">The present invention extends to methods, systems, and computer program products for recognizing multiple input point gestures. A computer system including a multi-touch input surface receives an ordered set of points. The ordered set of points indicates that contact between an object and the multi-touch input surface was detected at a first location on the multi-touch input surface. The ordered set of points also indicates that contact between an object and the multi-touch input surface was detected at a second location on the multi-touch input surface simultaneously with the detected contact at the first location and subsequent to detecting contact with the multi-touch input surface at the first location. The ordered set of points also indicates that contact between an object and the multi-touch input surface was detected at a third location on the multi-touch input surface simultaneously with the detected contact at the first location and at the second location and subsequent to detecting contact with the multi-touch input surface at the second location.</p>
<p id="p-0028" num="0027">The computer system calculates a line segment between the first location and the second location. The computer system determines that the third location is on a specified side of the line segment. The computer system recognizes an input gesture corresponding to detected contact at three or more locations on the multi-touch input surface based at least on the determination that the third location is on the specified side of the line segment.</p>
<p id="p-0029" num="0028">Embodiments of the invention can also be used to recognize gestures for four or more input points. For example, the ordered set of points can also indicate that contact between an object and the multi-touch input surface was detected at a fourth location on the multi-touch input surface simultaneously with the detected contact at the first location, at the second location, and at the third location and subsequent to detecting contact with the multi-touch input surface at the third location. The computer system calculates a second line segment between the second location and the third location and a third line segment between the third location and the fourth location.</p>
<p id="p-0030" num="0029">The computer system determines whether or not the fourth location is on the specified side of the second line segment. The computer system also detects whether or not the third line segment intersects with the first line segment. The computer system recognizes an input gesture corresponding to detected contact at four or more locations on the multi-touch input surface. Recognition is based at least on the determination of whether or not the fourth location is on the specified side of the second line segment and detecting whether or not the third line segment intersects with the first line segment.</p>
<p id="p-0031" num="0030">Other similar embodiments are applicable to five or more input points.</p>
<p id="p-0032" num="0031">Embodiments of the present invention may comprise a special purpose or general-purpose computer including computer hardware, as discussed in greater detail below. Embodiments within the scope of the present invention also include computer-readable media for carrying or having computer-executable instructions or data structures stored thereon. Such computer-readable media can be any available media that can be accessed by a general purpose or special purpose computer. By way of example, and not limitation, computer-readable media can comprise physical (or recordable type) computer-readable storage media, such as, RAM, ROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store desired program code means in the form of computer-executable instructions or data structures and which can be accessed by a general purpose or special purpose computer.</p>
<p id="p-0033" num="0032">In this description and in the following claims, a &#x201c;network&#x201d; is defined as one or more data links that enable the transport of electronic data between computer systems and/or modules. When information is transferred or provided over a network or another communications connection (either hardwired, wireless, or a combination of hardwired or wireless) to a computer, the computer properly views the connection as a computer-readable medium. Thus, by way of example, and not limitation, computer-readable media can also comprise a network or data links which can be used to carry or store desired program code means in the form of computer-executable instructions or data structures and which can be accessed by a general purpose or special purpose computer.</p>
<p id="p-0034" num="0033">Computer-executable instructions comprise, for example, instructions and data which cause a general purpose computer, special purpose computer, or special purpose processing device to perform a certain function or group of functions. The computer executable instructions may be, for example, binaries, intermediate format instructions such as assembly language, or even source code. Although the subject matter has been described in language specific to structural features and/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the described features or acts described above. Rather, the described features and acts are disclosed as example forms of implementing the claims.</p>
<p id="p-0035" num="0034">Those skilled in the art will appreciate that the invention may be practiced in network computing environments with many types of computer system configurations, including, personal computers, desktop computers, laptop computers, message processors, hand-held devices, table top computers, multi-processor systems, microprocessor-based or programmable consumer electronics, network PCs, minicomputers, mainframe computers, mobile telephones, PDAs, pagers, and the like. The invention may also be practiced in distributed system environments where local and remote computer systems, which are linked (either by hardwired data links, wireless data links, or by a combination of hardwired and wireless data links) through a network, both perform tasks. In a distributed system environment, program modules may be located in both local and remote memory storage devices.</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 1A</figref> illustrates an example computer architecture <b>100</b> that facilitates recognizing multiple input point gestures. Depicted in computer architecture <b>100</b> table top computer system <b>101</b>. Table top computer system includes sensors <b>103</b> and input module <b>109</b>. Input module <b>109</b> further includes set creation module <b>104</b> and gesture recognition module <b>107</b>.</p>
<p id="p-0037" num="0036">Table top computer system <b>101</b> can be connected to a network, such as, for example, a Local Area Network (&#x201c;LAN&#x201d;), a Wide Area Network (&#x201c;WAN&#x201d;), or even the Internet. Thus, the various components can receive data from and send data to other components connected to the network. Accordingly, the components can create message related data and exchange message related data (e.g., Internet Protocol (&#x201c;IP&#x201d;) datagrams and other higher layer protocols that utilize IP datagrams, such as, Transmission Control Protocol (&#x201c;TCP&#x201d;), Hypertext Transfer Protocol (&#x201c;HTTP&#x201d;), Simple Mail Transfer Protocol (&#x201c;SMTP&#x201d;), etc.) over the network.</p>
<p id="p-0038" num="0037">Generally, the components of table top computer system <b>101</b> are configured to recognize multiple input point gestures made on multi-touch input surface <b>102</b>.</p>
<p id="p-0039" num="0038">Sensors <b>103</b> can be configured to detect when a physical object (e.g., a bottle, a glass, a finger, a hat, etc.) has come into physical contact with a portion of the multi-touch input surface <b>102</b>. For example, sensors <b>103</b> can detect when a portion of hand <b>137</b> has come in contact with multi-touch input surface <b>102</b>. Sensors <b>103</b> can be embedded in multi-touch input surface <b>102</b> and can include for example, pressure sensors, temperature sensors, image scanners, barcode scanners, etc., that interoperate with the components of input module <b>109</b> to recognize a multiple point input gesture and subsequently identify a corresponding input operation.</p>
<p id="p-0040" num="0039">In some embodiments, multi-touch input surface <b>102</b> includes sensors for implementing a touch screen interface. For example, the multi-touch input surface can include an interactive multi-touch surface. Thus, it may be that multi-touch input surface <b>102</b> also functions as a presentation surface to display video output data to a user of table top computer system <b>101</b>.</p>
<p id="p-0041" num="0040">Sensors <b>103</b> can be included (e.g., embedded) in a plurality of locations across the multi-touch input surface <b>102</b>. Sensors <b>103</b> can be configured to interoperation with set creation module <b>104</b> to create ordered sets of (input) points. Sensors <b>103</b> can differentiate locations where physical contact with the multi-touch input surface <b>102</b> has occurred. Thus, sensors <b>103</b> can differentiate between contact at a plurality of different locations on the multi-touch input surface <b>102</b> simultaneously.</p>
<p id="p-0042" num="0041">Set creation module <b>104</b> can also be configured to provide the coordinate location (e.g., Cartesian position) for a point in response to detecting an object of some size surrounding the coordinate location. For example, when a finger (or thumb) contacts multi-touch input surface <b>102</b>, contact can be detected over an area approximately equal to the finger (or thumb) tip of the finger (or thumb). Set creation module <b>104</b> can utilize geometric calculations to calculate an appropriate coordinate location for an input point based on a detected area of contact. For example, if a (e.g., finger tip) contact area was the general shape of an oval, a set of coordinates at or near the center of the oval can be calculated as the input point.</p>
<p id="p-0043" num="0042">Gesture recognition module <b>107</b> is configured to receive an ordered set of points from set creation module and recognize an input gesture corresponding to the input points. From a recognized input gesture, gesture recognition module <b>107</b> can identify an input operation that is to be sent to an input subsystem of table top computer system <b>101</b>.</p>
<p id="p-0044" num="0043">Generally, an input point gesture is started by placing a finger (or thumb or some other form of contact) down on multi-touch input surface <b>102</b>. The location where the first finger is placed is the first &#x2018;point&#x2019; in the point gesture. Once a finger is in contact with the surface, the finger can remain stationary until the end of the gesture. Subsequently, one or more other fingers are consecutively placed onto the surface in different locations, adding more points to the point gesture. The meaning of the gesture can change based on the amount of points, the relative location of the points, and the order in which the points were added. The gesture is completed when all fingers are released from the input surface.</p>
<p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. 1B</figref> illustrates a series of views of input region <b>121</b> (a portion of multi-touch input surface <b>102</b>) depicting receiving points of an input gesture. The sequence of views is in chronological order starting with view <b>131</b> and ending with view <b>135</b>. <figref idref="DRAWINGS">FIG. 1B</figref> will be described with some reference back to components of <figref idref="DRAWINGS">FIG. 1A</figref>.</p>
<p id="p-0046" num="0045">In view <b>131</b>, hand <b>137</b> is above input region <b>121</b> and no contact with multi-touch input surface <b>102</b> is detected.</p>
<p id="p-0047" num="0046">In view <b>132</b>, the thumb of hand <b>137</b> is placed in contact with multi-touch input surface <b>102</b> in proximity to location <b>141</b>. Sensors <b>103</b> can detect the thumb contact and transfer the thumb contact area to set creation module <b>104</b>. Set creation module <b>104</b> can calculate an appropriate coordinate location for an input point based on the thumb contact area. Set creation module <b>104</b> can then add the coordinate location to an ordered input set for the current gesture. Set creation module <b>104</b> can then provide feedback <b>138</b> (e.g., to a graphical subsystem) to cause multi-touch input surface <b>102</b> to indicate visually that the point has been added to the ordered input set. As depicted in view <b>132</b>, visual feedback (e.g., a brightly colored circle) can be provided highlighting location <b>141</b>. Feedback can be updated in essentially real time as new input points are detected.</p>
<p id="p-0048" num="0047">In view <b>133</b>, the index finger of hand <b>137</b> is placed in contact with multi-touch input surface <b>102</b> in proximity to location <b>142</b>. Sensors <b>103</b> can detect the index finger contact and transfer the index finger contact area to set creation module <b>104</b>. Set creation module <b>104</b> can calculate an appropriate coordinate location for a second input point based on the index finger contact area. Set creation module <b>104</b> can then add the second coordinate location to the ordered input set for the current gesture. Set creation module <b>104</b> can then provide updated feedback to cause multi-touch input surface <b>102</b> to indicate visually that the second point has been added to the ordered input set. As depicted in view <b>132</b>, visual feedback (e.g., a brightly colored circle) can be provided highlighting location <b>142</b>. Further visual feedback (e.g., a brightly colored line) can also be provided to highlight a line segment <b>151</b> connecting location <b>141</b> and location <b>142</b>.</p>
<p id="p-0049" num="0048">In view <b>134</b>, the middle finger of hand <b>137</b> is placed in contact with multi-touch input surface <b>102</b> in proximity to location <b>143</b>. Sensors <b>103</b> can detect the middle finger contact and transfer the middle finger contact area to set creation module <b>104</b>. Set creation module <b>104</b> can calculate an appropriate coordinate location for a third input point based on the middle finger contact area. Set creation module <b>104</b> can then add the third coordinate location to the ordered input set for the current gesture. Set creation module <b>104</b> can then provide updated feedback to cause multi-touch input surface <b>102</b> to indicate visually that the third point has been added to the ordered input set. As depicted in view <b>134</b>, visual feedback (e.g., a brightly colored circle) can be provided highlighting location <b>143</b>. Further visual feedback (e.g., a brightly colored line) can also be provided to highlight a line segment <b>152</b> connecting location <b>142</b> and location <b>143</b>.</p>
<p id="p-0050" num="0049">In view <b>135</b>, hand <b>137</b> is above input region <b>121</b> and no contact with multi-touch input surface <b>102</b> is detected. Ceasing to detect contact with multi-touch input surface <b>102</b> can indicate that the gesture is complete. In response, set creation module <b>104</b> can send the ordered set of input points to gesture recognition module <b>107</b>.</p>
<p id="p-0051" num="0050">Accordingly, in <figref idref="DRAWINGS">FIG. 1A</figref>, point locations <b>106</b> generally represent a set of detected points and feedback <b>138</b> generally represents feedback for highlighting those detected points on multi-touch input surface <b>102</b>.</p>
<p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. 2</figref> illustrates a flowchart of a method <b>200</b> for recognizing a multiple input point gesture. <figref idref="DRAWINGS">FIG. 3</figref> illustrates a third point on a specified side of a line segment calculated between a first and a second point. The method <b>200</b> will be described with respect to the components and data depicted in computer architecture <b>100</b> and the points in <figref idref="DRAWINGS">FIG. 3</figref>.</p>
<p id="p-0053" num="0052">Method <b>200</b> includes an act of receiving an ordered set of points (act <b>201</b>). The ordered set of points indicates at least that: a) contact with the multi-touch input surface was detected at a first location on the multi-touch input surface, b) contact with the multi-touch input surface was detected at a second location on the multi-touch input surface simultaneously with the detected contact at the first location, subsequent to detecting contact with the multi-touch input surface at the first location, and c) contact with the multi-touch input surface was detected at a third location on the multi-touch input surface simultaneously with the detected contact at the first location and at the second location, subsequent to detecting contact with the multi-touch input surface at the second location.</p>
<p id="p-0054" num="0053">For example, gesture recognition module <b>107</b> can receive set <b>116</b> from set creation module <b>104</b>. Set <b>116</b> can include a set of input points detected from a plurality of fingers (or a thumb and one or more fingers) of hand <b>137</b> touching multi-touch surface <b>102</b>.</p>
<p id="p-0055" num="0054">Method <b>200</b> includes a result oriented step for identifying an input gesture based on the number of locations where contact with the multi-touch input surface was detected and the relative orientation of the locations where contact with the multi-touch input surface was detected relative to one another (step <b>205</b>). Functional, result-oriented step <b>205</b> may be accomplished via any suitable corresponding acts. However, in <figref idref="DRAWINGS">FIG. 2</figref>, the step <b>205</b> includes corresponding acts <b>202</b>, <b>203</b>, and <b>204</b>.</p>
<p id="p-0056" num="0055">Thus, step <b>205</b> includes a corresponding act of calculating a line segment between the first location and the second location (act <b>202</b>). For example, gesture recognition module <b>107</b> can calculate a line segment between the first and second detected points included in set <b>116</b>.</p>
<p id="p-0057" num="0056">Referring now to <figref idref="DRAWINGS">FIG. 3</figref>, gesture recognition module <b>107</b> can calculate line segment <b>314</b> from point <b>311</b> to point <b>312</b>. Points <b>311</b> and <b>312</b> can be Cartesian coordinates on multi-touch input surface <b>102</b>. Points <b>311</b> and <b>312</b> can be calculated as appropriate points for contact with multi-touch input surface <b>102</b> detected at or near locations <b>301</b> and <b>302</b> respectively.</p>
<p id="p-0058" num="0057">Step <b>205</b> also includes a corresponding act of determining that the third location is on a specified side of the line segment (act <b>203</b>). For example, gesture recognition module <b>207</b> can determine that a third detected point in set <b>116</b> is to the right or to the left of line segment between the first and second detected points in set <b>116</b>.</p>
<p id="p-0059" num="0058">Referring again to <figref idref="DRAWINGS">FIG. 3</figref>, gesture recognition module <b>107</b> can determine that point <b>313</b> is to the right of line segment <b>314</b>. Point <b>314</b> can be Cartesian coordinates on multi-touch input surface <b>102</b>. Point <b>314</b> can be calculated as an appropriate point for contact with multi-touch input surface <b>102</b> detected at or near location <b>303</b>.</p>
<p id="p-0060" num="0059">In some embodiments, a line segment is extended to assist in determining what side of a line segment a point is on. For example in <figref idref="DRAWINGS">FIG. 3</figref>, as depicted by the dashed lines, line segment <b>314</b> can be extended in both directions. Region <b>321</b> indicates points to the left of line segment <b>314</b> and region <b>322</b> indicates points to the right of line segment <b>314</b>. When a third point is detected to be on a line segment between a first and second point (or extension thereof), default behavior can be applied to place the third point to the left or right of the line segment as desired.</p>
<p id="p-0061" num="0060">Step <b>205</b> also includes a corresponding act of recognizing an input gesture corresponding to detected contact at three or more locations on the multi-touch input surface based at least on the determination that the third location is on the specified side of the line segment (act <b>204</b>). For example, gesture recognition module <b>107</b> can recognize an input gesture corresponding to detected contact at three or more locations on multi-touch input surface <b>102</b>. Gesture recognition module <b>107</b> can base the determination at least on whether a third detected point in set <b>116</b> is to the left or right of a line segment between the first and second detected points in set <b>116</b>. If the third detected point is on the right of the line segment, gesture recognition module <b>107</b> can recognize a first gesture. On the other hand, if the third detected point is on the left of the line segment, gesture recognition module <b>107</b> can recognize a second different gesture.</p>
<p id="p-0062" num="0061">As the number of input points increases so does the number of unique input gestures that can be generate based on the number of points. Table 1 depicts the number of possible gestures for 1, 2, 3, 4, and 5 input points.</p>
<p id="p-0063" num="0062">
<tables id="TABLE-US-00001" num="00001">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="offset" colwidth="28pt" align="left"/>
<colspec colname="1" colwidth="77pt" align="left"/>
<colspec colname="2" colwidth="112pt" align="left"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="2" rowsep="1">TABLE 1</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
<row>
<entry/>
<entry>Number of Points</entry>
<entry>Possible Gestures</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry>1 point</entry>
<entry>&#x2002;1 possible point gesture</entry>
</row>
<row>
<entry/>
<entry>2 points</entry>
<entry>&#x2002;1 possible point gesture</entry>
</row>
<row>
<entry/>
<entry>3 points</entry>
<entry>&#x2002;2 possible point gestures</entry>
</row>
<row>
<entry/>
<entry>4 points</entry>
<entry>&#x2002;6 possible point gestures</entry>
</row>
<row>
<entry/>
<entry>5 points</entry>
<entry>22 possible point gestures</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0064" num="0063">Thus, using five or fewer points a total of 32 different gestures are possible.</p>
<p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. 4</figref> illustrates various multiple input point gestures. <figref idref="DRAWINGS">FIG. 4</figref> depicts two, three, and four point gestures. Numbers within the depicted points indicate the order in which the points were detected. That is, 1 is detected first, 2 is detected second, 3 is detected third, and 4 is detected fourth.</p>
<p id="p-0066" num="0065">As depicted, gesture <b>402</b> can be recognized based on two input points. Either of gestures <b>403</b> or <b>413</b> can be recognized based on three input points. Gesture <b>403</b> can be recognized when the third detected point is to the right of the line segment connecting the first and second detected points. On the other hand, gesture <b>413</b> can be recognized when the third detected point is to the left of the line segment connecting the first and second detected points.</p>
<p id="p-0067" num="0066">Any of gestures <b>404</b>, <b>414</b>, <b>424</b>, <b>434</b>, <b>444</b>, and <b>454</b> can be recognized based on four input points. Detection of four point gestures can include determining whether the fourth detected point is to the right or left of a second line segment connecting the second and third detected points. Detection of four point gestures can also include determining whether a third line segment connecting the third and fourth detected points intersects the line segment connecting the first and second detected points.</p>
<p id="p-0068" num="0067">For example, gesture <b>404</b> can be recognized when the third detected point is to the right of line segment connecting the first and second detected points, the fourth detected point is to the right of a second line segment connecting the second and third detected points (relative to the second line segment), and a third line segment connecting the third and fourth detected points does not intersect the line segment connecting the first and second detected points.</p>
<p id="p-0069" num="0068">Gesture <b>414</b> can be recognized when the third detected point is to the right of line segment connecting the first and second detected points, the fourth detected point is to the right of a second line segment connecting the second and third detected points (relative to the second line segment), and a third line segment connecting the third and fourth detected points intersects the line segment connecting the first and second detected points.</p>
<p id="p-0070" num="0069">Gesture <b>424</b> can be recognized when the third detected point is to the right of line segment connecting the first and second detected points, the fourth detected point is to the left of a second line segment connecting the second and third detected points (relative to the second line segment), and a third line segment connecting the third and fourth detected points does not intersect the line segment connecting the first and second detected points.</p>
<p id="p-0071" num="0070">Gesture <b>434</b> can be recognized when the third detected point is to the left of line segment connecting the first and second detected points, the fourth detected point is to the left of a second line segment connecting the second and third detected points (relative to the second line segment), and a third line segment connecting the third and fourth detected points does not intersect the line segment connecting the first and second detected points.</p>
<p id="p-0072" num="0071">Gesture <b>444</b> can be recognized when the third detected point is to the left of line segment connecting the first and second detected points, the fourth detected point is to the left of a second line segment connecting the second and third detected points (relative to the second line segment), and a third line segment connecting the third and fourth detected points intersects the line segment connecting the first and second detected points.</p>
<p id="p-0073" num="0072">Gesture <b>454</b> can be recognized when the third detected point is to the left of line segment connecting the first and second detected points, the fourth detected point is to the right of a second line segment connecting the second and third detected points (relative to the second line segment), and a third line segment connecting the third and fourth detected points does not intersect the line segment connecting the first and second detected points.</p>
<p id="p-0074" num="0073">Thus, as depicted in <figref idref="DRAWINGS">FIG. 4</figref>, embodiments of the invention can also be used to recognize gestures for four or more input points.</p>
<p id="p-0075" num="0074">Accordingly, step <b>205</b> can include a corresponding act of receiving an ordered set of points indicating that contact with the multi-touch input surface was detected at a fourth location on the multi-touch input surface simultaneously with the detected contact at the first location, at the second location, and at the third location, subsequent to detecting contact with the multi-touch input surface at the third location. For example, set <b>116</b> can also indicate that contact between an object and multi-touch input surface <b>102</b> was detected at a fourth location on the multi-touch input surface <b>102</b> simultaneously with the detected contact at the first location, at the second location, and at the third location and subsequent to detecting contact with multi-touch input surface <b>102</b> at the third location.</p>
<p id="p-0076" num="0075">Step <b>205</b> can include a corresponding act of calculating a second line segment between the second location and the third location. For example, gesture recognition module <b>107</b> can calculate a second line segment between the second and third detected points included in set <b>116</b>.</p>
<p id="p-0077" num="0076">Step <b>205</b> can include a corresponding act of determining whether or not the fourth location is on the specified side of the second line segment. For example, gesture recognition module <b>107</b> can determine that the fourth location on multi-touch input surface <b>102</b> is to the left of the second line segment connecting the second and third locations (relative to the second line segment) on multi-touch input surface <b>102</b>.</p>
<p id="p-0078" num="0077">Step <b>205</b> can include a corresponding act of calculating a third line segment between the third location and the fourth location. For example, gesture recognition module <b>107</b> can calculate a third line segment between the third and fourth detected points included in set <b>116</b>.</p>
<p id="p-0079" num="0078">Step <b>205</b> can include a corresponding act of detecting whether or not the third line segment intersects with the first line segment. For example, gesture recognition module <b>107</b> can detect if the third line segment between the third and fourth detected points included in set <b>116</b> intersects the line segment between the first and second detected points included in set <b>116</b>.</p>
<p id="p-0080" num="0079">Step <b>205</b> can include a corresponding act of recognizing an input gesture corresponding to detected contact at four or more locations on the multi-touch input surface. Recognition is based at least on the determination of whether or not the fourth location is on the specified side of the second line segment and detecting whether or not the third line segment intersects with the first line segment. For example, gesture recognition module <b>107</b> can recognize a corresponding four point gesture detected on multi-touch input surface <b>102</b> based on whether the fourth detected point is to the left or right of the second line segment connecting the second and third detected points (relative to the second line segment). Gesture recognition module <b>107</b> can also recognize a corresponding four point gesture detected on multi-touch input surface <b>102</b> based on whether or not whether the third line segment intersects with the first line segment.</p>
<p id="p-0081" num="0080">Further embodiments can be used to recognize various different gestures having five or more input points. For example, gesture recognition module <b>107</b> can check to determine whether a fifth detected point is to the right or left of the third line segment. Gesture recognition module <b>107</b> can also detect whether a fourth line segment connecting the fourth and fifth detected points cross either of both of the first and second line segments. Gesture recognition module <b>107</b> can combine determination of a left or right side for the fifth point and intersection data for the fourth line segment with previously generated data for the first through fourth detected points to recognize a unique five point input gesture.</p>
<p id="p-0082" num="0081">Gesture recognition module <b>107</b> can be configured to recognize unique input gestures for virtually any number of input points. Thus, gesture recognition based on six, seven, eight, nine, ten, twenty, thirty, or more points is possible.</p>
<p id="p-0083" num="0082">Step <b>205</b> can include a corresponding act of looking up the meaning of the input gesture in a table of known input gestures. For example, gesture recognition module <b>107</b> may identify gesture <b>111</b> from set <b>116</b>. Subsequently, gesture recognition module <b>107</b> can identify an input related operation that corresponds to input gesture <b>111</b>. For example, gesture recognition module <b>107</b> can refer to gesture table <b>108</b> to identify that input operation <b>112</b> corresponds to gesture <b>111</b>. That is, when gesture <b>111</b> is recognized, a user of table top computer system <b>101</b> is requesting (through their input of points) that input operation <b>112</b> be performed.</p>
<p id="p-0084" num="0083">The different meanings applied to gestures can vary according to context. Listed below are a few examples of different contexts and applications:
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0084">1. Cut and Paste and other Common Tasks&#x2014;
        <ul id="ul0003" list-style="none">
            <li id="ul0003-0001" num="0085">The two three-point-gestures could be a substitute for the keyboard shortcut commands for Cut and Paste. Right could be used for control-C because the gesture resembles a &#x201c;C&#x201d; and Left could be used for control-V because the gesture resembles a &#x201c;V.&#x201d; Executing these gestures also feels quite similar to executing keyboard shortcuts.</li>
        </ul>
        </li>
        <li id="ul0002-0002" num="0086">2. Wizard Commands for a Role Playing Game&#x2014;
        <ul id="ul0004" list-style="none">
            <li id="ul0004-0001" num="0087">Each point gesture could be a command for a unique spell that a wizard avatar can cast. Users would be encouraged to learn more complex commands to improve their skills at the game&#x2014;perhaps the points of an offensive spell when touched in the opposite order will trigger a defensive spell. For example, Three-point right is a fire spell, and Three-point left is an ice shield.</li>
        </ul>
        </li>
        <li id="ul0002-0003" num="0088">3. Photo/Graphics Editing Application&#x2014;
        <ul id="ul0005" list-style="none">
            <li id="ul0005-0001" num="0089">The location data of each point in the gesture can be made available to client applications. In a graphics application, a &#x201c;4-point right, right, no intersection&#x201d; could rotate the closest graphic element to the initial point in the gesture (or the center of the gesture) by 90 degrees clockwise. Similarly, a &#x201c;4-point left, left, no intersection&#x201d; could perform the same function but with counterclockwise rotation.</li>
        </ul>
        </li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0085" num="0090">Further, the parameters of multiple input point gestures can be adjusted according to the needs of the application. For example, various optional parameters can be considered when performing gesture recognition. Optional parameters can include but are not limited to: significance of point order, presence of a graphical user interface where points of gesture must be entered, presence of a single point of orientation about which points of gesture are measured (might be center of screen, may or may not be a visible point), significance of pressing and releasing points in the gesture, significance of speed in which the gesture is executed, significance of physical size of gesture, and significance of orientation of gesture.</p>
<p id="p-0086" num="0091">In some embodiments, a gesture is recognized or identified based on detected contact at a number of locations on an input surface without regard for the order in which contact was detected. Thus, there can be a number of unique gestures equal to the number of possible simultaneous locations that can be contacted. For example, using a single human hand five unique gestures may be possible: a one point gesture, a two point gesture, a three point gesture, a four point gesture, and a five point gesture.</p>
<p id="p-0087" num="0092">Using a single hand, a one point gesture can be recognized when one finger (or thumb) is simultaneously detected on an input surface and then released. Using a single hand, a two point gesture can be recognized when two fingers (or a finger and a thumb) are simultaneously detected on the input surface and then released. A two finger press and release can be recognized or identified as a &#x201c;double-click&#x201d; command similar to that entered when pressing a mouse button twice in quick succession.</p>
<p id="p-0088" num="0093">Using a single hand, a three point gesture can be recognized when three fingers (or two fingers and a thumb) are simultaneously detected on the input surface and then released. Using a single hand, four point gesture can be recognized when four fingers (or three fingers and a thumb) simultaneously detected on the input surface and then released. Using a single hand, a five point gesture can be recognized when four fingers and a thumb are simultaneously detected on the input surface and then released.</p>
<p id="p-0089" num="0094"><figref idref="DRAWINGS">FIG. 5</figref> illustrates a flowchart of another method <b>500</b> for recognizing a multiple input point gesture. The method <b>500</b> will be described with respect to the components and data depicted in computer architecture <b>100</b>.</p>
<p id="p-0090" num="0095">Method <b>500</b> includes an act of receiving a set of points, the set of points indicating that simultaneous contact was detected at one or more locations on the multi-touch input surface without regard for the order in which contact was detected (act <b>501</b>). For example, gesture recognition module <b>107</b> can receive set <b>116</b>. In these embodiments, set <b>116</b> can be an unordered set that simply indicates that contact was simultaneously detected at one or more locations on multi-touch input surface <b>102</b>.</p>
<p id="p-0091" num="0096">Method <b>500</b> includes an act of determining the number of locations where contact was simultaneously detected based on the set of points (act <b>502</b>). For example, based on set <b>116</b> gesture recognition module <b>107</b> can determine the number of locations (e.g., one, two, three, four, five, etc.) where contact was simultaneously detected on multi-touch input surface <b>102</b>.</p>
<p id="p-0092" num="0097">Method <b>500</b> includes an act of recognizing an input gesture based on and corresponding to the number of locations where contact was simultaneously detected without regard for the order in which contact was detected (act <b>503</b>). For example, based on the number of locations where contact was simultaneously detected on multi-touch input surface <b>102</b>, gesture recognition module <b>107</b> can recognize an input gesture that corresponds to the number of locations. Thus, if simultaneous contact is detected at two locations (in any order), gesture recognition module <b>107</b> can recognize an input gesture corresponding to simultaneous contact at two locations. Likewise, if simultaneous contact is detected at three, four, or five locations (in any order), gesture recognition module <b>107</b> can recognize an input gesture corresponding to simultaneous contact at three, four, or five locations respectively.</p>
<p id="p-0093" num="0098">Other embodiments can include the recognition or identification of gestures entered with two hands (i.e., up to 10 points). In these other embodiments, the order of detection can be used as a parameter in recognition or identification of gestures to increase the number of possible unique gestures. On the other hand, the order of detection may not be used as a parameter in recognition or identification of gestures to reduce the number of possible unique gestures.</p>
<p id="p-0094" num="0099">Thus, embodiments of the present invention can accommodate multiple users without having to add additional hardware devices to a computer system. Gestures can also be orientation independent, permitting users to be in any position relative to a multi-touch input surface. Further, since gestures are based on points, gestures can be entered and processed more efficiently. Accordingly, using only fingers (or similar tools) on a touched based multi-touch input surface, multiple input point gestures facilitate quickly communicating different commands to a computer system.</p>
<p id="p-0095" num="0100">The present invention may be embodied in other specific forms without departing from its spirit or essential characteristics. The described embodiments are to be considered in all respects only as illustrative and not restrictive. The scope of the invention is, therefore, indicated by the appended claims rather than by the foregoing description. All changes which come within the meaning and range of equivalency of the claims are to be embraced within their scope.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. At a computer system including a processor and system memory, a method for recognizing a multiple input point gesture, the method comprising:
<claim-text>an act of receiving an unordered set of point locations, the unordered set of point locations indicating that simultaneous contact was detected at a plurality of locations on an input surface without regard for the order in which contact was detected at individual locations included in the plurality of locations, the plurality of locations including at least three locations, including a first location, a second location and a third location, where a user touched the input surface with one or more fingers or other objects;</claim-text>
<claim-text>an act of determining a number of locations included in the at least three locations where contact was simultaneously detected from the unordered set of points;</claim-text>
<claim-text>an act of determining that the user released the one or more fingers or other objects from the input surface, such that contact is no longer detected at the at least three locations on the input surface where the simultaneous contact was initially detected; and</claim-text>
<claim-text>in response to determining that the user released the one or more fingers or other objects from the input surface, such that contact is no longer detected at the number of locations where simultaneous contact was initially detected, an act of the processor recognizing an input gesture based on and corresponding to (1) the number of locations where simultaneous contact was initially detected, but no longer detected, without regard for the order in which contact was detected at individual locations included in the plurality of locations, and (2) a relative orientation of the at least three locations on the input surface where the fingers or other objects were initially placed on the input surface, without detecting movement of the fingers or other objects, wherein different gestures correspond to different relative orientations for different determined numbers of locations where simultaneous contact was initially detected, but where contact is no longer detected.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the act of determining the number locations where contact was simultaneously detected comprises an act of identifying four locations on the input surface where contact was simultaneously detected, but no longer detected.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the act of determining the number locations where contact was simultaneously detected comprises an act of identifying five locations on the input surface where contact was simultaneously detected, but no longer detected.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the act of recognizing an input gesture comprises an act of recognizing one of a cut gesture and a paste gesture.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising an act of looking up the meaning of the recognized input gesture in a table of known input gestures.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the act of recognizing an input gesture comprises an act of recognizing an input gesture based on a physical size of the input gesture.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. A computer program product for use at a computer system, the computer program product for implementing a method for recognizing a multiple input point gesture, the computer program product comprising one or more physical recordable type computer storage medium having stored there one computer-executable instructions, that when executed at a processor, cause the computer system to perform the method, including the following:
<claim-text>receive an unordered set of point locations, the unordered set of point locations indicating that simultaneous contact was detected at a plurality of locations on an input surface without regard for the order in which contact was detected at individual locations included in the plurality of locations, the plurality of locations including at least three locations, including a first location, a second location and a third location where a user touched the input surface with one or more fingers or other objects;</claim-text>
<claim-text>determine a number of locations included in the at least three locations where contact was simultaneously detected from the unordered set of points;</claim-text>
<claim-text>an act of determining that contact is no longer detected at the at least three locations on the input surface where the simultaneous contact was initially detected; and</claim-text>
<claim-text>in response to determining that the user released the one or more fingers or other objects from the input surface, such that contact is no longer detected at the number of locations where simultaneous contact was initially detected, recognize an input gesture based on and corresponding to (1) the number of locations where simultaneous contact was detected without regard for the order in which contact was detected, but no longer detected, at individual locations included in the plurality of locations, and (2) a relative orientation of the at least three locations on the input surface where the fingers or other objects were initially placed on the input surface, without detecting movement of the fingers or other objects.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The computer program product as recited in <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein computer-executable instructions, that when executed, cause the computer system to determine the number locations where contact was simultaneously detected comprise computer-executable instructions, that when executed, cause the computer system to identify four locations on the input surface where contact was simultaneously detected.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The computer program product as recited in <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein computer-executable instructions, that when executed, cause the computer system to determine the number locations where contact was simultaneously detected comprise computer-executable instructions, that when executed, cause the computer system to identify five locations on the input surface where contact was simultaneously detected.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The computer program product as recited in <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein computer-executable instructions, that when executed, cause the computer system to recognize an input gesture comprise computer-executable instructions, that when executed, cause the computer system to recognize one of a cut gesture and a paste gesture.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The computer program product as recited in <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising computer-executable instructions, that when executed, cause the computer system to look up the meaning of the recognized input gesture in a table of known input gestures.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The computer program product as recited in <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein computer-executable instructions, that when executed, cause the computer system to recognize an input gesture comprise computer-executable instructions, that when executed, cause the computer system to recognize an input gesture based on a speed in which the input gesture was executed.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. A computer system, the computer system comprising:
<claim-text>one or more processors;</claim-text>
<claim-text>an input surface; and</claim-text>
<claim-text>one or more computer storage media having stored thereon computer-executable instructions representing a gesture recognition module, the gesture recognition module configured to:</claim-text>
<claim-text>perform a first act of receiving an unordered set of point locations, the unordered set of point locations indicating that simultaneous contact was detected at two locations on an input surface where one or more fingers or other objects were initially placed on the input surface, without detecting movement of the fingers or other objects and without regard for the order in which contact was detected at the two locations;</claim-text>
<claim-text>perform a second act of determining contact was simultaneously detected from the unordered set of points at the two locations;</claim-text>
<claim-text>perform a third act of determining that contact is no longer detected at either of the two locations on the input surface where the simultaneous contact was initially detected; and</claim-text>
<claim-text>perform a forth act of equating the first, second and third act as a mouse button double-click gesture. </claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
