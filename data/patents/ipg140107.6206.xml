<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08627330-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08627330</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12256361</doc-number>
<date>20081022</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<us-term-of-grant>
<us-term-extension>1114</us-term-extension>
<disclaimer>
<text>This patent is subject to a terminal disclaimer.</text>
</disclaimer>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>13</main-group>
<subgroup>10</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>718106</main-classification>
<further-classification>718105</further-classification>
</classification-national>
<invention-title id="d2e55">Workload manager managing a workload of an enterprise data warehouse</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>3411139</doc-number>
<kind>A</kind>
<name>Lynch et al.</name>
<date>19681100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>710  1</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5881284</doc-number>
<kind>A</kind>
<name>Kubo</name>
<date>19990300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>718100</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6338072</doc-number>
<kind>B1</kind>
<name>Durand et al.</name>
<date>20020100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>718104</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6353844</doc-number>
<kind>B1</kind>
<name>Bitar et al.</name>
<date>20020300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>718102</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6763519</doc-number>
<kind>B1</kind>
<name>McColl et al.</name>
<date>20040700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>718100</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>7657501</doc-number>
<kind>B1</kind>
<name>Brown et al.</name>
<date>20100200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>707999002</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>7664561</doc-number>
<kind>B1</kind>
<name>Chen et al.</name>
<date>20100200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>700101</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2002/0138679</doc-number>
<kind>A1</kind>
<name>Koning et al.</name>
<date>20020900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>710244</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2005/0160074</doc-number>
<kind>A1</kind>
<name>Vos et al.</name>
<date>20050700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>707  1</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2005/0235289</doc-number>
<kind>A1</kind>
<name>Barillari et al.</name>
<date>20051000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>718100</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2006/0206900</doc-number>
<kind>A1</kind>
<name>Ooyama et al.</name>
<date>20060900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>718105</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2007/0100793</doc-number>
<kind>A1</kind>
<name>Brown et al.</name>
<date>20070500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>707  2</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2007/0106991</doc-number>
<kind>A1</kind>
<name>Yoo</name>
<date>20070500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>718103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2007/0169125</doc-number>
<kind>A1</kind>
<name>Qin</name>
<date>20070700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>718102</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2007/0271570</doc-number>
<kind>A1</kind>
<name>Brown et al.</name>
<date>20071100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>718105</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2008/0077932</doc-number>
<kind>A1</kind>
<name>Ruppach et al.</name>
<date>20080300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>718105</main-classification></classification-national>
</us-citation>
<us-citation>
<nplcit num="00017">
<othercit>Brown et al. &#x201c;Managing Memory to Meet Multiclass Workload Response Time Goals&#x201d;, Technical Report # 1146, Apr. 1993.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
</us-references-cited>
<number-of-claims>20</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>None</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>7</number-of-drawing-sheets>
<number-of-figures>7</number-of-figures>
</figures>
<us-related-documents>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>61042954</doc-number>
<date>20080407</date>
</document-id>
</us-provisional-application>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>61010132</doc-number>
<date>20080103</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20090178042</doc-number>
<kind>A1</kind>
<date>20090709</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Mehta</last-name>
<first-name>Abhay</first-name>
<address>
<city>Austin</city>
<state>TX</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Gupta</last-name>
<first-name>Chetan Kumar</first-name>
<address>
<city>Austin</city>
<state>TX</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Dayal</last-name>
<first-name>Umeahwar</first-name>
<address>
<city>Saratoga</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Mehta</last-name>
<first-name>Abhay</first-name>
<address>
<city>Austin</city>
<state>TX</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Gupta</last-name>
<first-name>Chetan Kumar</first-name>
<address>
<city>Austin</city>
<state>TX</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Dayal</last-name>
<first-name>Umeahwar</first-name>
<address>
<city>Saratoga</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Hewlett-Packard Development Company, L.P.</orgname>
<role>02</role>
<address>
<city>Houston</city>
<state>TX</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Coleman</last-name>
<first-name>Eric</first-name>
<department>2183</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Described herein is a workload manager for managing a workload in a database that includes: an admission controller operating to divide the workload into a plurality of batches, with each batch having at least one workload process to be performed in the database, and each batch having a memory requirement based on the available memory for processing workloads in the database; a scheduler operating to assign a unique priority to each of the at least one workload process in each of the plurality of batches, the unique priority provides an order in which each workload process is executed in the database; and an execution manager operating to execute the at least one workload process in each of the plurality of batches in accordance with the unique priority assigned to each workload process.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="176.02mm" wi="216.92mm" file="US08627330-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="215.90mm" wi="187.03mm" orientation="landscape" file="US08627330-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="215.90mm" wi="188.72mm" orientation="landscape" file="US08627330-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="215.90mm" wi="165.18mm" orientation="landscape" file="US08627330-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="215.90mm" wi="169.08mm" orientation="landscape" file="US08627330-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="215.90mm" wi="115.40mm" file="US08627330-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="215.90mm" wi="101.68mm" file="US08627330-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="215.90mm" wi="170.60mm" orientation="landscape" file="US08627330-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE</heading>
<p id="p-0002" num="0001">This application claims the benefit of U.S. Provisional Application Ser. No. 61/042,954, entitled, &#x201c;SCHEDULING MEMORY USAGE OF A WORKLOAD,&#x201d; as filed on Apr. 7, 2008; and U.S. Provisional Application Ser. No. 61/010,132, entitled, &#x201c;PROCESSING BATCH DATABASE WORKLOAD WHILE AVOIDING OVERLOAD&#x201d;, as filed on Jan. 3, 2008. These applications are herein incorporated by reference in their entireties.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">BACKGROUND</heading>
<p id="p-0003" num="0002">Many enterprises, such as companies, corporations, and organizations, are creating and deploying &#x201c;enterprise data warehouses&#x201d; to serve as the single source of corporate data for business intelligence. As referred herein, a data warehouse is a data storage or repository that includes one or more electrical or electronic data storage devices such as computers, servers, computer databases, and the like. Also, as referred herein and understood in the art, business intelligence (BI) includes applications and technologies that work together to collect, provide access to, and analyze data and information about operations of a business or a desired entity. Enterprise data warehouses are expected to both scale to enormous data volumes (hundreds of terabytes) and perform well under increasingly complex workloads, which typically include batch and incremental data loads, batch reports, and/or complex ad hoc queries. A key challenge is to manage each complex workload in order to meet stringent performance objectives. For example, batch load tasks may be required to finish within a specified time window before reports or queries can be serviced, batch reports may issue thousands of &#x201c;roll-up&#x201d; (aggregation) queries that are required to complete within a specified time window, and ad hoc queries may have user-specified deadlines and priorities.</p>
<p id="p-0004" num="0003">Accordingly, there is a desire to have effective workload management of an enterprise data warehouse (EDW) in order to allocate resources therein and execute workloads assigned thereto so as to meet desired performance objectives.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0005" num="0004">Embodiments are illustrated by way of example and not limited in the following figure(s), in which like numerals indicate like elements, in which:</p>
<p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. 1</figref> illustrates a graph of throughput curves for different workloads to show optimum throughput desired to be achieved, in accordance with one embodiment.</p>
<p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. 2</figref> illustrates a workload manager for managing workloads in an enterprise data warehouse (EDW), in accordance with one embodiment.</p>
<p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. 3</figref> illustrates a graph of CPU utilization and memory pressure <b>320</b> against time for an EDW system wherein thrashing has occurred, in accordance with one embodiment.</p>
<p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. 4</figref> illustrates memory pressure curves for the typical equal priority multiprogramming and priority gradient multiprogramming in accordance with one embodiment.</p>
<p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. 5</figref> illustrates a scheduling order process performed by a scheduler in a business intelligence workload manager, in accordance with one embodiment.</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 6</figref> illustrates an admission control policy in accordance with one embodiment.</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 7</figref> illustrates a platform in which a business intelligence workflow manager may be implemented, in accordance with one embodiment.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0013" num="0012">For simplicity and illustrative purposes, the principles of the embodiments are described by referring mainly to examples thereof. In the following description, numerous specific details are set forth in order to provide a thorough understanding of the embodiments. It will be apparent however, to one of ordinary skill in the art, that the embodiments may be practiced without limitation to these specific details. In other instances, well known methods and structures have not been described in detail so as not to unnecessarily obscure the embodiments.</p>
<p id="p-0014" num="0013">A common workload that is handled by an enterprise data warehouse (EDW) system is a batch of queries. The workload-management objective for such a workload is to minimize the response time of the workload. This is important because an EDW system often spends a considerable amount of its operational time running batch workloads such as roll-up queries and reports. The current trend is towards even larger queries and batches as data mining and analytics are increasingly becoming central activities for a large data warehouse. The response time of a batch workload running on an EDW system depends on many factors, such as the number and type of queries, system configuration, number of concurrent streams of queries running on the system, etc. One metric of measuring the response time of a workload is throughput. The throughput is measured in queries completed in a unit time. That is, in the context of a batch of queries, throughput is more concerned with the overall response time for a batch of queries rather than the individual response time of each query in the batch.</p>
<p id="p-0015" num="0014">One way of looking at throughput is with a throughput curve, wherein throughput is plotted against the &#x201c;load&#x201d; on an EDW system. Traditionally, multiprogramming level (MPL) has been used to measure a &#x201c;load&#x201d; and as the manipulated variable of choice for workload management. As referred herein and understood in the art, MPL is the number of processes or jobs that may be active (that is, executed) at a time in a data processing system such as the EDW system. <figref idref="DRAWINGS">FIG. 1</figref> illustrates a graph of throughput curves for two different workloads, a &#x201c;large&#x201d; workload (curve <b>110</b>) that includes several large, resource intensive queries and a &#x201c;medium&#x201d; workload (curve <b>120</b>) that includes several medium queries. In the graph, the x-axis is the multiprogramming level (MPL) and the y-axis is throughput. The throughput curves <b>110</b> and <b>120</b> may be divided into three regions. <figref idref="DRAWINGS">FIG. 1</figref> illustrates these three regions for the throughput curve <b>120</b> of the medium workload: (i) the underload region <b>142</b> (where, by increasing MPL a higher throughput can be achieved), (ii) the pptimal load region <b>144</b> (also known as saturation where by increasing MPL there is not much change in throughput), and (iii) the overload region <b>146</b> (where increasing MPL results in lower throughputs). The overload or thrashing region <b>146</b> is considered an issue for maintaining optimal throughput.</p>
<p id="p-0016" num="0015">An issue with MPL is that as the workload changes, the MPL also needs to be changed. That is, when a system operator or user first confronts a new workload, the precise shape of the throughput curve is unknown to the user, who then has to determine the MPL at which to execute the workload. Typically, the user does not want to be on the left part of the curve, that is, in the underload region <b>142</b>, because it remains possible to increase the throughput by increasing the MPL. However, as the MPL is increased, there is a danger of entering the overload or thrashing region <b>146</b>, wherein higher MPLs mean a lower throughput. Very often, increasing the MPL by even one from an MPL on the boundary of the optimal region <b>144</b> may cause an immediate severe performance deterioration rather than a gradual decline in performance. This issue is further exacerbated in the EDW space by the fact that a typical BI workload may fluctuate rapidly between long, resource intensive queries, and short, less intensive queries. Thus, at each instant of time, the EDW system may experience a different mix of queries that requires a different optimal setting of MPL.</p>
<p id="p-0017" num="0016">As described in various embodiments herein, the goal of workload management is to operate the EDW system in the optimal load region <b>144</b> in accordance with performance objectives by properly admitting, scheduling, and executing queries and allocating resources for such queries. Accordingly, described herein are systems and methods for a BI workload manager that may be employed in an EDW system for running batches of queries or other types of memory-accessing workloads while protecting against both underload and overload. Thus, the BI workload manager aims to maximize the throughput of BI batch workloads while protecting against underload and overload. The BI workload manager may be employed by operators or users of the EDW system that are interested in enhancing or improving the performance of the EDW system. For example, the system operators or users potentially includes in-house personnel of a company that maintains the EDW system, outside consultants setting up a new EDW system or optimizing an existing EDW system in a company. Although various embodiments as described herein refer to an EDW system, it should be understood that such embodiments are also applicable for any electrical, electronic, or other types of data storage device or system without deviating from the scope of the embodiments.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 2</figref> illustrates a workload manager <b>200</b>, such as a workload manager for business intelligence (BI), in accordance with one embodiment. The BI workload manager <b>200</b> includes three components: an admission controller <b>210</b>, a scheduler <b>220</b>, and an execution manager <b>230</b>. These components may be implemented as software programs, software applications, or software modules in an EDW system. In one embodiment, the BI workload manager <b>200</b> is operable to: a) find a manipulated variable whose predicted value is more suitable than MPL for workload management; b) make the system stable over a wide range of this variable, that is, the system does not go either into underload or overload over a wide range of prediction errors for this manipulated variable; and c) once this range for the manipulated variable is known, control the manipulated variable to maintain its value in the middle of this range. Although various embodiments as described herein use queries as exemplary processes, or jobs, in workloads to be executed by the EDW system, it should be understood that such embodiments are also applicable to those workloads that include processes, or jobs, which require access to memory resources in the EDW system.</p>
<p id="p-0019" num="0018">A manipulated variable may be any system resource that causes a bottleneck when processing queries on the EDW system, such as a central processing unit (CPU), a disk drive, and memory and message buffers. While saturation of CPU, disc, or message buffers may be limiting factors, once they are saturated, the throughput cannot be improved or degraded. However, over subscription of memory may lead to serious degradation in throughput performance because of thrashing. Thus, in one embodiment, memory is used as the manipulated variable. Accordingly, the admission controller <b>210</b> uses memory as a basis for admitting queries, wherein each batch is divided into sub-batches such that the memory requirement of queries in each sub-batch adds up to the available memory on the system. In execution control, the execution manager <b>230</b> executes each of these sub-batches of queries on a priority gradient called priority gradient multiprogramming (PGM) process. That is, each query in a sub-batch is executed with a unique priority, that stabilizes the EDW system over a wide range of memory prediction errors. The scheduler <b>220</b> is employed to provide assignment of priorities to the queries. The components <b>210</b>, <b>220</b>, and <b>230</b> are further described below.</p>
<p id="p-0020" num="0019">With memory being used as the manipulated variable of choice in the BI workload manager <b>200</b>, for a query Q<sub>i</sub>, with execution time E<sub>i</sub>, its memory requirement m<sub>i </sub>is given as:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>m</i><sub>i</sub>=max {avg(<i>m</i><sub>ict</sub>)|0<i>&#x3c;t&#xb7;E</i><sub>i</sub>},<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where m<sub>ict </sub>is the memory required by query Q<sub>i </sub>at time t at CPU c and the average is taken over all the CPUs at time t. For a batch of queries Q<sub>1</sub>, Q<sub>2</sub>, . . . , Q<sub>n</sub>&#xb7;W (where a query Q<sub>i </sub>belongs to a workload W), and the average available memory across all CPUs is M, the size of the workload is given as xF where:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>x=&#xb7;m</i><sub>i</sub><i>/M. </i><?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
That is, the workload is a factor x times the size of the average available memory, F=M, across all CPUs. For example, if the memory requirement of queries in a workload adds up to 12 GB and the average available memory across all CPUs in the EDW system is 4 GB, then the workload is of size 3 F.
</p>
<p id="p-0021" num="0020">The aim of execution control by the execution manager <b>230</b> is to stabilize for memory prediction errors. As shown from the above equations, when queries in a workload are executed at the same priority, hereinafter called equal priority multiprogramming or EPM, and the size of the workload is greater than the amount of memory available on an EDW system, system thrashing may occur and in turn produces severe performance deterioration. That is, in an EDW system that employs EPM, all processes, such as queries, are running with the same priority. When a process p page faults, it goes to the wait queue. When its page arrives, it goes to the end of the ready queue. All other processes prior to p in the ready queue are either finished or page fault before the CPU get to p. Thus, all the processes in the EDW system get a &#x201c;fair&#x201d; share of the system resources. EPM is robust for a reasonable range of overestimates. That is, if the size for a workload is overestimated such that actual memory required is less then expected, the throughput will remain in the optimal region. For example, it is found that EPM is stable for workloads between the sizes of &#x2153; F and 1 F. However, EPM is very unstable for underestimates, wherein there is a sudden drop in throughput as the size of the workload increases beyond the available memory.</p>
<p id="p-0022" num="0021">To overcome the aforementioned issue with EPM, a priority gradient multiprogramming (PGM) process is employed by the execution manager <b>230</b>, wherein queries are executed at different priorities such that a gradient of priorities is created. That is, in an EDW system that employs a PGM process, if a highest priority process q page faults, once its required page arrives in memory, the process q will preempt the currently running process in the CPU instead of going to the back of the ready queue. The remaining memory resources are automatically allocated to the query running at the second highest priority level. This continues down the priority gradient, which results in processes such as queries asking for and releasing memory at different rates. The releasing of memory by prioritized processes or jobs in a workload alleviates memory contention, which is a primary cause of thrashing as further discussed later.</p>
<p id="p-0023" num="0022">Accordingly, in one embodiment, the PGM process is a mechanism for executing a batch of queries Q<sub>1</sub>, Q<sub>2</sub>, . . . , Q<sub>n</sub>&#xb7;W (where a query Q<sub>i </sub>belongs to a workload or batch W) in a database on the following conditions provided by the scheduler <b>220</b>:
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0023">a) order all queries: all queries are uniquely ordered according to some ordering function F<sub>ord</sub>, such that, F<sub>ord</sub>(Q<sub>i</sub>)=j, where j&#xb7;[1, . . . , n] and for all i, j&#xb7;n, F<sub>ord</sub>(Q<sub>i</sub>)&#xb7;F<sub>ord</sub>(Q<sub>i</sub>); and</li>
        <li id="ul0002-0002" num="0024">b) pick queries in order and assign priorities in that order: pick query Q<sub>a</sub>, where for Q<sub>a</sub>, F<sub>ord</sub>(Q<sub>a</sub>)=1 and assign the highest priority P<sub>1 </sub>to it. Then, pick query Q<sub>b </sub>where for Q<sub>b</sub>, F<sub>ord</sub>(Q<sub>b</sub>)=2 and assign a priority P<sub>2 </sub>such that P<sub>2</sub>&#x3c;P<sub>1</sub>, and so on until all the queries in the workload have been assigned a priority or the range of permissible priorities runs out.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0024" num="0025">The difference between any two successive priorities, P<sub>i+1 </sub>and P<sub>i </sub>is a step-size constant k. Because most operating systems have a fixed number of allowable priorities, setting k=1 permits for the largest possible number of queries being assigned a valid priority. For example, there are ten queries Q<sub>1</sub>, . . . , Q<sub>10 </sub>in a workload, and the highest permissible priority for a query is 200. If the order function is chosen as F<sub>ord</sub>(Q<sub>i</sub>)=i and the step-size constant is chosen as k=1. then the priority of Q<sub>1 </sub>is 200, Q<sub>2 </sub>is 199 and so on to Q<sub>10 </sub>having a priority of 191. Thus, the workload with these priorities is admitted into the execution manager <b>230</b>. For some database systems where different operations of a query are assigned different priorities by the execution manager <b>230</b>, k may be larger. The ordering function F<sub>ord </sub>may be a function that assigns order based on a system characteristic, such as expected time taken, expected memory usage, or some other system resource characteristics. Alternatively, F<sub>ord </sub>may be a function that assigns a random order, which does not require computation of the aforementioned characteristics.</p>
<p id="p-0025" num="0026">The aforementioned PGM process is operable in an EDW system that employs an operating system with a preemptive priority scheduler. As known in the art, a preemptive priority scheduler is available in a number of existing operating systems to receive a process that arrives at the ready queue, compare the priority of the newly-arrived process with a currently running process, and preempt the CPU for the newly-arrived process if its priority is higher than the priority of the currently running process. The PGM process provides an effective mechanism for stabilizing the right side (overload region <b>146</b>) of the throughput curve as illustrated in <figref idref="DRAWINGS">FIG. 1</figref> in order to protect against overload or thrashing.</p>
<p id="p-0026" num="0027">A main cause of thrashing is severe memory contention, which may be explained in the context of a global page replacement policy that replaces virtual memory pages (or memory frames in the case of physical memory) regardless of the process to which such pages belong. For example, when a process (such as a query) p input to the EDW system requires more memory pages, it starts faulting and taking away pages from other processes. Because these other processes also need those pages, they also fault and take pages from other processes. These faulting processes must use a paging device to swap pages in and out. As they queue up for the paging device, the ready queue empties. As processes wait for the paging device, the CPU utilization drops. The CPU scheduler sees the decreasing CPU utilization and increases the number of processes. The new processes start by taking pages from the existing running processes, which further exacerbates memory contention, and CPU utilization drops further. As a result, the CPU executes more processes, and thrashing occurs with the throughput plunging significantly because the processes are spending all their time in page faulting.</p>
<p id="p-0027" num="0028"><figref idref="DRAWINGS">FIG. 3</figref> illustrates a graph of CPU utilization <b>310</b> and memory pressure <b>320</b> against time for an EDW system wherein thrashing has occurred. As shown in <figref idref="DRAWINGS">FIG. 3</figref>, once the workload starts, the CPU quickly attains a 100% utilization. The memory pressure slowly begins to build up as an increasing number of processes demand memory. Once the memory pressure builds up beyond a certain point the CPU utilization starts to drop and very quickly falls to around 20% utilization. Only once the memory pressure begins to go down, the CPU utilization goes up. Thus, if the rise in memory pressure may be halted without losing too much CPU utilization, then there is a solution to the issue of thrashing, which the PGM process achieves. This is further explained with reference to <figref idref="DRAWINGS">FIG. 4</figref>, which illustrates the memory pressure curves (system memory used as a function of time) of two memory profiles or schemes for a typical workload.</p>
<p id="p-0028" num="0029">As illustrated in <figref idref="DRAWINGS">FIG. 4</figref>, the peak memory requirement for the PGM profile or process <b>410</b> is substantially lower than that of the EPM profile or scheme <b>420</b>. This clearly indicates that PGM process reduces the peak memory requirement. This happens because the PGM process starts freeing memory sooner than EPM. That is, the higher priority queries get all the resource it needs and gets done quicker to free up its assigned memory. The saw-tooth behavior of the PGM curve <b>410</b> is an indicator that as the higher priority queries get done, they release their memory to reduce the peak value of memory pressure. Furthermore, the initial slope of the PGM memory profile has a lesser slope than that of the EPM memory profile. This happens because the queries lower down in the priority order do not get a chance to ask for all the memory they need. Thus, by requesting memory at a slower rate and releasing memory quicker, the PGM process eliminates or at least reduces thrashing. Thrashing is eliminated if the peak memory requirement never gets so high as to cause thrashing, and even if it becomes high, the peak values remain less than those peak values in EPM.</p>
<p id="p-0029" num="0030">The execution control by the execution manager <b>230</b> relies on unique priority given to queries or that such queries be arranged in some order F<sub>ord</sub>. In one embodiment, as noted earlier, the order of priorities is provided by the scheduler <b>220</b> so as to implement the PGM profile, and such an order may be randomly chosen. In another embodiment, if a workload is in the optimal region <b>144</b>, the order of priorities provided by the scheduler <b>220</b> may be randomly chosen. However, it is noted that the throughput penalty for being in the overload region <b>146</b> is much higher than being in the underload region <b>142</b>. Hence, the scheduler <b>220</b> may provide a scheduling order that stabilizes the EDW system for memory underestimation errors. In one embodiment, the scheduling order provided is the Largest Memory Priority (LMP) order as described in U.S. Provisional Patent Application Ser. No. 61/042,954 as noted above.</p>
<p id="p-0030" num="0031">Under the LMP order, the queries are assigned priority in the order of their memory requirement. As the name implies, the query with the largest memory requirement is given the highest priority. That is, as illustrated in <figref idref="DRAWINGS">FIG. 5</figref>, a LMP order F<sub>LMP </sub>of a batch of queries, Q<sub>1</sub>, Q<sub>2</sub>, . . . , Q<sub>n</sub>&#xb7;W (where a query Q<sub>i </sub>belongs to a workload or batch W), in a database is arranged as follows: At <b>510</b>, the memory requirement M<sub>i </sub>is computed for each query Q<sub>i</sub>. At <b>520</b>, the queries are arranged in descending order F<sub>LMP</sub>, of memory requirement m<sub>i</sub>. such that, F<sub>LMP</sub>(Q<sub>i</sub>)=j, where j&#xb7;[1, . . . , n], and for all i, j&#xb7;n, F<sub>LMP</sub>(Q<sub>i</sub>)&#xb7;F<sub>LMP</sub>(Q<sub>j</sub>). At <b>530</b>, the queries are picked in the order of F<sub>LMP </sub>and their priorities are assigned in the same order. For example, query Q<sub>a </sub>is picked, where F<sub>LMP</sub>(Q<sub>a</sub>)=1 with the highest priority P<sub>1 </sub>assigned to it. Next, query Q<sub>b </sub>is picked, where F<sub>LMP</sub>(Q<sub>b</sub>)=2 with a second highest priority P<sub>2 </sub>assigned to it, such that P<sub>2</sub>&#x3c;P<sub>1</sub>. This is performed until all the queries in a sub-batch have been assigned a priority.</p>
<p id="p-0031" num="0032">With LMP ordering, the query with the largest memory requirement gets the highest priority and is amongst the earliest to be done and releases its memory. Typically, queries in a workload start building up their memories, and the memory requirement continues to rise unless some memory is released. In EPM, this causes thrashing because all queries have the same priority. In a PGM process, the optimal region <b>144</b> is extended as queries finish up and release their memories. LMP ordering further extenuates the PGM process by giving the highest priority to the query that would release the largest amount of memory.</p>
<p id="p-0032" num="0033">With memory as the manipulated variable of choice and the aforementioned scheduler <b>220</b> and execution manager <b>230</b>, the admission controller <b>210</b> employs an admission policy in accordance with a process as illustrated in <figref idref="DRAWINGS">FIG. 6</figref>.</p>
<p id="p-0033" num="0034">At <b>610</b>, a workload such as a batch of queries are divided into sub-batches such that each sub-batch is of a size 1 F, that is, utilizing substantially the size of the average available memory across all CPUs in the EDW system. Known methods may be used for dividing a batch into sub-batches. For example, the division of a batch of queries into sub batches may be reduced to a 1-dimensional bin packing issue that includes packing irregular 1-dimensional object into bins of fixed size such that the number of bins is a minimum, with 1 F being the size of the bin and the queries as the packages that need to be packed in the bins. This issue is a nondeterministic polynomial-time hard (NP-Hard) issue. Any known approximate algorithm, such as the First Fit Decreasing (FFD) algorithm, may be used to solve such an issue. FFD has known bounds of the number of bins being at most (11/9)+1 times the optimal number of bins. Accordingly, applying to the BI workload manager <b>200</b>, FFD may be rewritten as: a) arranging queries of the batch in descending order of memory requirement m<sub>i</sub>; b) for every query Q<sub>i </sub>in this order, insert or assign Q<sub>i </sub>in the first sub batch that can accommodate the query (without the size of the sub batch exceeding 1 F); and c) repeating step b until all the queries have been assigned to a sub-batch.</p>
<p id="p-0034" num="0035">At <b>620</b>, each sub-batch has its queries therein assigned with unique priorities based on the LMP ordering by the scheduler <b>220</b>.</p>
<p id="p-0035" num="0036">At <b>630</b>, each sub-batch of queries is executed by the execution manager <b>230</b> in accordance with assigned priorities of the queries in the PGM process.</p>
<p id="p-0036" num="0037">At <b>640</b>, once execution of a sub-batch is completed or done, a new sub-batch is introduced for execution by the BI workload manager <b>200</b> until all sub-batches of the workload are executed. In one embodiment, a batch (or sub-batch) of queries is done when all three of the following conditions are satisfied: a) When a predetermined threshold T<sub>finish </sub>number of queries are done; b) the memory pressure falls below a predetermined threshold T<sub>mem</sub>; and c) the average CPU utilization falls below a predetermined threshold T<sub>CPU</sub>.</p>
<p id="p-0037" num="0038"><figref idref="DRAWINGS">FIG. 7</figref> illustrates a block diagram of a computerized system <b>700</b> that is operable to be used as a platform for implementing the BI workload manager <b>200</b>. The computerized system <b>700</b> may be a standalone BI workload manager <b>200</b> that is in communication with or has access to an EDW system to manage the workloads therein. Alternatively, the computerized system <b>700</b> may be a part of the EDW system, wherein memory storage components in the system <b>700</b> may be used to implement a data repository and the BI workload manager <b>200</b>.</p>
<p id="p-0038" num="0039">The computer system <b>700</b> includes one or more processors, such as processor <b>702</b>, providing an execution platform for executing software. Thus, the computerized system <b>700</b> includes one or more single-core or multi-core processors of any of a number of computer processors, such as processors from Intel, AMD, and Cyrix. As referred herein, a computer processor may be a general-purpose processor, such as a central processing unit (CPU) or any other multi-purpose processor or microprocessor. A computer processor also may be a special-purpose processor, such as a graphics processing unit (GPU), an audio processor, a digital signal processor, or another processor dedicated for one or more processing purposes. Commands and data from the processor <b>702</b> are communicated over a communication bus <b>704</b> or through point-to-point links with other components in the computer system <b>700</b>.</p>
<p id="p-0039" num="0040">The computer system <b>700</b> also includes a main memory <b>706</b> where software is resident during runtime, and a secondary memory <b>708</b>. The secondary memory <b>708</b> may also be a computer-readable medium (CRM) that may be used to store software programs, applications, or modules that implement components in the BI workflow manager <b>200</b>. Thus, the CRM is operable to store software programs, applications, or modules that implement the components <b>210</b>, <b>220</b>, and <b>230</b> in the BI workload manager <b>200</b>. The main memory <b>706</b> and secondary memory <b>708</b> (and an optional removable storage unit <b>714</b>) each includes, for example, a hard disk drive <b>710</b> and/or a removable storage drive <b>712</b> representing a floppy diskette drive, a magnetic tape drive, a compact disk drive, etc., or a nonvolatile memory where a copy of the software is stored. In one example, the secondary memory <b>708</b> also includes ROM (read only memory), EPROM (erasable, programmable ROM), EEPROM (electrically erasable, programmable ROM), or any other electronic, optical, magnetic, or other storage or transmission device capable of providing a processor or processing unit with computer-readable instructions. The computer system <b>700</b> includes a display <b>720</b> connected via a display adapter <b>722</b>, user interfaces comprising one or more input devices <b>718</b>, such as a keyboard, a mouse, a stylus, and the like. However, the input devices <b>718</b> and the display <b>720</b> are optional. A network interface <b>730</b> is provided for communicating with other computer systems via, for example, a network.</p>
<p id="p-0040" num="0041">What has been described and illustrated herein is an embodiment along with some of its variations. The terms, descriptions and figures used herein are set forth by way of illustration only and are not meant as limitations. Those skilled in the art will recognize that many variations are possible within the spirit and scope of the subject matter, which is intended to be defined by the following claims&#x2014;and their equivalents&#x2014;in which all terms are meant in their broadest reasonable sense unless otherwise indicated.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A workload manager for managing a workload in a database, comprising:
<claim-text>an admission controller to divide the workload into a plurality of batches, with each batch having at least one workload process to be performed in the database, and each batch having a memory requirement based on available memory for processing the workload;</claim-text>
<claim-text>a scheduler, executed by a hardware processor, to assign a unique priority to each workload process in each of the plurality of batches, the unique priorities to provide an order in which each workload process is executed in the database, wherein each assigned unique priority is different from any other of the assigned unique priorities; and</claim-text>
<claim-text>an execution manager to execute each workload process in each of the plurality of batches in accordance with the unique priority assigned to each workload process.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The workload manager of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the workload comprises a plurality of database queries, and each of the plurality of batches includes at least one database query as the at least one workload process.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The workload manager of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the admission controller is further to arrange each workload process in each of the plurality of batches in order of memory requirement so as to divide the workload into the plurality of batches.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The workload manager of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the scheduler is to randomly assign the unique priority to each workload process in each of the plurality of batches.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The workload manager of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the scheduler is to assign the unique priority to each workload process in each of the plurality of batches based on a memory requirement of the workload process.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The workload manager of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein each workload process in a first batch of the plurality of batches comprises a plurality of workload processes, and a workload process of the plurality of workload processes with the highest memory requirement in the first batch is assigned a highest priority as its unique priority.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The workload manager of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the scheduler is to assign the unique priority to each of the at least one workload process in each of the plurality of batches based on a resource characteristic in the database.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. A method for managing a workload of queries in a database, comprising:
<claim-text>dividing the workload of queries into a plurality of batches of queries such that each batch has a memory requirement of no more than an average available memory for processing workloads in the database;</claim-text>
<claim-text>assigning a unique priority to each of the queries, the unique priorities providing an order in which each query is executed in the database, wherein each assigned unique priority is different from any other of the assigned unique priorities; and</claim-text>
<claim-text>executing each of the plurality of batches by:</claim-text>
<claim-text>a) executing at least one query in one batch based on the unique priority assigned to the at least one query in the one batch;</claim-text>
<claim-text>b) determining whether the execution of the one batch is done; and</claim-text>
<claim-text>c) executing another batch of the plurality of batches once the execution of the selected batch is done.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein determining whether the execution of the one batch is done comprises at least one of:
<claim-text>first determining whether a predetermined threshold number of queries in the one batch is done;</claim-text>
<claim-text>second determining whether a usage of memory of the database by the one batch falls below a predetermined memory threshold; and</claim-text>
<claim-text>third determining whether an average utilization of a processing power of the database by the one batch falls below a predetermined processing threshold.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein determining whether the execution of the one batch is done comprises the steps of first determining, second determining, and third determining.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein dividing the workload of queries into a plurality of batches of queries comprises:
<claim-text>arranging the queries in the workload in descending order of memory requirement;</claim-text>
<claim-text>assigning a first one of the queries in the arranged descending order to one of the plurality of batches that is able to accommodate each query without exceeding an average available memory for processing workloads in the database; and</claim-text>
<claim-text>repeating the step of assigning each query, in the arranged descending order, until all of the queries in the workload are assigned to the plurality of batches.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein assigning a unique priority to each of the queries in each of the plurality of batches comprises:
<claim-text>uniquely ordering the queries in each of the plurality of batches; and</claim-text>
<claim-text>assigning the unique priority to each of the queries in each batch from highest priority to lowest priority in accordance with the unique ordering of the queries in each batch.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein uniquely ordering the queries comprises:
<claim-text>uniquely ordering the queries in each of the plurality of batches according to an ordering function that randomly orders the queries.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein unique ordering the queries comprises:
<claim-text>uniquely ordering the queries in each of the plurality of batches according to an ordering function based on a memory requirement of each of the queries in each batch.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein uniquely ordering the queries in each of the plurality of batches according to an ordering function further comprises:
<claim-text>ranking a query that has a highest memory requirement in the batch as a first query, and ranking a query that has a lowest memory requirement in the batch as a last query.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein assigning the unique priority to each of the queries in each batch includes assigning the unique priority from highest priority to lowest priority, wherein the highest priority is assigned to the first ranked query and the lowest priority is assigned to the last-ranked query.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. A non-transitory computer readable medium on which is encoded code that when executed by a processing unit, the code is to:
<claim-text>divide the workload of queries into a plurality of batches of queries such that each batch has a memory requirement of no more than an average available memory for processing workloads in the database;</claim-text>
<claim-text>assign a unique priority to each of the queries in each of the plurality of batches, the unique priorities to provide an order in which each workload process is executed in the database, wherein each assigned unique priority is different from any other of the assigned unique priorities; and</claim-text>
<claim-text>execute each of the plurality of batches, wherein to execute each of the plurality of batches, the code is to:</claim-text>
<claim-text>a) execute one of the plurality of batches based on the unique priority assigned to each of the queries in the one batch;</claim-text>
<claim-text>b) determine whether the execution of the one batch is done; and</claim-text>
<claim-text>c) execute another one of the plurality of batches once the execution of the one batch is done.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The non-transitory computer readable medium of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the code to divide the workload of queries into a plurality of batches of queries comprises code to:
<claim-text>arrange the queries in the workload in descending order of memory requirement;</claim-text>
<claim-text>assign a first one of the queries in the arranged descending order to one of the plurality of batches that is able to accommodate each query without exceeding an average available memory for processing the workloads in the database; and</claim-text>
<claim-text>repeat the assignment of each query, in the arranged descending order, until all of the queries in the workload are assigned to the plurality of batches.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The non-transitory computer readable medium of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the code to assign a unique priority to each of the queries in each of the plurality of batches comprises code to:
<claim-text>uniquely order the queries in each of the plurality of batches; and</claim-text>
<claim-text>assign the unique priority to each of the queries in each batch from highest priority to lowest priority in accordance with the unique ordering of the queries in each batch.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein uniquely ordering the queries comprises:
<claim-text>uniquely ordering the queries in each of the plurality of batches according to an ordering function that randomly orders the queries; and</claim-text>
<claim-text>uniquely ordering the queries in each of the plurality of batches according to an ordering function that is based a memory requirement of each of the queries in each batch. </claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
