<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625930-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625930</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13312223</doc-number>
<date>20111206</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>36</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382284</main-classification>
<further-classification>382128</further-classification>
<further-classification>382133</further-classification>
<further-classification>422 63</further-classification>
<further-classification>436 46</further-classification>
</classification-national>
<invention-title id="d2e43">Digital microscope slide scanning system and methods</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>4635293</doc-number>
<kind>A</kind>
<name>Watanabe</name>
<date>19870100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5036326</doc-number>
<kind>A</kind>
<name>Andrieu et al.</name>
<date>19910700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5123056</doc-number>
<kind>A</kind>
<name>Wilson</name>
<date>19920600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5140647</doc-number>
<kind>A</kind>
<name>Ise et al.</name>
<date>19920800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5428690</doc-number>
<kind>A</kind>
<name>Bacus et al.</name>
<date>19950600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382128</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>5818525</doc-number>
<kind>A</kind>
<name>Elabd</name>
<date>19981000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>5833607</doc-number>
<kind>A</kind>
<name>Chou et al.</name>
<date>19981100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>5880778</doc-number>
<kind>A</kind>
<name>Akagi</name>
<date>19990300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>5982951</doc-number>
<kind>A</kind>
<name>Katayama et al.</name>
<date>19991100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6044181</doc-number>
<kind>A</kind>
<name>Szeliski et al.</name>
<date>20000300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6101265</doc-number>
<kind>A</kind>
<name>Bacus et al.</name>
<date>20000800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382133</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6195093</doc-number>
<kind>B1</kind>
<name>Nelson et al.</name>
<date>20010200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>6215892</doc-number>
<kind>B1</kind>
<name>Douglass et al.</name>
<date>20010400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382128</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>6239870</doc-number>
<kind>B1</kind>
<name>Heuft</name>
<date>20010500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>6400395</doc-number>
<kind>B1</kind>
<name>Hoover et al.</name>
<date>20020600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>6416477</doc-number>
<kind>B1</kind>
<name>Jago</name>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>6418236</doc-number>
<kind>B1</kind>
<name>Ellis et al.</name>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382128</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>6424752</doc-number>
<kind>B1</kind>
<name>Katayama et al.</name>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>6456323</doc-number>
<kind>B1</kind>
<name>Mancuso et al.</name>
<date>20020900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>6463172</doc-number>
<kind>B1</kind>
<name>Yoshimura</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>6674881</doc-number>
<kind>B2</kind>
<name>Bacus et al.</name>
<date>20040100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382128</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>6773677</doc-number>
<kind>B2</kind>
<name>Thorne, IV et al.</name>
<date>20040800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>6785427</doc-number>
<kind>B1</kind>
<name>Zhou et al.</name>
<date>20040800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>6800249</doc-number>
<kind>B2</kind>
<name>de la Torre-Bueno</name>
<date>20041000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>422 63</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>6941029</doc-number>
<kind>B1</kind>
<name>Hatori</name>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>7226788</doc-number>
<kind>B2</kind>
<name>De La Torre-Bueno</name>
<date>20070600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>436 46</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>7869641</doc-number>
<kind>B2</kind>
<name>Wetzel et al.</name>
<date>20110100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382128</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>7876948</doc-number>
<kind>B2</kind>
<name>Wetzel et al.</name>
<date>20110100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382133</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>7961216</doc-number>
<kind>B2</kind>
<name>Grindstaff et al.</name>
<date>20110600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>8098956</doc-number>
<kind>B2</kind>
<name>Tatke et al.</name>
<date>20120100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>US</country>
<doc-number>8116543</doc-number>
<kind>B2</kind>
<name>Perz et al.</name>
<date>20120200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382128</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>US</country>
<doc-number>8369591</doc-number>
<kind>B2</kind>
<name>Perz et al.</name>
<date>20130200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382128</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00033">
<document-id>
<country>US</country>
<doc-number>2002/0018589</doc-number>
<kind>A1</kind>
<name>Beuker et al.</name>
<date>20020200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00034">
<document-id>
<country>US</country>
<doc-number>2002/0090120</doc-number>
<kind>A1</kind>
<name>Wetzel et al.</name>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382128</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00035">
<document-id>
<country>US</country>
<doc-number>2002/0163582</doc-number>
<kind>A1</kind>
<name>Gruber et al.</name>
<date>20021100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00036">
<document-id>
<country>US</country>
<doc-number>2003/0048949</doc-number>
<kind>A1</kind>
<name>Bern et al.</name>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00037">
<document-id>
<country>US</country>
<doc-number>2003/0234866</doc-number>
<kind>A1</kind>
<name>Cutler</name>
<date>20031200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00038">
<document-id>
<country>US</country>
<doc-number>2004/0009098</doc-number>
<kind>A1</kind>
<name>De La Torre-Bueno</name>
<date>20040100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>422 63</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00039">
<document-id>
<country>US</country>
<doc-number>2004/0071269</doc-number>
<kind>A1</kind>
<name>Wang et al.</name>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00040">
<document-id>
<country>US</country>
<doc-number>2004/0119817</doc-number>
<kind>A1</kind>
<name>Maddison et al.</name>
<date>20040600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00041">
<document-id>
<country>US</country>
<doc-number>2005/0104902</doc-number>
<kind>A1</kind>
<name>Zhang et al.</name>
<date>20050500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00042">
<document-id>
<country>US</country>
<doc-number>2005/0110882</doc-number>
<kind>A1</kind>
<name>Fukuda</name>
<date>20050500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00043">
<document-id>
<country>US</country>
<doc-number>2005/0123181</doc-number>
<kind>A1</kind>
<name>Freund et al.</name>
<date>20050600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382128</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00044">
<document-id>
<country>US</country>
<doc-number>2005/0123211</doc-number>
<kind>A1</kind>
<name>Wong et al.</name>
<date>20050600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00045">
<document-id>
<country>US</country>
<doc-number>2005/0213849</doc-number>
<kind>A1</kind>
<name>Kreang-Arekul et al.</name>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00046">
<document-id>
<country>US</country>
<doc-number>2006/0028549</doc-number>
<kind>A1</kind>
<name>Grindstaff et al.</name>
<date>20060200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00047">
<document-id>
<country>US</country>
<doc-number>2006/0034543</doc-number>
<kind>A1</kind>
<name>Bacus et al.</name>
<date>20060200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00048">
<document-id>
<country>US</country>
<doc-number>2006/0098737</doc-number>
<kind>A1</kind>
<name>Sethuraman et al.</name>
<date>20060500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00049">
<document-id>
<country>US</country>
<doc-number>2006/0120925</doc-number>
<kind>A1</kind>
<name>Takayama et al.</name>
<date>20060600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00050">
<document-id>
<country>US</country>
<doc-number>2006/0204072</doc-number>
<kind>A1</kind>
<name>Wetzel et al.</name>
<date>20060900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382133</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00051">
<document-id>
<country>US</country>
<doc-number>2006/0213994</doc-number>
<kind>A1</kind>
<name>Faiz et al.</name>
<date>20060900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00052">
<document-id>
<country>US</country>
<doc-number>2007/0030529</doc-number>
<kind>A1</kind>
<name>Eichhorn et al.</name>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00053">
<document-id>
<country>US</country>
<doc-number>2007/0031062</doc-number>
<kind>A1</kind>
<name>Pal et al.</name>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00054">
<document-id>
<country>US</country>
<doc-number>2008/0152258</doc-number>
<kind>A1</kind>
<name>Tulkki</name>
<date>20080600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00055">
<document-id>
<country>US</country>
<doc-number>2008/0180550</doc-number>
<kind>A1</kind>
<name>Gulliksson</name>
<date>20080700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00056">
<document-id>
<country>US</country>
<doc-number>2008/0240613</doc-number>
<kind>A1</kind>
<name>Dietz et al.</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00057">
<document-id>
<country>US</country>
<doc-number>2008/0278518</doc-number>
<kind>A1</kind>
<name>Mei et al.</name>
<date>20081100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00058">
<document-id>
<country>US</country>
<doc-number>2012/0076411</doc-number>
<kind>A1</kind>
<name>Dietz et al.</name>
<date>20120300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00059">
<document-id>
<country>US</country>
<doc-number>2012/0076436</doc-number>
<kind>A1</kind>
<name>Dietz et al.</name>
<date>20120300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00060">
<document-id>
<country>US</country>
<doc-number>2012/0092481</doc-number>
<kind>A1</kind>
<name>Dietz et al.</name>
<date>20120400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00061">
<document-id>
<country>WO</country>
<doc-number>WO 2008/118886</doc-number>
<kind>A1</kind>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00062">
<othercit>PCT/US08/58077 Written Opinion dated Aug. 21, 2008.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00063">
<othercit>PCT/US08/58077 IPRP dated Sep. 29, 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>10</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>None</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>6</number-of-drawing-sheets>
<number-of-figures>12</number-of-figures>
</figures>
<us-related-documents>
<division>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>12054309</doc-number>
<date>20080324</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>8098956</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13312223</doc-number>
</document-id>
</child-doc>
</relation>
</division>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60896832</doc-number>
<date>20070323</date>
</document-id>
</us-provisional-application>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60896852</doc-number>
<date>20070323</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120076391</doc-number>
<kind>A1</kind>
<date>20120329</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Tatke</last-name>
<first-name>Lokesh</first-name>
<address>
<city>Sunnyvale</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Somwanshi</last-name>
<first-name>Suraj</first-name>
<address>
<city>Cupertino</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Sabata</last-name>
<first-name>Bikash</first-name>
<address>
<city>Beverly Hills</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="004" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Allen</last-name>
<first-name>Ronald L.</first-name>
<address>
<city>San Jose</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="005" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Patil</last-name>
<first-name>Suhas</first-name>
<address>
<city>Pune</city>
<country>IN</country>
</address>
</addressbook>
<residence>
<country>IN</country>
</residence>
</us-applicant>
<us-applicant sequence="006" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Chivate</last-name>
<first-name>Sujit</first-name>
<address>
<city>Pune</city>
<country>IN</country>
</address>
</addressbook>
<residence>
<country>IN</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Tatke</last-name>
<first-name>Lokesh</first-name>
<address>
<city>Sunnyvale</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Somwanshi</last-name>
<first-name>Suraj</first-name>
<address>
<city>Cupertino</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Sabata</last-name>
<first-name>Bikash</first-name>
<address>
<city>Beverly Hills</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="004" designation="us-only">
<addressbook>
<last-name>Allen</last-name>
<first-name>Ronald L.</first-name>
<address>
<city>San Jose</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="005" designation="us-only">
<addressbook>
<last-name>Patil</last-name>
<first-name>Suhas</first-name>
<address>
<city>Pune</city>
<country>IN</country>
</address>
</addressbook>
</inventor>
<inventor sequence="006" designation="us-only">
<addressbook>
<last-name>Chivate</last-name>
<first-name>Sujit</first-name>
<address>
<city>Pune</city>
<country>IN</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Ventana Medical Systems, Inc.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Ventana Medical Systems, Inc.</orgname>
<role>02</role>
<address>
<city>Tucson</city>
<state>AZ</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Tsai</last-name>
<first-name>Tsung-Yin</first-name>
<department>2668</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Provided herein are systems methods including a design of a microscope slide scanner for digital pathology applications which provides high quality images and automated batch-mode operation at low cost. The instrument architecture is advantageously based on a convergence of high performance, yet low cost, computing technologies, interfaces and software standards to enable high quality digital microscopy at very low cost. Also provided is a method based in part on a stitching method that allows for dividing an image into a number of overlapping tiles and reconstituting the image with a magnification without substantial loss of accuracy. A scanner is employed in capturing snapshot images. The method allows for overlapping images captured in consecutive snapshots.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="134.03mm" wi="138.68mm" file="US08625930-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="177.12mm" wi="150.96mm" file="US08625930-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="195.07mm" wi="183.56mm" file="US08625930-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="229.36mm" wi="195.07mm" orientation="landscape" file="US08625930-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="195.24mm" wi="135.47mm" orientation="landscape" file="US08625930-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="218.19mm" wi="182.71mm" orientation="landscape" file="US08625930-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="189.48mm" wi="185.76mm" orientation="landscape" file="US08625930-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE</heading>
<p id="p-0002" num="0001">This application is a divisional application of U.S. patent application Ser. No. 12/054,309, filed Mar. 24, 2008, now U.S. Pat. No. 8,098,956 now allowed, which claims the benefit of U.S. Provisional Application No. 60/896,832, filed Mar. 23, 2007, and U.S. Provisional Application No. 60/896,852, filed Mar. 23, 2007, the contents of which are incorporated herein by reference in their entirety.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">Molecular imaging-Identification of changes in the cellular structures indicative of disease remains a key to the better understanding in medicinal science. Microscopy applications are applicable to microbiology (e.g., gram staining, etc.), Plant tissue culture, animal cell culture (e.g. phase contrast microscopy, etc.), molecular biology, immunology (e.g., ELISA, etc.), cell biology (e.g., immunofluorescence, chromosome analysis, etc.) Confocal microscopy: Time-Lapse and Live Cell Imaging, Series and Three-Dimensional Imaging.</p>
<p id="p-0004" num="0003">There have been advances in confocal microscopy that have unraveled many of the secrets occurring within the cell and the transcriptional and translational level changes can be detected using fluorescence markers. The advantage of the confocal approach results from the capability to image individual optical sections at high resolution in sequence through the specimen. However, there remains a need for systems and methods for digital processing of images of pathological tissue that provide accurate analysis of pathological tissues, at a relatively low cost.</p>
<heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0005" num="0004">The present invention provides method for reconstituting an image comprising dividing an image into a number of overlapping tiles captured in consecutive snapshots and reconstituting the image with a magnification without substantial loss of accuracy. A digital scanner is employed in capturing snapshot images.</p>
<p id="p-0006" num="0005">In one embodiment the method comprises calculating stitch points between two consecutive snapshots acquired by the scanner for stitching adjacent tiles. Images have a common overlap of at least N pixels between them wherein N is greater than 1 pixel.</p>
<p id="p-0007" num="0006">In one embodiment, the overlap between two adjacent tiles is up to 30% of the total pixels in each tile.</p>
<p id="p-0008" num="0007">In another embodiment, the overlap between two adjacent snapshots is up to 25%, up to 20%, up to 15%, up to 10%, up to 5%, or up to 1% of the total pixels in each tile.</p>
<p id="p-0009" num="0008">In yet another embodiment the overlap between two adjacent snapshots is up 1% of the total pixels in each tile.</p>
<p id="p-0010" num="0009">In yet another embodiment the method of the invention does not require alignment between adjacent snapshots.</p>
<p id="p-0011" num="0010">In a further embodiment the method allows more flexibility in motion control while taking the snapshots.</p>
<p id="p-0012" num="0011">In a still further embodiment stitching involves a step with an input comprising a set of two images and an output comprising stitch points for stitching the two images.</p>
<p id="p-0013" num="0012">In yet another embodiment the stitching is carried out through corner points selected in overlapping regions of the two tiles to be stitched. The step comprises determining correspondence between corner points in a minimum overlap region, for example in the left/top (herein after referred as Tile<b>1</b>) and maximum overlap region in the right/bottom (herein after referred as Tile<b>2</b>). The search region around the corner and the number of corners are iteratively increased until a desired match is achieved. The strength of the correspondence computation may be enhanced by employing schemes such as voting, and histograms of displacement values. A confidence value is associated with the computed stitch displacement. The confidence may be computed using methods that include but are not limited to statistical measures and cross correlation values,</p>
<p id="p-0014" num="0013">In yet another embodiment the stitching may be carried out based on finding the difference between an overlap region in the two tiles. For example, a method for computing the difference includes, without limitation, measuring difference of pixel values along two corresponding lines of the two image tiles. This is based in part on the observation that the pixel difference would be minimum along the line when the lines match in the two tiles. The method may include selecting a candidate line in the first tile and searching for the corresponding best match in the second tile. The search may be restricted to the region of expected overlap in the second tile. The search space includes both horizontal and vertical directions. The optimal point is found where the mean pixel difference is minimized. The accuracy of the method in enhanced method by considering a multitude of candidate lines in the first tile and finding the best match in the second tile. A consensus is measured using all candidates. A measure evaluates the goodness of the candidate lines in the first tile. The goodness measure includes but is not restricted to using the variance of the pixels in the line.</p>
<p id="p-0015" num="0014">In yet another embodiment the stitching may be carried out based on cross-correlation using multiple sized windows between the overlapped regions of the two tiles to be stitched. Candidate windows are selected in the first image based on goodness values. A measure of goodness value includes but is not limited to the contrast or variance of the pixels within the window. Candidate matching windows in the second tile are iteratively found by first searching for the candidate using smaller windows and then increasing the size of the windows for more accurate estimates. For each window location on the first tile, multiple candidates are found in the second tile which are qualified and rejected in multiple iterations. The search is extended by iteratively adding candidate windows in the first tile.</p>
<p id="p-0016" num="0015">In a further embodiment the stitch points identify the area of overlap between adjacent images wherein the overlap area is cropped from one of the images such that when the adjacent images are put together the entire scene is rendered without loss of information.</p>
<p id="p-0017" num="0016">In a further embodiment the images are not transformed by blending, warping and there is no significant loss of image information.</p>
<p id="p-0018" num="0017">In a further embodiment the pixels in the overlap region between adjacent tiles are stored as the &#x201c;seams&#x201d; of the stitching.</p>
<p id="p-0019" num="0018">In still a further embodiment the output produces an image at least equivalent to a tiling system that takes an image and through motion control moves the high resolution camera such that the image boundary is aligned with no or negligible overlap of images.</p>
<p id="p-0020" num="0019">Another embodiment of the invention provides a method for correcting a scanned image.</p>
<p id="p-0021" num="0020">In one embodiment the correction employs a flat field correction algorithm.</p>
<p id="p-0022" num="0021">In yet another embodiment the method employs air-blank or glass-blank image in conjunction with solving non-uniform illumination and abrasions.</p>
<p id="p-0023" num="0022">In a further embodiment the correction step comprises:
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0023">capturing a glass blank image or air-blank image;</li>
        <li id="ul0002-0002" num="0024">smoothing the image using Gaussian filter (e.g.; radius <b>10</b>);</li>
        <li id="ul0002-0003" num="0025">dividing each pixel in image by Max of R, G and B plane values and then reciprocating the image;</li>
        <li id="ul0002-0004" num="0026">calculating a multiplying factor for R, G and B planes for each pixel; and</li>
        <li id="ul0002-0005" num="0027">multiplying all the pixels in the given image with multiplying factor determined from the blank image.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0024" num="0028">One embodiment provides a method for reconstituting an image comprising dividing the image into a number of overlapping tiles captured in a series of snapshots and reconstituting the image with a magnification without substantial loss of accuracy; wherein the method comprises:
<ul id="ul0003" list-style="none">
    <li id="ul0003-0001" num="0000">
    <ul id="ul0004" list-style="none">
        <li id="ul0004-0001" num="0029">capturing snapshot images through a digital scanner;</li>
        <li id="ul0004-0002" num="0030">calculating stitch points between two consecutive snapshots defining a first tile and a second tile, respectively; wherein the first and second tile images have a common overlap of at least N pixels between them; wherein the method further comprises:
        <ul id="ul0005" list-style="none">
            <li id="ul0005-0001" num="0031">a. detecting and recording corner points in a minimum overlap region and a maximum overlap region, whereby detected corner points or a selected subset of corner points are sorted and maintained in a list;</li>
            <li id="ul0005-0002" num="0032">b. for each corner point selected in the first tile, determining a set of possible stitch points in the second tile; wherein candidate points are selected using a goodness criterion;</li>
            <li id="ul0005-0003" num="0033">c. maintaining a pair of stitch points that have a matching score that is greater than a defined or computed threshold;</li>
            <li id="ul0005-0004" num="0034">d. computing a displacement between the tiles;</li>
            <li id="ul0005-0005" num="0035">e. calculating a confidence score associated with the computed displacement; and</li>
            <li id="ul0005-0006" num="0036">f. determining a stitching point for stitching the first and second tiles displaying or showing the stitch image for further analysis.</li>
        </ul>
        </li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0025" num="0037">Another embodiment provides a method wherein stitch points identify an area of overlap between tile <b>1</b> and tile <b>2</b>.</p>
<p id="p-0026" num="0038">Another embodiment provides a method further comprising cropping the overlap area from one of the image tiles such that when the first and second tiles are put together an entire scene is rendered without significant loss of information.</p>
<p id="p-0027" num="0039">Yet another embodiment provides a method further comprising correcting a scanned image by:
<ul id="ul0006" list-style="none">
    <li id="ul0006-0001" num="0000">
    <ul id="ul0007" list-style="none">
        <li id="ul0007-0001" num="0040">a. capturing a glass blank image or Air-blank image;</li>
        <li id="ul0007-0002" num="0041">b. smoothing the image;</li>
        <li id="ul0007-0003" num="0042">c. dividing each pixel in the image by Max of R, G and B plane values and then reciprocating the image;</li>
        <li id="ul0007-0004" num="0043">d. computing a multiplying factor for R, G and B planes for each pixel; and</li>
        <li id="ul0007-0005" num="0044">e. multiplying all the pixels in the given image with multiplying factor determined from the blank image to apply flat-field correction to the given image and displaying or storing the corrected image for further analysis.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0028" num="0045">Another aspect of the invention provides a slide rack auto-loader for use in a digital microscope slide scanning system comprising a user removable rack comprising multiple slide holders.</p>
<p id="p-0029" num="0046">In one embodiment the holders hold 20 slides per basket and are arranged with two stacked baskets in four columns.</p>
<p id="p-0030" num="0047">In another embodiment the slide autoloader further comprises a reflective, IR sensor, for example, at a distance of few millimeters from the end of the slides.</p>
<p id="p-0031" num="0048">In yet another embodiment the slide rack auto-loader further comprises a detector which looks at light scattered from the slide.</p>
<p id="p-0032" num="0049">In yet a further embodiment the slide rack auto-loader comprises four laser/detector pairs to scan down each of the four rows of slides.</p>
<p id="p-0033" num="0050">In still another embodiment, as the slide rack is translated vertically, each slide is sensed and the position recorded.</p>
<p id="p-0034" num="0051">In a further embodiment the positions are used for:
<ul id="ul0008" list-style="none">
    <li id="ul0008-0001" num="0000">
    <ul id="ul0009" list-style="none">
        <li id="ul0009-0001" num="0052">determining which slides are present for scanning and/or</li>
        <li id="ul0009-0002" num="0053">determining the required vertical height to safely extract the slide from the rack for scanning.
<br/>
In still another embodiment the invention provides a slide rack autoloader comprising:
</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0035" num="0054">low cost, non-contact slide detector sensor system to determine which slides are present;</p>
<p id="p-0036" num="0055">sensor system to accurately determine slide position for safe slide extraction from rack to scanning system; and</p>
<p id="p-0037" num="0056">set of standard slide racks for compatibility to other slide processing equipment.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0038" num="0057">Features of the invention are set forth with particularity in the appended claims. A better understanding of the features and advantages of the present invention will be obtained by reference to the following detailed description that sets forth illustrative embodiments, in which the principles of the invention are utilized, and the accompanying drawings of which:</p>
<p id="p-0039" num="0058"><figref idref="DRAWINGS">FIG. 1</figref> shows a tile translated in the XY plane according to one embodiment of the invention;</p>
<p id="p-0040" num="0059"><figref idref="DRAWINGS">FIGS. 2 and 3</figref> show a slide rack according to one embodiment of the invention;</p>
<p id="p-0041" num="0060"><figref idref="DRAWINGS">FIG. 4</figref> shows how a detector looks at light scattered from a slide;</p>
<p id="p-0042" num="0061"><figref idref="DRAWINGS">FIG. 5</figref><i>a </i>shows a slide from an end on view in a properly loaded condition. <figref idref="DRAWINGS">FIG. 5</figref><i>b </i>shows the resulting detected laser pulse wherein the width of the pulse z<sub>s </sub>is compared to reference values t<sub>1 </sub>and t<sub>2</sub>;</p>
<p id="p-0043" num="0062"><figref idref="DRAWINGS">FIG. 6</figref><i>a </i>shows two slides in adjacent slots presented in an end on view. <figref idref="DRAWINGS">FIG. 6</figref><i>b </i>shows the resulting detected laser pulses;</p>
<p id="p-0044" num="0063"><figref idref="DRAWINGS">FIGS. 7</figref><i>a</i>, <b>7</b><i>b</i>, <b>8</b><i>a </i>and <b>8</b><i>b </i>illustrate how error conditions are detected. A double-slide condition is shown in <figref idref="DRAWINGS">FIG. 7</figref><i>a </i>along with the detector signals shown in <b>7</b><i>b </i>and a tilted slide condition is shown in <figref idref="DRAWINGS">FIG. 8</figref><i>a </i>along with the detector signals shown in <b>8</b><i>b. </i></p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION OF THE INVENTION</heading>
<p id="p-0045" num="0064">In one embodiment, the invention provides a design of a microscope slide scanner for digital pathology applications which provides high quality images and automated batch-mode operation at low cost. High quality optics and high precision mechanical systems are generally very expensive, and existing products that address this need are priced accordingly. The instrument architecture implemented according to the invention is advantageously based on a convergence of high performance, yet low cost, computing technologies, interfaces and software standards to enable high quality digital microscopy at very low cost.</p>
<p id="p-0046" num="0065">The invention is based in part on a stitching method that allows for dividing an image into a number of overlapping tiles and reconstituting the image with a magnification without substantial loss of accuracy. A scanner is employed in capturing snapshot images. The method allows for overlapping images captured in consecutive snapshots. One embodiment computes the stitch points between two consecutive snapshots acquired by the scanner. These images have a common overlap of at least N pixels between them. N is greater than 1 pixel. The method of the invention allows for significant overlap between consecutive snapshots. The method provides reconstituted images that are a substantially accurate magnification of an original slide with overlaps of up to 30% of the tile. That is, if each snapshot comprises 1000 pixels, the overlap between two adjacent snapshots can be up to 300 pixels. The method provides results of increasing accuracy with overlaps between two adjacent snapshots of up to 25%, up to 20%, up to 15%, up to 10%, up to 5%, or up to 1%. A 1% overlap between two adjacent snapshots containing 1000 pixels each is 10 pixels, which is significantly more than overlap allowed by conventional methods. That is, the invention provides substantially accurate magnified renderings while allowing significant overlap between adjacent snapshots.</p>
<p id="p-0047" num="0066">Allowing a substantial overlap (significantly more than 1 pixel) between adjacent snapshots in the order of 1% or more of the total pixels in the tile provides great flexibility in the design of the scanner. Embodiments of the invention do not require the levels of accuracy in positioning the snapshots required by conventional methods. There is no need to require alignment between adjacent snapshots. In addition, allowing for overlap and providing an accurate method for processing overlapping snapshots provides more flexibility in motion control while taking the snapshots.</p>
<p id="p-0048" num="0067">In one embodiment, the input to the stitching method is a set of two images and output is the stitch points to be used for stitching the two images to form an image combining the features contained in the two images or image tiles. The steps involved include the following&#x2014;
<ul id="ul0010" list-style="none">
    <li id="ul0010-0001" num="0000">
    <ul id="ul0011" list-style="none">
        <li id="ul0011-0001" num="0068">a. Detect and record corner points in the minimum overlap region in the left/top (Tile<b>1</b>) and maximum overlap region in the right/bottom (Tile<b>2</b>). All detected corner points or a selected subset of corner points are sorted and maintained in a list.</li>
        <li id="ul0011-0002" num="0069">b. For each corner point selected in Tile <b>1</b>, the method determines the set of possible stitch points in Tile <b>2</b>. The candidate points are selected using a goodness criterion that is based on but not restricted to the tolerance band around each point in tile <b>1</b>.</li>
        <li id="ul0011-0003" num="0070">c. Maintain the pair of stitch points that have a matching score that is greater than a defined or computed threshold. An example matching score is the cross correlation of the intensity values around the corner point. Other matching scores may be used and are contemplated to be with the scope of the invention.</li>
        <li id="ul0011-0004" num="0071">d. Robust methods are used for computing the final displacement between the tiles. These methods include but are not limited to using the votes associated with each candidate displacement. For example, the displacement histogram of all the corner points of tile <b>1</b> computes the votes. A displacement getting the most votes and satisfying an acceptance criterion is chosen as the final displacement.</li>
        <li id="ul0011-0005" num="0072">e. A confidence score is associated with the computed displacement. An example confidence measure is the cross correlation values. Alternative statistical or non statistical methods can also be used.</li>
        <li id="ul0011-0006" num="0073">f. If a robust displacement estimate is not available then the computation is repeated by iteratively increasing the search region and/or relaxing the goodness criterion of step b above.</li>
        <li id="ul0011-0007" num="0074">g. If the algorithm still fails to get the stitch point return the error code indicating stitching failure.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0049" num="0075">In one embodiment, the stitch points identify the area of overlap between adjacent images. This overlap area is cropped from one of the images such that when the adjacent images are put together the entire scene is rendered without significant loss of information. The images used in this operation are not transformed by blending, warping, etc., therefore there is no significant loss of image information. The output of this operation produces an image at least equivalent to a tiling system that takes an image and through complicated motion control moves the high resolution camera such that the image boundary is aligned (within one pixel) with negligible overlap of images. For example, for a conventional system utilizing a 20&#xd7; microscope objective, the pixel size is about one-half of a micron. A rule of thumb for specifying the resolution of the motion control would be 5-10&#xd7; better than the accuracy a conventional method would be attempting to achieve, therefore the motion control system required by such conventional method would be about 0.05 to 0.1 micron. Use of a 40&#xd7; scanner would require resolutions of the motion control system to 0.025 to 0.05 micron. For example, the Aperio Scanscope T2 utilizes a Renishaw RGH24G tape scale optical encoder with a resolution of 0.05 or 0.1 micron. By contrast, the method of the invention does not require such high level of accuracy in motion control system, allowing the cost of production of the scanner to be greatly reduced.</p>
<p id="p-0050" num="0076">The methods of the invention allow for the pixel values in the overlap regions between the tiles that are not used in the final composed image to be saved. The saved seams are used for evaluating the quality of the stitching and for correcting any introduced errors.</p>
<p id="p-0051" num="0077">The methods of the invention allow for obtaining images of high accuracy while providing much flexibility in the scanner design. Coupling the methods of the invention with high performance PCs, with fast processors, large amounts of memory, and very high capacity disk drives, which today can be obtained at very reasonable cost enables the system designer to relax the specifications and performance requirements for the optical and mechanical subsystems without compromising the quality of the images obtained. Since these subsystems are typically the most expensive parts of a digital microscope, the total cost of the system can be greatly reduced with the approach provided by the present invention.</p>
<p id="p-0052" num="0078">In another embodiment the invention provides a method for correcting a scanned image. In this embodiment, a flat field correction algorithm is employed for correcting the image using air-blank or glass-blank image and solving non-uniform illumination and abrasions.</p>
<p id="p-0053" num="0079">In one embodiment, the method comprises the following steps:
<ul id="ul0012" list-style="none">
    <li id="ul0012-0001" num="0000">
    <ul id="ul0013" list-style="none">
        <li id="ul0013-0001" num="0080">a. Capturing a glass blank image or Air-blank image.</li>
        <li id="ul0013-0002" num="0081">b. Smoothing the image, for example, by using Gaussian filter (radius <b>10</b>)</li>
        <li id="ul0013-0003" num="0082">c. Dividing each pixel in the image by Max of R, G and B plane values and then reciprocating the image.</li>
        <li id="ul0013-0004" num="0083">d. Computing multiplying factor for R, G and B planes for each pixel.</li>
        <li id="ul0013-0005" num="0084">e. Multiplying all the pixels in the given image with multiplying factor determined from blank image to apply flat-field correction to the given image.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0054" num="0085">The image correction method according to the invention is advantageous in that it allows for solving the non-uniform illumination observed on the acquired images. For example, the image acquired may be rectangular (due to the CCD design) and the illumination being projected on the slide for acquiring image may be circular as it comes through the objective.</p>
<p id="p-0055" num="0086">The present invention also provides method for finding a region of interest in a slide of a tissue using a low resolution (thumbnail) image of the whole tissue in the slide to help a scanner to start scanning the slide only in the region of interest and thus avoids the unnecessary scan of the entire slide. In one embodiment, the thumbnail image is capture in a single shot.</p>
<p id="p-0056" num="0087">An embodiment of the invention segments the thumbnail image into different regions of the slide and identifies the different regions including the region that corresponds to the tissue area.</p>
<p id="p-0057" num="0088">A further embodiment first segments and identifies the label region of the glass slide image. Most pathology glass slides have a label attached on the slide. These regions have the barcode and other identifying information printed on a label. In some cases handwritten markings are left on the glass. All such label regions are segmented and identified by finding the bounding box that encloses all the information.</p>
<p id="p-0058" num="0089">An embodiment of the invention identifies the cover-slip boundaries on the glass slide. The tissues are usually placed under a glass coverslip on the slide. The four boundaries of the coverslip are straight lines that are usually aligned to the horizontal or the vertical. However, they may also be at an angle. The invention detects the presence of such straight lines and combines the information to detect the cover slip boundary.</p>
<p id="p-0059" num="0090">An embodiment of the current invention also detects the staining artifacts that appear at the edges of the coverslip and the slides.</p>
<p id="p-0060" num="0091">An embodiment of the invention detects the tissue area within the region left out after the label, staining artifacts, and the cover slip boundaries are detected. Within the remaining area the method segments the pixels into tissue vs. non-tissue. The steps involve the automatic detection of an intensity threshold that separates tissue pixels from the non tissue pixels. Following the detection of a kernel region of tissue, the region is grown to incorporate the entire tissue area. The growing process uses automatically determined hysterisis thresholds that allow light tissue also to be included into the regions. Small hypothesized areas are removed as they usually correspond to dirt or artifacts on the slide. As a final step all the detected tissue areas are merged into a larger region of interest for the scanning process.</p>
<p id="p-0061" num="0092">In one embodiment, the invention provides a method for finding a region of interest using the thumbnail image of the whole tissue in the slide. This helps the scanner to start scanning the slide only in the region of interest and thus avoids the unnecessary scan of the entire slide.</p>
<p id="p-0062" num="0093">The thumbnail image consists of a representation of the glass slide as a whole. This includes the label and the actual tissue spread on the glass slide.</p>
<p id="p-0063" num="0094">Acquiring the thumbnail image has the following benefits.</p>
<p id="p-0064" num="0095">The label from the thumbnail image can be used an identifier. The label image may have a barcode specifying the details of the case, stain type used etc. It may also have label information that may not include a barcode. This can be used as a cross verification tool to correlate the glass slide with the actual image.</p>
<p id="p-0065" num="0096">The thumbnail image also gives a very good indication of the tissue spread on the glass slide. The tissue spread on the glass slide is determined by the algorithm of the invention.</p>
<p id="p-0066" num="0097">The method of the invention allows for saving time while scanning the glass slide, as only the area with the tissue needs to be imaged and the surrounding empty glass slide need not be scanned.</p>
<p id="p-0067" num="0098">The method of the invention also allows better identification of focus points. This helps selection of a proper auto focus algorithm based on the layout of the tissue on the slide.</p>
<p id="p-0068" num="0099">The embodiment of the invention detects the different regions within the slide and identifies the region corresponding to the tissue area of interest. The steps involved in the detection of region of interest are
<ul id="ul0014" list-style="none">
    <li id="ul0014-0001" num="0000">
    <ul id="ul0015" list-style="none">
        <li id="ul0015-0001" num="0100">a. Identify regions containing characters written on the slide. Sometimes there are characters written near labels on the slide. This step first determines the presence of such characters near the label and then tries to identify the region that contains these characters. The detection procedure identifies such regions by a combination of geometrical and color information. Remove the identified regions for searching the tissue region of interest.</li>
        <li id="ul0015-0002" num="0101">b. The dark black regions (like black label strip and corner regions) are also removed from the ROI input gray image. A combination of geometric and color information is used to robustly detect such regions.</li>
        <li id="ul0015-0003" num="0102">c. Identify cover slip lines in the Thumbnail AOI (Area of Interest) input image. A line detector that is biased towards horizontal lines is used for the detection. An example of such a method, but is not restricted to, uses the Hough transform. A set of horizontal edges magnitude peaks are identified in the regions of the images where the cover slip is expected to be present e.g. Lower part (0 to Image_Height/6) and Upper part (Image_Height to Image_Height/3) of the image. The number of such peaks is also a program parameter or is selected based on empirical evidence. A validation step verifies that the detected lines define a valid cover slip region. The detected area is then used to update the bounding box within which the tissue is searched.</li>
        <li id="ul0015-0004" num="0103">d. One embodiment for the identification of tissue region within search bounding box consists of
        <ul id="ul0016" list-style="none">
            <li id="ul0016-0001" num="0104">i. Processing the input RGB image in the L*ab color space.</li>
            <li id="ul0016-0002" num="0105">ii. Enhancing the contrast in the L (Luminosity) channel.</li>
            <li id="ul0016-0003" num="0106">iii. Smoothing the image by applying an appropriate smoothing filter. An example of such a filter is a gaussian with a parameterized kernel size.</li>
            <li id="ul0016-0004" num="0107">iv. Automatically computing a threshold for the pixel intensity such that the tissue pixels are distinguished from the background pixels. One such method for automatic detection comprises finding the large change in the histogram. Compute the histogram and find Maximum of the derivative. Consider only the middle span of the histogram. The threshold value is a function of the detected change in histogram point.</li>
            <li id="ul0016-0005" num="0108">v. Another method of automatically computing the threshold is by using a Mixture of Gaussian model for the foreground pixels.</li>
            <li id="ul0016-0006" num="0109">vi. Yet another method of automatically computing the threshold is the use of OTSU method/algorithm.</li>
            <li id="ul0016-0007" num="0110">vii. Yet another method of automatically computing the threshold is the use of k-means clustering on the intensity values.</li>
            <li id="ul0016-0008" num="0111">viii. Threshold image using the values computed and segment into foreground and background.</li>
            <li id="ul0016-0009" num="0112">ix. Fill Holes in the segmented objects and then grow the foreground region. The region growing procedure uses a second threshold parameter computed from the image data and a hyperparameter set based on empirical data.</li>
            <li id="ul0016-0010" num="0113">x. Another method of automated threshold selection for region growing distinguishes between whole tissue slides (WS) and tissue microarray (TMA) slides. The distinction between TMA and WS is based on using the feature that the TMA has a regular grid of tissue samples placed on the slide. The regularity of grid feature is detected using the frequency spectrum information of the image.</li>
            <li id="ul0016-0011" num="0114">xi. Final optional post processing is done using Morphological opening to smooth borders.</li>
        </ul>
        </li>
        <li id="ul0015-0005" num="0115">e. Identify the regions of interest using the segmented image:
        <ul id="ul0017" list-style="none">
            <li id="ul0017-0001" num="0116">i. Label the segmented objects to get the area and bounding box information for each object.</li>
            <li id="ul0017-0002" num="0117">ii. Combine overlapping object AOIs if any.</li>
            <li id="ul0017-0003" num="0118">iii. Reject smaller objects or objects spanning across the image from left to right with a very small height as these are cover slip artifacts in the image. Reject edge connected objects with high aspect ratio.</li>
        </ul>
        </li>
        <li id="ul0015-0006" num="0119">f. Delete small AOIs identified in the image as these are small artifacts in the image.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0069" num="0120">The invention also provides scanning imaging systems with features that enhance image analysis when combined with the image processing methods disclosed herein.</p>
<p id="p-0070" num="0121">In one embodiment, the invention provides a scanning imaging system with a flexible and adaptive illumination.</p>
<p id="p-0071" num="0122">Illumination: Brightfield microscopy typically uses Kohler illumination, which is a commonly used technique that provides even illumination over the desired field of view. In Kohler illumination, each point in the field of view is illuminated by a &#x201c;cone&#x201d; of light, with a cone angle designed to match the numerical aperture of the microscope objective. The illuminated area is also matched to the field of view of the objective. Kohler illumination typically requires a powerful light source, because to achieve uniform illumination, the light source is imaged into the sample plane at very high magnification, which means that most of the light source energy is rejected by the illumination system. Such light sources are expensive, require large power supplies, generate significant heat, and also have very short lifetimes.</p>
<p id="p-0072" num="0123">To enable low-cost digital microscopy for pathology, it is desirable to use solid-state light sources such as LEDs. These require very low power, which reduces the need for large power supplies and heat sinks, and also have long lifetimes, reducing system maintenance costs. Another advantage of LED illumination is stable spectral output. With lamps, the color spectrum changes drastically with intensity, but this is not the case with LEDs. Microscopes using incandescent lamps often use blue filters in the illumination path, to remove the yellowish tint. With LEDs, this is not required.</p>
<p id="p-0073" num="0124">The invention provides an illumination system that can mimic Kohler illumination system but uses white LEDs, with the following characteristics:
<ul id="ul0018" list-style="none">
    <li id="ul0018-0001" num="0000">
    <ul id="ul0019" list-style="none">
        <li id="ul0019-0001" num="0125">a. Commercially available, inexpensive white LED, consisting of a blue LED die, covered with phosphor coating to broaden the spectrum through the visible region.</li>
        <li id="ul0019-0002" num="0126">b. Adjustable illumination area, to match the field of view of the objective.</li>
        <li id="ul0019-0003" num="0127">c. Adjustable numerical aperture (up to max of 0.5NA), to appropriately fill and match the numerical aperture of the microscope objective.</li>
        <li id="ul0019-0004" num="0128">d. Very low power, typically 50 to 100 mW (versus 10 to 100 Watts with typical lamps)</li>
        <li id="ul0019-0005" num="0129">e. Approximately 20% variation in illumination intensity across the field of view, corrected (flat-fielded) by software calibration techniques.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0074" num="0130">Imaging system: The design described herein can be implemented using a conventional 20&#xd7;/0.5 NA infinity-corrected microscope objective, that is available from several manufacturers. Together with a tube lens, it projects an image of the sample on to a CCD sensor, with pixel count of 1024 by 768, and 4.65 micron pixel pitch. To achieve image pixels with approximately 0.5 micron spacing (which is approximately the same as the resolution of the objective), we need 10&#xd7; magnification from the sample to the CCD. This is achieved by using a tube lens with focal length of 90 mm, which is &#xbd; the focal length of the 180 mm tube lens used in a standard microscope. With this magnification, each image captured by the camera covers a rectangular field of 0.512 mm by 0.384 mm. The invention is not limited to the 1024&#xd7;768 CCD sensor. For example, with appropriate choice of tube lens, higher density arrays such as 1032&#xd7;776 or 1392&#xd7;10<sup>32 </sup>or 1624&#xd7;1224 can be used to increase the rectangular field at the object plane. Choosing higher density, larger CCD arrays, further decreases the scan time by acquiring fewer frames while maintaining optical resolution. The trade-off with increasing array size is the potential difficulty in correcting focus in the frame area if the biological tissue varies widely in height.</p>
<p id="p-0075" num="0131">Coupled with the illumination system described above, the required exposure time to saturate a high-quantum-efficiency CCD is approximately 500 microseconds. Also, since it is possible to drive the LED with significantly higher power, up to 1 or a few Watts, it is possible to reduce the exposure time down to approximately 10 to 50 microseconds</p>
<p id="p-0076" num="0132">In normal operation, the LED current and/or CCD exposure time is adjusted before starting a scan, so that an image of a &#x201c;blank&#x201d; sample will result in nearly a full-scale, or &#x201c;saturated&#x201d; signal on the CCD camera. While scanning, the LED current and CCD exposure time remain fixed at those values.</p>
<p id="p-0077" num="0133">Slide-scanning operation: To scan an entire microscope slide with a &#x201c;tiled image&#x201d; architecture (see <figref idref="DRAWINGS">FIG. 1</figref>), the slide is translated in X and Y (the focus axis is in Z), pausing at each frame location while an image is acquired. Using the camera and imaging system described above, to scan an area of 15 mm by 15 mm requires approximately 1400 image frames. (This number allows for some overlap between adjacent frames, which is required to adaptively stitch the frames together.) To achieve reasonable scan speeds of approximately 5 minutes per microscope slide, it is required to move very quickly from one frame to the next, focus the objective in that location, acquire the focused image, then move to the next frame and repeat the process. The stitching methods of the invention allow for reduced motion control, for example higher motion speed, yet preserve the accuracy of the processed slides.</p>
<p id="p-0078" num="0134">An additional benefit of using LEDs relates to the short duration, high brightness exposure which allows an alternative method of scanning comparable to &#x201c;on-the-fly&#x201d; scanning. Whereby the stage is commanded to execute a constant velocity move and the camera acquires images at a timing that allows coverage, minimal overlap of one frame to the next, of the complete area of interest on the slide. At 500 microsecond and lower, no image blur is encountered at the appropriate choice of stage velocity. By eliminating the step and settle times associated with the aforementioned step-and-repeat method scan times can be further reduced.</p>
<p id="p-0079" num="0135">Another aspect of the invention relates to the flexibility provided in the vibrations allowed while taking the snapshots. For tiled imaging systems, it is critical that system vibrations be minimized and allowed to dampen out before the frame image is captured. Vibration amplitudes that are more than about 1/10 of a pixel would cause blurring of the image. However, any delays to allow vibrations to dampen can increase the scan time considerably. For example, a 200 ins delay at each frame, multiplied by 1400 frames, would add over 4 minutes to the total scan time for a 15 mm by 15 mm region.</p>
<p id="p-0080" num="0136">By providing a very bright illumination source coupled with a very sensitive detector, it is possible (as described above) to capture images with very short exposure times. If the exposure time is short enough relative to the vibration velocity of the sample, it is acceptable to capture the image before the vibrations have dampened to sub-pixel amplitudes. Effectively, the short exposure is acting as a strobe light to freeze an image of the moving sample. The disadvantage to this technique is that the image is captured at an unknown phase of the vibration ring-down process. The result is that the XY location of the sample is not precisely known, and a software technique will be required to adaptively stitch adjacent images together at the correct relative positions. The stitching techniques described herein allow the user to overcome this problem.</p>
<p id="p-0081" num="0137">This technique can be extended to use feedback from electronic acceleration sensors, to accurately time the image acquisition to occur at a certain phase in the vibration ring-down profile. For example, the image could be acquired at a peak of vibration excursion, when the vibration velocity is near zero. Another mode of operation would be to capture the image at a zero-crossing of the measured acceleration, which would allow for image acquisition with the minimized vibrational position uncertainty. The accelerometer can also be used to detect when external vibrations, such as a user bumping against the instrument, may have caused imaging blurring. In this case, the portion of the image that is suspect can be automatically re-scanned, or a warning can be displayed to the user.</p>
<p id="p-0082" num="0138">The embodiments described here allow for a low-cost, high speed, tiled imaging system. Other features of the system of the invention include:
<ul id="ul0020" list-style="none">
    <li id="ul0020-0001" num="0000">
    <ul id="ul0021" list-style="none">
        <li id="ul0021-0001" num="0139">a. bright light source</li>
        <li id="ul0021-0002" num="0140">b. imaging detector with high quantum efficiency</li>
        <li id="ul0021-0003" num="0141">c. image capture without regard to vibrations, both internally-generated and externally-generated.</li>
        <li id="ul0021-0004" num="0142">d. software algorithms to correct for position uncertainty due to vibration</li>
        <li id="ul0021-0005" num="0143">e. optionally, the invention may use feedback from acceleration sensors to fine-tune the timing of image capture to minimize blurring, or to minimize position uncertainty due to vibration.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0083" num="0144">In another embodiment, the invention provides a slide auto-loader system: The design of a microscope slide scanner for digital pathology applications generally requires the user to scan many specimen slides in a high throughput fashion. Consequently, the instrument must allow the user to load many slides at a single time. A slide auto-loader system should:
<ul id="ul0022" list-style="none">
    <li id="ul0022-0001" num="0000">
    <ul id="ul0023" list-style="none">
        <li id="ul0023-0001" num="0145">a. Detect all available slides in the system without user input.</li>
        <li id="ul0023-0002" num="0146">b. If possible, use off-the-shelf slide racks for compatibility with prior and subsequent processing equipment for ease of use.</li>
        <li id="ul0023-0003" num="0147">c. Detect and select slides improperly positioned in the slide rack to ensure no slide breakage.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0084" num="0148"><figref idref="DRAWINGS">FIGS. 2 and 3</figref> illustrate slide rack auto-loader systems as described herein. Other system configurations are also contemplated.</p>
<p id="p-0085" num="0149">In one embodiment according to the invention, the slide rack auto-loader comprises a user removable rack that contains multiple commercial slide holders. In this embodiment, the commercial racks are Sakura baskets (Part #4768) holding 20 slides per basket and arranged with two stacked baskets in four columns. The assembly is shown below. A reflective, IR sensor such as Fairchild #QRE00034 is placed a few millimeters from the end of the slides. Alternatively, an inexpensive laser diode, such as those used on laser pointers can be used to obtain greater selectivity of the slide edge. This laser is monitored by a detector which looks at light scattered from the slide. This is shown schematically in <figref idref="DRAWINGS">FIG. 4</figref>. In one embodiment, the invention uses 4 laser/detector pairs to scan down each of the 4 rows of slides. As the slide rack is translated vertically, each slide edge reflects laser light to the detector. The detector produces an analog voltage change which is fed to an A/D on a microprocessor. The shape of the reflected light pulse can be processed in terms of width of pulse and height of pulse to determine if a slide is in the slide basket slot or not. Furthermore, the shape of the reflected light pulse can be used to determine if two slides are loaded into the same slot or a slide is tilted within the slot. This information is useful to prevent mishandling or damage of slide by the slide grabber mechanism. <figref idref="DRAWINGS">FIG. 5</figref><i>a </i>shows a slide from an end on view in a properly loaded condition. <figref idref="DRAWINGS">FIG. 5</figref><i>b </i>shows the resulting detected laser pulse wherein the width of the pulse z<sub>s </sub>is compared to reference values t<sub>1 </sub>and t<sub>2</sub>. If z<sub>s </sub>falls between the reference values slide picking continues, if z<sub>s </sub>falls either below or above the reference values an error condition is produced.</p>
<p id="p-0086" num="0150">Referring to <figref idref="DRAWINGS">FIG. 6</figref><i>a </i>two slides are presented in adjacent slots again presented in an end on view. The pulse widths are compared as explained above and the spacing or pitch of the pulse widths are compared to additional reference value z<sub>1</sub>. If the pitch of pulse widths are not within z<sub>1 </sub>of a multiple of fixed slide positions an error is produced. Two important error conditions are detected with this methodology, a double-slide condition shown in <figref idref="DRAWINGS">FIG. 7</figref><i>a </i>along with the detector signals shown in <b>7</b><i>b </i>and a tilted slide condition shown in <figref idref="DRAWINGS">FIG. 8</figref><i>a </i>along with the detector signals shown in <b>8</b><i>b. </i></p>
<p id="p-0087" num="0151">The detection means is robust for the myriad of slide widths, thicknesses and surface finishes encountered in pathology labs.</p>
<p id="p-0088" num="0152">The positions of slide present in slot are used for:
<ul id="ul0024" list-style="none">
    <li id="ul0024-0001" num="0000">
    <ul id="ul0025" list-style="none">
        <li id="ul0025-0001" num="0153">a. Determining which slides are present for scanning.</li>
        <li id="ul0025-0002" num="0154">b. Determining the required x, y, z position to safely extract the slide from the rack for scanning.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0089" num="0155">Thus, the invention provides a low=cost, high volume slide auto-loader system. In one embodiment, the autoloader according to the invention comprises:
<ul id="ul0026" list-style="none">
    <li id="ul0026-0001" num="0000">
    <ul id="ul0027" list-style="none">
        <li id="ul0027-0001" num="0156">a. A low cost, non-contact slide detector sensor system to determine which slides are present.</li>
        <li id="ul0027-0002" num="0157">b. Sensor system to accurately determine slide position for safe slide extraction from rack to scanning system.</li>
        <li id="ul0027-0003" num="0158">c. Set of standard slide racks for compatibility to other slide processing equipment.</li>
        <li id="ul0027-0004" num="0159">d. A means for detecting a misloaded slide which prevents errors or damage to the slide from the slide grabber mechanism.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0090" num="0160">The slide auto-loader of the invention is advantageous in that conventional scanner equipment either do not auto-detect slides or utilize expensive sensors systems to detect slides.</p>
<p id="p-0091" num="0161">While preferred embodiments of the present invention have been shown and described herein, it will be obvious to those skilled in the art that such embodiments are provided by way of example only. Numerous variations, changes, and substitutions will now occur to those skilled in the art without departing from the invention. It should be understood that various alternatives to the embodiments of the invention described herein may be employed in practicing the invention. It is intended that the following claims define the scope of the invention and that methods and structures within the scope of these claims and their equivalents be covered thereby.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method for finding a regions of interest in a first image of a tissue-containing slide having a label and a cover slip having at least one cover slip boundary placed thereon, said method comprising:
<claim-text>acquiring a first low resolution image of a least a portion of the tissue-containing slide;</claim-text>
<claim-text>identifying a first tissue-containing region in the first low resolution image, said first tissue containing region comprising a first portion of the tissue;</claim-text>
<claim-text>identifying a second tissue-containing region in the first low resolution image, said second tissue-containing region comprising a second portion of the tissue;</claim-text>
<claim-text>identifying at least one label-containing region in the first low resolution image, said at least one label-containing region comprising a portion of the label;</claim-text>
<claim-text>identifying at least one cover slip boundary-containing region in the first low resolution image, said at least one cover slip boundary-containing region comprising a portion of the cover-slip;</claim-text>
<claim-text>identifying, in the first low resolution image, a staining artifacts region, said staining artifacts region comprising staining artifacts near the at least one cover slip boundary of the cover slip; and</claim-text>
<claim-text>segmenting the first image into at least five regions, the at least five regions comprising:
<claim-text>the first tissue-containing region;</claim-text>
<claim-text>the second tissue-containing region;</claim-text>
<claim-text>the at least one label-containing region;</claim-text>
<claim-text>the at least one cover slip boundary-containing region; and</claim-text>
<claim-text>the staining artifacts region;</claim-text>
</claim-text>
<claim-text>merging the first tissue-containing region and the second tissue-containing region into a single region of interest; and</claim-text>
<claim-text>acquiring a subsequent image of only the single region of interest, after the merging step.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the low resolution image is a thumbnail image that consists of a representation of the tissue-containing slide as a whole.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref> where the thumbnail image includes the label and the actual tissue spread on the tissue-containing slide.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref> wherein the thumbnail image can be used an identifier.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref> wherein the label image has a barcode specifying the details of a case and a stain type used.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref> wherein the label is used as a cross verification tool to correlate the tissue-containing slide with the actual image.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the low resolution image is a thumbnail image.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the segmenting step is based on an intensity threshold that separates pixels of the first image into tissue-containing pixels and non tissue-containing pixels.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step of identifying the first tissue-containing region and the second tissue-containing region involves separating pixels of the first low resolution image into tissue-containing pixels and non tissue-containing pixels to identify a kernel region of tissue, and wherein the kernel region of tissue is grown to incorporate an entire tissue region.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the step of identifying the first tissue-containing region and the second tissue-containing region involves automatic detection of an intensity threshold that separates tissue-containing pixels from non tissue-containing pixels. </claim-text>
</claim>
</claims>
</us-patent-grant>
