<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626739-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626739</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13323693</doc-number>
<date>20111212</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<us-term-of-grant>
<disclaimer>
<text>This patent is subject to a terminal disclaimer.</text>
</disclaimer>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>17</main-group>
<subgroup>30</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>707709</main-classification>
<further-classification>707740</further-classification>
<further-classification>709224</further-classification>
</classification-national>
<invention-title id="d2e51">Methods and systems for processing media files</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5701469</doc-number>
<kind>A</kind>
<name>Brandli et al.</name>
<date>19971200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5742816</doc-number>
<kind>A</kind>
<name>Barr et al.</name>
<date>19980400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>707728</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5907836</doc-number>
<kind>A</kind>
<name>Sumita et al.</name>
<date>19990500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6026388</doc-number>
<kind>A</kind>
<name>Liddy et al.</name>
<date>20000200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6092101</doc-number>
<kind>A</kind>
<name>Birrell et al.</name>
<date>20000700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6188277</doc-number>
<kind>B1</kind>
<name>Borodulin et al.</name>
<date>20010200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6282548</doc-number>
<kind>B1</kind>
<name>Burner et al.</name>
<date>20010800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6330589</doc-number>
<kind>B1</kind>
<name>Kennedy</name>
<date>20011200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6487555</doc-number>
<kind>B1</kind>
<name>Bharat</name>
<date>20021100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6513036</doc-number>
<kind>B2</kind>
<name>Fruensgaard et al.</name>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6708293</doc-number>
<kind>B2</kind>
<name>Kaler et al.</name>
<date>20040300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>714 39</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6728763</doc-number>
<kind>B1</kind>
<name>Chen</name>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709219</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>7162473</doc-number>
<kind>B2</kind>
<name>Dumais et al.</name>
<date>20070100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>7346613</doc-number>
<kind>B2</kind>
<name>Hurst-Hiller et al.</name>
<date>20080300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>7370035</doc-number>
<kind>B2</kind>
<name>Gross et al.</name>
<date>20080500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>7634546</doc-number>
<kind>B1</kind>
<name>Strickholm et al.</name>
<date>20091200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>7650403</doc-number>
<kind>B2</kind>
<name>Koetke et al.</name>
<date>20100100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>7676553</doc-number>
<kind>B1</kind>
<name>Laucius et al.</name>
<date>20100300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>8099407</doc-number>
<kind>B2</kind>
<name>Auerbach et al.</name>
<date>20120100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>8203952</doc-number>
<kind>B2</kind>
<name>Borkovsky et al.</name>
<date>20120600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3702301</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>8386728</doc-number>
<kind>B1</kind>
<name>Ionescu et al.</name>
<date>20130200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>2001/0049677</doc-number>
<kind>A1</kind>
<name>Talib et al.</name>
<date>20011200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>2002/0055844</doc-number>
<kind>A1</kind>
<name>L'Esperance et al.</name>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>2002/0059265</doc-number>
<kind>A1</kind>
<name>Valorose, III</name>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>707100</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>2003/0050966</doc-number>
<kind>A1</kind>
<name>Dutta et al.</name>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>2003/0083947</doc-number>
<kind>A1</kind>
<name>Hoffman et al.</name>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>2003/0154293</doc-number>
<kind>A1</kind>
<name>Zmolek</name>
<date>20030800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>2003/0185379</doc-number>
<kind>A1</kind>
<name>O'Connor et al.</name>
<date>20031000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>2003/0212654</doc-number>
<kind>A1</kind>
<name>Harper et al.</name>
<date>20031100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>2003/0233419</doc-number>
<kind>A1</kind>
<name>Beringer</name>
<date>20031200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>US</country>
<doc-number>2004/0044962</doc-number>
<kind>A1</kind>
<name>Green et al.</name>
<date>20040300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>US</country>
<doc-number>2005/0033803</doc-number>
<kind>A1</kind>
<name>Vleet et al.</name>
<date>20050200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00033">
<document-id>
<country>US</country>
<doc-number>2005/0080792</doc-number>
<kind>A1</kind>
<name>Ghatare</name>
<date>20050400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00034">
<document-id>
<country>US</country>
<doc-number>2005/0165777</doc-number>
<kind>A1</kind>
<name>Hurst-Hiller et al.</name>
<date>20050700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00035">
<document-id>
<country>US</country>
<doc-number>2006/0100912</doc-number>
<kind>A1</kind>
<name>Kumar et al.</name>
<date>20060500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00036">
<document-id>
<country>US</country>
<doc-number>2007/0033275</doc-number>
<kind>A1</kind>
<name>Toivonen et al.</name>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00037">
<document-id>
<country>EP</country>
<doc-number>1209866</doc-number>
<kind>A2</kind>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00038">
<othercit>Pingali et al., Instantly Indexed Multimedia Databases of Real World Events, Multimedia, IEEE Transactions on, vol. 4, Issue 2, Jun. 2002, pp. 269-282.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
</us-references-cited>
<number-of-claims>17</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>707708</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707715</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707739</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707740</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707749</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707758</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707771</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707953</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707709</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>709224</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>3</number-of-drawing-sheets>
<number-of-figures>3</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>10813895</doc-number>
<date>20040331</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>8099407</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13323693</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120179664</doc-number>
<kind>A1</kind>
<date>20120712</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Auerbach</last-name>
<first-name>David Benjamin</first-name>
<address>
<city>Brooklyn</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Lawrence</last-name>
<first-name>Stephen R.</first-name>
<address>
<city>Mountain View</city>
<state>AZ</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Marmaros</last-name>
<first-name>David</first-name>
<address>
<city>Mountain View</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Auerbach</last-name>
<first-name>David Benjamin</first-name>
<address>
<city>Brooklyn</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Lawrence</last-name>
<first-name>Stephen R.</first-name>
<address>
<city>Mountain View</city>
<state>AZ</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Marmaros</last-name>
<first-name>David</first-name>
<address>
<city>Mountain View</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Fenwick &#x26; West LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Google Inc.</orgname>
<role>02</role>
<address>
<city>Mountain View</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Robinson</last-name>
<first-name>Greta</first-name>
<department>2169</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Systems and methods for processing media files are described. In one embodiment, one or more events are captured having associated event data and associated with a client device, wherein each event is associated with an article and at least one of the articles is a media file, wherein at least one of the events is captured in real time upon the occurrence of the event, at least some of the event data and articles associated with the events are indexed and stored, a search query is received, and the at least one media file is determined as relevant to the search query.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="230.46mm" wi="167.47mm" file="US08626739-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="239.01mm" wi="178.14mm" file="US08626739-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="204.39mm" wi="119.80mm" file="US08626739-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="215.31mm" wi="114.72mm" file="US08626739-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application is a continuation of co-pending U.S. application Ser. No. 10/813,895, filed Mar. 31, 2004, which is herein incorporated by reference in its entirety.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">FIELD OF THE INVENTION</heading>
<p id="p-0003" num="0002">The invention generally relates to search engines. More particularly, the invention relates to methods and systems for processing media files.</p>
<heading id="h-0003" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0004" num="0003">Users generate and access a large number of articles, such as emails, web pages, word processing documents, media files, spreadsheet documents, instant messenger messages, and presentation documents, using a client device, such as a personal computer, personal digital assistant, or mobile phone. Some articles are stored on one or more storage devices coupled to, accessible by, or otherwise associated with the client device(s). Users sometimes wish to search the storage device(s) for articles.</p>
<p id="p-0005" num="0004">Conventional client-device search applications may significantly degrade the performance of the client device. For example, certain conventional client-device search applications typically use batch processing to index all articles, which can result in noticeably slower performance of the client device during the batch processing. Additionally, batch processing occurs only periodically. Therefore, when a user performs a search, the most recent articles are sometimes not included in the results. Moreover, if the batch processing is scheduled for a time when the client device is not operational and is thus not performed for an extended period of time, the index of articles associated with the client device can become outdated. Conventional client-device search applications can also need to rebuild the index at each batch processing or build new partial indexes and perform a merge operation that can use a lot of client-device resources. Conventional client-device search applications also sometimes use a great deal of system resources when operational, resulting in slower performance of the client device.</p>
<p id="p-0006" num="0005">Additionally, conventional client-device search applications can require an explicit search query from a user to generate results, and may be limited to examining file names or the contents of a particular application's files.</p>
<heading id="h-0004" level="1">SUMMARY</heading>
<p id="p-0007" num="0006">Embodiments of the present invention comprise methods and systems for processing media files. In one embodiment, one or more events are captured having associated event data and associated with a client device, wherein each event is associated with an article and at least one of the articles is a media file, wherein at least one of the events is captured in real time upon the occurrence of the event, at least some of the event data and articles associated with the events are indexed and stored, a search query is received, and the at least one media file is determined as relevant to the search query. The media file can be one or more of an audio file, a video file, an image file, some combination, or a scripted timeline presentation of different media files.</p>
<p id="p-0008" num="0007">This exemplary embodiment is mentioned not to limit or define the invention, but to provide an example of an embodiment of the invention to aid understanding thereof. Exemplary embodiments are discussed in the Detailed Description, and further description of the invention is provided there. Advantages offered by the various embodiments of the present invention may be further understood by examining this specification.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0009" num="0008">These and other features, aspects, and advantages of the present invention are better understood when the following Detailed Description is read with reference to the accompanying drawings, wherein:</p>
<p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. 1</figref> is a diagram illustrating an exemplary environment in which one embodiment of the present invention may operate;</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 2</figref> is a flow diagram illustrating an exemplary method of capturing, indexing, and storing an event associated with a client device in one embodiment of the present invention; and</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 3</figref> is a flow diagram illustrating an exemplary method of processing a search query according to one embodiment of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0006" level="1">DETAILED DESCRIPTION OF SPECIFIC EMBODIMENTS</heading>
<p id="p-0013" num="0012">Embodiments of the present invention comprise methods and systems for processing media files.</p>
<heading id="h-0007" level="1">System Architecture</heading>
<p id="p-0014" num="0013">Referring now to the drawings in which like numerals indicate like elements throughout the several figures, <figref idref="DRAWINGS">FIG. 1</figref> is a block diagram illustrating an exemplary environment for implementation of an embodiment of the present invention. While the environment shown in <figref idref="DRAWINGS">FIG. 1</figref> reflects a client-side search engine architecture embodiment, other embodiments are possible. The system <b>100</b> shown in <figref idref="DRAWINGS">FIG. 1</figref> includes multiple client devices <b>102</b><i>a</i>-<i>n </i>that can communicate with a server device <b>150</b> over a network <b>106</b>. The network <b>106</b> shown in <figref idref="DRAWINGS">FIG. 1</figref> comprises the Internet. In other embodiments, other networks, such as an intranet, may be used instead. Moreover, methods according to the present invention may operate within a single client device that does not communicate with a server device or a network.</p>
<p id="p-0015" num="0014">The client devices <b>102</b><i>a</i>-<i>n </i>shown in <figref idref="DRAWINGS">FIG. 1</figref> each include a computer-readable medium <b>108</b>. The embodiment shown in <figref idref="DRAWINGS">FIG. 1</figref> includes a random access memory (RAM) <b>108</b> coupled to a processor <b>110</b>. The processor <b>110</b> executes computer-executable program instructions stored in memory <b>108</b>. Such processors may include a microprocessor, an ASIC, state machines, or other processor, and can be any of a number of suitable computer processors, such as processors from Intel Corporation of Santa Clara, Calif. and Motorola Corporation of Schaumburg, Ill. Such processors include, or may be in communication with, media, for example computer-readable media, which stores instructions that, when executed by the processor, cause the processor to perform the steps described herein. Embodiments of computer-readable media include, but are not limited to, an electronic, optical, magnetic, or other storage or transmission device capable of providing a processor, such as the processor <b>110</b> of client <b>102</b><i>a</i>, with computer-readable instructions. Other examples of suitable media include, but are not limited to, a floppy disk, CD-ROM, DVD, magnetic disk, memory chip, ROM, RAM, an ASIC, a configured processor, all optical media, all magnetic tape or other magnetic media, or any other medium from which a computer processor can read instructions. Also, various other forms of computer-readable media may transmit or carry instructions to a computer, including a router, private or public network, or other transmission device or channel, both wired and wireless. The instructions may comprise code from any suitable computer-programming language, including, for example, C, C++, C#, Visual Basic, Java, Python, Perl, and JavaScript.</p>
<p id="p-0016" num="0015">Client devices <b>102</b><i>a</i>-<i>n </i>can be coupled to a network <b>106</b>, or alternatively, can be stand alone machines. Client devices <b>102</b><i>a</i>-<i>n </i>may also include a number of external or internal devices such as a mouse, a CD-ROM, DVD, a keyboard, a display device, or other input or output devices. Examples of client devices <b>102</b><i>a</i>-<i>n </i>are personal computers, digital assistants, personal digital assistants, cellular phones, mobile phones, smart phones, pagers, digital tablets, laptop computers, Internet appliances, and other processor-based devices. In general, the client devices <b>102</b><i>a</i>-<i>n </i>may be any type of processor-based platform that operates on any suitable operating system, such as Microsoft&#xae; Windows&#xae; or Linux, capable of supporting one or more client application programs. For example, the client device <b>102</b><i>a </i>can comprise a personal computer executing client application programs, also known as client applications <b>120</b>. The client applications <b>120</b> can be contained in memory <b>108</b> and can include, for example, a word processing application, a spreadsheet application, an email application, an instant messenger application, a presentation application, an Internet browser application, a calendar/organizer application, a video playing application, an audio playing application, an image display application, a file management program, an operating system shell, and other applications capable of being executed by a client device. Client applications may also include client-side applications that interact with or accesses other applications (such as, for example, a web-browser executing on the client device <b>102</b><i>a </i>that interacts with a remote e-mail server to access e-mail).</p>
<p id="p-0017" num="0016">The user <b>112</b><i>a </i>can interact with the various client applications <b>120</b> and articles associated with the client applications <b>120</b> via various input and output devices of the client device <b>102</b><i>a</i>. Articles include, for example, word processor documents, spreadsheet documents, presentation documents, emails, instant messenger messages, database entries, calendar entries, appointment entries, task manager entries, source code files, and other client application program content, files, messages, items, web pages of various formats, such as HTML, XML, XHTML, Portable Document Format (PDF) files, and media files, such as image files, audio files, and video files, or any other documents or items or groups of documents or items or information of any suitable type whatsoever.</p>
<p id="p-0018" num="0017">The user's <b>112</b><i>a </i>interaction with articles, the client applications <b>120</b>, and the client device <b>102</b><i>a </i>creates event data that may be observed, recorded, analyzed or otherwise used. An event can be any occurrence possible associated with an article, client application <b>120</b>, or client device <b>102</b><i>a</i>, such as playing an audio file, editing a video file, uploading an image file, inputting text in an article, displaying an article on a display device, sending an article, receiving an article, manipulating an input device, opening an article, saving an article, printing an article, closing an article, opening a client application program, closing a client application program, idle time, processor load, disk access, memory usage, bringing a client application program to the foreground, changing visual display details of the application (such as resizing or minimizing) and any other suitable occurrence associated with an article, a client application program, or the client device whatsoever. Additionally, event data can be generated when the client device <b>102</b><i>a </i>interacts with an article independent of the user <b>112</b><i>a</i>, such as when receiving an email or performing a scheduled task.</p>
<p id="p-0019" num="0018">The memory <b>108</b> of the client device <b>102</b><i>a </i>can also contains a capture processor <b>124</b>, a queue <b>126</b>, and a search engine <b>122</b>. The client device <b>102</b><i>a </i>can also contain or is in communication with a data store <b>140</b>. The capture processor <b>124</b> can capture events and pass them to the queue <b>126</b>. The queue <b>126</b> can pass the captured events to the search engine <b>122</b> or the search engine <b>122</b> can retrieve new events from the queue <b>126</b>. In one embodiment, the queue <b>126</b> notifies the search engine <b>122</b> when a new event arrives in the queue <b>126</b> and the search engine <b>122</b> retrieves the event (or events) from the queue <b>126</b> when the search engine <b>122</b> is ready to process the event (or events). When the search engine receives an event it can be processed and can be stored in the data store <b>140</b>. The search engine <b>122</b> can receive an explicit query from the user <b>112</b><i>a </i>or generate an implicit query and it can retrieve information from the data store <b>140</b> in response to the query. In another embodiment, the queue is located in the search engine <b>122</b>. In still another embodiment, the client device <b>102</b><i>a </i>does not have a queue and the events are passed from the capture processor <b>124</b> directly to the search engine <b>122</b>. According to other embodiments, the event data is transferred using an information exchange protocol. The information exchange protocol can comprise, for example, any suitable rule or convention facilitating data exchange, and can include, for example, any one of the following communication mechanisms: Extensible Markup Language-Remote Procedure Calling protocol (XML/RPC), Hypertext Transfer Protocol (HTTP), Simple Object Access Protocol (SOAP), shared memory, sockets, local or remote procedure calling, or any other suitable information exchange mechanism.</p>
<p id="p-0020" num="0019">The capture processor <b>124</b> can capture an event by identifying and compiling event data associated with an event. Examples of events include playing an audio file, editing a video file, uploading an image file, sending or receiving an email message, a user viewing a web page, saving a word processing document, printing a spreadsheet document, inputting text to compose or edit an email, opening a presentation application, closing an instant messenger application, entering a keystroke, moving the mouse, and hovering the mouse over a hyperlink. An example of event data captured by the capture processor <b>124</b> for an event involving the playing of an audio file by the user <b>112</b><i>a </i>can include the title of the file, the location of the file, the date the file was saved to the client device, the data contained in the file, the quality of the file, the format of the file, and a pointer to the location of the file.</p>
<p id="p-0021" num="0020">In the embodiment shown in <figref idref="DRAWINGS">FIG. 1</figref>, the capture processor <b>124</b> comprises multiple capture components. For example, the capture processor <b>124</b> shown in <figref idref="DRAWINGS">FIG. 1</figref> comprises a separate capture component for each client application in order to capture events associated with each application. The capture processor <b>124</b> can also comprises a separate capture component that monitors overall network activity in order to capture event data associated with network activity, such as the receipt or sending of an audio or video stream, such as Unicast, Multicast or broadcast. The capture processor <b>124</b> shown in <figref idref="DRAWINGS">FIG. 1</figref> also can comprise a separate client device capture component that monitors overall client device performance data, such as processor load, idle time, disk access, the client applications in use, and the amount of memory available. The capture processor <b>124</b> shown in <figref idref="DRAWINGS">FIG. 1</figref> also comprises a separate capture component to monitor and capture keystrokes input by the user and a separate capture component to monitor and capture items, such as text, displayed on a display device associated with the client device <b>102</b><i>a</i>. An individual capture component can monitor multiple client applications and multiple capture components can monitor different aspects of a single client application.</p>
<p id="p-0022" num="0021">In one embodiment, the capture processor <b>124</b>, through the individual capture components, can monitor activity on the client device and can capture events by a generalized event definition and registration mechanism, such as an event schema. Each capture component can define its own event schema or can use a predefined one. Event schemas can differ depending on the client application or activity the capture component is monitoring. Generally, the event schema can describe the format for an event, for example, by providing fields for event data associated with the event (such as the time of the event) and fields related to any associated article (such as the title) as well as the content of any associated article (such as the document body). An event schema can describe the format for any suitable event data that relates to an event. For example, an event schema for an audio file event, such as a song in mp3 format played by the user <b>112</b><i>a</i>, can include the title of the song, the artist of the song, the album title, the genre of the song, the location of the file, the data in the file that was saved to the client device, the source where the file was acquired from, the quality of the file, the format of the file, the ownership information of the file, the Digital Rights Management (DRM) information, and other suitable information. An event schema for a video, such as a home movie being viewed by the user <b>112</b><i>a </i>can include the title of the file, the date the file was created, a date the file was edited (if any), the date the file was saved to the client device, the format of the file, the location of the file, the individuals who appear in the file, annotated information on the file, and links to similar files. Links to similar files can be based on time, source, themes or genres, individuals appearing in the file, and other suitable linking information. An event schema for an image file being saved by a user can include the title of the image file, the time saved, the location of the image file, the format of the image file, a license associated with the file, the location where the image was taken, the quality of the image, the camera aspects of the image, such as lens, focus, lighting and flash. More generally, an event schema can describe the state of the system around the time of the event. For example, an event schema can contain a URL for a web page event associated with a previous web page that the user navigated from. In addition, event schema can describe fields with more complicated structure like lists. For example, an event schema can contain fields that list multiple recipients. An event schema can also contain optional fields so that an application can include additional event data if desired.</p>
<p id="p-0023" num="0022">The capture processor <b>124</b> can capture events occurring presently (or &#x201c;real-time events&#x201d;) and can capture events that have occurred in the past (or &#x201c;historical events&#x201d;). Real-time events can be &#x201c;indexable&#x201d; or &#x201c;non-indexable&#x201d;. In one embodiment, the search engine <b>122</b> indexes indexable real-time events, but does not index non-indexable real-time events. The search engine <b>122</b> may determine whether to index an event based on the importance of the event. Indexable real-time events can be more important events associated with an article, such as viewing a web page, loading or saving a file, and receiving or sending an instant message or email. Non-indexable events can be deemed not important enough by the search engine <b>122</b> to index and store the event, such as moving the mouse or selecting a portion of text in an article. Non-indexable events can be used by the search engine <b>122</b> to update the current user state. While all real-time events can relate to what the user is currently doing (or the current user state), indexable real-time events can be indexed and stored in the data store <b>140</b>. Alternatively, the search engine <b>122</b> can index all real-time events. Real-time events can include, for example, sending or receiving an article, such as email, examining a portion of an article, such as selecting a portion of text or moving a mouse over a portion of a web page, changing an article, such as editing a video file, closing an article, such as closing a media player application or changing an audio file being listened to, loading, saving, opening, or viewing an article, such as an image file, listening to or saving an mp3 file or other audio/video file, or updating the metadata of an article, such as book marking a web page, printing a presentation document, deleting a word processing document, or moving a spreadsheet document.</p>
<p id="p-0024" num="0023">Historical events are similar to indexable real-time events except that the event occurred before the installation of the search engine <b>122</b> or was otherwise not captured, because, for example, the search engine <b>122</b> was not operational for a period of time while the client device <b>102</b><i>a </i>was operational or because no capture component existed for a specific type of historical event at the time the event took place. Examples of historical events include the user's saved word processing documents, media files, presentation documents, calendar entries, and spreadsheet documents, the emails in a user's inbox, and the web pages book marked by the user. The capture processor <b>124</b> can capture historical events by periodically crawling the memory <b>108</b> and any associated data storage device for events not previously captured by the capture processor <b>124</b>. The capture processor <b>124</b> can also capture historical events by requesting certain client applications, such as a web browser or an email application, to retrieve articles and other associated information. For example, the capture processor <b>124</b> can request that the web browser application obtain all viewed web pages by the user or request that the email application obtain all email messages associated with the user. These articles may not currently exist in memory <b>108</b> or on a storage device of the client device <b>102</b><i>a</i>. For example, the email application may have to retrieve emails from a server device. In one embodiment, the search engine <b>122</b> indexes historical events.</p>
<p id="p-0025" num="0024">In the embodiment shown in <figref idref="DRAWINGS">FIG. 1</figref>, events captured by the capture processor <b>124</b> are sent to the queue <b>126</b> in the format described by an event schema. The capture processor <b>124</b> can also send performance data to the queue <b>126</b>. Examples of performance data include current processor load, average processor load over a predetermined period of time, idle time, disk access, the client applications in use, and the amount of memory available. Performance data can also be provided by specific performance monitoring components, some of which may be part of the search engine <b>122</b>, for example. The performance data in the queue <b>126</b> can be retrieved by the search engine <b>122</b> and the capture components of the capture processor <b>124</b>. For example, capture components can retrieve the performance data to alter how many events are sent to the queue <b>126</b> or how detailed the events are that are sent (fewer or smaller events when the system is busy) or how frequently event are sent (events are sent less often when the system is busy or there are too many events waiting to be processed). The search engine <b>122</b> can use performance data to determine when it indexes various events and when and how often it issues implicit queries.</p>
<p id="p-0026" num="0025">In one embodiment, the queue <b>126</b> holds events until the search engine <b>122</b> is ready to process an event or events. Alternatively, the queue <b>126</b> uses the performance data to help determine how quickly to provide the events to the search engine <b>122</b>. The queue <b>126</b> can comprise one or more separate queues including a user state queue and an index queue. The index queue can queue indexable events, for example. Alternatively, the queue <b>126</b> can have additional queues or comprise a single queue. The queue <b>126</b> can be implemented as a circular priority queue using memory mapped files. The queue can be a multiple-priority queue where higher priority events are served before lower priority events, and other components may be able to specify the type of events they are interested in. Generally, real-time events can be given higher priority than historical events, and indexable events can be given higher priority than non-indexable real-time events. Other implementations of the queue <b>126</b> are possible. In another embodiment, the client device <b>102</b><i>a </i>does not have a queue <b>126</b>. In this embodiment, events are passed directly from the capture processor <b>124</b> to the search engine <b>122</b>. In other embodiments, events can be transferred between the capture components and the search engine using suitable information exchange mechanisms such as: Extensible Markup Language-Remote Procedure Calling protocol (XML/RPC), Hypertext Transfer Protocol (HTTP), Simple Object Access Protocol (SOAP), shared memory, sockets, local or remote procedure calling, or any other suitable information exchange mechanism.</p>
<p id="p-0027" num="0026">The search engine <b>122</b> can contain an indexer <b>130</b>, a query system <b>132</b>, and a formatter <b>134</b>. The query system <b>132</b> can retrieve real-time events and performance data from the queue <b>126</b>. The query system <b>132</b> can use performance data and real-time events to update the current user state and generate an implicit query. An implicit query can be an automatically generated query based on the current user state. The query system <b>132</b> can also receive and process explicit queries from the user <b>112</b><i>a</i>. Performance data can also be retrieved by the search engine <b>122</b> from the queue <b>126</b> for use in determining the amount of activity possible by the search engine <b>122</b>.</p>
<p id="p-0028" num="0027">In the embodiment shown in <figref idref="DRAWINGS">FIG. 1</figref>, indexable real-time events and historical events (indexable events) are retrieved from the queue <b>126</b> by the indexer <b>130</b>. Alternatively, the queue <b>126</b> may send the indexable events to the indexer <b>130</b>. The indexer <b>130</b> can index the indexable events and can send them to the data store <b>140</b> where they are stored. The data store <b>140</b> can be any type of computer-readable media and can be integrated with the client device <b>102</b><i>a</i>, such as a hard drive, or external to the client device <b>102</b><i>a</i>, such as an external hard drive or on another data storage device accessed through the network <b>106</b>. The data store can be one or more logical or physical storage areas. In one embodiment, the data store <b>140</b> can be in memory <b>108</b>. The data store <b>140</b> may facilitate one or a combination of methods for storing data, including without limitation, arrays, hash tables, lists, and pairs, and may include compression and encryption. In the embodiment shown in <figref idref="DRAWINGS">FIG. 1</figref>, the data store comprises an index <b>142</b>, a database <b>144</b> and a repository <b>146</b>.</p>
<p id="p-0029" num="0028">In the embodiment shown in <figref idref="DRAWINGS">FIG. 1</figref>, when the indexer <b>130</b> receives an event, the indexer <b>130</b> can determine, from the event, terms (if any) associated with the event, the time of the event (if available), images (if any) associated with the event, and/or other information defining the event. The indexer <b>130</b> can also determine if the event relates to other events and associate the event with related events. For example, for a media file event, the indexer <b>130</b> can associate the media file event with other versions (if any) associated with the media file event. The media file events can be associated with each other in an associated events object, which can be stored in the data store <b>140</b>. The indexer <b>130</b> may further identify and compile event data from external sources, such as local and global databases, websites, and a network search engine, allowing the indexer <b>130</b> to augment the available information or event data associated with the event. For example, if a user plays a song and the media file containing the song only has title information associated with it, the indexer <b>130</b> can determine the lyrics, artist, year, genre, album, and album image for external sources and associate this data with the event.</p>
<p id="p-0030" num="0029">The indexer <b>130</b> can send and incorporate the terms and times, associated with the event in the index <b>142</b> of the data store <b>140</b>. The event can be sent to the database <b>144</b> for storage and the content of the associated article and any associated images can be stored in the repository <b>146</b>. The associated events object can be stored in the database <b>144</b>.</p>
<p id="p-0031" num="0030">In the embodiment shown in <figref idref="DRAWINGS">FIG. 1</figref>, a user <b>112</b><i>a </i>can input an explicit query into a search engine interface displayed on the client device <b>102</b><i>a</i>, which is received by the search engine <b>122</b>. The search engine <b>122</b> can also generate an implicit query based on a current user state, which can be determined by the query system <b>132</b> from real-time events. Based on the query, the query system <b>132</b> can locate relevant information in the data store <b>140</b> and provide a result set. In one embodiment, the result set comprises article identifiers for articles associated with the client applications <b>120</b> or client articles. Client articles include articles associated with the user <b>112</b><i>a </i>or client device <b>102</b><i>a</i>, such as the user's emails, word processing documents, instant messenger messages, previously viewed web pages and any other article or portion of an article associated with the client device <b>102</b><i>a </i>or user <b>112</b><i>a</i>. An article identifier may be, for example, a Uniform Resource Locator (URL), a file name, a link, an icon, a path for a local file, or other suitable information that may identify an article. In another embodiment, the result set also comprises article identifiers for articles located on the network <b>106</b> or network articles located by a search engine on a server device. Network articles include articles located on the network <b>106</b> not previously viewed or otherwise referenced by the user <b>112</b><i>a</i>, such as web pages not previously viewed by the user <b>112</b><i>a. </i></p>
<p id="p-0032" num="0031">The formatter <b>134</b> can receive the search result set from the query system <b>132</b> of the search engine <b>122</b> and can format the results for output to a display processor <b>128</b>. In one embodiment, the formatter <b>134</b> can format the results in XML, HTML, or tab delineated text. The display processor <b>128</b> can be contained in memory <b>108</b> and can control the display of the result set on a display device associated with the client device <b>102</b><i>a</i>. The display processor <b>128</b> may comprise various components. For example, in one embodiment, the display processor <b>128</b> comprises a Hypertext Transfer Protocol (HTTP) server that receives requests for information and responds by constructing and transmitting Hypertext Markup Language (HTML) pages. In one such embodiment, the HTTP server comprises a scaled-down version of the Apache Web server. The display processor <b>128</b> can be associated with a set of APIs to allow various applications to receive the results and display them in various formats. The display APIs can be implemented in various ways, including as, for example, DLL exports, COM interface, VB, JAVA, or .NET libraries, or a web service.</p>
<p id="p-0033" num="0032">Through the client devices <b>102</b><i>a</i>-<i>n</i>, users <b>112</b><i>a</i>-<i>n </i>can communicate over the network <b>106</b>, with each other and with other systems and devices coupled to the network <b>106</b>. As shown in <figref idref="DRAWINGS">FIG. 1</figref>, a server device <b>150</b> can be coupled to the network <b>106</b>. In the embodiment shown in <figref idref="DRAWINGS">FIG. 1</figref>, the search engine <b>122</b> can transmit a search query comprised of an explicit or implicit query or both to the server device <b>150</b>. The user <b>112</b><i>a </i>can also enter a search query in a search engine interface, which can be transmitted to the server device <b>150</b> by the client device <b>102</b><i>a </i>via the network <b>106</b>. In another embodiment, the query signal may instead be sent to a proxy server (not shown), which then transmits the query signal to server device <b>150</b>. Other configurations are also possible.</p>
<p id="p-0034" num="0033">The server device <b>150</b> can include a server executing a search engine application program, such as the Google&#x2122; search engine. In other embodiments, the server device <b>150</b> can comprise a related information server or an advertising server. Similar to the client devices <b>102</b><i>a</i>-<i>n</i>, the server device <b>150</b> can include a processor <b>160</b> coupled to a computer-readable memory <b>162</b>. Server device <b>150</b>, depicted as a single computer system, may be implemented as a network of computer processors. Examples of a server device <b>150</b> are servers, mainframe computers, networked computers, a processor-based device, and similar types of systems and devices. The server processor <b>160</b> can be any of a number of computer processors, such as processors from Intel Corporation of Santa Clara, Calif. and Motorola Corporation of Schaumburg, Ill. In another embodiment, the server device <b>150</b> may exist on a client-device. In still another embodiment, there can be multiple server devices <b>150</b>.</p>
<p id="p-0035" num="0034">Memory <b>162</b> contains the search engine application program, also known as a network search engine <b>170</b>. The search engine <b>170</b> can locate relevant information from the network <b>106</b> in response to a search query from a client device <b>102</b><i>a</i>. The search engine <b>170</b> then can provide a result set to the client device <b>102</b><i>a </i>via the network <b>106</b>. The result set can comprise one or more article identifiers. An article identifier may be, for example, a Uniform Resource Locator (URL), a file name, a link, an icon, a path for a local file, or anything else that identifies an article. In one embodiment, an article identifier can comprise a URL associated with an article.</p>
<p id="p-0036" num="0035">In one embodiment, the server device <b>150</b>, or related device, has previously performed a crawl of the network <b>106</b> to locate articles, such as web pages, stored at other devices or systems coupled to the network <b>106</b>, and indexed the articles in memory <b>162</b> or on another data storage device. It should be appreciated that other methods for indexing articles in lieu of or in combination with crawling may be used, such as manual submission.</p>
<p id="p-0037" num="0036">It should be noted that other embodiments of the present invention may comprise systems having different architecture than that which is shown in <figref idref="DRAWINGS">FIG. 1</figref>. For example, in some other embodiments of the present invention, the client device <b>102</b><i>a </i>is a stand-alone device and is not connected to a network. The system <b>100</b> shown in <figref idref="DRAWINGS">FIG. 1</figref> is merely exemplary, and is used to explain the exemplary methods shown in <figref idref="DRAWINGS">FIGS. 2 through 3</figref>.</p>
<heading id="h-0008" level="1">Process</heading>
<p id="p-0038" num="0037">Various methods in accordance with embodiments of the present invention may be carried out. For example, in one embodiment, one or more events are captured having associated event data and associated with a client device, wherein each event is associated with an article and at least one of the articles is a media file, wherein at least one of the events is captured in real time upon the occurrence of the event, at least some of the event data and articles associated with the events are indexed and stored, a search query is received, and the at least one media file is determined as relevant to the search query. An event with an associated media file article may be referred to as a media file event. The media files can be audio files, a video files, image files, some combination, or a scripted timeline presentation of different media files.</p>
<p id="p-0039" num="0038">The search query can be an explicit query or an implicit query. In one embodiment, capturing the event associated with the media file can comprise monitoring a media application to determine event data associated with the event and compiling the event from at least some of the event data. Capturing the event associated with the media file can comprise determining event data external to the media file. The event data external to the media file can be determined based at least in part on one or more of a local database, a global database, a web page, and a network search engine. Capturing the event associated with the media file can also comprise determining text that identifies the media file and including the text as event data associated with the event. In one embodiment, indexing the event associated with the media file can comprise associating the media file event with associated events. The associated event can comprise a different version of the event.</p>
<p id="p-0040" num="0039">According to one embodiment, capturing the event associated with the media file can comprise identifying the event based at least in part on one or more of network activity, system activity, and media application activity. Capturing the event associated with the media file can comprise identifying the event based at least in part on a display area associated with a media application and identifying at least some of the event data by analyzing the display area. Capturing the event associated with the media file can comprise identifying the event based at least in part on calls to input or output devices and identifying at least some of the event data by analyzing the calls.</p>
<p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. 2</figref> illustrates an exemplary method <b>200</b> that provides a method for capturing, indexing, and storing an event. This exemplary method is provided by way of example, as it will be appreciated from the foregoing description of exemplary embodiments that there are a variety of ways to carry out methods according to the present invention. The method <b>200</b> shown in <figref idref="DRAWINGS">FIG. 2</figref> can be executed or otherwise performed by any of various systems. The method <b>200</b> is described below as carried out by the system <b>100</b> shown in <figref idref="DRAWINGS">FIG. 1</figref> by way of example, and various elements of the system <b>100</b> are referenced in explaining the example method of <figref idref="DRAWINGS">FIG. 2</figref>. However, it will be appreciated that the invention is not limited to a particular system, such as that described with reference to <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0042" num="0041">In <b>202</b>, the capture processor <b>124</b> captures an event. The event can be a media file event, such as an audio, video, or image file event, a combination media file event, or a scripted timeline presentation of different media files event that may have auxiliary associated content, such as subtitles, optional audio tracks and director commentary, or can be a non-media file event, such as an email, instant messenger, web page or presentation event. The event can be a real-time event or can be a historical event. The capture processor <b>124</b> can capture a real-time event by identifying and compiling event data associated with the event upon the occurrence of the event. The capture processor <b>124</b> can capture a historical event, for example, by crawling the memory <b>108</b> or associated data storage device of the client device <b>112</b><i>a </i>for previously uncaptured articles or receiving articles or data from client applications and identifying and compiling event data associated with the event. Such a crawl may be requested by the user, may be performed at certain intervals, or may occur upon some other suitable condition. The capture processor <b>124</b> may have separate capture components for each client application, network monitoring, performance data capture, keystroke capture, and display capture. For example, the capture processor <b>124</b> may have a different capture component associated with each media application or each type of media application. Alternatively, a single capture component may be used to capture media file events.</p>
<p id="p-0043" num="0042">In one embodiment, the capture component can use a generalized event definition mechanism, such as an event schema that it has previously defined and registered with the client device <b>102</b><i>a</i>, to capture or express the event. For example, an event schema for an audio file event, such as a song in mp3 format played by the user <b>112</b><i>a</i>, can include the title of the song, the artist of the song, the album title, the genre of the song, the location of the file, the data the file was saved to the client device, the quality of the file, the format of the file, and a pointer to the location of the file. An event schema for a video, such as a home movie being viewed by the user <b>112</b><i>a </i>can include the title of the file, the date the file was created, a date the file was edited (if any), the date the file was saved to the client device, the format of the file, the location of the file, and a pointer to the location of the file. An event schema for an image file being saved by a user can include the title of the image file, the time saved, the location of the image file, the format of the image file, and a pointer to the location of the image file.</p>
<p id="p-0044" num="0043">The capture processor <b>124</b> can identify and compile event data according to the format described by the relevant event schema in order to capture the event. Event data for a media file event can be included in associated metadata from the file, such as ID3 tags in a music file. The event data for a media file event can also be determined from sources external to the media file, such as a local or global database or a network search engine. For example, event data for an audio media file event, such as a song, can be determined by looking up information about the song, such as, for example, artist, title, genre, album cover art, and lyrics, based on some identifier. This lookup may be done using the network search engine <b>170</b>. Alternatively, the lookup may be done using a database associated with the client-device <b>102</b><i>a</i>. The identifier can be, for example, a unique value based on the wavelength, length or other information associated with the audio media file, the name of the file, metadata within the file, or a fingerprint that has been added to the file. This identification and compilation of event data can occur in real time as the events are occurring or can occur during a crawl of the user's files. In one embodiment, the capture processor <b>124</b> can determine or generate text that can identify the audio portion of a media file and include this text as event data in the event. This text can be used in indexing the event. For an audio media file event, such as a song, for example, in one embodiment, the capture processor <b>124</b> can determine from an external source, such as a web site, the lyrics associated with the song in a text format and compile this text as event data associated with the media file event.</p>
<p id="p-0045" num="0044">In one embodiment, the capture processor <b>124</b> can identify a media file event by network activity, such as detection of the downloading of an audio file from an associated network by monitoring network traffic or with a browser plug-in. The capture processor <b>124</b> can identify a media file event by system activity. For example, the capture processor <b>124</b> can determine that a media application is in use, such as through use of an Application Programming Interface (API) with the media application, monitoring calls to an audio or video driver, or monitoring display calls to a display area, such as a window, associated with the media application. The capture processor <b>124</b> can capture event data by monitoring calls to an input or output device or driver, such as a microphone or speaker. Additionally, the capture processor <b>124</b> can extract event data associated with the media file from a display area associated with the media application. For example, the display area may contain metadata about an audio file being played, such as the title, artist, and other information. As another example, the display area can be analyzed to identify images that are likely to be photographs based on the distribution and variation in pixel intensity across sections of the screen. Whereas typical items on a display device, such as text and borders, contain sharp edges, photographs may contain smooth variations in pixel values, for example. The amount of compression possible on a segment of the screen may also help identify a photograph, because photographs may be less compressible. These features combined with the typical size of photographs on the display device, can allow the identification and extraction of photos. Analyzing the display area associated with a display device may be advantageous because identifying the media file alone may not allow identification of all desired event data, and analysis of the media application alone may not allow access to the event data for the media file.</p>
<p id="p-0046" num="0045">In <b>204</b>, the capture processor <b>124</b> determines whether the event captured is an indexable event. As explained above, some real-time events may not be indexed (non-indexable real-time events). In one embodiment, non-indexable real-time events are used to update the current user state and are, for example, examining a portion of an article, changing an article, and closing an article. In this embodiment, non-indexable events are not indexed or sent for storage by the indexer <b>130</b>. Indexable events can be indexable real-time events or historical events. For example, in one embodiment, the playing, viewing or editing of a media file can be considered an indexable event. Opening a media application may be considered a non-indexable event.</p>
<p id="p-0047" num="0046">If an indexable event is determined, then, in <b>206</b>, the event can be sent by the capture processor <b>124</b> to the queue <b>126</b> with an indication that it is an indexable event. In the embodiment shown, indexable real-time events are sent to both a user state queue and an index queue within queue <b>126</b> and historical events are sent to the index queue within the queue <b>126</b>. Alternatively, indexable real-time events may not be sent to the user state queue to save computational time. The capture processor <b>124</b> can send the event in a form described by an event schema to the queue <b>126</b>. If the event is determined to be a non-indexable event, then, in <b>206</b>, the non-indexable event can be sent by the capture processor <b>124</b> to the user state queue of the queue <b>126</b> with an indication that it is not to be indexed.</p>
<p id="p-0048" num="0047">In one embodiment, the queue <b>126</b> holds the event until the search engine is ready to receive it. Based on the event data, the event can be prioritized on the queue <b>126</b> for handling. For example, in one embodiment, historical events are given a lower priority for processing by the queue <b>126</b> than real-time events. In one embodiment, when the indexer <b>130</b> is ready to process another event, it can retrieve an event or events from the index queue in the queue <b>126</b>. The query system <b>132</b> can retrieve an event or events from the user state queue of the queue <b>126</b>, when it is ready to update the user state. In another embodiment, a queue is not used and events are sent directly to the search engine <b>122</b> from the capture processor <b>124</b>.</p>
<p id="p-0049" num="0048">In <b>208</b>, the event is indexed. In one embodiment, the indexer <b>130</b> indexes the event and can retrieve an event from the queue <b>126</b> when it is ready to process the event. In one embodiment, the indexer <b>130</b> determines if the event is a duplicate event and if not assigns an Event ID to the event. The indexer <b>130</b> can also associate the event with related events. For example, if the user <b>112</b><i>a </i>edits an image file and saves it with the same filename, thus overwriting the original file, the revised image file can be associated to the original image file by the indexer <b>130</b> through an associated events object. In the embodiment shown in <figref idref="DRAWINGS">FIG. 2</figref>, the indexer determines indexable terms associated with the event, dates and times associated with the event, and other data associated with the event from the event schema.</p>
<p id="p-0050" num="0049">In <b>210</b>, the event can be stored in the database <b>144</b> and the content of the event can be stored in the repository <b>146</b>. The event, such as a media file event can be stored in the data store <b>140</b>. In the embodiment shown in <figref idref="DRAWINGS">FIG. 2</figref>, the associated media file can be stored in the repository <b>146</b>. The event and associated events object can be stored in the database <b>144</b>. In one embodiment, the Event ID from the event is associated with terms in the index <b>142</b> that equate to the indexable terms of the event. For example, for an audio file event given an event ID &#x201c;17&#x201d; (for example) associated with an audio file of a David Bowie song entitled &#x201c;Let's Dance&#x201d;, the event ID, 17, can be associated with the terms &#x201c;David&#x201d;, &#x201c;Bowie&#x201d;, and &#x201c;Dance&#x201d; in the index <b>142</b>.</p>
<p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. 3</figref> illustrates an exemplary method <b>300</b> that provides a method for processing a search query, in accordance with one embodiment. This exemplary method is provided by way of example, as it will be appreciated from the foregoing description of exemplary embodiments that there are a variety of ways to carry out methods according to the present invention. The method <b>300</b> shown in <figref idref="DRAWINGS">FIG. 3</figref> can be executed or otherwise performed by any of various systems. The method <b>300</b> is described below as carried out by the system <b>100</b> shown in <figref idref="DRAWINGS">FIG. 1</figref> by way of example, various elements of the system <b>100</b> are referenced in the example method of <figref idref="DRAWINGS">FIG. 3</figref>.</p>
<p id="p-0052" num="0051">In <b>302</b>, the query system <b>132</b> receives a search query. The query can be an explicit query or an implicit query. An explicit query can be generated by a user inputting query terms into a search engine interface displayed on the client device <b>102</b><i>a</i>. An implicit query can be generated by the query system <b>132</b> based on a current user state. For example, the user <b>112</b><i>a </i>can enter the terms &#x201c;David Bowie&#x201d; into a search engine interface and the query system <b>132</b> can generate a search query based on the input terms. Alternatively, the user <b>112</b><i>a </i>can input the terms &#x201c;David Bowie&#x201d; in an email the user <b>112</b><i>a </i>is drafting. The capture processor <b>124</b> can capture this contextual event and it can be received by the query system <b>132</b>. The query system <b>132</b> can generate a search query based on the terms captured in this contextual event.</p>
<p id="p-0053" num="0052">In <b>304</b>, the query system <b>132</b> locates articles relevant to the search query. The query system <b>132</b> can locate relevant articles by matching the terms of the search query with terms located in the index <b>142</b>. For example, the query system <b>132</b> can match the terms of the query with terms in the index <b>142</b>. From the matched terms, the query system <b>132</b> can determine events, stored in the database <b>144</b>, associated with the terms through event IDs associated with the matched terms. From these events, the query system <b>132</b> can determine articles associated with the events. The articles can be stored in the repository <b>146</b>. For the search query &#x201c;David Bowie,&#x201d; for example, the query system <b>132</b> can locate various articles from various client applications that contain the query terms, such as, audio files, video files, image files, emails, instant messenger messages, previously viewed web pages, and other articles matched to the terms of the query. The query can also designate a time and the query system <b>132</b> can locate articles that are relevant to the query time. The query system <b>132</b> can also retrieve network articles, such as web pages, from a search engine <b>170</b> that are relevant to the query.</p>
<p id="p-0054" num="0053">In <b>306</b>, the query system <b>132</b> generates a search result set based on the located articles. In one embodiment, the query system <b>132</b> can sort and rank the located articles based on a variety of signals indicating the user's <b>112</b><i>a </i>preference for the articles. The search result set can contain a ranked list of article identifiers for articles associated with a variety of different client applications <b>120</b> and article identifiers for network articles. Each link can also contain a summary of the article as well as an image associated with the article. The result set can contain media files, such as audio, video, and image files and other articles (non-media files), such as emails, instant messenger messages, and web pages. For example, for the query &#x201c;David Bowie&#x201d;, the result set may contain audio files of songs by David Bowie, video files of music videos from David Bowie, image files of David Bowie, emails and instant messenger messages discussing David Bowie, and web sites relating to David Bowie.</p>
<p id="p-0055" num="0054">In <b>308</b>, the search result set is formatted by the formatter <b>134</b>. In one embodiment, the formatter <b>134</b> formats the result set in XML or HTML. In <b>310</b>, the search result set is output, such as displayed on the client device <b>102</b><i>a</i>, by the display processor <b>128</b>. For example, the search result set can be displayed on the display device associated with the client device <b>102</b><i>a</i>. If the search result set was generated in response to an implicit query the display processor <b>128</b> can determine an appropriate time to display the implicit search result set. The user <b>112</b><i>a </i>can then select an article identifier for an article and cause the output of the associated article on the display device.</p>
<p id="p-0056" num="0055">The environment shown reflects a client-side search engine architecture embodiment. Other embodiments are possible, such as a stand-alone client device or a network search engine.</p>
<p id="p-0057" num="0056">While the above description contains many specifics, these specifics should not be construed as limitations on the scope of the invention, but merely as exemplifications of the disclosed embodiments. Those skilled in the art will envision many other possible variations that are within the scope of the invention.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>That which is claimed:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A system for processing media files comprising:</claim-text>
<claim-text>a processor for executing computer program code; and</claim-text>
<claim-text>a non-transitory computer-readable storage medium storing executable computer program code for:
<claim-text>monitoring at least one application for occurrences of events wherein at least one event is associated with a media file;</claim-text>
<claim-text>capturing the at least one event upon the occurrence of the event by queuing event data associated with the at least one event at a position in the queue;</claim-text>
<claim-text>indexing and storing at least some of the event data and the media file associated with the at least one event at a time after the occurrence of the at least one event, wherein the time is based on performance data indicating a readiness to process the at least one event and the position in the queue;</claim-text>
<claim-text>receiving a search query;</claim-text>
<claim-text>locating at least one relevant media file from the indexed and stored events relevant to the search query; and</claim-text>
<claim-text>outputting a result set comprising the at least one relevant media file.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the search query is an explicit query.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the search query is an implicit query.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein capturing the at least one event associated with the media file comprises monitoring a media application to determine event data associated with the at least one event and compiling the at least one event from at least some of the event data.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein capturing the at least one event associated with the media file comprises determining event data external to the media file.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The system of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the event data external to the media file is determined based at least in part on one or more of a local database, a global database, a web page, and a network search engine.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the media file comprises an audio file.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the media file comprises a video file.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the media file comprises an image file.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the media file comprises a combination of audio and video.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the media file comprises a scripted presentation of audio and video.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein capturing the at least one event associated with the media file comprises determining text that identifies the media file and including the text as event data associated with the at least one event.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein indexing at least some of the event data associated with the media file comprises associating the at least one event with at least one associated event.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the associated event comprises a different version of the at least one event.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein capturing the at least one event associated with the media file comprises identifying the at least one event based at least in part on one or more of network activity, system activity, and media application activity.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein capturing the at least one event associated with the media file comprises identifying the at least one event based at least in part on a display area associated with a media application and identifying at least some of the event data by analyzing the display area.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein capturing the event at least one associated with the media file comprises identifying the at least one event based at least in part on calls to input or output devices and identifying at least some of the event data by analyzing the calls. </claim-text>
</claim>
</claims>
</us-patent-grant>
