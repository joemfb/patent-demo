<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626427-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626427</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13422235</doc-number>
<date>20120316</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>KR</country>
<doc-number>10-2011-0045568</doc-number>
<date>20110516</date>
</priority-claim>
</priority-claims>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20110101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>19</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>701119</main-classification>
<further-classification>123 4112</further-classification>
<further-classification>123 4131</further-classification>
<further-classification>123 4149</further-classification>
<further-classification>123 4164</further-classification>
<further-classification>165 41</further-classification>
<further-classification>165 51</further-classification>
<further-classification>165122</further-classification>
<further-classification>165140</further-classification>
<further-classification>165299</further-classification>
<further-classification>362548</further-classification>
<further-classification>362369</further-classification>
<further-classification>180412</further-classification>
</classification-national>
<invention-title id="d2e61">User interface method for terminal for vehicle and apparatus thereof</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5406493</doc-number>
<kind>A</kind>
<name>Goto et al.</name>
<date>19950400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>701462</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6385539</doc-number>
<kind>B1</kind>
<name>Wilson et al.</name>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>701468</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>7039521</doc-number>
<kind>B2</kind>
<name>Hortner et al.</name>
<date>20060500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>7353110</doc-number>
<kind>B2</kind>
<name>Kim</name>
<date>20080400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>7457705</doc-number>
<kind>B2</kind>
<name>Takahashi et al.</name>
<date>20081100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>8195386</doc-number>
<kind>B2</kind>
<name>Hu et al.</name>
<date>20120600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>701436</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2005/0192736</doc-number>
<kind>A1</kind>
<name>Sawada et al.</name>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>701117</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2008/0033632</doc-number>
<kind>A1</kind>
<name>Lee</name>
<date>20080200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2008/0195315</doc-number>
<kind>A1</kind>
<name>Hu et al.</name>
<date>20080800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>701212</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>KR</country>
<doc-number>10-2006-0027717</doc-number>
<kind>A</kind>
<date>20060300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>KR</country>
<doc-number>10-2007-0030449</doc-number>
<kind>A</kind>
<date>20070300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>KR</country>
<doc-number>10-2009-0064946</doc-number>
<kind>A</kind>
<date>20090600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>28</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>701119</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>123 4112</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>123 4131</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>123 4149</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>123 4164</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>165 41</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>165 51</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>165122</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>165140</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>165299</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>165916</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>192 4892</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>192 50</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>362548</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>362369</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>180412</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification> 74650</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>11</number-of-drawing-sheets>
<number-of-figures>12</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120296561</doc-number>
<kind>A1</kind>
<date>20121122</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Park</last-name>
<first-name>Hyung-Kil</first-name>
<address>
<city>Gyeonggi-do</city>
<country>KR</country>
</address>
</addressbook>
<residence>
<country>KR</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Kim</last-name>
<first-name>Chang-Soo</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
<residence>
<country>KR</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Choi</last-name>
<first-name>Jun-Yong</first-name>
<address>
<city>Gyeonggi-do</city>
<country>KR</country>
</address>
</addressbook>
<residence>
<country>KR</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Park</last-name>
<first-name>Hyung-Kil</first-name>
<address>
<city>Gyeonggi-do</city>
<country>KR</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Kim</last-name>
<first-name>Chang-Soo</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Choi</last-name>
<first-name>Jun-Yong</first-name>
<address>
<city>Gyeonggi-do</city>
<country>KR</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Cha &#x26; Reiter, LLC</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Samsung Electronics Co., Ltd.</orgname>
<role>03</role>
<address>
<city>Suwon-si, Gyeonggi-do</city>
<country>KR</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Marc</last-name>
<first-name>McDieunel</first-name>
<department>3667</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A user interface method for a terminal for a vehicle is provided. The terminal obtains position information to detect a point of a road. A road image of a driving direction is obtained, a lanes of the road represented on the obtained road image is recognized, and a point of a road and lane in which the vehicle having the terminal arranged therein is detected. A virtual road image regarding the recognized lanes is generated, and the generated virtual lanes are added to the road image of the driving direction of the vehicle, and displayed. Traffic information for each lane and surrounding information (i.e. lane closure, construction, accident, etc.) at the detected point of the relevant road are obtained, and the obtained information is displayed for each virtual lane.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="104.14mm" wi="168.74mm" file="US08626427-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="215.90mm" wi="159.68mm" file="US08626427-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="118.36mm" wi="136.23mm" file="US08626427-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="142.49mm" wi="146.64mm" file="US08626427-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="257.47mm" wi="168.49mm" file="US08626427-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="150.96mm" wi="111.00mm" file="US08626427-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="189.74mm" wi="215.90mm" file="US08626427-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="135.55mm" wi="160.10mm" file="US08626427-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="146.39mm" wi="168.83mm" file="US08626427-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="130.05mm" wi="155.87mm" file="US08626427-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="138.01mm" wi="158.33mm" file="US08626427-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="220.30mm" wi="142.92mm" file="US08626427-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">CLAIM OF PRIORITY</heading>
<p id="p-0002" num="0001">This application claims the benefit of priority under 35 U.S.C. &#xa7;119(a) from a Korean patent application filed in the Korean Intellectual Property Office on May 16, 2011 and assigned Serial No. 10-2011-0045568, the entire disclosure of which is hereby incorporated by reference in its entirety.</p>
<heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">1. Field of the Invention</p>
<p id="p-0004" num="0003">The present invention relates to a terminal for a vehicle. More particularly, the present invention relates to a user interface method for providing information helpful to an operator while driving a vehicle and an apparatus thereof.</p>
<p id="p-0005" num="0004">2. Description of the Related Art</p>
<p id="p-0006" num="0005">An automobile in many parts of the world has become a primary movement means of a modern society. However, it is well known that an automobile traffic accident can endanger lives of a driver and a fellow passengers, and way to reduce the amount of automobile accidents is sought by traffic experts, safety experts, and insurance companies.</p>
<p id="p-0007" num="0006">Recently, various terminals for a vehicle have been developed to assist with driving, and example of such a terminal is a navigation terminal. The navigation terminal provides information of a path and directions for the fastest route to drive, and even informs the driver when to turn, etc. Furthermore, a vehicle has various equipment for safe driving, such as, for example, a rear camera allowing a user to view the back of the vehicle when pulling out of a parking spot, a distance sensor informing a distance between cars to assist in a safe park, etc.</p>
<p id="p-0008" num="0007">With regard to the navigation terminal, typically, such navigation terminals provide only a path from a departure to a destination and does not provide traffic information for regarding potential hazards on the roadway. As there is a danger of a traffic accident caused by a car in front, back or to either side of the vehicle always exists, an alternative method for providing traffic information for each surrounding vehicle is needed.</p>
<heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0009" num="0008">An exemplary aspect of the present invention provides a user interface method for assisting a user to drive safely and an apparatus thereof for driving a vehicle.</p>
<p id="p-0010" num="0009">Another exemplary aspect of the present invention is to provide a user interface method for providing information helpful to driving a vehicle, and an apparatus thereof.</p>
<p id="p-0011" num="0010">Still another exemplary aspect of the present invention is to provide a user interface method for providing traffic information for each road and surrounding information and an apparatus thereof for driving a vehicle.</p>
<p id="p-0012" num="0011">Yet another exemplary aspect of the present invention is to provide a user interface method for providing traffic information and surrounding information of a roadway to be changed with priority in case of changing the carriageway in driving a vehicle, and an apparatus thereof.</p>
<p id="p-0013" num="0012">Further yet another exemplary aspect of the present invention is to provide a user interface method for inducing a user's safe driving by additionally showing traffic information and surrounding information for each roadway and the surrounding lanes by providing an image that shows an actual road, and an apparatus thereof.</p>
<p id="p-0014" num="0013">Other exemplary aspects, advantages and salient features of the presently claimed invention will become apparent a person of ordinary skill in the art from the following detailed description, which, taken in conjunction with the annexed drawings, discloses exemplary embodiments of the invention.</p>
<p id="p-0015" num="0014">In accordance with an exemplary aspect of the present invention, a user interface method for a terminal for a vehicle comprises obtaining position information to detect a point of a road, obtaining a road image of a driving direction, recognizing a road represented on the obtained road image, and detecting a point of a road, generating a virtual road regarding the recognized road, adding the generated virtual road to the road image of the driving direction, and displaying the same, and obtaining traffic information for each road and surrounding information at the detected point of the relevant road, and informing an operator or passenger of the vehicle with the obtained information for each virtual road.</p>
<p id="p-0016" num="0015">In accordance with another exemplary aspect of the present invention, an apparatus for a terminal for a vehicle comprises an output unit that outputs image data and voice data, a position information receiver that obtains position information, a rear camera that captures a road image of a driving direction, a traffic information receiver that receives traffic information for each road via at least one network, a vehicle sensor receiver that communicates with a vehicle via a wired line or wirelessly, and obtains surrounding information for each road from the vehicle, a storage that stores a predetermined program for controlling an overall operation, and various data input/output when a control operation is performed, and a controller that performs an overall control; wherein the controller recognizes a road from the road image of the driving direction, detects a road in which the terminal is positioned, generates a virtual road for the recognized road and adds the generated virtual road to the road image of the driving direction and displays the same, detects a point of a road from the position information, and displays traffic information and surrounding information for each road at the detected point of the relevant road for each virtual road.</p>
<p id="p-0017" num="0016">A vehicle having a user interface apparatus according to the present invention is provided wherein the apparatus comprises an output unit for outputting image data and voice data, a position information receiver for obtaining position information, a rear camera for capturing a road image of a driving direction, a traffic information receiver for receiving traffic information for each road being traveled via at least one network, a vehicle sensor receiver for communicating with a vehicle via a wired line or wirelessly, and obtaining surrounding information for each road from the vehicle, a storage for storing a predetermined program for controlling an overall operation, and various data input/output when a control operation is performed, and a controller for performing an overall control, wherein the controller recognizes a road from the road image of the driving direction, detects a road in which the terminal is positioned, generates a virtual road for the recognized road and adds the generated virtual road to the road image of the driving direction and displays the same, detects a point of a road from the position information, and displays traffic information and surrounding information for each road at the detected point of the relevant road for each virtual road.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0018" num="0017">The above and other exemplary aspects, features and advantages of certain exemplary embodiments of the present invention will be more apparent from the following description taken in conjunction with the accompanying drawings in which:</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 1</figref> is a perspective view illustrating a terminal for a vehicle according to an exemplary embodiment of the present invention;</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 2A</figref> is a view illustrating that a camera and a sensor are configured in a vehicle according to an exemplary embodiment of the present invention;</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 2B</figref> is a block diagram illustrating an apparatus for obtaining sensor information and vehicle operation information in a vehicle according to an exemplary embodiment of the present invention;</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 3</figref> is a block diagram illustrating a terminal for a vehicle according to an exemplary embodiment of the present invention;</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 4</figref> is a flowchart illustrating exemplary operation of a user interface procedure for a terminal for a vehicle according to an exemplary embodiment of the present invention;</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 5</figref> is a screen showing a road image of a front side of a vehicle in a terminal for a vehicle according to an exemplary embodiment of the present invention;</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 6</figref> is a screen showing a user interface in a terminal for a vehicle according to an exemplary embodiment of the present invention;</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 7</figref> is a screen showing a user interface in a terminal for a vehicle according to an exemplary embodiment of the present invention;</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIGS. 8 and 9</figref> are screens showing a user interface in a terminal for a vehicle according to an exemplary embodiment of the present invention; and</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIGS. 10A</figref> and B are screens showing a user interface in a terminal for a vehicle according to an exemplary embodiment of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<p id="p-0029" num="0028">Throughout the drawings, like reference numerals will be understood to refer to like parts, components and structures.</p>
<heading id="h-0005" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0030" num="0029">The following description with reference to the accompanying drawings is provided to assist a person of ordinary skill in the art with a comprehensive understanding of exemplary embodiments of the User Interface Method for a Terminal For a Vehicle and Apparatus thereof as defined by the claims and their equivalents. The description contains various specific details to assist the person of ordinary skill in the art with that understanding but these details are to be regarded as merely exemplary and illustrative. Accordingly, those of ordinary skill in the art will recognize that various changes and modifications of the exemplary embodiments described herein can be made without departing from the scope and spirit of the invention as defined by the appended claims. Also, descriptions of well-known functions and constructions may be omitted for clarity and conciseness when their inclusion may obscure appreciation of the present invention with such well-known functions and constructions.</p>
<p id="p-0031" num="0030">The terms and words used in the following description and claims are not limited to the bibliographical meanings, but, are merely used by the inventor to enable a clear and consistent understanding of the invention. Accordingly, it should be apparent to those skilled in the art that the following description of exemplary embodiments of the present invention are provided for illustrative purposes only and not for the purpose of limiting the invention as defined by the appended claims and their equivalents.</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 1</figref> is a perspective view illustrating a terminal for a vehicle according to an exemplary embodiment of the present invention.</p>
<p id="p-0033" num="0032">Referring now to <figref idref="DRAWINGS">FIG. 1</figref>, the terminal <b>10</b> for a vehicle includes a case frame <b>11</b> forming an appearance and the following elements inserted into the case frame <b>11</b>. The terminal <b>10</b> for a vehicle preferably includes a display <b>12</b> for displaying image data, a speaker <b>13</b> for outputting voice data, a button <b>14</b> for input, a front camera <b>15</b>-<b>1</b> for image capturing, a rear camera <b>15</b>-<b>2</b> (not shown), an antenna <b>16</b> required for transmitting/receiving a Radio Frequency (RF) signal, and an external connector <b>17</b> for electrically connecting with an external device to communicate with the same. When a touch panel is added to the display <b>12</b>, a touch screen may be provided. The terminal <b>10</b> for a vehicle has a sensor, included but not limited to a Global Positioning System (GPS) sensor to detect a position and a movement velocity. Furthermore, the terminal <b>10</b> for a vehicle provides a driving path to a destination using a stored map, and provides traffic information regarding the driving path in the form of an image or voice. The traffic information regarding driving may be received via various networks.</p>
<p id="p-0034" num="0033">More particularly, the terminal <b>10</b> for a vehicle according to an exemplary embodiment of the present invention detects a point of a road corresponding to the position of the terminal <b>10</b> itself using the GPS sensor. In addition, the terminal <b>10</b> obtains a road image of a front direction of the vehicle, that is, the driving direction via the rear camera <b>15</b>-<b>2</b>, and recognizes a road represented on the obtained road image to detect a road on which the terminal <b>10</b> itself is positioned. The terminal <b>10</b> generates a virtual road regarding the recognized road and adds the virtual road to the image of the front direction of the vehicle. Furthermore, the terminal <b>10</b> obtains traffic information for each road at the detected point of the road and informs the traffic information for each virtual road to assist the user in visualizes the surrounding environment and possible risk. Furthermore, the terminal <b>10</b> for a vehicle according to an exemplary embodiment of the present invention may communicate with a vehicle and receive sensor information and vehicle operation information from the vehicle to inform the information for each virtual road. The sensor information provided by the vehicle may be information detected by a distance sensor. The distance sensor detects a distance the vehicle is from an obstacle. The obstacle can be fixed or in motion. The vehicle operation information provided by the vehicle may be information regarding a vehicle operation such as flickering of left/right directional light, steering operation of a steering wheel, on/off of a break, etc. Furthermore, the terminal <b>10</b> may track a user's eyes viewing a screen using the front camera <b>15</b>-<b>1</b>. As described later, only a relevant virtual road and corresponding information may be provided depending on flickering of the left/right directional light or steering direction of the steering wheel, and the user's eyes.</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 2A</figref> is a view illustrating a camera and a sensor are configured in a vehicle according to an exemplary embodiment of the present invention. Referring now to <figref idref="DRAWINGS">FIG. 2A</figref>, a vehicle according to an exemplary embodiment of the present invention includes one or more distance sensors <b>21</b>-<b>1</b>, <b>21</b>-<b>2</b>, <b>22</b>-<b>1</b>, and <b>22</b>-<b>2</b> provided to the front, the rear, and the sides of the vehicle. The distance sensor detects a distance up to an obstacle. The distance sensor comprise an ultrasonic sensor or an infrared sensor.</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 2B</figref> is a block diagram illustrating an apparatus for obtaining sensor information and vehicle operation information in a vehicle according to an exemplary embodiment of the present invention.</p>
<p id="p-0037" num="0036">Referring now to <figref idref="DRAWINGS">FIG. 2B</figref>, the apparatus preferably includes a controller <b>210</b> for performing a control of an overall operation, and a communication unit <b>270</b> for communicating with the terminal <b>10</b> for a vehicle. The elements for providing information to the controller <b>210</b> may include a distance sensor <b>220</b>, a directional light flicker detector <b>230</b>, a break operation detector <b>240</b>, a steering detector <b>250</b>, a camera unit <b>260</b>, etc. The distance sensor <b>220</b>, which can use optical or ultrasound, for example, detects a distance up to an object (for example, a counterpart vehicle) in the front, the rear, or the sideway, separated from the vehicle, and informs the controller <b>210</b> of the distance. The directional light flicker detector <b>230</b> detects flickering of a left directional light or a right directional light and informs the controller <b>210</b> of the detected flickering. The break operation detector <b>240</b> detects that a break operates and informs the controller <b>210</b> of the detected operation. The steering detector <b>250</b> detects steering of the steering wheel and informs the controller <b>210</b> of the detected steering. The camera unit <b>260</b> obtains an image of the front of the vehicle or an image of the rear of the vehicle and provides the obtained image to the controller <b>210</b>. The controller <b>210</b>, which comprises a processor or microprocessor, transmits information provided from respective elements to the terminal for a vehicle via the communication unit <b>280</b>. The controller may be configured for communicating with &#x201c;on-board&#x201d; systems that are included in some newly built vehicles, and the present invention can replace or augment such systems.</p>
<p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. 3</figref> is a block diagram illustrating a terminal for a vehicle according to an exemplary embodiment of the present invention.</p>
<p id="p-0039" num="0038">Referring now to <figref idref="DRAWINGS">FIG. 3</figref>, the terminal for a vehicle preferably includes an image receiver <b>310</b>, a sensor receiver <b>320</b>, a vehicle operation information receiver <b>330</b>, a traffic information receiver <b>340</b>, an output unit <b>350</b>, a storage unit <b>360</b>, an eyeball tracking unit <b>370</b>, and a controller <b>380</b>. These components may be further separated or further integrated than the exemplary layout shown in the drawings. The image receiver <b>310</b> receives image information and preferably includes a vehicle camera image receiver <b>311</b> and a terminal camera image receiver <b>312</b>. The vehicle camera image receiver <b>311</b> receives image information captured by a camera provided to the vehicle from the vehicle. The terminal camera image receiver <b>312</b> receives image information captured by cameras provided to the terminal. The terminal camera image receiver <b>312</b> includes a front camera image receiver <b>313</b> for obtaining image information captured by a front camera <b>315</b>, and a rear camera image receiver <b>314</b> for obtaining image information captured by a rear camera <b>316</b>. The rear camera <b>316</b> keeps &#x201c;eyes&#x201d; on the front of a vehicle, that is, a road of the driving direction.</p>
<p id="p-0040" num="0039">A person of ordinary skill in the art viewing the terminal in <figref idref="DRAWINGS">FIG. 1</figref> will appreciate that camera <b>15</b>-<b>1</b> is positioned similarly to front camera <b>315</b> and camera <b>15</b>-<b>2</b> is positioned similarly to rear camera <b>316</b>. In other words, the &#x201c;rear camera&#x201d; is in the rear of the terminal <b>10</b>, and when in the vehicle, would capture in its intended position, would capture images in front of the vehicle, whereas front camera <b>315</b> would capture images of the driver or rear of the vehicle. Thus, if the terminal is positioned, for example, on the top of the dashboard, the rear camera <b>316</b> would obtain images of the front of the road, similar to rear camera <b>15</b>-<b>2</b> (shown in <figref idref="DRAWINGS">FIG. 1</figref>) which is actually located in the back of the terminal <b>10</b>, which is why it is not shown and represented by dashed lines. The front camera <b>315</b> observes a driver's view of a vehicle, and could be positioned, for example, similar to front camera <b>15</b>-<b>1</b> shown in <figref idref="DRAWINGS">FIG. 1</figref>, at least in terms of front versus rear.</p>
<p id="p-0041" num="0040">The sensor receiver <b>320</b> receives information detected by various sensors, and includes a vehicle sensor receiver <b>321</b> and a terminal sensor receiver <b>322</b>. The vehicle sensor receiver <b>321</b> receives information detected by a sensor (for example, a distance sensor) provided to the vehicle from the vehicle. The terminal sensor receiver <b>322</b> obtains information detected by sensors provided to the terminal. Known sensors provided to the terminal may be an acceleration sensor <b>323</b>, a gyro sensor <b>324</b>, a GPS sensor <b>326</b>, etc. More particularly, position information provided from the GPS sensor <b>326</b> is used for detecting a point of a road corresponding to the position of the terminal, and thus, the position of the vehicle.</p>
<p id="p-0042" num="0041">The vehicle operation information receiver <b>330</b> communicates with a vehicle wirelessly or via a wired line, and receives information regarding a vehicle operation from the vehicle. The vehicle operation may be flickering of left/right directional light, steering of a steering wheel, on/off of a break, etc.</p>
<p id="p-0043" num="0042">The traffic information receiver <b>340</b> receives information regarding a factor having an influence on traffic at the point of the road detected using the position information via at least one network. More particularly, the traffic information receiver <b>340</b> may receive traffic information for each road, including the road currently being traveled on and adjacent roads or roads that recommended by navigation to arrive at one's desired destination. The factor having an influence on the traffic may be a traffic circumstance for each road section, an accident, a construction, etc.</p>
<p id="p-0044" num="0043">The output unit <b>350</b> preferably includes an image output unit <b>351</b>, typically a display) for outputting image data and a voice output unit <b>352</b> for outputting voice data.</p>
<p id="p-0045" num="0044">The storage <b>360</b> stores map data for providing a recommended path for travel, a predetermined program for controlling an overall operation, and various data input/output when a control operation is performed.</p>
<p id="p-0046" num="0045">The eyeball tracking unit <b>370</b> tracks a user's eye(s) while viewing a screen from an image provided from the front camera image receiver <b>313</b> of the image receiver <b>310</b>, and informs the controller <b>380</b> of the position of the user's eye(s) being tracked.</p>
<p id="p-0047" num="0046">The controller <b>380</b>, which comprises a processor or microprocessor configured to control an overall operation of the terminal, and includes a path generator <b>381</b>, a road capture unit <b>382</b>, a virtual road generator <b>383</b>, a traffic information indicator generator <b>384</b>, a surrounding information indicator generator <b>385</b>, and a voice generator <b>386</b>.</p>
<p id="p-0048" num="0047">The path generator <b>381</b> generates a path from map data of the storage <b>360</b> and outputs the generated path to the output unit <b>350</b>.</p>
<p id="p-0049" num="0048">With continued reference to <figref idref="DRAWINGS">FIG. 3</figref>, the position determining unit <b>382</b> receives position information from the terminal sensor receiver <b>322</b> to detect a point of a road corresponding to the position information.</p>
<p id="p-0050" num="0049">The road capture unit <b>383</b> recognizes a road represented on a road image of the front of a vehicle provided from the rear camera image receiver <b>314</b>, and detects a road as well as the particular lane corresponding to the position of the vehicle having the terminal. The road capture unit <b>383</b> may recognize a road and identify the lane by analyzing a road image of the front of the vehicle via an image analysis method such as an intelligence analysis method, a two dimensional binary method, etc.</p>
<p id="p-0051" num="0050">The virtual road generator <b>384</b> generates a virtual image of a road with traffic lanes for a road recognized by the carriage capture unit <b>383</b>, and adds the virtual image of the road to the image displayed of the front of the vehicle afterward being output to the image output unit <b>352</b>.</p>
<p id="p-0052" num="0051">The traffic information indicator generator <b>385</b> receives traffic information for each road obtained by the traffic information receiver <b>340</b> with respect to a point of a relevant road detected by the position determining unit <b>382</b>, and loads an indicator for informing the user of traffic information for each virtual road from the storage <b>360</b>.</p>
<p id="p-0053" num="0052">After that, the traffic information indicator generator <b>385</b> outputs a relevant indictor for each lane of the virtual road via the image output unit <b>352</b>. Furthermore, the traffic information indicator generator <b>385</b> may represent a degree of swift driving (i.e. speed) using visually distinctive characteristics (i.e. different colors, brightness, size, highlighting, etc.) of a virtual road based on the traffic information for each road and surrounding information. The surrounding information is subsequently described herein. For example, a virtual road allowing relatively high speeds may be represented using a blue color, and a virtual road requiring relatively slower driving may be represented using a red color.</p>
<p id="p-0054" num="0053">The surrounding information indicator generator <b>386</b> determines surrounding information from information sensed by vehicle sensors obtained by the sensor receiver <b>320</b> and information sensed by sensors inside the terminal, and generates/loads an indicator for representing the determined surrounding information from the storage <b>360</b>. The surrounding information may include a distance up to a front vehicle, a distance up to a vehicle in the sideway, etc. After that, the surrounding information indicator generator <b>386</b> outputs a relevant indicator for each virtual road via the image output unit <b>352</b>. For example, an indicator representing a distance up to a front vehicle is displayed on a virtual road and/or a lane on which the vehicle is positioned, and an indicator representing a distance up to a vehicle in the sideway is displayed on a side virtual road.</p>
<p id="p-0055" num="0054">The voice generator <b>387</b> loads voice regarding the traffic information and the surrounding information from the storage <b>360</b>, and outputs the voice to the voice output unit <b>352</b>.</p>
<p id="p-0056" num="0055">More particularly, the controller <b>380</b> may control the elements to inform a relevant virtual carriageway, corresponding traffic information, and corresponding surrounding information depending on a user's eyes detected by the eyeball tracking unit <b>370</b> among a plurality of virtual roads. For example, when the user views a left road on a road image of the front of a vehicle, a virtual road for the left road, corresponding traffic information, and corresponding surrounding information are displayed.</p>
<p id="p-0057" num="0056">Furthermore, the controller <b>380</b> may control the elements to inform a relevant virtual road, corresponding traffic information, and corresponding surrounding information depending on information provided by the vehicle operation information receiver <b>330</b> among a plurality of virtual roads. For example, when a left directional light flickers or a steering wheel is rotated to the left, a virtual road for the left road, corresponding traffic information, and corresponding surrounding information are added to a road image of the front of the vehicle and displayed.</p>
<p id="p-0058" num="0057">In addition, the controller <b>380</b> may show only a virtual road to which the vehicle belongs from among a plurality of virtual roads, and provide traffic information and surrounding information regarding this virtual road.</p>
<p id="p-0059" num="0058">In addition, the controller <b>380</b> may change a method for reporting traffic information and surrounding information depending on information provided by the vehicle operation information receiver <b>330</b>. For example, when the vehicle's braking system engages to slow or stop the vehicle, an indicator representing a distance up to another vehicle directly in front or in the path of the present vehicle where the driver and terminal are positioned is displayed in relief.</p>
<p id="p-0060" num="0059">As described above, the controller <b>380</b> outputs data regarding a user interface via the output unit <b>350</b> of the terminal for a vehicle. The controller <b>380</b> is not limited thereto, but may transmit data regarding the user interface to a vehicle that communicates with the terminal via a wired line or wirelessly, and the vehicle may output the user interface via a relevant output means (for example, a display), which can include audio, visual, or a combination thereof.</p>
<p id="p-0061" num="0060">The controller <b>380</b> of the terminal for a vehicle according to an exemplary embodiment of the present invention may preferably include the image receiver <b>310</b>, a sensor receiver <b>320</b>, a vehicle information receiver <b>330</b>, a traffic information receiver <b>340</b>, and an eyeball tracking unit <b>370</b>.</p>
<p id="p-0062" num="0061">Based on this construction, a user interface method of the controller <b>380</b> according to an exemplary embodiment of the present invention is described with further reference to the accompanying drawings.</p>
<p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. 4</figref> is a flowchart illustrating exemplary operation of a user interface procedure for a terminal for a vehicle according to an exemplary embodiment of the present invention.</p>
<p id="p-0064" num="0063">Referring now to <figref idref="DRAWINGS">FIG. 4</figref>, at (S<b>401</b>) the controller <b>380</b> obtains position information to detect a point of a road corresponding to the position information.</p>
<p id="p-0065" num="0064">At (S<b>403</b>), the controller <b>380</b> obtains a road image of the area in front of the vehicle, recognizes a road represented on the obtained road image, and detects the road where the terminal is positioned. The recognition may optionally include the type of road and the name of the road.</p>
<p id="p-0066" num="0065">At (S<b>405</b>), the controller <b>380</b> generates a virtual image of the road corresponding to the recognized road from (S<b>403</b>), adds the virtual road to the road image of the front of the vehicle, and displays the same image.</p>
<p id="p-0067" num="0066">At (S<b>407</b>), the controller <b>380</b> obtains traffic information for each road and surrounding information at the detected point of the road, and informs the information for each virtual road.</p>
<p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. 5</figref> is a screen showing a road image of a front side of a vehicle in a terminal for a vehicle according to an exemplary embodiment of the present invention. This image is substantially similar to what a driver might see looking through the windshield.</p>
<p id="p-0069" num="0068">Referring now to <figref idref="DRAWINGS">FIG. 5</figref>, a road image <b>500</b> of the front of a different vehicle according to an exemplary embodiment of the present invention is captured by viewing a driving direction using a camera in at least one of a vehicle terminal or a camera of a vehicle. The road image <b>500</b> shows a surrounding vehicle, building, traffic lanes <b>511</b>, <b>512</b>, <b>513</b>, and <b>514</b>, and traffic lanes <b>501</b> and <b>502</b> discriminating respective roads. A vehicle and a terminal for a vehicle are positioned at a road <b>512</b> on the center of the screen in the lane adjust the very-most left lane, when one considers the solid line to be a painted or physical division for traffic that moves in opposite directions.</p>
<p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. 6</figref> is a screen showing a user interface in a terminal for a vehicle according to an exemplary embodiment of the present invention.</p>
<p id="p-0071" num="0070">Referring now to <figref idref="DRAWINGS">FIG. 6</figref>, a user interface according to an exemplary embodiment of the present invention adds a virtual road corresponding to a recognized road onto a road image <b>600</b> in a driving direction and displays the same. More particularly, each lane of the virtual road is designated and displayed by a color representing a degree of the speed of driving depending on traffic information and surrounding information. For example, the driving speed can be divided into high, medium, and low. A color corresponding to each degree may be designated. For example, a virtual lane <b>611</b> of the road whose driving speed is fast may be represented by a red color, a virtual lane <b>612</b> whose driving speed is low may be represented by a blue color, and a virtual lane <b>613</b> whose degree of swift driving is medium may be represented by a yellow color.</p>
<p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. 7</figref> is a screen showing a user interface in a terminal for a vehicle according to an exemplary embodiment of the present invention. As described with reference to <figref idref="DRAWINGS">FIG. 6</figref>, the user interface according to an exemplary embodiment of the present invention adds a virtual lane corresponding to a recognized road onto the road image <b>600</b> of the driving direction, and displays the same. More particularly, each virtual lane is designated and displayed using a color representing a degree of speed depending on traffic information and surrounding information. In <figref idref="DRAWINGS">FIG. 7</figref>, the color may signify slow speed as there is a construction sign, indicating that lane has construction further ahead (500 m away).</p>
<p id="p-0073" num="0072">Furthermore, a user interface according to an exemplary embodiment of the present invention adds an indicator <b>621</b> depending on traffic information for each lane of the road and indicators <b>622</b> and <b>623</b> depending on surrounding information for each lane to a virtual road, and displays the same.</p>
<p id="p-0074" num="0073"><figref idref="DRAWINGS">FIGS. 8 and 9</figref> are screens showing a user interface in a terminal for a vehicle according to an exemplary embodiment of the present invention. The user interface displays the construction described in <figref idref="DRAWINGS">FIGS. 6 and 7</figref>. However, in the case where vehicle operation information, more particularly, a relevant directional light flickers or eyes are tracked to a relevant road of a road image, only an indicator of a relevant virtual road and corresponding traffic information and an indicator of surrounding information are displayed. For example, when a left directional light of a vehicle flickers or a user keeps eyes on a left road of a road image, indicators of a virtual road corresponding to the left road, corresponding traffic information, and surrounding information are displayed.</p>
<p id="p-0075" num="0074"><figref idref="DRAWINGS">FIGS. 10A</figref> and B are screens showing a user interface in a terminal for a vehicle according to an exemplary embodiment of the present invention.</p>
<p id="p-0076" num="0075">As illustrated in <figref idref="DRAWINGS">FIGS. 10A and 10B</figref>, when a path is designated, the path is displayed (<b>1010</b>) on the road image <b>1000</b> of the front of the vehicle shown in real-time. However, as is known from a difference between <figref idref="DRAWINGS">FIG. 10A</figref> and <figref idref="DRAWINGS">FIG. 10B</figref>, the path is shown based on a lane where the vehicle is currently positioned and if one were to follow the exact path shown in either the lane in <figref idref="DRAWINGS">FIG. 10A</figref> or <b>10</b>B, would receive a traffic infraction. The present invention would in lieu of or in association with GPS or other types of navigation, direct the driver to move to the rightmost lane at a safe and legal distance prior to exiting the road to follow path <b>1010</b>.</p>
<p id="p-0077" num="0076">Consequently, the present invention is helpful to a user's safe driving by providing a virtual road with lanes based on the actual road being traveled, and includes traffic information for each lane and surrounding information to an image showing an actual road.</p>
<p id="p-0078" num="0077">The above-described methods according to the present invention can be implemented in hardware or as software or computer code that can be stored in a recording medium such as a CD ROM, an RAM, a floppy disk, a hard disk, or a magneto-optical disk or downloaded over a network and stored on a non-transitory machine readable medium, so that the methods described herein can be rendered in such software using a general purpose computer, or a special processor or in programmable or dedicated hardware, such as an ASIC or FPGA. As would be understood in the art, the computer, the processor, microprocessor controller or the programmable hardware include memory components, e.g., RAM, ROM, Flash, etc. that may store or receive software or computer code that when accessed and executed by the computer, processor or hardware implement the processing methods described herein. In addition, it would be recognized that when a general purpose computer accesses code for implementing the processing shown herein, the execution of the code transforms the general purpose computer into a special purpose computer for executing the processing shown herein.</p>
<p id="p-0079" num="0078">Although the invention has been shown and described with reference to certain exemplary embodiments thereof, it will be understood by those skilled in the art that various changes in form and details may be made therein without departing from the spirit and scope of the invention as defined by the appended claims and their equivalents. Therefore, the scope of the present invention should not be limited to the above-described embodiments but should be determined by not only the appended claims but also the equivalents thereof.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A user interface method for a terminal for a vehicle, the method comprising:
<claim-text>obtaining by a controller position information to detect a point of a road on which a vehicle having the terminal arranged therein is situated;</claim-text>
<claim-text>obtaining by the controller a road image of a driving direction of said vehicle having the terminal arranged therein, recognizing a type of road represented on the obtained road image, and detecting the point of a road;</claim-text>
<claim-text>generating by the controller a virtual road including lane markings identifying lanes as captured by the road image regarding the recognized road, and adding the generated virtual road including lanes to the road image of the driving direction displayed by a display; and</claim-text>
<claim-text>obtaining by the controller traffic information for each lane of the road, respectively, and surrounding information at the detected point of the road, and outputting the obtained information for each lane of the virtual road by the display.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the obtaining of the traffic information for each lane of the road and surrounding information at the detected point of said vehicle on the road, and the outputting of the obtained information for each virtual road comprises:
<claim-text>determining, by a processor, a degree of driving speed of each lane from the traffic information and surrounding, information; and</claim-text>
<claim-text>displaying the virtual road and including indicators about the determined degree of driving speed in each lane.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the displaying of the virtual road including indicators of the determined degree of driving speed in each lane comprises:
<claim-text>when the degree of driving speed is indicated as high, medium or low relatively as compared to a predetermined driving speed or a driving speed of lanes adjacent to a particular lane in which said vehicle is traveling, and displaying each lane of the virtual road using visually distinctive characteristics.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the visually distinctive characteristics include distinguishing colors.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the obtaining of the traffic information for each lane of the road and surrounding information at the detected point of the road, and the outputting the obtained information for each virtual road comprises:
<claim-text>obtaining the traffic information for each lane of the road with respect to the detected point of said vehicle in the road via at least one network; and</claim-text>
<claim-text>communicating with said vehicle via a wired line or wirelessly, and obtaining the surrounding information for each lane of the road from said vehicle.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the traffic information comprises at least one of a traffic circumstance for each section of road in front of said vehicle, wherein the traffic circumstance includes an accident, or a road construction.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the surrounding information comprises at least one of a predetermined distance from a front of said vehicle, a predetermined distance from a rear of said vehicle, and a lateral distance from a said vehicle.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising the user interface communicating with a communication unit of said vehicle, obtaining vehicle operation information from the vehicle, and displaying at least one of the virtual road, the traffic information, and the surrounding information depending on the obtained vehicle operation information.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the vehicle operation information comprises sensing one of flickering of a directional light and sensing a steering operation of a steering wheel.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein when communicating with the vehicle, obtaining vehicle operation information from the vehicle, and displaying at least one of the virtual road, the traffic information, and the surrounding information depending on the obtained vehicle operation information comprises:
<claim-text>when a left directional light of the vehicle flickers, or it is detected that a steering wheel is steered in a left direction, displaying a virtual road or traffic information or surrounding information for a leftmost lane of the road, and when a right directional light of the vehicle flickers, or it is detected that a steering wheel is steered in a right direction, displaying a virtual road or traffic information or surrounding information for a rightmost lane of the road.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein obtaining traffic information for each lane of the road and surrounding information at the detected point of the road where said vehicle is situated, and outputting the obtained information for each virtual lane of the road comprises:
<claim-text>outputting a virtual road corresponding to a lane of the road to which the terminal within said vehicle currently belongs, corresponding traffic information and surrounding information.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein obtaining traffic information for each lane of the road and surrounding information at the detected point of the road where said vehicle is situated, and outputting the obtained information for each virtual lane of the road comprises:
<claim-text>tracking a user's eyes, and displaying at least one of the virtual lane of the road corresponding to a road to which the tracked eyes are directed in the road image, the corresponding traffic information, and the surrounding information.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein obtaining traffic information for each lane of the road and surrounding information at the detected point of the road where said vehicle is situated, and outputting the obtained information for each virtual lane of the road comprises:
<claim-text>loading indicators for the traffic information and surrounding information to add the indicators to the display of the virtual road and display the same.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>communicating with a communicating unit of said vehicle via a wired line or wirelessly, and transmitting at least one of the road image of the driving direction, the virtual road, and the traffic information to said vehicle.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. An apparatus for a terminal for a vehicle, the apparatus comprising:
<claim-text>an output unit on a front of the terminal comprising at least a display for outputting image data and voice data;</claim-text>
<claim-text>a position information receiver for obtaining position information of a particular vehicle in which said terminal is arranged;</claim-text>
<claim-text>a rear camera on a back of the terminal for capturing a road image of a driving direction when the terminal is positioned in the vehicle so that the front of the terminal faces a steering control;</claim-text>
<claim-text>a traffic information receiver for receiving traffic information for each lane of road via at least one network;</claim-text>
<claim-text>a vehicle sensor receiver for communicating with said vehicle in which the terminal is arranged via a wired line or wirelessly, and obtaining surrounding information for each road from the vehicle;</claim-text>
<claim-text>a non-transitory storage for storing a predetermined program for controlling an overall operation, and various data input/output when a control operation is performed; and</claim-text>
<claim-text>a controller for performing an overall control of the terminal,</claim-text>
<claim-text>wherein the controller recognizes a road from the road image of the driving direction, detects a particular lane of the road in which the terminal is positioned, generates a virtual road image for the recognized road and adds generated virtual lanes to the road image of the driving direction and displays the same, detects a point of a road from the position information, and displays traffic information and surrounding information for each lane of the road at the detected point of the road in which said vehicle is situated.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The apparatus of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the controller determines a driving speed for each lane of the road from the traffic information and surrounding information, and displays each virtual lane of the road displayed with visually distinguishment depending on the determined driving speed of other vehicles lanes of the road.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The apparatus of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the controller categorizes driving speed into high, medium, and low, said driving speed relative to a predetermined speed or by comparing each of the lanes of the road to each other, and visually distinguishes the driving speed of each of the lanes of the road.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The apparatus of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein when the driving speed is high, displays the relevant virtual road using a first color or shade of color, when the degree of swiftness in driving is medium, displays the relevant virtual road using a second or shade of color, and when the degree of swiftness in driving is low, displays the relevant virtual road using a third color or shade of color.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The apparatus of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the traffic information comprises at least one of a traffic circumstance for each section of upcoming road within a predetermined distance, an accident, and a construction.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The apparatus of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the surrounding information comprises at least one of a distance from a front of said vehicle, a distance from a rear of said vehicle, and a lateral distance from said vehicle.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The apparatus of <claim-ref idref="CLM-00015">claim 15</claim-ref>, further comprising a vehicle operation information receiver for communicating with said vehicle via a wired line or wirelessly, and obtaining vehicle operation information from said vehicle.</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The apparatus of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the controller controls display of at least one of the virtual road, the traffic information, and the surrounding information depending on the vehicle operation information.</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The apparatus of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the vehicle operation information comprises at least one of flickering of a directional light and a steering operation of a steering wheel.</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. The apparatus of <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein when a left directional light of the vehicle flickers, or it is detected that a steering wheel is steered in a leftward direction, the controller displays a virtual road or traffic information or surrounding information for a leftward lane road relative to said vehicle, and when a right directional light of the vehicle flickers, or it is detected that a steering wheel is steered in a right direction, the controller displays a virtual road or traffic information or surrounding information for a rightward lane of the road relative to said vehicle.</claim-text>
</claim>
<claim id="CLM-00025" num="00025">
<claim-text>25. The apparatus of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the controller controls output of display of a virtual road corresponding to the road to which the terminal currently belongs, corresponding traffic information and surrounding information.</claim-text>
</claim>
<claim id="CLM-00026" num="00026">
<claim-text>26. The apparatus of <claim-ref idref="CLM-00015">claim 15</claim-ref>, further comprising a front camera on a front of the terminal for detecting a user's eyeballs and an eyeball tracking unit for tracking the user's eyes while viewing the road image of the driving direction via the front camera.</claim-text>
</claim>
<claim id="CLM-00027" num="00027">
<claim-text>27. The apparatus of <claim-ref idref="CLM-00026">claim 26</claim-ref>, wherein the controller controls output of display of at least one of the virtual road corresponding to a road to which the tracked eyes are directed in the road image, the corresponding traffic information, and the surrounding information.</claim-text>
</claim>
<claim id="CLM-00028" num="00028">
<claim-text>28. The apparatus of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the controller controls the display of load indicators for the traffic information and surrounding information and adds the indicators to a virtual road displayed by the display. </claim-text>
</claim>
</claims>
</us-patent-grant>
