<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626321-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626321</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>11787938</doc-number>
<date>20070418</date>
</document-id>
</application-reference>
<us-application-series-code>11</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>GB</country>
<doc-number>0607707.7</doc-number>
<date>20060419</date>
</priority-claim>
<priority-claim sequence="02" kind="national">
<country>GB</country>
<doc-number>0616677.1</doc-number>
<date>20060823</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>1468</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>17</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>700 94</main-classification>
<further-classification>381 61</further-classification>
<further-classification>381 62</further-classification>
<further-classification>381 63</further-classification>
</classification-national>
<invention-title id="d2e90">Processing audio input signals</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5208860</doc-number>
<kind>A</kind>
<name>Lowe et al.</name>
<date>19930500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5467401</doc-number>
<kind>A</kind>
<name>Nagamitsu et al.</name>
<date>19951100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>381 63</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5544249</doc-number>
<kind>A</kind>
<name>Opitz</name>
<date>19960800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>381 63</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5687239</doc-number>
<kind>A</kind>
<name>Inanaga et al.</name>
<date>19971100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5729612</doc-number>
<kind>A</kind>
<name>Abel et al.</name>
<date>19980300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>5761315</doc-number>
<kind>A</kind>
<name>Iida et al.</name>
<date>19980600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>381 18</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>5796843</doc-number>
<kind>A</kind>
<name>Inanaga et al.</name>
<date>19980800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>381 17</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6118875</doc-number>
<kind>A</kind>
<name>Moeller et al.</name>
<date>20000900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6307941</doc-number>
<kind>B1</kind>
<name>Tanner et al.</name>
<date>20011000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>381 17</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6385320</doc-number>
<kind>B1</kind>
<name>Lee</name>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>381 17</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6862356</doc-number>
<kind>B1</kind>
<name>Makino</name>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>7257230</doc-number>
<kind>B2</kind>
<name>Nagatani</name>
<date>20070800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>381 56</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>7822496</doc-number>
<kind>B2</kind>
<name>Asada et al.</name>
<date>20101000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>700 94</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2002/0141595</doc-number>
<kind>A1</kind>
<name>Jouppi</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2003/0172097</doc-number>
<kind>A1</kind>
<name>McGrath</name>
<date>20030900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>708300</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2003/0215104</doc-number>
<kind>A1</kind>
<name>Yamada et al.</name>
<date>20031100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>381309</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>2004/0136538</doc-number>
<kind>A1</kind>
<name>Cohen et al.</name>
<date>20040700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>2005/0265535</doc-number>
<kind>A1</kind>
<name>Kanada</name>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37920201</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>2005/0265558</doc-number>
<kind>A1</kind>
<name>Neoran</name>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>381 17</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>2006/0045294</doc-number>
<kind>A1</kind>
<name>Smyth</name>
<date>20060300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>381309</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>2006/0050897</doc-number>
<kind>A1</kind>
<name>Asada et al.</name>
<date>20060300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>381 98</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>2006/0222187</doc-number>
<kind>A1</kind>
<name>Jarrett et al.</name>
<date>20061000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>2007/0061026</doc-number>
<kind>A1</kind>
<name>Wang</name>
<date>20070300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>700 94</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>2007/0230725</doc-number>
<kind>A1</kind>
<name>Wang</name>
<date>20071000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>381309</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>EP</country>
<doc-number>1551205</doc-number>
<date>20050700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>WO</country>
<doc-number>WO 94/10816</doc-number>
<date>19940500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>17</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>700 94</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>28</number-of-drawing-sheets>
<number-of-figures>28</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20070255437</doc-number>
<kind>A1</kind>
<date>20071101</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Vernon</last-name>
<first-name>Christopher David</first-name>
<address>
<city>Beverley</city>
<country>GB</country>
</address>
</addressbook>
<residence>
<country>GB</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Vernon</last-name>
<first-name>Christopher David</first-name>
<address>
<city>Beverley</city>
<country>GB</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<last-name>Wray</last-name>
<first-name>James Creighton</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Sontia Logic Limited</orgname>
<role>03</role>
<address>
<city>Sheffield</city>
<country>GB</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>McCord</last-name>
<first-name>Paul</first-name>
<department>2656</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A method of processing an audio input signal represented as digital samples to produce a stereo output signal (having a left field and a right field) such that said stereo signal emulates the production of said audio signal from a specified audio source location relative to a listening source location. An audio input signal is received. An indication of an audio source location relative to a listening source location (an indicated location) is received. A broadband response file for each of the left field and the right field is selected from a plurality of stored files derived from empirical testing, dependant upon said indicated location. The audio input signal is convolved with each of the selected left field response file and the selected right field response file. Apparatus for processing an audio input signal.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="177.63mm" wi="138.60mm" file="US08626321-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="191.09mm" wi="130.89mm" file="US08626321-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="174.07mm" wi="142.58mm" file="US08626321-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="210.57mm" wi="115.06mm" file="US08626321-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="214.71mm" wi="65.79mm" file="US08626321-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="194.39mm" wi="136.48mm" file="US08626321-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="201.00mm" wi="125.05mm" file="US08626321-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="178.99mm" wi="128.61mm" file="US08626321-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="213.87mm" wi="139.62mm" orientation="landscape" file="US08626321-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="198.12mm" wi="133.77mm" file="US08626321-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="192.53mm" wi="128.10mm" orientation="landscape" file="US08626321-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="189.48mm" wi="72.81mm" file="US08626321-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="191.09mm" wi="73.58mm" file="US08626321-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="205.32mm" wi="138.77mm" file="US08626321-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="170.69mm" wi="117.18mm" file="US08626321-20140107-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="180.26mm" wi="147.32mm" file="US08626321-20140107-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00016" num="00016">
<img id="EMI-D00016" he="199.22mm" wi="117.69mm" file="US08626321-20140107-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00017" num="00017">
<img id="EMI-D00017" he="213.11mm" wi="137.75mm" file="US08626321-20140107-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00018" num="00018">
<img id="EMI-D00018" he="169.84mm" wi="115.57mm" file="US08626321-20140107-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00019" num="00019">
<img id="EMI-D00019" he="194.14mm" wi="115.32mm" file="US08626321-20140107-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00020" num="00020">
<img id="EMI-D00020" he="187.20mm" wi="137.58mm" file="US08626321-20140107-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00021" num="00021">
<img id="EMI-D00021" he="184.40mm" wi="144.53mm" file="US08626321-20140107-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00022" num="00022">
<img id="EMI-D00022" he="185.00mm" wi="121.41mm" file="US08626321-20140107-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00023" num="00023">
<img id="EMI-D00023" he="169.08mm" wi="116.50mm" file="US08626321-20140107-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00024" num="00024">
<img id="EMI-D00024" he="194.65mm" wi="147.66mm" orientation="landscape" file="US08626321-20140107-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00025" num="00025">
<img id="EMI-D00025" he="184.57mm" wi="139.36mm" orientation="landscape" file="US08626321-20140107-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00026" num="00026">
<img id="EMI-D00026" he="204.89mm" wi="139.45mm" orientation="landscape" file="US08626321-20140107-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00027" num="00027">
<img id="EMI-D00027" he="182.96mm" wi="148.51mm" file="US08626321-20140107-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00028" num="00028">
<img id="EMI-D00028" he="185.25mm" wi="155.96mm" file="US08626321-20140107-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application claims priority from United Kingdom Patent Application No. 06 07 707.7, filed Apr. 19, 2006, and United Kingdom Patent Application No. 06 16 677.1, filed Aug. 23, 2006, the entire disclosures of which are incorporated herein by reference in their entirety.</p>
<heading id="h-0002" level="1">TECHNICAL FIELD</heading>
<p id="p-0003" num="0002">The present invention relates to a method of processing audio input signals represented as digital samples to produce a stereo output signal having a left field and a right field. The invention also relates to apparatus for processing an audio input signal and a data storage facility having a plurality of broadband response files stored therein.</p>
<heading id="h-0003" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0004" num="0003">Attempts have been made to process audio input signals so as to place them in a perceived three-dimensional sound space. It has been assumed that to place a sound behind a subject for example, that this would require a source of sound (i.e. a loudspeaker) to be placed behind a subject. This logically implies that for three-dimensional sound to exist, complex speaker systems must be created with loudspeakers above and below the plane of the ears of the listener. Clearly, this is not a satisfactory solution, even for highly specified cinemas for example and therefore practical deployment of such systems has only existed in extreme environments with very specialised venues.</p>
<p id="p-0005" num="0004">Models have been constructed based upon attempting to hear what the ears hear. For example, experimentation has been performed using a standard dummy head in which the head has microphones mounted where each ear canal would normally sit. Experimentation has then been conducted in which many samples may be made of sounds from many positions. From this, it was possible to produce a head related transfer function, which is then in turn used to process sounds as though they had originated from certain desired positions. However, to date, the results have been less than ideal.</p>
<heading id="h-0004" level="1">BRIEF SUMMARY OF THE INVENTION</heading>
<p id="p-0006" num="0005">According to an aspect of the present invention, there is provided a method of processing an audio input signal represented as digital samples to produce a stereo output signal (having a left field and a right field) such that said stereo signal emulates the production of said audio signal from a specified audio source location relative to a listening source location, comprising the steps of: receiving said audio input signal; receiving an indication of an audio source location relative to a listening source location (an indicated location); selecting a broadband response file for a left field (a selected left field response file) from a plurality of stored files derived from empirical testing, dependant upon said indicated location; and selecting a broadband response file for a right field (a selected right field response file) from a plurality of stored files derived from empirical testing, dependant upon said indicated location; convolving the audio input signal with said selected left field response file; and convolving the audio input signal with said selected right field response file, to produce a stereo output signal such that said stereo output signal emulates the production of the audio input signal from said indicated location.</p>
<p id="p-0007" num="0006">According to a further aspect of the present invention, there is provided apparatus for processing an audio input signal, comprising: a first input device for receiving an audio input signal represented as digital samples; a second input device for receiving an indication of an audio source location relative to a listening source location (an indicated location); a processing device configured to: select a broadband response file for a left field (a selected left field response file) from a plurality of stored files derived from empirical testing, dependant upon said indicated location; select a broadband response file for a right field (a selected right field response file) from a plurality of stored files derived from empirical testing, dependant upon said indicated location; and convolve the audio input signal with said selected left field response file; and convolve the audio input signal with said selected right field response file, to produce a stereo output signal (having a left field and a right field) such that said stereo output signal emulates the production of the audio input signal from said indicated location.</p>
<p id="p-0008" num="0007">According to a second further aspect of the present invention, there is provided a computer-readable medium having computer-readable instructions executable by a computer such that, when executing said instructions, a computer will perform the steps of: receiving said audio input signal; receiving an indication of an audio source location relative to a listening source location (an indicated location); selecting a broadband response file for a left field (a selected left field response file) from a plurality of stored files derived from empirical testing, dependant upon said indicated location; and selecting a broadband response file for a right field (a selected right field response file) from a plurality of stored files derived from empirical testing, dependant upon said indicated location; convolving the audio input signal with said selected left field response file; and convolving the audio input signal with said selected right field response file, to produce a stereo output signal (having a left field and a right field) such that said stereo output signal emulates the production of the audio input signal from said indicated location.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE SEVERAL VIEWS OF THE DRAWINGS</heading>
<p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. 1</figref> shows a diagrammatic representation of a human subject;</p>
<p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. 2</figref> outlines a practical environment in which audio processing procedures described with reference to <figref idref="DRAWINGS">FIG. 1</figref> can be deployed;</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 3</figref> shows an overview of procedures performed to produce a broadband response file;</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 4</figref> illustrates steps to establish test points on an originating region according to a specific embodiment;</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 5</figref> illustrates apparatus for use in the production of broadband response files;</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 6</figref> illustrates use of the apparatus of <figref idref="DRAWINGS">FIG. 5</figref> to produce a first set of data for the production of broadband response files;</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 7</figref> illustrates use of the apparatus of <figref idref="DRAWINGS">FIG. 5</figref> to produce a second set of data for the production of broadband response files;</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 8</figref> illustrates a computer system identified in <figref idref="DRAWINGS">FIG. 5</figref>;</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 9</figref> shows procedures executed by the computer system of <figref idref="DRAWINGS">FIG. 8</figref>;</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 10</figref> illustrates the nature of generated output sounds;</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 11</figref> shows the storage of recorded reference input samples;</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 12</figref> shows the storage of recorded test input samples;</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 13</figref> shows further procedures executed by the computer system of <figref idref="DRAWINGS">FIG. 9</figref> to produce broadband response files;</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 14</figref> shows a convolution equation;</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 15</figref> illustrates a listener surrounded by an originating region from which sounds may be heard;</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 16</figref> shows further procedures executed by the computer system of <figref idref="DRAWINGS">FIG. 9</figref> to produce broadband response files;</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 17</figref> shows procedures executed in a method of processing an audio input signal in combination with a broadband response file;</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIGS. 18 and 19</figref> show further procedures executed in a method of processing an audio input signal in combination with a broadband response file;</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 20</figref> illustrates a sound emulating the production of an audio input signal from a moving source;</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 21</figref> illustrates a sound emulating the production of an audio input signal from an audio source location;</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 22</figref> shows the storage of broadband response files;</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 23</figref> shows a further procedure executed in a method of processing an audio input signal in combination with a broadband response file;</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 24</figref> illustrates a first example of a facility configured to make use of broadband response files;</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 25</figref> illustrates a second example of a facility configured to make use of broadband response files;</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 26</figref> illustrates a third example of a facility configured to make use of broadband response files;</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 27</figref> shows a first arrangement of loudspeakers; and</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 28</figref> shows a second arrangement of loudspeakers.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0006" level="1">DESCRIPTION OF THE BEST MODE FOR CARRYING OUT THE INVENTION</heading>
<p id="h-0007" num="0000"><figref idref="DRAWINGS">FIG. 1</figref></p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 1</figref> shows a diagrammatic representation of a human subject <b>101</b>.</p>
<p id="p-0037" num="0036">The human subject <b>101</b> is shown surrounded by a notional three-dimensional originating region <b>102</b>. An audio output may originate from a location, such as location <b>103</b>, relative to the human subject <b>101</b>. The left ear <b>104</b> and the right ear <b>105</b> of the human subject <b>101</b> may then receive the audio output. The inputs received by the left ear <b>104</b> and by the right ear <b>105</b> are subsequently processed in the brain of the human subject <b>101</b> to the effect that the human subject <b>101</b> perceives an origin of the audio output.</p>
<p id="p-0038" num="0037">It is desirable to receive an audio input signal represented as digital samples and to produce a stereo output signal having a left field and a right field in such a way that the stereo signal emulates the production of the audio signal from an originating position relative to the position of the human being.</p>
<p id="p-0039" num="0038">As described below, it is possible for a stereo signal, producing a left field and a right field, to emulate the generation of a sound source from a location relative to a listening source location.</p>
<p id="p-0040" num="0039">It is to be appreciated that whilst listening to sound from a particular audio source location, the perspective of the left ear <b>104</b> of the human subject <b>101</b> is different to the perspective of the right ear <b>105</b> of the human subject <b>101</b>. The brain of the human subject <b>101</b> processes the left perspective in combination with the right perspective to the effect that the perception of an origin of the audio output includes a perception of the distance of the audio source from the listening location in addition to relative bearings of the audio source.</p>
<p id="p-0041" num="0040">With reference to the notional originating region <b>102</b>, a sound originating position is defined by three co-ordinates based upon an origin at the centre of the region <b>102</b>, which in the diagrammatic representation of <figref idref="DRAWINGS">FIG. 1</figref> is the right ear <b>105</b> of the human subject <b>101</b>. From this origin, locations are defined in terms of a radial distance from the origin, leading to the notional generation of a sphere, such as the spherical shape of notional region <b>102</b>, and with respect to two angles defined with respect to a plane intercepting the origin. Thus, a plurality of co-ordinate locations, such as location <b>103</b>, on originating region <b>102</b> may be defined.</p>
<p id="p-0042" num="0041">In a specific embodiment, at least seven hundred and seventy (770) locations are defined. For each of these locations, a broadband response file is stored.</p>
<p id="p-0043" num="0042">When emulating an audio signal from a specified audio source location relative to a listening source location, a broadband response is selected dependent upon the relative audio source and listening source locations for each of a left field and a right field. Thereafter, each selected broadband response file is processed in combination with an audio input file by a process of convolution to produce left and right field outputs. A resulting stereo output signal will reproduce the audio input signal from the perspective of the listening location as if it had originated substantially from the indicated audio source location.</p>
<p id="h-0008" num="0000"><figref idref="DRAWINGS">FIG. 2</figref></p>
<p id="p-0044" num="0043">A practical environment in which audio processing procedures described with reference to <figref idref="DRAWINGS">FIG. 1</figref> can be deployed is outlined in <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0045" num="0044">At step <b>201</b> broadband response files are derived from empirical testing involving the use of at least one human subject. At step <b>202</b> the broadband response files are distributed to facilities such that they may then be used in the creation of three-dimensional sound effects. This approach may be used in many different types of facilities. For example, the approach may be used in sound recording applications, such as that described with respect to <figref idref="DRAWINGS">FIG. 24</figref>. Similarly, the techniques may be used for audio tracks in cinematographic film production as described with respect to <figref idref="DRAWINGS">FIG. 25</figref>. Furthermore, the techniques may be used for computer games, as described with respect to <figref idref="DRAWINGS">FIG. 26</figref>. It should also be appreciated that these applications are not exhaustive.</p>
<p id="p-0046" num="0045">At step <b>203</b> the data set is invoked in order to produce the enhanced sounds. Thus, at step <b>203</b> audio input commands are received at <b>204</b> and the processed audio output is produced at <b>205</b>.</p>
<p id="h-0009" num="0000"><figref idref="DRAWINGS">FIG. 3</figref></p>
<p id="p-0047" num="0046">An overview of procedures performed to produce each broadband response file is shown in <figref idref="DRAWINGS">FIG. 3</figref>.</p>
<p id="p-0048" num="0047">At step <b>301</b>, test points about a three-dimensional originating region are identified. The number of test points is determined and the position of each test point relative to the centre of the originating region is determined.</p>
<p id="p-0049" num="0048">A test position is selected at step <b>302</b>. A test position relates to the relative positioning and orientation between an audio output point and a listening point.</p>
<p id="p-0050" num="0049">At step <b>303</b> an audio output source is aligned for the test position selected at step <b>302</b>. The audio output source is located at the test point associated with the selected test position.</p>
<p id="p-0051" num="0050">At step <b>304</b>, a microphone is aligned for the test position selected at step <b>302</b>. The microphone is located at the recording point associated with the selected test position. An audio output from the aligned audio output source is generated at step <b>305</b> and the resultant microphone output is recorded at step <b>306</b>. At step <b>307</b>, the recorded signal is stored as a file for the selected test position.</p>
<p id="p-0052" num="0051">Steps <b>302</b> to <b>307</b> may then be repeated for each test position.</p>
<p id="p-0053" num="0052">For each selected test position, a plurality of sounds may be generated by the sound source such that the resulting signals recorded at the recording position relate to a range of frequencies.</p>
<p id="p-0054" num="0053">In a specific embodiment, a human subject is located in an anechoic chamber and an omnidirectional microphone is located just outside an ear canal of the human subject, in contact with the side of the head. A set of sounds is generated and the microphone output is recorded for each of the plurality of test positions to produce a set of test recordings. In a specific embodiment, the human subject is aligned at an azimuth position and recordings are taken for each elevation position before the human subject is aligned for a next azimuth position.</p>
<p id="p-0055" num="0054">Optionally, the microphone is located in the anechoic chamber absent the human subject, the same set of sounds is generated and the microphone output is recorded for each of the plurality of test positions to produce a set of reference recordings.</p>
<p id="p-0056" num="0055">An originating signal derived from the microphone output recordings is then deconvolved with each of the set of reference signals to produce a broadband response file for each test position.</p>
<p id="p-0057" num="0056">In this way, it is possible to produce a set of frequency resolved broadband signals for each of a large number of locations around a three-dimensional region surrounding a subject.</p>
<p id="p-0058" num="0057">Each broadband response file is then made available to be convolved with an audio input signal so as to produce a mono signal for a left field and for a right field. Thus, for a human subject, the left and right fields of the stereo signal represent the audio input signal as if originating from a specified location relative to the human head from the respective perspectives of the left ear and the right ear.</p>
<p id="p-0059" num="0058">It is appreciated that many complex effects are present that provide cues allowing a subject to identify the location of a sound. In the preferred embodiment, the information has been recorded empirically without a requirement to produce complex mathematical models which, to date, have been unsuccessful in terms of reproducing these three-dimensional cues.</p>
<p id="p-0060" num="0059">Compared to using artificial head systems, it is appreciated that the head itself is not a homogeneous mass. Sound transmitted through the flesh and bone structure of the head and also around the head provides significant information in addition to the sound travelling directly through the air.</p>
<p id="p-0061" num="0060">In order to provide further cues to the identification of three-dimensional position, it is also appreciated that high frequencies, that are above 20 kilohertz, also play their part, although not directly audible. It is therefore preferable for broadband microphones to be used and for frequencies to be generated over the notional audible range and to continue up to, for example, 96 kilohertz. Again, studies have shown that frequencies normally considered as being beyond the established human hearing range are of importance when giving quality to the sound and thereby facilitate the positioning of the sound. It is understood that these frequencies are transmitted via bone conduction rendering them perceptible by organs other than those (essentially the cochlea) responsible for hearing in the established range of 20 hertz to 20 kilohertz.</p>
<p id="p-0062" num="0061">Given the symmetrical nature of the human hearing response, it is not entirely necessary to provide sound recording with respect to both ears, given that the recordings achieved from one side may be reflected and reused on the alternative side. Thus, each recorded sample may effectively be deployed with respect to two originating locations.</p>
<p id="p-0063" num="0062">A second microphone may be provided to facilitate the recording of the otoacoustic response of the human subject by using a specialist microphone in the appropriate ear. As is known, otoacoustics have been used for many years to test the hearing of babies and young children. When a sound is played to the human eardrum it creates a sympathetic sound in response. Otoacoustic microphones are designed to detect these sounds and it is understood that otoacoustics may also have a significant bearing on the advanced interpretation or cueing of sound.</p>
<p id="h-0010" num="0000"><figref idref="DRAWINGS">FIG. 4</figref></p>
<p id="p-0064" num="0063">Steps to establish test points on an originating region according to a specific embodiment are illustrated in <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0065" num="0064">A cube <b>401</b> is selected as a geometric starting point. As indicated by arrow <b>402</b>, the cube <b>401</b> is subdivided using a subdivision surface algorithm. In a specific embodiment, a quad-based exponential method is used.</p>
<p id="p-0066" num="0065">Following a first step of subdivision of cube <b>401</b>, a polygon <b>403</b> is obtained providing <b>26</b> vertices. As indicated by arrows <b>404</b> and <b>405</b>, this process is repeated twice, giving a polygon <b>406</b> providing <b>285</b> vertices, such as vertex <b>407</b>. The quadrilateral sides of polygon <b>406</b> are then triangulated by adding a point at the centre of each side, as indicated by arrow <b>408</b>. This results in a polygon <b>409</b> providing seven hundred and seventy (770) points, such as point <b>410</b>. It can be seen from <figref idref="DRAWINGS">FIG. 4</figref> that each step produces a polygon that more closely approximates a sphere.</p>
<p id="p-0067" num="0066">Polygon <b>407</b> is considered to approximate a spherical originating region and each of the seven hundred and seventy (770) points about polygon <b>407</b> is to be used as a test point.</p>
<p id="p-0068" num="0067">The resultant distribution of the test points about polygon <b>407</b> is found to be practical. The subdivision surface method used serves to increase the evenness of distribution of points about a spherical polygon and reduce the concentration of points at the poles thereof. Further, the test points introduced through triangulation of the quadrilateral sides of polygon <b>407</b> serve to reduce the distance of each path between points across each quadrilateral side. These features serve to increase the uniformity of the paths between points around the originating region.</p>
<p id="p-0069" num="0068">By empirical testing, seven hundred and seventy (770) locations would appear to be consistent with the spatial resolution of human hearing. However, the greater the number of locations used, the smoother the tonality changes between originating locations. Hence, an increased number of locations may be used to reduce the incidence of tonal irregularities that may be identified by a listener as processed sound moves between emulated locations. Thus, in some applications, a thousand or several thousand locations may be derived and employed.</p>
<p id="h-0011" num="0000"><figref idref="DRAWINGS">FIG. 5</figref></p>
<p id="p-0070" num="0069">Apparatus for use in the production of broadband response files is illustrated in <figref idref="DRAWINGS">FIG. 5</figref>. The apparatus enables test positions over three hundred and sixty (360) degrees in both elevation and azimuth to be reproduced.</p>
<p id="p-0071" num="0070">A loudspeaker unit <b>501</b> is selected that is capable of playing high quality audio signals over the frequency range of interest; in a specific embodiment, up to 80 kilohertz. In a specific embodiment, the loudspeaker includes a first woofer speaker <b>502</b> for bass frequencies, a second tweeter speaker <b>503</b> for treble frequencies, and a third super tweeter speaker <b>504</b> for ultrasonic frequencies.</p>
<p id="p-0072" num="0071">The loudspeaker unit <b>501</b> is supported in a gantry <b>505</b>. The gantry <b>505</b> provides an arc along which the loudspeaker is movable. The arrangement of the loudspeaker unit <b>501</b> and gantry <b>505</b> is such that the sound emitted from the loudspeakers <b>502</b>, <b>503</b>, <b>504</b> is convergent at the centre <b>506</b> of the arc of the gantry <b>505</b>. The centre <b>506</b> of the arc is determined as the centre of originating region <b>507</b>. The emitted sound from the loudspeakers is time aligned such that the sounds are synchronised at the convergence point.</p>
<p id="p-0073" num="0072">In a specific embodiment, the radius of the arc of the gantry <b>505</b> is 2.2 (two point two) m. The gantry <b>507</b> defines restraining points along the length thereof to allow the loudspeaker unit <b>501</b> to be supported at different angles of elevation between plus ninety (+90) degrees above the centre <b>506</b>, zero (0) degrees level with the centre <b>506</b> and minus ninety (&#x2212;90) degrees below the centre <b>506</b>.</p>
<p id="p-0074" num="0073">A platform <b>508</b> is provided to assist at least one microphone, such as audio microphone <b>509</b>, to be supported at the centre <b>506</b> of the arc. As previously described, an otoacoustic microphone may additionally be used. Alternatively, a single microphone apparatus may be used for both audio and otoacoustic inputs.</p>
<p id="p-0075" num="0074">The platform <b>508</b> has a mesh structure to allow sounds to pass therethrough. The platform <b>508</b> is arranged to support a human subject with the audio microphone located in an ear of the human subject. In addition, the platform is arranged to optionally support a microphone stand that in turn supports the audio microphone.</p>
<p id="p-0076" num="0075">In order to reduce resonance and noise from the apparatus, insulating material may be used. For example, the gantry <b>505</b> and the platform <b>508</b> may be treated with noise control paint and/or foam to inhibit acoustic reflections and structure resonance. The desired effect is to contain sound in the vicinity of physical surfaces at which the sound is incident.</p>
<p id="p-0077" num="0076">A computer system <b>510</b>, a high-powered laptop computer being used in this embodiment, is also provided.</p>
<p id="p-0078" num="0077">Output signals to the loudspeaker unit <b>501</b> are supplied by the computer system <b>510</b>, while output signals received from the at least one microphone <b>509</b> are supplied to the computer system <b>510</b>.</p>
<p id="h-0012" num="0000"><figref idref="DRAWINGS">FIG. 6</figref></p>
<p id="p-0079" num="0078">Use of the apparatus of <figref idref="DRAWINGS">FIG. 5</figref> to produce a first set of data for the production of broadband response files is illustrated in <figref idref="DRAWINGS">FIG. 6</figref>.</p>
<p id="p-0080" num="0079">The apparatus is placed inside an anechoic acoustic chamber <b>601</b> along with human subject <b>101</b>. Microphone <b>509</b>, which in this embodiment is a contact transducer, is placed in the pinna (also known as the auricle or outer ear), adjacent the ear canal, of one ear, in this example the right ear of the human subject <b>101</b>. The human subject <b>101</b> and the platform <b>508</b> are arranged such that an ear (right ear) of the human subject <b>101</b> and hence the microphone <b>509</b> is located at the centre of the arc of the gantry <b>505</b>. Steps <b>302</b> to <b>307</b> of <figref idref="DRAWINGS">FIG. 3</figref> are repeated to produce a plurality of reference recordings.</p>
<p id="p-0081" num="0080">To reproduce each test point, the loudspeaker unit <b>501</b> is movable in elevation, as indicated by arrow <b>602</b>, and the human subject <b>101</b> is movable in azimuth, as indicated by arrow <b>603</b>.</p>
<p id="p-0082" num="0081">A first test position is selected. The particular position sought on the first iteration is not relevant to the overall process although a particular starting point and trajectory may be preferred in order to minimise movement of the apparatus.</p>
<p id="p-0083" num="0082">For the selected test position, the human subject <b>101</b> is aligned on the platform <b>508</b> and the loudspeaker unit <b>501</b> is aligned relative to the human subject <b>602</b>. Alignment may be facilitated by the use of at least one laser pointer. In a specific embodiment, at least one laser pointer is mounted upon the loudspeaker unit <b>501</b> to assist accurate alignment.</p>
<p id="p-0084" num="0083">Once aligned, an audio output from the loudspeaker unit <b>501</b> is generated at step <b>305</b> and the resultant input received by the microphone <b>509</b> is recorded. The recorded signal is stored as a reference recording for the selected test position. This process is repeated for the relevant degrees of elevation or degrees of elevation and degrees of azimuth.</p>
<p id="p-0085" num="0084">The number of test positions selected for reference recordings may vary according to the particular audio microphone used. Preferably, the audio microphone is omnidirectional with a high-resolution impulse response.</p>
<p id="p-0086" num="0085">In this way, a first set of data is produced that is stored as a first set of reference recordings.</p>
<p id="p-0087" num="0086">As previously described, a second otoacoustic input may also be used. In a specific application, an otoacoustic microphone is placed in the same ear (right ear) of the human subject <b>101</b> and the input received by the otoacoustic microphone is recorded in addition to that received by audio microphone <b>509</b>. In this way, first and second sets of data are produced that are stored as a first set and a second set of reference recordings.</p>
<p id="p-0088" num="0087">In a specific embodiment, movement of the loudspeaker unit <b>501</b> is controlled by high quality servomotors, which in turn receive commands from the computer system <b>510</b>. Alternatively, the loudspeaker unit <b>501</b> may be moved manually. Thus, the restraining points of the gantry <b>505</b> may be pinholes and a pin may be provided to fix the loudspeaker unit <b>501</b> at a selected pinhole. It is to be appreciated that the pinholes are to be acoustically transparent.</p>
<p id="p-0089" num="0088">Measuring equipment may then be used to feed signals back to the computer system <b>510</b> as to the location of the loudspeaker unit <b>501</b>.</p>
<p id="p-0090" num="0089">In a specific embodiment, both the gantry <b>505</b> and the platform <b>508</b> have visible demarcations of relevant degrees of elevation and azimuth respectively. It is also preferable for the human subject to maintain a uniform distance between their feet, as indicated at <b>604</b>, throughout the test recordings. In a specific embodiment, the distance between the feet is equal to the distance between the ears, as indicated at <b>605</b>, of the human subject <b>101</b>.</p>
<p id="h-0013" num="0000"><figref idref="DRAWINGS">FIG. 7</figref></p>
<p id="p-0091" num="0090">The plan view illustration of <figref idref="DRAWINGS">FIG. 7</figref> shows human subject <b>101</b> with their left ear <b>104</b> at the centre of a first spherical region <b>701</b> and their right ear <b>105</b> at the centre of a second similar spherical region <b>702</b>.</p>
<p id="p-0092" num="0091">A distance D, indicated at <b>703</b>, exists between the left and right ears <b>104</b>, <b>105</b> of the human subject <b>101</b>. It can be seen that the first and second spherical regions <b>701</b>, <b>702</b> overlap to the effect that the right region <b>701</b> extends distance D beyond that of the left region <b>702</b> to the right of the human subject <b>101</b> and vice versa.</p>
<p id="p-0093" num="0092">As described with reference to <figref idref="DRAWINGS">FIG. 6</figref>, a first set of reference recordings is produced for a first ear of the human subject. Data is also stored for the other ear of the human subject, and a second set of reference recordings may be produced by repeating the empirical procedure described with reference to <figref idref="DRAWINGS">FIG. 6</figref> for the other ear. Alternatively, the second set of data may be derived from the first set of data. Each item of data from the first set of reference recordings may be translated to the effect that the data is mirror imaged about the central axis, indicated at <b>704</b>, extending between the left and right ears <b>104</b>, <b>105</b> of human subject <b>101</b>. Thus, a negative transform is applied to an item of data at a test position in one region and is stored for the test position in the other region that in azimuth is in mirror image but in elevation is the same.</p>
<p id="p-0094" num="0093">Thus, data from test position <b>705</b> in the right region <b>701</b> can be reproduced as data for test position <b>706</b> in the left region <b>702</b>. Similarly, data from test position <b>707</b> in the right region <b>701</b> can be reproduced as data for test position <b>708</b> in the left region <b>702</b>.</p>
<p id="h-0014" num="0000"><figref idref="DRAWINGS">FIG. 8</figref></p>
<p id="p-0095" num="0094">Computer system <b>510</b> is illustrated in <figref idref="DRAWINGS">FIG. 8</figref>. The system includes a central processing unit <b>801</b> and randomly accessible memory devices <b>802</b>, connected via a system bus <b>803</b>. Permanent storage for programs and operational data is provided by a hard disc drive <b>804</b> and program data may be loaded from a CD or DVD ROM (such as ROM <b>805</b>) via an appropriate drive <b>806</b>.</p>
<p id="p-0096" num="0095">Input commands and output data are transferred to the computer system via an input/output circuit <b>807</b>. This allows manual operation via a keyboard, mouse or similar device and allows a visual output to be generated via a visual display unit. In the example shown, these peripherals are all incorporated within the laptop computer system. In addition, the computer system is provided with a high quality sound card <b>808</b> facilitating the generation of output signals to the loudspeaker unit <b>501</b> via an output port <b>809</b>, while input signals received at the at least one microphone <b>509</b> are supplied to the system via an input port <b>801</b>.</p>
<p id="h-0015" num="0000"><figref idref="DRAWINGS">FIG. 9</figref></p>
<p id="p-0097" num="0096">Procedures executed by the computer system <b>510</b> are detailed in <figref idref="DRAWINGS">FIG. 9</figref>.</p>
<p id="p-0098" num="0097">At step <b>901</b> a new folder for the storage of broadband response files is initiated. In addition, temporary data structures are also established, as detailed subsequently.</p>
<p id="p-0099" num="0098">At step <b>902</b> the system seeks confirmation of a first test position for which sounds are to be generated.</p>
<p id="p-0100" num="0099">At step <b>903</b> an audio output is selected. For the purposes of illustration, it is assumed that the procedure is initiated with a very low frequency (20 hertz say) and then incremented, for example in 1 or 5 hertz increments, up to the highest frequency of 96 kilohertz (sampled with 192 kilohertz sampling frequency). The acoustic chamber should be anechoic across the frequency range of the audio output.</p>
<p id="p-0101" num="0100">At step <b>904</b> an output sound is generated. Output sounds are generated in response to digital samples stored on hard disc drive <b>804</b>. Thus, for a computer system based upon the Windows operating system, for example, these data files may be stored in the WAV format.</p>
<p id="p-0102" num="0101">At step <b>905</b> and in response to the output sound being generated, the input is recorded. As previously described, this may be an audio input or both an audio input and otoacoustic input. At step <b>906</b> a question is asked as to whether another output sound is to be played and when answered in the affirmative control is returned to <b>903</b>, whereupon the next output sound is selected. Ultimately, the desired output sound or sounds will have been played for a particular test position and the question asked at step <b>906</b> will be answered in the negative.</p>
<p id="p-0103" num="0102">At step <b>907</b> a question is asked as to whether another test position is to be selected and when answered in the affirmative control is returned to step <b>902</b>. Again, at step <b>902</b> confirmation of the next position is sought and if another position is to be considered the frequency generation procedure is repeated. Ultimately, all of the positions will have been considered resulting in the question asked at step <b>907</b> being answered in the negative.</p>
<p id="p-0104" num="0103">At step <b>908</b> operations are finalised so as to populate an appropriate data table containing broadband response files whereupon the folder initiated at step <b>901</b> is closed.</p>
<p id="h-0016" num="0000"><figref idref="DRAWINGS">FIG. 10</figref></p>
<p id="p-0105" num="0104">As described with respect to <figref idref="DRAWINGS">FIG. 9</figref>, output sounds are generated at a number of frequencies. In a specific embodiment, each output sound generated takes the form of a single cycle, as illustrated in <figref idref="DRAWINGS">FIG. 10</figref>.</p>
<p id="p-0106" num="0105">In <figref idref="DRAWINGS">FIG. 10</figref>, <b>1001</b> represents the generation of a relatively low frequency, <b>1002</b> represents the generation of a medium frequency and <b>1003</b> represents the generation of a relatively high frequency. As can be seen from each of these examples, the output waveform takes the form of a single cycle, starting at the origin and completing a sinusoid for one period of the waveform.</p>
<p id="p-0107" num="0106">It should also be appreciated that each waveform is constructed from a plurality of digital samples illustrated by vertical lines, such as line <b>1004</b>. Thus, these data values are stored in each output file such that the periodic sinusoids may be generated in response to operation of the procedures described with respect to <figref idref="DRAWINGS">FIG. 9</figref>.</p>
<p id="p-0108" num="0107">In a specific embodiment, a sequence of discrete sinusoids, with each having a greater frequency than the previous, are generated as a &#x2018;frequency sweep&#x2019;, a sequence that when generated is heard as a rising note. In a specific embodiment, the frequency increases in 1 Hz increments. In a specific embodiment, the frequencies of the frequency sweep have a common fixed amplitude, as illustrated in <figref idref="DRAWINGS">FIG. 10</figref>.</p>
<p id="p-0109" num="0108">Preferably, there is no delay between sinusoids of a frequency sweep, so as to be a continuous sound, to minimise the length of the output sound. However, a delay may be provided between sinusoids if desired, and the delay may have a sufficiently short duration so as not to be identifiable by the human subject. In an alternative arrangement, the frequency may be increased during sinusoids to further reduce the duration of the output sound.</p>
<p id="p-0110" num="0109">A preferred duration for the set of sounds is three (3) seconds. The duration of the set of sounds may depend upon the ability of a human subject to maintain a still posture.</p>
<p id="p-0111" num="0110">The set of sounds is selected to generate acoustic stimulus across a frequency range of interest with equal energy, in a manner that improves the faithfulness of the captured impulse responses. It is found that accuracy is improved by operating the audio playback equipment to generate a single frequency at a time, as opposed to an alternative technique in which many frequencies are generated in a burst or click of noise. Using longer recordings for the deconvolution process is found to improve the resolution of the impulse response files.</p>
<p id="p-0112" num="0111">The format of the set of sounds is selected to allow accurate reproducibility so as not to introduce undesired variations between plays. A digital format allows the set of sounds to be modified, for example, to add or enhance a frequency or frequencies that are difficult to reproduce with a particular arrangement of audio playback equipment.</p>
<p id="h-0017" num="0000"><figref idref="DRAWINGS">FIG. 11</figref></p>
<p id="p-0113" num="0112">As described with respect to <figref idref="DRAWINGS">FIG. 9</figref>, at step <b>901</b> temporary data structures are established, an example of which is shown in <figref idref="DRAWINGS">FIG. 11</figref>. The data structure of <figref idref="DRAWINGS">FIG. 11</figref> stores each individual recorded sample for the output frequencies generated at each selected test position. In this example, audio inputs only are recorded.</p>
<p id="p-0114" num="0113">In a specific embodiment, for the first test position L<b>1</b> a set of output sounds is generated. This results in a sound sample R<b>1</b> being recorded. The next test position L<b>2</b> is selected at step <b>902</b>, the set of sounds is again generated and this in turn results in the data structure of <figref idref="DRAWINGS">FIG. 11</figref> being populated by sound sample R<b>2</b>. Samples continue to be collected for all output frequencies at all selected test positions. Thus, a reference signal is produced for each test position.</p>
<p id="p-0115" num="0114">In alternative applications in which discrete frequencies are generated and discrete samples recorded in response, a data structure may be populated by individual samples for a particular test position and the individual samples subsequently combined to produce a reference signal for that test position.</p>
<p id="p-0116" num="0115">The reference signals are representative of the impulse response of the apparatus used in the empirical testing, including that of the microphone and the human subject used. Each reference signal hence provides a &#x2018;sonic signature&#x2019; of the apparatus, the human subject and the acoustic event for each test position.</p>
<p id="p-0117" num="0116">In a specific application, a set of reference recordings is stored for each of a plurality of different human subjects and the results of the tests are averaged.</p>
<p id="p-0118" num="0117">The set of audio output sounds is played for each test position for each of the human subjects, the resulting microphone outputs are recorded, and the microphone outputs for each test position are averaged.</p>
<p id="p-0119" num="0118">In some applications, a filtering process may be performed to remove certain frequencies or noise, in particular low bass frequencies such as structure borne frequencies, from the reference recordings.</p>
<p id="h-0018" num="0000"><figref idref="DRAWINGS">FIG. 12</figref></p>
<p id="p-0120" num="0119">A further example of a temporary data structure established at step <b>901</b> as described with respect to <figref idref="DRAWINGS">FIG. 9</figref> is shown in <figref idref="DRAWINGS">FIG. 12</figref>. The data structure of <figref idref="DRAWINGS">FIG. 12</figref> stores each individual recorded sample for the output frequencies generated at each selected test position. In this example, separate audio and otoacoustic inputs are recorded.</p>
<p id="p-0121" num="0120">In a specific embodiment, for the first test position L<b>1</b> the set of output sounds is generated. This results in an audio sample RA<b>1</b> being recorded in addition to an otoacoustic signal RO<b>1</b> being recorded. The next test position is then selected at step <b>902</b> and the set of sounds is again generated. This in turn results in the data structure of <figref idref="DRAWINGS">FIG. 12</figref> being populated by audio sample RA<b>2</b> and otoacooustic sample RO<b>2</b>. Samples continue to be collected for all output frequencies at all selected test positions. The audio sample and otoacoustic sample recorded for each test position are then subsequently combined to produce a reference recording for each test position.</p>
<p id="p-0122" num="0121">In alternative applications in which individual frequencies are generated and individual samples recorded in response, a data structure may be populated by individual samples of both audio and otoacoustic types for a particular test position and the individual samples of each type subsequently combined for that test position.</p>
<p id="p-0123" num="0122">Again, the test recordings are representative of the impulse response of the apparatus used in the empirical testing, including that of the microphone(s) and the human subject used. The test recordings hence provide a &#x2018;sonic signature&#x2019; of the apparatus, the human subject and the acoustic event.</p>
<p id="p-0124" num="0123">In a specific application, a set of reference recordings is stored for each of a plurality of different human subjects and the results of the tests are averaged.</p>
<p id="p-0125" num="0124">Again, a filtering process may be performed to remove certain frequencies or noise, in particular low bass frequencies such as structure borne frequencies, from the reference recordings.</p>
<p id="h-0019" num="0000"><figref idref="DRAWINGS">FIG. 13</figref></p>
<p id="p-0126" num="0125">Finalising step <b>908</b> includes a process for deconvolving each reference signal with an originating signal to produce a broadband response file for each test position, as illustrated in <figref idref="DRAWINGS">FIG. 13</figref>.</p>
<p id="p-0127" num="0126">At step <b>1301</b> an originating signal is selected for use in a deconvolution process.</p>
<p id="p-0128" num="0127">At step <b>1302</b> a test position (L) is selected and at step <b>1303</b> an associated reference signal (R) is selected.</p>
<p id="p-0129" num="0128">At step <b>1304</b> the selected reference signal (R) is deconvolved with the selected originating signal and at step <b>1305</b> the result of the deconvolution process is stored as a broadband response file for the selected test position.</p>
<p id="p-0130" num="0129">Step <b>1306</b> is then entered where a question is asked as to whether another test position is to be selected. If this question is answered in the affirmative, control is returned to step <b>1302</b>. Alternatively, if this question is answered in the negative, this indicates that broadband response files have been stored for each test position.</p>
<p id="p-0131" num="0130">In a specific embodiment, the deconvolution process is a Fast Fourier Transform (FFT) convolution process. In alternative applications a direct deconvolution process may be used. Preferably, the broadband response files have a 28 bit or higher format. In a specific embodiment, the broadband response files have a 32 bit format.</p>
<p id="p-0132" num="0131">As previously described, each broadband response file can then be used in a convolution process, to emulate an audio input signal as though it originated substantially from an indicated audio source location relative to a listening source location. As will be described further herein, broadband response files are stored for a left field and for a right field.</p>
<p id="p-0133" num="0132">As described with reference to <figref idref="DRAWINGS">FIG. 7</figref>, data for one ear of a human subject may be derived from data produced for the other ear of the human subject. In a specific embodiment, broadband response files are produced for a first ear of the human subject only. A negative transform is then applied to each file for each of the test positions, and the resulting file is stored for the test position for the second ear that has a mirror image azimuth but the same elevation.</p>
<p id="h-0020" num="0000"><figref idref="DRAWINGS">FIG. 14</figref></p>
<p id="p-0134" num="0133">A convolution equation <b>1401</b> is illustrated in <figref idref="DRAWINGS">FIG. 14</figref>. As identified, h (a recorded signal) is the result of f (a first signal) convolved with g (a second signal).</p>
<p id="p-0135" num="0134">With reference to <figref idref="DRAWINGS">FIGS. 9 and 11</figref>, each reference signal R is a recording at a listening source location of a sound from an audio source location. With reference to convolution equation <b>1401</b>, each reference signal R may be identified as h (a recorded signal) and the output sound that was recorded may be identified as f (a first signal). The second signal (g) in the convolution equation <b>1401</b> is then identified as the impulse response of the arrangement of apparatus and human subject at the test position associated with the reference signal R. Thus, the impulse response of a reference signal R contains spatial cues relating to the relative positioning and orientation of the audio output relative to the listener. As described previously, the production of broadband response files involves a deconvolution process. Deconvolution is a process used to reverse the effects of convolution on a recorded signal. Referring to convolution equation <b>1401</b>, deconvolving h (a recorded signal) with f (a first signal) gives g (a second signal).</p>
<p id="p-0136" num="0135">Thus, deconvolving a reference signal R with the output sound that was recorded functions to extract the impulse response (IR) for the associated test position. If the output sound is then convolved with the IR for a selected test position, the result will emulate the reference signal R stored for that test position.</p>
<p id="p-0137" num="0136">Hence, if an audio signal is convolved with the IR for a selected test position, the result emulates the production of that audio signal from the selected test position. In this way it is possible to emulate the production of the audio signal from a specified audio source location relative to a listening source location.</p>
<p id="h-0021" num="0000"><figref idref="DRAWINGS">FIG. 15</figref></p>
<p id="p-0138" num="0137"><figref idref="DRAWINGS">FIG. 15</figref> illustrates a listener <b>101</b> surrounded by a notional three-dimensional originating region <b>1501</b>, from which listener <b>1501</b> may hear a sound.</p>
<p id="p-0139" num="0138">The listener is positioned at the centre of the originating region <b>1501</b>, facing in a direction indicated by arrow <b>1502</b>, which is identified as zero (0) degrees azimuth. The left ear <b>104</b> and the right ear <b>105</b> are at the height of the centre of the originating region <b>1501</b>, which is identified as zero (0) degrees elevation.</p>
<p id="p-0140" num="0139">According to the convention used herein, positive degrees azimuth increment in the clockwise direction from the zero (0) degrees azimuth position and negative degrees azimuth increment in the anticlockwise direction from the zero (0) degrees azimuth position.</p>
<p id="p-0141" num="0140">It is considered that generally the best angle of acceptance of sound by the right human ear is at plus seventy (+70) degrees azimuth, zero (0) degrees elevation, indicated by arrow <b>1503</b>. Similarly, it is considered that generally the best angle of acceptance of sound by the left human ear is minus seventy (&#x2212;70) degrees azimuth, zero (0) degrees elevation indicated by arrow <b>1504</b>. At these angles, the received sound is considered to be at its loudest, and least cluttered from reflections around the head.</p>
<p id="p-0142" num="0141">Thus, if using a single pair of audio loudspeakers to output a stereo audio signal (having a left field and a right field) it would be considered of benefit to the listener to position a left audio loudspeaker <b>1505</b> at minus seventy (&#x2212;70) degrees azimuth and a right audio loud speaker <b>1506</b> at plus seventy (+70) degrees azimuth.</p>
<p id="p-0143" num="0142">As previously described, if an audio signal is convolved with the IR for a selected audio source location relative to a listening source location, the result emulates the production of that audio signal from the selected audio source location.</p>
<p id="p-0144" num="0143">It may therefore by considered desirable to use an impulse response (IR) file that includes spatial transfer functions but that does not include spatial transfer functions for a speaker location relative to the listener location. This is because the speaker will physically contribute spatial transfer functions to the output sound. Hence, if the audio signal is convolved with an IR file containing spatial transfer functions for the speaker location relative to the listener location, the resulting sound will incorporate the spatial transfer functions for the speaker location twice.</p>
<p id="p-0145" num="0144">However, it may also be considered undesirable to use an impulse response (IR) file that includes spatial transfer functions but that does not include spatial transfer functions for a speaker location relative to the listener location. This is because if an audio signal is to be convolved with the IR file for that position, and the spatial transfer functions for that position are not available, the result will be an unprocessed audio signal.</p>
<p id="p-0146" num="0145">In addition, in the convolution process, it is desirable to use an impulse response (IR) file that includes spatial transfer functions but that does not include apparatus transfer functions. Again, this is because the speaker arrangement will physically contribute apparatus transfer functions to the output sound. Hence, if the audio signal is convolved with an IR file containing apparatus transfer functions, the resulting sound will incorporate both the transfer functions of the IR file and the apparatus transfer functions of the apparatus through which the processed audio signal is physically output.</p>
<p id="p-0147" num="0146">It is found that using a &#x2018;frequency sweep&#x2019; as described with reference to <figref idref="DRAWINGS">FIG. 10</figref> as the audio output to be recorded provides a deconvolved broadband impulse response signal with a good signal to noise ratio. This is desirable, since any signal convolved with the broadband response signal will inherit the characteristics of that broadband signal.</p>
<p id="h-0022" num="0000"><figref idref="DRAWINGS">FIG. 16</figref></p>
<p id="p-0148" num="0147">Procedures executed in a method of producing an originating signal for selection at step <b>1301</b> of <figref idref="DRAWINGS">FIG. 13</figref> are illustrated in <figref idref="DRAWINGS">FIG. 16</figref>.</p>
<p id="p-0149" num="0148">At step <b>1601</b>, a first reference signal from the data set of reference signals R stored for a first ear of the human subject is selected. At <b>1602</b>, the first selected reference signal is deconvolved with the output sound that was recorded. The resultant (IR) signal is then stored at step <b>1603</b> as a first IR file.</p>
<p id="p-0150" num="0149">Step <b>1604</b> is then entered at which a second reference signal from the data set of reference signals R stored for a first ear of the human subject is selected. At <b>1605</b>, the second selected reference signal is deconvolved with the output sound that was recorded. The resultant (IR) signal is then stored at step <b>1606</b> as a second IR response file.</p>
<p id="p-0151" num="0150">At step <b>1607</b>, the first and second IR response files are combined and the resulting signal is stored at step <b>1608</b> as an originating signal file. In a specific embodiment, Fourier coefficient data stored for each of the first and second IR response files is averaged, in effect producing data for a single signal waveform.</p>
<p id="p-0152" num="0151">In a specific embodiment, the duration of each broadband response file is approximately three (3) milliseconds.</p>
<p id="p-0153" num="0152">In an alternative embodiment, the signals of the first and second IR response files are summed, in effect producing two overlaid signal waveforms. However, when a &#x2018;frequency sweep&#x2019; as described with reference to <figref idref="DRAWINGS">FIG. 10</figref> is recorded, the length of the audio output is such that the human subject may move and hence the waveforms from the first and second reference signals may not align properly when summed.</p>
<p id="p-0154" num="0153">As described with reference to <figref idref="DRAWINGS">FIG. 13</figref>, each reference signal in the data set for a first ear of the human subject is then deconvolved with the selected originating signal to produce a broadband response file for each test position.</p>
<p id="p-0155" num="0154">By deconvolving each reference signal with an originating signal derived from at least one reference signal, the apparatus transfer functions are removed from the resulting IR signal, leaving the desired spatial transfer functions.</p>
<p id="p-0156" num="0155">By deconvolving each reference signal with an originating signal derived from two reference signals, the resulting IR signal for each of the selected reference signals will incorporate spatial transfer functions derived from the other selected reference signal. Thus, if an audio signal is convolved with an IR file containing spatial transfer functions for a speaker location relative to the listener location, the audio signal will still be processed.</p>
<p id="p-0157" num="0156">In a specific embodiment, the selected reference signals in the left field are those at minus thirty (&#x2212;30) degrees azimuth, zero (0) elevation and minus one hundred and ten (&#x2212;110) degrees azimuth, zero (0) elevation. In the right field, the selected reference signals are those at plus thirty (+30) degrees azimuth, zero (0) elevation and plus one hundred and ten (+110) degrees azimuth, zero (0) elevation.</p>
<p id="p-0158" num="0157">It is found that the brain will tend to process sounds coming from these positions to produce a phantom image from plus seventy (+70) degrees azimuth, zero (0) degrees elevation for the right ear at minus seventy (&#x2212;70) degrees azimuth, zero (0) degrees elevation for the left ear.</p>
<p id="h-0023" num="0000"><figref idref="DRAWINGS">FIG. 17</figref></p>
<p id="p-0159" num="0158">Procedures executed in a method of processing an audio input signal represented as digital samples to produce a stereo output signal (having a left field and a right field) that emulates the production of the audio signal from a specified audio source location relative to a listening source location are illustrated in <figref idref="DRAWINGS">FIG. 17</figref>.</p>
<p id="p-0160" num="0159">It can be seen that a first processing chain performs operations in parallel with a second processing chain to provide inputs for first and second convolution processes to produce left and right channel audio outputs.</p>
<p id="p-0161" num="0160">At step <b>1701</b>, an audio input signal is received. The audio input signal may be a live signal, a recorded signal or a synthesised signal.</p>
<p id="p-0162" num="0161">At step <b>1702</b>, an indication is received of an audio source location relative to a listening source location. The indication may include azimuth, elevation and radial distance co-ordinates or X, Y, and Z axis co-ordinates of the sound source location and the listening location. Thus, this step may include the application of a transform to identify co-ordinates in one co-ordinate system to co-ordinates in another co-ordinate system.</p>
<p id="p-0163" num="0162">At step <b>1703</b>, the angles for the left field are calculated for the indication input at <b>1702</b> and at step <b>1703</b> the angles for the right field are similarly calculated for the indication input at <b>1701</b>.</p>
<p id="p-0164" num="0163">Step <b>1705</b> is entered from step <b>1703</b> at which a broadband response file is selected for the left field. Similarly, step <b>1706</b> is entered from step <b>1704</b> at which a broadband response file is selected for the right field.</p>
<p id="p-0165" num="0164">Step <b>1707</b> is entered from step <b>1705</b>, where the audio input signal is convolved with the broadband response file selected for the left field and a left channel audio signal is output. Similarly, step <b>1708</b> is entered from step <b>1706</b>, where the audio input signal is convolved with the broadband response file selected for the right field and a right channel audio signal is output.</p>
<p id="p-0166" num="0165">It is to be appreciated that independent convolver apparatus is used for the left and right field audio signal processing.</p>
<p id="p-0167" num="0166">In a specific embodiment, the convolution process is a Fast Fourier Transform (FFT) convolution process. In alternative applications a direct convolution process may be used. In a specific embodiment, the duration of each broadband response file is approximately six (6) milliseconds.</p>
<p id="p-0168" num="0167">The processing operations function to produce dual mono outputs that reproduce the natural stereo hearing of a human being. Through the processing of reference signals in the production of the broadband response files as described with reference to <figref idref="DRAWINGS">FIGS. 13 to 16</figref>, it is possible to produce a signal that overcomes the perception by a listener of the origin of emulated sound as being located at speaker positions. Further, it is found that where the audio input signal has a lower bit depth than the broadband response files made available for the convolution process, desirably, the convolution process can add enhancing audio detail to the processed signal.</p>
<p id="h-0024" num="0000"><figref idref="DRAWINGS">FIG. 18</figref></p>
<p id="p-0169" num="0168">Procedures executed at step <b>1702</b> of <figref idref="DRAWINGS">FIG. 17</figref> are illustrated in <figref idref="DRAWINGS">FIG. 18</figref>.</p>
<p id="p-0170" num="0169">At step <b>1801</b>, an indication of the listening source location is received. Thus, both a fixed and a moving listening source location can be accommodated.</p>
<p id="p-0171" num="0170">At step <b>1802</b>, an indication is received of the distance D between the left fields and right fields of the listening source. As described with reference to <figref idref="DRAWINGS">FIG. 7</figref>, distance D relates to the distance between the left and right ears of the human subject. This may be user definable to account for different listeners.</p>
<p id="p-0172" num="0171">At step <b>1803</b>, an indication is received of the audio source location.</p>
<p id="h-0025" num="0000"><figref idref="DRAWINGS">FIG. 19</figref></p>
<p id="p-0173" num="0172">Further procedures executed in a method of processing an audio input signal represented as digital samples to produce a stereo output signal (having a left field and a right field) that emulates the production of the audio signal from a specified audio source location relative to a listening source location are illustrated in <figref idref="DRAWINGS">FIG. 19</figref>.</p>
<p id="p-0174" num="0173">It is desirable to adjust characteristics of the processed output audio signals according to movement of the emulated sound source towards or away from the listener.</p>
<p id="p-0175" num="0174">At step <b>1901</b>, an indication of the relative distance between the audio source location and the listener source location is received.</p>
<p id="p-0176" num="0175">At step <b>1902</b>, an indication of the speed of sound is received. The speed of sound may be user definable.</p>
<p id="p-0177" num="0176">The intensity of the output signal is calculated at step <b>1903</b>. It is desirable to increase the volume of the processed output signal as the emulated sound source moves towards the listening source location and to decrease the volume of the processed output signal as the emulated sound source moves away from the listening source location.</p>
<p id="p-0178" num="0177">At step <b>1904</b>, a degree of attenuation of the processed output signal is calculated. The closer the audio source location to the listener, the less an audio signal would be attenuated as a result of passing through the medium of air, for example. Therefore, the closer the audio source location to the listener, the less the degree of attenuation applied to the processed output signal.</p>
<p id="p-0179" num="0178">At step <b>1905</b>, a degree of delay of the actual outputting of the processed audio signal is calculated. The delay is dependent upon the distance between the audio source location and the listener source location and the speed of sound of the medium through which the audio wave is travelling. Thus, the closer the audio source location to the listener, the less the audio signal would be delayed. The delay is applied to the processing of the associated convolver apparatus, such that the number of convolutions per second is variable.</p>
<p id="h-0026" num="0000"><figref idref="DRAWINGS">FIG. 20</figref></p>
<p id="p-0180" num="0179">The plan view illustration of <figref idref="DRAWINGS">FIG. 20</figref> shows human subject <b>101</b> with their left ear <b>104</b> at the centre of a left region <b>701</b> and their right ear <b>105</b> at the centre of a right region <b>702</b>.</p>
<p id="p-0181" num="0180">A first moving emulated sound source is indicated generally by arrow <b>2001</b>. It can be seen that the angles and distance of the audio output source relative to the left and right ears <b>104</b>, <b>105</b> of the listener <b>101</b> vary as the sound source moves through spatial points <b>2002</b> to <b>2006</b> in the direction of arrow <b>2001</b>. Thus, it can be seen that angles and distance of the audio output source relative to the left and right ears <b>104</b>, <b>105</b> of the listener <b>101</b> at point <b>2004</b> are both different to those at point <b>2005</b>.</p>
<p id="p-0182" num="0181">A second moving emulated sound source is indicated generally by arrow <b>2007</b>. It can be seen that the angles and distance of the audio output source relative to the left and right ears <b>104</b>, <b>105</b> of the listener <b>101</b> vary as the sound source moves through spatial points <b>2008</b> to <b>2010</b> in the direction of arrow <b>2007</b>. In this example, it can be seen that both the angle and distance of the audio output source relative to the right ear <b>105</b> of the listener vary between points, however, only the distance and not the angle of the audio output source relative to the left ear <b>104</b> of the listener <b>101</b> varies between points.</p>
<p id="p-0183" num="0182">By processing the audio signal as described above, in particular with reference to <figref idref="DRAWINGS">FIG. 19</figref>, with reference to the distance of the output source and the speed of sound, it is possible to reproduce a natural Doppler effect of the moving sound.</p>
<p id="h-0027" num="0000"><figref idref="DRAWINGS">FIG. 21</figref></p>
<p id="p-0184" num="0183"><figref idref="DRAWINGS">FIG. 21</figref> is also a plan view of human subject <b>101</b> with their left ear <b>104</b> at the centre of a left region <b>701</b> and their right ear <b>105</b> at the centre of a right region <b>702</b>.</p>
<p id="p-0185" num="0184">An emulated sound source <b>2101</b> is shown, to the right side of human subject <b>101</b>. The angle of the sound source <b>2101</b> relative to the right ear <b>105</b> of the human subject <b>101</b> is such that the path <b>2102</b> from the sound source <b>2101</b> to the right ear <b>105</b> is directly incident upon the right ear <b>105</b>. In contrast, the angle of the sound source <b>2101</b> relative to the left ear <b>104</b> of the human subject <b>101</b> is such that the path <b>2103</b> from the sound source <b>2101</b> to the left ear <b>104</b> is indirectly incident upon the left ear <b>105</b>. It can be seen that the path <b>2103</b> is incident upon the nose <b>2104</b> of the human subject <b>101</b>. However, sound may travel from the nose <b>2104</b> around the head, as illustrated by arrow <b>2105</b>, to the left ear <b>104</b>.</p>
<p id="p-0186" num="0185">The difference in arrival time of sound between two ears is known as the interaural time difference and is important in the localisation of sounds as it provides a cue to the direction of sound source from the head. An interval between when a sound is heard by the ear closest to the sound source and when the sound is heard by the ear furthest from the sound source can be dependent upon sound travelling around the head of a listener.</p>
<p id="p-0187" num="0186">The head of a human subject may be modelled and data taken from the model may be utilised in order to enhance the reality of the perception of the emulated origin of processed audio. From the data model, it is possible to determine the distance of the path between the ears around the front of the head and also around the rear of the head, and also the distance between the nose and each of the left and right ears. Further, using the data model of the human subject, it is possible to determine whether the path of sound from a specified location to be emulated is directly or indirectly incident upon an ear of the human subject.</p>
<p id="p-0188" num="0187">Referring to step <b>1702</b> of <figref idref="DRAWINGS">FIG. 17</figref>, an indication is received regarding the audio source location relative to the listening source location. In a specific embodiment, a procedure may be performed to identify whether the audio source location is indirectly incident upon an ear of the human subject at the listening source location. In the event that the sound path is determined to be indirectly incident upon the ear of interest, an adjustment is made to the distance indication between that ear and the audio source location to include an additional distance related to the sound travelling a path around the head. The magnitude of the additional distance is determined on the basis that the incident sound will travel the shortest physical path available from the point of incidence with the head to the subject ear.</p>
<p id="p-0189" num="0188">In a specific embodiment, a scanning operation is performed to map the dimensions and contours of the head of each human subject in detail.</p>
<p id="p-0190" num="0189">As described, a particular position may be selected as the source of a perceived sound by selecting the appropriate broadband response signal. A further technique may be employed in order to adjust this perceived distance of the sound, that is to say, the radial displacement from the origin.</p>
<p id="p-0191" num="0190">In a specific embodiment, a procedure is performed to determine whether the audio source location is closer than a threshold radial distance <b>2106</b> from the ears of the listener at the listening source location. In the event that the audio source location is determined to be within a predetermined distance from the listening source location, the ear that is closest to the audio source location is identified. A component of unprocessed audio signal is then introduced into the channel output for the closest ear, whilst processing for the channel output for the other (furthest) ear remains unmodified. The closer the audio source location is identified to be to the closest ear, the greater the component of unprocessed audio signal is introduced into the channel output for that ear. In effect, cross fading is implemented to achieve a particular ratio of processed to unprocessed sound.</p>
<p id="h-0028" num="0000"><figref idref="DRAWINGS">FIG. 22</figref></p>
<p id="p-0192" num="0191">As illustrated in <figref idref="DRAWINGS">FIG. 22</figref>, broadband response files may be derived for each test position for different materials and environments.</p>
<p id="p-0193" num="0192">The apparatus illustrated in <figref idref="DRAWINGS">FIG. 5</figref> may be used to produce a plurality of broadband response files for each test position. The procedures detailed above for the production of a set of broadband response files using a human subject may be repeated replacing the human subject with a particular material or item. The resultant broadband response files are hence representative of the impulse response of the material or environment.</p>
<p id="p-0194" num="0193">In a specific embodiment, an audio microphone is placed at the centre of the arc of gantry <b>505</b>. A sound absorbing barrier is placed at a set distance from the microphone, between the microphone and the speaker unit <b>501</b>. The subject material is then placed between the sound absorbing barrier and the speaker unit <b>501</b>. The resultant broadband response files are thus representative of the way each material absorbs and reflects the output audio frequencies.</p>
<p id="p-0195" num="0194">In a specific embodiment, an audio microphone is placed at the centre of the arc of gantry <b>505</b>. Items of different materials and constructions are then placed around the microphone and the above detailed procedures performed to produce corresponding broadband response files.</p>
<p id="p-0196" num="0195">In this way, a library of broadband response files for different materials and environments may be derived and stored. The stored files may then be made available for use in a method of processing an audio input signal to produce a stereo output signal that emulates the production of the audio signal from a specified output source location relative to a listening source location region.</p>
<p id="p-0197" num="0196">Thus, for example, location L<b>1</b> may have a stored broadband response file derived from empirical testing involving a human subject, resulting in broadband response file B<b>1</b>, brick, resulting in broadband response file B<b>1</b>B and grass, resulting in broadband response file B<b>1</b>G, for example. Similarly, broadband response files B<b>3</b>, B<b>3</b>B and B<b>3</b>G stored are stored for location L<b>3</b>.</p>
<p id="p-0198" num="0197">Broadband response files may be derived from empirical testing involving one or more of, and not limited to: brick; metal; organic matter including wood and flora; fluids including water; interior surface coverings including carpet, plasterboard, paint, ceramic tiles, polystyrene tiles, oils, textiles; window glazing units; exterior surface coverings including slate, marble, sand, gravel, turf, bark; textiles including leather, fabric; soft furnishings including cushions, curtains.</p>
<p id="h-0029" num="0000"><figref idref="DRAWINGS">FIG. 23</figref></p>
<p id="p-0199" num="0198">Procedures executed to produce a stereo output signal (having a left field and a right field) that emulates the production of the audio signal from a specified audio source location relative to a listening source location may therefore take into account a material or environment, as indicated in <figref idref="DRAWINGS">FIG. 23</figref>.</p>
<p id="p-0200" num="0199">At step <b>2301</b>, an indication of the environment is received. Broadband response files associated with a particular material or environment may have one more attributes associated therewith, for example indicating an associated speed of sound.</p>
<p id="p-0201" num="0200">Such a library of broadband response files may be used to create the illusion of an audio environment according to a displayed scenario within a video gaming environment, for example. In this way, different virtual audio environments may be established.</p>
<p id="p-0202" num="0201">An environment may be modelled and data taken from the model may be utilised in order to enhance the reality of the perception of the emulated origin of processed audio. From the data model, it is possible to determine whether sound is reflected from different surfaces. In the event that early reflections from different surfaces are identified, it is possible to perform convolution operations with broadband response files selected to correspond to the different surfaces. This is found to be of particular assistance in the identification of the height and front-back spatial placement of sound by a listener, for which interaural time differences play less of a part than for left-right spatial placement of sound.</p>
<p id="p-0203" num="0202">Both spatial cues and material or environment cues may be incorporated in a broadband response file. Hence, in a specific embodiment, a single convolution is performed to convolve the audio input with a broadband response file including both spatial and material or environment cues.</p>
<p id="p-0204" num="0203">In an alternative process, however, a first convolution is performed to convolve the audio input signal with a spatial broadband response file and a second convolution is performed to convolve the audio input signal with a material broadband response file.</p>
<p id="p-0205" num="0204">Comparing the former and latter approaches, the processing time to perform a single convolution is quicker than the processing time to perform two separate convolutions. However, more memory is utilised to make available broadband response files including both spatial and material or environment cues than to make available broadband response files including material or environment cues along with to broadband response files including spatial cues.</p>
<p id="p-0206" num="0205">In a specific embodiment, broadband response files are stored with searchable text file names. The text file name preferably includes an indication of the associated location in an originating region and a prefix or suffix to indicate the associated environment or material. Thus, at steps <b>1705</b> and <b>1706</b> of <figref idref="DRAWINGS">FIG. 17</figref>, a scanning procedure is performed to locate the appropriate broadband response file for selection.</p>
<p id="h-0030" num="0000"><figref idref="DRAWINGS">FIG. 24</figref></p>
<p id="p-0207" num="0206">An example of a facility configured to make use of broadband response files, in order to simulate sound sources appearing in a three-dimensional space, is illustrated in <figref idref="DRAWINGS">FIG. 24</figref>. <figref idref="DRAWINGS">FIG. 24</figref> represents an audio recording environment in which live audio sources are received on input lines <b>2401</b> to <b>2406</b>. The audio signals are mixed and a stereo output is supplied to a stereo recording device <b>2411</b>. An audio mixer <b>2412</b> has a filtering section <b>2413</b> and a spatial section <b>2414</b>. For each input channel, the audio filtering section <b>2413</b> includes a plurality of controls illustrated generally as <b>2415</b> for the channel associated with input <b>2401</b>. These include volume controls (often provided in the form of a slider) along with tone controls, typically providing parametric equalisation.</p>
<p id="p-0208" num="0207">The spatial control area <b>2414</b> replaces standard stereo sliders or a rotary pan control. As distinct from positioning an audio source along a stereo field (essentially a linear field) three controls exist for each input channel. Thus, concerning input channel <b>2401</b> a first spatial control <b>2421</b> is included with a second spatial control <b>2422</b> and a third spatial control <b>2423</b>. In an embodiment, the first spatial control <b>2421</b> may be used to control the perceived distance of the sound radially from the notional listener. The second control <b>2422</b> may control the pan of the sound around the listener and the third control <b>2423</b> may control the angular pitch of the sound above and below the listener. In addition to these controls, a visual representation may be provided to a user such that the user may be given a visual view of where the sound should appear to originate from.</p>
<p id="h-0031" num="0000"><figref idref="DRAWINGS">FIG. 25</figref></p>
<p id="p-0209" num="0208">An alternative facility where spatial mixing may be deployed is illustrated in <figref idref="DRAWINGS">FIG. 25</figref>. The environment of <figref idref="DRAWINGS">FIG. 25</figref> represents cinematographic or video editing suite that includes a high definition video recorder <b>2501</b>.</p>
<p id="p-0210" num="0209">In this example, a video signal has been edited and a video input on input line V<b>1</b> is supplied to the video recorder <b>2501</b>. The video recorder <b>2501</b> is also configured to receive an audio left and an audio right signal from an audio mixing station <b>2502</b>.</p>
<p id="p-0211" num="0210">At the audio mixing station, video being supplied to the video recorder <b>2501</b> is displayed to an editor on a visual display <b>2503</b>. Four audio signals are received on audio input lines A<b>1</b>, A<b>2</b>, A<b>3</b> and A<b>4</b>. Each has a respective mixing channel and at each mixing channel, such as the third channel <b>2504</b> there are provided three spatial controls <b>2505</b>, <b>2506</b> and <b>2507</b>. These controls provide a substantially similar function to those described (as <b>2421</b>, <b>2422</b> and <b>2423</b>) in <figref idref="DRAWINGS">FIG. 24</figref>. Thus, they allow the perceived source of the sound to be moved in three-dimensional space.</p>
<p id="p-0212" num="0211">In the environment of <figref idref="DRAWINGS">FIG. 24</figref>, the positioning of sound has few constraints and is left to the creativity of the mixer. However, in the environment of <figref idref="DRAWINGS">FIG. 25</figref>, it is likely that audio inputs will be associated with recorded talent. Thus, an editor may view screen <b>2503</b> in order to identify the locations of said talent and thereby adjust the perceived location of the sound so as to co-ordinate the perceived sound location with that of the location of talent viewed on screen <b>2503</b>.</p>
<p id="h-0032" num="0000"><figref idref="DRAWINGS">FIG. 26</figref></p>
<p id="p-0213" num="0212">An alternative facility for the application of the techniques described herein is illustrated in <figref idref="DRAWINGS">FIG. 26</figref>. <figref idref="DRAWINGS">FIG. 26</figref> represents a video gaming environment having a processing device <b>2601</b> that, structurally, may be similar to the environment illustrated in <figref idref="DRAWINGS">FIG. 8</figref>. However, for the purposes of illustration, operations of the processing environment <b>2601</b> are shown functionally in <figref idref="DRAWINGS">FIG. 26</figref>.</p>
<p id="p-0214" num="0213">An image is shown to someone playing a game via a display unit <b>2602</b>. In addition, stereo loudspeakers <b>2603</b>L and <b>2603</b>R supply stereo audio to the person playing the game. The game is controlled by a hand held controller <b>2604</b>, that may be of a conventional configuration. The hand controller <b>2604</b> (in the functional environment disclosed) supplies control signals to a control system <b>2605</b>. The control system <b>2605</b> is programmed with the operationality of the game itself and generally maintains the movement of objects within a three-dimensional environment, while retaining appropriate historical data such that the game may progress and ultimately reach a conclusion. Part of the operation of the control system <b>2605</b> will be to recognise the extent to which images must be displayed on the monitor <b>2602</b> and provide appropriate three-dimensional data to a movement system <b>2606</b>.</p>
<p id="p-0215" num="0214">Movement system <b>2606</b> is responsible for providing an appropriate display to the user as illustrated on the display unit <b>2602</b> which will also incorporate appropriate audio signals supplied to the loudspeakers <b>2603</b>L and <b>2603</b>R. Thus, a three-dimensional world space is converted into a two-dimensional view, which is then rendered at a rendering system <b>2607</b> in order to provide images to the visual display <b>2602</b>. In combination with this, movement system <b>2606</b> also provides movement data to an audio system <b>2608</b> responsible for generating audio signals. The audio system <b>2608</b> includes synthesising technology to generate audio output signals. In addition, it also receives three-dimensional positional data from the movement system <b>2606</b> such that, by incorporating the techniques disclosed herein, it is possible to place an object within a three-dimensional perceived space. In this way, it is possible for the reality of the game to be enhanced given that sounds may appear as if emanating from a broader spectrum other than from a straight-forward stereo audio field. The listening source location may be identified as that of the player of a game or an avatar within the game, for example.</p>
<p id="h-0033" num="0000"><figref idref="DRAWINGS">FIG. 27</figref></p>
<p id="p-0216" num="0215"><figref idref="DRAWINGS">FIG. 27</figref> illustrates listener <b>101</b> positioned at the centre of the notional three-dimensional originating region <b>1501</b>.</p>
<p id="p-0217" num="0216">In the example, of <figref idref="DRAWINGS">FIG. 27</figref>, listener <b>101</b> is positioned between left audio loudspeaker <b>1504</b> and right audio loudspeaker <b>1505</b>. When facing forward, indicated by arrow <b>1503</b>, the position of each of the speakers <b>1504</b>, <b>1505</b> makes an angle <b>2701</b> of between sixty-five (65) and seventy-five (75) degrees, preferably substantially seventy (70) degrees, in azimuth from the forward direction in which the listener <b>101</b> is facing. As previously described, the positions of substantially plus seventy (+70) degrees and minus seventy (&#x2212;70) degrees in azimuth from the forward direction are considered to output sound at generally the best angle of acceptance for the human ears.</p>
<p id="p-0218" num="0217">In a specific embodiment, the spatial cues from sound outputted at the positions of substantially plus seventy (+70) degrees and minus seventy (&#x2212;70) degrees in azimuth from the forward direction are deconvolved from the broadband response files such that they are introduced by the speakers <b>1504</b>, <b>1505</b>. This has the effect for the listener of the stereo output sound being disconnected form the speaker positions. Thus, an emulated sound is not identified as coming from the speaker positions. Hence, from the perspective of the listener, this effect increases the reality of the perception of the origin of the emulated sound.</p>
<p id="p-0219" num="0218">In a specific embodiment, loudspeakers are located at positions having a common radial distance from the centre of the originating region.</p>
<p id="p-0220" num="0219">The processed stereo output signal may be received through a pair of headphones, such as stereo headphones <b>2702</b>. It is found that when stereo headphones are used to receive a processed stereo output signal there is negligible difference in the overall perception of the origin of the emulated sound from when the same processed stereo output signal is received through the speakers <b>1504</b>, <b>1505</b>. Thus, the techniques described herein enable a stereo output signal having independent left and rights fields to be produced that is perceived by a listener as the same sound whether the sound is output from stereo speakers or from stereo headphones.</p>
<p id="h-0034" num="0000"><figref idref="DRAWINGS">FIG. 28</figref></p>
<p id="p-0221" num="0220">In the environment of <figref idref="DRAWINGS">FIG. 26</figref>, the technique for generating three-dimensional sound position is being deployed and the sounds are being produced while the deployment takes place. This differs from the environments of <figref idref="DRAWINGS">FIGS. 24 and 25</figref> where the techniques are being deployed to generate the three-dimensional effects while the resulting sounds are being recorded for later reproduction.</p>
<p id="p-0222" num="0221">In environments where the sounds are to be reproduced for a group of people (such as a sound recording) or for a larger audience, as in the case of a cinematographic film, it is preferable for measures to be taken to ensure that the audience obtain maximum benefit from the processed sound.</p>
<p id="p-0223" num="0222">In the example of <figref idref="DRAWINGS">FIG. 28</figref>, a front left audio loudspeaker <b>2801</b> is provided along with a front right audio loudspeaker <b>2802</b>. When facing forward, indicated by arrow <b>2803</b>, the position of each of the speakers <b>2801</b>, <b>2802</b> makes an angle <b>2804</b> of between twenty-five (25) and thirty-five (35) degrees, preferably substantially thirty (30) degrees, in azimuth from the forward direction in which the listener <b>101</b> is facing.</p>
<p id="p-0224" num="0223">In addition, to enhance the stereo effect, rear speakers are provided, consisting of a left rear speaker <b>2805</b> and a right rear speaker <b>2806</b>.</p>
<p id="p-0225" num="0224">When facing forward, as illustrated in <figref idref="DRAWINGS">FIG. 28</figref>, the position of each rear speaker <b>2805</b>, <b>2806</b> makes an angle <b>2807</b> of between one hundred and five (105) degrees and one hundred and fifteen (115) degrees, preferably substantially one hundred and ten (110) degrees, from the forward direction in which the listener is facing.</p>
<p id="p-0226" num="0225">Left speakers <b>2801</b> and <b>2805</b> both receive the left channel signal and right speakers <b>2802</b> and <b>2806</b> both receive the right channel signal. Thus, the stereo channel signals provided to the front speakers <b>2801</b> and <b>2802</b> is duplicated for the rear speakers <b>2805</b> and <b>2806</b>.</p>
<p id="p-0227" num="0226">Thus, by the provision of four (4) loudspeakers in preference to two (2) loudspeakers, a region <b>2808</b> is defined such that when located in this region substantially all of the stereo and three-dimensional effects are perceived. In this way it is possible to increase the size of the &#x201c;sweet spot&#x201d; of the audio field. Such an approach is considered to be particularly attractive when reliance is being made on very high frequencies and otoacoustics in order to enhance the three-dimensional effect.</p>
<p id="p-0228" num="0227">When facing forward, as illustrated in <figref idref="DRAWINGS">FIG. 28</figref>, the listener <b>101</b> perceives the sound as originating from a location between the front and rear speakers. As previously described, with the front speakers located at minus thirty (&#x2212;30) degrees and plus thirty (+30) degrees and the rear speakers located at minus one hundred and ten (&#x2212;110) degrees and plus one hundred and ten (+110) degrees as described, the listener perceives a &#x2018;phantom image&#x2019; of the sound as generally originating from locations at substantially minus seventy (&#x2212;70) degrees and plus seventy (+70) degrees.</p>
<p id="p-0229" num="0228">The stereo channel signals provided to the front speakers <b>2801</b> and <b>2802</b> may be duplicated for each additional pair of speakers utilised in an application.</p>
<p id="p-0230" num="0229">As indicated in <figref idref="DRAWINGS">FIG. 28</figref>, additional left audio loudspeakers <b>2809</b> to <b>2811</b> may be located between the front and rear right audio speakers <b>2801</b>, <b>2805</b> whilst additional right audio loudspeakers <b>2812</b> to <b>2814</b> may be located between the front and rear right audio speakers <b>2802</b>, <b>2806</b>. It is found that the acoustic energy from these additional speakers does not affect the perception of a &#x2018;phantom image&#x2019; of the sound as generally originating from locations at substantially minus seventy (&#x2212;70) degrees and plus seventy (+70) degrees.</p>
<p id="p-0231" num="0230">As indicated, the stereo output signal can be physically output through a single pair of speakers or through multiple pairs of speakers.</p>
<p id="p-0232" num="0231">In an arrangement having a plurality of pairs of loudspeakers the left and right channels of the stereo signal are duplicated for the second and each additional pair of speakers.</p>
<p id="p-0233" num="0232">If four (4) discrete audio channels are available, the left channel signal is duplicated for a second left speaker and similarly the right channel signal is duplicated for a second right speaker.</p>
<p id="p-0234" num="0233">This is contrast to 4-2-4 processing systems that derive four (4) streams of information from two (2) input streams of information. In such systems, the two (2) input audio streams are used to directly feed left and right channels. Further processing is performed upon the audio streams to identify identical signals that are in phase, which are used to drive a third centre channel, and to identify identical signals in each stream that are out of phase, which are used to drive a fourth surround channel.</p>
<p id="p-0235" num="0234">In movie theatres, the centre channel is often used to feed a centre speaker, which serves to anchor the output sound to the movie screen, whilst the surround channel is used to feed a series of displaced speakers, intensity panning along the series of speakers utilised in order to emulate the production of a moving sound source.</p>
<p id="p-0236" num="0235">It is found that incorporating spatial cues into stereo output signals (having a left field and a right field) as described herein provides a better perceived panorama of sound than that achieved by intensity panning.</p>
<p id="p-0237" num="0236">Further, as previously described, spatial cues may be incorporated into the stereo output signals as described herein may be used to provide or remove anchoring effects in sounds emulating the production of said audio signal from a specified audio source location relative to a listening source location.</p>
<p id="p-0238" num="0237">The processing performed to extract information to drive the centre and surround channels results in loss of fidelity and quality of the output audio signals.</p>
<p id="p-0239" num="0238">By incorporating spatial cues into stereo output signals (having a left field and a right field) as described herein, the desired emulation of the production of said audio signal from a specified audio source location relative to a listening source location may be achieved more efficiently. The effect may be achieved through the use of a single pair of speakers. However, where the left and right channels are used to derive further channels, the duplication of channels results in improved fidelity and quality of sound, again using the additional channels efficiently to enhance the stereo effect.</p>
<p id="p-0240" num="0239">In Dolby Digital 5.1&#xae; and DTS Digital Sound&#xae; systems, six (6) discrete audio channels are encoded onto a digital data storage medium, such as a CD or film. These channels are then split up by a decoder and distributed for playing through an arrangement of different speakers.</p>
<p id="p-0241" num="0240">Thus, the left and right channels of stereo output signals produced as described herein may be used to feed six (6) or more audio channels such that existing hardware using such systems may be used to reproduce the audio signals.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method comprising processing an audio input signal represented as digital samples to produce a stereo output signal having a left field and a right field, said stereo output signal emulating the production of said audio input signal from a virtual audio source location relative to a listening location said listening location having a left listening position and right listening position corresponding to the left and right ears of a typical listener, using audio processing apparatus having a processing device, a first input device, a second input device and an output device, said method comprising the steps of:
<claim-text>receiving by said first input device said audio input signal:</claim-text>
<claim-text>receiving by said second input device an indication of a virtual audio source location relative to a listening location (an indicated location),</claim-text>
<claim-text>receiving by said second input device an indication of distance between said left listening position and said right listening position,</claim-text>
<claim-text>determining by said processing device said left and right listening positions based on said listening location and said indication of distance:</claim-text>
<claim-text>determining by said processing device a first direction from said indicated location to said left listening position and a second direction from said indicated location to said right listening position;</claim-text>
<claim-text>selecting by said processing device a left field broadband response file (a selected left field response file) from a plurality of stored files derived from empirical testing, said selected left field response file describing the impulse response of a left ear to a sound emitted from said first direction;</claim-text>
<claim-text>selecting by said processing device a right field broadband response file (a selected right field response file) from a plurality of stored files derived from empirical testing, said selected right field response file describing the impulse response of a right ear to a sound emitted from said second direction:</claim-text>
<claim-text>convolving by said processing device said audio input signal with said selected left field response file to produce the left field of said stereo signal output signal,</claim-text>
<claim-text>convolving by said processing device said audio input signal with said selected right field response file to produce the right field of said stereo output signal, and</claim-text>
<claim-text>supplying said stereo output signal to said output device.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said audio input signal is a live signal, a recorded signal or a synthesised signal.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the indicated location is manually indicated or indicated in response to operations performed within a computer game.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further including the step of receiving an indication of the speed of sound.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. A method according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, further including the step of calculating an output signal intensity.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. A method according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, further including the step of calculating an output signal attenuation.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. A method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, further including the step of calculating an output signal delay.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a broadband response file is stored for at least 770 test positions for each of a first ear and a second ear of a human subject.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a plurality of broadband response files is stored for each of a plurality of test positions selected during said empirical testing, each of the plurality of broadband response files for a test position relating to a different subject material or environment,
<claim-text>said method further includes the step of receiving an indication of a material or environment, and</claim-text>
<claim-text>said steps of selecting a broadband response file involve scanning the filenames of the plurality of broadband response files stored for a test position.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a Fast Fourier Transform convolution process is performed at each said stop of convolving.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. Apparatus for processing an audio input signal represented as digital samples to produce a stereo output signal having a left field and a right field, said stereo output signal emulating the production of said audio input signal from a virtual audio source location relative to a listening location said listening location having a left listening position and a right listening position corresponding to the left and right ears of a typical listener comprising:
<claim-text>a first input device for receiving said audio input signal;</claim-text>
<claim-text>a second input device for receiving an indication of a virtual audio source location relative to a listening location (an indicated location) and an indication of distance between said left listening position and said right listening position: and</claim-text>
<claim-text>a processing device configured to:</claim-text>
<claim-text>determine said left and right listening positions, based on said listening location and said indication of distance;</claim-text>
<claim-text>determine a first direction from said indicated location to said left listening position and a second direction from said indicated location to said right listening position:</claim-text>
<claim-text>select a left field broadband response file (a selected left field response file) from a plurality of stored files derived from empirical testing, said selected left field response file describing the impulse response of a left ear to a sound emitted from said first direction:</claim-text>
<claim-text>select a right field broadband response file (a selected right field response file) from a plurality of stored files derived from empirical testing, said selected right field response file describing the impulse response of a right ear to a sound emitted from said second direction:</claim-text>
<claim-text>convolve said audio input signal with said selected left field response file to produce the left field of said stereo output signal:</claim-text>
<claim-text>convolve said audio input signal with said selected right field response file to produce the right field of said stereo output signal and</claim-text>
<claim-text>supply said stereo output signal lo an output device.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. Apparatus according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein said audio input signal is a live signal, a recorded signal or a synthesised signal.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. Apparatus according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the indicated location is manually indicated or indicated in response to operations performed within a computer game.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. Apparatus according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein a Fast Fourier Transform convolution process is performed at each said step of convolving.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. Apparatus comprising a non-transitory computer-readable media having computer-readable instructions executable by a computer such that, when executing said instructions, a computer will perform a method comprising processing an audio input signal represented as digital samples to produce a stereo output signal having a left field and a right field, said stereo output signal emulating the production of said audio input signal from a virtual audio source location relative to a listening location, said listening location having a left listening position and a right listening position corresponding to the left and right ears of a typical listener, using an audio processing apparatus having a processing device, a first input device, a second input device and an output device, said method comprising the steps of:
<claim-text>receiving said audio input signal;</claim-text>
<claim-text>receiving an indication of a virtual audio source location relative to a listening source location (an indicated location) and an indication of distance between said left listening position and said right listening position;</claim-text>
<claim-text>determining by said processing device said left and right listening positions, based on said listening location and said indication of distance;</claim-text>
<claim-text>determining by said processing device a first direction from said indicated location to said left listening position and a second direction from said indicated location to said right listening position;</claim-text>
<claim-text>selecting by said processing device a left field broadband response file (a selected left field response file) form a plurality of stored files derived from empirical testing, said selected left field response file describing the impulse response of a left ear to a sound emitted from said first direction;</claim-text>
<claim-text>selecting by said processing device a right field broadband response file (a selected right field response file) form a plurality of stored files derived from empirical testing, said selected right field response file describing the impulse response of a le right t ear to a sound emitted from said second direction;</claim-text>
<claim-text>convolving said audio input signal with said selected left field response file to produce the left field of said stereo output signal;</claim-text>
<claim-text>convolving said audio input signal with said selected right field response file to produce the right field of said stereo output signal; and</claim-text>
<claim-text>supplying said stereo output to an output device.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. A computer-readable medium according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein said audio input signal is a live signal, a recorded signal or a synthesised signal.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. A computer-readable medium according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the indicated location is manually indicated or indicated in response to operations performed within a computer game. </claim-text>
</claim>
</claims>
</us-patent-grant>
