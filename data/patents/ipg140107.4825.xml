<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625918-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625918</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13178609</doc-number>
<date>20110708</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>JP</country>
<doc-number>2010-161658</doc-number>
<date>20100716</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>262</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382251</main-classification>
</classification-national>
<invention-title id="d2e71">Image processing apparatus and image processing method</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>6430224</doc-number>
<kind>B1</kind>
<name>Naito et al.</name>
<date>20020800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524016</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>7746931</doc-number>
<kind>B2</kind>
<name>Kato et al.</name>
<date>20100600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524016</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>8139150</doc-number>
<kind>B2</kind>
<name>Nakamura et al.</name>
<date>20120300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3484091</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>2013/0088570</doc-number>
<kind>A1</kind>
<name>Takahashi et al.</name>
<date>20130400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>JP</country>
<doc-number>2000-138938</doc-number>
<kind>A</kind>
<date>20000500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>10</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382232</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382234</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382236</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382238</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382248</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382250-252</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348 42</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3483941-3951</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3484001-4031</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3484081-4131</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3484161</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3484201-4211</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3484301-4311</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>37524012-24024</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>7</number-of-drawing-sheets>
<number-of-figures>7</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120014614</doc-number>
<kind>A1</kind>
<date>20120119</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Takahashi</last-name>
<first-name>Yoshitomo</first-name>
<address>
<city>Kanagawa</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Suzuki</last-name>
<first-name>Teruhiko</first-name>
<address>
<city>Kanagawa</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Kitamura</last-name>
<first-name>Takuya</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Takahashi</last-name>
<first-name>Yoshitomo</first-name>
<address>
<city>Kanagawa</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Suzuki</last-name>
<first-name>Teruhiko</first-name>
<address>
<city>Kanagawa</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Kitamura</last-name>
<first-name>Takuya</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Wolf, Greenfield &#x26; Sacks, P.C.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Sony Corporation</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Couso</last-name>
<first-name>Jose</first-name>
<department>2667</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">An image processing apparatus includes a quantizer configured to quantize orthogonal transform coefficients of the difference in block units between a coding target picture and a prediction image, a predictor configured to conduct parallax prediction utilizing correlations between the coding target picture and a picture whose view differs from the coding target picture, and a quantization controller configured to determine a protected area that protects image quality from the parallax prediction results, and reduce the quantization step size of the quantizer for quantization of the protected area.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="173.99mm" wi="252.05mm" file="US08625918-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="216.66mm" wi="143.09mm" orientation="landscape" file="US08625918-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="89.24mm" wi="137.58mm" file="US08625918-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="251.38mm" wi="173.06mm" orientation="landscape" file="US08625918-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="256.12mm" wi="162.81mm" file="US08625918-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="258.83mm" wi="162.14mm" file="US08625918-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="252.73mm" wi="162.14mm" file="US08625918-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="250.02mm" wi="156.04mm" orientation="landscape" file="US08625918-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND</heading>
<p id="p-0002" num="0001">The present disclosure relates to an image processing apparatus and an image processing method, and more specifically, to improving image quality for a multi-view image when coding an image with multiple views.</p>
<p id="p-0003" num="0002">In recent years, there exist apparatus that handle image information as digital data, and in this regard conduct highly efficient information transfer and accumulation, such as an apparatus compliant with a technique such as MPEG that compresses by discrete cosine transform or other orthogonal transform and motion compensation, for example. Such apparatus are becoming widespread in broadcasting stations and general households.</p>
<p id="p-0004" num="0003">Particularly, MPEG-2 (ISO/IEC 13818-2) is defined as a general-purpose image coding format, and currently is widely used in a broad range of applications for professional use and consumer use. Furthermore, an image coding format called H.264 and MPEG-4 Part 10 (hereinafter designated H.264/AVC (Advanced Video Coding)) is becoming standardized, and although more computation is demanded for its coding and decoding compared to coding formats such as MPEG-2, a higher coding efficiency can be realized. Also, with the coding of images by coding formats such as such as MPEG2, MPEG4, and H.264/AVC, it is typical to adjust the bitrate in order to obtain a higher coding efficiency. With bitrate adjustment, adaptive quantization taking vision characteristics into account is conducted. For example, in Japanese Unexamined Patent Application Publication No. 2000-138938, quantization is conducted by adaptively switching a quantization scale and an adjusted quantization scale according to the coding size of the current image, on the basis of a given parameter computed during the coding of a prior image.</p>
<heading id="h-0002" level="1">SUMMARY</heading>
<p id="p-0005" num="0004">Meanwhile, with frame-sequential (FS) AVC and multi-view video coding (MVC), not only temporal prediction using temporal correlations between pictures, but also parallax prediction using correlations between pictures with different views, is conducted.</p>
<p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. 1</figref> illustrates reference relationships for prediction when coding two-view motion image data, for example. Herein, Cam<b>0</b> is taken to be image data of a left-view image, and Cam<b>1</b> is taken to be image data of a right-view image. The Cam<b>1</b> image data is taken to be dependent view image data which is coded by using the Cam<b>0</b> image data as image data of a reference picture. Image data referenced when coding image data of a dependent view is called base view image data.</p>
<p id="p-0007" num="0006">The P picture (Pdv<b>1</b>) in the Cam<b>1</b> image data takes as a reference picture the Cam<b>0</b> I picture (Ib<b>1</b>), which is referenced by parallax prediction as indicated by the arrow. The P picture (Pdv<b>3</b>) in the Cam<b>1</b> image data takes as reference pictures the Cam<b>0</b> P picture (Pb<b>3</b>) which is referenced by parallax prediction and the P picture (Pdv<b>1</b>) which referenced by temporal prediction as indicated by the arrows. The B picture (Bdv<b>2</b>) in the Cam<b>1</b> image data takes as reference pictures the Cam<b>0</b> Bs picture (Bsb<b>2</b>) which is referenced by parallax prediction, and also the P picture (Pdv<b>1</b>) and the P picture (Pdv<b>3</b>) which are referenced by temporal prediction, as indicated by the arrows. Meanwhile, the P picture (Pb<b>3</b>) in the Cam<b>0</b> image data takes as a reference picture the I picture (Ib<b>1</b>) which is referenced by temporal prediction as indicated by the arrow. The Bs picture (Bsb<b>2</b>) in the Cam<b>1</b> image data takes as reference pictures the I picture (Ib<b>1</b>) and the P picture (Pb<b>3</b>) which are referenced by temporal prediction as indicated by the arrows.</p>
<p id="p-0008" num="0007">Even in such encoding of multi-view images, the bitrate is adjusted in order to obtain a higher coding efficiency.</p>
<p id="p-0009" num="0008">Accordingly, it is desirable to provide an image processing apparatus and an image processing method able to improve image quality for a multi-view image when coding an image with multiple views.</p>
<p id="p-0010" num="0009">One embodiment is an image processing apparatus including a quantizer that quantizes orthogonal transform coefficients of the difference in block units between a coding target picture and a prediction image, a predictor that conducts parallax prediction utilizing correlations between the coding target picture and a picture whose view differs from the coding target picture, and a quantization controller that determines a protected area that protects image quality from the parallax prediction results, and modifies and reduces the quantization step size of the quantizer for quantization of the protected area.</p>
<p id="p-0011" num="0010">In an embodiment, parallax prediction utilizing correlations between a coding target picture and a picture whose view differs from the coding target picture is conducted, and determination of a protected area is conducted using at least one from among a parallax vector detected by parallax prediction and error from when parallax compensation was conducted using the parallax vector. For example, in the case of using a parallax vector, the horizontal components of the parallax vector may be used, and a coding target block whose parallax vector is greater than statistics used as a basis for determining a protected area which are computed from the parallax vector of an already-coded picture (such as the average value of parallax vectors in a picture), and a coding target block whose parallax vector is greater than the parallax vector of an immediately preceding already-coded block, are determined to be a protected area. Also, in the case of using error from when parallax compensation was conducted, a coding target block whose error is greater than statistics used as a basis for determining a protected area which are computed from the error of an already-coded picture (such as the average value of the error in a picture) to be the protected area.</p>
<p id="p-0012" num="0011">Another embodiment is an image processing method including quantizing orthogonal transform coefficients of the difference in block units between a coding target picture and a prediction image, conducting parallax prediction utilizing correlations between the coding target picture and a picture whose view differs from the coding target picture, and determining a protected area that protects image quality from the parallax prediction results, and reducing the quantization step size for quantization of the protected area.</p>
<p id="p-0013" num="0012">According to an embodiment, parallax prediction utilizing correlations between a coding target picture and a picture whose view differs from the coding target picture is conducted, a protected area that protects image quality is determined from the parallax prediction results, and the quantization step size for quantization of the protected area is reduced. Consequently, the image quality of the protected area is protected during coding of a multi-view motion image, and the image quality for a multi-view image can be improved.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 1</figref> illustrates prediction reference relationships when coding two-view motion image data;</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 2</figref> illustrates a configuration of a coding system;</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 3</figref> illustrates a configuration of an image processing apparatus;</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 4</figref> is a flowchart illustrating a first operation of an image processing apparatus;</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 5</figref> is a flowchart illustrating a second operation of an image processing apparatus;</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 6</figref> is a flowchart illustrating a third operation of an image processing apparatus; and</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 7</figref> is illustrates a configuration in the case of conducting image coding by software processing.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION OF EMBODIMENTS</heading>
<p id="p-0021" num="0020">Hereinafter, embodiments of the disclosed technology will be described. Herein, the description will be conducted in the following order.
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0021">1. Exemplary configuration of coding system</li>
        <li id="ul0002-0002" num="0022">2. Configuration of image processing apparatus</li>
        <li id="ul0002-0003" num="0023">3. Operation of image processing apparatus
        <ul id="ul0003" list-style="none">
            <li id="ul0003-0001" num="0024">3-1. First operation of image processing apparatus</li>
            <li id="ul0003-0002" num="0025">3-2. Second operation of image processing apparatus</li>
            <li id="ul0003-0003" num="0026">3-3. Third operation of image processing apparatus</li>
            <li id="ul0003-0004" num="0027">3-4. Other operations of image processing apparatus</li>
        </ul>
        </li>
        <li id="ul0002-0004" num="0028">4. Case of conducting image coding by software processing</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0022" num="0029">&#x3c;1. Exemplary Configuration of Coding System&#x3e;</p>
<p id="p-0023" num="0030"><figref idref="DRAWINGS">FIG. 2</figref> is a diagram illustrating an exemplary configuration of a coding system to which the disclosed technology is applied. A coding system <b>10</b> includes a left-view image generating apparatus <b>11</b>L, a right-view image generating apparatus <b>11</b>R, and a multi-view coding apparatus <b>20</b>.</p>
<p id="p-0024" num="0031">The left-view image generating apparatus <b>11</b>L is an imaging apparatus or image data generating apparatus that generates image data for a left-view image. The right-view image generating apparatus <b>11</b>R is an imaging apparatus or an image data generating apparatus that generates a right-view image. The left-view image generating apparatus <b>11</b>L and the right-view image generating apparatus <b>11</b>R operate in synchronization.</p>
<p id="p-0025" num="0032">Input into the multi-view coding apparatus <b>20</b> is image data for a left-view image generated by the left-view image generating apparatus <b>11</b>L, and image data for a right-side view image generated by the right-view image generating apparatus <b>11</b>R. The multi-view coding apparatus <b>20</b> respectively codes the image data for the left-side-view image and the image data for the right-side-view image, multiplexes the obtained coded data, and outputs the data as a single bitstream.</p>
<p id="p-0026" num="0033">The multi-view coding apparatus <b>20</b> includes an image processing apparatus that codes image data for a left-view image and an image processing apparatus that codes image data for a right-view image. The image processing apparatus that codes image data for a left-view image codes image data for a left-view image as base view image data, for example. The image processing apparatus that codes image data for a right-view image codes image data for a right-side-view as dependent view image data which is coded by referencing the image data for a left-view image.</p>
<p id="p-0027" num="0034">The image processing apparatus generate feedback information from the results of already-coded picture and block parallax prediction. The image processing apparatus determine an image quality protected area on the basis of the generated feedback information, and modifies the quantization step size to make it smaller when coding the protected area. In other words, the image processing apparatus improve image quality for a multi-view image by conducting adaptive quantization in multi-view image coding.</p>
<p id="p-0028" num="0035">&#x3c;2. Configuration of Image Processing Apparatus&#x3e;</p>
<p id="p-0029" num="0036"><figref idref="DRAWINGS">FIG. 3</figref> illustrates a configuration of an image processing apparatus, such as an image processing apparatus <b>20</b> <i>dv </i>that codes dependent view image data, for example. The image processing apparatus <b>20</b><i>dv </i>is provided with an analog/digital converter (A/D converter) <b>21</b>, a picture sort buffer <b>22</b>, a subtractor <b>23</b>, an orthogonal transform unit <b>24</b>, a quantizer <b>25</b>, a reversible coder <b>26</b>, an accumulation buffer <b>27</b>, and a rate controller <b>28</b>. The image processing apparatus <b>20</b><i>dv </i>also includes a reverse quantizer <b>31</b>, an inverse orthogonal transform unit <b>32</b>, an adder <b>33</b>, a deblocking filter <b>34</b>, and frame memory <b>35</b>. The image processing apparatus <b>20</b><i>dv </i>is additionally provided with an intra predictor <b>41</b>, a motion/parallax prediction compensator <b>42</b>, a prediction image/optimal mode selector <b>43</b>, and a quantization controller <b>45</b>.</p>
<p id="p-0030" num="0037">The A/D converter <b>21</b> converts an analog image signal into digital image data and outputs the data to the picture sort buffer <b>22</b>.</p>
<p id="p-0031" num="0038">The picture sort buffer <b>22</b> sorts frames with respect to image data output from the A/D converter <b>21</b>. The picture sort buffer <b>22</b> sorts frames corresponding to a GOP (Group of Pictures) structure for a coding process, and outputs sorted image data to the subtractor <b>23</b>, the intra predictor <b>41</b>, and the motion/parallax prediction compensator <b>42</b>.</p>
<p id="p-0032" num="0039">The subtractor <b>23</b> is supplied with image data output from the picture sort buffer <b>22</b> and prediction image data selected by the prediction image/optimal mode selector <b>43</b> later described. The subtractor <b>23</b> computes prediction error data, which is the difference between image data output from the picture sort buffer <b>22</b> and prediction image data supplied from the prediction image/optimal mode selector <b>43</b>, and outputs the data to the orthogonal transform unit <b>24</b>.</p>
<p id="p-0033" num="0040">The orthogonal transform unit <b>24</b> conducts a discrete cosine transform (DCT), Karhunen-Loeve transform, or other orthogonal transform process on prediction error data output from the subtractor <b>23</b>. The orthogonal transform unit <b>24</b> outputs transform coefficients obtained by conducting an orthogonal transform process to the quantizer <b>25</b>.</p>
<p id="p-0034" num="0041">The quantizer <b>25</b> is supplied with transform coefficients output from the orthogonal transform unit <b>24</b>, a rate control signal from the rate controller <b>28</b> later described, and a comparison result signal from the quantization controller <b>45</b>. The quantizer <b>25</b> quantizes the transform coefficients and outputs quantization data to the reversible coder <b>26</b> and the reverse quantizer <b>31</b>. The quantizer <b>25</b> also switches the quantization step size on the basis of the rate control signal from the rate controller <b>28</b>, and causes the bitrate of the quantization data to change. Additionally, the quantizer <b>25</b> improves image quality for a multi-view image by switching the quantization step size inside pictures for a multi-view image on the basis of a quantization control signal from the quantization controller <b>45</b>.</p>
<p id="p-0035" num="0042">The reversible coder <b>26</b> is supplied with quantized data output from the quantizer <b>25</b> and prediction mode information from the intra predictor <b>41</b> later described, the motion/parallax prediction compensator <b>42</b>, and the prediction image/optimal mode selector <b>43</b>. Herein, a macroblock type indicating the coding block size, a prediction mode, reference indices, etc. for intra prediction or inter prediction are included in prediction mode information. The reversible coder <b>26</b> conducts a coding process on quantized data according to variable-length coding or arithmetic coding, for example, generates a coded stream, and outputs the stream to the accumulation buffer <b>27</b>. The reversible coder <b>26</b> also reversibly codes prediction mode information and adds the information to the coded stream's header information, for example.</p>
<p id="p-0036" num="0043">The accumulation buffer <b>27</b> accumulates a coded stream from the reversible coder <b>26</b>. The accumulation buffer <b>27</b> also outputs an accumulated coded stream at a transmission speed that depends on the transmission path.</p>
<p id="p-0037" num="0044">The rate controller <b>28</b> monitors free space in the accumulation buffer <b>27</b>, generates a rate control signal according to the free space, and outputs the signal to the quantizer <b>25</b>. The rate controller <b>28</b> acquires information indicating free space from the accumulation buffer <b>27</b>, for example. In the case where free space is becoming low, the rate controller <b>28</b> lowers the bitrate of quantized data with a rate control signal. Also, in the case of a sufficiently large amount of free space in the accumulation buffer <b>27</b>, the rate controller <b>28</b> raises the bitrate of quantized data with a rate control signal.</p>
<p id="p-0038" num="0045">The reverse quantizer <b>31</b> conducts reverse quantization process on quantized data supplied from the quantizer <b>25</b>. The reverse quantizer <b>31</b> outputs transform coefficients obtained by conducting a reverse quantization process to the inverse orthogonal transform unit <b>32</b>.</p>
<p id="p-0039" num="0046">The inverse orthogonal transform unit <b>32</b> conducts an inverse orthogonal transform process on transform coefficients supplied from the reverse quantizer <b>31</b>, and outputs data obtained as a result to the adder <b>33</b>.</p>
<p id="p-0040" num="0047">The adder <b>33</b> generates image data of a reference picture by adding data supplied from the inverse orthogonal transform unit <b>32</b> to prediction image data supplied from the prediction image/optimal mode selector <b>43</b>. The adder <b>33</b> outputs this image data to the deblocking filter <b>34</b> and the intra predictor <b>41</b>.</p>
<p id="p-0041" num="0048">The deblocking filter <b>34</b> conducts a filter process for decreasing blocking artifacts that occur when coding images. The deblocking filter <b>34</b> conducts a filter process that removes blocking artifacts from image data supplied from the adder <b>33</b>, and outputs filtered image data to the frame memory <b>35</b>.</p>
<p id="p-0042" num="0049">The frame memory <b>35</b> holds filtered image data supplied from the deblocking filter <b>34</b> and reference picture image data supplied from an image processing apparatus <b>20</b><i>bv </i>that codes the base view.</p>
<p id="p-0043" num="0050">The intra predictor <b>41</b> uses coding target reference picture image data output from the picture sort buffer <b>22</b> and image data supplied from the adder <b>33</b> to conduct an intra prediction process in all candidate intra prediction modes. Additionally, the intra predictor <b>41</b> computes a cost function value for each intra prediction mode, and selects the intra prediction mode with the minimum computed cost function, i.e. the intra prediction mode with the best coding efficiency, as the optimal intra prediction mode. The intra predictor <b>41</b> outputs prediction image data generated with the optimal intra prediction mode, prediction mode information regarding the optimal intra prediction mode, and the cost function value of the optimal intra prediction mode to the prediction image/optimal mode selector <b>43</b>. The intra predictor <b>41</b> also outputs prediction mode information regarding the intra prediction mode to the reversible coder <b>26</b> in the intra prediction process for each intra prediction mode, in order to obtain resulting code sizes used to compute cost function values as described later. Herein, a method implemented in the H.264/AVC reference software, called JM (Joint Model), can be given as an example of a method for computing cost function values.</p>
<p id="p-0044" num="0051">The motion/parallax prediction compensator <b>42</b> conducts a prediction/compensation process on each image of coding block size in a coding target picture read out from the picture sort buffer <b>22</b>. The motion/parallax prediction compensator <b>42</b> uses coding block image data and image data filtered for deblocking read out from the frame memory <b>35</b> to predict and detect motion vectors. The motion/parallax prediction compensator <b>42</b> also uses coding block image data and base view image data to predict and detect parallax vectors. Additionally, the motion/parallax prediction compensator <b>42</b> performs a compensation process on a reference picture on the basis of detected motion vectors (parallax vectors) to generate a prediction image.</p>
<p id="p-0045" num="0052">Also, the motion/parallax prediction compensator <b>42</b> computes a cost function value for each block size and reference picture, and selects the block size and reference picture with the minimum cost function value as the optimal inter prediction mode. The motion/parallax prediction compensator <b>42</b> outputs prediction image data generated with the optimal inter prediction mode, prediction mode information regarding the optimal inter prediction mode, and the cost function value of the optimal inter prediction mode to the prediction image/optimal mode selector <b>43</b>. The motion/parallax prediction compensator <b>42</b> also outputs prediction mode information regarding the inter prediction mode to the reversible coder <b>26</b> in the inter prediction process at the block size of each prediction mode, in order to obtain resulting code sizes used to compute cost function values.</p>
<p id="p-0046" num="0053">The prediction image/optimal mode selector <b>43</b> compares a cost function value supplied from the intra predictor <b>41</b> to a cost function value supplied from the motion/parallax prediction compensator <b>42</b>, and selects the smaller cost function value as the optimal mode with the best coding efficiency. The prediction image/optimal mode selector <b>43</b> also outputs prediction image data generated in the optimal mode to the subtractor <b>23</b> and the adder <b>33</b>. Additionally, the prediction image/optimal mode selector <b>43</b> outputs prediction mode information on the optimal mode (macroblock type, prediction mode, reference index, etc.) to the reversible coder <b>26</b>.</p>
<p id="p-0047" num="0054">The quantization controller <b>45</b> determines a protected area from the results of parallax prediction conducted at the motion/parallax prediction compensator <b>42</b>. Additionally, the quantization controller <b>45</b> generates and supplies a quantization control signal based on the determination result to the quantizer <b>25</b>. In so doing, the quantization controller <b>45</b> modifies the quantization step size in the quantizer <b>25</b> when coding the protected area, decreasing the quantization step size and protecting the image quality of the protected area. In this way, the quantization controller <b>45</b> adaptively switches the quantization step size on the basis of parallax prediction results when coding a multi-view image.</p>
<p id="p-0048" num="0055">&#x3c;3. Operation of Image Processing Apparatus&#x3e;</p>
<p id="p-0049" num="0056">When coding a multi-view image, an image processing apparatus modifies and decreases the quantization step size while coding a protected area, thereby protecting image quality and improving the subjective image quality of a multi-view image. Also, an image processing apparatus determines a protected area by using at least one from among a parallax vector detected by parallax prediction and the error when that parallax vector is used to conduct parallax compensation. An image processing apparatus, on the basis of parallax prediction results, determines a subject positioned in front (a 3D object) or the boundary of a 3D object as a protected area.</p>
<p id="p-0050" num="0057">&#x3c;3-1. First Operation of Image Processing Apparatus&#x3e;</p>
<p id="p-0051" num="0058">In a first operation of an image processing apparatus, the case of determining a protected area using a parallax vector of a coding target macroblock and a parallax vector detected in an already-coded picture will be described. Herein, the horizontal components of parallax vectors are used to determine a protected area in the case of generating base view and dependent view image data by differentiating the positions of the left view and the right view in the horizontal direction only.</p>
<p id="p-0052" num="0059"><figref idref="DRAWINGS">FIG. 4</figref> is a flowchart illustrating a first operation of an image processing apparatus. In a step ST<b>1</b>, an image processing apparatus <b>20</b><i>dv </i>determines whether or not the coding target picture is a dependent view picture. The image processing apparatus <b>20</b><i>dv </i>proceeds to a step ST<b>2</b> in the case where the coding target picture is a dependent view picture, and proceeds to a step ST<b>9</b> in the case of a base view picture.</p>
<p id="p-0053" num="0060">In step ST<b>2</b>, the image processing apparatus <b>20</b><i>dv </i>conducts parallax prediction on a coding target macroblock. The motion/parallax prediction compensator <b>42</b> of the image processing apparatus <b>20</b><i>dv </i>conducts parallax prediction using image data of the coding target macroblock and base view image data, detects a parallax vector, and proceeds to a step ST<b>3</b>.</p>
<p id="p-0054" num="0061">In step ST<b>3</b>, the image processing apparatus <b>20</b><i>dv </i>acquires a parallax vector. The quantization controller <b>45</b> of the image processing apparatus <b>20</b><i>dv </i>acquires a parallax vector detected by the motion/parallax prediction compensator <b>42</b> and proceeds to a step ST<b>4</b>.</p>
<p id="p-0055" num="0062">In step ST<b>4</b>, the image processing apparatus <b>20</b><i>dv </i>determines a protected area. The quantization controller <b>45</b> of the image processing apparatus <b>20</b><i>dv </i>compares feedback information generated in a step ST<b>8</b> later described to the parallax vector, determines a protected area, and proceeds to a step ST<b>5</b>. In the case where, for example, statistics used as a basis for determining a protected area are taken as feedback information as discussed later, the quantization controller <b>45</b> determines a macroblock of a 3D object positioned in front with a parallax vector greater than the feedback information to be a protected area.</p>
<p id="p-0056" num="0063">In step ST<b>5</b>, the image processing apparatus <b>20</b><i>dv </i>determines a quantization step size. The quantizer <b>25</b> of the image processing apparatus <b>20</b><i>dv </i>determines a quantization step size on the basis of the protected area determination results in step ST<b>4</b>, and proceeds to a step ST<b>6</b>. The quantizer <b>25</b> modifies the quantization step size on the basis of the protected area determination results, reducing the quantization step size for image portions of 3D objects positioned in front, for example.</p>
<p id="p-0057" num="0064">In step ST<b>6</b>, the image processing apparatus <b>20</b><i>dv </i>conducts a coding process. With the prediction image/optimal mode selector <b>43</b>, the image processing apparatus <b>20</b><i>dv </i>compares a cost function value supplied from the intra predictor <b>41</b> to a cost function value supplied from the motion/parallax prediction compensator <b>42</b>. The image processing apparatus <b>20</b><i>dv </i>, on the basis of the comparison results, selects the prediction mode with the lesser cost function value as the optimal mode with the best coding efficiency. The image processing apparatus <b>20</b><i>dv </i>also conducts a coding process using prediction image data generated in the optimal mode. Additionally, the image processing apparatus <b>20</b><i>dv </i>conducts the coding process using the quantization step size determined in step ST<b>5</b>.</p>
<p id="p-0058" num="0065">The processing from step ST<b>2</b> to step ST<b>6</b> is conducted in this way on each macroblock in a picture, and when processing is complete for a macroblock the process proceeds to a step ST<b>7</b>.</p>
<p id="p-0059" num="0066">In step ST<b>7</b>, the image processing apparatus <b>20</b><i>dv </i>determines whether or not the processing from step ST<b>2</b> to step ST<b>6</b> has finished for all macroblocks in the coding target picture. When an unprocessed macroblock remains, the image processing apparatus <b>20</b><i>dv </i>returns to step ST<b>2</b> and conducts the processing from step ST<b>2</b> to step ST<b>6</b> on a new unprocessed macroblock. Meanwhile, the image processing apparatus <b>20</b><i>dv </i>proceeds to a step ST<b>8</b> when processing has finished for all macroblocks.</p>
<p id="p-0060" num="0067">In step ST<b>8</b>, the image processing apparatus <b>20</b><i>dv </i>generates feedback information. The quantization controller <b>45</b> of the image processing apparatus <b>20</b><i>dv </i>generates feedback information from parallax vectors for a single picture that are obtained by conducting the processing from step ST<b>2</b> to step ST<b>6</b> on each macroblock in a picture. The feedback information is used in the coding process for the next picture. The quantization controller <b>45</b> takes statistics used as a basis for determining a protected area as the feedback information. For example, the quantization controller <b>45</b> may take an average value of parallax vectors in a single picture as feedback information. Also, the quantization controller <b>45</b> may add a correction value depending on a parallax vector distribution to the average value, and take that as feedback information. However, parallax vector statistics are not limited to average values, and median values, etc. may also be used.</p>
<p id="p-0061" num="0068">In step ST<b>9</b>, the image processing apparatus <b>20</b><i>dv </i>conducts a coding process similar to the related art in the case where the coding target picture is a base view picture. In other words, a coding process is conducted without conducting parallax prediction.</p>
<p id="p-0062" num="0069">According to the first operation, in a coding process for a dependent view, an image of a 3D object positioned in front, for example, is taken to be a protected area on the basis of parallax vectors obtained by parallax prediction. Consequently, image quality is protected for a 3D object positioned in front that readily draws attention, and the subjective image quality of a multi-view image can be improved.</p>
<p id="p-0063" num="0070">&#x3c;3-2. Second Operation of Image Processing Apparatus&#x3e;</p>
<p id="p-0064" num="0071">In a second operation of an image processing apparatus, the case of determining a protected area using a parallax vector of a coding target block and a parallax vector detected in the immediately preceding already-coded block will be described. Herein, in the second operation, similarly to the first operation, the horizontal components of parallax vectors are used to determine a protected area in the case of generating base view and dependent view image data by differentiating the positions of the left view and the right view in the horizontal direction only.</p>
<p id="p-0065" num="0072"><figref idref="DRAWINGS">FIG. 5</figref> is a flowchart illustrating a second operation of an image processing apparatus. In a step ST<b>11</b>, an image processing apparatus <b>20</b><i>dv </i>determines whether or not the coding target picture is a dependent view picture. The image processing apparatus <b>20</b><i>dv </i>proceeds to a step ST<b>12</b> in the case where the coding target picture is a dependent view picture, and proceeds to a step ST<b>19</b> in the case of a base view picture.</p>
<p id="p-0066" num="0073">In step ST<b>12</b>, the image processing apparatus <b>20</b><i>dv </i>conducts parallax prediction on a coding target macroblock. The motion/parallax prediction compensator <b>42</b> of the image processing apparatus <b>20</b><i>dv </i>conducts parallax prediction using image data of the coding target macroblock and base view image data, detects a parallax vector, and proceeds to a step ST<b>13</b>.</p>
<p id="p-0067" num="0074">In step ST<b>13</b>, the image processing apparatus <b>20</b><i>dv </i>acquires a parallax vector. The quantization controller <b>45</b> of the image processing apparatus <b>20</b><i>dv </i>acquires a parallax vector detected by the motion/parallax prediction compensator <b>42</b> and proceeds to a step ST<b>14</b>.</p>
<p id="p-0068" num="0075">In step ST<b>14</b>, the image processing apparatus <b>20</b><i>dv </i>determines a protected area. The quantization controller <b>45</b> of the image processing apparatus <b>20</b><i>dv </i>compares feedback information generated in a step ST<b>17</b> later described to the parallax vector, determines a protected area, and proceeds to a step ST<b>15</b>. In the case where, for example, a parallax vector of an immediately preceding already-coded macroblock is taken as feedback information as discussed later, the quantization controller <b>45</b> determines a boundary of a 3D object with a parallax vector greater than the feedback information to be a protected area.</p>
<p id="p-0069" num="0076">In step ST<b>15</b>, the image processing apparatus <b>20</b><i>dv </i>determines a quantization step size. The quantizer <b>25</b> of the image processing apparatus <b>20</b><i>dv </i>determines a quantization step size on the basis of the protected area determination results in step ST<b>14</b>, and proceeds to a step ST<b>16</b>. The quantizer <b>25</b> modifies the quantization step size on the basis of the protected area determination results, reducing the quantization step size for a boundary portion of a 3D object, for example.</p>
<p id="p-0070" num="0077">In step ST<b>16</b>, the image processing apparatus <b>20</b><i>dv </i>conducts a coding process. With the prediction image/optimal mode selector <b>43</b>, the image processing apparatus <b>20</b><i>dv </i>compares a cost function value supplied from the intra predictor <b>41</b> to a cost function value supplied from the motion/parallax prediction compensator <b>42</b>. The image processing apparatus <b>20</b><i>dv </i>, on the basis of the comparison results, selects the prediction mode with the lesser cost function value as the optimal mode with the best coding efficiency. The image processing apparatus <b>20</b><i>dv </i>also conducts a coding process using prediction image data generated in the optimal mode. Additionally, the image processing apparatus <b>20</b><i>dv </i>conducts the coding process using the quantization step size determined in step ST<b>15</b>, and proceeds to a step ST<b>17</b>.</p>
<p id="p-0071" num="0078">In step ST<b>17</b>, the image processing apparatus <b>20</b><i>dv </i>generates feedback information. The quantization controller <b>45</b> of the image processing apparatus <b>20</b><i>dv </i>takes the parallax vector obtained by conducting the processing from step ST<b>12</b> to step ST<b>16</b> on a macroblock to be feedback information used in the coding process of the next macroblock, and proceeds to a step ST<b>18</b>.</p>
<p id="p-0072" num="0079">In step ST<b>18</b>, the image processing apparatus <b>20</b><i>dv </i>determines whether or not the processing from step ST<b>12</b> to step ST<b>16</b> has finished for all macroblocks in the coding target picture. When an unprocessed macroblock remains, the image processing apparatus <b>20</b><i>dv </i>returns to step ST<b>12</b> and conducts the processing from step ST<b>12</b> to step ST<b>16</b> on a new unprocessed macroblock. Meanwhile, the image processing apparatus <b>20</b><i>dv </i>deletes the feedback information and codes the next coding target picture when processing has finished for all macroblocks.</p>
<p id="p-0073" num="0080">In step ST<b>19</b>, the image processing apparatus <b>20</b><i>dv </i>conducts a coding process similar to the related art in the case where the coding target picture is a base view picture. In other words, a coding process is conducted without conducting parallax prediction.</p>
<p id="p-0074" num="0081">According to the second operation, in a coding process for a dependent view, a boundary of a 3D object is taken to be a protected area on the basis of the parallax vector of an immediately preceding coding target macroblock. Consequently, image quality is protected for a boundary portion of a 3D object, and the subjective image quality of a multi-view image can be improved.</p>
<p id="p-0075" num="0082">&#x3c;3-3. Third Operation of Image Processing Apparatus&#x3e;</p>
<p id="p-0076" num="0083">In a third operation of an image processing apparatus, the case of determining a protected area using error from when compensating on the basis of the parallax vector of a coding target macroblock and error computed for an already-coded picture will be described.</p>
<p id="p-0077" num="0084"><figref idref="DRAWINGS">FIG. 6</figref> is a flowchart illustrating a third operation of an image processing apparatus. In a step ST<b>21</b>, an image processing apparatus <b>20</b><i>dv </i>determines whether or not the coding target picture is a dependent view picture. The image processing apparatus <b>20</b><i>dv </i>proceeds to a step ST<b>22</b> in the case where the coding target picture is a dependent view picture, and proceeds to a step ST<b>29</b> in the case of a base view picture.</p>
<p id="p-0078" num="0085">In step ST<b>22</b>, the image processing apparatus <b>20</b><i>dv </i>conducts parallax prediction on a coding target macroblock. The motion/parallax prediction compensator <b>42</b> of the image processing apparatus <b>20</b><i>dv </i>conducts parallax prediction using image data of the coding target macroblock and base view image data and detects a parallax vector.</p>
<p id="p-0079" num="0086">Additionally, the motion/parallax prediction compensator <b>42</b> conducts reference picture compensation using the detected parallax vector, computes the error (SAD: Sum of Absolute Differences, for example) with the coding target macroblock, and proceeds to a step ST<b>23</b>.</p>
<p id="p-0080" num="0087">In step ST<b>23</b>, the image processing apparatus <b>20</b><i>dv </i>acquires error. The quantization controller <b>45</b> of the image processing apparatus <b>20</b><i>dv </i>acquires error computed by the motion/parallax prediction compensator <b>42</b> and proceeds to a step ST<b>24</b>.</p>
<p id="p-0081" num="0088">In step ST<b>24</b>, the image processing apparatus <b>20</b><i>dv </i>determines a protected area. The quantization controller <b>45</b> of the image processing apparatus <b>20</b><i>dv </i>compares feedback information generated in a step ST<b>28</b> later described to the error, determines a protected area, and proceeds to a step ST<b>25</b>. At this point, a macroblock with a large error has a low parallax prediction efficiency, and there is a high probability that the area is an occlusion. An occlusion area is an area which exists in the coding target (reference) picture, but which does not exist in the reference (coding target) area. An area where an occlusion due to parallax occurs may be a boundary area where the parallax changes greatly, such as a boundary area between a 3D object projecting out in front and the background, for example. Consequently, in the case where, for example, the average value of the error in a single picture is taken as feedback information as discussed later, the quantization controller <b>45</b> determines a macroblock with an error greater than the feedback information to be a protected area. Herein, an occlusion area refers to an area which is visible in either the base view or the dependent view, but which is hidden and not visible in the other view, for example.</p>
<p id="p-0082" num="0089">In step ST<b>25</b>, the image processing apparatus <b>20</b><i>dv </i>determines a quantization step size. The quantizer <b>25</b> of the image processing apparatus <b>20</b><i>dv </i>determines a quantization step size on the basis of the protected area determination results in step ST<b>24</b>, and proceeds to a step ST<b>26</b>. The quantizer <b>25</b> modifies the quantization step size on the basis of the protected area determination results, reducing the quantization step size for a boundary portion of a 3D object, for example.</p>
<p id="p-0083" num="0090">In step ST<b>26</b>, the image processing apparatus <b>20</b><i>dv </i>conducts a coding process. With the prediction image/optimal mode selector <b>43</b>, the image processing apparatus <b>20</b><i>dv </i>compares a cost function value supplied from the intra predictor <b>41</b> to a cost function value supplied from the motion/parallax prediction compensator <b>42</b>. The image processing apparatus <b>20</b><i>dv </i>, on the basis of the comparison results, selects the prediction mode with the lesser cost function value as the optimal mode with the best coding efficiency. The image processing apparatus <b>20</b><i>dv </i>also conducts a coding process using prediction image data generated in the optimal mode. Additionally, the image processing apparatus <b>20</b><i>dv </i>conducts the coding process using the quantization step size determined in step ST<b>25</b>.</p>
<p id="p-0084" num="0091">The processing from step ST<b>22</b> to step ST<b>26</b> is conducted in this way on each macroblock in a picture, and when processing is complete for a macroblock the process proceeds to a step ST<b>27</b>.</p>
<p id="p-0085" num="0092">In step ST<b>27</b>, the image processing apparatus <b>20</b><i>dv </i>determines whether or not the processing from step ST<b>22</b> to step ST<b>26</b> has finished for all macroblocks in the coding target picture. When an unprocessed macroblock remains, the image processing apparatus <b>20</b><i>dv </i>returns to step ST<b>22</b> and conducts the processing from step ST<b>22</b> to step ST<b>26</b> on a new unprocessed macroblock. Meanwhile, the image processing apparatus <b>20</b><i>dv </i>proceeds to a step ST<b>28</b> when processing has finished for all macroblocks.</p>
<p id="p-0086" num="0093">In step ST<b>28</b>, the image processing apparatus <b>20</b><i>dv </i>generates feedback information. The quantization controller <b>45</b> of the image processing apparatus <b>20</b><i>dv </i>generates feedback information from error for a single picture that is obtained by conducting the processing from step ST<b>22</b> to step ST<b>26</b> on each macroblock in a picture. The feedback information is used in the coding process for the next picture. The quantization controller <b>45</b> computes statistics used as a basis for determining a protected area from the error of an already-coded picture, and takes them as feedback information. For example, the quantization controller <b>45</b> may take the average value of error in a single picture as feedback information. Also, the quantization controller <b>45</b> may add a correction value depending on the distribution of error in a single picture to the average value of error in a single picture, and take the result as feedback information.</p>
<p id="p-0087" num="0094">In step ST<b>29</b>, the image processing apparatus <b>20</b><i>dv </i>conducts a coding process similar to the related art in the case where the coding target picture is a base view picture. In other words, a coding process is conducted without conducting parallax prediction.</p>
<p id="p-0088" num="0095">According to the third operation, in a coding process for a dependent view, an boundary of a 3D object positioned is taken to be a protected area on the basis of error obtained by parallax prediction. Consequently, image quality is protected for a boundary portion of a 3D object, and the subjective image quality of a multi-view image can be improved.</p>
<p id="p-0089" num="0096">&#x3c;3-4. Other Operation of Image Processing Apparatus&#x3e;</p>
<p id="p-0090" num="0097">An image processing apparatus may also conduct the above-described first through third operations in combination, rather than the case of conducting first through third operations individually. In the first operation described above, the image quality of a 3D object positioned in front can be protected. Also, in the second operation or the third operation, the image quality of a boundary area of a 3D object can be protected. Consequently, by conducting the first operation and the second operation or the first operation and the third operation in combination, the image quality of an image of a 3D object and the image quality of an image of its boundary area are protected, and the subjective image quality of a multi-view image can be improved.</p>
<p id="p-0091" num="0098">Also, in the case of conducting the second operation, if for example the threshold value compared against a parallax vector is smaller than an optimal level, then over-detection of a protected area may occur, and an area differing from a boundary area of a 3D object may also be determined as a protected area. At this point, if the third operation is combined with the second operation and a protected area is determined by also using error rather than just a parallax vector, then occlusion areas are also determined on the basis of error. Consequently, if an area having a parallax vector and error that are greater than their threshold values is determined as a protected area, a boundary of a 3D object can be more precisely determined as a protected area compared to the case of determining a protected area with either the second operation or the third operation.</p>
<p id="p-0092" num="0099">Also, in the above-described operation, an area whose parallax vector and error are greater than their threshold values is taken to be a protected area, but it is also possible to take an area whose parallax vector and error are less than their threshold values to be a protected area. For example, consider the case where an object different from the object of interest is positioned in front and the object of interest is set farther back. In this case, it is possible to also protect the image quality of the object of interest by taking an area whose parallax vector and error are less than their threshold values to be a protected area.</p>
<p id="p-0093" num="0100">It may also be configured such image quality protection is additionally conducted on base view images rather than just dependent view images. In this case, an image processing apparatus that codes base view image data generates prediction image data by temporal prediction, determines a protected area on the basis of information obtained by parallax prediction, and conducts adaptive quantization according to the determination results. In so doing, the image quality of an object of interest is protected in base view and dependent view images, and the subjective image quality of a multi-view image can be improved.</p>
<p id="p-0094" num="0101">&#x3c;4. Configuration in the Case of Conducting Image Coding by Software Processing&#x3e;</p>
<p id="p-0095" num="0102">Additionally, an image processing apparatus may also be a computer that executes the above-described series of processes according to a program.</p>
<p id="p-0096" num="0103"><figref idref="DRAWINGS">FIG. 7</figref> illustrates an exemplary configuration of a computer that executes the above-described series of processes according to a program. The central processing unit (CPU) <b>61</b> of a computer <b>60</b> executes various processes in accordance with a computer program recorded in read-only memory (ROM) <b>62</b> or a recording unit <b>68</b>.</p>
<p id="p-0097" num="0104">Computer programs executed by the CPU <b>61</b> and data, etc. are stored as appropriate in random access memory (RAM) <b>63</b>. The CPU <b>61</b>, ROM <b>62</b>, and RAM <b>63</b> are coupled to each other by a bus <b>64</b>.</p>
<p id="p-0098" num="0105">An input/output interface <b>65</b> is also coupled to the CPU <b>61</b> via the bus <b>64</b>. Coupled to the input/output interface <b>65</b> is an input unit <b>66</b> such as a touch panel, keyboard, mouse, microphone, etc. and an output unit <b>67</b> such as a display, etc. The CPU <b>61</b> executes various processes corresponding to commands input from the input unit <b>66</b>, and outputs processing results to the output unit <b>67</b>.</p>
<p id="p-0099" num="0106">The recording unit <b>68</b> coupled to the input/output interface <b>65</b> may be realized by a hard disk or solid-state drive (SSD), for example, and records computer programs executed by the CPU <b>61</b> and various data. A communication unit <b>69</b> communicates with external apparatus via a wired or wireless communication medium such as the Internet, a local area network, or other network, or a digital broadcast. The computer apparatus <b>60</b> may also acquire computer programs via the communication unit <b>69</b> and record the programs in the ROM <b>62</b> or the recording unit <b>68</b>.</p>
<p id="p-0100" num="0107">When a magnetic disk, an optical disc, a magneto-optical disc, semiconductor memory, or other removable medium <b>72</b> is loaded into a drive <b>70</b>, the drive <b>70</b> drives the medium and acquires computer programs, data, etc. recorded thereon. Acquired computer programs and data are transferred to the ROM <b>62</b>, the RAM <b>63</b>, or the recording unit <b>68</b> as appropriate.</p>
<p id="p-0101" num="0108">The CPU <b>61</b> reads out and executes a computer program that conducts the above-described series of processes, conducting a coding process on image data of a multi-view image recorded in the recording unit <b>68</b> or the removable medium <b>72</b>, or on image data of a multi-view image supplied via the communication unit <b>69</b>.</p>
<p id="p-0102" num="0109">However, the disclosed technology is not to be interpreted as being limited to the above-described embodiment. For example, a multi-view image is not limited to three images, and may also be a two-view image. This embodiment of the disclosed technology discloses the disclosed technology by way of example, and obviously persons skilled in the art may make modifications or substitutions to the embodiment without departing from the principal matter of the disclosed technology. In other words, the principal matter of the disclosed technology should be judged upon consultation of the claims.</p>
<p id="p-0103" num="0110">The present disclosure contains subject matter related to that disclosed in Japanese Priority Patent Application JP 2010-161658 filed in the Japan Patent Office on Jul. 16, 2010, the entire contents of which are hereby incorporated by reference.</p>
<p id="p-0104" num="0111">It should be understood by those skilled in the art that various modifications, combinations, sub-combinations and alterations may occur depending on design requirements and other factors insofar as they are within the scope of the appended claims or the equivalents thereof.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. An image processing apparatus comprising:
<claim-text>a quantizer configured to quantize orthogonal transform coefficients of a difference in block units between a coding target picture and a prediction image;</claim-text>
<claim-text>a predictor configured to conduct parallax prediction utilizing correlations between the coding target picture and a picture whose view differs from the coding target picture; and</claim-text>
<claim-text>a quantization controller configured to determine a protected area that protects image quality from the parallax prediction results, and reduce a quantization step size of the quantizer for quantization of the protected area;</claim-text>
<claim-text>wherein:
<claim-text>the quantization controller is configured to determine the protected area by using at least one from among a parallax vector detected by the parallax prediction and error from when parallax compensation was conducted using the parallax vector; and</claim-text>
<claim-text>in a case of using a parallax vector detected by the parallax prediction, the quantization controller is configured to determine if a coding target block image is the protected area at least in part on the basis of at least one from among comparison results between a parallax vector of an already-coded picture and the coding target block and comparison results between a parallax vector of an immediately preceding already-coded block and a parallax vector of the coding target block.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The image processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the quantization controller is configured to determine a coding target block whose parallax vector is greater than statistics used as a basis for determining a protected area which are computed from the parallax vector of the already-coded picture, and a coding target block whose parallax vector is greater than the parallax vector of an immediately preceding already-coded block, to be the protected area.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The image processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein
<claim-text>the quantization controller is configured to determine the protected area using horizontal components of a parallax vector detected by the parallax prediction.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The image processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein in a case of using error from when the parallax compensation was conducted, the quantization controller is configured to determine if a coding target block image is the protected area at least in part on the basis of comparison results between error of an already-coded picture and parallax of the coding target block.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The image processing apparatus according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein
<claim-text>the quantization controller is configured to determine a coding target block whose error is greater than statistics used as a basis for determining a protected area which are computed from the error of the already-coded picture to be the protected area.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. An image processing method comprising acts of:
<claim-text>quantizing, by at least one computer processor, orthogonal transform coefficients of a difference in block units between a coding target picture and a prediction image;</claim-text>
<claim-text>conducting parallax prediction, by the at least one computer processor, utilizing correlations between the coding target picture and a picture whose view differs from the coding target picture; and</claim-text>
<claim-text>determining, by the at least one computer processor, a protected area that protects image quality from the parallax prediction results, and reducing a quantization step size for quantization of the protected area;</claim-text>
<claim-text>wherein:
<claim-text>the act of determining the protected area comprises the at least one computer processor using at least one from among a parallax vector detected by the parallax prediction and error from when parallax compensation was conducted using the parallax vector; and</claim-text>
<claim-text>in a case of the at least one computer processor using a parallax vector detected by the parallax prediction to determine the protected area, a determination whether a coding target block image is the protected area is based at least in part on at least one from among comparison results between a parallax vector of an already-coded picture and the coding target block and comparison results between a parallax vector of an immediately preceding already-coded block and a parallax vector of the coding target block.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The image processing method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the act of determining comprises the at least one computer processor determining a coding target block whose parallax vector is greater than statistics used as a basis for determining a protected area which are computed from the parallax vector of the already-coded picture, and a coding target block whose parallax vector is greater than the parallax vector of an immediately preceding already-coded block, to be the protected area.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The image processing method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the act of determining the protected area comprises the at least one computer processor using horizontal components of a parallax vector detected by the parallax prediction.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The image processing method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein, in a case of the at least one computer processor using error from when the parallax compensation was conducted, the act of determining comprises the at least one computer processor determining whether a coding target block image is the protected area at least in part on the basis of comparison results between error of an already-coded picture and parallax of the coding target block.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The image processing method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the act of determining comprises the at least one computer processor determining a coding target block whose error is greater than statistics used as a basis for determining a protected area which are computed from the error of the already-coded picture to be the protected area. </claim-text>
</claim>
</claims>
</us-patent-grant>
