<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626989-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626989</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13019969</doc-number>
<date>20110202</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<us-term-of-grant>
<us-term-extension>358</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>12</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>12</main-group>
<subgroup>02</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>12</main-group>
<subgroup>06</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>11</class>
<subclass>C</subclass>
<main-group>16</main-group>
<subgroup>06</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>711103</main-classification>
<further-classification>711154</further-classification>
<further-classification>711201</further-classification>
<further-classification>711E12008</further-classification>
</classification-national>
<invention-title id="d2e53">Control arrangements and methods for accessing block oriented nonvolatile memory</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>6684289</doc-number>
<kind>B1</kind>
<name>Gonzalez et al.</name>
<date>20040100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>7032065</doc-number>
<kind>B2</kind>
<name>Gonzalez et al.</name>
<date>20060400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>7171513</doc-number>
<kind>B2</kind>
<name>Gonzalez et al.</name>
<date>20070100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>7716448</doc-number>
<kind>B2</kind>
<name>Schneider</name>
<date>20100500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>8364929</doc-number>
<kind>B2</kind>
<name>Haines et al.</name>
<date>20130100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711171</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2009/0276601</doc-number>
<kind>A1</kind>
<name>Kancherla</name>
<date>20091100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2010/0057978</doc-number>
<kind>A1</kind>
<name>Takamura et al.</name>
<date>20100300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2010/0174851</doc-number>
<kind>A1</kind>
<name>Leibowitz et al.</name>
<date>20100700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2011/0099321</doc-number>
<kind>A1</kind>
<name>Haines et al.</name>
<date>20110400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711103</main-classification></classification-national>
</us-citation>
<us-citation>
<nplcit num="00010">
<othercit>Mark LaPedus, Micron Clears Up NAND with Integrated Part, Dec. 2, 2010, EE Times News &#x26; Analysis.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00011">
<othercit>The International Search Report and the Written Opinion of the International Searching Authority for International Application No. PCT/US2012/021560 which is associated with U.S. Appl. No. 13/019,969, Jul. 30, 2012, Republic of Korea.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>31</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>None</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>15</number-of-drawing-sheets>
<number-of-figures>18</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120198128</doc-number>
<kind>A1</kind>
<date>20120802</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Van Aken</last-name>
<first-name>Stephen P.</first-name>
<address>
<city>Boulder</city>
<state>CO</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Van Aken</last-name>
<first-name>Stephen P.</first-name>
<address>
<city>Boulder</city>
<state>CO</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Pritzkou Patent Group, LLC</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Micron Technology, Inc.</orgname>
<role>02</role>
<address>
<city>Boise</city>
<state>ID</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Verbrugge</last-name>
<first-name>Kevin</first-name>
<department>2182</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A read/write arrangement is described for use in accessing at least one nonvolatile memory device in read/write operations with the memory device being made up of a plurality of memory cells which memory cells are organized as a set of pages that are physically and sequentially addressable with each page having a page length such that a page boundary is defined between successive ones of the pages in the set. The read/write arrangement includes a control arrangement that is configured to store and access a group of data blocks that is associated with a given write operation in a successive series of pages of the memory such that at least an initial page in the series is filled and each block includes a block length that is different than the page length.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="74.93mm" wi="158.75mm" file="US08626989-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="194.82mm" wi="176.87mm" file="US08626989-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="196.09mm" wi="160.19mm" orientation="landscape" file="US08626989-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="244.77mm" wi="178.82mm" orientation="landscape" file="US08626989-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="235.20mm" wi="178.82mm" file="US08626989-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="223.01mm" wi="180.09mm" orientation="landscape" file="US08626989-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="199.90mm" wi="183.30mm" orientation="landscape" file="US08626989-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="224.28mm" wi="185.17mm" file="US08626989-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="235.80mm" wi="185.17mm" file="US08626989-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="246.04mm" wi="173.06mm" file="US08626989-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="191.01mm" wi="97.37mm" file="US08626989-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="196.09mm" wi="113.45mm" file="US08626989-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="244.18mm" wi="114.05mm" file="US08626989-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="158.75mm" wi="107.36mm" orientation="landscape" file="US08626989-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="210.23mm" wi="139.70mm" file="US08626989-20140107-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="218.52mm" wi="116.67mm" file="US08626989-20140107-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<p id="p-0002" num="0001">The present invention is related generally to the field of memory systems using non-volatile memory, such as solid-state drives (&#x201c;SSDs&#x201d;) and other storage devices, and, more particularly, to control arrangements and methods for accessing block oriented nonvolatile memory.</p>
<p id="p-0003" num="0002">Prior art solid state drives typically include a controller that can be interfaced with a standard host device as well as a plurality of flash memory devices. The flash memory devices are generally each in the form of a memory die. The connection between the controller and the flash memory devices is typically in the form of a flash-oriented interface with each memory device being individually connected to the flash-oriented interface. The controller may have a plurality of flash-oriented interfaces with each of these interfaces being connected to a plurality of memory devices. Applicants recognize that there are a number of disadvantages with respect to this prior art configuration, particularly with respect to larger capacity solid-state drives. For example, the controller must control a large number of individual memory devices. As another example, the number of signal connection points that are required on the controller increase in proportion to the capacity of the drive. In still another example, the controller is responsible for management functions (e.g., overhead functions, such as wear leveling) such that the interface between the controller and the memory devices is burdened by traffic relating to these functions. As yet another example, testing and monitoring of large numbers of memory devices by the controller is a complex task.</p>
<p id="p-0004" num="0003">Applicants also recognize a related concern with respect to the use of non-volatile memory in the prior art with regard to block-oriented non-volatile memory. It should be appreciated that the prevailing approach of the prior art with respect to access of block-oriented non-volatile memory is to allocate user data plus accompanying extra information exactly onto a physical page. For purposes of the present discussions, the term &#x201c;block-oriented&#x201d; relates to the use of a page based memory that is employed by using user data blocks that are generally of a length that is equal to the page length. As will be further discussed, with block-oriented memory, some pages may be partially unused based on a user data block that is shorter than one page length, but user data blocks are not longer than the page length. By way of non-limiting example, NAND flash is typically configured so as to be of this type of block-oriented memory. This type of inflexible allocation in block-oriented memory thus produces an exact correspondence between the stored information and the number of storage cells comprising each physical page and is customary within the industry for both Single Bit per Cell memory (SBC) and Multi-Bit per Cell memory (MBC).</p>
<p id="p-0005" num="0004">Applicants further recognize that such an approach is inflexible in matching user data to page-units in a particular memory device. The physical page size is a design parameter of the memory and cannot be changed. A penalty will be incurred in the event of a mismatch between the number of storage cells in a physical page and the number of cells required per block by an application. The penalty can be in the form of insufficient cells to meet ECC requirements, thus leading to poor reliability of the application; or it can be in the form of storage inefficiency if there are a more than the needed number of cells in each physical page. Thus, designers of non-volatile memory devices, e.g., NAND flash, are challenged with attempting to predict the needed page size before the memory is even produced. Almost inevitably, a given page size will be not be well-suited to at least some subset of the applications to which the use of the memory is directed.</p>
<p id="p-0006" num="0005">The foregoing examples of the related art and limitations related therewith are intended to be illustrative and not exclusive. Other limitations of the related art will become apparent to those of skill in the art upon a reading of the specification and a study of the drawings.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0001" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0007" num="0006">Exemplary embodiments are illustrated in referenced figures of the drawings. It is intended that the embodiments and figures disclosed herein are to be illustrative rather than limiting.</p>
<p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram which illustrates an SSD that is produced according to an embodiment of the present disclosure, shown here as part of an overall system.</p>
<p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram which illustrates further details of a SSD according to an embodiment of the present disclosure.</p>
<p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. 3</figref> is a block diagram illustrating an embodiment of a module that is used in a SSD, such as the SSD of <figref idref="DRAWINGS">FIGS. 1</figref> and/or <b>2</b>.</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 4</figref> is a block diagram which illustrates an embodiment of a module interface for use with a module, such as the module of <figref idref="DRAWINGS">FIG. 3</figref>.</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 5</figref> is a block diagram which diagrammatically illustrates the memory devices of a module in conjunction with an example arrangement of memory sections across those memory devices.</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 6</figref> is a block diagram of an embodiment of a function engine that can be used in a module, such as the module of <figref idref="DRAWINGS">FIG. 3</figref>.</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 7</figref> is a flow diagram illustrating an embodiment of a wear leveling function that can be performed using the function engine of <figref idref="DRAWINGS">FIG. 6</figref>.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 8</figref> is a flow diagram illustrating an embodiment of a garbage collection function that can be performed using the function engine of <figref idref="DRAWINGS">FIG. 6</figref>.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 9</figref> is a flow diagram illustrating an embodiment of a bit density configuration function that can be performed using the function engine of <figref idref="DRAWINGS">FIG. 6</figref>.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 10</figref> is a flow diagram which illustrates an embodiment of an FTL (Flash Translation Layer) function that can be performed using the function engine of <figref idref="DRAWINGS">FIG. 6</figref>.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 11</figref> is a flow diagram which illustrates an embodiment of a read operation that can be performed in cooperation with the function engine of <figref idref="DRAWINGS">FIG. 6</figref>.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 12</figref> is a flow diagram which illustrates an embodiment of a write operation that can be performed in cooperation with the function engine of <figref idref="DRAWINGS">FIG. 6</figref>.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 13</figref> is a flow diagram which illustrates an embodiment of an erase function that can be performed in cooperation with the function engine of <figref idref="DRAWINGS">FIG. 6</figref>.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 14</figref> is a diagrammatic illustration of user data blocks of differing length in relation to the length of a physical page.</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 15</figref> is a block diagram which diagrammatically illustrates a set of successive physical pages storing a set of user data blocks each of which includes a length that is less than the length of one physical page.</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 16</figref> is a block diagram which diagrammatically illustrates a set of successive physical pages storing a set of user data blocks each of which includes a length that is greater than the length of one physical page.</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 17</figref> is a flow diagram which illustrates an embodiment of a method for storing/writing user data blocks in a way which achieves the data structures described in the context of <figref idref="DRAWINGS">FIGS. 15 and 16</figref> and in which the block length and page length can be different.</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 18</figref> is another flow diagram which illustrates an embodiment of a method for reading user data blocks from the data structures described above in the context of <figref idref="DRAWINGS">FIGS. 15 and 16</figref> and in which the block length and page length can be different.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0002" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0026" num="0025">The following description is presented to enable one of ordinary skill in the art to make and use the invention and is provided in the context of a patent application and its requirements. Various modifications to the described embodiments will be readily apparent to those skilled in the art and the generic principles taught herein may be applied to other embodiments. Thus, the present invention is not intended to be limited to the embodiments shown, but is to be accorded the widest scope consistent with the principles and features described herein including modifications and equivalents, as defined within the scope of the appended claims. It is noted that the drawings are not to scale and are diagrammatic in nature in a way that is thought to best illustrate features of interest. Descriptive terminology may be adopted for purposes of enhancing the reader's understanding, with respect to the various views provided in the figures, and is in no way intended as being limiting.</p>
<p id="p-0027" num="0026">Attention is now directed to the figures wherein like items may refer to like components throughout the various views. <figref idref="DRAWINGS">FIG. 1</figref> is a block diagram of a solid state drive (SSD) <b>10</b>, produced according to an embodiment of the present disclosure, forming part of an overall system <b>12</b> wherein the solid state drive is interfaced with a host <b>20</b>. In the example embodiment of <figref idref="DRAWINGS">FIG. 2</figref>, storage capacity of the SSD <b>10</b> is provided by a total of 20 modules some of which are designated by the reference number <b>30</b>. Any suitable number of modules can be used while remaining within the scope of the teachings herein. Each module includes a non-volatile memory (&#x201c;NVM&#x201d;) section (e.g., a number of NVM devices, such as a number of NVM dies) which can be flash memory, but any suitable memory technology can be used such as, for example, phase change memory (&#x201c;PCM&#x201d;) and other forms of variable resistance memories. It should be appreciated that the module arrangement of <figref idref="DRAWINGS">FIG. 1</figref> supports the use of &#x201c;spare&#x201d; modules, such as to ensure against loss of use of some portion of the total number of modules. If the total capacity of the SSD is targeted at one terabyte (1 TB), it can be seen that sixteen modules, each of which has a capacity of 64 gigabytes (64 GB) satisfies this target. Thus, the total of 20 modules can provide an excess of 5 modules which can be treated as spares. Including a number of NVM devices in a single module can reduce the number of individual memory devices directly connected to the controller during read and write operations by a factor of the number of NVM die in the memory module.</p>
<p id="p-0028" num="0027">As will be seen, communication with a module is provided via a module interface. The module interface can be a point-to-point interface and an independent instance of the module interface can be provided for each module in the system. This can be practical because of a low module pin count. The low pin count can enable many module interface ports to be provided within conventional pad-ring constraints of SSD controller in ASIC form, and each port provides an interface to one module. The point-to-point interconnection of modules and SSD controller ports can enable dedicated allocation of the resources of a particular module interface to transactions between the SSD controller and that particular module. When the interconnection established by the module interface between the main controller and the respective module is dedicated, transactions between the main controller and the module do not have to share the resources of the interface with other modules; these transactions can therefore be executed at the full bandwidth of each module interface. Also, when the connection is dedicated to one module, arbitration and resource sharing, that would otherwise be necessary if the interface were connected to more than one module, can be avoided. This can, in turn, improve system performance by allowing for increased performance in terms of both numbers of transactions and amount of data transferred over the interface.</p>
<p id="p-0029" num="0028">Still referring to <figref idref="DRAWINGS">FIG. 1</figref>, a controller <b>40</b> is interfaced with each one of modules <b>30</b> via a module interface <b>42</b> (several of which are indicated using reference numbers) for each module. The controller is, in turn, interfaced to host <b>20</b> via a suitable high-level host interface <b>44</b> which can be any of various interfaces for attaching a memory system to a computer, for instance. By way of example, host interface <b>44</b> may be a SATA (Serial ATA) interface for some systems <b>12</b>. Other systems <b>12</b> may employ the use of a SAS (Serial Attached SCSI) interface or the interface may be back plane oriented such as, for example, using PCI Express. In this regard, a suitable form of host interface can be selected in view of a particular end use.</p>
<p id="p-0030" num="0029">A cache <b>46</b> can be included, such as to provide a performance enhancement to the overall SSD system <b>10</b>. A cache may be realized by attaching any suitable memory device such as a SDRAM, PCM, fast flash (for example, SBC), and the like. The SSD controller <b>40</b> can have a dedicated interface for communication with one or more cache devices. Depending on the nature of the cache technology selected for a particular SSD design, this interface may be dedicated to the cache. If the cache device <b>46</b> is a module, then the cache interface can be the same as or similar to a NVM module interface.</p>
<p id="p-0031" num="0030">Controller <b>40</b>, when compared with controllers for prior art SSDs, can be viewed as simplified. Because the modules in system <b>10</b> can be charged with detailed management functions yet to be described below, the SSD controller can be relieved of the burden of many functions that are typically performed by prior art controllers. Non-limiting examples of functions that are offloaded can at least include (1) wear leveling, (2) garbage collection, (3) logical to physical address translation, (4) physical page to logical block abstraction, (5) ECC functions, and (6) memory conditioning and signal processing. Because many of the functions can be reduced or removed which would require heavy CPU and other supporting activity in a prior art SSD controller, such as those listed above, the bandwidth of the SSD controller can be relatively more dedicated to transferring user data between the host and the connected modules.</p>
<p id="p-0032" num="0031">Turning to <figref idref="DRAWINGS">FIG. 2</figref> in conjunction with <figref idref="DRAWINGS">FIG. 1</figref>, the former is a block diagram which illustrates an embodiment of controller <b>40</b>. The controller includes a plurality of controller module ports <b>100</b>, each of which provides a module interface <b>42</b> to communicate with a plurality of storage modules <b>30</b> (<figref idref="DRAWINGS">FIG. 1</figref>). Each controller module port <b>100</b> provides one set of associated interface signal connections that are the same from one controller module port to the next. By way of non-limiting example, controller <b>40</b> can communicate with twenty modules <b>30</b> via twenty individual module interfaces <b>42</b>, as shown in <figref idref="DRAWINGS">FIG. 1</figref>. The number of controller module ports in an SSD controller can vary. The particular number of controller module ports implemented in a given SSD controller can depend on design considerations involving the SSD controller itself, as well as on the SSD system. These design considerations can involve the desired storage capacity of the SSD, the desired performance, and the desired cost. The benefits of the controller module interface are provided regardless of the number of controller module ports implemented in the SSD controller of a particular embodiment.</p>
<p id="p-0033" num="0032">Continuing with the description of main controller <b>40</b>, a data path and control logic section <b>120</b> is configured to perform data transfers between each active controller module port <b>100</b> and the remainder of the SSD controller using bidirectional data paths, such as bus connections <b>122</b>, several of which are indicated by reference numbers. A control bus <b>124</b> can provide a data path between a microprocessor <b>130</b>, operating using code stored in a memory <b>132</b>, and registers <b>134</b> contained in logic blocks making up the SSD controller. Each controller module port is connected to bus <b>124</b> to facilitate control by microprocessor <b>130</b>. Accordingly, microprocessor <b>130</b> can read from and write to registers <b>136</b> in each module port and, in doing so, can manage (e.g., set up, start, stop, and monitor) the operations of each controller module port. A data buffer section <b>140</b> can provide memory for staging blocks of data that flow through the SSD controller either as read data destined to be transferred to a host through a host interface port <b>144</b> or as write data destined to be transferred to a module through one of controller module ports <b>100</b>. A bidirectional data path <b>146</b> (several of which are indicated using reference numbers) can be provided between data path and control section <b>120</b> and data buffer section <b>140</b>. It should be appreciated that a one-to-one correspondence between the number of bidirectional data paths <b>146</b> and the number of controller module ports <b>100</b> is not required. In this regard, a sufficient number of discrete bidirectional data paths <b>146</b> can be present in order to ensure sufficient total data bandwidth. That is the total number of bidirectional data paths <b>146</b> can be less than the total number of controller module ports, equal to the total number of module ports, or greater than the total number of controller module ports.</p>
<p id="p-0034" num="0033">Memory <b>132</b> is accessible by microprocessor <b>130</b> in order to perform functions which manage the SSD controller, such as managing errors and exceptions as might occur during normal operation of the SSD controller, including those occurring because of errors in data received from one or more of the modules, because of exceptional conditions in the SSD controller (such as resource conflicts, buffer memory errors, and the like), and because of unanticipated problems occurring during the course of otherwise normal operations.</p>
<p id="p-0035" num="0034">A high speed data path, such as bus <b>150</b>, can be used to transfer data between buffering section <b>140</b> and host interface port <b>144</b>. This bus may be of any suitable design that supports the SSD system requirements with respect to, for example: aggregate transfer rate through the host interface; latency between commands issued through the host interface and response to those commands; and concurrency to support simultaneous execution of multiple commands. Host interface port <b>144</b> is in communication with microprocessor <b>130</b> via control bus <b>124</b> such that the microprocessor can manage (e.g., control and/or monitor) the host interface <b>44</b>. Although only a single host interface <b>44</b> is depicted in <figref idref="DRAWINGS">FIG. 2</figref>, more than one host interface may be present in a given SSD controller embodiment. For embodiments with multiple host interfaces, each instance of the host interface can be of the same type of interface (as would be the case with multiple SAS ports) or different instances may be of different types (such as, for example, some combination of SAS, SATA, and PCI Express).</p>
<p id="p-0036" num="0035">A host to SSD controller interface <b>44</b> can be provided by host interface port <b>144</b>. Such a port <b>144</b> includes signal connections and/or circuitry configured to perform all operations required by the host interface definition. If SAS or SATA interfaces are provided by host interface port <b>144</b>, the physical connection can be made up of a minimum four signal connections organized as two differential pairs. In other embodiments, for example in the instance where SAS is used and where secondary interface signal connections are used, additional differential signal pairs can be present. For the case of SAS, more than one host port may be used: in such an embodiment, two sets of physical connectors can be present instead of one.</p>
<p id="p-0037" num="0036">Attention is now directed to <figref idref="DRAWINGS">FIG. 3</figref> which is a block diagram illustrating a module <b>30</b> (see also, <figref idref="DRAWINGS">FIG. 1</figref>) produced in accordance with an embodiment of the present disclosure. The module can include a plurality of memory devices <b>170</b>. A total number of memory devices can vary from one to eight or more, and memory devices can be based on any appropriate technology. In some embodiments, the memory technology is NAND flash. The devices making up the overall memory of a module can be standard commercially available parts or devices that can be customized for use in a module. A number of memory devices <b>170</b> can be attached to a media port <b>176</b>, such as to increase concurrent operation of attached memory devices and/or to increase data throughput in a SSD. As seen in <figref idref="DRAWINGS">FIG. 3</figref>, in some embodiments, each media port <b>176</b> can provide an interface to a group of memory devices.</p>
<p id="p-0038" num="0037">Still referring to <figref idref="DRAWINGS">FIG. 3</figref>, each media port <b>176</b> provides a memory device interface <b>180</b> that can be used to communicate with a plurality of memory devices. Further, each module <b>30</b> can include multiple media ports <b>176</b>. In some embodiments, the number of media ports is two or four. Each media port <b>176</b> can provide an industry standard interface such as, for example, ONFI (Open NAND Flash Interface) for communication with standard NAND flash devices, or each media port can provide a custom interface intended for communicating with customized memory devices. As noted above, one or more memory devices <b>170</b> can communicate over each memory device interface <b>180</b>. In some embodiments, the number can range from one to four. Moreover, there is no requirement that the same number of memory devices be connected to each media port. Further, in some instances, a particular media port may not have any memory devices connected thereto. Such a situation can occur, for example, when a particular module arrangement does not use all available media ports.</p>
<p id="p-0039" num="0038">A data path and control section <b>190</b> forms part of module <b>30</b> and can be configured to direct data traffic to and from media ports <b>176</b> in a manner that manages (e.g., coordinates) data traffic to and from the multiple media ports using various system resources in conjunction with the media ports themselves. Multiple data paths can be supported in the data flow between each media port <b>176</b> and the rest of the module. Decoder read data paths <b>200</b> include data paths to decoders <b>202</b>; a direct data path <b>204</b> to a data buffer section <b>210</b>; data paths to read logic <b>212</b>; and data paths from one media port (in read mode) to another media port (in write mode) at least flowing through data path and control unit <b>190</b>. Write data paths can include data paths from encoders <b>220</b>; a direct data path <b>230</b> from data buffer section <b>210</b>; and data paths from write logic <b>222</b>. It is noted that most, if not all, data flows that can take place on these various data paths can be sustained concurrently with all other data flows. Some of the manners by which the subject concurrency may be maintained are described, as follows:</p>
<p id="p-0040" num="0039">a. One media port <b>176</b> is used to perform a first write function to one of its attached memory devices <b>170</b>. The data path used by the first write function is one of the available write data paths in the module. At the same time, another media port <b>176</b> is used to perform a second write function to one of its attached memory devices <b>170</b>. The data path used in the second write function may be any of the remaining write data paths not already being used by the first write function. Because two media ports are performing concurrent write functions, the two memory devices receiving the write mode data are distinct. It is noted that this scenario is not possible wherein a single memory device is the target of simultaneous commands from two media ports because each memory device attaches to only one media port.</p>
<p id="p-0041" num="0040">b. A first media port <b>176</b> is used to perform a write function to one of its attached memory devices <b>170</b>. The data path used by the write function is one of the available write data paths in the module. At the same time, a second media port <b>176</b> is used to perform a read function from one of its attached memory devices. The read data path can be any read data path available to the second media port.</p>
<p id="p-0042" num="0041">c. A single media port <b>176</b> can be used to simultaneously perform more than one command. This can occur as an active command on each of two or more memory devices attached to the same media port <b>176</b>, or as multiple commands on a single memory device attached to same media port <b>176</b>. In either situation, the concurrency of commands requires concurrent data flows over a single media interface <b>192</b> provided by a particular media port <b>176</b> and data path and control section <b>190</b> of the module. This concurrency of data flow is different from the cases described in (a) and (b) above in that there is a temporal constraint imposed by the fact that at any single instant in time, only one data value can be present on the particular media port interface <b>192</b>. In order to sustain multiple data flows, the particular media port interface <b>192</b> can operate such that it is shared over time so that each active command is allowed to transfer data for a limited amount of time before it must yield to one of the other active commands. A commonly used term in the data communication arts that refers to this concept is &#x201c;time domain multiplexing&#x201d; which will be familiar to those having ordinary skill in the art.</p>
<p id="p-0043" num="0042">d. Multiple media ports <b>176</b> can be used to cooperatively perform a single read or write command if a single unit of data (e.g., a block) is partitioned and written across more than one memory device. Data can be partitioned in various ways in the context of the present disclosure: one possible partition involves separating a block into two or more partitions by interleaving. Interleaving can be performed along bit or byte boundaries, or by using larger units such as, for example, words, double words, sectors, and the like. Interleaving may be performed either before or after encoding on a write operation. Whether the interleaving step occurs before or after encoding determines a corresponding order for a decoder. If interleaving is performed before encoding, a separate encode step is performed for each partition, which can mean that as many encoders as partitions are necessary to support the function. One advantage of an interleaving function for writing/reading single blocks across multiple memory devices is that it results in achieving much higher data throughput than would be possible with a non interleaved approach.</p>
<p id="p-0044" num="0043">Module <b>30</b> includes one or more decoders <b>202</b>. As a data path resource, in the present embodiment, each decoder <b>202</b> can support only one transfer (e.g., of a block of data or of a partition of data) at a time such that multiple simultaneous decoding of different transfers can take place when multiple decoders are provided in a module embodiment such as is illustrated by <figref idref="DRAWINGS">FIG. 3</figref>. The presence of multiple decoders provides the capability for true concurrent decoding of two different transfers.</p>
<p id="p-0045" num="0044">The configuration of a decoder can vary according to the way the memory devices are being used, and so there are multiple variations that may be present in a module. As one example, if memory devices of a particular module are based on flash technology, different types of flash memory can be used. Further, within types of available flash, there may be one or more modes of use for each flash type. For example, if commercially available flash devices are selected, a memory device (or a portion of a memory device) may be used in a Single Bit per Cell (SBC) mode or a Multiple Bits per Cell (MBC) mode. In an embodiment, data can be written which is encoded with an algebraic error correcting code (&#x201c;algebraic ECC&#x201d;) regardless of which mode is being used. Hence, an appropriate decoder for this case is one which decodes the particular code that has been applied and corrects errors up to the extent allowed by the particular code. As another case, a particular algebraic code used by a module for SBC or MBC memory can further depend on whether the SBC or MBC memory is accessed in units directly corresponding to the page structure of the memory device or if the module performs an abstraction service on behalf of the host that removes the page-level details from view (as well as management responsibility) of the host. Either case can involve a different decoder embodiment.</p>
<p id="p-0046" num="0045">Another example resides in the use of memory devices that offer visibility, access, and control with respect to the internal functions of the memory device related to reading, writing, and erasing. A module that is made up of such memory devices can be written and read with greater precision than standard memories typical of the example described immediately above. The greater precision can provide not only greater density of information storage, but also can provide additional information that can be used to advantage in the decoding function. This additional information can exist in the form of additional bits of precision in the data written to and read from the flash memory and can be referred to as &#x201c;soft bits&#x201d; which is a term that will be familiar to those of ordinary skill in the data communications arts. Such a memory also allows for functions that compensate for read- or write-related distortions in data, and so offers the capability for configuration (e.g., adaption and/or correction) of read and write functions so as to provide one or both of higher data storage density (more bits per cell) and greater reliability (often manifested as increased program/erase cycles in flash memory). Such described capabilities can, in turn, require different configurations of the decoder(s).</p>
<p id="p-0047" num="0046">In view of these examples, if one criterion for an embodiment of a module is to provide flexibility with respect to the use of different types of memory devices, the corresponding decoding arrangement in the module can provide a multi-mode function. Some of the particular variations that can be expected include:</p>
<p id="p-0048" num="0047">a) BCH (Bose Chaudhuri Hoquenghem) decoding only, with one or more bit error capacity settings,</p>
<p id="p-0049" num="0048">b) RS (Reed Solomon) decoding only, with one more symbol error capacity settings,</p>
<p id="p-0050" num="0049">c) BCH decoding combined with a suitable form of sequential decoding (for example, convolutional), and</p>
<p id="p-0051" num="0050">d) RS decoding combined with a suitable form of sequential decoding.</p>
<p id="p-0052" num="0051">A module that can be configured to provide more than one decoder function can operate particular instances of a decoder in particular modes. For example, a two-decoder module can support true decoding concurrency on two data paths. Part of the flexibility afforded by multiple decoders in a module accrues from the fact that at a given time, by way of non-limiting example, one decoder can be configured as a BCH-only decoder while the other decoder can be configured as a convolutional (Viterbi detector) decoder combined with a Reed-Solomon decoder.</p>
<p id="p-0053" num="0052">With respect to encoders <b>220</b> of <figref idref="DRAWINGS">FIG. 3</figref>, the discussion immediately above can be at least generally extended to encoders and the encoding function, as the two functions are directly analogous. Accordingly, the discussion immediately above with respect to memory usage, decoder modes, flexibility, and concurrency applies equally to the encoding case.</p>
<p id="p-0054" num="0053">Read logic <b>212</b> can be provided as operational support for at least semi-autonomous capabilities of a module as a storage subsystem. Functions performed by read logic <b>212</b> can include, by way of example, data processing used in conjunction with calibration or compensation such as, for example, the mean value and array processor functions described in commonly owned, copending U.S. patent application Ser. No. 12/888,585, filed on Sep. 23, 2010, entitled MEMORY QUALITY MONITOR BASED COMPENSATION METHOD AND APPARATUS (hereinafter, the '585 Application), for memory devices in which calibration and/or compensation functions afford specific control over their internal operations. In particular, compensation can involve signal processing using parameters associated with the physical arrangement of the memory devices, while calibration can involve a number of metric gathering analysis functions that can be performed as hardware functions under direct software control.</p>
<p id="p-0055" num="0054">With respect to write logic <b>222</b>, the discussion regarding read logic <b>212</b> and read functions discussed immediately above can also be at least generally extended to the write logic and write functions. In some cases, there may be a particular reason or advantage to perform an operation associated with compensation or adaptation in the write mode instead of the read mode. With regard to such adaptation, information such as, for example, signal-to-noise ratio (&#x201c;SNR&#x201d;) related data, can be gathered as the basis for subsequent parameter values. In the instance of bit density configuration and by way of non-limiting example, a module can utilize the SNR statistics relative to a particular physical region of the memory that is reclaimed by a garbage collection function in configuring a bit density of that region when that region is reallocated. This can be the primary difference between the write and read functions. A module can include any suitable combination of read and write functions. Accordingly, in a given module, there can be configurable write functions but no configurable read functions; or there can be configurable read functions but no configurable write functions. In some embodiments, both configurable read and write functions can be provided.</p>
<p id="p-0056" num="0055">Direct write data path <b>230</b> from data buffer section <b>210</b> to data path and control section <b>190</b> can provide low latency data communication when necessary, as well as an alternate data transfer path for transfers not requiring decode functions or read functions. The direct write path is a write path in that data transfer is to one of media ports <b>176</b> and an associated memory device <b>170</b>.</p>
<p id="p-0057" num="0056">Direct read data path <b>204</b> is analogous to direct write path <b>230</b> and extends from data path and control section <b>190</b> to data buffer section <b>210</b>, such as to provide low latency data communication when necessary, as well as an alternate data transfer path for transfers not requiring encode functions or write functions. The direct read path is a read path in that data transfer is from one of media ports <b>176</b> and an associated memory device <b>170</b>.</p>
<p id="p-0058" num="0057">Data buffer section <b>210</b> is connected to a controller port <b>240</b> that provides one instance of a module interface <b>42</b> (see <figref idref="DRAWINGS">FIGS. 1 and 2</figref>). The data buffer section implements a data buffer function that provides data staging for transfers between the data buffers and the media ports as well as between the data buffers and the module interface. Such a data buffer function can provide, for example, speed matching between transfers into and from the buffers. For instance, any flash read command can involve transfer of memory device data to the module interface as a combination of (1) discrete transfers that originate in a memory device <b>170</b> and terminate at buffer <b>210</b> and (2) subsequent transfers that originate at buffer <b>210</b> and flow through module interface <b>42</b>. Conversely, any function that writes to a memory device can involve a combination of (1) discrete transfers that enter through module interface <b>42</b> and terminate at data buffer <b>210</b>, and (2) subsequent transfers that originate at data buffer <b>210</b> and terminate at a memory device <b>170</b>.</p>
<p id="p-0059" num="0058">It should be appreciated that data buffer section <b>210</b> can include a DMA portion <b>242</b> that is configured to control the flow of data into and out of the data buffers. In general, a DMA resource is made up of a starting offset, a count, and a sequencing function is assigned to each possible data path.</p>
<p id="p-0060" num="0059">With continuing reference to <figref idref="DRAWINGS">FIG. 3</figref>, each module <b>30</b> includes an embedded microprocessor <b>300</b> that operates from a program memory <b>302</b>, and which microprocessor is connected via a microprocessor bus, which is understood to be present but which is not shown for purposes of illustrative clarity, to various components which make up the module. In particular, the microprocessor can access registers in the various components. Software stored in program memory <b>302</b> can vary based on a particular embodiment. One having ordinary skill in the art will recognize that the extent of hardware support for given functions can vary. Hence, cooperation between software and hardware in performing management functions can be configured responsive to variations in:</p>
<p id="p-0061" num="0060">a. memory device types.</p>
<p id="p-0062" num="0061">b. encoding and decoding functions (e.g., algorithms).</p>
<p id="p-0063" num="0062">c. compensation functions (applies to compensation on read data and to compensation before writing data).</p>
<p id="p-0064" num="0063">d. block erase functions for flash memory.</p>
<p id="p-0065" num="0064">e. module interface support (command execution) functions.</p>
<p id="p-0066" num="0065">f. autonomous wear leveling functions, in which the module executes wear leveling as a self-contained function independent from the main controller).</p>
<p id="p-0067" num="0066">g. semi-autonomous wear leveling functions, in which the main controller controls the wear leveling function by providing related parameters and commands to the module via the module interface.</p>
<p id="p-0068" num="0067">h. local autonomous flash translation layer (FTL) implementation.</p>
<p id="p-0069" num="0068">i. directed flash translation layer (controlled by host) implementation, in which the main controller controls flash translation operation by providing related parameters and commands to the module via the module interface.</p>
<p id="p-0070" num="0069">j. physical memory access directed by the host. The host can access physical memory in the plurality of modules by issuing appropriate commands to the main controller, and the main controller can respond to such commands by issuing appropriate direct-access commands to one of the memory devices attached to a selected module.</p>
<p id="p-0071" num="0070">k. physical memory access directed by the module, with logical block abstraction to the host.</p>
<p id="p-0072" num="0071">l. autonomous garbage collection (usually applies to the local FTL).</p>
<p id="p-0073" num="0072">m. semi-autonomous garbage collection in which the main controller exerts control over the garbage collection function in one or more of the modules attached to the main controller by writing appropriate parameters to the modules in which this control is to be applied.</p>
<p id="p-0074" num="0073">n. support for directed garbage collection (such as internal block copy functions) in which the main controller controls the garbage collection function by writing appropriate parameters to at least some subset of the plurality of modules via each module's module interface.</p>
<p id="p-0075" num="0074">o. local self test of attached memory devices</p>
<p id="p-0076" num="0075">p. autonomous power management functions</p>
<p id="p-0077" num="0076">It should be appreciated that the foregoing list is provided by way of example and is not intended to be all inclusive or limiting. Accordingly, the module configuration supports selection and reconfiguration of at least the above aspects of module operation in any suitable combination.</p>
<p id="p-0078" num="0077">Still referring to <figref idref="DRAWINGS">FIG. 3</figref>, module <b>30</b> includes a function engine <b>320</b> that can be used to perform (e.g. represent) any suitable combination of functions provided through CPU <b>300</b> and program memory <b>302</b> as resources. Embodiments of function engine <b>320</b> can perform, for example, one or more of the various management functions described above. For example, wear leveling, FTL, and garbage collection, as well as other functions that may be desired and/or required, can be provided in an embodiment. Furthermore, the function engine can receive a set of parameters, provided from controller <b>40</b> and/or from the module itself which configures the functions. That is, the management functions of the module are configurable, at least to some extent, based on parameters that are provided from either the controller or the module itself. For example, the extent to which the controller directs a management function can be configured based on parameters that can be set by the controller. Once the parameters are established, however, the management function can operate autonomously in an embodiment. Parameter priority will be discussed in detail at an appropriate point hereinafter. In any case, however, the function engine is configured to perform a function(s) of interest, as will be further described. Main controller <b>40</b> can determine characteristics of operation with respect to a given function through a set of configuration parameters that the controller provides to a shared parameter section <b>322</b> of the function engine. The shared parameter section can be populated, for example, by CPU <b>300</b> responsive to controller <b>40</b> (<figref idref="DRAWINGS">FIG. 2</figref>) and responsive to module interface <b>42</b> of module <b>30</b> (<figref idref="DRAWINGS">FIG. 3</figref>). These configuration parameters determine how the function engine will perform (e.g., initialize and/or operate) each function such as, for example, wear leveling, garbage collection and bit density configuration. The configuration defined by the parameters for each function can define the requirements for activities to be carried out by both the module and the main controller. For example, at least semi-autonomous wear leveling can be carried out by the module with minimal related communication from the main controller following original configuration. Directed wear leveling, on the other hand, can require continuing or periodic configuration from the controller accompanied by monitoring of the wear-leveling activity by the controller via some form of reporting from the module. Such reporting can be accomplished by a module, for example, through updating output parameter values that are provided via a reporting section that is yet to be described. Accordingly, the function engine of each module can be configured to perform a management function for the nonvolatile memory devices of only that module based at least partially on one or more module input parameters. The main controller provides for digital data communication with the host device and provides for module digital data communication with each module of the plurality of modules such that any data flowing to and from one of the modules flows through the controller. Further, the controller can be configured to provide input parameters to each module to configure the at least semi-autonomous execution of each function of interest.</p>
<p id="p-0079" num="0078">An input data path <b>330</b> from module interface <b>42</b> to data buffers <b>210</b> transfers data and control information from the module interface into the buffer section. This data path can be of a relatively higher speed, for example, relative to at least some other data paths extending between buffer section <b>210</b> and data path and control section <b>190</b> since incoming path <b>330</b> can serve a plurality of the data paths from data buffer section <b>210</b> to data path and control section <b>190</b>.</p>
<p id="p-0080" num="0079">An output data path <b>340</b> extends from data buffer section <b>210</b> to controller port <b>240</b> for transfer of data and control information from the buffer section to the module interface and forms part of an output data path from the module <b>30</b> to SSD controller <b>40</b> via the module interface <b>42</b> provided by controller port <b>240</b>. Data path <b>340</b> can be capable of higher bandwidth than other data paths to or from buffer section <b>210</b> since, like data path <b>330</b>, a plurality of other data paths can be served by output data path <b>340</b>.</p>
<p id="p-0081" num="0080">Details with respect to an embodiment of module interface <b>42</b> are illustrated by the block diagram of <figref idref="DRAWINGS">FIG. 4</figref> with the interface connection extending between module port <b>100</b> of controller <b>40</b> of <figref idref="DRAWINGS">FIG. 2</figref> and controller port <b>240</b> of module <b>30</b>, as shown in <figref idref="DRAWINGS">FIG. 3</figref>. The controller port and module port provide the command and data connection between each module <b>30</b> and controller <b>40</b> that makes up each module interface <b>42</b>. A data transmit path <b>400</b> provides a communication path for transmission of commands, data, status, and other information from the controller to the module. Of course, these items can originate from host device <b>20</b> (<figref idref="DRAWINGS">FIG. 1</figref>). In an embodiment, the data transmit path can be established by differential signaling, so two physical signals can make up the data transmit path. The data transmit path can optionally be embodied in the form of a bi-directional signal pair such that operation in the reverse direction can act as a performance enhancement to the data transfer capability of the data receive path which is yet to be described.</p>
<p id="p-0082" num="0081">A data receive path <b>402</b> provides a communication path for transmission of data, status, and other information from the module <b>30</b> to controller <b>40</b>. Like data transmit path <b>400</b>, path <b>402</b> can be established, in an embodiment, by differential signaling, so two physical signals make up the data receive path. In an embodiment, the data receive path can be implemented as a bi-directional signal pair and when operating in the reverse direction can act as a performance enhancement to the data transfer capacity of the data transmit path.</p>
<p id="p-0083" num="0082">A differential pair of signals can provide a clock <b>410</b> over module interface <b>42</b>. The clock signal that is directed onto the clock pair can be generated in the host and can also be used by the host for synchronizing data transmission and reception. The differential clock signal provided over the module interface can be used to synchronize data transmission and reception in the module. The clock received by the module is directly related to the clock generated by and used by the host. Because of transmission delay and other circuit effects, the clock signal received by the module may be out of phase relative to the signal used by the host, so sampling circuitry in both the host and the module can be assisted by clock alignment circuitry, such as a DLL or PLL, that can be dynamically configured (e.g., adapted) in order to achieve optimum alignment between clock and data signals.</p>
<p id="p-0084" num="0083">Using the described interface, significantly fewer pins can be required by the SSD controller. While each memory module can support 8 flash chips, connection of the module to the SSD interface can be performed via one reduced pin-count module interface. In this way, the number of pins needed to control all the flash devices in the system can be reduced compared with prior art implementations wherein each memory device connects directly to the controller. The embodiments shown in <figref idref="DRAWINGS">FIGS. 1 and 4</figref> can enable fewer pins by limiting the number of pins required for controller to module connections. That is, the reduced pin-count module interface of <figref idref="DRAWINGS">FIG. 4</figref> provides connectivity to the controller with few pins when compared with prior art interfaces such as, for example, ONFI. In this way the SSD controller can connect to and control many modules with few pins on the controller (when compared with alternatives such as ONFI), while achieving transaction rates and data throughput rates appropriate for SSD applications. The module interface described herein provides for abstraction of low-level flash functions from higher-level SSD functions. Accordingly, the controller can be unburdened by being configured only with high-level aspects of command execution, for example. Low-level functions, such as those concerned with physical flash characteristics, can be carried on at the module level. It should be appreciated that SSD performance can be enhanced with increasing numbers of attached memory devices though the use of at least semi-autonomous functionality for managing memory devices. Within a single hardware configuration (a particular combination of pre-configured modules with a particular SSD controller) an SSD can be configured in which the modules are at least semi-autonomous entities wherein the role of the controller resides in accessing logically addressed data units in the modules. In such a system, the role of the controller with respect to management can be primarily to manage the storage provided by each module, as an aggregation of module units, into a single larger volume.</p>
<p id="p-0085" num="0084">The module interface can include features and capabilities at the command and protocol levels which customize its intended application to SSD usage. These features and capabilities can include support for low latency read and write functions, support for high data throughput, support for command queuing and concurrently operating commands, support for out of order transfer of data blocks comprising a multi-block transfer operation, support for commands at different levels of block abstraction and a simple, yet advanced high-performance protocol, each of which will be described, in turn, immediately hereinafter.</p>
<p id="h-0003" num="0000">Support for Low Latency Read and Write Functions</p>
<p id="p-0086" num="0085">Low latency is supported in module <b>30</b> by supporting what can be the fastest possible response to commands received from controller <b>40</b>. This can be accomplished by module interface <b>42</b> using embodiments taking at least two approaches: First, through support for concurrently executing commands, and second, through the option of short data packets. Through the first approach, a command can be issued to module <b>30</b> from controller <b>40</b> and acted upon immediately even though one or more other commands may already be executing. This allows the command to access a targeted memory device (if it is not already being accessed by a command to the same memory device) immediately, and enables data to begin transferring between controller <b>40</b> and a memory device <b>170</b> in the module more quickly than would be possible if a currently active command were to require completion before another command begins executing. Interface support for command queuing and for concurrent command execution can enable such concurrent execution capability. Through the second approach, the data transfer phase of read or write commands can begin more quickly and therefore complete more quickly as compared to the use of longer packets. The difference between the second approach and approaches using longer rather than shorter packets, is that shorter packets allow lower latencies because of the shorter transfer time of short packets relative to long packets; hence all concurrently executing transfers are allocated buffer bandwidth on a low-latency basis. A given command can be presented with a shorter wait time for access to the interface with short data packets as compared with having to wait potentially longer if longer data packets are used. Data packet length can be configurable. However, data packet length for a particular embodiment can be dictated, for example, by a parameter used at section allocation time that prioritizes for access latency according to packet length. Hence, a maximum packet length may be determined by controller <b>40</b> for a given module configuration. Also by way of the second approach, the direction switching function discussed with respect to data transmit path <b>400</b> and data receive path <b>402</b> of <figref idref="DRAWINGS">FIG. 4</figref> can be employed. When selected, the direction switching function can enable faster transmission of all packet sizes, thereby increasing both throughput and latency performance.</p>
<p id="h-0004" num="0000">Support of High Data Throughput</p>
<p id="p-0087" num="0086">High throughput and low latency are sometimes conflicting purposes if larger packets are required for high throughput. While short packet sizes enhance latency performance, they also tend to degrade throughput performance because the inherent overhead of the interface is proportionally larger for small data packets than for larger packets. That is, short packets achieve lower transfer efficiency than long packets, hence when the demand placed on the interface approaches the bandwidth capacity of the interface, better data throughput is achieved for long packets than for short packets. As a configuration option, the ability to select packet size is also an aspect of the support for high data throughput in order to address the competing interests of high throughput and low latency.</p>
<p id="h-0005" num="0000">Support of Command Queuing and Concurrently Operating Commands</p>
<p id="p-0088" num="0087">A command queuing function can be present in module interface <b>42</b>, in an embodiment, as primarily a hardware function. Each command that executes must have support for all transactions that will be involved in the execution of the command, including command transfer, data transfers, and status of handshaking transactions. These transactions, in turn, can use facilities such as DMA resources in proceeding. Such hardware resources can be provided in the module interface definition to the extent required to enable the command queuing function. An additional function for command queuing can be generation and checking of a queue tag. The queue tag is a short binary value that denotes an integer used to address the aforementioned hardware resources required for a command. A queue tag is transmitted with each new command code sent from the host to a device, and remains active as long as the command continues to execute. After a command terminates, the queue tag is no longer active and so becomes available for re-assignment when a subsequent command is issued.</p>
<p id="h-0006" num="0000">Support for Out of Order Transfer of Data Blocks Comprising a Multi-Block Transfer Operation</p>
<p id="p-0089" num="0088">The module interface protocol can support a transaction model between the controller and module that enables out of order transmission of blocks to occur within a long multi-block transfer. The transaction model can provide that any read or write command be comprised of one or more &#x201c;connections&#x201d;. Each connection is a virtual construct that allocates addressing and counting control for a portion of a longer transfer. As such, a connection exists as a temporal entity that exists to sustain some portion of a command (normally data transfer) and that must terminate due to a time constraint on its existence. The time constraint exists in order to guarantee that the interface is available to other commands that may also be active. The constraint may be fixed or variable. A read or write transfer may only require a single connection or it may require many connections. The number of connections required for a command, then is a function of the total length in bytes of the read or write command, and of the size of the time constraint. This determination of the number of connections can be made by command handling software running in the module. Connections may be established one at a time such that only one connection exists within a given command at a given time, or a given command may have multiple active connections, with each connection transferring a different portion of the overall transfer length of the command. Out of order transfer (of blocks) can be supported, for example, by establishing connections that transfer a block or a few blocks that may be out of order relative to their offset within the entire range of blocks being transferred by the command.</p>
<p id="p-0090" num="0089">Each connection can be established as part of the execution of an overall command. A connection exists only long enough to transfer its allotted length of data which can be a block (as defined relative to a command) or as number of bytes defined by a count field in a command code. A connection terminates after the defined length of data is transferred.</p>
<p id="h-0007" num="0000">Support for Commands at Different Levels of Block Abstraction</p>
<p id="p-0091" num="0090">Controller <b>40</b> can utilize different approaches for accessing the memory devices attached to a module <b>30</b>. While there can be variations, two embodiments are prevalent, as will be described immediately hereinafter.</p>
<p id="p-0092" num="0091">In the first embodiment, the controller accesses memory devices attached to a module over a module interface <b>42</b> as standard media in which the memory devices include memory arranged as a hierarchy with pages at the lowest level of the hierarchy, and erase blocks at a higher level of the hierarchy. A read or write access targets a particular page with a page having a fixed amount of data and can include additional bits that are normally used for ECC parity. Another type of access is an erasure, which targets an erase block. An erase block, in flash memory, is a much larger structure than a page. In fact, an erasure block contains some integer number of pages, the number of which can vary. A typical value is 128 pages per erase block. Pages generally must be written as units, and erasures generally must be performed on entire erase blocks. Pages can be read from a memory device entirely or in part, but partial page reads can affect ECC encoding and decoding functions. Some ECC encoding functions may encode entire pages and thus require that an entire page be read in order for ECC decoding to be performed. Other ECC encoding functions may be implemented so that the encoding and decoding is performed on subdivisions of the data in a page: in this case, a partial page read can read one or more of these subdivisions in order for ECC decoding to be performed. Regardless of whether a full-page or partial page read, or a full-page write is to be performed, addressing of physical pages is performed using physical addresses. The use of physical addresses requires knowledge in the issuer of the command (controller <b>40</b>, in this instance) of the physical arrangement of pages in each memory device attached to the targeted module.</p>
<p id="p-0093" num="0092">The first embodiment is supported by encoding, decoding, and data path hardware in the module. Addressing, read, write, as well as ECC encoding and decoding can all be supported in the module as a selectable configuration in a manner that has not been seen heretofore.</p>
<p id="p-0094" num="0093">The second embodiment is one in which the memory devices are addressed and accessed as logical units comprised of logical sectors or logical blocks with logical (rather than physical) addresses. Data lengths are not constrained by the physical length of pages in this approach, so the module can manage physical pages and erase blocks such that the logical structure is mapped onto the physical structure whereby the controller need only add a logical address to a command in order transfer the desired data. Functions in the module that support this logical form of access (sometimes called block abstraction) include various aspects of the encoder and decoder configurations, as discussed with respect to <figref idref="DRAWINGS">FIG. 3</figref>). Other aspects of this support, also discussed with regard to <figref idref="DRAWINGS">FIG. 3</figref>, include data path arrangements, and the miscellaneous read and write functions as related to compensation and calibration. Software features and functions are an integral part of this support, as the software provides support for functions that are involved with or related to the block abstraction capability. A few of these functions are the local Flash Translation Layer (FTL), control over erasure operations (e.g., garbage collection), wear leveling, bit density configuration (e.g., encoder/decoder configuration), and the like. Still further details with respect to block abstraction will be presented with regard to <figref idref="DRAWINGS">FIG. 5</figref>, yet to be described.</p>
<p id="h-0008" num="0000">Simple, yet High-Performance Protocol</p>
<p id="p-0095" num="0094">By implementing only the most essential functions, the module interface is able to operate with lower command overhead than would otherwise be possible. Because the interface takes the form of a point-to-point connection only, the protocol is relieved of the burden of multi-device support. No device addresses have to be generated, recorded, or used in order for a host to initiate, execute, and monitor transactions with a device. Similarly, the point to point physical connections implemented by the module interface are dedicated rather than dynamically established (as distinct from the finite-time logical/virtual connections discussed above as part of command execution). The configuration of each host/device connection via its own module interface is established at manufacturing time, so the host has no need to configure its devices at power-on time.</p>
<p id="p-0096" num="0095">In addition, the interface supports both true concurrency (simultaneous independent transfers) and time domain multiplexed concurrency. True concurrency takes place when one or more read commands and one or more write commands execute concurrently. The interface concurrency occurs when data transfer phases for both read and write commands occur together, such that data receive path <b>402</b> of the module interface (<figref idref="DRAWINGS">FIG. 7</figref>) is actively transferring data from module <b>30</b> to controller <b>40</b> at the same time that data transmit path <b>400</b> is actively transferring data from controller <b>40</b> to module <b>30</b>. It is noted that this type of concurrency occurs only for a configuration or for command in which the direction switching function discussed above is not engaged.</p>
<p id="p-0097" num="0096">The time division multiplexed form of concurrency can frequently be achieved by the module interface. This occurs, for example, when one or more active commands have established connections that execute data phase transactions using data packets over the module interface. Even though multiple active connections may be established, at most one packet may be transmitted at any instant along a single data transmit or data receive signal pair. Time domain concurrency occurs because the active connections transfer data packets in an alternating fashion, with one connection transferring a data packet at one instant and another connection transferring a data packet at another instant. In this way, multiple commands can be transferring data packets concurrently, but doing so by sharing access to the physical transfer connection. For example, as seen in <figref idref="DRAWINGS">FIG. 4</figref>, packets labeled P<b>1</b>, P<b>2</b> and P<b>3</b> are transferred in a serial manner across data receive path <b>402</b> with each packet forming a different portion of an overall data transfer. Like transfers can also be supported across data transmit path <b>400</b>. In an embodiment, such packet transfers can be concurrent on the data transmit path and the data receive path.</p>
<p id="p-0098" num="0097">The SSD can be configured such that the controller provides different amounts of management oversight for different modules. Such an embodiment can be referred to as a mixed mode approach to the modules wherein one portion of the modules can be managed with at least some interaction with the controller in terms of performing functions such as, for example, wear leveling, garbage collection and bit density configuration, and another portion of the modules can operate autonomously with respect to detailed management of these functions.</p>
<p id="p-0099" num="0098">With respect to bit density configuration, each module can include memory devices capable of high density operation (e.g., greater than 2 bits per cell) and, in an embodiment, can be configured by the controller to either operate in a high density mode or to operate in a low density mode (e.g., 2 bits per cell or less) across all the memory devices included in the module. In some embodiments, a module can be configured by the controller to operate some memory devices in high density mode (e.g., greater than 2 bits per cell) and to operate other memory devices in low density mode (e.g., 2 bits per cell or less). Where a module includes memory devices capable of low density operation (e.g., 1 or 2 bits per cell), that module can be configured by the controller to operate with an encoding and decoding configuration consistent with the low density operation. Configuration parameters for this selection can include the data length applied to each ECC encoding and decoding unit, and the number of parity bits included with each ECC encoding and decoding unit. Furthermore, if the low density memory is configured as either one bit or two bit per cell, the module can be configured by the controller to use a selected low density mode.</p>
<p id="p-0100" num="0099">As discussed above, an at least semi-autonomous memory module can perform various functions in a stand-alone manner. Further considering the function of wear leveling, it should be appreciated that each memory module can be responsible for performing wear leveling within its own set of memory chips based on parameters provided to the module. Distributed wear leveling control removes most of the burden of having a controller monitor and adjust wear on each individual memory chip.</p>
<p id="p-0101" num="0100">Other functions, by way of non-limiting example, can be handled at least semi-autonomously by a module according to the present disclosure. For example, each memory module can manage its own power consumption. When a memory module is inactive or lightly loaded, power management circuits on the memory module can be activated to reduce power consumption on a per module basis. This results in distributed power management without the overhead of a controller having to monitor and control the power state of each memory module.</p>
<p id="p-0102" num="0101">As another example, each memory module can be responsible for monitoring and refreshing data within its own set of memory chips. This removes the burden of having the controller monitor the data quality in each memory chip.</p>
<p id="p-0103" num="0102">As still another example, each memory module can handle all the error correction required for its own NVM chips. Such distributed error correction processing results in a much higher throughput than could be achieved if a controller is made responsible for all the error correction, as in a conventional SSD. Because the error correction processing can be distributed within the SSD, the amount of error correction that can be applied can be much higher than in a conventional SSD, resulting in a higher data density within the individual memory chips of some embodiments.</p>
<p id="p-0104" num="0103">Due to the distributed processing power of the memory modules that have been brought to light herein, the throughput of an SSD that includes such modules can be increased many times over the throughput achieved by conventional SSD architectures. The SSD controller is not burdened (or at least has its burdens reduced) with functions such as, for example, error correction, data refresh and low-level wear leveling. Each memory module can handle a high data rate due to being able to control up to, for example, at least 8 NVM die per memory module. The high data rate can be multiplied by the fact that there can be multiple memory modules on each bus from the controller. This can again be multiplied by the fact that there can be several busses running in parallel between the controller and the memory modules. The overall result is an SSD that can achieve many times the data rate (throughput) of existing SSDs or hard drives.</p>
<p id="p-0105" num="0104">The module interface can enable higher performance than would otherwise be possible using prior art techniques at least for the reason that the module to controller interface can be relieved of the need to carry large amounts of data that relate to low level functions that are now at least semi-autonomously performed by each module in a distributed manner instead of being the responsibility of the controller. Thus, the controller is free to operate without the burden of these low level functions, which in turn provides for simpler, more efficient execution of high-level SSD functions by the SSD controller. The result of this separation of processing duties is a more efficient, high-performance SSD system.</p>
<p id="p-0106" num="0105">Referring to <figref idref="DRAWINGS">FIG. 5</figref> in conjunction with <figref idref="DRAWINGS">FIG. 3</figref>, the former is a block diagram which diagrammatically illustrates the multiple memory devices of one memory module <b>30</b> of <figref idref="DRAWINGS">FIG. 3</figref>, by way of example, with the module NVM memory collectively referred to by the reference number <b>500</b>. In the present example, module NVM memory includes 8 memory devices that are designated as NVM <b>1</b>-NVM <b>8</b>. It should be appreciated that the use of 8 memory devices is by way of example and not intended as limiting and that other modules can include different numbers of memory devices that are organized in different ways while remaining within the scope of the teachings herein. Accordingly, module NVM memory <b>500</b> can be organized by controller <b>40</b> (<figref idref="DRAWINGS">FIG. 2</figref>) to be made up of some number of sections. The controller can partition the multiple memory devices into sections by any of various criteria, as will be described below. Sections can be allocated among the 8 memory devices in a highly flexible manner. Initially, it should be appreciated that the section allocations of the memory of one module to the next can be completely different and independent in terms of both physical region and bit density even though both modules include the same characteristic type of NVM. As a first example, a first memory section <b>502</b> includes only memory device NVM<b>1</b>. As a second example, a second memory section <b>504</b> includes memory devices NVM<b>2</b> and NVM<b>4</b>. As a third example, a third memory section <b>506</b> includes a portion of memory device NVM<b>3</b> and the entirety of memory device NVM<b>5</b>. As a forth example, a forth memory section <b>508</b> includes a portion of memory device NVM<b>8</b>. For purposes of these examples, it can be assumed that the portions of the memory devices that are shown outside of sections are unallocated. Thus, any one section can include a portion of an individual memory device and/or the entirety of one or more memory devices in any desired combination. In embodiments using NAND flash memory devices, a lower bound on the size of a section can be the extent of data that can be stored in a single erase block. Accordingly, the memory devices making up a module can be partitioned for purposes of optimizing the storage space that is available in a given set of memory devices based on parameters such as, for example, storage capacity, storage retention, program/erase cycles, data throughput, or some combination thereof. Partitioning of memory devices can be a dynamic capability that allows for allocation, de-allocation, and reallocation of particular physical portions of the memory as required to manage the memory devices over their useful lifetimes. The use of sections can enable, for example, optimal use of the memory devices based on the physical characteristics or type of memory according to the innate characteristics of that physical extent as well as its wear history.</p>
<p id="p-0107" num="0106">Turning to <figref idref="DRAWINGS">FIG. 5</figref>, it should be appreciated that block abstraction generally involves the writing and reading of more than one page in a way that bridges across page boundaries and may be referred to as page wrapping. In this regard and by way of non-limiting example, NVM<b>1</b> is illustrated as containing physically adjacent pages <b>510</b><i>a</i>-<b>510</b><i>d</i>. If a read or write operation includes the hatched second half of page <b>510</b><i>a </i>as well as the first hatched half of page <b>510</b><i>b</i>, the read or write operation will involve reading or writing the entirety of pages <b>510</b><i>a </i>and <b>510</b><i>b</i>. The hatched areas of these pages, for example, can represent a codeword that is used by encoders <b>220</b> and decoders <b>202</b> of <figref idref="DRAWINGS">FIG. 3</figref>. Thus, page wrapping provides encode and decode functions extending across page boundaries as if there is no physical discontinuity between the pages for the purpose of the operation.</p>
<p id="p-0108" num="0107">Attention is now directed to <figref idref="DRAWINGS">FIG. 6</figref>, which is a block diagram illustrating an embodiment of function engine <b>320</b> of module <b>30</b>, as shown in <figref idref="DRAWINGS">FIG. 3</figref>. The components of the function engine as well as their interconnections are shown. It should be appreciated that any suitable combination of hardware and software can be utilized for purposes of achieving the functions of the various components that have been illustrated. In an embodiment, CPU <b>300</b> and program memory <b>302</b> can subsume the operation of the function engine. A common facility (e.g., area) <b>602</b> of the function engine can make up a set of data structures that include parameters (e.g., data tables including parameters) that are used and modified in a common or shared manner by a wear leveling function <b>610</b>, a garbage collection function <b>612</b>, a bit density configuration function <b>614</b>, and an FTL function <b>616</b>, for example. Each of these functions is configured for executing its assigned task based at least partially on certain input parameters. It is noted that shared parameter section <b>322</b> (<figref idref="DRAWINGS">FIG. 3</figref>) is shown as forming part of common facility <b>602</b> and may, therefore, be referred to with reference to either <figref idref="DRAWINGS">FIG. 3</figref> or <figref idref="DRAWINGS">FIG. 6</figref>. Other than the need for these input parameters, each function can operate autonomously with respect to other modules or the SDD system and controller, as a whole. Common facility <b>602</b> serves as an information conduit among the functions since, in some cases, the various functions share information and operate in a cooperative manner. In general, each function accesses up-to-date information from the data structures in the common facility in order to complete its tasks. For example, FTL function <b>616</b> uses an up-to-date logical to physical address translation that can be stored, for example, in a table area <b>620</b> in order to provide the function of returning physical locations of a data unit given the logical address of that data unit. Further, examples of the information that is maintained in the common facility will be provided at appropriate points hereinafter. In the present embodiment, the common facility further includes a reporting section <b>624</b> that is configured for reporting one or more output parameters (e.g., indicating use statistics) that relate to the nonvolatile memory of the module. The use statistics are available to the function engine itself as well as the main controller and can be based on a set of read values that are obtained during a read operation. The use statistics can include, by way of example, at least one of a mean read-back value for the set of read values that can be generated based on a read-back operation for a set of read-back values and a standard deviation for the set of read-back values. The use statistics can also include error correction statistics such as, for example, a cumulative error count and a per block error count in instances where the read operation decodes a block error correction code. It should be appreciated that the error correction statistics can be generated by one or more of decoders <b>202</b> (<figref idref="DRAWINGS">FIG. 3</figref>) from which the reporting section obtains the statistical information. Based on the use statistics, the function engine can set and/or select parameters in view of the health of the memory that is indicated by the use statistics. For example, if the use statistics indicate degradation of the memory based on a significant number of program/erase cycles, the bit density for the memory can be lowered. In an embodiment, the number of program/erase cycles can exceed a threshold program erase parameter that is specified for the particular nonvolatile memory that is in use.</p>
<p id="p-0109" num="0108">In some embodiments, the reporting section of a module of the subset of modules can be configured to provide a health indication relating to the nonvolatile memory of that module based on the use statistics. For example, a parameter can specify a threshold at which the function engine responds by retiring a portion of the memory previously allocated at higher bit density by copying existing data away from that portion; erasing the portion; and then re-allocating the portion to a relatively lower bit density so that further writing to that portion would be at the lower bit-density. This activity can override at least some controller parameters, for example, those provided in an initial configuration based on ongoing wear of the memory. In an embodiment, the controller can access and monitor the health indication and provide parametric control to instruct the module to retire some portion of the memory as described above. Of course, these various activities invoke the use of the functions that are provided by the function engine and described in detail at appropriate points below.</p>
<p id="p-0110" num="0109">In view of the foregoing, it should be appreciated that controller parameters <b>630</b> are provided by main controller <b>40</b> while module parameters <b>632</b> are originated internal to the module itself. Non-limiting examples of the latter include use statistics generated by a decoder <b>202</b> (<figref idref="DRAWINGS">FIG. 3</figref>). The controller parameters and module parameters are provided to common facility <b>602</b>. The parameters are primarily used by the four illustrated functions themselves, and can influence the configuration of each function.</p>
<p id="p-0111" num="0110">Still referring to <figref idref="DRAWINGS">FIG. 6</figref>, wear leveling function <b>610</b> is concerned with uniformly maximizing the number of use cycles across all memory devices in the module. One of ordinary skill in the art will be familiar with the underlying principles in the configuration of a wear leveling function and will appreciate the need for such a function resides in the fact that certain forms of nonvolatile memory exhibit a limited lifetime in withstanding a limited number of program/erase cycles. The wear leveling function is directed to extending the useful life of the memory devices of a particular module.</p>
<p id="p-0112" num="0111">Garbage collection function <b>612</b> identifies memory units (sections, blocks, or pages) that have been written with no-longer-needed data and returns this memory to the pool of memory units available for subsequent writing with new data. Like the wear leveling function, one of ordinary skill in the art will be familiar with the underlying principles in the configuration of a garbage collection function.</p>
<p id="p-0113" num="0112">Bit density configuration function <b>614</b> allocates memory units with bit density configurations determined as the result of choices made on criteria, yet to be described, for preferred bit density configurations in each applicable physical portion of the memory. In an embodiment, the memory units can be sections that are made up of blocks.</p>
<p id="p-0114" num="0113">FTL function <b>616</b> provides a physical location of a particular data unit (section, block or page) based only on the logical address of that unit. Because wear leveling, garbage collection, and bit density configuration can all be dynamic functions that cause on-going updates affecting address translation tables, the FTL and other functions can cooperate with one another on a real-time basis to maintain up-to-date status in common facility <b>602</b>.</p>
<p id="p-0115" num="0114">Where like parameters form part of controller parameters <b>630</b> and module parameters <b>632</b>, in an embodiment, priority is generally given to the controller parameters. In this regard, parameters may be organized in any suitable and flexible manner. For example, in some embodiments, the parameters can be organized in subsets wherein one subset of like parameters provides priority to values from controller <b>40</b> and another subset of like parameters provides priority to values from module <b>30</b>.</p>
<p id="p-0116" num="0115">Turning now to <figref idref="DRAWINGS">FIG. 7</figref>, an embodiment of wear leveling function <b>610</b> of <figref idref="DRAWINGS">FIG. 6</figref> is generally indicated by the reference number <b>700</b>. The wear leveling function can operate on the basis of parameters available from main controller <b>40</b> at <b>702</b> and internal parameters available from the module at <b>704</b> as contained by shared parameter section <b>322</b> (<figref idref="DRAWINGS">FIGS. 3 and 6</figref>) of the function engine. Thus, a great deal of flexibility is provided with respect to performance of the wear leveling function insofar as the particular sources of the parameters. Irrespective of the source of the parameters, the wear leveling function is typically concerned with maximizing the operational life time of the memory devices of the module when these memory devices are comprised of storage cells that degrade after some number of program and erase operations.</p>
<p id="p-0117" num="0116">Through the selection of parameters provided to shared parameter section <b>322</b>, main controller <b>40</b> (<figref idref="DRAWINGS">FIG. 2</figref>) can assert greater or lesser control over the wear leveling function with regard to wear leveling functions. As discussed above, parameters established by the main controller can take precedence over parameters provided by the module. The parameters provided from the controller and module can be characterized in terms of initialization parameters which are selected at <b>708</b> and operational parameters which are selected at <b>710</b> and are applicable to on-going wear leveling following initial operation. Examples of initialization parameters can include addresses that define initial extents of the memory capacity of the module in terms of total blocks, and in terms of what subset of the total number of blocks are to be initially made available for data storage. Initialization parameters can also include parameters such as section size, section allocation order, block allocation order, and the like, that serve as directives to the sequence by which steps <b>720</b>, <b>724</b>, and <b>730</b> allocate sections and blocks within the sections to the initial memory arrangement. It is noted that initialization parameter selection <b>708</b> comprises the beginning of an initialization branch <b>712</b> of the overall process. The initialization parameters are generally provided by the main controller and establish the configuration of the wear leveling function at the outset of its operation. Generally, initialization can be performed once in order to establish an original wear leveling configuration of the module. Operational parameters from the main controller can be applied to non-initialization aspects of the wear leveling function. In some embodiments, operational parameters provided by the module can define on-going wear leveling activity subsequent to initial operation on the basis of the initial parameters from the main controller. Examples of the kinds of parameters that can be included with respect to non-initialization aspects of the wear-leveling function include ordering directives for section and block allocation and reallocation. Other examples include numeric criteria on how long a block or section may contain static data before it must be reallocated, or other numeric criteria such as the number of accumulated write/erase cycles, error correction statistics, and the like, upon which wear-leveling reallocation decisions can be made.</p>
<p id="p-0118" num="0117">At <b>720</b>, initialization branch <b>712</b> continues, by performing a section allocation initialization to allocate an original set of sections among the memory devices of the module. A block allocation function <b>724</b> is then performed to allocate a block set within each section. Examples of section allocations are shown in <figref idref="DRAWINGS">FIG. 5</figref> and described above in relation thereto.</p>
<p id="p-0119" num="0118">Following <b>724</b>, an initial configuration has been established, which is stored at <b>730</b>, for example, in shared parameter section <b>322</b> of the function engine (<figref idref="DRAWINGS">FIGS. 3 and 6</figref>). Thus, the initial configuration concludes and is available for reference and updates during an operational branch <b>740</b> of the wear leveling function.</p>
<p id="p-0120" num="0119">At <b>744</b>, a retrieve configuration process is performed. Subsequent to initialization branch <b>712</b>, an initial configuration can be retrieved as established by the configuration stored at <b>730</b>. Thereafter and with ongoing operation, the configuration can be managed by operational branch <b>740</b>. During ongoing operation, current FTL table information can be retrieved via an FTL update path <b>748</b>, for example, from table area <b>620</b> (<figref idref="DRAWINGS">FIG. 6</figref>) of the function engine. FTL updates occur as a result of write and erase activity, to be described in further detail, where addition or deletion of physical addresses by the FTL function necessarily affect the wear leveling function. It should be appreciated that path <b>748</b> is bidirectional for purposes of updating the FTL function. The wear leveling function, likewise, can make changes visible to the FTL function, for example, by editing an FTL region of table area <b>620</b>. As one example, prior to a write operation, the FTL obtains a preferred location for the write from the wear leveling function. This is due to the role of the wear leveling function in defining the physical location for each new write operation from within all available locations, as will be described in further detail. An update from garbage collection function <b>612</b> is available on a garbage collection update path <b>750</b>. These updates can be in addition to normal FTL and garbage collection updates for exclusive use by the wear leveling function. Because the wear leveling function has visibility to the garbage collection function, for example, via table area <b>620</b>, the wear leveling function is able to allocate physical addresses for new writes from memory that has been returned to available status by the garbage collection function. It should be appreciated that path <b>750</b> is bidirectional for purposes of updating the garbage collection function.</p>
<p id="p-0121" num="0120">Each write operation at <b>760</b> can interface with retrieve configuration step <b>744</b> of the wear leveling function in order to establish the physical location for the write within the memory configuration. As will be further described, the wear leveling function normally communicates with both the write function (initiated by a write command from main controller <b>40</b> (<figref idref="DRAWINGS">FIG. 2</figref>) and the FTL function, hence paths <b>748</b> and <b>760</b> are bidirectional. When a new write data location is used by the write function, this new location must be added to the FTL as the physical location corresponding to the logical address for write data. If an existing logical data unit is being overwritten by the write command, then the wear leveling function must be aware that the logical overwrite corresponds (as is applicable to NAND flash) to not only a data write to a new physical address, but also to deallocation of the current physical location of the data for purposes of updating the configuration.</p>
<p id="p-0122" num="0121">An erase operation, like a write operation, involves a physical operation that invokes changes which both the wear leveling and FTL functions must accommodate. Thus, at <b>770</b>, each write operation can interface with retrieve configuration step <b>744</b> to establish the physical location for the erase within the memory configuration. It should be appreciated that path <b>770</b> is bidirectional for purposes of updating an erase function. A logical erase can involve deallocation of an existing physical data unit. A physical erase corresponds to actual erasure of a physical extent of memory that returns the subject memory to availability for new writing.</p>
<p id="p-0123" num="0122">At <b>774</b>, a section allocation step examines a current section configuration over the aggregate memory devices in the module based on the retrieved configuration and determines whether allocation of new sections is needed. Any section additions to the current section configuration are updated to the wear leveling configuration. The latter can be stored, for example, in table area <b>620</b> (<figref idref="DRAWINGS">FIG. 6</figref>) of the function engine.</p>
<p id="p-0124" num="0123">At <b>776</b>, within each section the block configuration is analyzed to determine if any currently unallocated blocks require allocation. Any additions result in additions to the appropriate parts of the wear leveling configuration. Also, changes can be referred to the FTL and garbage collection functions.</p>
<p id="p-0125" num="0124">At <b>778</b>, sections that have become full of obsolete blocks can be reallocated. In NAND flash, this would correspond to an erasure step followed by either addition of the newly erased memory to the set of available sections, or addition of the newly erased memory to vacant storage awaiting reassignment to a new section.</p>
<p id="p-0126" num="0125">At <b>780</b>, each block within a section can be assessed to determine if the block requires reallocation. Reallocation normally occurs as the result of a block being written with obsolete data (written but flagged as no longer valid), in which case the block can be erased, and the vacant block is then flagged as available for new writes. The resulting configuration is then stored at <b>730</b> and operation branch <b>740</b> ends pending the need for its re-execution.</p>
<p id="p-0127" num="0126">Referring to <figref idref="DRAWINGS">FIG. 8</figref>, an embodiment of garbage collection function <b>612</b> of <figref idref="DRAWINGS">FIG. 6</figref> is generally indicated by the reference number <b>800</b>. The garbage collection function can operate on the basis of a subset of parameters <b>801</b> available from main controller <b>40</b> at <b>804</b> and internal parameters available from the module at <b>806</b> as contained by shared parameter section <b>322</b> (<figref idref="DRAWINGS">FIG. 3</figref>) of the function engine. Irrespective of the source of the parameters, the garbage collection function is concerned with reclaiming previously written memory locations and making them accessible for subsequent writing in cooperation with the wear leveling function. As employed in this disclosure, memory is allocated for use as a hierarchy of units with pages at the lowest level. A limited number of pages are aggregated into a block. A page usually consists of a fixed number of physical storage cells though the amount of data stored in a page can vary according to the bit density configuration. Sections are made up of a number of blocks, and the number of blocks in a section may vary from section to section. Each module contains some number of sections (see <figref idref="DRAWINGS">FIG. 5</figref>) which may be allocated across the memory devices of a module in various ways.</p>
<p id="p-0128" num="0127">It should be appreciated that the operation of non-volatile memory is generally subject to rules that have to do with physical makeup of the memory devices. For purposes of the present discussion and by way of non-limiting example, rules that are generally consistent with the behavior of NAND flash memory will be applied, although the present discussions are readily adaptable by one having ordinary skill in the art with respect to other non-volatile memory technologies. The generalized rules for reading and writing memory devices such as, for example, NAND devices, then are as follows:</p>
<p id="p-0129" num="0128">a. A page is the minimum physical memory extent that may be written at one time. Writing to partial pages is not allowed.</p>
<p id="p-0130" num="0129">b. It is desirable (though not always mandatory) to collect enough data to fill all pages in a block before beginning to write the block; then, to write all the data into the block as a unit. This accommodates compensation for page-to-page interference mechanisms.</p>
<p id="p-0131" num="0130">c. A block is the minimum unit of memory that can be erased at one time.</p>
<p id="p-0132" num="0131">The rules above imply block-centric mechanisms for read and write access. As such, garbage collection steps can be responsible for retrieving storage capacity by separating obsolete pages from active data pages through consolidation of pages into blocks containing either all active data pages or all obsolete data pages. Blocks that contain nothing but obsolete data pages can be erased, and returned to an available status. It is here noted that the term &#x201c;obsolete&#x201d; in the current context refers to a unit of memory (a page, a block or a section) whose cells have been written with values but which are no longer recognized as valid by the module. This is in contrast with the alternate case of written data which is recognized as valid by the module. This latter case is referred to as &#x201c;active&#x201d; data. A data unit which currently contains no written values other than the erasure state in its cells is defined as &#x201c;available&#x201d;.</p>
<p id="p-0133" num="0132">Still referring to <figref idref="DRAWINGS">FIG. 8</figref>, subset of parameters <b>801</b> obtained from main controller <b>40</b> and module <b>30</b>, as well as other configuration parameters are applied to the wear leveling function. Other configuration parameters can include information regarding the memory configuration and the wear leveling configuration. Memory configuration includes the arrangement of sections, blocks and pages across allocated portions of the memory devices of the module, and the wear leveling configuration can include parameters involving the relationship between the garbage collection and wear leveling functions.</p>
<p id="p-0134" num="0133">Garbage collection involves a set of operations that result in the identification of obsolete pages, consolidation of obsolete pages into obsolete blocks and block erasure. The function can also operate at the level of sections, in which obsolete data units are consolidated into sections which, in turn, are migrated into available sections by erasure of the constituent blocks of the section. It should be appreciated that the garbage collection function and the erase function communicate to exchange information on updates that affect blocks in the memory. Erase function <b>802</b> informs the garbage collection function of block erasure completions that allow the garbage collection function to remove such erase blocks from a list of blocks pending erasure. At <b>803</b>, bi-directional communication is conducted between garbage collection function <b>612</b> and wear leveling function <b>800</b> via parameters <b>801</b>. For example, the wear leveling function signals both write completions and erasure completions to the garbage collection function. These notifications cause the garbage collection function to update configuration information it maintains regarding blocks pending erasure, as well as blocks being written. The garbage collection function, in turn, notifies the wear leveling function of these changes, thereby allowing the wear leveling function to order lists of available physical addresses according to priority as next locations for writing.</p>
<p id="p-0135" num="0134">The garbage collection function begins at start <b>810</b> and occurs on a section-by-section basis with the first step being retrieval of the status of the first section at <b>812</b>. At <b>814</b>, a section criterion is applied to measure the section condition as reflected by the section status. The section can: (a) be completely comprised of valid blocks; (b) be comprised of a mixture of obsolete and valid blocks; or (c) be completely comprised of obsolete blocks. If case (a) applies, there is no need for wear leveling activity in the current section, but within the section, there can be a need for garbage collection to be applied to the component blocks of that section which results in transfer of control to a block level process which begins with a get block status step <b>816</b>. If case (b) applies at <b>814</b>, the section criterion is affirmative and the number of obsolete blocks is compared with a block threshold parameter <b>818</b> that applies for the particular configuration of the current section. If the number of obsolete blocks is greater than the threshold, then a process is undertaken to reallocate the section at <b>820</b>. In an embodiment, the reallocation can involve allocation of a new section of the same configuration as the current section and copying any still valid blocks from the current section to the new section; then designating all blocks in the current section as obsolete. The current section can then be erased by erasing each of its blocks. After these steps, the current section can return to the pool of unallocated memory. If case (c) applies at <b>814</b>, the section criterion is affirmative and the number of obsolete blocks will exceed the threshold, since all of the blocks are obsolete. Reallocation at <b>820</b> can erase all of the blocks and return the current section to the pool of unallocated memory. Operation can then proceed to <b>822</b> which determines if another section is available. If not, the process ends at <b>824</b>. If another section is available, operation returns to <b>812</b> with that section becoming the current section. The process repeats until all allocated sections have been handled.</p>
<p id="p-0136" num="0135">For sections which enter the block level process at <b>816</b>, as discussed above, the block level process operates within a section. In some instances, every section may require block level processing such that completion of the block level process across the entire module can perform block-level processing successively in each of the sections of the module.</p>
<p id="p-0137" num="0136">A block criterion decision <b>830</b> invokes a page threshold parameter <b>832</b> of the number of obsolete pages within a block that calls for reallocation of the block. The block level process is analogous to the section level process in cases (a), (b), and (c), as described above with respect to the section criterion of <b>814</b> except that the obsolete units of concern are pages making up blocks instead of blocks making up sections. At <b>830</b>, if more than threshold value <b>832</b> of pages are obsolete within the current block, corresponding to cases (b) and (c), then a new block is allocated at <b>836</b> within the current section. The new block receives still-active pages from the current block. It is noted that, in order to fill up the newly allocated block with active pages, this step may be executed concurrently over multiple current blocks so that the newly-allocated block can contain active pages from multiple current blocks. The current block (or combination of current blocks) from which active pages have been moved, can then be erased accompanied by returning erased blocks to available storage within the section. Thus, a new block or blocks are allocated within the current section to receive valid pages from blocks targeted for erasure. Such allocated blocks contain active data pages.</p>
<p id="p-0138" num="0137">Upon completion of block reallocation, operation proceeds to <b>840</b> which checks for the last block. If another block is available, operation returns to <b>816</b> and assigns the next block as the current block for purposes of undergoing block level handling. If no other blocks are available, operation proceeds to <b>842</b> which tests for another section. If another section is available, operation returns to <b>812</b>. If, on the other hand, another section is not available, operation ends at <b>824</b>. Accordingly, the block level process executes for all blocks in a section undergoing block level processing, however, only blocks that contain some proportion of active data or obsolete pages are examined by the process. Blocks that contain neither active data pages nor obsolete data pages do not require examination by this process; they are however available for allocation by the process.</p>
<p id="p-0139" num="0138">Completion of the last section at <b>824</b> corresponds to completion of one application of the wear leveling function. Some embodiments can require that the wear leveling function only be executed by explicit invocation by the module; other embodiments can utilize the wear-leveling function as a continuously running background task. In the case of completion of wear leveling, either at the block level or at the section level, updates <b>750</b> (<figref idref="DRAWINGS">FIG. 7</figref>) can be applied by the garbage collection function that will be visible to and affect wear leveling function <b>700</b> (<figref idref="DRAWINGS">FIG. 7</figref>) and the FTL function which is yet to be described in detail.</p>
<p id="p-0140" num="0139">Referring to <figref idref="DRAWINGS">FIG. 9</figref>, an embodiment of bit density configuration function <b>614</b> of <figref idref="DRAWINGS">FIG. 6</figref> is generally indicated by the reference number <b>900</b> and shown as a flow diagram illustrating a function to configure the bit density of the nonvolatile memory of a module. The bit density configuration function can operate on the basis of a subset of parameters <b>902</b> available from main controller <b>40</b> at <b>904</b> and internal parameters available from the module at <b>906</b> as can also be contained by shared parameter section <b>322</b> (<figref idref="DRAWINGS">FIG. 3</figref>) of the function engine. In accordance with this embodiment, the function by which sections are allocated within a module for operation at a particular bit density is shown subsequent to initial allocation of the sections by the garbage collection function. It should be appreciated that many different bit densities can be available, and the available bit densities can be determined, at least in part, based on both memory technology of the memory devices of a module as well as the encoding/decoding function that is employed during read and write operations. Bit density configurations can vary from single bit per cell configurations to multiple-bit per cell configurations. Each bit density value in terms of bits stored per cell can also have variations that have to do with the encoding and decoding approach as well as the amount and type of additional overhead used by that encoding and decoding function. The encoding and decoding approach can involve a combination of codes as previously described that achieve results as a function of the amount of applied overhead. Convolutional codes and block codes can both operate at various levels of overhead, with the specific amount of overhead being determined by parameters applied to implementing such codes in a controller setting. The amount of overhead, in turn, is normally specified for optimum performance in the face of SNR and any other degradation mechanisms characteristic of the memory devices sections at hand. Bit density can be applied as an attribute which can apply at least down to the level of block units in NAND flash. Ultimately, the granularity of the bit density configuration can be a function of the selected encoding and decoding function as well as the memory technology in the memory devices of a module. In the example given in this disclosure, bit density configuration is set at the section level so that all blocks within a given section operate at the bit density selected for that section. It is noted that a section can be comprised by a single block; this would represent a respect to the use of NAND flash wherein individual bit density configurations are applied to data holding units as small as a single block.</p>
<p id="p-0141" num="0140">Method <b>900</b> includes an initialization branch <b>910</b> and an update branch <b>912</b>. Initialization branch <b>910</b> is executed whenever a new section is allocated (or reallocated, for example, due to garbage collection and memory reconfiguration). Update branch <b>912</b> can be executed periodically during the module lifetime for a bit density configuration that is applied to a region of physical memory underlying a section. Over the lifetime of a memory device, a particular extent of physical memory can belong to different sections at different times, although at any given time the particular extent can belong to no more than one section. For a section in active use, the bit density configuration function can monitor the condition of the physical memory making up the section. For example, the bit density configuration function monitoring some portion of physical memory operating at a particular bit density configuration can detect degradation of the physical memory such that subsequent reallocation of the same physical memory can require a bit density configuration more tolerant of a lower SNR (signal-to-noise ratio), as a result of the degradation.</p>
<p id="p-0142" num="0141">Parameters from main controller <b>40</b> (<figref idref="DRAWINGS">FIG. 2</figref>) can take precedence over those supplied by the module. In some cases, the main controller can customize most or all of the bit density configuration function performed by the function engine based on the controller parameters. In other cases, the main controller can delegate management to the module such that the bit density configuration function is defined by the module parameters. In still other cases, the governing set of parameters can be a cooperative combination of controller and module parameters. In any case, however, the actual bit density configuration function is performed by the module irrespective of the particular source of the parameters, for example, autonomously once the parameters have been established. It should be appreciated that parameter values can determine aspects of bit density with regard to selection of a bit density configuration for each section and with regard to alternation from one bit density configuration to another, and even with regard to deallocation of a section from the active memory pool (as in the case of the component memory having become &#x201c;worn out&#x201d; due to exceeding a maximum number of program/erase cycles).</p>
<p id="p-0143" num="0142">It is noted that the subset of parameters <b>902</b> is not intended as being comprehensive. Major parameter categories include targeted use modes, including capacity, density, and lifetime criteria (e.g., number of program/erase cycles). Other criteria manifested by the parameters are memory physical properties that are used by the bit density configuration function to perform allocation, reallocation and deallocation. Another parameter category defines possible configurations: this includes bit density configurations at the section level. A number of control directives may be used in conjunction with parameters defining thresholds that apply to error rates and other metrics for decisions on bit density configuration changes. These control directives can instruct the function engine to take (or not take) some action relative to a threshold criterion. As one example, when corrected error rates in pages start exceeding some defined threshold, an associated control directive can cause the garbage collection function to initiate a process on the affected section of the memory.</p>
<p id="p-0144" num="0143">With continuing reference to <figref idref="DRAWINGS">FIG. 9</figref>, initialization branch <b>910</b> applies to processes executed at the initial start of module operation. Initialization generally occurs only once due to the fact that continued bit density configuration depends upon ongoing maintenance of parameters, such as those arising from various aging and wear mechanisms during the operational lifetime of the module memory. At <b>920</b>, information is gathered in the form of memory capability data to enable decisions with respect to the capability of the particular memory in the module. Memory capability data can be organized with respect to localized portions of the memory devices in the module so that parameters (e.g., metrics) exist for each of these localized portions. The granularity of this localization can be implementation dependent and can even vary within a single module. The initial memory capability information can be gained as one result of self-testing or other function performed by the module or by the main controller for purposes of determining the capability of the memory with respect to bit density configurations. One useful parameter is a measure of the SNR (signal to noise ratio) of the memory which itself is an indicator of the number of bits the cells of the memory are capable of storing. As use history of the memory devices of a module accumulates, additional parameters (e.g., use statistics collected during updates to the bit density configuration functions) can enable subsequent decisions with respect to bit density configuration and reconfiguration for each physical extent of each memory device. Other use statistics are available, for example, from decoders <b>202</b> (<figref idref="DRAWINGS">FIG. 3</figref>). Quality metrics and related use statistics can be generated for purposes of initialization and operation, for example, as described in the above incorporated '585 Application. It should be appreciated that the general term applied in this disclosure to the parameters (e.g., set of measured information) relating to memory performance, as described above, is memory capability data.</p>
<p id="p-0145" num="0144">After an initial set of memory capability data has been acquired by initialization branch <b>910</b>, a set of bit density configuration parameters (e.g., templates) can be assembled, at <b>924</b>, which represent a bit density configuration menu for the module. The menu selections can be taken directly from or derived from a predefined set of bit density configurations which represent an overall combination of parameters for use by the encoding and decoding functions. In the present embodiment, each menu selection is applicable to a section of the memory, however, in other embodiments, menu selections can apply at the block level or lower in the memory hierarchy. A wide range of parameters is possible among the menu selections, but each parameter can define at least the number of bits in a memory cell, encoding/decoding parameters and an amount of parity to be employed.</p>
<p id="p-0146" num="0145">At <b>928</b>, a section map is created by applying the bit density configuration parameters, from <b>924</b>, in conjunction with other parameters from subset <b>902</b>. The section map is a data structure, as diagrammatically seen in <figref idref="DRAWINGS">FIG. 5</figref> that can list locations, extents, and configurations of each allocated section. The result is a set of sections mapped across the memory devices of the module (<figref idref="DRAWINGS">FIG. 5</figref>) in which each section can at least potentially be configured with a unique bit density configuration. The application of bit density configurations to the sections can encompass a wide range of variations extending from the case of every section using the same bit density configuration, for example, 4 bits per memory cell, to every section using a different bit density configuration.</p>
<p id="p-0147" num="0146">At <b>930</b>, after the section map has been created, the bit density configurations are applied as part of section allocation. This can include updates to FTL tables in shared parameter section <b>322</b> (<figref idref="DRAWINGS">FIG. 3</figref>) that define the location and extent of physical memory corresponding to a logical address. In addition, an association can be made between the bit density configuration (from one of the bit density configuration templates) and its corresponding section. This association enables subsequent reading, writing and erasure functions to identify not only the correct logical to physical mapping in order to access commands from the main controller, but also provides for correctly configuring read/write settings in the module.</p>
<p id="p-0148" num="0147">Each section as defined by section allocation <b>930</b> is ultimately comprised of a set of blocks, the number of which must be defined. At <b>934</b>, block allocation is performed for each section, for example, by updating table structures in table area <b>620</b> to accommodate the exact number of blocks that will comprise the section.</p>
<p id="p-0149" num="0148">At <b>936</b>, the section configuration for the module is stored. The section configuration can include appropriate parameters (e.g., tables and)templates to completely define the section structure of the module (at least as of the end of the initialization flow), and to enable operation of the memory cognizant of the various bit density configurations that may be involved, as well as to enable future updates as may be required by aging and wear effects arising from continued use.</p>
<p id="p-0150" num="0149">Turning now to a description of update branch <b>912</b> of the bit density configuration function, the update branch can run at intervals over the lifetime of the module in some embodiments. As is the case with the initialization process, parameters that are applied can be provided by either or both of the main controller and the module itself. Again, parameters from main controller <b>40</b> (<figref idref="DRAWINGS">FIG. 2</figref>) can take precedence over those supplied by the module. The bit density configuration function can be defined by controller provided parameters, module provided parameters or any suitable combination thereof. In any case, however, the actual bit density configuration function is performed by the module, irrespective of the particular source of the parameters. Update branch <b>912</b> is usually performed whenever changes to bit density configurations are required for an existing section. Examples include the need to deallocate and/or allocate sections due to wear leveling and garbage collection actions, or due to bit density configuration revisions necessitated by aging of the memory.</p>
<p id="p-0151" num="0150">At <b>940</b>, the update branch begins by revising the memory capability data originally developed by initialization branch <b>910</b>. Garbage collection and wear leveling functions provide inputs at <b>942</b> and <b>944</b>, respectively, to step <b>940</b> since these functions affect the memory capability data. As discussed above, the memory capability data is comprised of parameters which are indicators of the ability of the memory to store and retain data at various bit densities. With inputs from garbage collection and wear leveling functions, the memory capability data is revised. In view of the revised memory capability data, at <b>946</b>, a reallocation map is determined which reflects any changes in bit density configuration for current sections as compared to the current allocation for the current sections. This map establishes which sections are required in the new allocation (from garbage collection results) and where these sections can be placed (from wear leveling).</p>
<p id="p-0152" num="0151">Using the reallocation map from step <b>946</b>, step <b>948</b> produces an updated section map by specifying changes from the current allocation to the new allocation (where the current allocation was determined by either initialize branch <b>910</b> or a prior pass through update branch <b>912</b>) which changes are added to the current version of the section map. At <b>950</b>, the new sections are allocated to the memory as specified by the new/updated section map. The update can be accomplished, for example, by writing updates to data structures in table area <b>620</b> (<figref idref="DRAWINGS">FIG. 6</figref>). Block allocation then proceeds at <b>934</b>, as described above.</p>
<p id="p-0153" num="0152">Referring to <figref idref="DRAWINGS">FIG. 10</figref>, an embodiment of flash translation layer (FTL) function <b>616</b> of <figref idref="DRAWINGS">FIG. 6</figref> is generally indicated by the reference number <b>1000</b> and shown as a flow diagram for managing a flash translation layer relating to the nonvolatile memory of one of the modules of the present disclosure. The FTL function services main controller commands by finding the physical location for data based on a logical address provided in the command. In this regard, it noted with reference to <figref idref="DRAWINGS">FIG. 6</figref> that the activities of wear leveling function <b>610</b>, garbage collection function <b>612</b> and bit density configuration function <b>614</b> cause physical placement of stored data in a module to be dynamic. Accordingly, the FTL function is more complex than just a logical to physical address conversion for a static physical placement, but rather uses parameters generated from the other functions to determine dynamic FTL data structures corresponding to data placement in sections, blocks, and pages, so that active data units can always be located based only on their corresponding logical addresses. It is noted that these dynamic data structures can be stored in table area <b>620</b> of <figref idref="DRAWINGS">FIG. 6</figref>.</p>
<p id="p-0154" num="0153">An FTL parameters section <b>1002</b>, which can form part of shared parameter section <b>322</b> (<figref idref="DRAWINGS">FIG. 3</figref>) of the function engine, receives parameters from the main controller at <b>1004</b> and module parameters from the module at <b>1006</b>. The values of these parameters can influence a relative degree of customization of the FTL function by the main controller as opposed to the amount of customization that is exerted by the module. As is the case with other functions, as discussed above, some parameter values can cause the main controller to have greater influence on the FTL relative to the module while other parameters tend to favor influence by the module. Irrespective of the degree of influence by either the main controller or the module, the function engine can perform the FTL function, for example, autonomously based only on parameter inputs.</p>
<p id="p-0155" num="0154">FTL parameter section <b>1002</b> also receives information produced by: wear leveling function <b>700</b> (<figref idref="DRAWINGS">FIG. 7</figref>) at <b>1010</b>; garbage collection function <b>800</b> (<figref idref="DRAWINGS">FIG. 8</figref>) at <b>1012</b>; and bit density configuration function <b>900</b> (<figref idref="DRAWINGS">FIG. 9</figref>) at <b>1014</b>. An initialization branch <b>1020</b> of the FTL function executes prior to the first use of the module and before any access by the main controller in any execution of user data storage operation. At <b>1024</b>, input <b>1010</b> from the wear leveling function and input <b>1014</b> from the bit density configuration function, as reflected by FTL parameter section <b>1002</b>, allow the construction of initial data structures starting with the creation of logical table entries. These data structures can be multi-level in nature so as to provide mappings at the section, block and page levels. The mappings are also dynamic in nature since it necessarily intended they be modified as new data is written into the module and as previously written data is either invalidated due to overwrite or erased.</p>
<p id="p-0156" num="0155">At <b>1028</b>, logical to physical map entries are created for each logical address such that a physical location corresponds to each logical entry in the data structure. Because the initialization branch occurs before actual data is written into the module, the logical entries designate already-allocated, but still empty, areas in the module memory. Accordingly, the data structures at this point can refer to a set of sections which form a memory volume that will be accessible to the main controller, which as yet contains no data. It is a task of subsequent read, write and erase functions to augment these data structures with connections between logical data blocks and physical blocks and pages within the volume.</p>
<p id="p-0157" num="0156">Following initialization, at <b>1030</b>, normal operation is entered wherein the memory is accessible for storing data. It is noted that read, write and erase operations are described insofar as their interactions with the FTL function. One of ordinary skill in the art will be familiar with other aspects of these operations. A read branch <b>1034</b> of the function can execute responsive to a read command whereas at <b>1036</b> an erase/write branch of the function can execute responsive to a write command or an erase command, as will be further described.</p>
<p id="p-0158" num="0157">It should be appreciated that read branch <b>1034</b> is not required to update FTL data structures since read operations do not alter stored data. Thus, at <b>1038</b>, the read branch returns a physical address for each logical address submitted to it. Hence, when the module receives a read data command from the main controller, the resulting call to the FTL function involves only a physical address look-up operation. More than one pass through the read branch can occur responsive to a single read command, because the data requested in the read command can be comprised of data residing at multiple logical addresses.</p>
<p id="p-0159" num="0158">Erase/Write branch <b>1036</b> can be executed either when a previously unwritten data block is being written, or when the data residing at a currently written logical address is being overwritten. In the former case, the logical address or addresses must be assigned to physical locations in the memory; this necessarily requires physical memory to be designated from within existing sections to specific blocks and pages. As many passes through the erase/write branch as necessary can be executed in order to assign all data to be written by the command to corresponding physical locations. At <b>1040</b>, physical memory for a write operation can be allocated. At <b>1042</b>, the FTL tables are updated to reflect the write operation. If an erase operation is needed, in the latter case, step <b>1040</b> can account for erasure of the subject memory locations in order to invalidate data previously written to the physical address(es) corresponding to the logical address(es). Step <b>1042</b> can then update the FTL tables to designate the subject memory locations as containing invalid data preparatory to the write function. The FTL erase function can be performed in conjunction with a write command or an erase command from the main controller to the module. The FTL erase function of erase/write branch <b>1036</b> does not accomplish physical erasure of the corresponding logical address, but rather updates table structures to show that the correspondence between the logical address and physical location is no longer valid. Thus, a first pass through erase/write branch <b>1036</b> can account for the erasure in terms of the FTL structure, while a second pass can account for a corresponding write operation. Table entries can later be updated by garbage collection function <b>612</b> as it identifies obsolete physical data units, and performs physical block erasures to return the affected physical data units to availability for subsequent writing. Normal operation proceeds at <b>1044</b>.</p>
<p id="p-0160" num="0159">Turning now to <figref idref="DRAWINGS">FIG. 11</figref>, an embodiment of a read operation is shown in the form of a block diagram and is generally indicated by the reference number <b>1100</b>, for purposes of illustrating the supplemental interaction of the read operation with function engine <b>320</b> (<figref idref="DRAWINGS">FIG. 6</figref>). At <b>1102</b>, the read operation is initiated. It is noted that the mechanics of the read operation, insofar as accessing and reading individual memory cells, will be familiar to one of ordinary skill in the art and a wide variety of suitable processes can be employed for this purpose. Each read operation has been directed to a logical address. The logical address may have been obtained externally from a host as part of a command to perform a read, or it may have been obtained internally to a module. In any case, at <b>1104</b>, before actual data can be read, the read operation sends the logical address to FTL function <b>616</b> (<figref idref="DRAWINGS">FIGS. 6 and 10</figref>). At <b>1106</b>, FTL function <b>616</b>, in response to receiving a logical address from the read operation, returns a physical address to the read function. Because this is a read operation, updates to FTL tables in table section <b>620</b> (<figref idref="DRAWINGS">FIG. 6</figref>), as well as the wear leveling and garbage collection functions should not be required because no changes to the memory are being made. Having obtained a physical address, at <b>1108</b>, the read operation executes so as to obtain read data. While <figref idref="DRAWINGS">FIG. 11</figref> appears to refer to the read operation, based on the exchange of a single logical or physical address for purposes of descriptive clarity, it should be appreciated that a read operation can involves multiple addresses. Thus, the various steps of the read operation that have been illustrated can involve multiple logical to physical address translations with a corresponding read step to each of the physical addresses. At <b>1110</b>, the read operation is complete and normal operation resumes.</p>
<p id="p-0161" num="0160">Referring to <figref idref="DRAWINGS">FIG. 12</figref>, an embodiment of a write operation is shown in the form of a block diagram and is generally indicated by the reference number <b>1200</b>, for purposes of illustrating the supplemental interaction of the write operation with function engine <b>320</b> (<figref idref="DRAWINGS">FIG. 6</figref>). At <b>1202</b>, the write operation is initiated. It is noted that the mechanics of the write operation, insofar as accessing and writing to individual memory cells, will be familiar to one of ordinary skill in the art and a wide variety of suitable processes can be employed for this purpose. A write operation results in some extent of memory being written. Such an operation can be to a previously unwritten logical address or it can be to a currently written logical address. In either case, the write operation, at <b>1204</b>, sends the target logical address to FTL function <b>616</b> (<figref idref="DRAWINGS">FIGS. 6 and 10</figref>). The FTL function responds, at <b>1206</b> by returning a physical address to which write data will be physically written into the memory. Whether the write is new (to a previously unwritten logical address) or an over-write (targeting a currently written logical address), the FTL function returns a physical address to be written. Either case results in an update to the FTL configuration by the FTL function that will be visible to wear leveling function <b>610</b> (<figref idref="DRAWINGS">FIGS. 6 and 7</figref>) and garbage collection function <b>612</b> (<figref idref="DRAWINGS">FIGS. 6 and 8</figref>). At <b>1208</b>, responsive to the return of a physical address for use by the write operation, the write operation is executed. It should be appreciated that, while <figref idref="DRAWINGS">FIG. 12</figref> appears to refer to the write operation based on the exchange of a single logical or physical address for purposes of descriptive clarity, a write operation can involve multiple addresses. Thus, the various steps of the write operation that have been illustrated can involve multiple logical to physical address translations with a corresponding write step for each of the physical addresses. Likewise, FTL, wear leveling, and garbage collection parameters are updated to reflect the multiple physical addresses written by the write operation. At <b>1210</b>, normal operation resumes.</p>
<p id="p-0162" num="0161">Referring to <figref idref="DRAWINGS">FIG. 13</figref>, an embodiment of an erase operation is shown in the form of a block diagram and is generally indicated by the reference number <b>1300</b>, for purposes of illustrating the supplemental interaction of the erase operation with function engine <b>320</b> (<figref idref="DRAWINGS">FIG. 6</figref>). At <b>1302</b>, the erase operation is initiated. It is noted that the mechanics of the erase operation, insofar as accessing and erasing individual memory cells, will be familiar to one of ordinary skill in the art and a wide variety of suitable processes can be employed for this purpose.</p>
<p id="p-0163" num="0162">The result of the erase operation that is shown here is the physical erasure of an extent of memory. As such, this operation differs from a logical erasure as might result from an erase command sent from the host, although a host erase command ultimately invokes this operation. Also as such, such a physical erasure is likely to be directed from within the module rather than from the host.</p>
<p id="p-0164" num="0163">Referring to <figref idref="DRAWINGS">FIGS. 6 and 13</figref>, at <b>1304</b>, the current garbage collection configuration parameters are accessed for example from table section <b>620</b> of the function engine (<figref idref="DRAWINGS">FIG. 6</figref>) to obtain a physical block location to be erased (if available). The identification of such physical locations for erasure can be generated by garbage collection function <b>612</b> (<figref idref="DRAWINGS">FIG. 6</figref>). At <b>1306</b>, a determination is made as to whether a block is available for erasure. If no such block address is currently available, the erase procedure ends at <b>1310</b> with a return to normal operation. If a block address is available, the erase procedure advances to <b>1312</b> for purposes of obtaining the next block address that is available for erasure as specified by the garbage collection function. At <b>1314</b>, the erase function executes the erase operation to access and erase the block at the physical address obtained in <b>1312</b>. After the erasure has been completed, operation moves to <b>1316</b> which provides completion status to garbage collection function <b>612</b>, for example, via table section <b>620</b>. In an embodiment, the completion status can be provided directly to the garbage connection function which, in turn, enables the garbage collection function to update its configuration. At <b>1310</b>, the erase operation ends with a return to normal operation status.</p>
<p id="p-0165" num="0164">Referring to <figref idref="DRAWINGS">FIGS. 3 and 6</figref>, each module <b>30</b> can be configured for operation in various modes responsive to controller parameters <b>630</b>. In an embodiment, a module can operate in a first mode for a selected one of the functions based on a first set of the module input parameters from the controller and operate in a second mode for the selected function based on a second set of module input parameters from the controller with the first set of module input parameters being different from the second set of module input parameters. The parameter sets can be stored, for example, by shared parameter section <b>322</b>. A module can be configured, by way of example, to apply the first mode to a first portion (e.g., section) of the given nonvolatile memory section of the module and to apply the second mode to a second portion (e.g., section) of the given nonvolatile memory section based on the module input parameters with the first portion being different from the second portion. Thus, in <figref idref="DRAWINGS">FIG. 5</figref>, NVM <b>1</b> (section <b>502</b>) can be configured to store data at a first bit density while NVM <b>2</b> and NVM <b>4</b> (section <b>504</b>) can be configured to store data at a second bit density that is different from the first bit density based on input parameters. In an embodiment, the portions of the overall nonvolatile memory can be determined based on controller parameters <b>630</b> while the bit densities can be determined based on one or more module parameters (e.g., monitored characteristics) of the nonvolatile memory of the module. In some embodiments, the module parameters can be error correction statistics that are generated, for example, by a decoder, as will be further described below.</p>
<p id="p-0166" num="0165">In some embodiments, one or more modules can operate in a first mode for a selected one of the functions based on a first set of the module input parameters from the controller via shared parameter section <b>322</b> (<figref idref="DRAWINGS">FIGS. 3 and 6</figref>) such that each module, so configured, performs the function, for example, autonomously with no oversight from or interaction with the controller and operate in a second mode for the selected function based on a second set of module input parameters from shared parameter section <b>322</b> with the first set of module input parameters being different from the second set of module input parameters such that each module, so configured, performs the function in the second mode under at least partial control from the controller. For example, in the first mode, a module can autonomously manage a flash translation layer within table area <b>620</b> (<figref idref="DRAWINGS">FIG. 6</figref>) independent of the controller such that the local flash translation layer can be accessed by the controller but not altered while, in the second mode, a module can manage the module portion of an overall flash translation layer with some contribution from the controller such as, for example, causing changes in the module portion of the module flash translation layer since the controller has visibility to the flash translation layers of all the various modules.</p>
<p id="p-0167" num="0166">As discussed above with regard to <figref idref="DRAWINGS">FIG. 5</figref>, the present disclosure provides for block abstraction that can bridge across page boundaries. Further details with respect to block abstraction will now be provided by taking initial reference to <figref idref="DRAWINGS">FIG. 14</figref>. The latter is a diagrammatic illustration of a physical page <b>1402</b> that is provided by a typical nonvolatile memory in relation to a first user data block <b>1404</b> that is shorter than physical page <b>1402</b> as well as a second user data block <b>1406</b> that is longer than physical page <b>1402</b> wherein length represents an associated amount of data. As discussed above, the prior art generally imposes an exact match between physical page size and block size. In instances where a block is shorter than a page, filler bits can be used to fill empty space in an unfilled page, however, in an instance where a block is longer than the page, the block is not storable under the conventional constraints of the prior art. For storage under these circumstances in the prior art, at least some portion of each and every page will be unused with the size of the unused portion depending on the degree of mismatch between the block size and the larger page size. Thus, a page-sized unit of user data in the prior art case is a very specific amount of data. Each of the user data blocks of <figref idref="DRAWINGS">FIG. 14</figref>, however, are different in length than the physical page that is shown. In the case of a user data block that uses less than the total number of cells in the physical page, the prior art is able to cope with the situation since the block fits into one physical page, even though there are unused cells and therefore a loss in storage efficiency. When the user data block exceeds the extent of the physical page, the prior art is not able to cope with the situation since it is not possible to fit the user block into a single physical page.</p>
<p id="p-0168" num="0167">The present disclosure, however, sweeps aside the constraints of the prior art by storing blocks of information in non-volatile memory devices without having to size each block to fit into a single physical page. For example, user data blocks can be allocated to physical memory irrespective of how many spare bits each physical page includes. In this regard, it should be appreciated that each page is generally allocated with spare bits that can be directed, for example, to parity data, and metadata of various forms, including but not limited to logical block addresses, and miscellaneous flags, pointers and counters which can be used by the FTL, wear leveling, and garbage collection functions. Through the teachings herein, the combined user data bits plus spare bits in each physical page effectively all become storage space in each page without regard to use as user data, ECC parity or metadata. Thus, no differentiation is made in the user data blocks of <figref idref="DRAWINGS">FIG. 14</figref> between actual user data and overhead data. A block of user information with associated overhead, however, can be smaller than physical page <b>1402</b>, as is the case with first user data block <b>1404</b>, or larger than physical page <b>1402</b>, as is the case with second user data block <b>1406</b>. As will be seen, in either case, a multiple user-block sequence can be fitted to a group of physical pages with no correspondence between the number of user blocks and the number of physical pages. By way of non-limiting example, a user data block can be defined as one or more 512-byte units (sectors) with associated overhead; whether a block is made up of one sector or multiple sectors is not of significance since the block can be of any desired size. The origin (e.g., starting) point of each block within a particular page is characterized by an offset which is designated. Tracking of the offset can be implemented in firmware, hardware, or any suitable combination of both. When a user block extends beyond one physical page, a remaining portion of the block is allocated to a subsequent physical page. When this happens, the location of the break within the user block is tracked by the application in the same way that user block origins are tracked, as will be discussed in further detail below.</p>
<p id="p-0169" num="0168">Attention is now directed to <figref idref="DRAWINGS">FIG. 15</figref> which is a block diagram, generally indicated by the reference number <b>1500</b>, diagrammatically illustrating four successive physical pages PP<b>1</b>-PP<b>4</b> forming part of a nonvolatile memory. In being successive physical pages, this set of physical pages can be physically addressed in a successive manner, for example, by incrementing the physical address by a given amount from one page to the next. In this example, user data blocks B<b>1</b>-B<b>4</b> are used, each having a length as exemplified by first user data block <b>1404</b> of <figref idref="DRAWINGS">FIG. 14</figref> wherein the user data block is shorter than the physical page. For purposes of illustrative clarity, it is assumed that a block length BL is equal to 0.75 of a page length PL. As seen, block B<b>1</b> is completely contained by physical page PP<b>1</b>. Block B<b>2</b> includes a first portion <b>1510</b> stored in a final quarter of physical page PP<b>1</b> at an offset from the beginning of the page that corresponds to the length of block B<b>1</b> (i.e., &#xbe; PL) and a second portion <b>1512</b> that is stored in physical page PP<b>2</b>, starting from the beginning of PP<b>2</b> so as to fill the first half of PP<b>2</b> and representing the final &#x2154; of block B<b>2</b>. An initial portion <b>1520</b> of block B<b>3</b> is stored in the second half of physical page PP<b>2</b> at an offset of &#xbd; PL from the beginning of PP<b>2</b> while a final &#x2153; portion <b>1522</b> of block B<b>3</b> is stored in the first &#xbc; of physical page PP<b>3</b>. Block B<b>4</b> fills a remaining portion <b>1524</b> of physical page PP<b>3</b>. Block B<b>5</b> then fills an initial &#xbe; portion <b>1526</b> of physical page PP<b>4</b> at an offset of &#xbc; PL from the beginning of PP<b>4</b>. A final &#xbc; portion <b>1528</b> of PP<b>4</b> is shown as empty, however, it can be filled by a portion of a subsequent block or remain empty. In the present example, five user blocks are stored on four physical pages. As can be seen, physical pages can be completely utilized even though the block length is shorter than the page length. Moreover, blocks can be stored in a manner that bridges page boundaries. Storage, as exemplified by <figref idref="DRAWINGS">FIG. 15</figref>, can continue for an arbitrary number of user data blocks, or until the last user data block in a sequence is stored. In some cases, the last physical page in a sequence can contain unused memory cells, but all prior physical pages can be completely utilized with a net increase of efficiency, as compared to the prior art case.</p>
<p id="p-0170" num="0169"><figref idref="DRAWINGS">FIG. 16</figref> is a block diagram, generally indicated by the reference number <b>1600</b>, diagrammatically illustrating four successive physical pages PP<b>1</b>-PP<b>4</b> forming part of a nonvolatile memory. In this example, user data blocks B<b>1</b>-B<b>3</b> are used, each having a length as exemplified by second user data block <b>1406</b> of <figref idref="DRAWINGS">FIG. 14</figref> wherein the user data block is longer than the physical page. The prior art, of course, is incapable of storing the relatively longer blocks. For purposes of illustrative clarity in the present example, it is assumed that block length BL is equal to 1.25 of page length PL. As seen, an initial block B<b>1</b> is partially contained by physical page PP<b>1</b> with a final portion <b>1610</b> (20 percent) of B<b>1</b> contained by physical page PP<b>2</b>. An initial portion <b>1612</b> of block B<b>2</b> fills the remainder of physical page PP<b>2</b> at an offset of &#xbc; PL from the beginning of PP<b>2</b> and a final portion <b>1614</b> of block B<b>2</b> fills the first 50 percent of physical page PP<b>3</b>. An initial portion <b>1616</b> of block B<b>3</b> fills the final 50 percent of physical page PP<b>3</b> at an offset of &#xbd; PL from the beginning of PP<b>3</b> and a final portion <b>1618</b> of block B<b>3</b> fills an initial 75 percent of physical page PP<b>4</b>. The final 25 percent of PP<b>4</b> is unused if no more blocks are available or can be used by a subsequent block. In the present example, three user blocks are stored on four physical pages. Physical pages can be completely utilized even though the block length is longer than the page length. Again, blocks can be stored in a manner that bridges page boundaries. In this regard, any relationship of the user data block size to the physical page size can be accommodated based on the teachings that have been brought to light herein. The flexibility that is provided can increase or maximize storage efficiency over what is possible in the prior art. Such flexibility provides the ability to change user block sizes such as, for example, to add ECC parity to the user block format without requiring a corresponding increase in the size of the physical page in a given memory. Storage efficiency can be optimized by a tendency to fully utilize physical page resources in a given memory device.</p>
<p id="p-0171" num="0170"><figref idref="DRAWINGS">FIG. 17</figref> is a flow diagram, generally indicated by the reference number <b>1700</b>, which illustrates an embodiment of a method for storing/writing user data blocks in a way which achieves the data structures described above in the context of <figref idref="DRAWINGS">FIGS. 15 and 16</figref>. Initially, it is noted that a sequence or series of user data blocks comprising the write operation is receivable by the process, as illustrated by <figref idref="DRAWINGS">FIGS. 15 and 16</figref>, wherein the block length and page length can be different. The method begins at <b>1702</b> and moves to <b>1704</b> which retrieves an initial user data block at the outset of the process. At <b>1706</b>, the current user data block is mapped onto a physical page at some offset from the beginning of the physical page which can be a zero or non-zero offset value. The offset can be stored, for example, as a value in prior block metadata or as an address contained in tables created and used by the FTL function. At <b>1710</b>, a determination is made as to whether the current user data block is the last block in the write transfer. If not, operation proceeds to <b>1712</b>, which tests whether the current page is now full. If the current page is full, operation transfers to <b>1714</b> which then writes the page. Operation then returns to <b>1704</b>. At <b>1712</b>, if the page is not full operation returns to <b>1704</b>. Returning again to <b>1710</b>, if the last user block is detected, operation proceeds to <b>1716</b> which fills any remaining space in the current page, if present. At <b>1718</b> the last page is written and operation concludes at <b>1720</b> pending the next write operation.</p>
<p id="p-0172" num="0171"><figref idref="DRAWINGS">FIG. 18</figref> is another flow diagram, generally indicated by the reference number <b>1800</b>, which illustrates an embodiment of a method for reading user data blocks from the data structures described above in the context of <figref idref="DRAWINGS">FIGS. 15 and 16</figref> and in which the block length and page length can be different. The method begins at <b>1802</b> and moves to <b>1804</b> which retrieves an initial physical page at the outset of the process. At <b>1806</b>, the retrieved page is mapped to user blocks such that the user blocks are recovered. The mapping, for purposes of retrieving the user blocks can be based, for example, on offsets created during the write operation (<figref idref="DRAWINGS">FIG. 17</figref>) and stored as metadata with a prior block or as addresses stored in tables created and used by the FTL function. A test is made at <b>1808</b> to determine if the recovery of user data blocks is complete. If the recovery is complete, recovered blocks are transferred at <b>1810</b>. On the other hand, however, it should be appreciated that <b>1808</b> may identify a user data block is incomplete due to wrapping of the block from one physical page to the next physical page (see, for example, block B<b>2</b> in <figref idref="DRAWINGS">FIG. 15</figref> and block B<b>1</b> in <figref idref="DRAWINGS">FIG. 16</figref>). In this case, operation returns to <b>1804</b> for purposes of reading the next page which will contain the remainder of the incomplete block. Mapping <b>1806</b> will reassemble the incomplete block and pass the now complete block on to send block <b>1810</b>. At <b>1812</b>, a test determines whether the last user block of the current read operation has been recovered. If not, operation returns to <b>1804</b> to read the next page. If the last user block has been recovered, the current read operation ends at <b>1816</b>, awaiting the next read operation.</p>
<p id="p-0173" num="0172">In view of the foregoing, a highly flexible read/write arrangement and associated method are provided for use in accessing at least one solid state memory device in read/write operations with the memory device being made up of a plurality of memory cells which memory cells are organized as a set of pages that are physically and sequentially addressable with each page having a page length such that a page boundary is defined between successive ones of the pages in said set. The system provides a control arrangement that is configured for storing and accessing a group of data blocks that is associated with a given write operation in a successive series of the pages such that at least an initial page in the series is filled and where each block of the group can include a block length that is different than the page length. It should be appreciated that at least one of the blocks can be stored at an offset from any page boundary. In the instance of a series of physical pages having an initial group of two or more pages and a final page, the system can fill at least each page of the initial group of pages based on the group of data blocks. The final page can be filled or partially filled. If the final page is partially filled, the available space can be filled by at least a portion of another block. Physical pages can contain one or more block boundaries with the start of blocks identified, for example, by offsets from the beginning of the physical page. Stated in a slightly different way, a particular block in a group of data blocks can be stored such that a first portion of the particular block is stored in a first page and a second portion of the particular block is stored in a second page such that storage of the particular block crosses a page boundary between the first page and the second page. For purposes of this discussion, the first page and second page can be any two adjacent physical pages within the data structures of <figref idref="DRAWINGS">FIGS. 15 and 16</figref>.</p>
<p id="p-0174" num="0173">It should be appreciated that the SSD of the present disclosure and the associated modules described in detail herein, provide a system with distributed functionality that decentralizes activity that, in prior art approaches, tends toward being highly centralized. The attribute of scalability is more easily attained with decentralized (distributed) functionally than with centralized functionality. That is, distributed components with decentralized functionality, such as the modules previously described, provide for limited incremental addition of system overhead as a given system is scaled up. In contrast, a centralized system must increase the capability of its centralized controller as additional as system complexity increases with the addition of storage devices. With centralized embedded microprocessors and embedded software, the ability to manage resources becomes limited by the constraints of the computational and other abilities of a central controller. Hence scalability as an attribute is easier to achieve for systems that add functionality at lower levels of their functional hierarchies as compared with systems that attempt to add the same functionality as a centralized capability.</p>
<p id="p-0175" num="0174">The foregoing description of the invention has been presented for purposes of illustration and description. It is not intended to be exhaustive or to limit the invention to the precise form or forms disclosed, and other modifications and variations may be possible in light of the above teachings wherein those of skill in the art will recognize certain modifications, permutations, additions and sub-combinations thereof.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A read/write apparatus for use in accessing at least one nonvolatile memory device in read/write operations with the memory device being made up of a plurality of memory cells which memory cells are organized as a set of pages that are physically and sequentially addressable with each page having a page length such that a page boundary is defined between successive ones of the pages in the set, said read/write apparatus comprising:
<claim-text>a control apparatus configured for
<claim-text>retrieving an initial page to access at least one portion of one data block of a group of data blocks that is associated with a prior write operation stored in a subset of the pages including an initial group of two or more pages and a final page such that at least each page of the initial group of pages is filled based on the group of data blocks without writing a header to each page of the initial group of pages and the final page, and each data block of the group includes a block length that is different than the page length;</claim-text>
<claim-text>detecting the presence of an incomplete data block in the initial page;</claim-text>
<claim-text>responsive to detecting the incomplete data block, retrieving an additional page from the memory device; and</claim-text>
<claim-text>mapping the additional page to complete the incomplete data block.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The read/write apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the control apparatus is configured to retrieve at least one of the blocks at an offset from any page boundary.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The read/write apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the control apparatus is configured to store at least a particular page in the initial group of pages such that the particular page includes at least a final portion of a first data block and at least an initial portion of a second data block, and the particular page contains a boundary between the first data block and the second data block.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The read/write apparatus of <claim-ref idref="CLM-00003">claim 3</claim-ref> wherein the control apparatus is configured to establish an offset of the second data block in the particular page.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The read/write apparatus of <claim-ref idref="CLM-00003">claim 3</claim-ref> wherein the control apparatus is configured to store a particular block in the group of data blocks such that a first portion of the particular block is stored in a first page and a second portion of the particular block is stored in a second page such that storage of the particular block crosses a page boundary between the first page and the second page.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The read/write apparatus of <claim-ref idref="CLM-00004">claim 4</claim-ref> configured to maintain a translation layer at least for the nonvolatile memory and for designating the offset in the translation layer.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The read/write apparatus of <claim-ref idref="CLM-00006">claim 6</claim-ref> including a table for maintaining the translation layer therein and the table is external to the non-volatile memory device.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. A memory system including the read/write apparatus of <claim-ref idref="CLM-00004">claim 4</claim-ref> and further comprising a module which includes the nonvolatile memory device and a controller in data communication with the module.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The memory system of <claim-ref idref="CLM-00008">claim 8</claim-ref> wherein the module is configured to maintain a translation layer at least for the nonvolatile memory and to designate the offset in the translation layer.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The memory system of <claim-ref idref="CLM-00009">claim 9</claim-ref> wherein the translation layer is limited to address translations relating to the nonvolatile memory of the module.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The memory system of <claim-ref idref="CLM-00009">claim 9</claim-ref> wherein the translation layer is directed by the controller with the controller providing at least one of a parameter and a command to the module.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The read/write apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the memory device includes NAND flash memory.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The read/write apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref> including a garbage collection function for identifying the successive series of pages for storing the group of data blocks.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The read/write apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the block length is greater than the page length such that the initial page holds no more than the one portion of the one data block as the incomplete data block and the additional page holds at least a remaining portion of the incomplete data block.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The read/write apparatus of <claim-ref idref="CLM-00014">claim 14</claim-ref> wherein the additional page is adjacent to the initial page.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. A method for accessing at least one non-volatile memory device in read/write operations with the memory device being made up of a plurality of memory cells which memory cells are organized as a set of pages that are physically and sequentially addressable with each page having a page length such that a page boundary is defined between successive ones of the pages in the set, said method comprising:
<claim-text>retrieving an initial page to access at least a portion of one data block of a group of data blocks that is associated with a prior write operation stored in a subset of the pages including an initial group of pages based on the group of data blocks without writing a header to each page of the initial group of pages and the final page such that at least each page of the initial group of pages in the subset is filled and each data block of the group includes a block length that is different than the page length;</claim-text>
<claim-text>detecting the presence of an incomplete data block in the initial page;</claim-text>
<claim-text>responsive to detecting the incomplete data block, retrieving an additional page from the memory device; and</claim-text>
<claim-text>mapping the additional page to complete the incomplete data block.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref> wherein mapping comprises retrieving at least one of the blocks at an offset from any page boundary.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref> including, during the prior write operation, storing at least a particular page in the initial group of pages such that the particular page includes at least a final portion of a first data block and at least an initial portion of a second data block, and the particular page contains a boundary between the first data block and the second data block.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The method of <claim-ref idref="CLM-00018">claim 18</claim-ref> including establishing an offset of the second data block in the particular page.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The method of <claim-ref idref="CLM-00018">claim 18</claim-ref> including, during the prior write operation, storing a particular block in the group of data blocks such that a first portion of the particular block is stored in a first page and a second portion of the particular block is stored in a second page such that storage of the particular block crosses a page boundary between the first page and the second page.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The method of <claim-ref idref="CLM-00019">claim 19</claim-ref> including maintaining a translation layer at least for the nonvolatile memory and designating the offset in the translation layer.</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The method of <claim-ref idref="CLM-00021">claim 21</claim-ref> wherein maintaining includes storing the translation layer in a table that is external to the non-volatile memory device.</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The method of <claim-ref idref="CLM-00021">claim 21</claim-ref> including providing a module that includes the nonvolatile memory device and a controller in data communication with the module and maintaining the translation layer in the module.</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. The method of <claim-ref idref="CLM-00023">claim 23</claim-ref> including limiting the translation layer to address translations relating to the nonvolatile memory of the module.</claim-text>
</claim>
<claim id="CLM-00025" num="00025">
<claim-text>25. The method system of <claim-ref idref="CLM-00023">claim 23</claim-ref> including directing the translation layer using the controller with the controller providing at least one of a parameter and a command to the module.</claim-text>
</claim>
<claim id="CLM-00026" num="00026">
<claim-text>26. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref> wherein the memory device includes NAND flash memory, and the storing and accessing is based on a set rules for using NAND flash memory.</claim-text>
</claim>
<claim id="CLM-00027" num="00027">
<claim-text>27. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref> including performing a garbage collection function to identify the successive series of pages for storing the group of data blocks.</claim-text>
</claim>
<claim id="CLM-00028" num="00028">
<claim-text>28. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref> wherein the block length is greater than the page length such that no more than the one portion of the one data block is retrieved from the initial page as the incomplete data block and mapping comprises recovering the remaining portion of the incomplete data block from the additional page.</claim-text>
</claim>
<claim id="CLM-00029" num="00029">
<claim-text>29. The method of <claim-ref idref="CLM-00028">claim 28</claim-ref> further comprising locating the additional page adjacent to the initial page.</claim-text>
</claim>
<claim id="CLM-00030" num="00030">
<claim-text>30. A read/write apparatus for use in accessing at least one nonvolatile memory device in read/write operations with the memory device being made up of a plurality of memory cells which memory cells are organized as a set of pages that are physically and sequentially addressable with each page having a page length such that a page boundary is defined between successive ones of the pages in the set, said read/write apparatus comprising:
<claim-text>a control apparatus configured for
<claim-text>retrieving an initial page to access at least one portion of one data block of a group of data blocks that is associated with a prior write operation stored in a subset of the pages including an initial group of two or more pages and a final page such that at least each page of the initial group of pages is filled during the prior write operation based on the group of data blocks without writing a header to each page and each data block includes a block length that is different than the page length;</claim-text>
<claim-text>detecting the presence of an incomplete data block in the initial page;</claim-text>
<claim-text>responsive to detecting the incomplete data block, retrieving an additional page from the memory device; and</claim-text>
<claim-text>mapping the additional page to complete the incomplete data block.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00031" num="00031">
<claim-text>31. A method for accessing at least one non-volatile memory device in read/write operations with the memory device being made up of a plurality of memory cells which memory cells are organized as a set of pages that are physically and sequentially addressable with each page having a page length such that a page boundary is defined between successive ones of the pages in the set, said method comprising:
<claim-text>retrieving an initial page to access at least a portion of one data block of a group of data blocks that is associated with a prior write operation stored in a subset of the pages including an initial group of two or more pages and a final page and filling at least each page of the initial group of pages based on the group of data blocks without writing a header to each page during a prior write operation and each data block of the group includes a block length that is different than the page length;</claim-text>
<claim-text>detecting the presence of an incomplete data block in the initial page;</claim-text>
<claim-text>responsive to detecting the incomplete data block, retrieving an additional page from the memory device; and</claim-text>
<claim-text>mapping the additional page to complete the incomplete data block. </claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
