<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625898-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625898</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13198140</doc-number>
<date>20110804</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>JP</country>
<doc-number>2011-039030</doc-number>
<date>20110224</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>215</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>46</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>48</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>62</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382181</main-classification>
<further-classification>382190</further-classification>
<further-classification>382195</further-classification>
<further-classification>382199</further-classification>
<further-classification>382203</further-classification>
<further-classification>382209</further-classification>
</classification-national>
<invention-title id="d2e71">Computer-readable storage medium, image recognition apparatus, image recognition system, and image recognition method</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>4783829</doc-number>
<kind>A</kind>
<name>Miyakawa et al.</name>
<date>19881100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5420971</doc-number>
<kind>A</kind>
<name>Westerink et al.</name>
<date>19950500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5572603</doc-number>
<kind>A</kind>
<name>Koike</name>
<date>19961100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5870501</doc-number>
<kind>A</kind>
<name>Kim</name>
<date>19990200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6005976</doc-number>
<kind>A</kind>
<name>Naoi et al.</name>
<date>19991200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6094508</doc-number>
<kind>A</kind>
<name>Acharya et al.</name>
<date>20000700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6366358</doc-number>
<kind>B1</kind>
<name>Satou et al.</name>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>7274380</doc-number>
<kind>B2</kind>
<name>Navab et al.</name>
<date>20070900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345633</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>7519218</doc-number>
<kind>B2</kind>
<name>Takemoto et al.</name>
<date>20090400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382165</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>7676079</doc-number>
<kind>B2</kind>
<name>Uchiyama et al.</name>
<date>20100300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382154</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>7812871</doc-number>
<kind>B2</kind>
<name>Takemoto et al.</name>
<date>20101000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348286</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>7881560</doc-number>
<kind>B2</kind>
<name>John</name>
<date>20110200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382287</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>8073201</doc-number>
<kind>B2</kind>
<name>Satoh et al.</name>
<date>20111200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382106</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2002/0051570</doc-number>
<kind>A1</kind>
<name>Inagaki</name>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2002/0106051</doc-number>
<kind>A1</kind>
<name>Menhardt</name>
<date>20020800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2004/0234124</doc-number>
<kind>A1</kind>
<name>Nakai et al.</name>
<date>20041100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>2005/0036673</doc-number>
<kind>A1</kind>
<name>Ohba et al.</name>
<date>20050200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>2005/0041871</doc-number>
<kind>A1</kind>
<name>Abe</name>
<date>20050200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>2005/0094900</doc-number>
<kind>A1</kind>
<name>Abe</name>
<date>20050500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>2005/0248580</doc-number>
<kind>A1</kind>
<name>Osako</name>
<date>20051100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>2006/0264746</doc-number>
<kind>A1</kind>
<name>Frisa et al.</name>
<date>20061100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>2007/0040800</doc-number>
<kind>A1</kind>
<name>Forlines et al.</name>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>2007/0139321</doc-number>
<kind>A1</kind>
<name>Takemoto et al.</name>
<date>20070600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345 87</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>2007/0146325</doc-number>
<kind>A1</kind>
<name>Poston et al.</name>
<date>20070600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>2007/0206875</doc-number>
<kind>A1</kind>
<name>Ida et al.</name>
<date>20070900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>2009/0079740</doc-number>
<kind>A1</kind>
<name>Fitzmaurice et al.</name>
<date>20090300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>2009/0085911</doc-number>
<kind>A1</kind>
<name>Fitzmaurice et al.</name>
<date>20090400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>2009/0110241</doc-number>
<kind>A1</kind>
<name>Takemoto et al.</name>
<date>20090400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>2010/0048290</doc-number>
<kind>A1</kind>
<name>Baseley et al.</name>
<date>20100200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>463 25</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>2010/0188344</doc-number>
<kind>A1</kind>
<name>Shirakawa et al.</name>
<date>20100700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>US</country>
<doc-number>2011/0081048</doc-number>
<kind>A1</kind>
<name>Woo et al.</name>
<date>20110400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>US</country>
<doc-number>2011/0090252</doc-number>
<kind>A1</kind>
<name>Yoon et al.</name>
<date>20110400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345633</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00033">
<document-id>
<country>US</country>
<doc-number>2011/0157179</doc-number>
<kind>A1</kind>
<name>Fahn et al.</name>
<date>20110600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345427</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00034">
<document-id>
<country>US</country>
<doc-number>2011/0305368</doc-number>
<kind>A1</kind>
<name>Osako</name>
<date>20111200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00035">
<document-id>
<country>JP</country>
<doc-number>61-201385</doc-number>
<date>19860900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00036">
<document-id>
<country>JP</country>
<doc-number>3-99377</doc-number>
<date>19910400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00037">
<document-id>
<country>JP</country>
<doc-number>5-324830</doc-number>
<date>19931200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00038">
<document-id>
<country>JP</country>
<doc-number>09-079847</doc-number>
<date>19970300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00039">
<document-id>
<country>JP</country>
<doc-number>10-116344</doc-number>
<date>19980500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00040">
<document-id>
<country>JP</country>
<doc-number>2962556</doc-number>
<date>19990800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00041">
<document-id>
<country>JP</country>
<doc-number>2001-319239</doc-number>
<date>20011100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00042">
<document-id>
<country>JP</country>
<doc-number>2003-281504</doc-number>
<date>20031000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00043">
<document-id>
<country>JP</country>
<doc-number>2004-206672</doc-number>
<date>20040700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00044">
<document-id>
<country>JP</country>
<doc-number>2005-242600</doc-number>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00045">
<document-id>
<country>JP</country>
<doc-number>2009-20614</doc-number>
<date>20090100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00046">
<document-id>
<country>JP</country>
<doc-number>2010-267113</doc-number>
<date>20101100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00047">
<othercit>Wu et al., &#x201c;Research of quickly identifying markers on augmented reality&#x201d;, IEEE ICAMS 2010, vol. 3, pp. 671-675, Jul. 2010.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00048">
<othercit>Hirokazu Kato, Mark Billinghurst, Koichi Asano, and Keihachiro Tachibana, &#x201c;An Augmented Reality System and its Calibration based on Marker Tracking&#x201d;, Journal of the Virtual Reality Society of Japan, vol. 4, No. 4, 1999.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00049">
<othercit>Mar. 27, 2013 Office Action in U.S. Appl. No. 13/196,372, 10 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00050">
<othercit>Apr. 26, 2013 Office Action in U.S. Appl. No. 13/194,559, 13 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00051">
<othercit>Mar. 26, 2013 Japanese Office Action (4 pages) for JP 2012-11774, with English translation (5 pages).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00052">
<othercit>Kato, &#x201c;ARToolKit Library for Vision-based Augmented Reality&#x201d;, The Institute of Electronics, Information and Communication Engineers, Technical Report of IEICE issued on Feb. 14, 2002, vol. 101, No. 652, pp. 79-86, PRMU 2001-232, 9 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00053">
<othercit>Office Action dated Aug. 9, 20013, for U.S. Appl. No. 13/193,876, 20 pgs.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00054">
<othercit>Jun. 24, 2013 Office Action in U.S. Appl. No. 13/193,895, 31 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00055">
<othercit>Jul. 12, 2013 Office Action in U.S. Appl. No. 13/196,299, 13 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00056">
<othercit>Feiner et al., &#x201c;Windows on the World: 2D Windows for 3D Augmented Reality&#x201d;, Proceedings of the 6<sup>th </sup>Annual ACM Symposium on User Interface Software and Technology, UIST'93, Jan. 1, 1993, pp. 145-155, 11 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00057">
<othercit>Park et al., &#x201c;Jitter Suppression in Model-based Camera Tracking&#x201d;, Virtual Systems and Multimedia (VSMM), 2010 16<sup>th </sup>International Conference on, IEEE, Oct. 20, 2010, pp. 204-211, 8 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00058">
<othercit>Ohshima et al., &#x201c;AR<sup>2 </sup>Hockey: A Case Study of Collaborative Augmented Reality&#x201d;, Virtual Reality Annual International Symposium, 1998, Proceedings, IEEE, Mar. 14-18, 1998, IEEE Computer Society, Mar. 14, 1998, pp. 268-275, 8 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00059">
<othercit>Salti et al., &#x201c;SVR-Based Jitter Reduction for Markerless Augmented Reality&#x201d;, Image Analysis and Proceeding A ICIAP, 2009, Sep. 8, 2009, pp. 24-33, 10 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00060">
<othercit>Gordon et al., &#x201c;What and Where: 3D Object Recognition with Accurate Pose&#x201d;, Jan. 1, 2007, Toward Category&#x2014;Level Object Recognition Lecture Notes in Computer Science; LNCS, pp. 67-82, 16 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00061">
<othercit>U.S. Appl. No. 13/194,559, published Aug. 30, 2012</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00062">
<othercit>U.S. Appl. No. 13/193,876, published Aug. 30, 2012.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00063">
<othercit>U.S. Appl. No. 13/193,895, published Aug. 30, 2012.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00064">
<othercit>U.S. Appl. No. 13/196,372, published Aug. 30, 2012.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00065">
<othercit>U.S. Appl. No. 13/196,299, published Aug. 30, 2012.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00066">
<othercit>Jun. 8, 2012 Japanese Office Action for JP2011-039030, 2 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>19</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382103</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382181</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382190-197</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382199-206</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382209</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382216-222</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>43</number-of-drawing-sheets>
<number-of-figures>82</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120219228</doc-number>
<kind>A1</kind>
<date>20120830</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Osako</last-name>
<first-name>Satoru</first-name>
<address>
<city>Kyoto</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Maruko</last-name>
<first-name>Ryota</first-name>
<address>
<city>Kyoto</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Osako</last-name>
<first-name>Satoru</first-name>
<address>
<city>Kyoto</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Maruko</last-name>
<first-name>Ryota</first-name>
<address>
<city>Kyoto</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
</inventors>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Nintendo Co., Ltd.</orgname>
<role>03</role>
<address>
<city>Kyoto</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Bhatnagar</last-name>
<first-name>Anand</first-name>
<department>2668</department>
</primary-examiner>
<assistant-examiner>
<last-name>Park</last-name>
<first-name>Soo</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">First, a plurality of vertices of a contour of an object or of a design are detected from an image. Then, a predetermined number of division points are generated on each of sides connecting the plurality of detected vertices, so as to divide each side of at least one pair of two opposing sides into unequal parts. Then, a plurality of sample points are determined on the basis of straight lines connecting the division points on the two opposing sides to one another, and on the basis of pixel values of the sample points, it is determined whether or not a predetermined object or design is displayed in an area surrounded by the plurality of vertices in the image.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="103.29mm" wi="154.94mm" file="US08625898-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="241.22mm" wi="174.58mm" file="US08625898-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="217.17mm" wi="154.94mm" file="US08625898-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="206.67mm" wi="139.36mm" file="US08625898-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="250.36mm" wi="177.29mm" file="US08625898-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="224.96mm" wi="184.07mm" file="US08625898-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="219.54mm" wi="173.91mm" file="US08625898-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="217.51mm" wi="164.08mm" file="US08625898-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="214.04mm" wi="158.75mm" file="US08625898-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="219.54mm" wi="158.75mm" file="US08625898-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="173.91mm" wi="150.54mm" file="US08625898-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="164.76mm" wi="158.67mm" file="US08625898-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="219.96mm" wi="158.75mm" file="US08625898-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="234.95mm" wi="152.65mm" file="US08625898-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="241.22mm" wi="162.39mm" file="US08625898-20140107-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="252.73mm" wi="165.10mm" file="US08625898-20140107-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00016" num="00016">
<img id="EMI-D00016" he="251.38mm" wi="159.34mm" file="US08625898-20140107-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00017" num="00017">
<img id="EMI-D00017" he="184.83mm" wi="168.15mm" file="US08625898-20140107-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00018" num="00018">
<img id="EMI-D00018" he="234.78mm" wi="178.65mm" file="US08625898-20140107-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00019" num="00019">
<img id="EMI-D00019" he="226.65mm" wi="173.57mm" file="US08625898-20140107-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00020" num="00020">
<img id="EMI-D00020" he="226.99mm" wi="169.16mm" file="US08625898-20140107-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00021" num="00021">
<img id="EMI-D00021" he="252.48mm" wi="176.78mm" file="US08625898-20140107-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00022" num="00022">
<img id="EMI-D00022" he="255.52mm" wi="178.73mm" file="US08625898-20140107-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00023" num="00023">
<img id="EMI-D00023" he="250.44mm" wi="177.80mm" file="US08625898-20140107-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00024" num="00024">
<img id="EMI-D00024" he="211.58mm" wi="176.78mm" file="US08625898-20140107-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00025" num="00025">
<img id="EMI-D00025" he="241.22mm" wi="176.95mm" file="US08625898-20140107-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00026" num="00026">
<img id="EMI-D00026" he="251.04mm" wi="176.95mm" file="US08625898-20140107-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00027" num="00027">
<img id="EMI-D00027" he="226.31mm" wi="185.08mm" file="US08625898-20140107-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00028" num="00028">
<img id="EMI-D00028" he="239.52mm" wi="173.91mm" file="US08625898-20140107-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00029" num="00029">
<img id="EMI-D00029" he="231.73mm" wi="173.23mm" file="US08625898-20140107-D00029.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00030" num="00030">
<img id="EMI-D00030" he="247.65mm" wi="175.26mm" file="US08625898-20140107-D00030.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00031" num="00031">
<img id="EMI-D00031" he="254.42mm" wi="182.03mm" file="US08625898-20140107-D00031.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00032" num="00032">
<img id="EMI-D00032" he="232.16mm" wi="165.61mm" file="US08625898-20140107-D00032.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00033" num="00033">
<img id="EMI-D00033" he="207.01mm" wi="159.68mm" file="US08625898-20140107-D00033.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00034" num="00034">
<img id="EMI-D00034" he="220.90mm" wi="164.08mm" file="US08625898-20140107-D00034.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00035" num="00035">
<img id="EMI-D00035" he="227.67mm" wi="152.57mm" file="US08625898-20140107-D00035.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00036" num="00036">
<img id="EMI-D00036" he="237.49mm" wi="158.67mm" file="US08625898-20140107-D00036.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00037" num="00037">
<img id="EMI-D00037" he="166.79mm" wi="125.81mm" file="US08625898-20140107-D00037.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00038" num="00038">
<img id="EMI-D00038" he="251.71mm" wi="168.83mm" file="US08625898-20140107-D00038.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00039" num="00039">
<img id="EMI-D00039" he="246.97mm" wi="166.79mm" file="US08625898-20140107-D00039.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00040" num="00040">
<img id="EMI-D00040" he="246.63mm" wi="173.57mm" file="US08625898-20140107-D00040.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00041" num="00041">
<img id="EMI-D00041" he="184.40mm" wi="145.80mm" file="US08625898-20140107-D00041.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00042" num="00042">
<img id="EMI-D00042" he="242.91mm" wi="171.20mm" file="US08625898-20140107-D00042.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00043" num="00043">
<img id="EMI-D00043" he="212.43mm" wi="170.52mm" file="US08625898-20140107-D00043.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATION</heading>
<p id="p-0002" num="0001">The disclosure of Japanese Patent Application No. 2011-039030, filed on Feb. 24, 2011, is incorporated herein by reference.</p>
<heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">1. Field of the Invention</p>
<p id="p-0004" num="0003">The present invention relates to a computer-readable storage medium, an image recognition apparatus, an image recognition system, and an image recognition method, for detecting a predetermined capturing target from an image captured by capturing means.</p>
<p id="p-0005" num="0004">2. Description of the Background Art</p>
<p id="p-0006" num="0005">Conventionally, there is a technique of detecting a predetermined capturing target from an image captured by capturing means such as a camera (a captured image). For example, Non-Patent Literature 1 states that in augmented reality technology, an image recognition process is performed on a marker included in an image captured by a camera. Non-Patent Literature 1 states that connected regions are extracted by binarizing the captured image using a fixed threshold, and regions of appropriate sizes and shapes are selected, from among the extracted connected regions, to be marker candidates. Then, the marker is detected by performing pattern matching on the marker candidates.</p>
<p id="p-0007" num="0006">[Non-Patent Literature 1] Hirokazu Kato, Mark Billinghurst, Koichi Asarco, Keihachiro Tachibana, &#x201c;An Augmented Reality System and its Calibration based on Marker Tracking&#x201d;, Journal of the Virtual Reality Society of Japan, vol. 4, no. 4, 1999</p>
<p id="p-0008" num="0007">The detection method of a marker described in Non-Patent Literature 1 cannot necessarily detect a marker with high accuracy or a small processing load in various states (e.g., a bright state; a dark state; the state where part of the marker is hidden by, for example, a user's finger; the state where the marker does not face the camera in a full-face manner; and the state where a strong light is reflected by the marker).</p>
<p id="p-0009" num="0008">In addition, the detection method of a marker described in Non-Patent Literature 1 cannot prevent a slight deviation of the position of the detected position of the marker.</p>
<heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0010" num="0009">Therefore, it is an object of the present invention to provide a computer-readable storage medium, an image recognition apparatus, an image recognition system, and an image recognition method that are capable of detecting a predetermined object or design from an image with high accuracy or a small processing load, and to provide a computer-readable storage medium, an image recognition apparatus, an image recognition system, and an image recognition method that are capable of preventing or reducing a slight deviation of the position of a predetermined object or design that has been detected.</p>
<p id="p-0011" num="0010">To achieve the above object, the present invention may employ the following configurations.</p>
<p id="p-0012" num="0011">A first configuration example is a computer-readable storage medium having stored thereon an image recognition program causing a computer of an information processing apparatus to function as image acquisition means, vertex detection means, division point generation means, sample point determination means, and distinction means.</p>
<p id="p-0013" num="0012">The image acquisition means acquires an image. The vertex detection means detects from the image a plurality of vertices of a contour of an object or of a design. The division point generation means generates a predetermined number of division points on each of sides connecting the plurality of vertices to each other, so as to divide each side of at least one pair of two opposing sides into unequal parts. The sample point determination means determines a plurality of sample points on the basis of straight lines connecting the division points on the two opposing sides to one another. The distinction means, on the basis of pixel values of the sample points, determines whether or not a predetermined object or design is displayed in an area surrounded by the plurality of vertices in the image.</p>
<p id="p-0014" num="0013">Based on the first configuration example, it is possible to distinguish a predetermined object or design with accuracy.</p>
<p id="p-0015" num="0014">It should be noted that as a variation, the division point generation means may generate the division points on each side of at least one pair of two opposing sides such that the closer to one end of each side of the two opposing sides, the denser the division points.</p>
<p id="p-0016" num="0015">Based on the variation, it is possible to distinguish the predetermined object or design with improved accuracy.</p>
<p id="p-0017" num="0016">In addition, as another variation, the division point generation means may divide each side of at least one pair of two opposing sides into unequal parts in accordance with a ratio of: a distance between one end of one side of the two opposing sides and a corresponding one end of the other side; to a distance between the other ends.</p>
<p id="p-0018" num="0017">Based on the variation, it is possible to distinguish the predetermined object or design with improved accuracy.</p>
<p id="p-0019" num="0018">In addition, as another variation, the image recognition program may further cause the computer to function as determination means for determining whether or not, among the plurality of sides connecting the plurality of vertices to each other, given two opposing sides are parallel to each other, and the division point generation means may include: equal division means for, when the determination means has determined that any two opposing sides are parallel to each other, generating the division points so as to divide each side of the two opposing sides into equal parts; and unequal division means for, when the determination means has determined that any two opposing sides are not parallel to each other, generating the division points so as to divide each side of the two opposing sides into unequal parts.</p>
<p id="p-0020" num="0019">Based on the variation, it is possible to distinguish the predetermined object or design with improved accuracy.</p>
<p id="p-0021" num="0020">In addition, as another variation, the division point generation means may include: first vanishing point calculation means for calculating a first vanishing point, the first vanishing point being an intersection of straight lines extending from a first side and a second side, respectively, the first side and the second side opposing each other; second vanishing point calculation means for calculating a second vanishing point, the second vanishing point being an intersection of straight lines extending from a third side and a fourth side, respectively, the third side and the fourth side opposing each other; second straight line calculation means for calculating a second straight line, the second straight line being parallel to a first straight line connecting the first vanishing point to the second vanishing point, the second straight line passing through the vertex furthest from the first straight line; first equal division means for, on a line segment connecting an intersection of the straight line passing through the first side and the second straight line to an intersection of the straight line passing through the second side and the second straight line, generating a plurality of first equal division points so as to divide the line segment into equal parts; second equal division means for, on a line segment connecting an intersection of the straight line passing through the third side and the second straight line to an intersection of the straight line passing through the fourth side and the second straight line, generating a plurality of second equal division points so as to divide the line segment into equal parts; first generation means for, on the basis of straight lines connecting the first vanishing point to the first equal division points, generating the division points on each of the third side and the fourth side so as to divide each of the third side and the fourth side into unequal parts; and second generation means for, on the basis of straight lines connecting the second vanishing point to the second equal division points, generating the division points on each of the first side and the second side so as to divide each of the first side and the second side into unequal parts.</p>
<p id="p-0022" num="0021">Based on the variation, it is possible to distinguish the predetermined object or design with improved accuracy.</p>
<p id="p-0023" num="0022">It should be noted that the image recognition program can be stored in a given computer-readable storage medium (e.g., a flexible disk, a hard disk, an optical disk, a magnetic optical disk, a CD-ROM, a CD-R, a magnetic tape, a semiconductor memory card, a ROM, and a RAM).</p>
<p id="p-0024" num="0023">A second configuration example is an image recognition apparatus including: image acquisition means for acquiring an image; vertex detection means for detecting from the image a plurality of vertices of a contour of an object or of a design; division point generation means for generating a predetermined number of division points on each of sides connecting the plurality of vertices to each other, so as to divide each side of at least one pair of two opposing sides into unequal parts; sample point determination means for determining a plurality of sample points on the basis of straight lines connecting the division points on the two opposing sides to one another; and distinction means for, on the basis of pixel values of the sample points, determining whether or not a predetermined object or design is displayed in an area surrounded by the plurality of vertices in the image.</p>
<p id="p-0025" num="0024">A third configuration example is an image recognition method including: an image acquisition step of acquiring an image; a vertex detection step of detecting from the image a plurality of vertices of a contour of an object or of a design; a division point generation step of generating a predetermined number of division points on each of sides connecting the plurality of vertices to each other, so as to divide each side of at least one pair of two opposing sides into unequal parts; a sample point determination step of determining a plurality of sample points on the basis of straight lines connecting the division points on the two opposing sides to one another; and a distinction step of, on the basis of pixel values of the sample points, determining whether or not a predetermined object or design is displayed in an area surrounded by the plurality of vertices in the image.</p>
<p id="p-0026" num="0025">A fourth configuration example is an image recognition system including: image acquisition means for acquiring an image; vertex detection means for detecting from the image a plurality of vertices of a contour of an object or of a design; division point generation means for generating a predetermined number of division points on each of sides connecting the plurality of vertices to each other, so as to divide each side of at least one pair of two opposing sides into unequal parts; sample point determination means for determining a plurality of sample points on the basis of straight lines connecting the division points on the two opposing sides to one another; and distinction means for, on the basis of pixel values of the sample points, determining whether or not a predetermined object or design is displayed in an area surrounded by the plurality of vertices in the image.</p>
<p id="p-0027" num="0026">A fifth configuration example is an image recognition system including an image recognition apparatus and a marker in which a design is drawn. The image recognition apparatus includes: a capturing section for capturing the marker; image acquisition means for acquiring an image from the capturing section; vertex detection means for detecting from the image a plurality of vertices of a contour of the marker or of the design; division point generation means for generating a predetermined number of division points on each of sides connecting the plurality of vertices to each other, so as to divide each side of at least one pair of two opposing sides into unequal parts; sample point determination means for determining a plurality of sample points on the basis of straight lines connecting the division points on the two opposing sides to one another; and distinction means for, on the basis of pixel values of the sample points, determining whether or not a predetermined design is displayed in an area surrounded by the plurality of vertices in the image.</p>
<p id="p-0028" num="0027">Based on the above configuration examples, it is possible to detect a predetermined object or design from an image with high accuracy.</p>
<p id="p-0029" num="0028">These and other objects, features, aspects and advantages of the present invention will become more apparent from the following detailed description of the present invention when taken in conjunction with the accompanying drawings.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 1</figref> is a front view of a game apparatus <b>10</b> in an open state;</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 2A</figref> is a left side view of the game apparatus <b>10</b> in a closed state;</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 2B</figref> is a front view of the game apparatus <b>10</b> in the closed state;</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 2C</figref> is a right side view of the game apparatus <b>10</b> in the closed state;</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 2D</figref> is a rear view of the game apparatus <b>10</b> in the closed state;</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 3</figref> is a block diagram showing the internal configuration of the game apparatus <b>10</b>;</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 4</figref> is a diagram showing an image displayed on an upper LCD <b>22</b>;</p>
<p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. 5</figref> is a diagram showing a marker <b>50</b>;</p>
<p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. 6</figref> is a diagram showing a captured real image captured by an outer capturing section (left) <b>23</b><i>a </i>or an outer capturing section (right) <b>23</b><i>b; </i></p>
<p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. 7</figref> is a diagram showing the order of selecting a marked pixel in the captured real image;</p>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. 8</figref> is a diagram illustrating a determination method of an edge determination threshold;</p>
<p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. 9</figref> is a diagram illustrating the determination method of the edge determination threshold;</p>
<p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. 10</figref> is a diagram illustrating the determination method of the edge determination threshold;</p>
<p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. 11</figref> is a diagram illustrating the determination method of the edge determination threshold;</p>
<p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. 12</figref> is a diagram illustrating the determination method of the edge determination threshold;</p>
<p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. 13</figref> is a diagram illustrating an edge tracking process;</p>
<p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. 14</figref> is a diagram illustrating the edge tracking process;</p>
<p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. 15</figref> is a diagram illustrating the edge tracking process;</p>
<p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. 16</figref> is a diagram illustrating the edge tracking process;</p>
<p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. 17</figref> is a diagram illustrating the edge tracking process;</p>
<p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. 18</figref> is a diagram illustrating the edge tracking process;</p>
<p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. 19</figref> is a diagram illustrating the edge tracking process;</p>
<p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. 20</figref> is a diagram illustrating the edge tracking process;</p>
<p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. 21</figref> is a diagram illustrating the edge tracking process;</p>
<p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. 22</figref> is a diagram illustrating a straight line calculation process;</p>
<p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. 23</figref> is a diagram illustrating the straight line calculation process;</p>
<p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. 24</figref> is a diagram illustrating the straight line calculation process;</p>
<p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. 25</figref> is a diagram illustrating the straight line calculation process;</p>
<p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. 26</figref> is a diagram illustrating the straight line calculation process;</p>
<p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. 27</figref> is a diagram illustrating the straight line calculation process;</p>
<p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. 28</figref> is a diagram illustrating the straight line calculation process;</p>
<p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. 29</figref> is a diagram illustrating the straight line calculation process;</p>
<p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. 30</figref> is a diagram illustrating the straight line calculation process;</p>
<p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. 31</figref> is a diagram illustrating the straight line calculation process;</p>
<p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. 32</figref> is a diagram illustrating the straight line calculation process;</p>
<p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. 33</figref> is a diagram illustrating the straight line calculation process;</p>
<p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. 34</figref> is a diagram illustrating the straight line calculation process;</p>
<p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. 35</figref> is a diagram illustrating the straight line calculation process;</p>
<p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. 36</figref> is a diagram illustrating the straight line calculation process;</p>
<p id="p-0069" num="0068"><figref idref="DRAWINGS">FIG. 37</figref> is a diagram illustrating the straight line calculation process;</p>
<p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. 38</figref> is a diagram illustrating the straight line calculation process;</p>
<p id="p-0071" num="0070"><figref idref="DRAWINGS">FIG. 39</figref> is a diagram illustrating the straight line calculation process;</p>
<p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. 40</figref> is a diagram illustrating the straight line calculation process;</p>
<p id="p-0073" num="0072"><figref idref="DRAWINGS">FIG. 41</figref> is a diagram illustrating the straight line calculation process;</p>
<p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. 42</figref> is a diagram illustrating a vertex calculation process;</p>
<p id="p-0075" num="0074"><figref idref="DRAWINGS">FIG. 43</figref> is a diagram illustrating the vertex calculation process;</p>
<p id="p-0076" num="0075"><figref idref="DRAWINGS">FIG. 44</figref> is a diagram illustrating an exclusion condition A in a rough distinction process;</p>
<p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. 45</figref> is a diagram illustrating an exclusion condition B in the rough distinction process;</p>
<p id="p-0078" num="0077"><figref idref="DRAWINGS">FIG. 46</figref> is a diagram illustrating an exclusion condition C in the rough distinction process;</p>
<p id="p-0079" num="0078"><figref idref="DRAWINGS">FIG. 47</figref> is a diagram illustrating an exclusion condition D in the rough distinction process;</p>
<p id="p-0080" num="0079"><figref idref="DRAWINGS">FIG. 48</figref> is a diagram illustrating pattern definition data used in a design distinction process;</p>
<p id="p-0081" num="0080"><figref idref="DRAWINGS">FIG. 49</figref> is a diagram illustrating the pattern definition data used in the design distinction process;</p>
<p id="p-0082" num="0081"><figref idref="DRAWINGS">FIG. 50</figref> is a diagram showing a captured real image including the marker <b>50</b>;</p>
<p id="p-0083" num="0082"><figref idref="DRAWINGS">FIG. 51</figref> is a diagram showing an example of a determination method of the positions of sample points in the captured real image;</p>
<p id="p-0084" num="0083"><figref idref="DRAWINGS">FIG. 52</figref> is a diagram illustrating a first determination method of determining the positions of the sample points in the captured real image;</p>
<p id="p-0085" num="0084"><figref idref="DRAWINGS">FIG. 53</figref> is a diagram illustrating the first determination method of determining the positions of the sample points in the captured real image;</p>
<p id="p-0086" num="0085"><figref idref="DRAWINGS">FIG. 54</figref> is a diagram illustrating the first determination method of determining the positions of the sample points in the captured real image;</p>
<p id="p-0087" num="0086"><figref idref="DRAWINGS">FIG. 55</figref> is a diagram illustrating the first determination method of determining the positions of the sample points in the captured real image;</p>
<p id="p-0088" num="0087"><figref idref="DRAWINGS">FIG. 56</figref> is a diagram illustrating the first determination method of determining the positions of the sample points in the captured real image;</p>
<p id="p-0089" num="0088"><figref idref="DRAWINGS">FIG. 57</figref> is a diagram illustrating the first determination method of determining the positions of the sample points in the captured real image;</p>
<p id="p-0090" num="0089"><figref idref="DRAWINGS">FIG. 58</figref> is a diagram illustrating the first determination method of determining the positions of the sample points in the captured real image;</p>
<p id="p-0091" num="0090"><figref idref="DRAWINGS">FIG. 59</figref> is a diagram illustrating a second determination method of determining the positions of the sample points in the captured real image;</p>
<p id="p-0092" num="0091"><figref idref="DRAWINGS">FIG. 60</figref> is a diagram illustrating the second determination method of determining the positions of the sample points in the captured real image;</p>
<p id="p-0093" num="0092"><figref idref="DRAWINGS">FIG. 61</figref> is a diagram illustrating the design distinction process performed when the contours and the vertices of the marker have not been detected from a current captured real image;</p>
<p id="p-0094" num="0093"><figref idref="DRAWINGS">FIG. 62</figref> is a diagram illustrating the design distinction process performed when the contours and the vertices of the marker have not been detected from the current captured real image;</p>
<p id="p-0095" num="0094"><figref idref="DRAWINGS">FIG. 63</figref> is a diagram illustrating a marker position correction process;</p>
<p id="p-0096" num="0095"><figref idref="DRAWINGS">FIG. 64</figref> is a diagram illustrating the marker position correction process;</p>
<p id="p-0097" num="0096"><figref idref="DRAWINGS">FIG. 65</figref> is a diagram illustrating the marker position correction process;</p>
<p id="p-0098" num="0097"><figref idref="DRAWINGS">FIG. 66</figref> is a diagram for explaining the reason why the sizes of a threshold D<b>1</b> and a threshold D<b>2</b> are changed in accordance with the size of the marker in a captured real image;</p>
<p id="p-0099" num="0098"><figref idref="DRAWINGS">FIG. 67</figref> is a diagram for explaining the reason why the sizes of the threshold D<b>1</b> and the threshold D<b>2</b> are changed in accordance with the size of the marker in the captured real image;</p>
<p id="p-0100" num="0099"><figref idref="DRAWINGS">FIG. 68</figref> is a diagram showing a determination method of the threshold D<b>1</b>;</p>
<p id="p-0101" num="0100"><figref idref="DRAWINGS">FIG. 69</figref> is a diagram showing a determination method of the threshold D<b>2</b>;</p>
<p id="p-0102" num="0101"><figref idref="DRAWINGS">FIG. 70</figref> is a diagram illustrating a variation of the marker position correction process;</p>
<p id="p-0103" num="0102"><figref idref="DRAWINGS">FIG. 71</figref> is a diagram illustrating another variation of the marker position correction process;</p>
<p id="p-0104" num="0103"><figref idref="DRAWINGS">FIG. 72</figref> is a diagram illustrating a determination method of the correspondence relationships between markers;</p>
<p id="p-0105" num="0104"><figref idref="DRAWINGS">FIG. 73</figref> is a memory map of a main memory <b>32</b>;</p>
<p id="p-0106" num="0105"><figref idref="DRAWINGS">FIG. 74</figref> is a flow chart showing the overall flow of an image recognition process;</p>
<p id="p-0107" num="0106"><figref idref="DRAWINGS">FIG. 75</figref> is a flow chart showing the flow of a contour detection process;</p>
<p id="p-0108" num="0107"><figref idref="DRAWINGS">FIG. 76</figref> is a flow chart showing the flow of a vertex detection process;</p>
<p id="p-0109" num="0108"><figref idref="DRAWINGS">FIG. 77</figref> is a flow chart showing the flow of the rough distinction process;</p>
<p id="p-0110" num="0109"><figref idref="DRAWINGS">FIG. 78</figref> is a flow chart showing the flow of the design distinction process; and</p>
<p id="p-0111" num="0110"><figref idref="DRAWINGS">FIG. 79</figref> is a flow chart showing the flow of the marker position correction process.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading>
<p id="p-0112" num="0111">(Configuration of Game Apparatus)</p>
<p id="p-0113" num="0112">A description is given below of a game apparatus according to an embodiment of the present invention. A game apparatus <b>10</b> is a hand-held game apparatus. As shown in <figref idref="DRAWINGS">FIG. 1</figref> and <figref idref="DRAWINGS">FIGS. 2A through 2D</figref>, the game apparatus <b>10</b> includes a lower housing <b>11</b> and an upper housing <b>21</b>. The lower housing <b>11</b> and the upper housing <b>21</b> are connected to each other so as to be openable and closable in a folding manner (foldable).</p>
<p id="p-0114" num="0113">(Description of Lower Housing)</p>
<p id="p-0115" num="0114">As shown in <figref idref="DRAWINGS">FIG. 1</figref> and <figref idref="DRAWINGS">FIGS. 2A through 2D</figref>, the lower housing <b>11</b> includes a lower liquid crystal display (LCD) <b>12</b>, a touch panel <b>13</b>, operation buttons <b>14</b>A through <b>14</b>L, an analog stick <b>15</b>, LEDs <b>16</b>A and <b>16</b>B, an insertion slot <b>17</b>, and a microphone hole <b>18</b>.</p>
<p id="p-0116" num="0115">The touch panel <b>13</b> is mounted on the screen of the lower LCD <b>12</b>. The insertion slot <b>17</b> (a dashed line shown in <figref idref="DRAWINGS">FIGS. 1 and 2D</figref>) is provided on the upper side surface of the lower housing <b>11</b> so as to accommodate a stylus <b>28</b>.</p>
<p id="p-0117" num="0116">The cross button <b>14</b>A (direction input button <b>14</b>A), the button <b>14</b>B, the button <b>14</b>C, the button <b>14</b>D, the button <b>14</b>E, the power button <b>14</b>F, the select button <b>14</b>J, the home button <b>14</b>K, and the start button <b>14</b>L are provided on the inner surface (main surface) of the lower housing <b>11</b>.</p>
<p id="p-0118" num="0117">The analog stick <b>15</b> is a device for indicating a direction.</p>
<p id="p-0119" num="0118">The microphone hole <b>18</b> is provided on the inner surface of the lower housing <b>11</b>. Underneath the microphone hole <b>18</b>, a microphone <b>42</b> (see <figref idref="DRAWINGS">FIG. 3</figref>) is provided as the sound input device described later.</p>
<p id="p-0120" num="0119">As shown in <figref idref="DRAWINGS">FIGS. 2B and 2D</figref>, the L button <b>14</b>G and the R button <b>14</b>H are provided on the upper side surface of the lower housing <b>11</b>. Further, as shown in <figref idref="DRAWINGS">FIG. 2A</figref>, the sound volume button <b>14</b>I is provided on the left side surface of the lower housing <b>11</b> so as to adjust the sound volume of a loudspeaker <b>43</b> of the game apparatus <b>10</b>.</p>
<p id="p-0121" num="0120">As shown in <figref idref="DRAWINGS">FIG. 2A</figref>, a cover section <b>11</b>C is provided on the left side surface of the lower housing <b>11</b> so as to be openable and closable. Inside the cover section <b>11</b>C, a connector is provided for electrically connecting the game apparatus <b>10</b> and a data storage external memory <b>45</b>.</p>
<p id="p-0122" num="0121">As shown in <figref idref="DRAWINGS">FIG. 2D</figref>, on the upper side surface of the lower housing <b>11</b>, an insertion slot <b>11</b>D is provided, into which an external memory <b>44</b> is to be inserted.</p>
<p id="p-0123" num="0122">As shown in <figref idref="DRAWINGS">FIGS. 1 and 2C</figref>, the first LED <b>16</b>A is provided on the lower side surface of the lower housing <b>11</b> so as to notify a user of the on/off state of the power supply of the game apparatus <b>10</b>. Further, the second LED <b>16</b>B is provided on the right side surface of the lower housing <b>11</b> so as to notify the user of the establishment state of the wireless communication of the game apparatus <b>10</b>. The game apparatus <b>10</b> is capable of wirelessly communicating with other devices, and a wireless switch <b>19</b> is provided on the right side surface of the lower housing <b>11</b> so as to enable/disable the function of the wireless communication (see <figref idref="DRAWINGS">FIG. 2C</figref>).</p>
<p id="p-0124" num="0123">(Description of Upper Housing)</p>
<p id="p-0125" num="0124">As shown in <figref idref="DRAWINGS">FIG. 1</figref> and <figref idref="DRAWINGS">FIGS. 2A through 2D</figref>, the upper housing <b>21</b> includes an upper liquid crystal display (LCD) <b>22</b>, an outer capturing section <b>23</b> (an outer capturing section (left) <b>23</b><i>a </i>and an outer capturing section (right) <b>23</b><i>b</i>), an inner capturing section <b>24</b>, a 3D adjustment switch <b>25</b>, and a 3D indicator <b>26</b>.</p>
<p id="p-0126" num="0125">The upper LCD <b>22</b> is a display device capable of displaying a stereoscopically visible image. Specifically, the upper LCD <b>22</b> is a parallax barrier type display device capable of displaying an image stereoscopically visible with the naked eye. The upper LCD <b>22</b> allows the user to view the left-eye image with their left eye, and the right-eye image with their right eye, using the parallax barrier. This makes it possible to display an image giving the user a stereoscopic effect (a stereoscopic image). Further, the upper LCD <b>22</b> is capable of disabling the parallax barrier. When disabling the parallax barrier, the upper LCD <b>22</b> is capable of displaying an image in a planar manner. Thus, the upper LCD <b>22</b> is a display device capable of switching between: a stereoscopic display mode for displaying a stereoscopic image; and a planar display mode for displaying an image in a planar manner (displaying a planar view image). The switching of the display modes is performed by, for example, the 3D adjustment switch <b>25</b> described later.</p>
<p id="p-0127" num="0126">The &#x201c;outer capturing section <b>23</b>&#x201d; is the collective term of the two capturing sections (<b>23</b><i>a </i>and <b>23</b><i>b</i>) provided on an outer surface <b>21</b>D of the upper housing <b>21</b>. The outer capturing section (left) <b>23</b><i>a </i>and the outer capturing section (right) <b>23</b><i>b </i>can be used as a stereo camera, depending on the program executed by the game apparatus <b>10</b>.</p>
<p id="p-0128" num="0127">The inner capturing section <b>24</b> is provided on the inner surface <b>21</b>B of the upper housing <b>21</b>, and functions as a capturing section having a capturing direction that is the same as the inward normal direction of the inner surface.</p>
<p id="p-0129" num="0128">The 3D adjustment switch <b>25</b> is a slide switch, and is used to switch the display modes of the upper LCD <b>22</b> as described above. The 3D adjustment switch <b>25</b> is also used to adjust the stereoscopic effect of a stereoscopically visible image (stereoscopic image) displayed on the upper LCD <b>22</b>. A slider <b>25</b><i>a </i>of the 3D adjustment switch <b>25</b> is slidable to a given position in a predetermined direction (the up-down direction), and the display mode of the upper LCD <b>22</b> is set in accordance with the position of the slider <b>25</b><i>a</i>. Further, the view of the stereoscopic image is adjusted in accordance with the position of the slider <b>25</b><i>a. </i></p>
<p id="p-0130" num="0129">The 3D indicator <b>26</b> is an LED that indicates whether or not the upper LCD <b>22</b> is in the stereoscopic display mode.</p>
<p id="p-0131" num="0130">In addition, speaker holes <b>21</b>E are provided on the inner surface of the upper housing <b>21</b>. A sound from the loudspeaker <b>43</b> described later is output through the speaker holes <b>21</b>E.</p>
<p id="p-0132" num="0131">(Internal Configuration of Game Apparatus <b>10</b>)</p>
<p id="p-0133" num="0132">Next, with reference to <figref idref="DRAWINGS">FIG. 3</figref>, a description is given of the internal configuration of the game apparatus <b>10</b>. As shown in <figref idref="DRAWINGS">FIG. 3</figref>, the game apparatus <b>10</b> includes, as well as the components described above, electronic components, such as an information processing section <b>31</b>, a main memory <b>32</b>, an external memory interface (external memory I/F) <b>33</b>, a data storage external memory I/F <b>34</b>, a data storage internal memory <b>35</b>, a wireless communication module <b>36</b>, a local communication module <b>37</b>, a real-time clock (RTC) <b>38</b>, an acceleration sensor <b>39</b>, a power circuit <b>40</b>, and an interface circuit (I/F circuit) <b>41</b>.</p>
<p id="p-0134" num="0133">The information processing section <b>31</b> includes a central processing unit (CPU) <b>311</b> that executes a predetermined program, a graphics processing unit (GPU) <b>312</b> that performs image processing, and a video RAM (VRAM) <b>313</b>. The CPU <b>311</b> executes a program stored in a memory (e.g., the external memory <b>44</b> connected to the external memory I/F <b>33</b>, or the data storage internal memory <b>35</b>) included in the game apparatus <b>10</b>, and thereby performs processing corresponding to the program. It should be noted that the program executed by the CPU <b>311</b> may be acquired from another device by communication with said another device. The GPU <b>312</b> generates an image in accordance with an instruction from the CPU <b>311</b>, and draws the image in the VRAM <b>313</b>. The image drawn in the VRAM <b>313</b> is output to the upper LCD <b>22</b> and/or the lower LCD <b>12</b>, and the image is displayed on the upper LCD <b>22</b> and/or the lower LCD <b>12</b>.</p>
<p id="p-0135" num="0134">The external memory I/F <b>33</b> is an interface for establishing a detachable connection with the external memory <b>44</b>. The data storage external memory I/F <b>34</b> is an interface for establishing a detachable connection with the data storage external memory <b>45</b>.</p>
<p id="p-0136" num="0135">The main memory <b>32</b> is a volatile storage device used as a work area or a buffer area of (the CPU <b>311</b> of) the information processing section <b>31</b>.</p>
<p id="p-0137" num="0136">The external memory <b>44</b> is a nonvolatile storage device for storing the program and the like executed by the information processing section <b>31</b>. The external memory <b>44</b> is composed of, for example, a read-only semiconductor memory.</p>
<p id="p-0138" num="0137">The data storage external memory <b>45</b> is composed of a readable/writable non-volatile memory (e.g., a NAND flash memory), and is used to store given data.</p>
<p id="p-0139" num="0138">The data storage internal memory <b>35</b> is composed of a readable/writable non-volatile memory (e.g., a NAND flash memory), and is used to store predetermined data. For example, the data storage internal memory <b>35</b> stores data and/or programs downloaded by wireless communication through the wireless communication module <b>36</b>.</p>
<p id="p-0140" num="0139">The wireless communication module <b>36</b> has the function of establishing connection with a wireless LAN by, for example, a method based on the IEEE 802.11.b/g standard. Further, the local communication module <b>37</b> has the function of wirelessly communicating with another game apparatus of the same type by a predetermined communication method (e.g., communication using an independent protocol, or infrared communication).</p>
<p id="p-0141" num="0140">The acceleration sensor <b>39</b> detects the magnitudes of accelerations in the directions of straight lines along three axial (x, y, and z axes) directions (linear accelerations), respectively. The information processing section <b>31</b> can receive data representing the accelerations detected by the acceleration sensor <b>39</b> (acceleration data), and detect the orientation and the motion of the game apparatus <b>10</b>.</p>
<p id="p-0142" num="0141">The RTC <b>38</b> counts time, and outputs the counted time to the information processing section <b>31</b>. The information processing section <b>31</b> calculates the current time (date) on the basis of the time counted by the RTC <b>38</b>. The power circuit <b>40</b> controls the power from the power supply (a rechargeable battery) of the game apparatus <b>10</b>, and supplies power to each component of the game apparatus <b>10</b>.</p>
<p id="p-0143" num="0142">The touch panel <b>13</b>, the microphone <b>42</b>, and the loudspeaker <b>43</b> are connected to the I/F circuit <b>41</b>. The I/F circuit <b>41</b> includes: a sound control circuit that controls the microphone <b>42</b> and the loudspeaker <b>43</b> (amplifier); and a touch panel control circuit that controls the touch panel <b>13</b>. For example, the sound control circuit performs A/D conversion and D/A conversion on a sound signal, and converts the sound signal to sound data in a predetermined format. The touch panel control circuit generates touch position data in a predetermined format on the basis of a signal from the touch panel <b>13</b>, and outputs the touch position data to the information processing section <b>31</b>. The information processing section <b>31</b> acquires the touch position data, and thereby recognizes the position at which an input has been provided on the touch panel <b>13</b>.</p>
<p id="p-0144" num="0143">An operation button <b>14</b> includes the operation buttons <b>14</b>A through <b>14</b>L described above, and operation data is output from the operation button <b>14</b> to the information processing section <b>31</b>, the operation data indicating the states of inputs provided to the respective operation buttons <b>14</b>A through <b>14</b>I (indicating whether or not the operation buttons <b>14</b>A through <b>14</b>I have been pressed).</p>
<p id="p-0145" num="0144">The lower LCD <b>12</b> and the upper LCD <b>22</b> are connected to the information processing section <b>31</b>. Specifically, the information processing section <b>31</b> is connected to an LCD controller (not shown) of the upper LCD <b>22</b>, and causes the LCD controller to set the parallax barrier to on/off. When the parallax barrier is on in the upper LCD <b>22</b>, the right-eye image and the left-eye image that are stored in the VRAM <b>313</b> of the information processing section <b>31</b> are output to the upper LCD <b>22</b>. More specifically, the LCD controller repeatedly alternates the reading of pixel data of the right-eye image for one line in the vertical direction, and the reading of pixel data of the left-eye image for one line in the vertical direction, and thereby reads the right-eye image and the left-eye image from the VRAM <b>313</b>. Thus, the right-eye image and the left-eye image are each divided into strip images, each of which has one line of pixels arranged in the vertical direction, and an image including the divided left-eye strip images and the divided right-eye strip images alternately arranged is displayed on the screen of the upper LCD <b>22</b>. The user views the images through the parallax barrier of the upper LCD <b>22</b>, whereby the right-eye image is viewed with the user's right eye, and the left-eye image is viewed with the user's left eye. This causes the stereoscopically visible image to be displayed on the screen of the upper LCD <b>22</b>.</p>
<p id="p-0146" num="0145">The outer capturing section <b>23</b> and the inner capturing section <b>24</b> each capture an image in accordance with an instruction from the information processing section <b>31</b>, and output data of the captured image to the information processing section <b>31</b>.</p>
<p id="p-0147" num="0146">The 3D adjustment switch <b>25</b> transmits to the information processing section <b>31</b> an electrical signal in accordance with the position of the slider.</p>
<p id="p-0148" num="0147">The information processing section <b>31</b> controls whether or not the 3D indicator <b>26</b> is to be lit on. When, for example, the upper LCD <b>22</b> is in the stereoscopic display mode, the information processing section <b>31</b> lights on the 3D indicator <b>26</b>.</p>
<p id="p-0149" num="0148">(Overview of Image Processing)</p>
<p id="p-0150" num="0149">Next, with reference to <figref idref="DRAWINGS">FIGS. 4 through 72</figref>, a description is given of an overview of image processing performed by the game apparatus <b>10</b>. The image processing performed by the game apparatus <b>10</b> includes an image recognition process and an image generation process.</p>
<p id="p-0151" num="0150">The image recognition process is a process of detecting the position of a marker included in an image captured by a camera (the outer capturing section (left) <b>23</b><i>a </i>or the outer capturing section (right) <b>23</b><i>b</i>) (a captured real image). The image generation process is a process of generating an image to be displayed on the upper LCD <b>22</b>, using the result of the image recognition process.</p>
<p id="p-0152" num="0151">Using the result of the image recognition process, the CPU <b>311</b> of the game apparatus <b>10</b> can display on the upper LCD <b>22</b> an image as if a virtual object actually exists in the vicinity of the marker (e.g., on the marker) in the real world. For example, in the example of <figref idref="DRAWINGS">FIG. 4</figref>, an image is displayed on the upper LCD <b>22</b> of the game apparatus <b>10</b>, as if a virtual object <b>60</b> representing a dog actually exists on a marker <b>50</b> placed on a table. Such an image is obtained by combining a captured real image captured by the camera with an image of the virtual object <b>60</b> (a virtual space image). The virtual space image is drawn on the basis of a virtual camera placed in a virtual space, and the positional relationship between (the relative positions and orientations of) the virtual object and the virtual camera in the virtual space is controlled in real time so as to coincide with the positional relationship between the camera (the outer capturing section (left) <b>23</b><i>a </i>or the outer capturing section (right) <b>23</b><i>b</i>) and the marker <b>50</b> in real space. Consequently, an image is obtained as if the virtual object <b>60</b> actually exists in real space.</p>
<p id="p-0153" num="0152">As shown in <figref idref="DRAWINGS">FIG. 5</figref>, the marker <b>50</b> is rectangular, and has a white area along its periphery and a black area surrounded by the white area. Within the black area, a predetermined internal figure (here, an arrow as an example) is drawn. It should be noted that in the present embodiment, the marker <b>50</b> as shown in <figref idref="DRAWINGS">FIG. 5</figref> is used; however, this is merely illustrative. Alternatively, a marker having another shape, another pattern, or another color may be used. For example, the white area may be provided within the black area provided along the periphery of the marker. Yet alternatively, areas of different colors may be provided instead of the white area and the black area. For a contour detection process described later, however, the combination of highly contrasting colors is preferably used.</p>
<p id="p-0154" num="0153"><figref idref="DRAWINGS">FIG. 6</figref> is an example of the image captured by the camera (the outer capturing section (left) <b>23</b><i>a </i>or the outer capturing section (right) <b>23</b><i>b</i>) (the captured real image). The captured real image includes, for example, 512 pixels (horizontal direction)&#xd7;384 pixels (vertical direction).</p>
<p id="p-0155" num="0154">To detect the position of the marker <b>50</b> from the captured real image as shown in <figref idref="DRAWINGS">FIG. 6</figref>, the following processes are performed in the present embodiment.</p>
<p id="p-0156" num="0155">(1) Contour detection process</p>
<p id="p-0157" num="0156">(2) Vertex detection process</p>
<p id="p-0158" num="0157">(3) Rough distinction process</p>
<p id="p-0159" num="0158">(4) Design distinction process</p>
<p id="p-0160" num="0159">(5) Marker position correction process</p>
<p id="p-0161" num="0160">(Contour Detection Process)</p>
<p id="p-0162" num="0161">First, the contour detection process is described. The contour detection process is a process of detecting in the captured real image the contour of a design drawn in the marker <b>50</b> (the boundary between the white area and the black area shown in <figref idref="DRAWINGS">FIG. 5</figref>).</p>
<p id="p-0163" num="0162">In the present embodiment, in the captured real image, first, the pixel represented by the coordinates (<b>16</b>, <b>16</b>) is defined as a marked pixel. Then, on the basis of the luminance value of the marked pixel and the luminance value of the eighth pixel (<b>8</b>, <b>16</b>) to the left counting from the marked pixel, it is determined whether or not an edge (an edge with the white area on the left and the black area on the right) is present anywhere between the two pixels (the determination method will be described in detail later). When it is determined that an edge is not present, the eighth pixel (<b>24</b>, <b>16</b>) to the right counting from the currently marked pixel (<b>16</b>, <b>16</b>) is, as shown in <figref idref="DRAWINGS">FIG. 7</figref>, defined as a newly marked pixel. Then, on the basis of the luminance value of the newly marked pixel and the luminance value of the eighth pixel to the left (i.e., the most recently marked pixel (<b>16</b>, <b>16</b>)) counting from the newly marked pixel, it is determined whether or not an edge is present anywhere between the two pixels. Thereafter, similar processes are performed while sequentially updating the marked pixel. It should be noted that when the processes on the line having a Y-coordinate value of 16 are completed, similar processes are performed on the sixteenth line below (i.e., the line having a Y-coordinate value of 32) counting from the line having a Y-coordinate value of 16. It should be noted that such a manner of selecting the marked pixel is merely illustrative, and the present invention is not limited to this.</p>
<p id="p-0164" num="0163">In the following descriptions, the marked pixel is represented as a pixel P(n); the kth pixel to the left from the marked pixel is represented as a pixel P(n&#x2212;k); and the kth pixel to the right from the marked pixel is represented as a pixel P(n+k). Further, the luminance value of the marked pixel is represented as L(n); the luminance value of the kth pixel to the left from the marked pixel is represented as L(n&#x2212;k); and the luminance value of the kth pixel to the right from the marked pixel is represented as L(n+k).</p>
<p id="p-0165" num="0164">As shown in <figref idref="DRAWINGS">FIG. 8</figref>, the determination of whether or not an edge is present anywhere between the marked pixel (i.e., the pixel P(n)) and the eighth pixel to the left (i.e., a pixel P(n&#x2212;8)) from the marked pixel, is made on the basis of L(n&#x2212;8) and L(n). Specifically, the determination is made on the basis of whether or not L(n&#x2212;8)&#x2212;L(n) is equal to or greater than a predetermined value. When L(n&#x2212;8)&#x2212;L(n) is equal to or greater than the predetermined value, it is determined that an edge is present somewhere between these pixels. It should be noted that in the present embodiment, the luminance value of each pixel is represented by a value of from 0 to 255, and the predetermined value is 60. It should be noted that these numbers are merely illustrative, and the present invention is not limited to these.</p>
<p id="p-0166" num="0165">When it is determined that an edge is present somewhere between the pixel P(n) and the pixel P(n&#x2212;8), subsequently, an edge determination threshold used to detect the position of the edge is calculated on the basis of the luminance values of these two pixels and pixels near (around) these pixels. With reference to <figref idref="DRAWINGS">FIGS. 9 through 12</figref>, a description is given below of the calculation method of the edge determination threshold.</p>
<p id="p-0167" num="0166">First, as shown in <figref idref="DRAWINGS">FIGS. 9 through 12</figref>, a white area luminance value Lw is determined on the basis of the luminance values of the pixel P(n&#x2212;8) and pixels near the pixel P(n&#x2212;8). A description is given below of an example of the determination method of the white area luminance value Lw.</p>
<p id="p-0168" num="0167">First, it is determined whether or not the luminance value of a pixel P(n&#x2212;9) is smaller than the luminance value of the pixel P(n&#x2212;8). If smaller, the luminance value of the pixel P(n&#x2212;8) serves as the white area luminance value Lw. For example, in the examples of <figref idref="DRAWINGS">FIGS. 9</figref>, <b>11</b>, and <b>12</b>, the luminance value of the pixel P(n&#x2212;9) is 210, and the luminance value of the pixel P(n&#x2212;8) is 220. Thus, the luminance value of the pixel P(n&#x2212;8), namely 220, serves as the white area luminance value Lw.</p>
<p id="p-0169" num="0168">When the luminance value of the pixel P(n&#x2212;9) is equal to or greater than the luminance value of the pixel P(n&#x2212;8), subsequently, it is determined whether or not the luminance value of a pixel P(n&#x2212;10) is smaller than the luminance value of the pixel P(n&#x2212;9). If smaller, the luminance value of the pixel P(n&#x2212;9) serves as the white area luminance value Lw.</p>
<p id="p-0170" num="0169">When the luminance value of the pixel P(n&#x2212;9) is equal to or greater than the luminance value of the pixel P(n&#x2212;8), and also the luminance value of the pixel P(n&#x2212;10) is equal to or greater than the luminance value of the pixel P(n&#x2212;9), subsequently, it is determined whether or not the luminance value of a pixel P(n&#x2212;11) is smaller than the luminance value of the pixel P(n&#x2212;10). If smaller, the luminance value of the pixel P(n&#x2212;9) serves as the white area luminance value Lw. For example, in the example of <figref idref="DRAWINGS">FIG. 10</figref>, the luminance value of the pixel P(n&#x2212;11) is 210, and the luminance value of the pixel P(n&#x2212;10) is 220. Thus, the luminance value of the pixel P(n&#x2212;10), namely 220, serves as the white area luminance value Lw.</p>
<p id="p-0171" num="0170">The process as described above can be rephrased as a process of sequentially referring to the luminance values of pixels leftward from the pixel P(n&#x2212;8) and finding a local maximum value of the luminance values (the local maximum value that first appears). The process can also be rephrased as a process of detecting a local maximum value from among the luminance values of the pixel P(n&#x2212;8) and pixels around the pixel P(n&#x2212;8). Such a process makes it possible that even when, as shown in <figref idref="DRAWINGS">FIG. 10</figref>, the pixel P(n&#x2212;8) is placed at the boundary between the white area and the black area of the marker <b>50</b> (is displayed in gray in the captured real image), the luminance value of the white area of the marker <b>50</b> is correctly set as the white area luminance value Lw.</p>
<p id="p-0172" num="0171">It should be noted that in the present embodiment, the white area luminance value Lw is determined as described above; however, this is merely illustrative, and the determination method of the white area luminance value Lw is not limited to this. For example, the luminance value of the pixel. P(n&#x2212;8) and the luminance value of any pixel around the pixel P(n&#x2212;8) may be compared with each other. When the luminance value of the pixel around the pixel P(n&#x2212;8) is greater, the white area luminance value Lw may be calculated on the basis of the luminance value of the pixel around the pixel P(n&#x2212;8). Alternatively, for example, in the middle of the process of sequentially referring to the luminance values of pixels leftward from the pixel P(n&#x2212;8) and finding a local maximum value of the luminance values (the local maximum value that first appears), when the luminance value of a referred-to pixel has exceeded a predetermined value (e.g., 250), the process of finding a local maximum value of the luminance values may be suspended, and the white area luminance value Lw may be calculated on the basis of the luminance value of the referred-to pixel.</p>
<p id="p-0173" num="0172">Next, as shown in <figref idref="DRAWINGS">FIGS. 9 through 12</figref>, a black area luminance value Lb is determined on the basis of the luminance values of the pixel P(n) and pixels near the pixel P(n). A description is given below of an example of the determination method of the black area luminance value Lb.</p>
<p id="p-0174" num="0173">First, it is determined whether or not the luminance value of a pixel P(n+2) is equal to or less than the luminance value of the pixel P(n). If equal to or less than the luminance value of the pixel P(n), the luminance value of the pixel P(n) serves as the black area luminance value Lb. For example, in the examples of <figref idref="DRAWINGS">FIGS. 9 and 10</figref>, the luminance value of the pixel P(n+2) is 100, and the luminance value of the pixel P(n) is 100. Thus, the luminance value of the pixel P(n), namely 100, serves as the black area luminance value Lb.</p>
<p id="p-0175" num="0174">When the luminance value of the pixel P(n+2) is greater than the luminance value of the pixel P(n), subsequently, it is determined whether or not the value obtained by subtracting the luminance value of the pixel P(n+2) from the white area luminance value Lw (i.e., Lw&#x2212;L(n+2)) is equal to or greater than the predetermined value (i.e., 60). When the obtained value is equal to or greater than 60, the luminance value of the pixel P(n+2) serves as the black area luminance value Lb. When the obtained value is less than 60, the luminance value of the pixel P(n) serves as the black area luminance value Lb.</p>
<p id="p-0176" num="0175">For example, in the example of <figref idref="DRAWINGS">FIG. 11</figref>, the luminance value of the pixel P(n) is 20; the luminance value of the pixel P(n+2) is 100; and the white area luminance value Lw is 220. Then, Lw&#x2212;L(n+2)=120, and therefore, the luminance value of the pixel P(n+2), namely 100, serves as the black area luminance value Lb. In a captured real image subjected to an edge enhancement process (a contour enhancement process or a sharpness process), the luminance values of black area pixels adjacent to the white area may occasionally be, as shown in <figref idref="DRAWINGS">FIG. 11</figref>, significantly smaller than intrinsic luminance values of the black area. In response to this, in the present embodiment, the luminance value of the pixel P(n+2) serves as the black area luminance value Lb when the conditions as described above are satisfied, so that it is possible to determine an appropriate black area luminance value Lb even in such a case.</p>
<p id="p-0177" num="0176">On the other hand, in the example of <figref idref="DRAWINGS">FIG. 12</figref>, the luminance value of the pixel P(n) is 100; the luminance value of the pixel P(n+2) is 210; and the white area luminance value Lw is 220. Then, Lw&#x2212;L(n+2)=10, and therefore, the luminance value of the pixel P(n), namely 100, serves as the black area luminance value Lb.</p>
<p id="p-0178" num="0177">It should be noted that in the present embodiment, the black area luminance value Lb is determined as described above; however, this is merely illustrative, and the determination method of the black area luminance value Lb is not limited to this.</p>
<p id="p-0179" num="0178">When the white area luminance value Lw and the black area luminance value Lb have been determined as described above, subsequently, the edge determination threshold is calculated on the basis of the white area luminance value Lw and the black area luminance value Lb. In the present embodiment, the average value of the white area luminance value Lw and the black area luminance value Lb is determined as the edge determination threshold. For example, in each of the examples of <figref idref="DRAWINGS">FIGS. 9 through 12</figref>, the edge determination threshold is 160. This is, however, merely illustrative, and the calculation method of the edge determination threshold is not limited to this.</p>
<p id="p-0180" num="0179">When the edge determination threshold has been determined as described above, the position where the edge is present between the pixel P(n) and the pixel P(n&#x2212;8) is detected, using the edge determination threshold. Specifically, it is determined that a pixel having a luminance value greater than the edge determination threshold is the white area, and it is determined that a pixel having a luminance value smaller than the edge determination threshold is the black area. Then, it is determined that the boundary between the white area and the black area is the edge. It should be noted that in the present embodiment, a black area pixel adjacent to the white area is detected as an &#x201c;edge pixel&#x201d; placed on the edge (or adjacent to the edge). For example, in the example of <figref idref="DRAWINGS">FIG. 9</figref>, it is determined that a pixel P(n&#x2212;5) is an edge pixel. In the example of <figref idref="DRAWINGS">FIG. 10</figref>, it is determined that a pixel P(n&#x2212;7) is an edge pixel. In the example of <figref idref="DRAWINGS">FIG. 11</figref>, it is determined that the pixel P(n) is an edge pixel. In the example of <figref idref="DRAWINGS">FIG. 12</figref>, it is determined that a pixel P(n&#x2212;6) is an edge pixel. It should be noted that in another embodiment, a white area pixel adjacent to the black area may be detected as an &#x201c;edge pixel&#x201d; placed on the edge.</p>
<p id="p-0181" num="0180">Each pixel of the captured real image is associated with a flag indicating whether or not the pixel is an edge pixel (an edge flag). The edge flag of a pixel determined as an edge pixel is set to on.</p>
<p id="p-0182" num="0181">The edge pixel detected as described above is referred to as a &#x201c;starting edge pixel&#x201d; in the following descriptions. The starting edge pixel is estimated as a part of the contour of the design drawn in the marker <b>50</b> (the boundary between the white area and the black area shown in <figref idref="DRAWINGS">FIG. 5</figref>). If the starting edge pixel is a part of the contour of the design drawn in the marker <b>50</b>, it is possible to detect the contour of the design drawn in the marker <b>50</b> (the boundary between the white area and the black area shown in <figref idref="DRAWINGS">FIG. 5</figref>), by sequentially tracking adjacent edge pixels such that the starting point is the starting edge pixel.</p>
<p id="p-0183" num="0182">A description is given below of a process of sequentially tracking adjacent edge pixels such that the starting point is the starting edge pixel (an edge tracking process).</p>
<p id="p-0184" num="0183">First, as shown, in <figref idref="DRAWINGS">FIG. 13</figref>, black area pixels are searched for in the order of, with the starting edge pixel as a reference, the lower left adjacent pixel, the lower adjacent pixel, the lower right adjacent pixel, the right adjacent pixel, the upper right adjacent pixel, the upper adjacent pixel, and the upper left adjacent pixel (i.e., counterclockwise around the starting edge pixel, starting from the left adjacent pixel). The first detected black area pixel is detected as a new edge pixel subsequent to the starting edge pixel. The determination of whether or not each adjacent pixel is a black area pixel is made on the basis of the edge determination threshold used when the starting edge pixel has been detected. More specifically, when the luminance value of an adjacent pixel is smaller than the edge determination threshold, it is determined that the adjacent pixel is a black area pixel.</p>
<p id="p-0185" num="0184">For example, in the example of <figref idref="DRAWINGS">FIG. 14</figref>, the lower left adjacent pixel is detected as a new edge pixel. In the example of <figref idref="DRAWINGS">FIG. 15</figref>, the right adjacent pixel is detected as a new edge pixel.</p>
<p id="p-0186" num="0185">When, in the edge tracking process, edge pixels have been sequentially detected such that the starting point is the starting edge pixel, the coordinate values of the detected edge pixels are sequentially stored in the main memory <b>32</b> as a series of edge pixels. It should be noted that in the following descriptions, the edge pixel last detected in the edge tracking process is referred to as a &#x201c;front edge pixel&#x201d;, and the edge pixel detected immediately before the front edge pixel is referred to as a &#x201c;second edge pixel&#x201d;.</p>
<p id="p-0187" num="0186">A new edge pixel subsequent to the front edge pixel is detected by searching for a black area pixel counterclockwise around the front edge pixel, such that the starting point is the adjacent pixel placed in the direction shifted 135 degrees counterclockwise from the direction of the second edge pixel as viewed from the front edge pixel (in another embodiment, the starting point may be the adjacent pixel placed in the direction shifted 45 degrees counterclockwise, or may be the adjacent pixel placed in the direction shifted 90 degrees counterclockwise). Then, the black area pixel first detected in the search is detected as a new edge pixel (i.e., a new front edge pixel).</p>
<p id="p-0188" num="0187">For example, as shown in <figref idref="DRAWINGS">FIG. 16</figref>, when the second edge pixel is the upper right adjacent pixel to the front edge pixel, a black area pixel is searched for counterclockwise around the front edge pixel, starting from the left adjacent pixel. Accordingly, in the example of <figref idref="DRAWINGS">FIG. 17</figref>, the lower adjacent pixel to the front edge pixel is detected as a new edge pixel.</p>
<p id="p-0189" num="0188">In addition, for example, as shown in <figref idref="DRAWINGS">FIG. 18</figref>, when the second edge pixel is the left adjacent pixel to the front edge pixel, a black area pixel is searched for counterclockwise around the front edge pixel, starting from the lower right adjacent pixel. Accordingly, in the example of <figref idref="DRAWINGS">FIG. 19</figref>, the right adjacent pixel to the front edge pixel is detected as a new edge pixel.</p>
<p id="p-0190" num="0189">New edge pixels are sequentially detected by repeating the process as described above. Then, ultimately, the front edge pixel reaches the starting edge pixel, whereby the detection of the contour of the black area is completed (i.e., data concerning a series of an edge pixel group indicating the contour of the black area is stored in the main memory <b>32</b>).</p>
<p id="p-0191" num="0190">It should be noted that in the present embodiment, each time a new edge pixel is detected in the edge tracking process, it is determined, on the basis of the edge flag, whether or not the new edge pixel is included in the series of an edge pixel group that has already been detected. When it is determined three consecutive times that the new edge pixel is included in the series of an edge pixel group that has already been detected, the edge tracking process is suspended.</p>
<p id="p-0192" num="0191">For example, as shown in <figref idref="DRAWINGS">FIG. 20</figref>, when it is determined only two consecutive times that the new edge pixel is included in the series of an edge pixel group that has already been detected, the edge tracking process is not suspended.</p>
<p id="p-0193" num="0192">As shown in <figref idref="DRAWINGS">FIG. 21</figref>, however, when it is determined three consecutive times that the new edge pixel is included in the series of an edge pixel group that has already been detected, the edge tracking process is suspended. This is because there is a high possibility that the black area as shown in <figref idref="DRAWINGS">FIG. 21</figref> is not the contour of the design drawn in the marker <b>50</b> (i.e., is the contour of an object other than the marker <b>50</b>). Further, even if the black area is the contour of the design drawn in the marker <b>50</b>, it is highly unlikely to be able to normally perform a pattern matching process described later and the like. This makes it possible to avoid unnecessary processes. It should be noted that the number of times, namely three times, is merely illustrative, and may be another number of times.</p>
<p id="p-0194" num="0193">It should be noted that in the above description, as an example, the contour of the black area is tracked counterclockwise such that the starting point is the starting edge pixel. Alternatively, in another embodiment, the contour of the black area may be tracked clockwise such that the starting point is the starting edge pixel.</p>
<p id="p-0195" num="0194">As described above, in the contour detection process, the white area luminance value Lw and the black area luminance value Lb are determined, and the edge determination threshold is calculated on the basis of the white area luminance value Lw and the black area luminance value Lb. Accordingly, even when the brightness of a captured real image has entirely or partially changed, and the luminances of the white area and the black area of the marker <b>50</b> in the captured real image have changed in accordance with the change, it is possible to perform the contour detection process using an appropriate edge determination threshold. This improves the accuracy of recognizing the marker <b>50</b>.</p>
<p id="p-0196" num="0195">It should be noted that in the contour detection process, the contour is extracted on the basis of the luminance values of pixels; however, the present invention is not limited to luminance values. Alternatively, the contour may be detected on the basis of other given pixel values (typically, color values).</p>
<p id="p-0197" num="0196">In addition, in the contour detection process, first, it is determined whether or not an edge is present between two pixels separate in the horizontal direction (the pixel P(n) and the pixel P(n&#x2212;8)); however, the manner of selecting two pixels is not limited to this. For example, it may be determined whether or not an edge is present between two pixels separate in the vertical direction. Alternatively, it may be determined whether or not an edge is present between two pixels separate in a diagonal direction.</p>
<p id="p-0198" num="0197">In addition, in the contour detection process, when L(n&#x2212;8)&#x2212;L(n) is equal to or greater than a predetermined value, it is determined that an edge is present between the pixel P(n) and the pixel P(n&#x2212;8) (in this case, it is possible to find an edge with the white area on the left and the black area on the right). Alternatively, in another embodiment, when the absolute value of L(n&#x2212;8)&#x2212;L(n) is equal to or greater than a predetermined value, it may be determined that an edge is present between the pixel P(n) and the pixel P(n&#x2212;8). In this case, it is possible to find not only an edge with the white area on the left and the black area on the right, but also an edge with the black area on the left and the white area on the right.</p>
<p id="p-0199" num="0198">In addition, in the contour detection process, the edge determination threshold used when the starting edge pixel has been detected is used in the edge tracking process. Alternatively, in another embodiment, the edge determination threshold may be used to detect an edge pixel from a given area in the captured real image (e.g., the entire captured real image). For example, the following may be detected on the basis of the edge determination threshold: an edge pixel on the line placed one line lower than the line including the starting edge pixel; and an edge pixel of a contour other than the contour including the starting edge pixel.</p>
<p id="p-0200" num="0199">(Vertex Detection Process)</p>
<p id="p-0201" num="0200">Next, the vertex detection process is described. The vertex detection process is a process of detecting the four vertices of the black area of the marker <b>50</b> in the captured real image, and includes the following processes.
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0201">Straight line calculation process</li>
        <li id="ul0002-0002" num="0202">Straight line integration process</li>
        <li id="ul0002-0003" num="0203">Straight line selection process</li>
        <li id="ul0002-0004" num="0204">Vertex calculation process</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0202" num="0205">In the straight line calculation process, a plurality of straight lines are calculated on the basis of the data concerning the series of an edge pixel group indicating the contour of the black area, the data stored in the main memory <b>32</b> in the contour detection process described above. With reference to <figref idref="DRAWINGS">FIGS. 22 through 36</figref>, the straight line calculation process is described in detail below.</p>
<p id="p-0203" num="0206">In the data concerning the series of an edge pixel group stored in the main memory <b>32</b>, a plurality of edge pixels are ordered. In the following descriptions, for convenience, the direction of tracking the contour of the black area counterclockwise (i.e., the left direction as viewed from the black area in the direction of the white area) is defined as forward, and the direction of tracking the contour of the black area clockwise (i.e., the right direction as viewed from the black area in the direction of the white area) is defined as backward.</p>
<p id="p-0204" num="0207">First, as shown in <figref idref="DRAWINGS">FIG. 22</figref>, a first straight line Li(<b>0</b>-<b>4</b>) is generated on the basis of five edge pixels Pe(<b>0</b>) through Pe(<b>4</b>), starting from the starting edge pixel Pe(<b>0</b>) to the edge pixel Pe (<b>4</b>), which is four pixels ahead of the starting edge pixel Pe(<b>0</b>), and data indicating the straight line Li(<b>0</b>-<b>4</b>) is stored in the main memory <b>32</b>. It should be noted that it is possible to employ various methods as a method of generating a straight line on the basis of a plurality of edge pixels. In the present embodiment, a straight line is generated by a least squares method.</p>
<p id="p-0205" num="0208">Next, as shown in <figref idref="DRAWINGS">FIG. 23</figref>, a provisional straight line is generated by, for example, a least squares method on the basis of five edge pixels Pe(<b>5</b>) through Pe(<b>9</b>) immediately ahead of the straight line Li(<b>0</b>-<b>4</b>), and it is determined whether or not the straight line Li(<b>0</b>-<b>4</b>) and the provisional straight line are placed on the same straight line. The determination is made on the basis of, for example, the angle of the provisional straight line with respect to the straight line Li(<b>0</b>-<b>4</b>). In the present embodiment, as shown in <figref idref="DRAWINGS">FIG. 24</figref>, when the angle of the provisional straight line with respect to the straight line Li(<b>0</b>-<b>4</b>) (on the assumption that the counterclockwise direction is positive) is in the range from &#x2212;30&#xb0; to +30&#xb0;, it is determined that the straight line Li(<b>0</b>-<b>4</b>) and the provisional straight line are placed on the same straight line. It should be noted that the values such as &#x2212;30&#xb0; and +30&#xb0; are merely illustrative, and the present invention is not limited to these.</p>
<p id="p-0206" num="0209">When it is determined that the straight line Li(<b>0</b>-<b>4</b>) and the provisional straight line are placed on the same straight line, a straight line Li(<b>0</b>-<b>9</b>) is calculated as shown in <figref idref="DRAWINGS">FIG. 25</figref> by, for example, a least squares method on the basis of 10 edge pixels, namely the edge pixels Pe(<b>0</b>) through Pe(<b>4</b>) corresponding to the straight line Li(<b>0</b>-<b>4</b>) and the edge pixels Pe(<b>5</b>) through Pe(<b>9</b>) corresponding to the provisional straight line, and the data indicating the straight line Li(<b>0</b>-<b>4</b>) stored in the main memory <b>32</b> is updated to data indicating the straight line Li(<b>0</b>-<b>9</b>). Such a process is repeated, whereby the straight line is sequentially updated (extended). It should be noted that in the present embodiment, a provisional straight line is generated every five edge pixels; however, this is merely illustrative, and the present invention is not limited to this.</p>
<p id="p-0207" num="0210">In the example of <figref idref="DRAWINGS">FIG. 26</figref>, the angle of a provisional straight line (i.e., a provisional straight line generated on the basis of five edge pixels Pe(<b>15</b>) through Pe(<b>19</b>) ahead of a straight line Li(<b>0</b>-<b>14</b>)) with respect to the straight line Li(<b>0</b>-<b>14</b>) exceeds +30&#xb0;, and therefore, it is determined that the black area has a convex angle near the intersection of the straight line Li(<b>0</b>-<b>14</b>) and the provisional straight line (i.e., the black area is pointed outward near the intersection of the straight line Li(<b>0</b>-<b>14</b>) and the provisional straight line) (see <figref idref="DRAWINGS">FIG. 24</figref>). In this case, it is determined that the provisional straight line is a new straight line different from the straight line Li(<b>0</b>-<b>14</b>). Then, a new straight line Li(<b>15</b>-<b>19</b>) corresponding to the provisional straight line is generated (see <figref idref="DRAWINGS">FIG. 27</figref>). In this case, data indicating the straight line Li(<b>0</b>-<b>14</b>) is held as it is in the main memory <b>32</b>, and data indicating the straight line Li(<b>15</b>-<b>19</b>) is newly stored in the main memory <b>32</b>.</p>
<p id="p-0208" num="0211">Subsequently, as shown in <figref idref="DRAWINGS">FIG. 28</figref>, it is determined whether or not the straight line Li(<b>15</b>-<b>19</b>) and a provisional straight line generated on the basis of five edge pixels Pe(<b>20</b>) through Pe(<b>24</b>) ahead of the straight line Li(<b>15</b>-<b>19</b>) are placed on the same straight line. When it is determined that these lines are placed on the same straight line, the straight line Li(<b>15</b>-<b>19</b>) is updated to a straight line Li(<b>15</b>-<b>24</b>) on the basis of the edge pixels Pe(<b>15</b>) through Pe(<b>24</b>), as shown in <figref idref="DRAWINGS">FIG. 29</figref>.</p>
<p id="p-0209" num="0212">The process as described above is repeated, whereby a plurality of straight lines are ultimately calculated as shown in <figref idref="DRAWINGS">FIG. 30</figref> (six straight lines, namely straight lines A through F, in the example of <figref idref="DRAWINGS">FIG. 30</figref>). It should be noted that these lines may include a short straight line, such as the straight line D.</p>
<p id="p-0210" num="0213">As described above, in the straight line calculation process, each straight line is calculated from, among a series of edge pixels, some edge pixels placed on the same straight line.</p>
<p id="p-0211" num="0214">It should be noted that in the straight line calculation process, a straight line is generated or updated such that the starting point is the starting edge pixel Pe(<b>0</b>); however, the present invention is not limited to this. Alternatively, a straight line may be generated or updated such that a given edge pixel other than the starting point is the starting edge pixel Pe(<b>0</b>).</p>
<p id="p-0212" num="0215">In addition, in the straight line calculation process, a straight line is generated or updated counterclockwise; however, the present invention is not limited to this. Alternatively, a straight line may be generated or updated clockwise.</p>
<p id="p-0213" num="0216">In addition, in the straight line calculation process, it is determined whether or not an already generated straight line and a provisional straight line adjacent thereto are placed on the same straight line, and when it is determined that these lines are placed on the same straight line, a straight line is calculated on the basis of a plurality of edge pixels corresponding to the straight line and the provisional straight line. Alternatively, in another embodiment, after numerous provisional straight lines are generated first, a straight line may be calculated on the basis of a plurality of edge pixels corresponding to, among the numerous provisional straight lines, a plurality of provisional straight lines placed on the same straight line.</p>
<p id="p-0214" num="0217">The case is considered where, as shown in <figref idref="DRAWINGS">FIG. 31</figref>, a black object is displayed so as to partially overlap a side of the black area of the marker <b>50</b> in the captured real image. In this case, in the straight line calculation process, after a straight line A and a straight line B have been generated, it is determined, as shown in <figref idref="DRAWINGS">FIG. 32</figref>, whether or not the straight line B and a provisional straight line ahead of the straight line B are placed on the same straight line. The angle of the provisional straight line with respect to the straight line B is smaller than &#x2212;30&#xb0;, and therefore, it is determined that the black area has a concave angle near the intersection of the straight line B and the provisional straight line (i.e., the black area is depressed inward near the intersection of the straight line B and the provisional straight line) (see <figref idref="DRAWINGS">FIG. 24</figref>). There is no concave angle in the contour of the black area of the marker <b>50</b> (i.e., there are only convex angles), and therefore, when such a concave angle has been detected, it is estimated that the provisional straight line indicates a part of the contour of an object other than the marker.</p>
<p id="p-0215" num="0218">As described above, when a concave angle has been detected in the straight line calculation process, it is determined that it is not possible to perform a process of updating (extending) or newly generating a straight line. Then, the process is suspended of updating (extending) or newly generating a straight line counterclockwise in the black area, and a process is started of updating (extending) or newly generating a straight line in the direction opposite to the previous direction (i.e., clockwise). For example, in the example of <figref idref="DRAWINGS">FIG. 31</figref>, a process is performed of extending backward the straight line A, clockwise from the starting edge pixel, and also a process is performed of generating a new straight line. As a result, as shown in <figref idref="DRAWINGS">FIG. 33</figref>, the straight line A shown in <figref idref="DRAWINGS">FIG. 31</figref> is updated to a straight line A&#x2032;, and straight lines C through E are sequentially generated. It should be noted that a concave angle is detected at the rear end of the straight line E, and therefore, the straight line calculation process ends at the time of the detection. As a result, five straight lines, namely the straight line A&#x2032; and the straight lines B through E, are calculated.</p>
<p id="p-0216" num="0219">As yet another example, as shown in <figref idref="DRAWINGS">FIG. 34</figref>, the case is considered where a white object is displayed so as to partially overlap a side of the black area of the marker <b>50</b> in the captured real image. In this case, in the straight line calculation process, after straight lines A through C have been generated, a concave angle is detected. Then, a process is started of extending backward the straight line A, clockwise from the starting edge pixel, and also a process is started of generating a new straight line. The straight line A is updated to a straight line A&#x2032;, and straight lines D through G are sequentially generated. As a result, seven straight lines, namely the straight line A&#x2032; and the straight lines B through G, are calculated.</p>
<p id="p-0217" num="0220">As yet another example, the ease is considered where, as shown in <figref idref="DRAWINGS">FIG. 35</figref>, a black object is displayed so as to overlap a vertex of the black area of the marker <b>50</b> in the captured real image. In this case, as a result, four straight lines, namely, a straight line A&#x2032; and straight lines B through D, are calculated.</p>
<p id="p-0218" num="0221">As yet another example, the case is considered where, as shown in <figref idref="DRAWINGS">FIG. 36</figref>, a white object is displayed so as to overlap a vertex of the black area of the marker <b>50</b> in the captured real image. In this case, as a result, six straight lines, namely a straight line A&#x2032; and straight lines B through F, are calculated.</p>
<p id="p-0219" num="0222">When the straight line calculation process is completed, the straight line integration process is subsequently performed. The straight line integration process is a process of integrating, among a plurality of straight lines calculated in the straight line calculation process, a plurality of straight lines placed on the same straight line and directed in the same direction into one straight line.</p>
<p id="p-0220" num="0223">For example, in the example of <figref idref="DRAWINGS">FIG. 30</figref>, the straight line A and the straight line F are placed on the same straight line and directed in the same direction, and therefore, as shown in <figref idref="DRAWINGS">FIG. 37</figref>, these two straight lines are integrated into one straight line A+F.</p>
<p id="p-0221" num="0224">In addition, for example, in the example of <figref idref="DRAWINGS">FIG. 33</figref>, the straight line B and the straight line E are placed on the same straight line and directed in the same direction, and therefore, as shown in <figref idref="DRAWINGS">FIG. 38</figref>, these two straight lines are integrated into one straight line B+E.</p>
<p id="p-0222" num="0225">In addition, for example, in the example of <figref idref="DRAWINGS">FIG. 34</figref>, the straight line B and the straight line F are placed on the same straight line and directed in the same direction, and therefore, as shown in <figref idref="DRAWINGS">FIG. 39</figref>, these two straight lines are integrated into one straight line B+F.</p>
<p id="p-0223" num="0226">When the straight line integration process is completed, the straight line selection process is subsequently performed. The straight line selection process is a process of selecting straight lines corresponding to the four sides of the black area of the marker <b>50</b>, from among the plurality of straight lines finally remaining after the straight line calculation process and the straight line integration process.</p>
<p id="p-0224" num="0227">In the present embodiment, the four longest straight lines (i.e., the longest straight line, the second longest straight line, the third longest straight line, and the fourth longest straight line) are selected as straight lines corresponding to the four sides of the black area of the marker <b>50</b>, from among the plurality of finally remaining straight lines. In the following descriptions, the selected four straight lines are referred to as a &#x201c;first straight line&#x201d;, a &#x201c;second straight line&#x201d;, a &#x201c;third straight line&#x201d;, and a &#x201c;fourth straight line&#x201d;, counterclockwise from a given straight line among these lines.</p>
<p id="p-0225" num="0228">For example, in the example of <figref idref="DRAWINGS">FIG. 37</figref>, the straight line A+F, the straight line B, the straight line C, and the straight line E are selected from among five straight lines, namely the straight line A+F and the straight lines B through E, in the straight line selection process (see <figref idref="DRAWINGS">FIG. 40</figref>).</p>
<p id="p-0226" num="0229">In addition, for example, in the example of <figref idref="DRAWINGS">FIG. 39</figref>, the straight line A&#x2032;, the straight line B+F, the straight line E, and the straight line D are selected from among six straight lines, namely the straight line A&#x2032;, the straight line B+F, the straight lines C through E, and the straight line G, in the straight line selection process.</p>
<p id="p-0227" num="0230">In addition, for example, in the example of <figref idref="DRAWINGS">FIG. 36</figref>, the straight line A&#x2032;, the straight line B, the straight line E, and the straight line D are selected from among six straight lines, namely the straight line A&#x2032; and the straight lines B through F, in the straight line selection process (see <figref idref="DRAWINGS">FIG. 41</figref>).</p>
<p id="p-0228" num="0231">It should be noted that in the examples of <figref idref="DRAWINGS">FIGS. 35 and 38</figref>, only four straight lines are present, and therefore, these four straight lines are selected in the straight line selection process.</p>
<p id="p-0229" num="0232">It should be noted that in the straight line selection process, four straight lines are selected because the black area of the marker <b>50</b> is rectangular. Accordingly, if the black area of the marker <b>50</b> is, for example, hexagonal, six straight lines are selected in the straight line selection process.</p>
<p id="p-0230" num="0233">When the straight line selection process is completed, the vertex calculation process is subsequently performed. In the vertex calculation process, the positions of the four vertices of the black area of the marker <b>50</b> are calculated on the basis of the four straight lines (first through fourth straight lines) selected in the straight line selection process.</p>
<p id="p-0231" num="0234">Specifically, the position of the intersection of the first straight line and the second straight line is calculated as the position of a first vertex of the black area of the marker <b>50</b>. The position of the intersection of the second straight line and the third straight line is calculated as the position of a second vertex of the black area of the marker <b>50</b>. The position of the intersection of the third straight line and the fourth straight line is calculated as the position of a third vertex of the black area of the marker <b>50</b>. The position of the intersection of the fourth straight line and the first straight line is calculated as the position of a fourth vertex of the black area of the marker <b>50</b>.</p>
<p id="p-0232" num="0235">For example, in the example of <figref idref="DRAWINGS">FIG. 40</figref>, the positions of the first through fourth vertices are calculated as shown in <figref idref="DRAWINGS">FIG. 42</figref>. Further, for example, in the example of <figref idref="DRAWINGS">FIG. 41</figref>, the positions of the first through fourth vertices are calculated as shown in <figref idref="DRAWINGS">FIG. 43</figref>.</p>
<p id="p-0233" num="0236">As described above, the four vertices of the black area of the marker <b>50</b> in the captured real image are detected through the straight line calculation process, the straight line integration process, the straight line selection process, and the vertex calculation process.</p>
<p id="p-0234" num="0237">The positions of the vertices thus detected are calculated as the intersections of straight lines generated on the basis of a plurality of edge pixels placed on the same straight line, and therefore have high accuracy. For example, when any one of a series of edge pixels is determined as a vertex, the position of the vertex deviates due, for example, to the effect of environmental light. The position of a vertex detected as described above, however, is calculated on the basis of numerous edge pixels, and therefore, such a deviation does not occur.</p>
<p id="p-0235" num="0238">(Rough Distinction Process)</p>
<p id="p-0236" num="0239">Next, the rough distinction process is described. The rough distinction process is a process of, prior to the design distinction process described later, determining whether or not the four vertices detected in the vertex detection process are the four vertices of the marker <b>50</b>, on the basis of the positional relationships between the four vertices.</p>
<p id="p-0237" num="0240">In the present embodiment, when exclusion conditions A through D shown below have been satisfied, it is determined that the four vertices detected in the vertex detection process are not the four vertices of the marker <b>50</b>.</p>
<p id="p-0238" num="0241">(Exclusion Condition A) The Case where the Distance Between any Two Adjacent Vertices is Too Small</p>
<p id="p-0239" num="0242">Specifically, the case is: where the distance between the first vertex and the second vertex is smaller than a predetermined threshold (a first minimum acceptable distance); where the distance between the second vertex and the third vertex is smaller than the first minimum acceptable distance; where the distance between the third vertex and the fourth vertex is smaller than the first minimum acceptable distance; or where the distance between the fourth vertex and the first vertex is smaller than the first minimum acceptable distance. For example, in the example of <figref idref="DRAWINGS">FIG. 44</figref>, the distance between the first vertex and the second vertex is too small, and therefore, it is determined that the first through fourth vertices are not the four vertices of the marker <b>50</b>.</p>
<p id="p-0240" num="0243">(Exclusion Condition B) The Case where the Distance Between any Vertex and Either One of the Two Sides not Adjacent to the Vertex is Too Small</p>
<p id="p-0241" num="0244">Specifically, the case is: where the distance between the first vertex and the third straight line is smaller than a predetermined threshold (a second minimum acceptable distance); where the distance between the first vertex and the fourth straight line is smaller than the second minimum acceptable distance; where the distance between the second vertex and the fourth straight line is smaller than the second minimum acceptable distance; where the distance between the second vertex and the first straight line is smaller than the second minimum acceptable distance; where the distance between the third vertex and the first straight line is smaller than the second minimum acceptable distance; where the distance between the third vertex and the second straight line is smaller than the second minimum acceptable distance; where the distance between the fourth vertex and the second straight line is smaller than the second minimum acceptable distance; or where the distance between the fourth vertex and the third straight line is smaller than the second minimum acceptable distance. For example, in the example of <figref idref="DRAWINGS">FIG. 45</figref>, the distance between the first vertex and the third straight line is too small, and therefore, it is determined that the first through fourth vertices are not the four vertices of the marker <b>50</b>.</p>
<p id="p-0242" num="0245">(Exclusion Condition C) The Case where the Straight Lines (Vectors) of any Two Opposing Sides are Directed in Generally the Same Direction</p>
<p id="p-0243" num="0246">Specifically, the case is: where the first straight line (the vector connecting the fourth vertex to the first vertex) and the third straight line (the vector connecting the second vertex to the third vertex) are directed in generally the same direction; or where the second straight line (the vector connecting the first vertex to the second vertex) and the fourth straight line (the vector connecting the fourth vertex to the first vertex) are directed in generally the same direction. For example, in the example of <figref idref="DRAWINGS">FIG. 46</figref>, the first straight line (the vector connecting the fourth vertex to the first vertex) and the third straight line (the vector connecting the second vertex to the third vertex) are directed in generally the same direction, and therefore, it is determined that the first through fourth vertices are not the four vertices of the marker <b>50</b>. It should be noted that it is possible to determine whether or not two vectors are directed in generally the same direction, on the basis of the angle between the two vectors, for example, as shown in <figref idref="DRAWINGS">FIG. 24</figref>.</p>
<p id="p-0244" num="0247">(Exclusion Condition D) The Case where a Concave Angle is Included</p>
<p id="p-0245" num="0248">Specifically, the case is where any one of the first through fourth vertices has a concave angle. For example, in the example of <figref idref="DRAWINGS">FIG. 47</figref>, the second vertex has a concave angle, and therefore, it is determined that the first through fourth vertices are not the four vertices of the marker <b>50</b>.</p>
<p id="p-0246" num="0249">It should be noted that in the present embodiment, the rough distinction process is performed on the basis of the exclusion conditions A through D; however, this is merely illustrative. Alternatively, one or more of the exclusion conditions may be used, or an exclusion condition different from these exclusion conditions may be used.</p>
<p id="p-0247" num="0250">When any of the exclusion conditions A through D are satisfied, it is determined that the four vertices detected in the vertex detection process are not the four vertices of the marker <b>50</b>, and the detected four vertices are excluded from process objects in the design distinction process described later. This reduces processing load required in the design distinction process.</p>
<p id="p-0248" num="0251">(Design Distinction Process)</p>
<p id="p-0249" num="0252">Next, the design distinction process is described. The design distinction process is a process of determining whether or not the design displayed in the area surrounded by the four vertices detected in the vertex detection process is the same as the design drawn in the marker <b>50</b>.</p>
<p id="p-0250" num="0253">In the design distinction process, it is determined, using pattern definition data generated in advance on the basis of the design drawn in the marker <b>50</b>, whether or not the design displayed in the area surrounded by the four vertices detected in the vertex detection process is the same as the design drawn in the marker <b>50</b>.</p>
<p id="p-0251" num="0254">The pattern definition data is data representing the design drawn in the marker <b>50</b>, and, in the present embodiment, is data in which, as shown in <figref idref="DRAWINGS">FIG. 48</figref>, the intersections of the grid generated by dividing each side of the marker <b>50</b> into 16 equal parts are used as sample points (S(<b>1</b>, <b>1</b>) through S(<b>15</b>, <b>15</b>)), and the pixel values of the sample points in the marker <b>50</b> are defined (see <figref idref="DRAWINGS">FIG. 49</figref>). It should be noted that in the present embodiment, each side of the marker <b>50</b> is divided into 16 parts; however, this is merely illustrative. The present invention is not limited to division into 16 parts. Further, in the present embodiment, the intersections of the grid generated by dividing each side of the marker <b>50</b> into equal parts are used as the sample points. Alternatively, in another embodiment, the centers of the rectangles separated by the grid may be used as the sample points.</p>
<p id="p-0252" num="0255">In the present embodiment, the intersection closest to the upper left vertex of the marker <b>50</b> is the sample point S(<b>1</b>, <b>1</b>); the intersection closest to the lower left vertex of the marker <b>50</b> is the sample point S(<b>1</b>, <b>15</b>); the intersection closest to the upper right vertex of the marker <b>50</b> is the sample point S(<b>15</b>, <b>1</b>); and the intersection closest to the lower right vertex of the marker <b>50</b> is the sample point S(<b>15</b>, <b>15</b>).</p>
<p id="p-0253" num="0256">It should be noted that in the example of <figref idref="DRAWINGS">FIG. 49</figref>, the pixel values are defined as &#x201c;black&#x201d; or &#x201c;white&#x201d; in the columns of the pixel values. Alternatively, in another embodiment, the pixel values may be defined as, for example, color values (RGB value) and luminance values.</p>
<p id="p-0254" num="0257"><figref idref="DRAWINGS">FIG. 50</figref> shows a captured real image including the marker <b>50</b>. The positions of the first through fourth vertices are calculated by performing the contour detection process and the vertex detection process that are described above on the captured real image.</p>
<p id="p-0255" num="0258"><figref idref="DRAWINGS">FIG. 51</figref> shows an example where, in the captured real image, each side of the rectangle surrounded by the first through fourth vertices is divided into 16 equal parts, whereby the positions of the sample points included in the area surrounded by these vertices are determined.</p>
<p id="p-0256" num="0259">In the design distinction process, the pixel values of the sample points in the captured real image are checked against the pattern definition data (e.g., correlation coefficients are calculated), whereby it is determined whether or not the design displayed in the area surrounded by the first through fourth vertices in the captured real image is the same as the design drawn in the marker <b>50</b>. However, when the sample points in the captured real image are determined by a method as shown in <figref idref="DRAWINGS">FIG. 51</figref>, it may not be possible to make accurate determinations. For example, the pixel value of the sample point S(<b>8</b>, <b>2</b>) in the pattern definition data is &#x201c;black&#x201d;, whereas the pixel value of the sample point S(<b>8</b>, <b>2</b>) in the captured real image shown in <figref idref="DRAWINGS">FIG. 51</figref> is &#x201c;white&#x201d;. Further, the pixel value of the sample point S(<b>8</b>, <b>13</b>) in the pattern definition data is &#x201c;white&#x201d;, whereas the pixel value of the sample point S(<b>8</b>, <b>13</b>) in the captured real image shown in <figref idref="DRAWINGS">FIG. 51</figref> is &#x201c;black&#x201d;. As a result, it may be erroneously determined that the marker <b>50</b> is not included in the captured real image shown in <figref idref="DRAWINGS">FIG. 50</figref>.</p>
<p id="p-0257" num="0260">To solve the above problem, it is necessary to devise the determination method of the positions of the sample points in the captured real image. A description is given below of another example of the determination method of the positions of the sample points in the captured real image (a first determination method and a second determination method).</p>
<p id="p-0258" num="0261">First, with reference to <figref idref="DRAWINGS">FIGS. 52 through 58</figref>, a description is given of the first determination method of determining the positions of the sample points in the captured real image.</p>
<p id="p-0259" num="0262">In the first determination method, in the rectangle surrounded by the first through fourth vertices: when two opposing sides are parallel (including the case where they are generally parallel) to each other, the two sides are each divided into 16 equal parts; and when two opposing sides are not parallel to each other, the two sides are each divided into 16 unequal parts. The intersections of the grid thus generated are used as the sample points.</p>
<p id="p-0260" num="0263">For example, in the example of <figref idref="DRAWINGS">FIG. 52</figref>, the side connecting the first vertex to the fourth vertex and the side connecting the second vertex to the third vertex are parallel to each other, and therefore, the side connecting the first vertex to the fourth vertex and the side connecting the second vertex to the third vertex are each divided into 16 equal parts. On the other hand, the side connecting the first vertex to the second vertex and the side connecting the fourth vertex to the third vertex are not parallel to each other, and therefore, the side connecting the first vertex to the second vertex and the side connecting the fourth vertex to the third vertex are not each divided into 16 equal parts, but are each divided into 16 unequal parts. That is, as shown in <figref idref="DRAWINGS">FIG. 52</figref>, the points dividing the side connecting the first vertex to the second vertex into 16 parts (hereinafter referred to as &#x201c;division points&#x201d;) are shifted to the first vertex side, as compared to the case where the same side is divided into 16 equal parts. Further, the division points dividing the side connecting the fourth vertex to the third vertex into 16 parts are shifted to the fourth vertex side, as compared to the case where the same side is divided into 16 equal parts.</p>
<p id="p-0261" num="0264">With reference to <figref idref="DRAWINGS">FIGS. 53 through 55</figref>, a description is given below of an example of the determination method of the division points on the side connecting the first vertex to the second vertex, and the division points on the side connecting the fourth vertex to the third vertex, in <figref idref="DRAWINGS">FIG. 52</figref>.</p>
<p id="p-0262" num="0265">It should be noted that in the following descriptions, the 15 division points dividing the side connecting the first vertex to the second vertex into 16 parts are referred to as a &#x201c;first division point M<b>1</b>&#x201d;, a &#x201c;second division point M<b>2</b>&#x201d;, a &#x201c;third division point M<b>3</b>&#x201d; . . . , and a &#x201c;fifteenth division point M<b>15</b>&#x201d;, in the order from the point closer to the first vertex. Similarly, the 15 division points dividing the side connecting the fourth vertex to the third vertex into 16 parts are referred to as a &#x201c;first division point N<b>1</b>&#x201d;, a &#x201c;second division point N<b>2</b>&#x201d;, a &#x201c;third division point N<b>3</b>&#x201d; . . . , and a &#x201c;fifteenth division point N<b>15</b>&#x201d;, in the order from the point closer to the fourth vertex.</p>
<p id="p-0263" num="0266">First, as shown in <figref idref="DRAWINGS">FIG. 53</figref>, the eighth division point M<b>8</b> and the eighth division point N<b>8</b> are determined. Specifically, first, the distance between the first vertex and the fourth vertex and the distance between the second vertex and the third vertex are calculated. Then, when the distance between the first vertex and the fourth vertex is a and the distance between the second vertex and the third vertex is b, the point dividing, in a ratio of a:b, the straight line connecting the first vertex to the second vertex, and the point dividing, in a ratio of a:b, the straight line connecting the fourth vertex to the third vertex are calculated. Then, the first calculated point is determined as the eighth division point M<b>8</b>, and the second calculated point is determined as the eighth division point N<b>8</b>. As a result, the eighth division point M<b>8</b> is determined at a position closer to the first vertex than the midpoint of the first vertex and the second vertex is. The eighth division point N<b>8</b> is determined at a position closer to the fourth vertex than the midpoint of the fourth vertex and the third vertex is.</p>
<p id="p-0264" num="0267">Next, as shown in <figref idref="DRAWINGS">FIG. 54</figref>, the fourth division point M<b>4</b> and the fourth division point N<b>4</b> are determined. Specifically, first, the distance between the eighth division point M<b>8</b> and the eighth division point N<b>8</b> is calculated. Then, when the calculated distance is c, the point dividing, in a ratio of a:c, the straight line connecting the first vertex to the eighth division point M<b>8</b>, and the point dividing, in a ratio of a:c, the straight line connecting the fourth vertex to the eighth division point N<b>8</b> are calculated. Then, the first calculated point is determined as the fourth division point M<b>4</b>, and the second calculated point is determined as the fourth division point N<b>4</b>. As a result, the fourth division point M<b>4</b> is determined at a position closer to the first vertex than the midpoint of the first vertex and the eighth division point M<b>8</b> is. The fourth division point N<b>4</b> is determined at a position closer to the fourth vertex than the midpoint of the fourth vertex and the eighth division point N<b>8</b> is.</p>
<p id="p-0265" num="0268">Next, as shown in <figref idref="DRAWINGS">FIG. 55</figref>, the second division point M<b>2</b> and the second division point N<b>2</b> are determined. Specifically, first, the distance between the fourth division point M<b>4</b> and the fourth division point N<b>4</b> is calculated. Then, when the calculated distance is d, the point dividing, in a ratio of a:d, the straight line connecting the first vertex to the fourth division point M<b>4</b>, and the point dividing, in a ratio of a:d, the straight line connecting the fourth vertex to the fourth division point N<b>4</b> are calculated. Then, the first calculated point is determined as the second division point M<b>2</b>, and the second calculated point is determined as the second division point N<b>2</b>. As a result, the second division point M<b>2</b> is determined at a position closer to the first vertex than the midpoint of the first vertex and the fourth division point M<b>4</b> is. The second division point N<b>2</b> is determined at a position closer to the fourth vertex than the midpoint of the fourth vertex and the fourth division point N<b>4</b> is.</p>
<p id="p-0266" num="0269">Subsequently, similarly, the remaining division points (the first division point M<b>1</b>, the third division point M<b>3</b>, the firth division point MS through the seventh division point M<b>7</b>, the ninth division point M<b>9</b> through the fifteenth division point M<b>15</b>, the first division point N<b>1</b>, the third division point N<b>3</b>, the fifth division point N<b>5</b> through the seventh division point N<b>7</b>, and the ninth division point N<b>9</b> through the fifteenth division point N<b>15</b>) are determined, and the sample points as shown in <figref idref="DRAWINGS">FIG. 52</figref> are ultimately determined.</p>
<p id="p-0267" num="0270">It should be noted that in the first determination method, when two opposing sides of the rectangle surrounded by the first through fourth vertices are parallel to each other, the two sides are divided into 16 equal parts. With reference to <figref idref="DRAWINGS">FIGS. 56 through 58</figref>, the reason for this is explained below.</p>
<p id="p-0268" num="0271">When the first through fourth vertices as shown in <figref idref="DRAWINGS">FIG. 56</figref> have been detected in the captured real image, the side connecting the first vertex to the fourth vertex and the side connecting the second vertex to the third vertex are parallel to each other. In such a case, if these sides are divided into 16 parts using the method shown in <figref idref="DRAWINGS">FIGS. 53 through 55</figref>, the division points dividing the side connecting the first vertex to the fourth vertex are placed closer to the first vertex as a whole, and the division points dividing the side connecting the second vertex to the third vertex are placed closer to the second vertex as a whole, as shown in <figref idref="DRAWINGS">FIG. 57</figref>.</p>
<p id="p-0269" num="0272">However, when in the captured real image, the marker <b>50</b> is displayed in the shape as shown in <figref idref="DRAWINGS">FIG. 56</figref> (i.e., when the side connecting the first vertex to the fourth vertex and the side connecting the second vertex to the third vertex are parallel to each other), the distance between the camera and the first vertex and the distance between the camera and the fourth vertex are almost the same. Similarly, the distance between the camera and the second vertex and the distance between the camera and the third vertex are almost the same. Accordingly, in such a case, as shown in <figref idref="DRAWINGS">FIG. 58</figref>, the division of the side connecting the first vertex to the fourth vertex into 16 equal parts, and the division of the side connecting the second vertex to the third vertex into 16 equal parts, make it possible to determine the positions of the sample points at more appropriate positions.</p>
<p id="p-0270" num="0273">Next, with reference to <figref idref="DRAWINGS">FIGS. 59 and 60</figref>, a description is given of the second determination method of determining the positions of the sample points in the captured real image.</p>
<p id="p-0271" num="0274">In the second determination method, as shown in <figref idref="DRAWINGS">FIG. 59</figref>, first, pairs of two opposing sides of the rectangle surrounded by the four vertices detected in the vertex detection process are extended, and the intersections of the respective pairs of two opposing sides (a first vanishing point and a second vanishing point) are calculated. In the following descriptions, among the four vertices detected in the vertex detection process, the vertex closest to the straight line connecting the first vanishing point to the second vanishing point (a first straight line) is referred to as a &#x201c;vertex A&#x201d;, and the remaining vertices are referred to as a &#x201c;vertex B&#x201d;, a &#x201c;vertex C&#x201d;, and a &#x201c;vertex D&#x201d;, counterclockwise from the vertex A.</p>
<p id="p-0272" num="0275">Next, a straight line passing through the vertex C and parallel to the straight line (first straight line) connecting the first vanishing point to the second vanishing point (a second straight line) is calculated. Then, the intersection of a straight line passing through the vertices A and B and the second straight line (a first point), and the intersection of a straight line passing through the vertices A and D and the second straight line (a second point) are calculated.</p>
<p id="p-0273" num="0276">Next, as shown in <figref idref="DRAWINGS">FIG. 60</figref>, 15 division points dividing the straight line connecting the first point to the vertex C into 16 equal parts are calculated, and the calculated division points are connected to the second vanishing point. Similarly, 15 division points dividing the straight line connecting the vertex C to the second point into 16 equal parts are calculated, and the calculated division points are connected to the first vanishing point. The intersections thus generated are determined as the sample points in the captured real image.</p>
<p id="p-0274" num="0277">According to the result of verification carried out by the present inventor, it has been confirmed that the employment of the second determination method makes it possible to determine the sample points in the captured real image more appropriately than the first determination method. Even the first determination method, however, has a great advantage over the method shown in <figref idref="DRAWINGS">FIG. 51</figref>, and therefore, a designer may appropriately determine whether to employ the first determination method or the second determination method, taking into account conditions, such as a required detection accuracy and the complexity of the design of the marker <b>50</b>.</p>
<p id="p-0275" num="0278">As in the first determination method and the second determination method, the sample points in the captured real image are determined by dividing each side of at least one pair of two opposing sides into unequal parts, whereby it is possible to distinguish the marker <b>50</b> more accurately than the method shown in <figref idref="DRAWINGS">FIG. 51</figref>.</p>
<p id="p-0276" num="0279">It should be noted that the method of dividing each side of two opposing sides into unequal parts is not limited to the first determination method and the second determination method, and another method may be employed.</p>
<p id="p-0277" num="0280">It should be noted that, immediately after the vertex detection process, it is not possible to determine which vertex among the first through fourth vertices detected in the vertex detection process corresponds to which vertex among the upper left vertex, the lower left vertex, the lower right vertex, and the upper right vertex of the marker <b>50</b>. Accordingly, there are the following four possible cases: where the first vertex corresponds to the upper left vertex; where the first vertex corresponds to the lower left vertex; where the first vertex corresponds to the lower right vertex; and where the first vertex corresponds to the upper right vertex. Thus, in the design distinction process, the pixel values of the sample points in the captured real image in each of these four cases are checked against the pattern definition data. As a result, in the captured real image, the coordinates of the upper left vertex, the coordinates of the lower left vertex, the coordinates of the lower right vertex, and the coordinates of the upper right vertex are detected, and the detected coordinates are stored in the main memory <b>32</b> as marker position information.</p>
<p id="p-0278" num="0281">In the game apparatus <b>10</b>, on the basis of captured real images sequentially acquired in real time from the camera, the contour detection process, the vertex detection process, the rough distinction process, and the design distinction process are performed in a predetermined cycle (e.g., in a cycle of 1/60 seconds) and repeated. This makes it possible to detect in real time the position of the marker <b>50</b> in the captured real image.</p>
<p id="p-0279" num="0282">However, due, for example, to the manner of the application of light to the marker <b>50</b>, even though the marker <b>50</b> is displayed in the captured real image, the detection of the contour and the vertices of the marker <b>50</b> in the captured real image may temporarily fail. When the detection of the contour and the vertices of the marker <b>50</b> has temporarily failed, for example, in the state shown in <figref idref="DRAWINGS">FIG. 4</figref>, the virtual object <b>60</b> temporarily disappears even though the user has not moved the game apparatus <b>10</b>. If the virtual object <b>60</b> frequently disappears and appears, the user's interest is dampened.</p>
<p id="p-0280" num="0283">In the present embodiment, to prevent such an unfavorable phenomenon (an unintended change in the position of the marker), when the detection of the contour and the vertices of the marker <b>50</b> has failed in the current captured real image, the design distinction process is performed on the current captured real image on the basis of the positions of the vertices (or the sample points) of the marker <b>50</b> detected from the most recent captured real image. It should be noted that the &#x201c;current captured real image&#x201d; means the captured real image that is currently being processed, and does not necessarily mean the latest captured real image captured by the camera.</p>
<p id="p-0281" num="0284">Specifically, when the detection of the contour and the vertices of the marker <b>50</b> has failed in the current captured real image, the positions of the four vertices of the marker <b>50</b> detected from the most recent captured real image (or the sample points determined on the basis of the four vertices) are, as shown in <figref idref="DRAWINGS">FIG. 61</figref>, acquired from the marker position information stored in the main memory <b>32</b>. Then, as shown in. <figref idref="DRAWINGS">FIG. 62</figref>, on the basis of the positions of the four vertices of the marker <b>50</b> detected from the most recent captured real image, the positions of the sample points in the current captured real image are determined, and the design distinction process is performed using the pixel values of the sample points thus determined (or the sample points determined in the previous design distinction process). As a result of the design distinction process, when it is determined that in the current captured real image, the marker <b>50</b> is present at the same position as that in the most recent captured real image, the coordinates of the four vertices of the marker <b>50</b> in the most recent captured real image are stored in the main memory <b>32</b> as marker position information corresponding to the current captured real image.</p>
<p id="p-0282" num="0285">By the process as described above, in the case where the user has not moved the game apparatus <b>10</b>, even if the detection of the contour and the vertices of the marker <b>50</b> in the captured real image has temporarily failed, it is possible to detect the position of the marker <b>50</b>. Accordingly, as described above, it is possible to prevent the virtual object <b>60</b> from frequently disappearing and appearing even though the user has not moved the game apparatus <b>10</b>.</p>
<p id="p-0283" num="0286">It should be noted that the position of the marker <b>50</b> in the current captured real image may be slightly shifted from the position of the marker <b>50</b> in the most recent captured real image. In response, in another embodiment, the design distinction process may be performed not only on the position of the marker <b>50</b> in the most recent captured real image, but also on the range near the position of the marker <b>50</b> in the most recent captured real image. For example, the design distinction process may be performed multiple times while slightly shifting the positions of the four vertices of the marker <b>50</b> detected from the most recent captured real image. Then, among these results, the positions of the four vertices having the highest degree of similarity to the pattern definition data may be determined as the positions of the four vertices of the marker <b>50</b> in the current captured real image.</p>
<p id="p-0284" num="0287">It should be noted that in the present embodiment, when the detection of the contour and the vertices of the marker <b>50</b> in the current captured real image has failed, the design distinction process is performed on the current captured real image on the basis of the positions of the vertices (or the sample points) of the marker <b>50</b> detected from the most recent captured real image; however, the present invention is not limited to this. Alternatively, the design distinction process may be performed on the current captured real image on the basis of the positions of the vertices (or the sample points) of the marker <b>50</b> detected from another given captured real image obtained prior to the current captured real image (i.e., on the basis of marker position information corresponding to another given captured real image, the information already stored in the main memory <b>32</b>).</p>
<p id="p-0285" num="0288">It should be noted that in the present embodiment, when the detection of the contour and the vertices of the marker <b>50</b> in the current captured real image has failed, the design distinction process is performed on the current captured real image on the basis of the positions of the vertices (or the sample points) of the marker <b>50</b> detected from the most recent captured real image. Such a process is not limited to the case of performing the design distinction process using the pattern matching technique shown in <figref idref="DRAWINGS">FIGS. 48 through 60</figref>, but is also effective in the case of performing the design distinction process using another given known pattern matching technique.</p>
<p id="p-0286" num="0289">(Marker Position Correction Process)</p>
<p id="p-0287" num="0290">Next, the marker position correction process is described. The marker position correction process is a process of appropriately correcting the position of the marker <b>50</b> detected in the design distinction process (i.e., the positions of the four vertices of the black area of the marker <b>50</b>).</p>
<p id="p-0288" num="0291">Before specifically describing the marker position correction process, first, a description is given of problems that may arise if the marker position correction process is not performed.</p>
<p id="p-0289" num="0292">As described above, the position of the marker <b>50</b> in the captured real image is detected on the basis of the result of the contour detection process performed on the captured real image. In the contour detection process, edge pixels are detected by comparing the edge determination threshold to the luminance value of each pixel. Here, when a pixel is present that has a luminance value very close to the edge determination threshold, the pixel may be, for example, determined as a black area in a captured real image, and then determined as a white area in the next captured real image. This can occur even when the camera (i.e., the game apparatus <b>10</b>) has not been moved at all. This is because the luminance value of each pixel can slightly vary over time due, for example, to environmental light. When the results of the contour detection process vary, the position of the marker <b>50</b> ultimately detected in the captured real image also varies. Accordingly, for example, the position and the orientation of the virtual object <b>60</b> shown in <figref idref="DRAWINGS">FIG. 4</figref> vary even though the user has not moved the game apparatus <b>10</b>. That is, the virtual space image seems to deviate. To prevent (or reduce) such an unfavorable phenomenon, in the present embodiment, the marker position correction process is performed.</p>
<p id="p-0290" num="0293">In the marker position correction process, first, the amount of movement of the marker <b>50</b> in the captured real image is calculated on the basis of the position of the marker <b>50</b> detected from the most recent captured real image and the position of the marker <b>50</b> detected from the current captured real image. With reference to <figref idref="DRAWINGS">FIG. 63</figref>, a description is given of an example of the calculation method of the amount of movement of the marker <b>50</b> in the captured real image.</p>
<p id="p-0291" num="0294">As shown in <figref idref="DRAWINGS">FIG. 63</figref>, the upper left vertex of the marker <b>50</b> detected from the most recent captured real image is Vp<b>1</b>; the lower left vertex is Vp<b>2</b>; the lower right vertex is Vp<b>3</b>; and the upper right vertex is Vp<b>4</b>. The upper left vertex of the marker <b>50</b> detected from the current captured real image is Vc<b>1</b>; the lower left vertex is Vc<b>2</b>; the lower right vertex is Vc<b>3</b>; and the upper right vertex is Vc<b>4</b>. Then, the distance between Vp<b>1</b> and Vc<b>1</b> is a; the distance between Vp<b>2</b> and Vc<b>2</b> is b; the distance between Vp<b>3</b> and Vc<b>3</b> is c; and the distance between Vp<b>4</b> and Vc<b>4</b> is d. In this case, the amount of movement of the marker <b>50</b> in the captured real image is a^2+b^2+c^2+d^2 (&#x201c;^&#x201d; represents power).</p>
<p id="p-0292" num="0295">It should be noted that the calculation method described above is merely illustrative, and the calculation method of the amount of movement of the marker <b>50</b> in the captured real image is not limited to this.</p>
<p id="p-0293" num="0296">When the amount of movement of the marker <b>50</b> in the captured real image has been calculated, subsequently, the position of the marker <b>50</b> detected in the design distinction process is corrected on the basis of the calculated amount of movement. With reference to <figref idref="DRAWINGS">FIG. 64</figref>, a description is given of the correction method of the position of the marker <b>50</b>.</p>
<p id="p-0294" num="0297">When the amount of movement of the marker <b>50</b> is less than D<b>1</b>, the positions of the vertices Vc (Vc<b>1</b> through Vc<b>4</b> shown in <figref idref="DRAWINGS">FIG. 63</figref>) of the marker <b>50</b> in the current captured real image are corrected to the positions of the vertices Vp (Vp<b>1</b> through Vp<b>4</b> shown in <figref idref="DRAWINGS">FIG. 63</figref>) that have been previously detected. It should be noted that D<b>1</b> is a predetermined threshold, and as described later, the value of D<b>1</b> varies depending on the size of the marker <b>50</b> in the captured real image.</p>
<p id="p-0295" num="0298">When the amount of movement of the marker <b>50</b> is D<b>1</b> or greater but less than D<b>2</b>, the positions of the vertices Vc (Vc<b>1</b> through Vc<b>4</b> shown in <figref idref="DRAWINGS">FIG. 63</figref>) of the marker <b>50</b> in the current captured real image are corrected to the positions calculated by Vp&#xd7;A+Vc&#xd7;(1&#x2212;A). It should be noted that D<b>2</b> is a predetermined threshold greater than D<b>1</b>, and as described later, the value of D<b>2</b> varies depending on the size of the marker <b>50</b> in the captured real image. Further, A is a predetermined value greater than 0 but less than 1, and as described later, the value of A varies depending on the motion vector of the marker <b>50</b>.</p>
<p id="p-0296" num="0299">When the amount of movement of the marker <b>50</b> is D<b>2</b> or greater, the positions of the vertices Vc (Vc<b>1</b> through Vc<b>4</b> shown in <figref idref="DRAWINGS">FIG. 63</figref>) of the marker <b>50</b> in the current captured real image are not corrected.</p>
<p id="p-0297" num="0300">As described above, when the amount of movement of the marker <b>50</b> is less than D<b>1</b> (i.e., the amount of movement of the marker <b>50</b> is very small), it is determined that the marker <b>50</b> has not moved at all from the position of the marker <b>50</b> in the most recent captured real image. Accordingly, it is possible to prevent an unintended change in the position of the marker, and consequently, it is possible to prevent a deviation in the virtual space image.</p>
<p id="p-0298" num="0301">In addition, when the amount of movement of the marker <b>50</b> is D<b>1</b> or greater but less than D<b>2</b> (i.e., the amount of movement of the marker <b>50</b> is small), as shown in <figref idref="DRAWINGS">FIG. 65</figref>, the position of the marker <b>50</b> is corrected to a position on the line segments connecting the position of the marker <b>50</b> in the most recent captured real image to the position of the marker <b>50</b> in the current captured real image (i.e., points internally dividing the line segments connecting Vp to Vc, respectively, in a ratio of (1&#x2212;A):A). Accordingly, it is possible to reduce an unintended change in the position of the marker as described above, and consequently, it is possible to reduce a deviation in the virtual space image. Further, unlike the case where the amount of movement of the marker <b>50</b> is less than D<b>1</b>, the corrected position of the marker <b>50</b> is a position closer to the position of the marker <b>50</b> in the current captured real image than to the position of the marker <b>50</b> in the most recent captured real image. Thus, when the user has moved the game apparatus <b>10</b> by a small amount (or slowly), the position of the marker <b>50</b> (i.e., the position of the virtual object <b>60</b>) is updated in accordance with the motion of the game apparatus <b>10</b>, while reducing an unintended change in the position of the marker.</p>
<p id="p-0299" num="0302">It should be noted that when the amount of movement of the marker <b>50</b> is D<b>2</b> or greater (i.e., when the amount of movement of the marker <b>50</b> is large), the position of the marker <b>50</b> is not corrected. Accordingly, when the user has moved the game apparatus <b>10</b> rapidly by a large amount, the position of the marker <b>50</b> is updated in immediate response to such a rapid motion of the game apparatus <b>10</b>. Thus, for example, the virtual object <b>60</b> shown in <figref idref="DRAWINGS">FIG. 4</figref> is not displayed so as to be shifted significantly from the marker <b>50</b>.</p>
<p id="p-0300" num="0303">Next, a description is given of the determination method of the thresholds D<b>1</b> and D<b>2</b> described above.</p>
<p id="p-0301" num="0304">As described above, the thresholds D<b>1</b> and D<b>2</b> are thresholds for determining the level of the amount of movement of the marker <b>50</b> in the captured real image (i.e., very small, small, or large), and these thresholds are preferably changed depending on the size of the marker <b>50</b> in the captured real image. With reference to <figref idref="DRAWINGS">FIGS. 66 and 67</figref>, the reason for this is explained below.</p>
<p id="p-0302" num="0305"><figref idref="DRAWINGS">FIG. 66</figref> is an example where the virtual object <b>60</b> is displayed at the most recent position of the marker when the size of the marker <b>50</b> in the captured real image is large (i.e., when the position of the marker <b>50</b> is close to the camera in the real world). Here, the distance (the distance in the captured real image) between the most recent position of the marker and the current position of the marker is D. In this case, it seems to the user that the virtual object <b>60</b> is not shifted significantly from the current position of the marker.</p>
<p id="p-0303" num="0306"><figref idref="DRAWINGS">FIG. 67</figref> is an example where the virtual object <b>60</b> is displayed at the most recent position of the marker (i.e., the position of the marker <b>50</b> in the most recent captured real image) when the size of the marker <b>50</b> in the captured real image is small (i.e., when the position of the marker <b>50</b> is far from the camera in the real world). Here, the distance (the distance in the captured real image) between the most recent position of the marker and the current position of the marker is also D, as in <figref idref="DRAWINGS">FIG. 66</figref>. In this case, it seems to the user that the virtual object <b>60</b> is shifted significantly from the current position of the marker.</p>
<p id="p-0304" num="0307">As is clear from <figref idref="DRAWINGS">FIGS. 66 and 67</figref>, even when the distance (the distance in the captured real image) between the most recent position of the marker and the current position of the marker is the same, it seems to the user that the smaller the size of the marker <b>50</b> in the captured real image is, the more significantly the most recent position of the marker is shifted from the current position of the marker.</p>
<p id="p-0305" num="0308">In response, as shown in <figref idref="DRAWINGS">FIG. 68</figref>, it is preferable that the value of D<b>1</b> should be increased when the size of the marker <b>50</b> in the captured real image is large, and the value of D<b>1</b> should be decreased when the size of the marker <b>50</b> in the captured real image is small. That is, it is preferable that the smaller the size of the marker <b>50</b> in the captured real image, the smaller the value of the D<b>1</b>.</p>
<p id="p-0306" num="0309">Similarly, as shown in <figref idref="DRAWINGS">FIG. 69</figref>, it is preferable that the value of D<b>2</b> should be increased when the size of the marker <b>50</b> in the captured real image is large, and the value of D<b>2</b> should be decreased when the size of the marker <b>50</b> in the captured real image is small. That is, it is preferable that the smaller the size of the marker <b>50</b> in the captured real image, the smaller the value of D<b>2</b>.</p>
<p id="p-0307" num="0310">It should be noted that various possible methods can be used as the calculation method of the size of the marker <b>50</b> in the captured real image. For example, the area of the marker <b>50</b> in the captured real image may be calculated as the size of the marker <b>50</b> in the captured real image. In another embodiment, the size of the cross product of the two diagonals of the marker <b>50</b> in the captured real image may be calculated as the size of the marker <b>50</b> in the captured real image. In yet another embodiment, the diameter of a circle including the four vertices of the marker <b>50</b> in the captured real image may be calculated as the size of the marker <b>50</b> in the captured real image. In yet another embodiment, the size of the marker <b>50</b> in the captured real image may be calculated on the basis of the width in the X-axis direction and the width in the Y-axis direction of the marker <b>50</b> in the captured real image.</p>
<p id="p-0308" num="0311">Next, a description is given of the determination method of the predetermined value A.</p>
<p id="p-0309" num="0312">As described above, when the amount of movement of the marker <b>50</b> is D<b>1</b> or greater but less than D<b>2</b>, the position of the marker <b>50</b> is, as shown in <figref idref="DRAWINGS">FIG. 65</figref>, corrected to the points internally dividing, in a ratio of (1&#x2212;A):A, the line segments connecting the position of the marker <b>50</b> in the most recent captured real image to the position of the marker <b>50</b> in the current captured real image.</p>
<p id="p-0310" num="0313">Here, when the value of A is fixed to a small value (e.g., 0.1), the position of the marker <b>50</b> is corrected to almost the same position as the position of the marker <b>50</b> in the most recent captured real image. Accordingly, the responsiveness decreases, and even when the position of the marker <b>50</b> in the captured real image changes, the position of the virtual object <b>60</b> does not significantly change. Thus, a problem arises where, for example, when the user has moved the game apparatus <b>10</b> slowly and continuously in a desired direction, the virtual object <b>60</b> seems to be clearly shifted from the marker <b>50</b>.</p>
<p id="p-0311" num="0314">Conversely, when the value of A is fixed to a large value (e.g., 0.9), the position of the marker <b>50</b> is corrected to almost the same position as the position of the marker <b>50</b> in the current captured real image. Accordingly, although the responsiveness increases, a problem arises where the effect of reducing an unintended change in the position of the marker as described above is greatly impaired.</p>
<p id="p-0312" num="0315">In the present embodiment, to solve both of the above two problems, the value of A is varied in accordance with the motion vector of the marker <b>50</b>.</p>
<p id="p-0313" num="0316">Specifically, on the basis of the position of the marker <b>50</b> in a captured real image (e.g., the positions of the vertices of the marker <b>50</b>) and the position of the marker <b>50</b> in the most recent captured real image, motion vectors indicating in which direction the marker <b>50</b> has moved are sequentially calculated and sequentially stored in the main memory <b>32</b>. Then, on the basis of a newly calculated motion vector and a motion vector calculated in the past and stored in the main memory <b>32</b>, it is determined whether or not the marker <b>50</b> is continuously moving in a constant direction (which may be a generally constant direction) in the captured real image. When the marker <b>50</b> is continuously moving in a constant direction, the value of A is increased. If not, the value of A is decreased.</p>
<p id="p-0314" num="0317">The variation of the value of A as described above improves the responsiveness, for example, while the user is moving the game apparatus <b>10</b> slowly in a desired direction. Accordingly, the virtual object <b>60</b> does not seem to be shifted significantly from the marker <b>50</b>. Further, the responsiveness decreases in other situations, and therefore, the effect of reducing an unintended change in the position of the marker as described above is sufficiently exerted.</p>
<p id="p-0315" num="0318">It should be noted that in the present embodiment, as shown in <figref idref="DRAWINGS">FIG. 64</figref>, the correction method of the position of the marker <b>50</b> (e.g., the positions of the four vertices) is switched between: the case where the amount of movement of the marker <b>50</b> is less than D<b>1</b>; the case where the amount of movement of the marker <b>50</b> is D<b>1</b> or greater but less than D<b>2</b>; and the case where the amount of movement of the marker <b>50</b> is D<b>2</b> or greater. This is, however, merely illustrative, and the correction method of the position of the marker <b>50</b> is not limited to this. For example, as another embodiment, as shown in <figref idref="DRAWINGS">FIG. 70</figref>, when the amount of movement of the marker <b>50</b> is less than D<b>3</b>, the position of the marker <b>50</b> detected on the basis of the current captured real image may be corrected to the position of the marker <b>50</b> in the most recent captured real image. When the amount of movement of the marker <b>50</b> is D<b>3</b> or greater, the position of the marker <b>50</b> detected on the basis of the current captured real image may be used as it is without being corrected. This makes it possible to prevent an unintended change in the position of the marker. It should be noted that D<b>3</b> is a predetermined threshold, and therefore, the value of D<b>3</b> may vary in accordance with the size of the marker <b>50</b> in the captured real image.</p>
<p id="p-0316" num="0319">In addition, as yet another embodiment, as shown in <figref idref="DRAWINGS">FIG. 71</figref>, when the amount of movement of the marker <b>50</b> is less than D<b>4</b>, the position of the marker <b>50</b> detected on the basis of the current captured real image may be corrected to the points internally dividing, in a ratio of (1&#x2212;A):A, the line segments connecting the position of the marker <b>50</b> in the most recent captured real image to the position of the marker <b>50</b> in the current captured real image. When the amount of movement of the marker <b>50</b> is D<b>4</b> or greater, the position of the marker <b>50</b> detected on the basis of the current captured real image may be used as it is without being corrected. This makes it possible to reduce an unintended change in the position of the marker. It should be noted that D<b>4</b> is a predetermined threshold, and therefore, the value of D<b>4</b> may vary in accordance with the size of the marker <b>50</b> in the captured real image.</p>
<p id="p-0317" num="0320">It should be noted that the marker position correction process is, as described above, performed using the amount of movement of the marker <b>50</b> and the motion vector of the marker <b>50</b>. However, when a plurality of markers of the same design are included in the captured real image, it is necessary to determine where each marker has moved to. For example, as shown in <figref idref="DRAWINGS">FIG. 72</figref>, when a plurality of markers (a marker A and a marker B) of the same design as each other have been detected from the most recent captured real image and a plurality of markers (a first marker and a second marker) of the same design as the above have been detected from the current captured real image, it is necessary to determine the correspondence relationships between the markers.</p>
<p id="p-0318" num="0321">In the present embodiment, the distance between a representative point of each marker detected from the most recent captured real image and a representative point of the corresponding marker detected from the current captured real image is calculated. When the distance is smaller than a predetermined threshold, it is determined that the two markers correspond to each other. As a representative point of each marker, for example, the coordinates obtained by averaging the coordinates of the four vertices of the marker can be used.</p>
<p id="p-0319" num="0322">A specific description is given with reference to <figref idref="DRAWINGS">FIG. 72</figref>. First, the distance between the representative point of the marker A and the representative point of the first marker is calculated. When the distance is smaller than a predetermined threshold (it is preferable that the larger the size of the marker A or the first marker in the captured real image, the greater the threshold), it is determined that the marker A and the first marker correspond to each other.</p>
<p id="p-0320" num="0323">Similarly, the distance between the representative point of the marker A and the representative point of the second marker is calculated. When the distance is smaller than a predetermined threshold (it is preferable that the larger the size of the marker A or the second marker in the captured real image, the greater the threshold), it is determined that the marker A and the second marker correspond to each other.</p>
<p id="p-0321" num="0324">Yet similarly, the distance between the representative point of the marker B and the representative point of the first marker is calculated. When the distance is smaller than a predetermined threshold (it is preferable that the larger the size of the marker B or the first marker in the captured real image, the greater the threshold), it is determined that the marker B and the first marker correspond to each other.</p>
<p id="p-0322" num="0325">Yet similarly, the distance between the representative point of the marker B and the representative point of the second marker is calculated. When the distance is smaller than a predetermined threshold (it is preferable that the larger the size of the marker B or the second marker in the captured real image, the greater the threshold), it is determined that the marker B and the second marker correspond to each other.</p>
<p id="p-0323" num="0326">The determinations of the correspondence relationships between the markers as described above make it possible that even when a plurality of markers of the same design are included in the captured real image, the amount of movement and the motion vector of each marker are calculated.</p>
<p id="p-0324" num="0327">It should be noted that when a plurality of markers of different designs are included in the captured real image, it is possible to determine the correspondence relationships between the markers on the basis of the designs.</p>
<p id="p-0325" num="0328">It should be noted that in the marker position correction process described above, the amount of movement of the marker <b>50</b> in the captured real image is calculated on the basis of the position of the marker <b>50</b> detected from the most recent captured real image and the position of the marker <b>50</b> detected from the current captured real image; however, the present invention is not limited to this. Alternatively, for example, the amount of movement of the marker <b>50</b> in the captured real image may be calculated on the basis of the position of the marker <b>50</b> detected from a given captured real image acquired prior to the current captured real image (e.g., a captured real image acquired two images before the current captured real image) and the position of the marker <b>50</b> detected from the current captured real image.</p>
<p id="p-0326" num="0329">As described above, the position of the marker <b>50</b> (e.g., the positions of the four vertices of the black area of the marker <b>50</b>) is detected from the captured real image through the contour detection process, the vertex detection process, the rough distinction process, the design distinction process, and the marker position correction process. Then, the positional relationship between the camera (the outer capturing section (left) <b>23</b><i>a </i>or the outer capturing section (right) <b>23</b><i>b</i>) and the marker <b>50</b> in real space is calculated on the basis of the position of the marker <b>50</b> thus detected. The positional relationship between the virtual camera and the virtual object <b>60</b> in the virtual space is set on the basis of the calculation result. Then, the virtual space image is generated on the basis of the virtual camera, the virtual space image is combined with a captured real image captured by the camera, and the combined image is displayed on the upper LCD <b>22</b>.</p>
<p id="p-0327" num="0330">Next, a specific description is given of the flow of the image recognition process performed by the CPU <b>311</b> on the basis of the image recognition program.</p>
<p id="p-0328" num="0331"><figref idref="DRAWINGS">FIG. 73</figref> shows programs and data stored in the main memory <b>32</b>.</p>
<p id="p-0329" num="0332">The main memory <b>32</b> stores an image recognition program <b>70</b>, an image generation program <b>71</b>, virtual object data <b>72</b>, virtual camera data <b>73</b>, pattern definition data <b>74</b>, captured real image data <b>75</b>, an edge determination threshold <b>76</b>, edge pixel information <b>77</b>, straight line information <b>78</b>, vertex information <b>78</b>, marker position information <b>80</b>, motion vector information <b>81</b>, and various variables <b>82</b>.</p>
<p id="p-0330" num="0333">The image recognition program <b>70</b> is a computer program for detecting a marker from a captured real image. The image generation program <b>71</b> is a computer program for combining the captured real image with a virtual space image on the basis of the position of the marker detected on the basis of the image recognition program. These programs may be loaded into the main memory <b>32</b> from the data storage internal memory <b>35</b>, or may be loaded into the main memory <b>32</b> from the external memory <b>44</b>, or may be loaded into the main memory <b>32</b> from a server device or another game apparatus through the wireless communication module <b>36</b> or the local communication module <b>37</b>. It should be noted that the image generation process performed by the CPU <b>311</b> on the basis of the image generation program <b>71</b> may use a known technique, and has little relevance to the present invention, and therefore is not described in detail in the present specification. Further, the image recognition program <b>70</b> and the image generation program <b>71</b> may be configured as one image processing program.</p>
<p id="p-0331" num="0334">The virtual object data <b>72</b> is data concerning, for example, the shape, the color, and the pattern of the virtual object <b>60</b> placed in the virtual space.</p>
<p id="p-0332" num="0335">The virtual camera data <b>73</b> is data concerning, for example, the position and the orientation of the virtual camera placed in the virtual space.</p>
<p id="p-0333" num="0336">The pattern definition data <b>74</b> is data indicating the design of the marker <b>50</b>, the data used to distinguish the marker <b>50</b> and stored in advance (<figref idref="DRAWINGS">FIG. 49</figref>).</p>
<p id="p-0334" num="0337">The captured real image data <b>75</b> is image data of a captured real image captured by the camera (the outer capturing section (left) <b>23</b><i>a </i>or the outer capturing section (right) <b>23</b><i>b</i>).</p>
<p id="p-0335" num="0338">The edge determination threshold <b>76</b> is a threshold for, in the contour detection process, determining whether or not each pixel of the captured real image is an edge pixel.</p>
<p id="p-0336" num="0339">The edge pixel information <b>77</b> is information about a pixel determined as an edge pixel in the contour detection process.</p>
<p id="p-0337" num="0340">The straight line information <b>78</b> is information about a straight line generated or updated in the straight line calculation process.</p>
<p id="p-0338" num="0341">The vertex information <b>78</b> is information about a vertex calculated in the vertex calculation process.</p>
<p id="p-0339" num="0342">The marker position information <b>80</b> is information indicating the position of the marker <b>50</b> (the positions of the four vertices of the black area of the marker <b>50</b>) in the captured real image, the information generated in the vertex detection process and updated where necessary in the marker position correction process. The marker position information <b>80</b> includes not only information (<b>80</b><i>a</i>) indicating the position of the marker <b>50</b> detected from the current captured real image, but also information (<b>80</b><i>b</i>) indicating the position of the marker <b>50</b> detected from the most recent captured real image.</p>
<p id="p-0340" num="0343">The motion vector information <b>81</b> is information indicating the motion vector indicating the direction in which the marker <b>50</b> has moved.</p>
<p id="p-0341" num="0344">The various variables <b>82</b> are various variables (e.g., the white area luminance value Lw, the black area luminance value Lb, the threshold D<b>1</b>, the threshold D<b>2</b>, and the predetermined value A) used when the image recognition program <b>70</b> and the image generation program <b>71</b> are executed.</p>
<p id="p-0342" num="0345">First, with reference to <figref idref="DRAWINGS">FIG. 74</figref>, a description is given of the overall flow of the image recognition process performed by the CPU <b>311</b> on the basis of the image recognition program.</p>
<p id="p-0343" num="0346">In step S<b>1</b>, the CPU <b>311</b> acquires a captured real image captured by the camera (the outer capturing section (left) <b>23</b><i>a </i>or the outer capturing section (right) <b>23</b><i>b</i>), and stores the acquired captured real image in the main memory <b>32</b>.</p>
<p id="p-0344" num="0347">In step S<b>2</b>, the CPU <b>311</b> performs the contour detection process. A specific flow of the contour detection process will be described later with reference to <figref idref="DRAWINGS">FIG. 75</figref>.</p>
<p id="p-0345" num="0348">In step S<b>3</b>, the CPU <b>311</b> determines whether or not the contour has been detected in the contour detection process in step S<b>2</b>. When the contour has been detected, the processing proceeds to step S<b>4</b>. If not, the processing proceeds to step S<b>9</b>.</p>
<p id="p-0346" num="0349">In step S<b>4</b>, the CPU <b>311</b> performs the vertex detection process. A specific flow of the vertex detection process will be described later with reference to <figref idref="DRAWINGS">FIG. 76</figref>.</p>
<p id="p-0347" num="0350">In step S<b>5</b>, the CPU <b>311</b> determines whether or not the vertices have been detected in the vertex detection process in step S<b>4</b>. When the vertices have been detected, the processing proceeds to step S<b>6</b>. If not, the processing proceeds to step S<b>9</b>.</p>
<p id="p-0348" num="0351">In step S<b>6</b>, the CPU <b>311</b> performs the rough distinction process. A specific flow of the rough distinction process will be described later with reference to <figref idref="DRAWINGS">FIG. 77</figref>.</p>
<p id="p-0349" num="0352">In step S<b>7</b>, on the basis of the result of the rough distinction process in step S<b>6</b>, the CPU <b>311</b> determines whether or not vertices to be candidates for the marker <b>50</b> are present. When vertices to be candidates for the marker <b>50</b> have been detected, the processing proceeds to step S<b>8</b>. If not, the processing proceeds to step S<b>12</b>.</p>
<p id="p-0350" num="0353">In step S<b>8</b>, the CPU <b>311</b> performs the design distinction process. A specific flow of the design distinction process will be described later with reference to <figref idref="DRAWINGS">FIG. 78</figref>.</p>
<p id="p-0351" num="0354">In step S<b>9</b>, the CPU <b>311</b> performs the design distinction process on the basis of the position of the marker <b>50</b> in the most recent captured real image (i.e., the positions of the vertices detected in the most recent captured real image or the positions of the sample points determined in the most recent captured real image).</p>
<p id="p-0352" num="0355">In step S<b>10</b>, the CPU <b>311</b> determines whether or not the marker <b>50</b> has been detected in the design distinction process in step S<b>8</b> or step S<b>9</b>. When the marker <b>50</b> has been detected, the processing proceeds to step S<b>11</b>. If not, the processing proceeds to step S<b>12</b>.</p>
<p id="p-0353" num="0356">In step S<b>11</b>, the CPU <b>311</b> performs the marker position correction process. A specific flow of the marker position correction process will be described later with reference to <figref idref="DRAWINGS">FIG. 79</figref>.</p>
<p id="p-0354" num="0357">In step S<b>12</b>, the CPU <b>311</b> determines whether or not the image recognition process is to be ended. When the image recognition process is to be continued, the processing returns to step S<b>1</b>. When the image recognition process is to be ended, the CPU <b>311</b> ends the execution of the image recognition program.</p>
<p id="p-0355" num="0358">Next, with reference to <figref idref="DRAWINGS">FIG. 75</figref>, a description is given of the flow of the contour detection process performed by the CPU <b>311</b> on the basis of the image recognition program.</p>
<p id="p-0356" num="0359">In step S<b>21</b>, the CPU <b>311</b> determines the marked pixel P(n).</p>
<p id="p-0357" num="0360">In step S<b>22</b>, the CPU <b>311</b> determines whether or not L(n&#x2212;8) L(n) is 60 or greater. When L(n&#x2212;8) L(n) is 60 or greater, the processing proceeds to step S<b>23</b>. If not, the processing proceeds to step S<b>34</b>.</p>
<p id="p-0358" num="0361">In step S<b>23</b>, the CPU <b>311</b> assigns 8 to a variable k as an initial value.</p>
<p id="p-0359" num="0362">In step S<b>24</b>, the CPU <b>311</b> determines whether or not L(n&#x2212;k&#x2212;1) is smaller than L(n&#x2212;k). When L(n&#x2212;k&#x2212;1) is smaller than L(n&#x2212;k), the processing proceeds to step S<b>25</b>. If not, the processing proceeds to step S<b>26</b>.</p>
<p id="p-0360" num="0363">In step S<b>25</b>, the CPU <b>311</b> increments the variable k.</p>
<p id="p-0361" num="0364">In step S<b>26</b>, the CPU <b>311</b> determines that L(n&#x2212;k) is the white area luminance value Lw.</p>
<p id="p-0362" num="0365">In step S<b>27</b>, the CPU <b>311</b> determines whether or not L(n) is smaller than L(n+2). When L(n) is smaller than L(n+2), the processing proceeds to step S<b>28</b>. If not, the processing proceeds to step S<b>30</b>.</p>
<p id="p-0363" num="0366">In step S<b>28</b>, the CPU <b>311</b> determines whether or not Lw&#x2212;L(n+2) is 60 or greater. When Lw&#x2212;L(n+2) is 60 or greater, the processing proceeds to step S<b>29</b>. If not, the processing proceeds to step S<b>30</b>.</p>
<p id="p-0364" num="0367">In step S<b>29</b>, the CPU <b>311</b> determines that L(n+2) is the black area luminance value Lb.</p>
<p id="p-0365" num="0368">In step S<b>30</b>, the CPU <b>311</b> determines that L(n) is the black area luminance value Lb.</p>
<p id="p-0366" num="0369">In step S<b>31</b>, the CPU <b>311</b> determines that the average value of Lw and Lb is the edge determination threshold.</p>
<p id="p-0367" num="0370">In step S<b>32</b>, the CPU <b>311</b> detects the starting edge pixel on the basis of the edge determination threshold. The coordinates of the starting edge pixel are stored in the main memory <b>32</b> as edge pixel information.</p>
<p id="p-0368" num="0371">In step S<b>33</b>, the CPU <b>311</b> performs the edge tracking process of sequentially tracking adjacent edge pixels such that the starting point is the starting edge pixel. The coordinates of the edge pixels sequentially detected in the edge tracking process are sequentially stored in the main memory <b>32</b> as edge pixel information.</p>
<p id="p-0369" num="0372">In step S<b>34</b>, the CPU <b>311</b> determines whether or not a next marked pixel is present. When a next marked pixel is present, the processing returns to step S<b>21</b>. If not (i.e., when the processes on all the marked pixel candidates in the captured real image are completed), the contour detection process is ended.</p>
<p id="p-0370" num="0373">Next, with reference to <figref idref="DRAWINGS">FIG. 76</figref>, a description is given of the flow of the vertex detection process performed by the CPU <b>311</b> on the basis of the image recognition program.</p>
<p id="p-0371" num="0374">In step S<b>41</b>, the CPU <b>311</b> generates a first straight line (i.e., a straight line Li(<b>0</b>-<b>5</b>) corresponding to a vector V(<b>0</b>-<b>5</b>) connecting from a starting edge pixel Pe(<b>0</b>) to an edge pixel Pe (<b>5</b>)), and stores data indicating the straight line in the main memory <b>32</b>.</p>
<p id="p-0372" num="0375">In step S<b>42</b>, the CPU <b>311</b> determines whether or not the generated straight line and a vector following the straight line are placed on the same straight line, the determination made on the basis of the angle of the vector with respect to the straight line (see <figref idref="DRAWINGS">FIG. 24</figref>). When it is determined that the straight line and the vector are placed on the same straight line, the processing proceeds to step S<b>43</b>. If not, the processing proceeds to step S<b>44</b>.</p>
<p id="p-0373" num="0376">In step S<b>43</b>, the CPU <b>311</b> updates the straight line. Specifically, on the basis of sample edge pixels included from the rear end of the straight line to the head of the vector, the CPU <b>311</b> calculates the straight line by a least squares method, and updates the straight line in accordance with the calculation result (i.e., updates the data indicating the straight line stored in the main memory <b>32</b>).</p>
<p id="p-0374" num="0377">In step S<b>44</b>, on the basis of the angle of the vector with respect to the straight line, the CPU <b>311</b> determines whether or not the black area has a convex angle at the intersection of the straight line and the vector following the straight line (see <figref idref="DRAWINGS">FIG. 24</figref>). When it is determined that the black area has a convex angle, the processing proceeds to step S<b>45</b>. If not, the processing proceeds to step S<b>47</b>.</p>
<p id="p-0375" num="0378">In step S<b>45</b>, the CPU <b>311</b> newly generates a straight line corresponding to the vector, and newly stores data indicating the newly generated straight line in the main memory <b>32</b>.</p>
<p id="p-0376" num="0379">In step S<b>46</b>, the CPU <b>311</b> determines whether or not a circuit of the contour of the black area has been completed (i.e., the detection has returned to the starting edge pixel). When a circuit has been completed, the processing proceeds to step S<b>49</b>. If not, the processing returns to step S<b>42</b>.</p>
<p id="p-0377" num="0380">In step S<b>47</b>, the CPU <b>311</b> determines whether or not the straight line calculation process (i.e., the processes of steps S<b>42</b> through S<b>46</b>) is being performed counterclockwise. When the straight line calculation process is being performed counterclockwise, the processing proceeds to step S<b>48</b>. If not, the processing proceeds to step S<b>49</b>. It should be noted that in the present embodiment, the straight line calculation process is started counterclockwise first.</p>
<p id="p-0378" num="0381">In step S<b>48</b>, the CPU <b>311</b> switches the straight line calculation process from counterclockwise to clockwise.</p>
<p id="p-0379" num="0382">In step S<b>49</b>, on the basis of data representing the plurality of straight lines stored in the main memory <b>32</b>, the CPU <b>311</b> determines whether or not, among the plurality of straight lines, a plurality of straight lines placed on the same straight line and directed in the same direction are present. When a plurality of straight lines placed on the same straight line and directed in the same direction are present, the processing proceeds to step S<b>50</b>. If not, the processing proceeds to step S<b>51</b>.</p>
<p id="p-0380" num="0383">In step S<b>50</b>, the CPU <b>311</b> integrates the plurality of straight lines placed on the same straight line and directed in the same direction into one straight line, and updates the data concerning the plurality of straight lines stored in the main memory <b>32</b>.</p>
<p id="p-0381" num="0384">In step S<b>51</b>, on the basis of the data concerning the plurality of straight lines stored in the main memory <b>32</b>, the CPU <b>311</b> selects four straight lines from among the plurality of straight lines. Specifically, the CPU <b>311</b> calculates the length of each straight line, and selects the longest straight line, the second longest straight line, the third longest straight line, and the fourth longest straight line.</p>
<p id="p-0382" num="0385">In step S<b>52</b>, the CPU <b>311</b> calculates the positions of the four vertices of the black area by calculating the positions of the intersections of the four straight lines. Then, the CPU <b>311</b> stores the positions of the four vertices in the main memory <b>32</b>, and ends the vertex detection process.</p>
<p id="p-0383" num="0386">Next, with reference to <figref idref="DRAWINGS">FIG. 77</figref>, a description is given of the flow of the rough distinction process performed by the CPU <b>311</b> on the basis of the image recognition program.</p>
<p id="p-0384" num="0387">In step S<b>61</b>, the CPU <b>311</b> determines whether or not the four vertices detected in the vertex detection process satisfy the exclusion condition A. The exclusion condition A is, as described above, the case where the distance between any two adjacent vertices is too small. When the exclusion condition A is satisfied, the processing proceeds to step S<b>65</b>. If not, the processing proceeds to step S<b>62</b>.</p>
<p id="p-0385" num="0388">In step S<b>62</b>, the CPU <b>311</b> determines whether or not the four vertices detected in the vertex detection process satisfy the exclusion condition B. The exclusion condition B is, as described above, the case where the distance between any vertex and either one of the two sides not adjacent to the vertex is too small. When the exclusion condition B is satisfied, the processing proceeds to step S<b>65</b>. If not, the processing proceeds to step S<b>63</b>.</p>
<p id="p-0386" num="0389">In step S<b>63</b>, the CPU <b>311</b> determines whether or not the four vertices detected in the vertex detection process satisfy the exclusion condition C. The exclusion condition C is, as described above, the case where the straight lines of any two opposing sides are directed in generally the same direction. When the exclusion condition. C is satisfied, the processing proceeds to step S<b>65</b>. If not, the processing proceeds to step S<b>64</b>.</p>
<p id="p-0387" num="0390">In step S<b>64</b>, the CPU <b>311</b> determines whether or not the four vertices detected in the vertex detection process satisfy the exclusion condition D. The exclusion condition D is, as described above, the case where a concave angle is included. When the exclusion condition D is satisfied, the processing proceeds to step S<b>65</b>. If not, the rough distinction process is ended.</p>
<p id="p-0388" num="0391">In step S<b>65</b>, the CPU <b>311</b> excludes the four vertices detected in the vertex detection process from process objects in the design distinction process (e.g., deletes data concerning the four vertices from the main memory <b>32</b>). Then, the rough distinction process is ended.</p>
<p id="p-0389" num="0392">Next, with reference to <figref idref="DRAWINGS">FIG. 78</figref>, a description is given of the flow of the design distinction process performed by the CPU <b>311</b> on the basis of the image recognition program.</p>
<p id="p-0390" num="0393">In step S<b>71</b>, the CPU <b>311</b> selects any two opposing sides from among the four sides of the rectangle surrounded by the four vertices detected in the vertex detection process.</p>
<p id="p-0391" num="0394">In step S<b>72</b>, the CPU <b>311</b> determines whether or not the two sides selected in step S<b>71</b> are parallel (including the case where they are generally parallel) to each other. When the two sides are parallel to each other, the processing proceeds to step S<b>73</b>. If not, the processing proceeds to step S<b>74</b>.</p>
<p id="p-0392" num="0395">In step S<b>73</b>, the CPU <b>311</b> divides each of the two sides selected in step S<b>71</b> into 16 equal parts.</p>
<p id="p-0393" num="0396">In step S<b>74</b>, the CPU <b>311</b> divides each of the two sides selected in step S<b>71</b> into 16 unequal parts (e.g., by the method shown in <figref idref="DRAWINGS">FIGS. 53 through 55</figref>).</p>
<p id="p-0394" num="0397">In step S<b>75</b>, the CPU <b>311</b> selects the two opposing sides that have not been selected in step S<b>71</b>, from among the four sides of the rectangle surrounded by the four vertices detected in the vertex detection process.</p>
<p id="p-0395" num="0398">In step S<b>76</b>, the CPU <b>311</b> determines whether or not the two sides selected in step S<b>75</b> are parallel (including the case where there are generally parallel) to each other. When the two sides are parallel to each other, the processing proceeds to step S<b>77</b>. If not, the processing proceeds to step S<b>78</b>.</p>
<p id="p-0396" num="0399">In step S<b>77</b>, the CPU <b>311</b> divides each of the two sides selected in step S<b>75</b> into 16 equal parts.</p>
<p id="p-0397" num="0400">In step S<b>78</b>, the CPU <b>311</b> divides each of the two sides selected in step S<b>75</b> into 16 unequal parts (e.g., by the method shown in <figref idref="DRAWINGS">FIGS. 53 through 55</figref>).</p>
<p id="p-0398" num="0401">In step S<b>79</b>, on the basis of the pixel values of the sample points determined by dividing the four sides of the rectangle surrounded by the four vertices detected in the vertex detection process and the basis of the pixel values of the sample points defined in the pattern definition data, the CPU <b>311</b> calculates correlation coefficients representing the degrees of similarity between the pixel values.</p>
<p id="p-0399" num="0402">In step S<b>80</b>, on the basis of the correlation coefficients calculated in step S<b>79</b>, the CPU <b>311</b> determines whether or not the design displayed in the area surrounded by the four vertices detected in the vertex detection process coincides with the design of the marker <b>50</b>. When the designs coincide with each other, the processing proceeds to step S<b>81</b>. If not, the design distinction process is ended.</p>
<p id="p-0400" num="0403">In step S<b>81</b>, the CPU <b>311</b> stores, as marker position information in the main memory <b>32</b>, the coordinates of the four vertices (the upper left vertex, the lower left vertex, the lower right vertex, and the upper right vertex) of the marker <b>50</b> in the captured real image.</p>
<p id="p-0401" num="0404">Next, with reference to <figref idref="DRAWINGS">FIG. 79</figref>, a description is given of the flow of the marker position correction process performed by the CPU <b>311</b> on the basis of the image recognition program.</p>
<p id="p-0402" num="0405">In step S<b>91</b>, the CPU <b>311</b> calculates the size of the marker <b>50</b> in the captured real image, and stores the calculation result in the main memory <b>32</b>.</p>
<p id="p-0403" num="0406">In step S<b>92</b>, the CPU <b>311</b> determines whether or not the same marker as that detected from the current captured real image has been present in the most recent captured real image. When the same marker has been present, the processing proceeds to step S<b>93</b>. If not, the marker position correction process is ended (i.e., the position of the marker detected from the current captured real image is used as it is without being corrected). It should be noted that the determination of whether or not the same marker as that detected from the current captured real image has been present in the most recent captured real image can be made, for example, on the basis of the size of the marker <b>50</b> calculated in step S<b>91</b> and by the method as described with reference to <figref idref="DRAWINGS">FIG. 72</figref>.</p>
<p id="p-0404" num="0407">In step S<b>93</b>, on the basis of the size of the marker <b>50</b> calculated in step S<b>91</b>, the CPU <b>311</b> calculates the thresholds D<b>1</b> and D<b>2</b> (<figref idref="DRAWINGS">FIGS. 68 and 69</figref>), and stores the calculated thresholds D<b>1</b> and D<b>2</b> in the main memory <b>32</b>.</p>
<p id="p-0405" num="0408">In step S<b>94</b>, the CPU <b>311</b> determines whether or not the amount of movement of the marker <b>50</b> is less than D<b>1</b>. When the amount of movement of the marker <b>50</b> is less than D<b>1</b>, the processing proceeds to step S<b>95</b>. If not, the processing proceeds to step S<b>96</b>.</p>
<p id="p-0406" num="0409">In step S<b>95</b>, the CPU <b>311</b> corrects the position of the marker <b>50</b> detected in the design distinction process to the position of the marker that has been previously detected. Then, the marker position correction process is ended.</p>
<p id="p-0407" num="0410">In step S<b>96</b>, the CPU <b>311</b> determines whether or not the amount of movement of the marker <b>50</b> is less than D<b>2</b>. When the amount of movement of the marker <b>50</b> is less than D<b>2</b>, the processing proceeds to step S<b>97</b>. If not, the marker position correction process is ended (i.e., the position of the marker detected from the current captured real image is used as it is without being corrected).</p>
<p id="p-0408" num="0411">In step S<b>97</b>, the CPU <b>311</b> calculates the motion vector of the marker <b>50</b>, and determines the predetermined value A on the basis of the motion vector and a motion vector calculated in the past. Specifically, on the basis of these motion vectors, the CPU <b>311</b> determines whether or not the marker <b>50</b> is continuously moving in a constant direction in the captured real image. When the marker <b>50</b> is continuously moving in a constant direction, the value of A is increased. If not, the value of A is decreased. The motion vector calculated in this process is stored in the main memory <b>32</b> as the motion vector information <b>81</b>.</p>
<p id="p-0409" num="0412">In step S<b>98</b>, the CPU <b>311</b> corrects the position of the marker <b>50</b> to the points internally dividing, in a ratio of (1&#x2212;A):A, the line segments connecting the position of the marker <b>50</b> in the most recent captured real image to the position of the marker <b>50</b> in the current captured real image. Then, the marker position correction process is ended.</p>
<p id="p-0410" num="0413">(Variations)</p>
<p id="p-0411" num="0414">It should be noted that in the above embodiment, specific processing methods are described for: (1) the contour detection process; (2) the vertex detection process; (3) the rough distinction process; (4) the design distinction process; and (5) the marker position correction process. Alternatively, one or more of these processes may be replaced with known techniques.</p>
<p id="p-0412" num="0415">In addition, an image serving as a process object of the image recognition process is not limited to captured real images sequentially acquired in real time from the camera. Alternatively, for example, the image may be an image captured by the camera in the past and stored in the data storage internal memory <b>35</b> or the like, or may be an image received from another device, or may be an image acquired through an external storage medium.</p>
<p id="p-0413" num="0416">In addition, a recognition object of the image recognition process is not limited to the black area of the marker <b>50</b>. The recognition object may be a given object (e.g., a person's face or hand), or a given design, included in an image.</p>
<p id="p-0414" num="0417">In addition, the result of the image recognition process can be used not only in AR technology, but also in another given application.</p>
<p id="p-0415" num="0418">In addition, in the present embodiment, image processing (the image recognition process and the image generation process) is performed by the game apparatus <b>10</b>; however, the present invention is not limited to this. Alternatively, image processing may be performed by a given information processing apparatus (or a given information processing system) such as a stationary game apparatus, a personal computer, and a mobile phone.</p>
<p id="p-0416" num="0419">In addition, in the present embodiment, the image processing is performed by one game apparatus <b>10</b>. Alternatively, in another embodiment, the image processing may be performed by a plurality of information processing apparatuses capable of communicating with one another in a shared manner.</p>
<p id="p-0417" num="0420">In addition, in the present embodiment, the image recognition program and the like are performed by one CPU <b>311</b>. Alternatively, in another embodiment, the image recognition program and the like may be performed by a plurality of CPUs <b>311</b> in a shared manner.</p>
<p id="p-0418" num="0421">In addition, in the present embodiment, the image processing is performed by the CPU <b>311</b> on the basis of the image recognition program and the like. Alternatively, in another embodiment, part of the image processing may be achieved by hardware, instead of the CPU <b>311</b>.</p>
<p id="p-0419" num="0422">In addition, in the above embodiment, a captured real image captured by the camera is combined with a virtual space image, and the combined image is displayed on the upper LCD <b>22</b> (a video see-through method). Alternatively, instead of the video see-through method, an optical see-through method may be employed in which a virtual object is displayed on a transmissive display screen so that a user views an image as if the virtual object actually exists in a real world that is visible through the transmissive display screen.</p>
<p id="p-0420" num="0423">While the invention has been described in detail, the foregoing description is in all aspects illustrative and not restrictive. It will be understood that numerous other modifications and variations can be devised without departing from the scope of the invention.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A non-transitory computer-readable storage medium having stored thereon an image recognition program which, when executed by a computer of an information processing apparatus, causes the computer to perform operations comprising:
<claim-text>acquiring an image;</claim-text>
<claim-text>detecting from the image a plurality of vertices of a contour of an object or of a design;</claim-text>
<claim-text>generation means generating a predetermined number of division points on each of sides connecting the plurality of vertices to each other, so as to divide each side of at least one pair of two opposing sides into unequal parts, wherein the division points on each side of at least one pair of two opposing sides are generated such that the closer to one end of each side of the two opposing sides, the denser the division points;</claim-text>
<claim-text>determining a plurality of sample points on the basis of straight lines connecting the division points on the two opposing sides to one another; and</claim-text>
<claim-text>determining, on the basis of pixel values of the sample points, whether or not an object or design is displayed in an area surrounded by the plurality of vertices in the image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The non-transitory computer-readable storage medium according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the operations further comprise
<claim-text>dividing each side of at least one pair of two opposing sides into unequal parts in accordance with a ratio of: a distance between one end of one side of the two opposing sides and a corresponding one end of the other side, to a distance between the other ends.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The non-transitory computer-readable storage medium according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, the operations further comprise:
<claim-text>determining whether or not, among the plurality of sides connecting the plurality of vertices to each other, given two opposing sides are parallel to each other;
<claim-text>generating, when it is determined that any two opposing sides are parallel to each other, the division points so as to divide each side of the two opposing sides into equal parts; and</claim-text>
</claim-text>
<claim-text>generating, when it is determining that any two opposing sides are not parallel to each other, the division points so as to divide each side of the two opposing sides into unequal parts.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The non-transitory computer-readable storage medium according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the operations further comprising
<claim-text>calculating a first vanishing point, the first vanishing point being an intersection of straight lines extending from a first side and a second side, respectively, the first side and the second side opposing each other;</claim-text>
<claim-text>calculating a second vanishing point, the second vanishing point being an intersection of straight lines extending from a third side and a fourth side, respectively, the third side and the fourth side opposing each other;</claim-text>
<claim-text>calculating a second straight line, the second straight line being parallel to a first straight line connecting the first vanishing point to the second vanishing point, the second straight line passing through the vertex furthest from the first straight line;</claim-text>
<claim-text>generating, on a line segment connecting an intersection of the straight line passing through the first side and the second straight line to an intersection of the straight line passing through the second side and the second straight line, a plurality of first equal division points so as to divide the line segment into equal parts;</claim-text>
<claim-text>generating, on a line segment connecting an intersection of the straight line passing through the third side and the second straight line to an intersection of the straight line passing through the fourth side and the second straight line, a plurality of second equal division points so as to divide the line segment into equal parts;</claim-text>
<claim-text>generating, on the basis of straight lines connecting the first vanishing point to the first equal division points, the division points on each of the third side and the fourth side so as to divide each of the third side and the fourth side into unequal parts; and</claim-text>
<claim-text>generating, on the basis of straight lines connecting the second vanishing point to the second equal division points, the division points on each of the first side and the second side so as to divide each of the first side and the second side into unequal parts.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The non-transitory computer-readable storage medium according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the operations further comprise
<claim-text>repeatedly acquiring images, and</claim-text>
<claim-text>detecting the plurality of vertices from the repeatedly acquired images.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The non-transitory computer-readable storage medium according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the operation further comprise
<claim-text>determining, by comparing the pixel values of the sample points with pixel values corresponding to the object or design that are stored in advance, whether or not the object or design is displayed in the area surrounded by the plurality of vertices in the image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. An image recognition apparatus comprising:
<claim-text>memory; and</claim-text>
<claim-text>at least one processing circuitry communicatively coupled to the memory, and configured to;</claim-text>
<claim-text>receive an image;</claim-text>
<claim-text>detect from the image a plurality of vertices of a contour of an object or of a design;</claim-text>
<claim-text>generate a predetermined number of division points on each of sides connecting the plurality of vertices to each other, so as to divide each side of at least one pair of two opposing sides into unequal parts, wherein the division points on each side of at least one pair of two sides are generated such that the closer to one end of each side of the two opposing sides, the denser the division points;</claim-text>
<claim-text>determine a plurality of sample points on the basis of straight lines connecting the division points on the two opposing sides to one another; and</claim-text>
<claim-text>determine, for on the basis of pixel values of the sample points, whether or not a object or design is displayed in an area surrounded by the plurality of vertices in the image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The image recognition apparatus according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the at least one processor is further configured to:
<claim-text>divide each side of at least one pair of two opposing sides into unequal parts in accordance with a ratio of: a distance between one end of one side of the two opposing sides and a corresponding one end of the other side, to a distance between the other ends.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The image recognition apparatus according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the at least one processor is further configured to:
<claim-text>determine whether or not, among the plurality of sides connecting the plurality of vertices to each other, given two opposing sides are parallel to each other;</claim-text>
<claim-text>when it has been determined that any two opposing sides are parallel to each other, generating the division points so as to divide each side of the two opposing sides into equal parts; and</claim-text>
<claim-text>when it has been determined that any two opposing sides are not parallel to each other, generating the division points so as to divide each side of the two opposing sides into unequal parts.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The image recognition apparatus according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the at least one processor is further configured to:
<claim-text>calculate a first vanishing point, the first vanishing point being an intersection of straight lines extending from a first side and a second side, respectively, the first side and the second side opposing each other;</claim-text>
<claim-text>calculate a second vanishing point, the second vanishing point being an intersection of straight lines extending from a third side and a fourth side, respectively, the third side and the fourth side opposing each other;</claim-text>
<claim-text>calculate a second straight line, the second straight line being parallel to a first straight line connecting the first vanishing point to the second vanishing point, the second straight line passing through the vertex furthest from the first straight line;</claim-text>
<claim-text>generate, on a line segment connecting an intersection of the straight line passing through the first side and the second straight line to an intersection of the straight line passing through the second side and the second straight line, a plurality of first equal division points so as to divide the line segment into equal parts;</claim-text>
<claim-text>generate, on a line segment connecting an intersection of the straight line passing through the third side and the second straight line to an intersection of the straight line passing through the fourth side and the second straight line, a plurality of second equal division points so as to divide the line segment into equal parts;</claim-text>
<claim-text>generate, on the basis of straight lines connecting the first vanishing point to the first equal division points, generating the division points on each of the third side and the fourth side so as to divide each of the third side and the fourth side into unequal parts; and</claim-text>
<claim-text>generate, on the basis of straight lines connecting the second vanishing point to the second equal division points, the division points on each of the first side and the second side so as to divide each of the first side and the second side into unequal parts.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. An image recognition method comprising:
<claim-text>receiving an image;</claim-text>
<claim-text>detecting from the image a plurality of vertices of a contour of an object or of a design;</claim-text>
<claim-text>generating a predetermined number of division points on each of sides connecting the plurality of vertices to each other, so as to divide each side of at least one pair of two opposing sides into unequal parts, wherein the division points on each side of at least one pair of two opposing sides are generated such that the closer to one end of each side of the two opposing sides, the denser the division points;</claim-text>
<claim-text>determining a plurality of sample points on the basis of straight lines connecting the division points on the two opposing sides to one another; and</claim-text>
<claim-text>determine, on the basis of pixel values of the sample points, whether or not a object or design is displayed in an area surrounded by the plurality of vertices in the image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The image recognition method according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising: dividing each side of at least one pair of two opposing sides into unequal parts in accordance with a ratio of: a distance between one end of one side of the two opposing sides and a corresponding one end of the other side, to a distance between the other ends.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The image recognition method according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising:
<claim-text>determining whether or not, among the plurality of sides connecting the plurality of vertices to each other, given two opposing sides are parallel to each other;</claim-text>
<claim-text>when it has been determined that any two opposing sides are parallel to each other, generating the division points so as to divide each side of the two opposing sides into equal parts; and</claim-text>
<claim-text>when it has been determined that any two opposing sides are not parallel to each other, generating the division points so as to divide each side of the two opposing sides into unequal parts.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The image recognition method according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising:
<claim-text>calculating a first vanishing point, the first vanishing point being an intersection of straight lines extending from a first side and a second side, respectively, the first side and the second side opposing each other;</claim-text>
<claim-text>calculating a second vanishing point, the second vanishing point being an intersection of straight lines extending from a third side and a fourth side, respectively, the third side and the fourth side opposing each other;</claim-text>
<claim-text>calculating a second straight line, the second straight line being parallel to a first straight line connecting the first vanishing point to the second vanishing point, the second straight line passing through the vertex furthest from the first straight line;</claim-text>
<claim-text>generating, on a line segment connecting an intersection of the straight line passing through the first side and the second straight line to an intersection of the straight line passing through the second side and the second straight line, a plurality of first equal division points so as to divide the line segment into equal parts;</claim-text>
<claim-text>generating, on a line segment connecting an intersection of the straight line passing through the third side and the second straight line to an intersection of the straight line passing through the fourth side and the second straight line, a plurality of second equal division points so as to divide the line segment into equal parts;</claim-text>
<claim-text>generating, on the basis of straight lines connecting the first vanishing point to the first equal division points, generating the division points on each of the third side and the fourth side so as to divide each of the third side and the fourth side into unequal parts; and</claim-text>
<claim-text>generating, on the basis of straight lines connecting the second vanishing point to the second equal division points, the division points on each of the first side and the second side so as to divide each of the first side and the second side into unequal parts.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. An image recognition system comprising:
<claim-text>an image capture device for capturing an image; and</claim-text>
<claim-text>at least one processor communicatively coupled to the image capture device and configured to:</claim-text>
<claim-text>receive the image;</claim-text>
<claim-text>detect from the image a plurality of vertices of a contour of an object or of a design;</claim-text>
<claim-text>generate a predetermined number of division points on each of sides connecting the plurality of vertices to each other, so as to divide each side of at least one pair of two opposing sides into unequal parts, wherein the division points on each side of at least one pair of two opposing sides are generated such that the closer to one end of each side of the two opposing sides, the denser the division points;</claim-text>
<claim-text>determine a plurality of sample points on the basis of straight lines connecting the division points on the two opposing sides to one another; and</claim-text>
<claim-text>determine, on the basis of pixel values of the sample points, whether or not a object or design is displayed in an area surrounded by the plurality of vertices in the image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. An image recognition system including an image recognition apparatus and a marker in which a design is drawn, the image recognition apparatus comprising:
<claim-text>a capturing section for capturing the marker; and</claim-text>
<claim-text>at least one processor communicatively coupled to the image capture device and configured to:</claim-text>
<claim-text>receive an image from the capturing section;</claim-text>
<claim-text>detect from the image a plurality of vertices of a contour of the marker or of the design;</claim-text>
<claim-text>generate a predetermined number of division points on each of sides connecting the plurality of vertices to each other, so as to divide each side of at least one pair of two opposing sides into unequal parts, wherein the division points on each side of at least one pair of two opposing sides are generated such that the closer to one end of each side of the two opposing sides, the denser the division points;</claim-text>
<claim-text>determine a plurality of sample points on the basis of straight lines connecting the division points on the two opposing sides to one another; and</claim-text>
<claim-text>determine, on the basis of pixel values of the sample points, whether or not a design is displayed in an area surrounded by the plurality of vertices in the image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The image recognition system according to <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the at least one processor is further configured to divide each side of at least one pair of two opposing sides into unequal parts in accordance with a ratio of: a distance between one end of one side of the two opposing sides and a corresponding one end of the other side, to a distance between the other ends.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The image recognition system according to <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the at least one processor is further configured to:
<claim-text>determine whether or not, among the plurality of sides connecting the plurality of vertices to each other, given two opposing sides are parallel to each other;</claim-text>
<claim-text>when it has been determined that any two opposing sides are parallel to each other, generate the division points so as to divide each side of the two opposing sides into equal parts; and</claim-text>
<claim-text>when it has been determined that any two opposing sides are not parallel to each other, generate the division points so as to divide each side of the two opposing sides into unequal parts.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The image recognition system according to <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the at least one processor is further configured to:
<claim-text>calculate a first vanishing point, the first vanishing point being an intersection of straight lines extending from a first side and a second side, respectively, the first side and the second side opposing each other;</claim-text>
<claim-text>calculate a second vanishing point, the second vanishing point being an intersection of straight lines extending from a third side and a fourth side, respectively, the third side and the fourth side opposing each other;</claim-text>
<claim-text>calculate a second straight line, the second straight line being parallel to a first straight line connecting the first vanishing point to the second vanishing point, the second straight line passing through the vertex furthest from the first straight line;</claim-text>
<claim-text>calculate, on a line segment connecting an intersection of the straight line passing through the first side and the second straight line to an intersection of the straight line passing through the second side and the second straight line, a plurality of first equal division points so as to divide the line segment into equal parts;</claim-text>
<claim-text>calculate, on a line segment connecting an intersection of the straight line passing through the third side and the second straight line to an intersection of the straight line passing through the fourth side and the second straight line, a plurality of second equal division points so as to divide the line segment into equal parts;</claim-text>
<claim-text>calculate, on the basis of straight lines connecting the first vanishing point to the first equal division points, generating the division points on each of the third side and the fourth side so as to divide each of the third side and the fourth side into unequal parts; and</claim-text>
<claim-text>calculate, on the basis of straight lines connecting the second vanishing point to the second equal division points, the division points on each of the first side and the second side so as to divide each of the first side and the second side into unequal parts.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
