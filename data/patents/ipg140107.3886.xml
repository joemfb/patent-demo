<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08624953-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08624953</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13532509</doc-number>
<date>20120625</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<us-term-of-grant>
<disclaimer>
<text>This patent is subject to a terminal disclaimer.</text>
</disclaimer>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>7</main-group>
<subgroup>14</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>7</main-group>
<subgroup>15</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20110101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>7</main-group>
<subgroup>173</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classifications-cpc>
<main-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>7</main-group>
<subgroup>152</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
</main-cpc>
<further-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>7</main-group>
<subgroup>173</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
</further-cpc>
</classifications-cpc>
<classification-national>
<country>US</country>
<main-classification>348 1401</main-classification>
<further-classification>348 1408</further-classification>
<further-classification>348 1412</further-classification>
</classification-national>
<invention-title id="d2e51">Non-bandwidth intensive method for providing multiple levels of censoring in an A/V stream</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>8086076</doc-number>
<kind>B2</kind>
<name>Tian et al.</name>
<date>20111200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>2003/0193559</doc-number>
<kind>A1</kind>
<name>Fernandez et al.</name>
<date>20031000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>2004/0012613</doc-number>
<kind>A1</kind>
<name>Rast</name>
<date>20040100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>2005/0060740</doc-number>
<kind>A1</kind>
<name>Stecyk</name>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00005">
<othercit>Brickley, Dan, et al., &#x201c;Semantic Webapps? Lightweight RDF Interfaces for SVG,&#x201d; Jul. 21, 2004; 5 pages; http://www.w3.org/2001/sw/Europe/200407/svgwebapps/.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00006">
<othercit>Klingemann, Mario, &#x201c;Superfast Blur,&#x201d; Quasimondo: Incubator: Processing: Superfast Blur Blog; [Retrieved and printed Dec. 26, 2011] http://incubator.quasimondo.com/processing/superfast<sub>&#x2014;</sub>blur.pdf; 1 page.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00007">
<othercit>Olaizola, Igor G., et al., &#x201c;MHP Oriented Interactive Augmented Reality System for Sports Broadcasting Environments,&#x201d; Journal of Virtual Reality and Broadcasting, vol. 3 (2006) No. 13, 11 pages http://www.jvrb.org/3.2006/euroitv2006/786/.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>20</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>348 1401</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348 1402</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348 1408</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348 1409</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348 1412- 1415</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>370259</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>370260</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>709204</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>725 25</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>5</number-of-drawing-sheets>
<number-of-figures>5</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>12175009</doc-number>
<date>20080717</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>8223187</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13532509</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120262539</doc-number>
<kind>A1</kind>
<date>20121018</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Joshi</last-name>
<first-name>Neil</first-name>
<address>
<city>Milpitas</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Rexroad</last-name>
<first-name>Michael</first-name>
<address>
<city>San Jose</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Joshi</last-name>
<first-name>Neil</first-name>
<address>
<city>Milpitas</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Rexroad</last-name>
<first-name>Michael</first-name>
<address>
<city>San Jose</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Patent Capital Group</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Cisco Technology, Inc.</orgname>
<role>02</role>
<address>
<city>San Jose</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Nguyen</last-name>
<first-name>Tuan D</first-name>
<department>2656</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Particular embodiments generally relate to providing different censoring levels for audio-video streams. In one embodiment, an audio-video stream is received. Metadata including censoring information for the audio-video stream is also received. The audio-video stream and metadata may be sent by a source in separate streams. A first level of censoring associated with a first receiver is determined along with a second level of censoring for a second receiver. A first audio-video stream is generated for the first receiver with a first censor level and a second audio-video stream is generated for the second receiver with a second censor level. For example, the first audio-video stream may include censoring of faces and the second audio-video stream may be an audio-video stream without censoring or may censor some other part of the audio-video. The first audio-video stream and the second audio-video stream are then sent to the first and second receivers, respectively.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="202.01mm" wi="256.12mm" file="US08624953-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="190.16mm" wi="146.47mm" orientation="landscape" file="US08624953-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="256.29mm" wi="201.00mm" orientation="landscape" file="US08624953-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="208.36mm" wi="90.85mm" file="US08624953-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="234.44mm" wi="151.13mm" file="US08624953-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="175.01mm" wi="159.00mm" file="US08624953-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">RELATED APPLICATION</heading>
<p id="p-0002" num="0001">This application is a continuation (and claims the benefit of priority under 35 U.S.C. &#xa7;120) of U.S. application Ser. No. 12/175,009, filed Jul. 17, 2008, entitled &#x201c;NON-BANDWIDTH INTENSIVE METHOD FOR PROVIDING MULTIPLE LEVELS OF CENSORING IN AN A/V STREAM,&#x201d; Inventors Neil Joshi, et al. The disclosure of the prior application is considered part of (and is incorporated by reference in) the disclosure of this application.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">TECHNICAL FIELD</heading>
<p id="p-0003" num="0002">Particular embodiments generally relate to networking.</p>
<heading id="h-0003" level="1">BACKGROUND</heading>
<p id="p-0004" num="0003">In audio-video conferencing, a source may send audio-video that is destined for multiple receivers. If censoring is required by some receivers but not others, the source has to send different audio-video streams to different receivers. For example, different receivers may require different levels of censoring, such as one receiver may require the blurring of faces and another receiver may require the original (i.e., non-censored) audio-video stream. Thus, a source has to send a censored audio-video stream along with a non-censored audio-video stream. Sending the censored and non-censored audio-video streams from the source uses valuable bandwidth, which may also affect the quality of the conference being provided.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0005" num="0004"><figref idref="DRAWINGS">FIG. 1</figref> depicts a simplified system for providing audio-video streams according to one embodiment.</p>
<p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. 2</figref> depicts a more detailed example of a source endpoint and a multi-point control unit according to one embodiment.</p>
<p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. 3</figref> depicts a simplified flowchart for sending audio-video and metadata from source endpoint.</p>
<p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. 4</figref> depicts a simplified flowchart of a method for processing the audio-video stream and metadata at multi-point control unit according to one embodiment.</p>
<p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. 5</figref> depicts an example of a location that includes endpoint <b>104</b> or <b>106</b> according to one embodiment.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION OF EMBODIMENTS</heading>
<p id="h-0006" num="0000">Overview</p>
<p id="p-0010" num="0009">Particular embodiments generally relate to providing different censoring levels for audio-video streams. In one embodiment, an audio-video stream is received. Metadata including censoring information for the audio-video stream is also received. The audio-video stream and metadata may be sent by a source in separate streams. A first level of censoring associated with a first receiver is determined along with a second level of censoring for a second receiver. A first audio-video stream is generated for the first receiver with a first censoring level and a second audio-video stream is generated for the second receiver with a second censoring level. For example, the first audio-video stream may include censoring of faces and the second audio-video stream may be an audio-video stream without censoring or may censor some other part of the audio-video, such as the audio. The first audio-video stream and the second audio-video stream are then sent to the first and second receivers, respectively.</p>
<heading id="h-0007" level="1">Example Embodiments</heading>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 1</figref> depicts a simplified system for providing audio-video streams according to one embodiment. A multi-point control unit (MCU) <b>102</b>, a source endpoint <b>104</b>, and destination endpoints <b>106</b> are provided. It will be understood that other components that can be provided in a network may be used but are not shown. For example, audio-video may be sent over networks including wireless or wired networks. Also, various routers, switches, and conference equipment may also be included to provide the audio-video conference.</p>
<p id="p-0012" num="0011">Source endpoint <b>104</b> and destination endpoints <b>106</b> may be endpoints that can send and receive audio-video. For example, source endpoints and destination endpoints may be conference managers used in an audio-video conference. In one embodiment, source endpoint <b>104</b> may include capture devices, such as cameras and microphones, that capture audio-video of a conference, such as audio and/or video of users participating in the conference. That audio and/or video may be sent to destination endpoints <b>106</b>. Destination endpoints <b>106</b> are then configured to render the audio and/or video.</p>
<p id="p-0013" num="0012">Multi-point control unit <b>102</b> is configured to receive audio-video from source endpoint <b>104</b> and switch it to different destination endpoints <b>106</b>. For example, audio-video may be received from a number of endpoints, switched, and sent to other endpoints. Although the disclosure discusses a single source endpoint <b>104</b> and multiple destination endpoints <b>106</b>, it will be understood that each endpoint may include both functions (source and destination functions). Additionally, multi-point control unit <b>102</b> may be receiving multiple audio-video streams from any source or destination endpoint to provide switching in the conference.</p>
<p id="p-0014" num="0013">Source endpoint <b>104</b> provides an audio-video stream to MCU <b>102</b>. The audio-video stream may be a standard, uncensored audio-video stream. It should be noted that if multiple segments are being captured, such as in a three-screen display conference, three audio-video streams may be sent. That is, video for three different angles of a conference may be captured using three cameras. Also, audio may be captured, which may be the same for all three angles or may be captured from different microphones. When an audio-video stream is described, it will be understood that this may mean a video stream only, an audio stream only, or an audio and video stream. The audio and video stream may be two separate streams but the audio is associated with the video (i.e., it is time synced with the video).</p>
<p id="p-0015" num="0014">The audio-video stream may include a stream of packets that carries content captured. Any protocol may be used to send the content. For example, the Real-time Transport Protocol (or RTP) defines a standardized packet format for delivering audio and video over the Internet.</p>
<p id="p-0016" num="0015">Metadata for the audio-video stream may also be sent where the metadata is used for censoring the audio-video content. The metadata may be of a streaming nature that includes time stamps as to when it should be applied to the audio-video stream. Examples of metadata include coordinates of bounding boxes and image frames to be blurred or time-stamp flags indicating when to distort audio. If three audio-video streams are being sent, three different metadata streams may also be sent for the audio-video streams (or a single metadata stream may be sent that includes metadata for the three audio-video streams).</p>
<p id="p-0017" num="0016">The metadata indicating when and what to censor is transmitted alongside the audio-video streams from source endpoint <b>104</b> when media is transmitted. The metadata can be transmitted on a per segment basis for each audio-video stream sent. For a triple screen endpoint, there would be three metadata streams being transmitted. Each metadata stream contains censoring information for a single audio-video stream.</p>
<p id="p-0018" num="0017">Multi-point control unit <b>102</b> is configured to receive the audio-video stream and the metadata stream and can provide different levels of censoring for destination endpoints <b>106</b>. Each destination endpoint <b>106</b> may have a different censoring level associated with it. For example, a policy may provide different censoring levels. In one example, a destination endpoint <b>106</b>-<b>1</b> may be set to receive face-blurring censoring and unscrambled audio. In this case, the video stream received may be processed to provide censoring using the metadata provided. The metadata may include bounding box coordinates that indicate which areas of the video should be censored. Also, destination endpoint <b>106</b>-<b>2</b> may be set to receive scrambled audio. In this case, the metadata may be used to determine a time to scramble the audio. Further, destination endpoint <b>106</b>-<b>3</b> may be set to receive standard audio-video, that is, uncensored audio-video. Multi-point control unit <b>102</b> generates these audio-video streams with different censoring levels and sends them to the various destination endpoints <b>106</b>.</p>
<p id="p-0019" num="0018">Accordingly, particular embodiments provide different levels of censoring. A standard audio-video stream is sent from source endpoint <b>104</b> along with the metadata. This allows a single audio-video stream to be sent from source endpoint <b>104</b> to multi-point control unit <b>102</b>. This saves bandwidth in that source endpoint <b>104</b> does not need to send multiple audio-video streams with different censoring levels. The metadata is then used by multi-point control unit <b>102</b> to censor the audio-video and to send different audio-video streams to destination endpoints <b>106</b>.</p>
<p id="p-0020" num="0019">Because multi-point control unit <b>102</b> has to send audio-video streams individually to each destination endpoint <b>106</b> even if no censoring at all is provided, additional bandwidth is not being used by providing different censoring levels. However, if multiple audio-video streams with different censoring levels are sent from source endpoint <b>104</b> to multi-point control unit <b>102</b>, additional bandwidth is being used because if no censoring is being performed, only one audio-video stream needs to be sent from source endpoint <b>104</b> to multi-point control unit <b>102</b>. Accordingly, particular embodiments provide different levels of censoring with minimal impact to bandwidth by allowing source endpoint <b>104</b> to send an uncensored audio-video stream with a metadata stream, which uses less bandwidth than sending another audio-video stream.</p>
<p id="p-0021" num="0020">Also, providing the censoring processing in multi-point control unit <b>102</b> leverages CPU processing in the multi-point control unit that may be more powerful than that which is included in source endpoint <b>104</b>. For example, multi-point control unit <b>102</b> or a proxy may include accelerated hardware that is dedicated to decoding, censoring, and encoding with low latency. A source endpoint may not include this hardware. Thus, it is desirable to offload the censoring to multi-point control unit <b>102</b>. Also, additional hardware on an endpoint is not needed to provide different censoring levels.</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 2</figref> depicts a more detailed example of source endpoint <b>104</b> and multi-point control unit <b>102</b> according to one embodiment. Source endpoint <b>104</b> receives audio-video at a metadata determiner <b>202</b>. Metadata determiner <b>202</b> may analyze the audio-video to determine censoring information. For example, face detection may be performed to detect bounding boxes of faces found in the video. Also, the audio stream may be analyzed to determine when audio should be distorted. Other methods of determining censoring information may also be used.</p>
<p id="p-0023" num="0022">A metadata sender <b>204</b> is configured to generate a message that can be sent to multi-point control unit <b>102</b>. In one example, a metadata message is transmitted via real-time transfer control protocol (RTCP) application messages. The messages may include a list that is in the following structure: {&#x3c;RTP time-stamp from audio-video stream &#x3e;, &#x3c;error flags&#x3e;, &#x3c;upper left x, upper left y, lower left x, lower right y&#x3e;, &#x3c; . . . &#x3e;, . . . }. In essence, each packet includes a time-stamp to match the respective audio-video stream, error flags in cases where the metadata determination process has failed or errors have occurred (e.g., face detection failed), and a list of bounding boxes to be blurred.</p>
<p id="p-0024" num="0023">The time stamp is a time referring to when audio-video content should be censored. The bounding box coordinates are coordinates that define a blurring area. Method other than using coordinates may be used to identify a blurring area. The error flags indicate whether any errors may have occurred during the metadata generation process. It should be noted that other censoring information may be provided, such as a time-stamp to distort audio instead of the bounding box coordinates.</p>
<p id="p-0025" num="0024">Additionally, different bounding boxes may block one person but not another on the same screen. However, in some cases, all persons may be blocked if blurring of faces is desired. Also, if a triple screen endpoint is being used, it is possible to block one segment (i.e., one camera angle) and not the other. Also, the censoring may be applied to an entire endpoint regardless of the number of segments (all faces are blocked from an endpoint).</p>
<p id="p-0026" num="0025">In addition to sending metadata, an audio-video sender <b>206</b> sends the audio-video information to multi-point control unit <b>102</b>. Thus, as discussed above, an audio-video stream is sent along with a metadata stream. In one embodiment, the metadata may be included with the audio-video stream, such as in a header or embedded within the transport stream.</p>
<p id="p-0027" num="0026">The metadata stream and audio-video stream is received at audio-video and metadata receiver <b>208</b>. Receiver <b>208</b> determines which destination endpoints <b>106</b> audio-video needs to be sent.</p>
<p id="p-0028" num="0027">An endpoint audio-video processor <b>212</b> is configured to process the audio-video stream using censoring information in the metadata. An endpoint policy determiner <b>210</b> determines different policies for various destination endpoints <b>106</b>. Based on the policy, a different censoring level is provided for destination endpoints <b>106</b>.</p>
<p id="p-0029" num="0028">Before censoring can be applied, the video may need to be decoded. A decoder <b>214</b> is configured to decode the audio-video. For example, the audio-video may have been compressed by source <b>104</b> and sent over a network. The compressed audio-video is decoded. The audio-video is decoded because censoring needs to be applied to the decoded audio-video.</p>
<p id="p-0030" num="0029">A censor processor <b>216</b> is configured to censor the audio-video. For example, an overlay is generated and applied to the decoded audio-video stream based on the coordinates found in the metadata. Based on the metadata information, a bounding box is determined and an overlay is generated in the video stream. This overlay blurs the faces that were detected based on the coordinates provided. In another example, a time-stamp may be determined and the audio stream is distorted during the time specified.</p>
<p id="p-0031" num="0030">An encoder <b>218</b> is configured to encode the audio-video with the censoring information in it. The encoded audio-video may then be sent to destination endpoint <b>106</b>. As shown, three different audio-video streams may be sent with different censoring levels to destination endpoints <b>106</b>.</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 3</figref> depicts a simplified flowchart for sending audio-video and metadata from source endpoint <b>104</b>. Step <b>302</b> captures the audio-video. For example, in a three-screen endpoint, three different video streams may be captured. Also, a single audio stream or multiple audio streams may be captured.</p>
<p id="p-0033" num="0032">Step <b>304</b> determines metadata for censoring the audio-video. For example, face detection may be performed to determine bounding box coordinates for user's faces. The metadata may be determined for each audio-video stream.</p>
<p id="p-0034" num="0033">Step <b>306</b> sends the audio-video. For example, audio-video may be encoded and sent to multi-point control unit <b>102</b>.</p>
<p id="p-0035" num="0034">Step <b>308</b> sends the metadata. For example, for each audio-video stream, metadata stream may be transmitted.</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 4</figref> depicts a simplified flowchart of a method for processing the audio-video stream and metadata at multi-point control unit <b>102</b> according to one embodiment. Step <b>402</b> receives the audio-video stream and metadata.</p>
<p id="p-0037" num="0036">Step <b>404</b> determines if the audio-video should be censored. If the audio-video does not need to be censored, step <b>406</b> sends uncensored audio-video to destination endpoints <b>106</b>.</p>
<p id="p-0038" num="0037">In some cases, destination endpoints <b>106</b> may require censoring. Accordingly, step <b>408</b> determines a policy for each destination endpoint <b>106</b>.</p>
<p id="p-0039" num="0038">Step <b>410</b> applies metadata to the audio-video based on the policy. For example, the metadata may be analyzed to determine censoring information. The censoring information is then used to censor the audio-video, such as a bounding box may be determined from the metadata and the bounding box is used to blur the audio-video.</p>
<p id="p-0040" num="0039">Step <b>412</b> sends the censored audio-video to the destination endpoint <b>106</b>. Step <b>414</b> determines if more destination endpoints require censoring. If so, the process reiterates to step <b>408</b>. If not, the process reiterates to step <b>402</b> as audio-video is constantly processed for censoring.</p>
<p id="p-0041" num="0040">To provide for increased security for a censoring policy, certain actions may be performed if the metadata stream is blocked, corrupt, or lost. For example, because the metadata stream is sent separately, there is a chance it may be lost. It may not be desirable to display uncensored audio-video if the metadata is lost. In one example, if a destination endpoint is set to receive face blurring and unscrambled audio, the audio-video to that specific destination endpoint may be dropped or the entire screen blurred with unscrambled audio if the metadata is dropped. For example, multi-point control unit <b>102</b> may provide the best experience for the conference without violating the censoring policy. In this case, instead of taking a chance of revealing a person's face, the entire audio-video may be blurred. This ensures that at least the censoring policy is adhered to.</p>
<p id="p-0042" num="0041">In another embodiment, a confidence level along with a list of coordinates may be provided in the metadata. The confidence level may be generated by metadata determiner <b>202</b> (e.g., a face detection algorithm) and inserted into each packet with censoring coordinates. Multi-point control unit <b>102</b> may make policy-level decisions regarding how much censoring to perform based on this confidence level. For example, if two faces have been detected in a segment, and this number suddenly drops to one face detected, the confidence level may be dropped. Multi-point control unit <b>102</b> may have a threshold for waiting for the second participant to return to a position where their face can be accurately detected. If a time-out occurs, it is recognized that the participant has left the segment. If multi-point control unit <b>102</b> does not receive a confidence level above a configured amount, then the segment could be censored to a lowest allowable level for each destination endpoint <b>106</b>. Also, a hand-shaking mechanism to guarantee receipt of the censoring information may be provided. For example, an acknowledgement may be sent after multi-point control unit <b>102</b> has received a metadata packet.</p>
<p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. 5</figref> depicts an example of a location that includes endpoint <b>104</b> or <b>106</b> according to one embodiment. Although a three screen display is shown, it will be understood that other configurations may be provided. For example, the arrangement and number of displays and users may be different.</p>
<p id="p-0044" num="0043">Users may be participating in a conference and may be situated around conference table <b>502</b>. During the conference, the users may engage in the session as speakers.</p>
<p id="p-0045" num="0044">Display screens <b>504</b> include any devices that can display an image of one or more conference users at location <b>104</b>. Examples of display screens <b>504</b> include a flat screen TVs, notebook PCs, monitors, etc. In one embodiment, display screens <b>504</b> may display three different segments. For example, video streams from three different locations may be displayed. The three video streams display different users from different locations. Although three display screens are described, it will be understood that any number of screens may be used. The screens may be virtual, such as a display device may have three windows displaying three locations.</p>
<p id="p-0046" num="0045">In one embodiment, location may include a number of cameras <b>510</b> that capture video of the users. For example, three cameras <b>510</b> may capture video of three different areas. Although three cameras are described, it will be understood that any number of cameras <b>510</b> may be provided. The three cameras <b>510</b> generate three video streams that may be sent to a conference end point-conference manager <b>506</b>. Conference manager <b>506</b> may then send the video streams to multi-point control unit <b>102</b>. In addition to the video streams, audio may be captured for the users from microphones <b>508</b>. For example, audio for the entire location may be captured by all three microphones <b>508</b>. In accordance with an example embodiment, individual audio streams may be captured by placing microphones in the vicinity of each conference participants. In accordance with this embodiment, each one of the audio streams is associated with a video stream from the corresponding conference participant. Each location may have three video streams captured (i.e., segments) as well as three associated audio streams. Any of these segments may be displayed on display screens <b>504</b> in remote locations.</p>
<p id="p-0047" num="0046">Accordingly, three audio-video streams may be captured and sent by conference manager <b>502</b>. The location is configured to provide an enhanced conference experience. The bandwidth used to send the three audio-video streams may be large. Thus, sending multiple versions audio-video streams may use too much bandwidth to provide an effective conference. Thus, particular embodiments provide a method to provide a high quality conference while minimizing bandwidth used because only uncensored audio-video streams versions of the captured video streams are sent.</p>
<p id="p-0048" num="0047">Particular embodiments may be useful for anybody who desires multiple levels of privacy or censoring and would like to do so with minimal bandwidth impact. Different applications may use particular embodiments. For example, in a judicial situation with witnesses or informants, retaining anonymity is very important. Also, military-intelligence may need identities to be secret. Also, anonymous tips for police to use allow for back and forth interaction. Other applications may include anonymous interactions for users while still retaining contacts of voice or body language while censoring the face, anonymous therapy, conference observation, or other applications may be provided.</p>
<p id="p-0049" num="0048">Although the description has been described with respect to particular embodiments thereof, these particular embodiments are merely illustrative, and not restrictive. Although a conference is described, it will be understood that particular embodiments may be used in other applications, such as streaming video, Internet protocol television (IPTV), etc.</p>
<p id="p-0050" num="0049">Any suitable programming language can be used to implement the routines of particular embodiments including C, C++, Java, assembly language, etc. Different programming techniques can be employed such as procedural or object oriented. The routines can execute on a single processing device or multiple processors. Although the steps, operations, or computations may be presented in a specific order, this order may be changed in different particular embodiments. In some particular embodiments, multiple steps shown as sequential in this specification can be performed at the same time.</p>
<p id="p-0051" num="0050">Particular embodiments may be implemented in a computer-readable storage medium for use by or in connection with the instruction execution system, apparatus, system, or device. Particular embodiments can be implemented in the form of control logic in software or hardware or a combination of both. The control logic, when executed by one or more processors, may be operable to perform that which is described in particular embodiments.</p>
<p id="p-0052" num="0051">Particular embodiments may be implemented by using a programmed general purpose digital computer, by using application specific integrated circuits, programmable logic devices, field programmable gate arrays, optical, chemical, biological, quantum or nanoengineered systems, components and mechanisms may be used. In general, the functions of particular embodiments can be achieved by any means as is known in the art. Distributed, networked systems, components, and/or circuits can be used. Communication, or transfer, of data may be wired, wireless, or by any other means.</p>
<p id="p-0053" num="0052">It will also be appreciated that one or more of the elements depicted in the drawings/figures can also be implemented in a more separated or integrated manner, or even removed or rendered as inoperable in certain cases, as is useful in accordance with a particular application. It is also within the spirit and scope to implement a program or code that can be stored in a machine-readable medium to permit a computer to perform any of the methods described above.</p>
<p id="p-0054" num="0053">As used in the description herein and throughout the claims that follow, &#x201c;a&#x201d;, &#x201c;an&#x201d;, and &#x201c;the&#x201d; includes plural references unless the context clearly dictates otherwise. Also, as used in the description herein and throughout the claims that follow, the meaning of &#x201c;in&#x201d; includes &#x201c;in&#x201d; and &#x201c;on&#x201d; unless the context clearly dictates otherwise.</p>
<p id="p-0055" num="0054">Thus, while particular embodiments have been described herein, latitudes of modification, various changes, and substitutions are intended in the foregoing disclosures, and it will be appreciated that in some instances some features of particular embodiments will be employed without a corresponding use of other features without departing from the scope and spirit as set forth. Therefore, many modifications may be made to adapt a particular situation or material to the essential scope and spirit.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>We claim:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method comprising:
<claim-text>receiving an audio-video (&#x201c;A/V&#x201d;) stream from a source;</claim-text>
<claim-text>receiving metadata comprising censoring information for the A/V stream;</claim-text>
<claim-text>applying the metadata to the A/V stream based on a censorship policy for a first destination endpoint to generate a first censored A/V stream; and</claim-text>
<claim-text>transmitting the first censored A/V stream to the first destination endpoint.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising:
<claim-text>applying the metadata to the A/V stream based on a censorship policy for a second destination endpoint to generate a second censored A/V stream; and</claim-text>
<claim-text>transmitting the second censored A/V stream to the second destination endpoint.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising:
<claim-text>transmitting the received A/V stream to a second destination endpoint in uncensored form.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref> further comprising:
<claim-text>determining that an error has occurred in connection with the metadata; and</claim-text>
<claim-text>transmitting the first censored A/V stream to the second destination endpoint as well as the first destination endpoint.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the metadata comprises coordinate information associated with a video component of the A/V stream, wherein the coordinate information is used to censor the video component at coordinates identified by the coordinate information.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the metadata comprises a first data stream and the A/V stream comprises a second data stream separate from the first data stream.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the metadata includes a timestamp indicating when censoring information should be applied to the A/V stream.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising:
<claim-text>decoding the received A/V stream prior to applying the metadata thereto; and</claim-text>
<claim-text>encoding the first censored A/V stream prior to transmission thereof to the first destination endpoint.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising:
<claim-text>adjusting application of the metadata to the received A/V stream based on a confidence level included with the metadata.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. An apparatus comprising:
<claim-text>one or more processors; and</claim-text>
<claim-text>logic encoded in one or more non-transitory tangible media for execution by the one or more processors and when executed operable to:
<claim-text>receive an audio-video (&#x201c;A/V&#x201d;) stream from a source;</claim-text>
<claim-text>receive metadata comprising censoring information for the A/V stream;</claim-text>
<claim-text>apply the metadata to the A/V stream based on a censorship policy for a first destination endpoint to generate a first censored A/V stream; and</claim-text>
<claim-text>transmit the first censored A/V stream to the first destination endpoint.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The apparatus of <claim-ref idref="CLM-00010">claim 10</claim-ref> wherein the logic is further operable to:
<claim-text>apply the metadata to the A/V stream based on a censorship policy for a second destination endpoint to generate a second censored A/V stream; and</claim-text>
<claim-text>transmit the second censored A/V stream to the second destination endpoint.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The apparatus of <claim-ref idref="CLM-00010">claim 10</claim-ref> wherein the logic is further operable to:
<claim-text>transmit the received A/V stream to a second destination endpoint in uncensored form.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The apparatus of <claim-ref idref="CLM-00012">claim 12</claim-ref> wherein the logic is further operable to:
<claim-text>determine that an error has occurred in connection with the metadata; and</claim-text>
<claim-text>transmit the first censored A/V stream to the second destination endpoint as well as the first destination endpoint.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The apparatus of <claim-ref idref="CLM-00010">claim 10</claim-ref> wherein the logic is further operable to:
<claim-text>decode the received A/V stream prior to applying the metadata thereto; and</claim-text>
<claim-text>encode the first censored A/V stream prior to transmission thereof to the first destination endpoint.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The apparatus of <claim-ref idref="CLM-00010">claim 10</claim-ref> wherein the logic is further operable to:
<claim-text>adjust application of the metadata to the received A/V stream based on a confidence level included with the metadata.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. An apparatus comprising:
<claim-text>a memory element configured to store data;</claim-text>
<claim-text>a processor operable to execute instructions associated with the data, and</claim-text>
<claim-text>an end point audio-video (&#x201c;A/V&#x201d;) processing module configured to:
<claim-text>receive an A/V stream from a source;</claim-text>
<claim-text>receive metadata comprising censoring information for the A/V stream;</claim-text>
<claim-text>apply the metadata to the A/V stream based on a censorship policy for a first destination endpoint to generate a first censored A/V stream; and</claim-text>
<claim-text>transmit the first censored A/V stream to the first destination endpoint.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The apparatus of <claim-ref idref="CLM-00016">claim 16</claim-ref> wherein the A/V endpoint processing module is further configured to:
<claim-text>apply the metadata to the A/V stream based on a censorship policy for a second destination endpoint to generate a second censored A/V stream; and</claim-text>
<claim-text>transmit the second censored A/V stream to the second destination endpoint.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The apparatus of <claim-ref idref="CLM-00016">claim 16</claim-ref> wherein the A/V endpoint processing module is further configured to:
<claim-text>transmit the received A/V stream to a second destination endpoint in uncensored form.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The apparatus of <claim-ref idref="CLM-00018">claim 18</claim-ref> wherein the A/V endpoint processing module is further configured to:
<claim-text>determine that an error has occurred in connection with the metadata; and</claim-text>
<claim-text>transmit the first censored A/V stream to the second destination endpoint as well as the first destination endpoint.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The apparatus of <claim-ref idref="CLM-00016">claim 16</claim-ref> wherein the A/V endpoint processing module is further configured to:
<claim-text>decode the received A/V stream prior to applying the metadata thereto; and</claim-text>
<claim-text>encode the first censored A/V stream prior to transmission thereof to the first destination endpoint. </claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
